{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Emergence of the Shape Bias Results from Communicative Efficiency. (arXiv:2109.06232v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06232","description":"<p>By the age of two, children tend to assume that new word categories are based\non objects' shape, rather than their color or texture; this assumption is\ncalled the shape bias. They are thought to learn this bias by observing that\ntheir caregiver's language is biased towards shape based categories. This\npresents a chicken and egg problem: if the shape bias must be present in the\nlanguage in order for children to learn it, how did it arise in language in the\nfirst place? In this paper, we propose that communicative efficiency explains\nboth how the shape bias emerged and why it persists across generations. We\nmodel this process with neural emergent language agents that learn to\ncommunicate about raw pixelated images. First, we show that the shape bias\nemerges as a result of efficient communication strategies employed by agents.\nSecond, we show that pressure brought on by communicative need is also\nnecessary for it to persist across generations; simply having a shape bias in\nan agent's input language is insufficient. These results suggest that, over and\nabove the operation of other learning strategies, the shape bias in human\nlearners may emerge and be sustained by communicative pressures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portelance_E/0/1/0/all/0/1\">Eva Portelance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_M/0/1/0/all/0/1\">Michael C. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language Models via Knowledge Distillation. (arXiv:2109.06243v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06243","description":"<p>The development of over-parameterized pre-trained language models has made a\nsignificant contribution toward the success of natural language processing.\nWhile over-parameterization of these models is the key to their generalization\npower, it makes them unsuitable for deployment on low-capacity devices. We push\nthe limits of state-of-the-art Transformer-based pre-trained language model\ncompression using Kronecker decomposition. We use this decomposition for\ncompression of the embedding layer, all linear mappings in the multi-head\nattention, and the feed-forward network modules in the Transformer layer. We\nperform intermediate-layer knowledge distillation using the uncompressed model\nas the teacher to improve the performance of the compressed model. We present\nour KroneckerBERT, a compressed version of the BERT_BASE model obtained using\nthis framework. We evaluate the performance of KroneckerBERT on well-known NLP\nbenchmarks and show that for a high compression factor of 19 (5% of the size of\nthe BERT_BASE model), our KroneckerBERT outperforms state-of-the-art\ncompression methods on the GLUE. Our experiments indicate that the proposed\nmodel has promising out-of-distribution robustness and is superior to the\nstate-of-the-art compression methods on SQuAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tahaei_M/0/1/0/all/0/1\">Marzieh S. Tahaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlaix_E/0/1/0/all/0/1\">Ella Charlaix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation. (arXiv:2109.06253v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06253","description":"<p>Neural Machine Translation (NMT) is known to suffer from a beam-search\nproblem: after a certain point, increasing beam size causes an overall drop in\ntranslation quality. This effect is especially pronounced for long sentences.\nWhile much work was done analyzing this phenomenon, primarily for\nautoregressive NMT models, there is still no consensus on its underlying cause.\nIn this work, we analyze errors that cause major quality degradation with large\nbeams in NMT and Automatic Speech Recognition (ASR). We show that a factor that\nstrongly contributes to the quality degradation with large beams is\n\\textit{dataset length-bias} - \\textit{NMT datasets are strongly biased towards\nshort sentences}. To mitigate this issue, we propose a new data augmentation\ntechnique -- \\textit{Multi-Sentence Resampling (MSR)}. This technique extends\nthe training examples by concatenating several sentences from the original\ndataset to make a long training example. We demonstrate that MSR significantly\nreduces degradation with growing beam size and improves final translation\nquality on the IWSTL$15$ En-Vi, IWSTL$17$ En-Fr, and WMT$14$ En-De datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1\">Ivan Provilkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Multiway Multilingual NMT in the Turkic Languages. (arXiv:2109.06262v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06262","description":"<p>Despite the increasing number of large and comprehensive machine translation\n(MT) systems, evaluation of these methods in various languages has been\nrestrained by the lack of high-quality parallel corpora as well as engagement\nwith the people that speak these languages. In this study, we present an\nevaluation of state-of-the-art approaches to training and evaluating MT systems\nin 22 languages from the Turkic language family, most of which being extremely\nunder-explored. First, we adopt the TIL Corpus with a few key improvements to\nthe training and the evaluation sets. Then, we train 26 bilingual baselines as\nwell as a multi-way neural MT (MNMT) model using the corpus and perform an\nextensive analysis using automatic metrics as well as human evaluations. We\nfind that the MNMT model outperforms almost all bilingual baselines in the\nout-of-domain test sets and finetuning the model on a downstream task of a\nsingle pair also results in a huge performance boost in both low- and\nhigh-resource scenarios. Our attentive analysis of evaluation criteria for MT\nmodels in Turkic languages also points to the necessity for further research in\nthis direction. We release the corpus splits, test sets as well as models to\nthe public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1\">Jamshidbek Mirzakhalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Anoop Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunafin_A/0/1/0/all/0/1\">Aigiz Kunafin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1\">Ahsan Wahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moydinboyev_B/0/1/0/all/0/1\">Behzod Moydinboyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_S/0/1/0/all/0/1\">Sardana Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzokova_M/0/1/0/all/0/1\">Mokhiyakhon Uzokova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulatova_S/0/1/0/all/0/1\">Shaxnoza Pulatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyers_F/0/1/0/all/0/1\">Francis Tyers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1\">John Licato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappan_S/0/1/0/all/0/1\">Sriram Chellappan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-OCR Document Correction with large Ensembles of Character Sequence Models. (arXiv:2109.06264v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06264","description":"<p>In this paper, we propose a novel method based on character\nsequence-to-sequence models to correct documents already processed with Optical\nCharacter Recognition (OCR) systems. The main contribution of this paper is a\nset of strategies to accurately process strings much longer than the ones used\nto train the sequence model while being sample- and resource-efficient,\nsupported by thorough experimentation. The strategy with the best performance\ninvolves splitting the input document in character n-grams and combining their\nindividual corrections into the final output using a voting scheme that is\nequivalent to an ensemble of a large number of sequence models. We further\ninvestigate how to weigh the contributions from each one of the members of this\nensemble. We test our method on nine languages of the ICDAR 2019 competition on\npost-OCR text correction and achieve a new state-of-the-art performance in five\nof them. Our code for post-OCR correction is shared at\nhttps://github.com/jarobyte91/post_ocr_correction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_Orta_J/0/1/0/all/0/1\">Juan Ramirez-Orta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xamena_E/0/1/0/all/0/1\">Eduardo Xamena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguitman_A/0/1/0/all/0/1\">Ana Maguitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos Milios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Axel J. Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STraTA: Self-Training with Task Augmentation for Better Few-shot Learning. (arXiv:2109.06270v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06270","description":"<p>Despite their recent successes in tackling many NLP tasks, large-scale\npre-trained language models do not perform as well in few-shot settings where\nonly a handful of training examples are available. To address this shortcoming,\nwe propose STraTA, which stands for Self-Training with Task Augmentation, an\napproach that builds on two key ideas for effective leverage of unlabeled data.\nFirst, STraTA uses task augmentation, a novel technique that synthesizes a\nlarge amount of data for auxiliary-task fine-tuning from target-task unlabeled\ntexts. Second, STraTA performs self-training by further fine-tuning the strong\nbase model created by task augmentation on a broad distribution of\npseudo-labeled data. Our experiments demonstrate that STraTA can substantially\nimprove sample efficiency across 12 few-shot benchmarks. Remarkably, on the\nSST-2 sentiment dataset, STraTA, with only 8 training examples per class,\nachieves comparable results to standard fine-tuning with 67K training examples.\nOur analyses reveal that task augmentation and self-training are both\ncomplementary and independently effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_G/0/1/0/all/0/1\">Grady Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks. (arXiv:2109.06275v1 [cs.AI])","link":"http://arxiv.org/abs/2109.06275","description":"<p>An ideal integration of autonomous agents in a human world implies that they\nare able to collaborate on human terms. In particular, theory of mind plays an\nimportant role in maintaining common ground during human collaboration and\ncommunication. To enable theory of mind modeling in situated interactions, we\nintroduce a fine-grained dataset of collaborative tasks performed by pairs of\nhuman subjects in the 3D virtual blocks world of Minecraft. It provides\ninformation that captures partners' beliefs of the world and of each other as\nan interaction unfolds, bringing abundant opportunities to study human\ncollaborative behaviors in situated language communication. As a first step\ntowards our goal of developing embodied AI agents able to infer belief states\nof collaborative partners in situ, we build and present results on\ncomputational models for several theory of mind tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bara_C/0/1/0/all/0/1\">Cristian-Paul Bara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+CH_Wang_S/0/1/0/all/0/1\">Sky CH-Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Algorithms for Multiparallel Word Alignment. (arXiv:2109.06283v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06283","description":"<p>With the advent of end-to-end deep learning approaches in machine\ntranslation, interest in word alignments initially decreased; however, they\nhave again become a focus of research more recently. Alignments are useful for\ntypological research, transferring formatting like markup to translated texts,\nand can be used in the decoding of machine translation systems. At the same\ntime, massively multilingual processing is becoming an important NLP scenario,\nand pretrained language and machine translation models that are truly\nmultilingual are proposed. However, most alignment algorithms rely on bitexts\nonly and do not leverage the fact that many parallel corpora are multiparallel.\nIn this work, we exploit the multiparallelity of corpora by representing an\ninitial set of bilingual alignments as a graph and then predicting additional\nedges in the graph. We present two graph algorithms for edge prediction: one\ninspired by recommender systems and one based on network link prediction. Our\nexperimental results show absolute improvements in $F_1$ of up to 28% over the\nbaseline bilingual word aligner in different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senel_L/0/1/0/all/0/1\">L&#xfc;tfi Kerem &#x15e;enel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration. (arXiv:2109.06304v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06304","description":"<p>Phrase representations derived from BERT often do not exhibit complex phrasal\ncompositionality, as the model relies instead on lexical similarity to\ndetermine semantic relatedness. In this paper, we propose a contrastive\nfine-tuning objective that enables BERT to produce more powerful phrase\nembeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal\nparaphrases, which is automatically generated using a paraphrase generation\nmodel, as well as a large-scale dataset of phrases in context mined from the\nBooks3 corpus. Phrase-BERT outperforms baselines across a variety of\nphrase-level similarity tasks, while also demonstrating increased lexical\ndiversity between nearest neighbors in the vector space. Finally, as a case\nstudy, we show that Phrase-BERT embeddings can be easily integrated with a\nsimple autoencoder to build a phrase-based neural topic model that interprets\ntopics as mixtures of words and phrases by performing a nearest neighbor search\nin the embedding space. Crowdsourced evaluations demonstrate that this\nphrase-based topic model produces more coherent and meaningful topics than\nbaseline word and phrase-level topic models, further validating the utility of\nPhrase-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shufan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_L/0/1/0/all/0/1\">Laure Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Catastrophic Forgetting in Scheduled Sampling with Elastic Weight Consolidation in Neural Machine Translation. (arXiv:2109.06308v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06308","description":"<p>Despite strong performance in many sequence-to-sequence tasks, autoregressive\nmodels trained with maximum likelihood estimation suffer from exposure bias,\ni.e. a discrepancy between the ground-truth prefixes used during training and\nthe model-generated prefixes used at inference time. Scheduled sampling is a\nsimple and often empirically successful approach which addresses this issue by\nincorporating model-generated prefixes into the training process. However, it\nhas been argued that it is an inconsistent training objective leading to models\nignoring the prefixes altogether. In this paper, we conduct systematic\nexperiments and find that it ameliorates exposure bias by increasing model\nreliance on the input sequence. We also observe that as a side-effect, it\nworsens performance when the model-generated prefix is correct, a form of\ncatastrophic forgetting. We propose using Elastic Weight Consolidation as\ntrade-off between mitigating exposure bias and retaining output quality.\nExperiments on two IWSLT'14 translation tasks demonstrate that our approach\nalleviates catastrophic forgetting and significantly improves BLEU compared to\nstandard scheduled sampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korakakis_M/0/1/0/all/0/1\">Michalis Korakakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Constraints and Descriptive Segmentation for Subevent Detection. (arXiv:2109.06316v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06316","description":"<p>Event mentions in text correspond to real-world events of varying degrees of\ngranularity. The task of subevent detection aims to resolve this granularity\nissue, recognizing the membership of multi-granular events in event complexes.\nSince knowing the span of descriptive contexts of event complexes helps infer\nthe membership of events, we propose the task of event-based text segmentation\n(EventSeg) as an auxiliary task to improve the learning for subevent detection.\nTo bridge the two tasks together, we propose an approach to learning and\nenforcing constraints that capture dependencies between subevent detection and\nEventSeg prediction, as well as guiding the model to make globally consistent\ninference. Specifically, we adopt Rectifier Networks for constraint learning\nand then convert the learned constraints to a regularization term in the loss\nfunction of the neural model. Experimental results show that the proposed\nmethod outperforms baseline methods by 2.3% and 2.5% on benchmark datasets for\nsubevent detection, HiEve and IC, respectively, while achieving a decent\nperformance on EventSeg prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space. (arXiv:2109.06324v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06324","description":"<p>In cross-lingual language models, representations for many different\nlanguages live in the same space. Here, we investigate the linguistic and\nnon-linguistic factors affecting sentence-level alignment in cross-lingual\npretrained language models for 101 languages and 5,050 language pairs. Using\nBERT-based LaBSE and BiLSTM-based LASER as our models, and the Bible as our\ncorpus, we compute a task-based measure of cross-lingual alignment in the form\nof bitext retrieval performance, as well as four intrinsic measures of vector\nspace alignment and isomorphism. We then examine a range of linguistic,\nquasi-linguistic, and training-related features as potential predictors of\nthese alignment metrics. The results of our analyses show that word order\nagreement and agreement in morphological complexity are two of the strongest\nlinguistic predictors of cross-linguality. We also note in-family training data\nas a stronger predictor than language-specific training data across the board.\nWe verify some of our linguistic findings by looking at the effect of\nmorphological segmentation on English-Inuktitut alignment, in addition to\nexamining the effect of word order agreement on isomorphism for 66 zero-shot\nlanguage pairs from a different corpus. We make the data and code for our\nexperiments publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Alex Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transferability of BERT Models on Uralic Languages. (arXiv:2109.06327v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06327","description":"<p>Transformer-based language models such as BERT have outperformed previous\nmodels on a large number of English benchmarks, but their evaluation is often\nlimited to English or a small number of well-resourced languages. In this work,\nwe evaluate monolingual, multilingual, and randomly initialized language models\nfrom the BERT family on a variety of Uralic languages including Estonian,\nFinnish, Hungarian, Erzya, Moksha, Karelian, Livvi, Komi Permyak, Komi Zyrian,\nNorthern S\\'ami, and Skolt S\\'ami. When monolingual models are available\n(currently only et, fi, hu), these perform better on their native language, but\nin general they transfer worse than multilingual models or models of\ngenetically unrelated languages that share the same character set. Remarkably,\nstraightforward transfer of high-resource models, even without special efforts\ntoward hyperparameter optimization, yields what appear to be state of the art\nPOS and NER tools for the minority Uralic languages where there is sufficient\ndata for finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acs_J/0/1/0/all/0/1\">Judit &#xc1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levai_D/0/1/0/all/0/1\">D&#xe1;niel L&#xe9;vai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornai_A/0/1/0/all/0/1\">Andr&#xe1;s Kornai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Old BERT, New Tricks: Artificial Language Learning for Pre-Trained Language Models. (arXiv:2109.06333v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06333","description":"<p>We extend the artificial language learning experimental paradigm from\npsycholinguistics and apply it to pre-trained language models -- specifically,\nBERT (Devlin et al., 2019). We treat the model as a subject in an artificial\nlanguage learning experimental setting: in order to learn the relation between\ntwo linguistic properties A and B, we introduce a set of new, non-existent,\nlinguistic items, give the model information about their variation along\nproperty A, then measure to what extent the model learns property B for these\nitems as a result of training. We show this method at work for degree modifiers\n(expressions like \"slightly\", \"very\", \"rather\", \"extremely\") and test the\nhypothesis that the degree expressed by modifiers (low, medium or high degree)\nis related to their sensitivity to sentence polarity (whether they show\npreference for affirmative or negative sentences or neither). Our experimental\nresults are compatible with existing linguistic observations that relate degree\nsemantics to polarity-sensitivity, including the main one: low degree semantics\nleads to positive polarity sensitivity (that is, to preference towards\naffirmative contexts). The method can be used in linguistics to elaborate on\nhypotheses and interpret experimental results, as well as for more insightful\nevaluation of linguistic representations in language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bylinina_L/0/1/0/all/0/1\">Lisa Bylinina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garmash_E/0/1/0/all/0/1\">Ekaterina Garmash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning. (arXiv:2109.06349v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06349","description":"<p>In this work, we focus on a more challenging few-shot intent detection\nscenario where many intents are fine-grained and semantically similar. We\npresent a simple yet effective few-shot intent detection schema via contrastive\npre-training and fine-tuning. Specifically, we first conduct self-supervised\ncontrastive pre-training on collected intent datasets, which implicitly learns\nto discriminate semantically similar utterances without using any labels. We\nthen perform few-shot intent detection together with supervised contrastive\nlearning, which explicitly pulls utterances from the same intent closer and\npushes utterances across different intents farther. Experimental results show\nthat our proposed method achieves state-of-the-art performance on three\nchallenging intent detection datasets under 5-shot and 10-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Hung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Walter Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Machine Translation Evaluation. (arXiv:2109.06352v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06352","description":"<p>Several neural-based metrics have been recently proposed to evaluate machine\ntranslation quality. However, all of them resort to point estimates, which\nprovide limited information at segment level. This is made worse as they are\ntrained on noisy, biased and scarce human judgements, often resulting in\nunreliable quality predictions. In this paper, we introduce uncertainty-aware\nMT evaluation and analyze the trustworthiness of the predicted quality. We\ncombine the COMET framework with two uncertainty estimation methods, Monte\nCarlo dropout and deep ensembles, to obtain quality scores along with\nconfidence intervals. We compare the performance of our uncertainty-aware MT\nevaluation methods across multiple language pairs from the QT21 dataset and the\nWMT20 metrics task, augmented with MQM annotations. We experiment with varying\nnumbers of references and further discuss the usefulness of uncertainty-aware\nquality estimation (without references) to flag possibly critical translation\nmistakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glushkova_T/0/1/0/all/0/1\">Taisiya Glushkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hunspell for Sorani Kurdish Spell Checking and Morphological Analysis. (arXiv:2109.06374v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06374","description":"<p>Spell checking and morphological analysis are two fundamental tasks in text\nand natural language processing and are addressed in the early stages of the\ndevelopment of language technology. Despite the previous efforts, there is no\nprogress in open-source to create such tools for Sorani Kurdish, also known as\nCentral Kurdish, as a less-resourced language. In this paper, we present our\nefforts in annotating a lexicon with morphosyntactic tags and also, extracting\nmorphological rules of Sorani Kurdish to build a morphological analyzer, a\nstemmer and a spell-checking system using Hunspell. This implementation can be\nused for further developments in the field by researchers and also, be\nintegrated into text editors under a publicly available license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Sina Ahmadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation. (arXiv:2109.06379v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06379","description":"<p>Natural language generation (NLG) spans a broad range of tasks, each of which\nserves for specific objectives and desires different properties of generated\ntext. The complexity makes automatic evaluation of NLG particularly\nchallenging. Previous work has typically focused on a single task and developed\nindividual evaluation metrics based on specific intuitions. In this paper, we\npropose a unifying perspective based on the nature of information change in NLG\ntasks, including compression (e.g., summarization), transduction (e.g., text\nrewriting), and creation (e.g., dialog). Information alignment between input,\ncontext, and output text plays a common central role in characterizing the\ngeneration. With automatic alignment prediction models, we develop a family of\ninterpretable metrics that are suitable for evaluating key aspects of different\nNLG tasks, often without need of gold reference data. Experiments show the\nuniformly designed metrics achieve stronger or comparable correlations with\nhuman judgement compared to state-of-the-art metrics in each of diverse tasks,\nincluding text summarization, style transfer, and knowledge-grounded dialog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Mingkai Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rationales for Sequential Predictions. (arXiv:2109.06387v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06387","description":"<p>Sequence models are a critical component of modern NLP systems, but their\npredictions are difficult to explain. We consider model explanations though\nrationales, subsets of context that can explain individual model predictions.\nWe find sequential rationales by solving a combinatorial optimization: the best\nrationale is the smallest subset of input tokens that would predict the same\noutput as the full sequence. Enumerating all subsets is intractable, so we\npropose an efficient greedy algorithm to approximate this objective. The\nalgorithm, which is called greedy rationalization, applies to any model. For\nthis approach to be effective, the model should form compatible conditional\ndistributions when making predictions on incomplete subsets of the context.\nThis condition can be enforced with a short fine-tuning step. We study greedy\nrationalization on language modeling and machine translation. Compared to\nexisting baselines, greedy rationalization is best at optimizing the\ncombinatorial objective and provides the most faithful rationales. On a new\ndataset of annotated sequential rationales, greedy rationales are most similar\nto human rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vafa_K/0/1/0/all/0/1\">Keyon Vafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuntian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1\">David M. Blei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos. (arXiv:2109.06398v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06398","description":"<p>We address the problem of temporal sentence localization in videos (TSLV).\nTraditional methods follow a top-down framework which localizes the target\nsegment with pre-defined segment proposals. Although they have achieved decent\nperformance, the proposals are handcrafted and redundant. Recently, bottom-up\nframework attracts increasing attention due to its superior efficiency. It\ndirectly predicts the probabilities for each frame as a boundary. However, the\nperformance of bottom-up model is inferior to the top-down counterpart as it\nfails to exploit the segment-level interaction. In this paper, we propose an\nAdaptive Proposal Generation Network (APGN) to maintain the segment-level\ninteraction while speeding up the efficiency. Specifically, we first perform a\nforeground-background classification upon the video and regress on the\nforeground frames to adaptively generate proposals. In this way, the\nhandcrafted proposal design is discarded and the redundant proposals are\ndecreased. Then, a proposal consolidation module is further developed to\nenhance the semantic of the generated proposals. Finally, we locate the target\nmoments with these generated proposals following the top-down framework.\nExtensive experiments on three challenging benchmarks show that our proposed\nAPGN significantly outperforms previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding. (arXiv:2109.06400v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06400","description":"<p>A key solution to temporal sentence grounding (TSG) exists in how to learn\neffective alignment between vision and language features extracted from an\nuntrimmed video and a sentence description. Existing methods mainly leverage\nvanilla soft attention to perform the alignment in a single-step process.\nHowever, such single-step attention is insufficient in practice, since\ncomplicated relations between inter- and intra-modality are usually obtained\nthrough multi-step reasoning. In this paper, we propose an Iterative Alignment\nNetwork (IA-Net) for TSG task, which iteratively interacts inter- and\nintra-modal features within multiple steps for more accurate grounding.\nSpecifically, during the iterative reasoning process, we pad multi-modal\nfeatures with learnable parameters to alleviate the nowhere-to-attend problem\nof non-matched frame-word pairs, and enhance the basic co-attention mechanism\nin a parallel manner. To further calibrate the misaligned attention caused by\neach reasoning step, we also devise a calibration module following each\nattention module to refine the alignment knowledge. With such iterative\nalignment scheme, our IA-Net can robustly capture the fine-grained relations\nbetween vision and language domains step-by-step for progressively reasoning\nthe temporal boundaries. Extensive experiments conducted on three challenging\nbenchmarks demonstrate that our proposed model performs better than the\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Personality and Online Social Engagement: An Investigation of MBTI Users on Twitter. (arXiv:2109.06402v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06402","description":"<p>Text-based personality prediction by computational models is an emerging\nfield with the potential to significantly improve on key weaknesses of\nsurvey-based personality assessment. We investigate 3848 profiles from Twitter\nwith self-labeled Myers-Briggs personality traits (MBTI) - a framework closely\nrelated to the Five Factor Model of personality - to better understand how\ntext-based digital traces from social engagement online can be used to predict\nuser personality traits. We leverage BERT, a state-of-the-art NLP architecture\nbased on deep learning, to analyze various sources of text that hold most\npredictive power for our task. We find that biographies, statuses, and liked\ntweets contain significant predictive power for all dimensions of the MBTI\nsystem. We discuss our findings and their implications for the validity of the\nMBTI and the lexical hypothesis, a foundational theory underlying the Five\nFactor Model that links language use and behavior. Our results hold optimistic\nimplications for personality psychologists, computational linguists, and other\nsocial scientists aiming to predict personality from observational text data\nand explore the links between language and core behavioral traits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadambi_P/0/1/0/all/0/1\">Partha Kadambi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction. (arXiv:2109.06415v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06415","description":"<p>Low-resource Relation Extraction (LRE) aims to extract relation facts from\nlimited labeled corpora when human annotation is scarce. Existing works either\nutilize self-training scheme to generate pseudo labels that will cause the\ngradual drift problem, or leverage meta-learning scheme which does not solicit\nfeedback explicitly. To alleviate selection bias due to the lack of feedback\nloops in existing LRE learning paradigms, we developed a Gradient Imitation\nReinforcement Learning method to encourage pseudo label data to imitate the\ngradient descent direction on labeled data and bootstrap its optimization\ncapability through trial and error. We also propose a framework called GradLRE,\nwhich handles two major scenarios in low-resource relation extraction. Besides\nthe scenario where unlabeled data is sufficient, GradLRE handles the situation\nwhere no unlabeled data is available, by exploiting a contextualized\naugmentation method to generate data. Experimental results on two public\ndatasets demonstrate the effectiveness of GradLRE on low resource relation\nextraction when comparing with baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yawen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-document Event Identity via Dense Annotation. (arXiv:2109.06417v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06417","description":"<p>In this paper, we study the identity of textual events from different\ndocuments. While the complex nature of event identity is previously studied\n(Hovy et al., 2013), the case of events across documents is unclear. Prior work\non cross-document event coreference has two main drawbacks. First, they\nrestrict the annotations to a limited set of event types. Second, they\ninsufficiently tackle the concept of event identity. Such annotation setup\nreduces the pool of event mentions and prevents one from considering the\npossibility of quasi-identity relations. We propose a dense annotation approach\nfor cross-document event coreference, comprising a rich source of event\nmentions and a dense annotation effort between related document pairs. To this\nend, we design a new annotation workflow with careful quality control and an\neasy-to-use annotation interface. In addition to the links, we further collect\noverlapping event contexts, including time, location, and participants, to shed\nsome light on the relation between identity decisions and context. We present\nan open-access dataset for cross-document event coreference, CDEC-WN, collected\nfrom English Wikinews and open-source our annotation toolkit to encourage\nfurther research on cross-document tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1\">Adithya Pratapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_K/0/1/0/all/0/1\">Kimihiro Hasegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamakawa_Y/0/1/0/all/0/1\">Yukari Yamakawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense-Focused Dialogues for Response Generation: An Empirical Study. (arXiv:2109.06427v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06427","description":"<p>Smooth and effective communication requires the ability to perform latent or\nexplicit commonsense inference. Prior commonsense reasoning benchmarks (such as\nSocialIQA and CommonsenseQA) mainly focus on the discriminative task of\nchoosing the right answer from a set of candidates, and do not involve\ninteractive language generation as in dialogue. Moreover, existing dialogue\ndatasets do not explicitly focus on exhibiting commonsense as a facet. In this\npaper, we present an empirical study of commonsense in dialogue response\ngeneration. We first auto-extract commonsensical dialogues from existing\ndialogue datasets by leveraging ConceptNet, a commonsense knowledge graph.\nFurthermore, building on social contexts/situations in SocialIQA, we collect a\nnew dialogue dataset with 25K dialogues aimed at exhibiting social commonsense\nin an interactive setting. We evaluate response generation models trained using\nthese datasets and find that models trained on both extracted and our collected\ndata produce responses that consistently exhibit more commonsense than\nbaselines. Finally we propose an approach for automatic evaluation of\ncommonsense that relies on features derived from ConceptNet and pre-trained\nlanguage and dialog models, and show reasonable correlation with human\nevaluation of responses' commonsense quality. We are releasing a subset of our\ncollected data, Commonsense-Dialogues, containing about 11K dialogs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YES SIR!Optimizing Semantic Space of Negatives with Self-Involvement Ranker. (arXiv:2109.06436v1 [cs.IR])","link":"http://arxiv.org/abs/2109.06436","description":"<p>Pre-trained model such as BERT has been proved to be an effective tool for\ndealing with Information Retrieval (IR) problems. Due to its inspiring\nperformance, it has been widely used to tackle with real-world IR problems such\nas document ranking. Recently, researchers have found that selecting \"hard\"\nrather than \"random\" negative samples would be beneficial for fine-tuning\npre-trained models on ranking tasks. However, it remains elusive how to\nleverage hard negative samples in a principled way. To address the\naforementioned issues, we propose a fine-tuning strategy for document ranking,\nnamely Self-Involvement Ranker (SIR), to dynamically select hard negative\nsamples to construct high-quality semantic space for training a high-quality\nranking model. Specifically, SIR consists of sequential compressors implemented\nwith pre-trained models. Front compressor selects hard negative samples for\nrear compressor. Moreover, SIR leverages supervisory signal to adaptively\nadjust semantic space of negative samples. Finally, supervisory signal in rear\ncompressor is computed based on condition probability and thus can control\nsample dynamic and further enhance the model performance. SIR is a lightweight\nand general framework for pre-trained models, which simplifies the ranking\nprocess in industry practice. We test our proposed solution on MS MARCO with\ndocument ranking setting, and the results show that SIR can significantly\nimprove the ranking performance of various pre-trained models. Moreover, our\nmethod became the new SOTA model anonymously on MS MARCO Document ranking\nleaderboard in May 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_R/0/1/0/all/0/1\">Ruizhi Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Ruofei Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zikai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinxia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongkang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yantao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering Implicit Gender Bias in Narratives through Commonsense Inference. (arXiv:2109.06437v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06437","description":"<p>Pre-trained language models learn socially harmful biases from their training\ncorpora, and may repeat these biases when used for generation. We study gender\nbiases associated with the protagonist in model-generated stories. Such biases\nmay be expressed either explicitly (\"women can't park\") or implicitly (e.g. an\nunsolicited male character guides her into a parking space). We focus on\nimplicit biases, and use a commonsense reasoning engine to uncover them.\nSpecifically, we infer and analyze the protagonist's motivations, attributes,\nmental states, and implications on others. Our findings regarding implicit\nbiases are in line with prior work that studied explicit biases, for example\nshowing that female characters' portrayal is centered around appearance, while\nmale figures' focus on intellect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tenghao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding. (arXiv:2109.06466v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06466","description":"<p>Task-adaptive pre-training (TAPT) and Self-training (ST) have emerged as the\nmajor semi-supervised approaches to improve natural language understanding\n(NLU) tasks with massive amount of unlabeled data. However, it's unclear\nwhether they learn similar representations or they can be effectively combined.\nIn this paper, we show that TAPT and ST can be complementary with simple TFS\nprotocol by following TAPT -&gt; Finetuning -&gt; Self-training (TFS) process.\nExperimental results show that TFS protocol can effectively utilize unlabeled\ndata to achieve strong combined gains consistently across six datasets covering\nsentiment classification, paraphrase identification, natural language\ninference, named entity recognition and dialogue slot classification. We\ninvestigate various semi-supervised settings and consistently show that gains\nfrom TAPT and ST can be strongly additive by following TFS procedure. We hope\nthat TFS could serve as an important semi-supervised baseline for future NLP\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Untrustworthy Samples: Data Filtering for Open-domain Dialogues with Bayesian Optimization. (arXiv:2109.06471v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06471","description":"<p>Being able to reply with a related, fluent, and informative response is an\nindispensable requirement for building high-quality conversational agents. In\norder to generate better responses, some approaches have been proposed, such as\nfeeding extra information by collecting large-scale datasets with human\nannotations, designing neural conversational models (NCMs) with complex\narchitecture and loss functions, or filtering out untrustworthy samples based\non a dialogue attribute, e.g., Relatedness or Genericness. In this paper, we\nfollow the third research branch and present a data filtering method for\nopen-domain dialogues, which identifies untrustworthy samples from training\ndata with a quality measure that linearly combines seven dialogue attributes.\nThe attribute weights are obtained via Bayesian Optimization (BayesOpt) that\naims to optimize an objective function for dialogue generation iteratively on\nthe validation set. Then we score training samples with the quality measure,\nsort them in descending order, and filter out those at the bottom. Furthermore,\nto accelerate the \"filter-train-evaluate\" iterations involved in BayesOpt on\nlarge-scale datasets, we propose a training framework that integrates maximum\nlikelihood estimation (MLE) and negative training method (NEG). The training\nmethod updates parameters of a trained NCMs on two small sets with newly\nmaintained and removed samples, respectively. Specifically, MLE is applied to\nmaximize the log-likelihood of newly maintained samples, while NEG is used to\nminimize the log-likelihood of newly removed ones. Experimental results on two\ndatasets show that our method can effectively identify untrustworthy samples,\nand NCMs trained on the filtered datasets achieve better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haolan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logic-level Evidence Retrieval and Graph-based Verification Network for Table-based Fact Verification. (arXiv:2109.06480v1 [cs.AI])","link":"http://arxiv.org/abs/2109.06480","description":"<p>Table-based fact verification task aims to verify whether the given statement\nis supported by the given semi-structured table. Symbolic reasoning with\nlogical operations plays a crucial role in this task. Existing methods leverage\nprograms that contain rich logical information to enhance the verification\nprocess. However, due to the lack of fully supervised signals in the program\ngeneration process, spurious programs can be derived and employed, which leads\nto the inability of the model to catch helpful logical operations. To address\nthe aforementioned problems, in this work, we formulate the table-based fact\nverification task as an evidence retrieval and reasoning framework, proposing\nthe Logic-level Evidence Retrieval and Graph-based Verification network\n(LERGV). Specifically, we first retrieve logic-level program-like evidence from\nthe given table and statement as supplementary evidence for the table. After\nthat, we construct a logic-level graph to capture the logical relations between\nentities and functions in the retrieved evidence, and design a graph-based\nverification network to perform logic-level graph-based reasoning based on the\nconstructed graph to classify the final entailment relation. Experimental\nresults on the large-scale benchmark TABFACT show the effectiveness of the\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate. (arXiv:2109.06481v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06481","description":"<p>Non-autoregressive neural machine translation (NART) models suffer from the\nmulti-modality problem which causes translation inconsistency such as token\nrepetition. Most recent approaches have attempted to solve this problem by\nimplicitly modeling dependencies between outputs. In this paper, we introduce\nAligNART, which leverages full alignment information to explicitly reduce the\nmodality of the target distribution. AligNART divides the machine translation\ntask into $(i)$ alignment estimation and $(ii)$ translation with aligned\ndecoder inputs, guiding the decoder to focus on simplified one-to-one\ntranslation. To alleviate the alignment estimation problem, we further propose\na novel alignment decomposition method. Our experiments show that AligNART\noutperforms previous non-iterative NART models that focus on explicit modality\nreduction on WMT14 En$\\leftrightarrow$De and WMT16 Ro$\\rightarrow$En.\nFurthermore, AligNART achieves BLEU scores comparable to those of the\nstate-of-the-art connectionist temporal classification based models on WMT14\nEn$\\leftrightarrow$De. We also observe that AligNART effectively addresses the\ntoken repetition problem even without sequence-level knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jongyoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilevel profiling of situation and dialogue-based deep networks for movie genre classification using movie trailers. (arXiv:2109.06488v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06488","description":"<p>Automated movie genre classification has emerged as an active and essential\narea of research and exploration. Short duration movie trailers provide useful\ninsights about the movie as video content consists of the cognitive and the\naffective level features. Previous approaches were focused upon either\ncognitive or affective content analysis. In this paper, we propose a novel\nmulti-modality: situation, dialogue, and metadata-based movie genre\nclassification framework that takes both cognition and affect-based features\ninto consideration. A pre-features fusion-based framework that takes into\naccount: situation-based features from a regular snapshot of a trailer that\nincludes nouns and verbs providing the useful affect-based mapping with the\ncorresponding genres, dialogue (speech) based feature from audio, metadata\nwhich together provides the relevant information for cognitive and affect based\nvideo analysis. We also develop the English movie trailer dataset (EMTD), which\ncontains 2000 Hollywood movie trailers belonging to five popular genres:\nAction, Romance, Comedy, Horror, and Science Fiction, and perform\ncross-validation on the standard LMTD-9 dataset for validating the proposed\nframework. The results demonstrate that the proposed methodology for movie\ngenre classification has performed excellently as depicted by the F1 scores,\nprecision, recall, and area under the precision-recall curves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_D/0/1/0/all/0/1\">Dinesh Kumar Vishwakarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_M/0/1/0/all/0/1\">Mayank Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ayush Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aditya Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"conSultantBERT: Fine-tuned Siamese Sentence-BERT for Matching Jobs and Job Seekers. (arXiv:2109.06501v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06501","description":"<p>In this paper we focus on constructing useful embeddings of textual\ninformation in vacancies and resumes, which we aim to incorporate as features\ninto job to job seeker matching models alongside other features. We explain our\ntask where noisy data from parsed resumes, heterogeneous nature of the\ndifferent sources of data, and crosslinguality and multilinguality present\ndomain-specific challenges.\n</p>\n<p>We address these challenges by fine-tuning a Siamese Sentence-BERT (SBERT)\nmodel, which we call conSultantBERT, using a large-scale, real-world, and high\nquality dataset of over 270,000 resume-vacancy pairs labeled by our staffing\nconsultants. We show how our fine-tuned model significantly outperforms\nunsupervised and supervised baselines that rely on TF-IDF-weighted feature\nvectors and BERT embeddings. In addition, we find our model successfully\nmatches cross-lingual and multilingual textual content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lavi_D/0/1/0/all/0/1\">Dor Lavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medentsiy_V/0/1/0/all/0/1\">Volodymyr Medentsiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graus_D/0/1/0/all/0/1\">David Graus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tribrid: Stance Classification with Neural Inconsistency Detection. (arXiv:2109.06508v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06508","description":"<p>We study the problem of performing automatic stance classification on social\nmedia with neural architectures such as BERT. Although these architectures\ndeliver impressive results, their level is not yet comparable to the one of\nhumans and they might produce errors that have a significant impact on the\ndownstream task (e.g., fact-checking). To improve the performance, we present a\nnew neural architecture where the input also includes automatically generated\nnegated perspectives over a given claim. The model is jointly learned to make\nsimultaneously multiple predictions, which can be used either to improve the\nclassification of the original perspective or to filter out doubtful\npredictions. In the first case, we propose a weakly supervised method for\ncombining the predictions into a final one. In the second case, we show that\nusing the confidence scores to remove doubtful predictions allows our method to\nachieve human-like performance over the retained information, which is still a\nsizable part of the original input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Song Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbani_J/0/1/0/all/0/1\">Jacopo Urbani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation. (arXiv:2109.06513v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06513","description":"<p>Dialog grounding enables conversational models to make full use of external\ninformation to establish multiple desired qualities, such as knowledgeable,\nengaging and empathetic. However, naturally grounded dialog corpora are usually\nnot directly available, which puts forward requirements for the few-shot\nlearning ability of conversational models. Motivated by recent advances in\npre-trained language models and prompt-based learning, in this paper we explore\nprompt-based few-shot learning for grounded dialog generation (GDG). We first\nformulate the prompt construction for GDG tasks, based on which we then conduct\ncomprehensive empirical analysis on two common types of prompting methods:\ntemplate-based prompting and soft-prompting. We demonstrate the potential of\nprompt-based methods in few-shot learning for GDG and provide directions of\nimprovement for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Netmarble AI Center's WMT21 Automatic Post-Editing Shared Task Submission. (arXiv:2109.06515v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06515","description":"<p>This paper describes Netmarble's submission to WMT21 Automatic Post-Editing\n(APE) Shared Task for the English-German language pair. First, we propose a\nCurriculum Training Strategy in training stages. Facebook Fair's WMT19 news\ntranslation model was chosen to engage the large and powerful pre-trained\nneural networks. Then, we post-train the translation model with different\nlevels of data at each training stages. As the training stages go on, we make\nthe system learn to solve multiple tasks by adding extra information at\ndifferent training stages gradually. We also show a way to utilize the\nadditional data in large volume for APE tasks. For further improvement, we\napply Multi-Task Learning Strategy with the Dynamic Weight Average during the\nfine-tuning stage. To fine-tune the APE corpus with limited data, we add some\nrelated subtasks to learn a unified representation. Finally, for better\nperformance, we leverage external translations as augmented machine translation\n(MT) during the post-training and fine-tuning. As experimental results show,\nour APE system significantly improves the translations of provided MT results\nby -2.848 and +3.74 on the development dataset in terms of TER and BLEU,\nrespectively. It also demonstrates its effectiveness on the test dataset with\nhigher quality than the development dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Shinhyeok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Sion Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shounan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_I/0/1/0/all/0/1\">Insoo Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sampling of Dependency Structures. (arXiv:2109.06521v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06521","description":"<p>Probabilistic distributions over spanning trees in directed graphs are a\nfundamental model of dependency structure in natural language processing,\nsyntactic dependency trees. In NLP, dependency trees often have an additional\nroot constraint: only one edge may emanate from the root. However, no sampling\nalgorithm has been presented in the literature to account for this additional\nconstraint. In this paper, we adapt two spanning tree sampling algorithms to\nfaithfully sample dependency trees from a graph subject to the root constraint.\nWilson (1996)'s sampling algorithm has a running time of $\\mathcal{O}(H)$ where\n$H$ is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm\nhas a running time of $\\mathcal{O}(N^3)$, which is often greater than the mean\nhitting time of a directed graph. Additionally, we build upon Colbourn's\nalgorithm and present a novel extension that can sample $K$ trees without\nreplacement in $\\mathcal{O}(K N^3 + K^2 N)$ time. To the best of our knowledge,\nno algorithm has been given for sampling spanning trees without replacement\nfrom a directed graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zmigrod_R/0/1/0/all/0/1\">Ran Zmigrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks. (arXiv:2109.06524v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06524","description":"<p>Loading models pre-trained on the large-scale corpus in the general domain\nand fine-tuning them on specific downstream tasks is gradually becoming a\nparadigm in Natural Language Processing. Previous investigations prove that\nintroducing a further pre-training phase between pre-training and fine-tuning\nphases to adapt the model on the domain-specific unlabeled data can bring\npositive effects. However, most of these further pre-training works just keep\nrunning the conventional pre-training task, e.g., masked language model, which\ncan be regarded as the domain adaptation to bridge the data distribution gap.\nAfter observing diverse downstream tasks, we suggest that different tasks may\nalso need a further pre-training phase with appropriate training tasks to\nbridge the task formulation gap. To investigate this, we carry out a study for\nimproving multiple task-oriented dialogue downstream tasks through designing\nvarious tasks at the further pre-training phase. The experiment shows that\ndifferent downstream tasks prefer different further pre-training tasks, which\nhave intrinsic correlation and most further pre-training tasks significantly\nimprove certain target tasks rather than all. Our investigation indicates that\nit is of great importance and effectiveness to design appropriate further\npre-training tasks modeling specific information that benefit downstream tasks.\nBesides, we present multiple constructive empirical conclusions for enhancing\ntask-oriented dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Bill Similarity with Annotated and Augmented Corpora of Bills. (arXiv:2109.06527v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06527","description":"<p>Bill writing is a critical element of representative democracy. However, it\nis often overlooked that most legislative bills are derived, or even directly\ncopied, from other bills. Despite the significance of bill-to-bill linkages for\nunderstanding the legislative process, existing approaches fail to address\nsemantic similarities across bills, let alone reordering or paraphrasing which\nare prevalent in legal document writing. In this paper, we overcome these\nlimitations by proposing a 5-class classification task that closely reflects\nthe nature of the bill generation process. In doing so, we construct a\nhuman-labeled dataset of 4,721 bill-to-bill relationships at the\nsubsection-level and release this annotated dataset to the research community.\nTo augment the dataset, we generate synthetic data with varying degrees of\nsimilarity, mimicking the complex bill writing process. We use BERT variants\nand apply multi-stage training, sequentially fine-tuning our models with\nsynthetic and human-labeled datasets. We find that the predictive performance\nsignificantly improves when training with both human-labeled and synthetic\ndata. Finally, we apply our trained model to infer section- and bill-level\nsimilarities. Our analysis shows that the proposed methodology successfully\ncaptures the similarities across legal documents at various levels of\naggregation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griggs_E/0/1/0/all/0/1\">Elden Griggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">In Song Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder. (arXiv:2109.06536v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06536","description":"<p>Recent work has proposed several efficient approaches for generating\ngradient-based adversarial perturbations on embeddings and proved that the\nmodel's performance and robustness can be improved when they are trained with\nthese contaminated embeddings. While they paid little attention to how to help\nthe model to learn these adversarial samples more efficiently. In this work, we\nfocus on enhancing the model's ability to defend gradient-based adversarial\nattack during the model's training process and propose two novel adversarial\ntraining approaches: (1) CARL narrows the original sample and its adversarial\nsample in the representation space while enlarging their distance from\ndifferent labeled samples. (2) RAR forces the model to reconstruct the original\nsample from its adversarial representation. Experiments show that the proposed\ntwo approaches outperform strong baselines on various text classification\ndatasets. Analysis experiments find that when using our approaches, the\nsemantic representation of the input sentence won't be significantly affected\nby adversarial perturbations, and the model's performance drops less under\nadversarial attack. That is to say, our approaches can effectively improve the\nrobustness of the model. Besides, RAR can also be used to generate text-form\nadversarial samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenging Instances are Worth Learning: Generating Valuable Negative Samples for Response Selection Training. (arXiv:2109.06538v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06538","description":"<p>Retrieval-based chatbot selects the appropriate response from candidates\naccording to the context, which heavily depends on a response selection module.\nA response selection module is generally a scoring model to evaluate candidates\nand is usually trained on the annotated positive response and sampled negative\nresponses. Sampling negative responses lead to two risks: a). The sampled\nnegative instances, especially that from random sampling methods, are mostly\nirrelevant to the dialogue context and too easy to be fitted at the training\nstage while causing a weak model in the real scenario. b). The so-called\nnegative instances may be positive, which is known as the fake negative\nproblem. To address the above issue, we employ pre-trained language models,\nsuch as the DialoGPT to construct more challenging negative instances to\nenhance the model robustness. Specifically, we provide garbled context to the\npre-trained model to generate responses and filter the fake negative ones. In\nthis way, our negative instances are fluent, context-related, and more\nchallenging for the model to learn, while can not be positive. Extensive\nexperiments show that our method brings significant and stable improvements on\nthe dialogue response selection capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Huiying Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking Space: inference from spatial linguistic meanings. (arXiv:2109.06554v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06554","description":"<p>This paper concerns the intersection of natural language and the physical\nspace around us in which we live, that we observe and/or imagine things within.\nMany important features of language have spatial connotations, for example,\nmany prepositions (like in, next to, after, on, etc.) are fundamentally\nspatial. Space is also a key factor of the meanings of many\nwords/phrases/sentences/text, and space is a, if not the key, context for\nreferencing (e.g. pointing) and embodiment.\n</p>\n<p>We propose a mechanism for how space and linguistic structure can be made to\ninteract in a matching compositional fashion. Examples include Cartesian space,\nsubway stations, chesspieces on a chess-board, and Penrose's staircase. The\nstarting point for our construction is the DisCoCat model of compositional\nnatural language meaning, which we relax to accommodate physical space. We\naddress the issue of having multiple agents/objects in a space, including the\ncase that each agent has different capabilities with respect to that space,\ne.g., the specific moves each chesspiece can make, or the different velocities\none may be able to reach.\n</p>\n<p>Once our model is in place, we show how inferences drawing from the structure\nof physical space can be made. We also how how linguistic model of space can\ninteract with other such models related to our senses and/or embodiment, such\nas the conceptual spaces of colour, taste and smell, resulting in a rich\ncompositional model of meaning that is close to human experience and embodiment\nin the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Mascianica_V/0/1/0/all/0/1\">Vincent Wang-Mascianica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP. (arXiv:2109.06598v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06598","description":"<p>A key part of the NLP ethics movement is responsible use of data, but exactly\nwhat that means or how it can be best achieved remain unclear. This position\npaper discusses the core legal and ethical principles for collection and\nsharing of textual data, and the tensions between them. We propose a potential\nchecklist for responsible data (re-)use that could both standardise the peer\nreview of conference submissions, as well as enable a more in-depth view of\npublished research across the community. Our proposal aims to contribute to the\ndevelopment of a consistent standard for data (re-)use, embraced across NLP\nconferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Tim Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leins_K/0/1/0/all/0/1\">Kobi Leins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation. (arXiv:2109.06604v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06604","description":"<p>Recently, $k$NN-MT has shown the promising capability of directly\nincorporating the pre-trained neural machine translation (NMT) model with\ndomain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve\ndomain adaptation without retraining. Despite being conceptually attractive, it\nheavily relies on high-quality in-domain parallel corpora, limiting its\ncapability on unsupervised domain adaptation, where in-domain parallel corpora\nare scarce or nonexistent. In this paper, we propose a novel framework that\ndirectly uses in-domain monolingual sentences in the target language to\nconstruct an effective datastore for $k$-nearest-neighbor retrieval. To this\nend, we first introduce an autoencoder task based on the target language, and\nthen insert lightweight adapters into the original NMT model to map the\ntoken-level representation of this task to the ideal representation of\ntranslation task. Experiments on multi-domain datasets demonstrate that our\nproposed approach significantly improves the translation accuracy with\ntarget-side monolingual data, while achieving comparable performance with\nback-translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model. (arXiv:2109.06605v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06605","description":"<p>Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a\nlanguage model on domain-specific text, improves the modelling of text for\ndownstream tasks within the domain. Numerous real-world applications are based\non domain-specific text, e.g. working with financial or biomedical documents,\nand these applications often need to support multiple languages. However,\nlarge-scale domain-specific multilingual pretraining data for such scenarios\ncan be difficult to obtain, due to regulations, legislation, or simply a lack\nof language- and domain-specific text. One solution is to train a single\nmultilingual model, taking advantage of the data available in as many languages\nas possible. In this work, we explore the benefits of domain adaptive\npretraining with a focus on adapting to multiple languages within a specific\ndomain. We propose different techniques to compose pretraining corpora that\nenable a language model to both become domain-specific and multilingual.\nEvaluation on nine domain-specific datasets-for biomedical named entity\nrecognition and financial sentence classification-covering seven different\nlanguages show that a single multilingual domain-specific model can outperform\nthe general multilingual model, and performs close to its monolingual\ncounterpart. This finding holds across two different pretraining methods,\nadapter-based pretraining and full model pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jorgensen_R/0/1/0/all/0/1\">Rasmus K&#xe6;r J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_M/0/1/0/all/0/1\">Mareike Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Font Reconstruction with Dual Latent Manifolds. (arXiv:2109.06627v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06627","description":"<p>We propose a deep generative model that performs typography analysis and font\nreconstruction by learning disentangled manifolds of both font style and\ncharacter shape. Our approach enables us to massively scale up the number of\ncharacter types we can effectively model compared to previous methods.\nSpecifically, we infer separate latent variables representing character and\nfont via a pair of inference networks which take as input sets of glyphs that\neither all share a character type, or belong to the same font. This design\nallows our model to generalize to characters that were not observed during\ntraining time, an important task in light of the relative sparsity of most\nfonts. We also put forward a new loss, adapted from prior work that measures\nlikelihood using an adaptive distribution in a projected space, resulting in\nmore natural images without requiring a discriminator. We evaluate on the task\nof font reconstruction over various datasets representing character types of\nmany languages, and compare favorably to modern style transfer systems\naccording to both automatic and manually-evaluated metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivatsan_N/0/1/0/all/0/1\">Nikita Srivatsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Si Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An MRC Framework for Semantic Role Labeling. (arXiv:2109.06660v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06660","description":"<p>Semantic Role Labeling (SRL) aims at recognizing the predicate-argument\nstructure of a sentence and can be decomposed into two subtasks: predicate\ndisambiguation and argument labeling. Prior work deals with these two tasks\nindependently, which ignores the semantic connection between the two tasks. In\nthis paper, we propose to use the machine reading comprehension (MRC) framework\nto bridge this gap. We formalize predicate disambiguation as multiple-choice\nmachine reading comprehension, where the descriptions of candidate senses of a\ngiven predicate are used as options to select the correct sense. The chosen\npredicate sense is then used to determine the semantic roles for that\npredicate, and these semantic roles are used to construct the query for another\nMRC model for argument labeling. In this way, we are able to leverage both the\npredicate semantics and the semantic role semantics for argument labeling. We\nalso propose to select a subset of all the possible semantic roles for\ncomputational efficiency. Experiments show that the proposed framework achieves\nstate-of-the-art results on both span and dependency benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expert Knowledge-Guided Length-Variant Hierarchical Label Generation for Proposal Classification. (arXiv:2109.06661v1 [cs.LG])","link":"http://arxiv.org/abs/2109.06661","description":"<p>To advance the development of science and technology, research proposals are\nsubmitted to open-court competitive programs developed by government agencies\n(e.g., NSF). Proposal classification is one of the most important tasks to\nachieve effective and fair review assignments. Proposal classification aims to\nclassify a proposal into a length-variant sequence of labels. In this paper, we\nformulate the proposal classification problem into a hierarchical multi-label\nclassification task. Although there are certain prior studies, proposal\nclassification exhibit unique features: 1) the classification result of a\nproposal is in a hierarchical discipline structure with different levels of\ngranularity; 2) proposals contain multiple types of documents; 3) domain\nexperts can empirically provide partial labels that can be leveraged to improve\ntask performances. In this paper, we focus on developing a new deep proposal\nclassification framework to jointly model the three features. In particular, to\nsequentially generate labels, we leverage previously-generated labels to\npredict the label of next level; to integrate partial labels from experts, we\nuse the embedding of these empirical partial labels to initialize the state of\nneural networks. Our model can automatically identify the best length of label\nsequence to stop next label prediction. Finally, we present extensive results\nto demonstrate that our method can jointly model partial labels, textual\ninformation, and semantic dependencies in label sequences, and, thus, achieve\nadvanced performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Ziyue Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Inference for Multilingual Neural Machine Translation. (arXiv:2109.06679v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06679","description":"<p>Multilingual NMT has become an attractive solution for MT deployment in\nproduction. But to match bilingual quality, it comes at the cost of larger and\nslower models. In this work, we consider several ways to make multilingual NMT\nfaster at inference without degrading its quality. We experiment with several\n\"light decoder\" architectures in two 20-language multi-parallel settings:\nsmall-scale on TED Talks and large-scale on ParaCrawl. Our experiments\ndemonstrate that combining a shallow decoder with vocabulary filtering leads to\nmore than twice faster inference with no loss in translation quality. We\nvalidate our findings with BLEU and chrF (on 380 language pairs), robustness\nevaluation and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dain Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kweonwoo Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Transformer with Unified Bidirectional Decoder for Automatic Speech Recognition. (arXiv:2109.06684v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06684","description":"<p>Non-autoregressive (NAR) transformer models have been studied intensively in\nautomatic speech recognition (ASR), and a substantial part of NAR transformer\nmodels is to use the casual mask to limit token dependencies. However, the\ncasual mask is designed for the left-to-right decoding process of the\nnon-parallel autoregressive (AR) transformer, which is inappropriate for the\nparallel NAR transformer since it ignores the right-to-left contexts. Some\nmodels are proposed to utilize right-to-left contexts with an extra decoder,\nbut these methods increase the model complexity. To tackle the above problems,\nwe propose a new non-autoregressive transformer with a unified bidirectional\ndecoder (NAT-UBD), which can simultaneously utilize left-to-right and\nright-to-left contexts. However, direct use of bidirectional contexts will\ncause information leakage, which means the decoder output can be affected by\nthe character information from the input of the same position. To avoid\ninformation leakage, we propose a novel attention mask and modify vanilla\nqueries, keys, and values matrices for NAT-UBD. Experimental results verify\nthat NAT-UBD can achieve character error rates (CERs) of 5.0%/5.5% on the\nAishell1 dev/test sets, outperforming all previous NAR transformer models.\nMoreover, NAT-UBD can run 49.8x faster than the AR transformer baseline when\ndecoding in a single step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuan-Fei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian-Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Song-Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A system for information extraction from scientific texts in Russian. (arXiv:2109.06703v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06703","description":"<p>In this paper, we present a system for information extraction from scientific\ntexts in the Russian language. The system performs several tasks in an\nend-to-end manner: term recognition, extraction of relations between terms, and\nterm linking with entities from the knowledge base. These tasks are extremely\nimportant for information retrieval, recommendation systems, and\nclassification. The advantage of the implemented methods is that the system\ndoes not require a large amount of labeled data, which saves time and effort\nfor data labeling and therefore can be applied in low- and mid-resource\nsettings. The source code is publicly available and can be used for different\nresearch purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruches_E/0/1/0/all/0/1\">Elena Bruches</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezentseva_A/0/1/0/all/0/1\">Anastasia Mezentseva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batura_T/0/1/0/all/0/1\">Tatiana Batura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KFCNet: Knowledge Filtering and Contrastive Learning Network for Generative Commonsense Reasoning. (arXiv:2109.06704v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06704","description":"<p>Pre-trained language models have led to substantial gains over a broad range\nof natural language processing (NLP) tasks, but have been shown to have\nlimitations for natural language generation tasks with high-quality\nrequirements on the output, such as commonsense generation and ad keyword\ngeneration. In this work, we present a novel Knowledge Filtering and\nContrastive learning Network (KFCNet) which references external knowledge and\nachieves better generation performance. Specifically, we propose a BERT-based\nfilter model to remove low-quality candidates, and apply contrastive learning\nseparately to each of the encoder and decoder, within a general\nencoder--decoder architecture. The encoder contrastive module helps to capture\nglobal target semantics during encoding, and the decoder contrastive module\nenhances the utility of retrieved prototypes while learning general features.\nExtensive experiments on the CommonGen benchmark show that our model\noutperforms the previous state of the art by a large margin: +6.6 points (42.5\nvs. 35.9) for BLEU-4, +3.7 points (33.3 vs. 29.6) for SPICE, and +1.3 points\n(18.3 vs. 17.0) for CIDEr. We further verify the effectiveness of the proposed\ncontrastive module on ad keyword generation, and show that our model has\npotential commercial value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (arXiv:2109.06705v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06705","description":"<p>Table filling based relational triple extraction methods are attracting\ngrowing research interests due to their promising performance and their\nabilities on extracting triples from complex sentences. However, this kind of\nmethods are far from their full potential because most of them only focus on\nusing local features but ignore the global associations of relations and of\ntoken pairs, which increases the possibility of overlooking some important\ninformation during triple extraction. To overcome this deficiency, we propose a\nglobal feature-oriented triple extraction model that makes full use of the\nmentioned two kinds of global associations. Specifically, we first generate a\ntable feature for each relation. Then two kinds of global associations are\nmined from the generated table features. Next, the mined global associations\nare integrated into the table feature of each relation. This\n\"generate-mine-integrate\" process is performed multiple times so that the table\nfeature of each relation is refined step by step. Finally, each relation's\ntable is filled based on its refined table feature, and all triples linked to\nthis relation are extracted based on its filled table. We evaluate the proposed\nmodel on three benchmark datasets. Experimental results show our model is\neffective and it achieves state-of-the-art results on all of these datasets.\nThe source code of our work is available at: https://github.com/neukg/GRTE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shujuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaduo Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Answer Type Prediction using BERT: IAI at the ISWC SMART Task 2020. (arXiv:2109.06714v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06714","description":"<p>This paper summarizes our participation in the SMART Task of the ISWC 2020\nChallenge. A particular question we are interested in answering is how well\nneural methods, and specifically transformer models, such as BERT, perform on\nthe answer type prediction task compared to traditional approaches. Our main\nfinding is that coarse-grained answer types can be identified effectively with\nstandard text classification methods, with over 95% accuracy, and BERT can\nbring only marginal improvements. For fine-grained type detection, on the other\nhand, BERT clearly outperforms previous retrieval-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Setty_V/0/1/0/all/0/1\">Vinay Setty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balog_K/0/1/0/all/0/1\">Krisztian Balog</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Dialogue Generation with Disentangled Multi-grained Style Specification and Attribute Consistency Reward. (arXiv:2109.06717v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06717","description":"<p>Controllable text generation is an appealing but challenging task, which\nallows users to specify particular attributes of the generated outputs. In this\npaper, we propose a controllable dialogue generation model to steer response\ngeneration under multi-attribute constraints. Specifically, we define and\ncategorize the commonly used control attributes into global and local ones,\nwhich possess different granularities of effects on response generation. Then,\nwe significantly extend the conventional seq2seq framework by introducing a\nnovel two-stage decoder, which first uses a multi-grained style specification\nlayer to impose the stylistic constraints and determine word-level control\nstates of responses based on the attributes, and then employs a response\ngeneration layer to generate final responses maintaining both semantic\nrelevancy to the contexts and fidelity to the attributes. Furthermore, we train\nour model with an attribute consistency reward to promote response control with\nexplicit supervision signals. Extensive experiments and in-depth analyses on\ntwo datasets indicate that our model can significantly outperform competitive\nbaselines in terms of response quality, content diversity and controllability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiwei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fuzzy Attention for Structural Sentiment Analysis. (arXiv:2109.06719v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06719","description":"<p>Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructural sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structural sentiment analysis. We further explore the\nparsing modeling on structural sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letain Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Modelling with Applications to Music Recommendation, Fact-Checking, and Speed Reading. (arXiv:2109.06736v1 [cs.IR])","link":"http://arxiv.org/abs/2109.06736","description":"<p>Sequential modelling entails making sense of sequential data, which naturally\noccurs in a wide array of domains. One example is systems that interact with\nusers, log user actions and behaviour, and make recommendations of items of\npotential interest to users on the basis of their previous interactions. In\nsuch cases, the sequential order of user interactions is often indicative of\nwhat the user is interested in next. Similarly, for systems that automatically\ninfer the semantics of text, capturing the sequential order of words in a\nsentence is essential, as even a slight re-ordering could significantly alter\nits original meaning. This thesis makes methodological contributions and new\ninvestigations of sequential modelling for the specific application areas of\nsystems that recommend music tracks to listeners and systems that process text\nsemantics in order to automatically fact-check claims, or \"speed read\" text for\nefficient further classification. (Rest of abstract omitted due to arXiv\nabstract limit)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Christian Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Information Seeking for Open-Domain Question Answering. (arXiv:2109.06747v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06747","description":"<p>Information seeking is an essential step for open-domain question answering\nto efficiently gather evidence from a large corpus. Recently, iterative\napproaches have been proven to be effective for complex questions, by\nrecursively retrieving new evidence at each step. However, almost all existing\niterative approaches use predefined strategies, either applying the same\nretrieval function multiple times or fixing the order of different retrieval\nfunctions, which cannot fulfill the diverse requirements of various questions.\nIn this paper, we propose a novel adaptive information-seeking strategy for\nopen-domain question answering, namely AISO. Specifically, the whole retrieval\nand answer process is modeled as a partially observed Markov decision process,\nwhere three types of retrieval operations (e.g., BM25, DPR, and hyperlink) and\none answer operation are defined as actions. According to the learned policy,\nAISO could adaptively select a proper retrieval action to seek the missing\nevidence at each step, based on the collected evidence and the reformulated\nquery, or directly output the answer when the evidence set is sufficient for\nthe question. Experiments on SQuAD Open and HotpotQA fullwiki, which serve as\nsingle-hop and multi-hop open-domain QA benchmarks, show that AISO outperforms\nall baseline methods with predefined strategies in terms of both retrieval and\nanswer evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunchang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-shot Cross-lingual Transfer between Closely Related Languages by injecting Character-level Noise. (arXiv:2109.06772v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06772","description":"<p>Cross-lingual transfer between a high-resource language and its dialects or\nclosely related language varieties should be facilitated by their similarity,\nbut current approaches that operate in the embedding space do not take surface\nsimilarity into account. In this work, we present a simple yet effective\nstrategy to improve cross-lingual transfer between closely related varieties by\naugmenting the data of the high-resource parent language with character-level\nnoise to make the model more robust towards spelling variations. Our strategy\nshows consistent improvements over several languages and tasks: Zero-shot\ntransfer of POS tagging and topic identification between language varieties\nfrom the Germanic, Uralic, and Romance language genera. Our work provides\nevidence for the usefulness of simple surface-level noise in improving transfer\nbetween language varieties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aepli_N/0/1/0/all/0/1\">No&#xeb;mi Aepli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction. (arXiv:2109.06798v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06798","description":"<p>Zero-shot cross-lingual information extraction (IE) describes the\nconstruction of an IE model for some target language, given existing\nannotations exclusively in some other language, typically English. While the\nadvance of pretrained multilingual encoders suggests an easy optimism of \"train\non English, run on any language\", we find through a thorough exploration and\nextension of techniques that a combination of approaches, both new and old,\nleads to better performance than any one cross-lingual strategy in particular.\nWe explore techniques including data projection and self-training, and how\ndifferent pretrained encoders impact them. We use English-to-Arabic IE as our\ninitial example, demonstrating strong performance in this setting for event\nextraction, named entity recognition, part-of-speech tagging, and dependency\nparsing. We then apply data projection and self-training to three tasks across\neight target languages. Because no single set of techniques performs the best\nacross all tasks, we encourage practitioners to explore various configurations\nof the techniques described in this work when seeking to improve on zero-shot\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yarmohammadi_M/0/1/0/all/0/1\">Mahsa Yarmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marone_M/0/1/0/all/0/1\">Marc Marone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebner_S/0/1/0/all/0/1\">Seth Ebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1\">Guanghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jialiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harman_C/0/1/0/all/0/1\">Craig Harman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Temporal Variational Model for Story Generation. (arXiv:2109.06807v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06807","description":"<p>Recent language models can generate interesting and grammatically correct\ntext in story generation but often lack plot development and long-term\ncoherence. This paper experiments with a latent vector planning approach based\non a TD-VAE (Temporal Difference Variational Autoencoder), using the model for\nconditioning and reranking for text generation. The results demonstrate strong\nperformance in automatic cloze and swapping evaluations. The human judgments\nshow stories generated with TD-VAE reranking improve on a GPT-2 medium baseline\nand show comparable performance to a hierarchical LSTM reranking model.\nConditioning on the latent vectors proves disappointing and deteriorates\nperformance in human evaluation because it reduces the diversity of generation,\nand the models don't learn to progress the narrative. This highlights an\nimportant difference between technical task performance (e.g. cloze) and\ngenerating interesting stories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilmot_D/0/1/0/all/0/1\">David Wilmot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What are the attackers doing now? Automating cyber threat intelligence extraction from text on pace with the changing threat landscape: A survey. (arXiv:2109.06808v1 [cs.CR])","link":"http://arxiv.org/abs/2109.06808","description":"<p>Cybersecurity researchers have contributed to the automated extraction of CTI\nfrom textual sources, such as threat reports and online articles, where\ncyberattack strategies, procedures, and tools are described. The goal of this\narticle is to aid cybersecurity researchers understand the current techniques\nused for cyberthreat intelligence extraction from text through a survey of\nrelevant studies in the literature. We systematically collect \"CTI extraction\nfrom text\"-related studies from the literature and categorize the CTI\nextraction purposes. We propose a CTI extraction pipeline abstracted from these\nstudies. We identify the data sources, techniques, and CTI sharing formats\nutilized in the context of the proposed pipeline. Our work finds ten types of\nextraction purposes, such as extraction indicators of compromise extraction,\nTTPs (tactics, techniques, procedures of attack), and cybersecurity keywords.\nWe also identify seven types of textual sources for CTI extraction, and textual\ndata obtained from hacker forums, threat reports, social media posts, and\nonline news articles have been used by almost 90% of the studies. Natural\nlanguage processing along with both supervised and unsupervised machine\nlearning techniques such as named entity recognition, topic modelling,\ndependency parsing, supervised classification, and clustering are used for CTI\nextraction. We observe the technical challenges associated with these studies\nrelated to obtaining available clean, labelled data which could assure\nreplication, validation, and further extension of the studies. As we find the\nstudies focusing on CTI information extraction from text, we advocate for\nbuilding upon the current CTI extraction work to help cybersecurity\npractitioners with proactive decision making such as threat prioritization,\nautomated threat modelling to utilize knowledge from past cybersecurity\nincidents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Rayhanur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Hezaveh_R/0/1/0/all/0/1\">Rezvan Mahdavi-Hezaveh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1\">Laurie Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Critic: Language Models for Unsupervised Grammatical Error Correction. (arXiv:2109.06822v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06822","description":"<p>Training a model for grammatical error correction (GEC) requires a set of\nlabeled ungrammatical / grammatical sentence pairs, but manually annotating\nsuch pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has\ndemonstrated strong results on learning to repair a broken program without any\nlabeled examples, but this relies on a perfect critic (e.g., a compiler) that\nreturns whether an example is valid or not, which does not exist for the GEC\ntask. In this work, we show how to leverage a pretrained language model (LM) in\ndefining an LM-Critic, which judges a sentence to be grammatical if the LM\nassigns it a higher probability than its local perturbations. We apply this\nLM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap\nrealistic ungrammatical / grammatical pairs for training a corrector. We\nevaluate our approach on GEC datasets across multiple domains (CoNLL-2014,\nBEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing\nmethods in both the unsupervised setting (+7.7 F0.5) and the supervised setting\n(+0.5 F0.5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Types of Out-of-Distribution Texts and How to Detect Them. (arXiv:2109.06827v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06827","description":"<p>Despite agreement on the importance of detecting out-of-distribution (OOD)\nexamples, there is little consensus on the formal definition of OOD examples\nand how to best detect them. We categorize these examples by whether they\nexhibit a background shift or a semantic shift, and find that the two major\napproaches to OOD detection, model calibration and density estimation (language\nmodeling for text), have distinct behavior on these types of OOD data. Across\n14 pairs of in-distribution and OOD English natural language understanding\ndatasets, we find that density estimation methods consistently beat calibration\nmethods in background shift settings, while performing worse in semantic shift\nsettings. In addition, we find that both methods generally fail to detect\nexamples from challenge data, highlighting a weak spot for current methods.\nSince no single method works well across all settings, our results call for an\nexplicit definition of OOD examples when evaluating different detection\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_U/0/1/0/all/0/1\">Udit Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">William Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation. (arXiv:2109.06835v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06835","description":"<p>Recent text generation research has increasingly focused on open-ended\ndomains such as story and poetry generation. Because models built for such\ntasks are difficult to evaluate automatically, most researchers in the space\njustify their modeling choices by collecting crowdsourced human judgments of\ntext quality (e.g., Likert scores of coherence or grammaticality) from Amazon\nMechanical Turk (AMT). In this paper, we first conduct a survey of 45\nopen-ended text generation papers and find that the vast majority of them fail\nto report crucial details about their AMT tasks, hindering reproducibility. We\nthen run a series of story evaluation experiments with both AMT workers and\nEnglish teachers and discover that even with strict qualification filters, AMT\nworkers (unlike teachers) fail to distinguish between model-generated text and\nhuman-generated references. We show that AMT worker judgments improve when they\nare shown model-generated output alongside human-generated references, which\nenables the workers to better calibrate their ratings. Finally, interviews with\nthe English teachers provide deeper insights into the challenges of the\nevaluation process, particularly when rating model-generated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpinska_M/0/1/0/all/0/1\">Marzena Karpinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akoury_N/0/1/0/all/0/1\">Nader Akoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding. (arXiv:2109.06838v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06838","description":"<p>While large language models have shown exciting progress on several NLP\nbenchmarks, evaluating their ability for complex analogical reasoning remains\nunder-explored. Here, we introduce a high-quality crowdsourced dataset of\nnarratives for employing proverbs in context as a benchmark for abstract\nlanguage understanding. The dataset provides fine-grained annotation of aligned\nspans between proverbs and narratives, and contains minimal lexical overlaps\nbetween narratives and proverbs, ensuring that models need to go beyond\nsurface-level reasoning to succeed. We explore three tasks: (1) proverb\nrecommendation and alignment prediction, (2) narrative generation for a given\nproverb and topic, and (3) identifying narratives with similar motifs. Our\nexperiments show that neural language models struggle in our tasks compared to\nhumans, and the tasks pose multiple learning challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BenchIE: Open Information Extraction Evaluation Based on Facts, Not Tokens. (arXiv:2109.06850v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06850","description":"<p>Intrinsic evaluations of OIE systems are carried out either manually -- with\nhuman evaluators judging the correctness of extractions -- or automatically, on\nstandardized benchmarks. The latter, while much more cost-effective, is less\nreliable, primarily because of the incompleteness of the existing OIE\nbenchmarks: the ground truth extractions do not include all acceptable variants\nof the same fact, leading to unreliable assessment of models' performance.\nMoreover, the existing OIE benchmarks are available for English only. In this\nwork, we introduce BenchIE: a benchmark and evaluation framework for\ncomprehensive evaluation of OIE systems for English, Chinese and German. In\ncontrast to existing OIE benchmarks, BenchIE takes into account informational\nequivalence of extractions: our gold standard consists of fact synsets,\nclusters in which we exhaustively list all surface forms of the same fact. We\nbenchmark several state-of-the-art OIE systems using BenchIE and demonstrate\nthat these systems are significantly less effective than indicated by existing\nOIE benchmarks. We make BenchIE (data and evaluation code) publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gashteovski_K/0/1/0/all/0/1\">Kiril Gashteovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingying Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glavas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension. (arXiv:2109.06853v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06853","description":"<p>How can we generate concise explanations for multi-hop Reading Comprehension\n(RC)? The current strategies of identifying supporting sentences can be seen as\nan extractive question-focused summarization of the input text. However, these\nextractive explanations are not necessarily concise i.e. not minimally\nsufficient for answering a question. Instead, we advocate for an abstractive\napproach, where we propose to generate a question-focused, abstractive summary\nof input paragraphs and then feed it to an RC system. Given a limited amount of\nhuman-annotated abstractive explanations, we train the abstractive explainer in\na semi-supervised manner, where we start from the supervised model and then\ntrain it further through trial and error maximizing a conciseness-promoted\nreward function. Our experiments demonstrate that the proposed abstractive\nexplainer can generate more compact explanations than an extractive explainer\nwith limited supervision (only 2k instances) while maintaining sufficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1\">Naoya Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Steven Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning. (arXiv:2109.06860v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06860","description":"<p>Commonsense is defined as the knowledge that is shared by everyone. However,\ncertain types of commonsense knowledge are correlated with culture and\ngeographic locations and they are only shared locally. For example, the\nscenarios of wedding ceremonies vary across regions due to different customs\ninfluenced by historical and religious factors. Such regional characteristics,\nhowever, are generally omitted in prior work. In this paper, we construct a\nGeo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test\nvision-and-language models' ability to understand cultural and\ngeo-location-specific commonsense. In particular, we study two state-of-the-art\nVision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard\nmultimodal commonsense benchmark with images primarily from Western regions. We\nthen evaluate how well the trained models can generalize to answering the\nquestions in GD-VCR. We find that the performance of both models for\nnon-Western regions including East Asia, South Asia, and Africa is\nsignificantly lower than that for Western region. We analyze the reasons behind\nthe performance disparity and find that the performance gap is larger on QA\npairs that: 1) are concerned with culture-related scenarios, e.g., weddings,\nreligious activities, and festivals; 2) require high-level geo-diverse\ncommonsense reasoning rather than low-order perception and recognition. Dataset\nand code are released at https://github.com/WadeYin9712/GD-VCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legal Transformer Models May Not Always Help. (arXiv:2109.06862v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06862","description":"<p>Deep learning-based Natural Language Processing methods, especially\ntransformers, have achieved impressive performance in the last few years.\nApplying those state-of-the-art NLP methods to legal activities to automate or\nsimplify some simple work is of great value. This work investigates the value\nof domain adaptive pre-training and language adapters in legal NLP tasks. By\ncomparing the performance of language models with domain adaptive pre-training\non different tasks and different dataset splits, we show that domain adaptive\npre-training is only helpful with low-resource downstream tasks, thus far from\nbeing a panacea. We also benchmark the performance of adapters in a typical\nlegal NLP task and show that they can yield similar performance to full model\ntuning with much smaller training costs. As an additional result, we release\nLegalRoBERTa, a RoBERTa model further pre-trained on legal corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Sakbo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">R&#xe9;mi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition. (arXiv:2109.06870v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06870","description":"<p>This paper is a study of performance-efficiency trade-offs in pre-trained\nmodels for automatic speech recognition (ASR). We focus on wav2vec 2.0, and\nformalize several architecture designs that influence both the model\nperformance and its efficiency. Putting together all our observations, we\nintroduce SEW (Squeezed and Efficient Wav2vec), a pre-trained model\narchitecture with significant improvements along both performance and\nefficiency dimensions across a variety of training setups. For example, under\nthe 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x\ninference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in\nword error rate. With a similar inference time, SEW reduces word error rate by\n25-50% across different model sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jing Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Hyper-Parameter Optimization for Neural Machine Translation on GPU Architectures. (arXiv:1805.02094v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1805.02094","description":"<p>Neural machine translation (NMT) has been accelerated by deep learning neural\nnetworks over statistical-based approaches, due to the plethora and\nprogrammability of commodity heterogeneous computing architectures such as\nFPGAs and GPUs and the massive amount of training corpuses generated from news\noutlets, government agencies and social media. Training a learning classifier\nfor neural networks entails tuning hyper-parameters that would yield the best\nperformance. Unfortunately, the number of parameters for machine translation\ninclude discrete categories as well as continuous options, which makes for a\ncombinatorial explosive problem. This research explores optimizing\nhyper-parameters when training deep learning neural networks for machine\ntranslation. Specifically, our work investigates training a language model with\nMarian NMT. Results compare NMT under various hyper-parameter settings across a\nvariety of modern GPU architecture generations in single node and multi-node\nsettings, revealing insights on which hyper-parameters matter most in terms of\nperformance, such as words processed per second, convergence rates, and\ntranslation accuracy, and provides insights on how to best achieve\nhigh-performing NMT systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_R/0/1/0/all/0/1\">Robert Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_H/0/1/0/all/0/1\">Hieu Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briers_M/0/1/0/all/0/1\">Mark Briers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malony_A/0/1/0/all/0/1\">Allen Malony</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00033","description":"<p>With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic has been\ndeclared one of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Ad-dressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nTo address this gap, we release a large dataset of 16K manually annotated\ntweets for fine-grained disinformation analysis that (i) focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society, and (iii) covers Arabic,\nBulgarian, Dutch, and English. Finally, we show strong evaluation results using\npretrained Transformers, thus con-firming the practical utility of the dataset\nin monolingual vs. multilingual, and single task vs. multitask settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1\">Alex Nikolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Homaid_A/0/1/0/all/0/1\">Abdulaziz Al-Homaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaghouani_W/0/1/0/all/0/1\">Wajdi Zaghouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caselli_T/0/1/0/all/0/1\">Tommaso Caselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danoe_G/0/1/0/all/0/1\">Gijs Danoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolk_F/0/1/0/all/0/1\">Friso Stolk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruntink_B/0/1/0/all/0/1\">Britt Bruntink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L2R2: Leveraging Ranking for Abductive Reasoning. (arXiv:2005.11223v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2005.11223","description":"<p>The abductive natural language inference task ($\\alpha$NLI) is proposed to\nevaluate the abductive reasoning ability of a learning system. In the\n$\\alpha$NLI task, two observations are given and the most plausible hypothesis\nis asked to pick out from the candidates. Existing methods simply formulate it\nas a classification problem, thus a cross-entropy log-loss objective is used\nduring training. However, discriminating true from false does not measure the\nplausibility of a hypothesis, for all the hypotheses have a chance to happen,\nonly the probabilities are different. To fill this gap, we switch to a ranking\nperspective that sorts the hypotheses in order of their plausibilities. With\nthis new perspective, a novel $L2R^2$ approach is proposed under the\nlearning-to-rank framework. Firstly, training samples are reorganized into a\nranking form, where two observations and their hypotheses are treated as the\nquery and a set of candidate documents respectively. Then, an ESIM model or\npre-trained language model, e.g. BERT or RoBERTa, is obtained as the scoring\nfunction. Finally, the loss functions for the ranking task can be either\npair-wise or list-wise for training. The experimental results on the ART\ndataset reach the state-of-the-art in the public leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunchang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Graph for Neural Machine Translation. (arXiv:2012.03477v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.03477","description":"<p>Previous works have shown that contextual information can improve the\nperformance of neural machine translation (NMT). However, most existing\ndocument-level NMT methods only consider a few number of previous sentences.\nHow to make use of the whole document as global contexts is still a challenge.\nTo address this issue, we hypothesize that a document can be represented as a\ngraph that connects relevant contexts regardless of their distances. We employ\nseveral types of relations, including adjacency, syntactic dependency, lexical\nconsistency, and coreference, to construct the document graph. Then, we\nincorporate both source and target graphs into the conventional Transformer\narchitecture with graph convolutional networks. Experiments on various NMT\nbenchmarks, including IWSLT English--French, Chinese-English, WMT\nEnglish--German and Opensubtitle English--Russian, demonstrate that using\ndocument graphs can significantly improve the translation quality. Extensive\nanalysis verifies that the document graph is beneficial for capturing discourse\nphenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingzhou Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek. F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Mathematics in Artificial Intelligence. (arXiv:2101.04255v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2101.04255","description":"<p>In the decade since 2010, successes in artificial intelligence have been at\nthe forefront of computer science and technology, and vector space models have\nsolidified a position at the forefront of artificial intelligence. At the same\ntime, quantum computers have become much more powerful, and announcements of\nmajor advances are frequently in the news.\n</p>\n<p>The mathematical techniques underlying both these areas have more in common\nthan is sometimes realized. Vector spaces took a position at the axiomatic\nheart of quantum mechanics in the 1930s, and this adoption was a key motivation\nfor the derivation of logic and probability from the linear geometry of vector\nspaces. Quantum interactions between particles are modelled using the tensor\nproduct, which is also used to express objects and operations in artificial\nneural networks.\n</p>\n<p>This paper describes some of these common mathematical areas, including\nexamples of how they are used in artificial intelligence (AI), particularly in\nautomated reasoning and natural language processing (NLP). Techniques discussed\ninclude vector spaces, scalar products, subspaces and implication, orthogonal\nprojection and negation, dual vectors, density matrices, positive operators,\nand tensor products. Application areas include information retrieval,\ncategorization and implication, modelling word-senses and disambiguation,\ninference in knowledge bases, and semantic composition.\n</p>\n<p>Some of these approaches can potentially be implemented on quantum hardware.\nMany of the practical steps in this implementation are in early stages, and\nsome are already realized. Explaining some of the common mathematical tools can\nhelp researchers in both AI and quantum computing further exploit these\noverlaps, recognizing and exploring new directions along the way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Widdows_D/0/1/0/all/0/1\">Dominic Widdows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitto_K/0/1/0/all/0/1\">Kirsty Kitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation. (arXiv:2102.05766v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.05766","description":"<p>Recently, representation learning for text and speech has successfully\nimproved many language related tasks. However, all existing methods suffer from\ntwo limitations: (a) they only learn from one input modality, while a unified\nrepresentation for both speech and text is needed by tasks such as end-to-end\nspeech translation, and as a result,(b) they can not exploit various\nlarge-scale text and speech data and their performance is limited by the\nscarcity of parallel speech translation data.To address these problems, we\npropose a Fused Acoustic and Text Masked Language Model (FAT-MLM) which jointly\nlearns a unified representation for both acoustic and text input from various\ntypes of corpora including parallel data for speech recognition and machine\ntranslation, and even pure speech and text data. Within this cross-modal\nrepresentation learning framework, we further present an end-to-end model for\nFused Acoustic and Text Speech Translation (FAT-ST). Experiments on three\ntranslation directions show that by fine-tuning from FAT-MLM, our proposed\nspeech translation models substantially improve translation quality by up to\n+5.9 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01378","description":"<p>Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model's decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving and Simplifying Pattern Exploiting Training. (arXiv:2103.11955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11955","description":"<p>Recently, pre-trained language models (LMs) have achieved strong performance\nwhen fine-tuned on difficult benchmarks like SuperGLUE. However, performance\ncan suffer when there are very few labeled examples available for fine-tuning.\nPattern Exploiting Training (PET) is a recent approach that leverages patterns\nfor few-shot learning. However, PET uses task-specific unlabeled data. In this\npaper, we focus on few-shot learning without any unlabeled data and introduce\nADAPET, which modifies PET's objective to provide denser supervision during\nfine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any\ntask-specific unlabeled data. Our code can be found at\nhttps://github.com/rrmenon10/ADAPET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1\">Derek Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1\">Rakesh R Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages. (arXiv:2103.14583v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14583","description":"<p>Pre-trained speech representations like wav2vec 2.0 are a powerful tool for\nautomatic speech recognition (ASR). Yet many endangered languages lack\nsufficient data for pre-training such models, or are predominantly oral\nvernaculars without a standardised writing system, precluding fine-tuning.\nQuery-by-example spoken term detection (QbE-STD) offers an alternative for\niteratively indexing untranscribed speech corpora by locating spoken query\nterms. Using data from 7 Australian Aboriginal languages and a regional variety\nof Dutch, all of which are endangered or vulnerable, we show that QbE-STD can\nbe improved by leveraging representations developed for ASR (wav2vec 2.0: the\nEnglish monolingual model and XLSR53 multilingual model). Surprisingly, the\nEnglish model outperformed the multilingual model on 4 Australian language\ndatasets, raising questions around how to optimally leverage self-supervised\nspeech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0\nrepresentations (either English or XLSR53) offer large improvements (56-86%\nrelative) over state-of-the-art approaches on our endangered language datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Browne_M/0/1/0/all/0/1\">Mitchell Browne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_L/0/1/0/all/0/1\">Lily Clifford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibson_F/0/1/0/all/0/1\">Fiona Gibson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_J/0/1/0/all/0/1\">John Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_D/0/1/0/all/0/1\">David Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">Jane Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turpin_M/0/1/0/all/0/1\">Myfany Turpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollmer_M/0/1/0/all/0/1\">Maria Vollmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilmoth_S/0/1/0/all/0/1\">Sasha Wilmoth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting Attributions and QA Model Behavior on Realistic Counterfactuals. (arXiv:2104.04515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04515","description":"<p>When a model attribution technique highlights a particular part of the input,\na user might understand this highlight as making a statement about\ncounterfactuals (Miller, 2019): if that part of the input were to change, the\nmodel's prediction might change as well. This paper investigates how well\ndifferent attribution techniques align with this assumption on realistic\ncounterfactuals in the case of reading comprehension (RC). RC is a particularly\nchallenging test case, as token-level attributions that have been extensively\nstudied in other NLP tasks such as sentiment analysis are less suitable to\nrepresent the reasoning that RC models perform. We construct counterfactual\nsets for three different RC settings, and through heuristics that can connect\nattribution methods' outputs to high-level model behavior, we can evaluate how\nuseful different attribution methods and even different formats are for\nunderstanding counterfactuals. We find that pairwise attributions are better\nsuited to RC than token-level attributions across these different RC settings,\nwith our best performance coming from a modification that we propose to an\nexisting pairwise attribution method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_R/0/1/0/all/0/1\">Rohan Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Zero-Shot Multifaceted Visually Grounded Word Embeddings via Multi-Task Training. (arXiv:2104.07500v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07500","description":"<p>Language grounding aims at linking the symbolic representation of language\n(e.g., words) into the rich perceptual knowledge of the outside world. The\ngeneral approach is to embed both textual and visual information into a common\nspace -the grounded space-confined by an explicit relationship between both\nmodalities. We argue that this approach sacrifices the abstract knowledge\nobtained from linguistic co-occurrence statistics in the process of acquiring\nperceptual information. The focus of this paper is to solve this issue by\nimplicitly grounding the word embeddings. Rather than learning two mappings\ninto a joint space, our approach integrates modalities by determining a\nreversible grounded mapping between the textual and the grounded space by means\nof multi-task learning. Evaluations on intrinsic and extrinsic tasks show that\nour embeddings are highly beneficial for both abstract and concrete words. They\nare strongly correlated with human judgments and outperform previous works on a\nwide range of benchmarks. Our grounded embeddings are publicly available here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahmohammadi_H/0/1/0/all/0/1\">Hassan Shahmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. (arXiv:2104.08455v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08455","description":"<p>Dialogue systems powered by large pre-trained language models (LM) exhibit an\ninnate ability to deliver fluent and natural-looking responses. Despite their\nimpressive generation performance, these models can often generate factually\nincorrect statements impeding their widespread adoption. In this paper, we\nfocus on the task of improving the faithfulness -- and thus reduce\nhallucination -- of Neural Dialogue Systems to known facts supplied by a\nKnowledge Graph (KG). We propose Neural Path Hunter which follows a\ngenerate-then-refine strategy whereby a generated response is amended using the\nk-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level\nfact critic to identify plausible sources of hallucination followed by a\nrefinement stage consisting of a chain of two neural LM's that retrieves\ncorrect entities by crafting a query signal that is propagated over the k-hop\nsubgraph. Our proposed model can easily be applied to any dialogue generated\nresponses without retraining the model. We empirically validate our proposed\napproach on the OpenDialKG dataset against a suite of metrics and report a\nrelative improvement of faithfulness over dialogue responses by 20.35% based on\nFeQA (Durmus et al., 2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1\">Avishek Joey Bose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation. (arXiv:2104.08771v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08771","description":"<p>We study the power of cross-attention in the Transformer architecture within\nthe context of transfer learning for machine translation, and extend the\nfindings of studies into cross-attention when training from scratch. We conduct\na series of experiments through fine-tuning a translation model on data where\neither the source or target language has changed. These experiments reveal that\nfine-tuning only the cross-attention parameters is nearly as effective as\nfine-tuning all parameters (i.e., the entire translation model). We provide\ninsights into why this is the case and observe that limiting fine-tuning in\nthis manner yields cross-lingually aligned embeddings. The implications of this\nfinding for researchers and practitioners include a mitigation of catastrophic\nforgetting, the potential for zero-shot translation, and the ability to extend\nmachine translation models to several new language pairs with reduced parameter\nstorage overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gheini_M/0/1/0/all/0/1\">Mozhdeh Gheini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Low-Dimensional Linear Geometry of Contextualized Word Representations. (arXiv:2105.07109v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07109","description":"<p>Black-box probing models can reliably extract linguistic features like tense,\nnumber, and syntactic role from pretrained word representations. However, the\nmanner in which these features are encoded in representations remains poorly\nunderstood. We present a systematic study of the linear geometry of\ncontextualized word representations in ELMO and BERT. We show that a variety of\nlinguistic features (including structured dependency relationships) are encoded\nin low-dimensional subspaces. We then refine this geometric picture, showing\nthat there are hierarchical relations between the subspaces encoding general\nlinguistic categories and more specific ones, and that low-dimensional feature\nencodings are distributed rather than aligned to individual neurons. Finally,\nwe demonstrate that these linear subspaces are causally related to model\nbehavior, and can be used to perform fine-grained manipulation of BERT's output\ndistribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_E/0/1/0/all/0/1\">Evan Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coreference-Aware Dialogue Summarization. (arXiv:2106.08556v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08556","description":"<p>Summarizing conversations via neural approaches has been gaining research\ntraction lately, yet it is still challenging to obtain practical solutions.\nExamples of such challenges include unstructured information exchange in\ndialogues, informal interactions between speakers, and dynamic role changes of\nspeakers as the dialogue evolves. Many of such challenges result in complex\ncoreference links. Therefore, in this work, we investigate different approaches\nto explicitly incorporate coreference information in neural abstractive\ndialogue summarization models to tackle the aforementioned challenges.\nExperimental results show that the proposed approaches achieve state-of-the-art\nperformance, implying it is useful to utilize coreference information in\ndialogue summarization. Evaluation results on factual correctness suggest such\ncoreference-aware models are better at tracing the information flow among\ninterlocutors and associating accurate status/actions with the corresponding\ninterlocutors and person mentions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Ke Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05541","description":"<p>Chatbots are intelligent software built to be used as a replacement for human\ninteraction. However, existing studies typically do not provide enough support\nfor low-resource languages like Bangla. Moreover, due to the increasing\npopularity of social media, we can also see the rise of interactions in Bangla\ntransliteration (mostly in English) among the native Bangla speakers. In this\npaper, we propose a novel approach to build a Bangla chatbot aimed to be used\nas a business assistant which can communicate in Bangla and Bangla\nTransliteration in English with high confidence consistently. Since annotated\ndata was not available for this purpose, we had to work on the whole machine\nlearning life cycle (data preparation, machine learning modeling, and model\ndeployment) using Rasa Open Source Framework, fastText embeddings, Polyglot\nembeddings, Flask, and other systems as building blocks. While working with the\nskewed annotated dataset, we try out different setups and pipelines to evaluate\nwhich works best and provide possible reasoning behind the observed results.\nFinally, we present a pipeline for intent classification and entity extraction\nwhich achieves reasonable performance (accuracy: 83.02%, precision: 80.82%,\nrecall: 83.02%, F1-score: 80%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahim Shahriar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1\">Mueeze Al Mushabbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation. (arXiv:2107.10821v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.10821","description":"<p>Automatic metrics are commonly used as the exclusive tool for declaring the\nsuperiority of one machine translation system's quality over another. The\ncommunity choice of automatic metric guides research directions and industrial\ndevelopments by deciding which models are deemed better. Evaluating metrics\ncorrelations with sets of human judgements has been limited by the size of\nthese sets. In this paper, we corroborate how reliable metrics are in contrast\nto human judgements on -- to the best of our knowledge -- the largest\ncollection of judgements reported in the literature. Arguably, pairwise\nrankings of two systems are the most common evaluation tasks in research or\ndeployment scenarios. Taking human judgement as a gold standard, we investigate\nwhich metrics have the highest accuracy in predicting translation quality\nrankings for such system pairs. Furthermore, we evaluate the performance of\nvarious metrics across different language pairs and domains. Lastly, we show\nthat the sole use of BLEU impeded the development of improved models leading to\nbad deployment decisions. We release the collection of 2.3M sentence-level\nhuman judgements for 4380 systems for further analysis and replication of our\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grundkiewicz_R/0/1/0/all/0/1\">Roman Grundkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsushita_H/0/1/0/all/0/1\">Hitokazu Matsushita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08614","description":"<p>Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but barely utilize semantic\ndata and knowledge. This paper presents the first QA system that can seamlessly\noperate over RDF datasets and text corpora, or both together, in a unified\nframework. Our method, called UNIQORN, builds a context graph on the fly, by\nretrieving question-relevant triples from the RDF data and/or the text corpus,\nwhere the latter case is handled by automatic information extraction. The\nresulting graph is typically rich but highly noisy. UNIQORN copes with this\ninput by advanced graph algorithms for Group Steiner Trees, that identify the\nbest answer candidates in the context graph. Experimental results on several\nbenchmarks of complex questions with multiple entities and relations, show that\nUNIQORN, an unsupervised method with only five parameters, produces results\ncomparable to the state-of-the-art on KGs, text corpora, and heterogeneous\nsources. The graph-based methodology provides user-interpretable evidence for\nthe complete answering process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEGREE: A Data-Efficient Generative Event Extraction Model. (arXiv:2108.12724v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12724","description":"<p>Event extraction (EE) aims to identify structured events, including event\ntriggers and their corresponding arguments, from unstructured text. Most of the\nexisting works rely on a large number of labeled instances to train models,\nwhile the labeled data could be expensive to be obtained. In this work, we\npresent a data-efficient event extraction method by formulating event\nextraction as a natural language generation problem. The formulation allows us\nto inject knowledge of label semantics, event structure, and output\ndependencies into the model. Given a passage and an event type, our model\nlearns to summarize this passage into a templated sentence in a predefined\nstructure. The template is event-type-specific, manually created, and contains\nevent trigger and argument information. Lastly, a rule-based algorithm is used\nto derive the trigger and argument predictions from the generated sentence. Our\nmethod inherently enjoys the following benefits: (1) The pretraining of the\ngenerative language models help incorporate the semantics of the labels for\ngenerative EE. (2) The autoregressive generation process and our end-to-end\ndesign for extracting triggers and arguments force the model to capture the\ndependencies among the output triggers and their arguments. (3) The predefined\ntemplates form concrete yet flexible rules to hint the models about the valid\npatterns for each event type, reducing the models' burden to learn structures\nfrom the data. Empirical results show that our model achieves superior\nperformance over strong baselines on EE tasks in the low data regime and\nachieves competitive results to the current state-of-the-art when more data\nbecomes available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Retrieval Augmented Generation for Zero-shot Slot Filling. (arXiv:2108.13934v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13934","description":"<p>Automatically inducing high quality knowledge graphs from a given collection\nof documents still remains a challenging problem in AI. One way to make headway\nfor this problem is through advancements in a related task known as slot\nfilling. In this task, given an entity query in form of [Entity, Slot, ?], a\nsystem is asked to fill the slot by generating or extracting the missing value\nexploiting evidence extracted from relevant passage(s) in the given document\ncollection. The recent works in the field try to solve this task in an\nend-to-end fashion using retrieval-based language models. In this paper, we\npresent a novel approach to zero-shot slot filling that extends dense passage\nretrieval with hard negatives and robust training procedures for retrieval\naugmented generation models. Our model reports large improvements on both T-REx\nand zsRE slot filling datasets, improving both passage retrieval and slot value\ngeneration, and ranking at the top-1 position in the KILT leaderboard.\nMoreover, we demonstrate the robustness of our system showing its domain\nadaptation capability on a new variant of the TACRED dataset for slot filling,\nthrough a combination of zero/few-shot learning. We release the source code and\npre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixup Decoding for Diverse Machine Translation. (arXiv:2109.03402v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03402","description":"<p>Diverse machine translation aims at generating various target language\ntranslations for a given source language sentence. Leveraging the linear\nrelationship in the sentence latent space introduced by the mixup training, we\npropose a novel method, MixDiversity, to generate different translations for\nthe input sentence by linearly interpolating it with different sentence pairs\nsampled from the training corpus when decoding. To further improve the\nfaithfulness and diversity of the translations, we propose two simple but\neffective approaches to select diverse sentence pairs in the training corpus\nand adjust the interpolation weight for each pair correspondingly. Moreover, by\ncontrolling the interpolation weight, our method can achieve the trade-off\nbetween faithfulness and diversity without any additional training, which is\nrequired in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14\nen-de, and WMT'17 zh-en are conducted to show that our method substantially\noutperforms all previous diverse machine translation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuanfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories. (arXiv:2109.03754v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03754","description":"<p>Measuring event salience is essential in the understanding of stories. This\npaper takes a recent unsupervised method for salience detection derived from\nBarthes Cardinal Functions and theories of surprise and applies it to longer\nnarrative forms. We improve the standard transformer language model by\nincorporating an external knowledgebase (derived from Retrieval Augmented\nGeneration) and adding a memory mechanism to enhance performance on longer\nworks. We use a novel approach to derive salience annotation using\nchapter-aligned summaries from the Shmoop corpus for classic literary works.\nOur evaluation against this data demonstrates that our salience detection model\nimproves performance over and above a non-knowledgebase and memory augmented\nlanguage model, both of which are crucial to this improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilmot_D/0/1/0/all/0/1\">David Wilmot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PPT: Pre-trained Prompt Tuning for Few-shot Learning. (arXiv:2109.04332v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04332","description":"<p>Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks. (arXiv:2109.04604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04604","description":"<p>The general goal of text simplification (TS) is to reduce text complexity for\nhuman consumption. This paper investigates another potential use of neural TS:\nassisting machines performing natural language processing (NLP) tasks. We\nevaluate the use of neural TS in two ways: simplifying input texts at\nprediction time and augmenting data to provide machines with additional\ninformation during training. We demonstrate that the latter scenario provides\npositive effects on machine performance on two separate datasets. In\nparticular, the latter use of TS improves the performances of LSTM (1.82-1.98%)\nand SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,\nreal-world relation extraction task. Further, the same setting yields\nimprovements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT\ntext classifier on MNLI, a practical natural language inference dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1\">Hoang Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems. (arXiv:2109.04645v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04645","description":"<p>As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema (definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5) is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoR: Read-over-Read for Long Document Machine Reading Comprehension. (arXiv:2109.04780v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04780","description":"<p>Transformer-based pre-trained models, such as BERT, have achieved remarkable\nresults on machine reading comprehension. However, due to the constraint of\nencoding length (e.g., 512 WordPiece tokens), a long document is usually split\ninto multiple chunks that are independently read. It results in the reading\nfield being limited to individual chunks without information collaboration for\nlong document machine reading comprehension. To address this problem, we\npropose RoR, a read-over-read method, which expands the reading field from\nchunk to document. Specifically, RoR includes a chunk reader and a document\nreader. The former first predicts a set of regional answers for each chunk,\nwhich are then compacted into a highly-condensed version of the original\ndocument, guaranteeing to be encoded once. The latter further predicts the\nglobal answers from this condensed document. Eventually, a voting strategy is\nutilized to aggregate and rerank the regional and global answers for final\nprediction. Extensive experiments on two benchmarks QuAC and TriviaQA\ndemonstrate the effectiveness of RoR for long document reading. Notably, RoR\nranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of\nsubmission (May 17th, 2021).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05238","description":"<p>Simultaneous machine translation (SiMT) generates translation before reading\nthe entire source sentence and hence it has to trade off between translation\nquality and latency. To fulfill the requirements of different translation\nquality and latency in practical applications, the previous methods usually\nneed to train multiple SiMT models for different latency levels, resulting in\nlarge computational costs. In this paper, we propose a universal SiMT model\nwith Mixture-of-Experts Wait-k Policy to achieve the best translation quality\nunder arbitrary latency with only one trained model. Specifically, our method\nemploys multi-head attention to accomplish the mixture of experts where each\nhead is treated as a wait-k expert with its own waiting words number, and given\na test latency and source inputs, the weights of the experts are accordingly\nadjusted to produce the best translation. Experiments on three datasets show\nthat our method outperforms all the strong baselines under different latency,\nincluding the state-of-the-art adaptive policy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. (arXiv:2109.05244v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05244","description":"<p>Cross-attention is an important component of neural machine translation\n(NMT), which is always realized by dot-product attention in previous methods.\nHowever, dot-product attention only considers the pair-wise correlation between\nwords, resulting in dispersion when dealing with long sentences and neglect of\nsource neighboring relationships. Inspired by linguistics, the above issues are\ncaused by ignoring a type of cross-attention, called concentrated attention,\nwhich focuses on several central words and then spreads around them. In this\nwork, we apply Gaussian Mixture Model (GMM) to model the concentrated attention\nin cross-attention. Experiments and analyses we conducted on three datasets\nshow that the proposed method outperforms the baseline and has significant\nimprovement on alignment quality, N-gram accuracy, and long sentence\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora. (arXiv:2109.05406v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05406","description":"<p>Human conversations consist of reasonable and natural topic flows, which are\nobserved as the shifts of the mentioned concepts across utterances. Previous\nchatbots that incorporate the external commonsense knowledge graph prove that\nmodeling the concept shifts can effectively alleviate the dull and\nuninformative response dilemma. However, there still exists a gap between the\nconcept relations in the natural conversation and those in the external\ncommonsense knowledge graph, which is an issue to solve. Specifically, the\nconcept relations in the external commonsense knowledge graph are not\nintuitively built from the conversational scenario but the world knowledge,\nwhich makes them insufficient for the chatbot construction. To bridge the above\ngap, we propose the method to supply more concept relations extracted from the\nconversational corpora and reconstruct an enhanced concept graph for the\nchatbot construction. In addition, we present a novel, powerful, and fast graph\nencoding architecture named the Edge-Transformer to replace the traditional GNN\narchitecture. Experimental results on the Reddit conversation dataset indicate\nour proposed method significantly outperforms strong baseline systems and\nachieves new SOTA results. Further analysis individually proves the\neffectiveness of the enhanced concept graph and the Edge-Transformer\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_P/0/1/0/all/0/1\">Pengda Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Language-Dependent Ethnic Bias in BERT. (arXiv:2109.05704v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05704","description":"<p>BERT and other large-scale language models (LMs) contain gender and racial\nbias. They also exhibit other dimensions of social bias, most of which have not\nbeen studied in depth, and some of which vary depending on the language. In\nthis paper, we study ethnic bias and how it varies across languages by\nanalyzing and mitigating ethnic bias in monolingual BERT for English, German,\nSpanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we\ndevelop a novel metric called Categorical Bias score. Then we propose two\nmethods for mitigation; first using a multilingual model, and second using\ncontextual word alignment of two monolingual models. We compare our proposed\nmethods with monolingual BERT and show that these methods effectively alleviate\nthe ethnic bias. Which of the two methods works better depends on the amount of\nNLP resources available for that language. We additionally experiment with\nArabic and Greek to verify that our proposed methods work for a wider variety\nof languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Jaimeen Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation. (arXiv:2109.05729v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05729","description":"<p>In this paper, we take the advantage of previous pre-trained models (PTMs)\nand propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different\nfrom previous Chinese PTMs, CPT is designed for both natural language\nunderstanding (NLU) and natural language generation (NLG) tasks. CPT consists\nof three parts: a shared encoder, an understanding decoder, and a generation\ndecoder. Two specific decoders with a shared encoder are pre-trained with\nmasked language modeling (MLM) and denoising auto-encoding (DAE) tasks,\nrespectively. With the partially shared architecture and multi-task\npre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks\nwith two decoders and (2) be fine-tuned flexibly that fully exploits the\npotential of the model. Moreover, the unbalanced Transformer saves the\ncomputational and storage cost, which makes CPT competitive and greatly\naccelerates the inference of text generation. Experimental results on a wide\nrange of Chinese NLU and NLG tasks show the effectiveness of CPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhichao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_L/0/1/0/all/0/1\">Li Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Categorical Semantics of Reversible Pattern-Matching. (arXiv:2109.05837v2 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2109.05837","description":"<p>This paper is concerned with categorical structures for reversible\ncomputation. In particular, we focus on a typed, functional reversible language\nbased on Theseus. We discuss how join inverse rig categories do not in general\ncapture pattern-matching, the core construct Theseus uses to enforce\nreversibility. We then derive a categorical structure to add to join inverse\nrig categories in order to capture pattern-matching. We show how such a\nstructure makes an adequate model for reversible pattern-matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemonnier_L/0/1/0/all/0/1\">Louis Lemonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chardonnet_K/0/1/0/all/0/1\">Kostia Chardonnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valiron_B/0/1/0/all/0/1\">Beno&#xee;t Valiron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question Answering over Electronic Devices: A New Benchmark Dataset and a Multi-Task Learning based QA Framework. (arXiv:2109.05897v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05897","description":"<p>Answering questions asked from instructional corpora such as E-manuals,\nrecipe books, etc., has been far less studied than open-domain factoid\ncontext-based question answering. This can be primarily attributed to the\nabsence of standard benchmark datasets. In this paper we meticulously create a\nlarge amount of data connected with E-manuals and develop suitable algorithm to\nexploit it. We collect E-Manual Corpus, a huge corpus of 307,957 E-manuals and\npretrain RoBERTa on this large corpus. We create various benchmark QA datasets\nwhich include question answer pairs curated by experts based upon two\nE-manuals, real user questions from Community Question Answering Forum\npertaining to E-manuals etc. We introduce EMQAP (E-Manual Question Answering\nPipeline) that answers questions pertaining to electronics devices. Built upon\nthe pretrained RoBERTa, it harbors a supervised multi-task learning framework\nwhich efficiently performs the dual tasks of identifying the section in the\nE-manual where the answer can be found and the exact answer span within that\nsection. For E-Manual annotated question-answer pairs, we show an improvement\nof about 40% in ROUGE-L F1 scores over the most competitive baseline. We\nperform a detailed ablation study and establish the versatility of EMQAP across\ndifferent circumstances. The code and datasets are shared at\nhttps://github.com/abhi1nandy2/EMNLP-2021-Findings, and the corresponding\nproject website is https://sites.google.com/view/emanualqa/home.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nandy_A/0/1/0/all/0/1\">Abhilash Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Soumya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddhashiya_S/0/1/0/all/0/1\">Shubham Maddhashiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_K/0/1/0/all/0/1\">Kapil Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color. (arXiv:2109.06129v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06129","description":"<p>Pretrained language models have been shown to encode relational information,\nsuch as the relations between entities or concepts in knowledge-bases --\n(Paris, Capital, France). However, simple relations of this type can often be\nrecovered heuristically and the extent to which models implicitly reflect\ntopological structure that is grounded in world, such as perceptual structure,\nis unknown. To explore this question, we conduct a thorough case study on\ncolor. Namely, we employ a dataset of monolexemic color terms and color chips\nrepresented in CIELAB, a color space with a perceptually meaningful distance\nmetric.\n</p>\n<p>Using two methods of evaluating the structural alignment of colors in this\nspace with text-derived color term representations, we find significant\ncorrespondence. Analyzing the differences in alignment across the color\nspectrum, we find that warmer colors are, on average, better aligned to the\nperceptual color space than cooler ones, suggesting an intriguing connection to\nfindings from recent work on efficient communication in color naming. Further\nanalysis suggests that differences in alignment are, in part, mediated by\ncollocationality and differences in syntactic usage, posing questions as to the\nrelationship between color perception and usage and context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdou_M/0/1/0/all/0/1\">Mostafa Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulmizev_A/0/1/0/all/0/1\">Artur Kulmizev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1\">Stella Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Generatively Augmented Neural Network Watchdog for Image Classification Networks. (arXiv:2109.06168v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06168","description":"<p>The identification of out-of-distribution data is vital to the deployment of\nclassification networks. For example, a generic neural network that has been\ntrained to differentiate between images of dogs and cats can only classify an\ninput as either a dog or a cat. If a picture of a car or a kumquat were to be\nsupplied to this classifier, the result would still be either a dog or a cat.\nIn order to mitigate this, techniques such as the neural network watchdog have\nbeen developed. The compression of the image input into the latent layer of the\nautoencoder defines the region of in-distribution in the image space. This\nin-distribution set of input data has a corresponding boundary in the image\nspace. The watchdog assesses whether inputs are in inside or outside this\nboundary. This paper demonstrates how to sharpen this boundary using generative\nnetwork training data augmentation thereby bettering the discrimination and\noverall performance of the watchdog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_J/0/1/0/all/0/1\">Justin M. Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amigo_G/0/1/0/all/0/1\">Glauco A. Amigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_R/0/1/0/all/0/1\">Robert J. Marks II</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Abstraction in Distributed Probabilistic SLAM Graphs. (arXiv:2109.06241v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06241","description":"<p>Scene graphs represent the key components of a scene in a compact and\nsemantically rich way, but are difficult to build during incremental SLAM\noperation because of the challenges of robustly identifying abstract scene\nelements and optimising continually changing, complex graphs. We present a\ndistributed, graph-based SLAM framework for incrementally building scene graphs\nbased on two novel components. First, we propose an incremental abstraction\nframework in which a neural network proposes abstract scene elements that are\nincorporated into the factor graph of a feature-based monocular SLAM system.\nScene elements are confirmed or rejected through optimisation and incrementally\nreplace the points yielding a more dense, semantic and compact representation.\nSecond, enabled by our novel routing procedure, we use Gaussian Belief\nPropagation (GBP) for distributed inference on a graph processor. The time per\niteration of GBP is structure-agnostic and we demonstrate the speed advantages\nover direct methods for inference of heterogeneous factor graphs. We run our\nsystem on real indoor datasets using planar abstractions and recover the major\nplanes with significant compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Joseph Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_T/0/1/0/all/0/1\">Talfan Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucar_E/0/1/0/all/0/1\">Edgar Sucar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2109.06274v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06274","description":"<p>Automatic methods to segment the vestibular schwannoma (VS) tumors and the\ncochlea from magnetic resonance imaging (MRI) are critical to VS treatment\nplanning. Although supervised methods have achieved satisfactory performance in\nVS segmentation, they require full annotations by experts, which is laborious\nand time-consuming. In this work, we aim to tackle the VS and cochlea\nsegmentation problem in an unsupervised domain adaptation setting. Our proposed\nmethod leverages both the image-level domain alignment to minimize the domain\ndivergence and semi-supervised training to further boost the performance.\nFurthermore, we propose to fuse the labels predicted from multiple models via\nnoisy label correction. Our results on the challenge validation leaderboard\nshowed that our unsupervised method has achieved promising VS and cochlea\nsegmentation performance with mean dice score of 0.8261 $\\pm$ 0.0416; The mean\ndice value for the tumor is 0.8302 $\\pm$ 0.0772. This is comparable to the\nweakly-supervised based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dingjie Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McNeil_A/0/1/0/all/0/1\">Andrew McNeil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawant_B/0/1/0/all/0/1\">Benoit M.Dawant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks. (arXiv:2109.06275v1 [cs.AI])","link":"http://arxiv.org/abs/2109.06275","description":"<p>An ideal integration of autonomous agents in a human world implies that they\nare able to collaborate on human terms. In particular, theory of mind plays an\nimportant role in maintaining common ground during human collaboration and\ncommunication. To enable theory of mind modeling in situated interactions, we\nintroduce a fine-grained dataset of collaborative tasks performed by pairs of\nhuman subjects in the 3D virtual blocks world of Minecraft. It provides\ninformation that captures partners' beliefs of the world and of each other as\nan interaction unfolds, bringing abundant opportunities to study human\ncollaborative behaviors in situated language communication. As a first step\ntowards our goal of developing embodied AI agents able to infer belief states\nof collaborative partners in situ, we build and present results on\ncomputational models for several theory of mind tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bara_C/0/1/0/all/0/1\">Cristian-Paul Bara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+CH_Wang_S/0/1/0/all/0/1\">Sky CH-Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Camera Localization for Automated Vehicles Using Image Retrieval. (arXiv:2109.06296v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06296","description":"<p>We address the problem of finding the current position and heading angle of\nan autonomous vehicle in real-time using a single camera. Compared to methods\nwhich require LiDARs and high definition (HD) 3D maps in real-time, the\nproposed approach is easily scalable and computationally efficient, at the\nprice of lower precision.\n</p>\n<p>The new method combines and adapts existing algorithms in three different\nfields: image retrieval, mapping database, and particle filtering. The result\nis a simple, real-time localization method using an image retrieval method\nwhose performance is comparable to other monocular camera localization methods\nwhich use a map built with LiDARs.\n</p>\n<p>We evaluate the proposed method using the KITTI odometry dataset and via\nclosed-loop experiments with an indoor 1:10 autonomous vehicle. The tests\ndemonstrate real-time capability and a 10cm level accuracy. Also, experimental\nresults of the closed-loop indoor tests show the presence of a positive\nfeedback loop between the localization error and the control error. Such\nphenomena is analysed in details at the end of the article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joa_E/0/1/0/all/0/1\">Eunhyek Joa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borrelli_F/0/1/0/all/0/1\">Francesco Borrelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics Driven Domain Specific Transporter Framework with Attention Mechanism for Ultrasound Imaging. (arXiv:2109.06346v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06346","description":"<p>Most applications of deep learning techniques in medical imaging are\nsupervised and require a large number of labeled data which is expensive and\nrequires many hours of careful annotation by experts. In this paper, we propose\nan unsupervised, physics driven domain specific transporter framework with an\nattention mechanism to identify relevant key points with applications in\nultrasound imaging. The proposed framework identifies key points that provide a\nconcise geometric representation highlighting regions with high structural\nvariation in ultrasound videos. We incorporate physics driven domain specific\ninformation as a feature probability map and use the radon transform to\nhighlight features in specific orientations. The proposed framework has been\ntrained on130 Lung ultrasound (LUS) videos and 113 Wrist ultrasound (WUS)\nvideos and validated on 100 Lung ultrasound (LUS) videos and 58 Wrist\nultrasound (WUS) videos acquired from multiple centers across the globe. Images\nfrom both datasets were independently assessed by experts to identify\nclinically relevant features such as A-lines, B-lines and pleura from LUS and\nradial metaphysis, radial epiphysis and carpal bones from WUS videos. The key\npoints detected from both datasets showed high sensitivity (LUS = 99\\% , WUS =\n74\\%) in detecting the image landmarks identified by experts. Also, on\nemploying for classification of the given lung image into normal and abnormal\nclasses, the proposed approach, even with no prior training, achieved an\naverage accuracy of 97\\% and an average F1-score of 95\\% respectively on the\ntask of co-classification with 3 fold cross-validation. With the purely\nunsupervised nature of the proposed approach, we expect the key point detection\napproach to increase the applicability of ultrasound in various examination\nperformed in emergency and point of care.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tripathi_A/0/1/0/all/0/1\">Arpan Tripathi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rakkunedeth_A/0/1/0/all/0/1\">Abhilash Rakkunedeth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panicker_M/0/1/0/all/0/1\">Mahesh Raveendranatha Panicker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jack Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boora_N/0/1/0/all/0/1\">Naveenjyote Boora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knight_J/0/1/0/all/0/1\">Jessica Knight</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob Jaremko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yale Tung Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayan_K/0/1/0/all/0/1\">Kiran Vishnu Narayan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+C_K/0/1/0/all/0/1\">Kesavadas C</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POPCORN: Progressive Pseudo-labeling with Consistency Regularization and Neighboring. (arXiv:2109.06361v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06361","description":"<p>Semi-supervised learning (SSL) uses unlabeled data to compensate for the\nscarcity of annotated images and the lack of method generalization to unseen\ndomains, two usual problems in medical segmentation tasks. In this work, we\npropose POPCORN, a novel method combining consistency regularization and\npseudo-labeling designed for image segmentation. The proposed framework uses\nhigh-level regularization to constrain our segmentation model to use similar\nlatent features for images with similar segmentations. POPCORN estimates a\nproximity graph to select data from easiest ones to more difficult ones, in\norder to ensure accurate pseudo-labeling and to limit confirmation bias.\nApplied to multiple sclerosis lesion segmentation, our method demonstrates\ncompetitive results compared to other state-of-the-art SSL strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamraoui_R/0/1/0/all/0/1\">Reda Abdellah Kamraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ta_V/0/1/0/all/0/1\">Vinh-Thong Ta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Compaire_F/0/1/0/all/0/1\">Fanny Compaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjon_J/0/1/0/all/0/1\">Jos&#xe9; V Manjon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coupe_P/0/1/0/all/0/1\">Pierrick Coup&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensor Adversarial Traits: Analyzing Robustness of 3D Object Detection Sensor Fusion Models. (arXiv:2109.06363v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06363","description":"<p>A critical aspect of autonomous vehicles (AVs) is the object detection stage,\nwhich is increasingly being performed with sensor fusion models: multimodal 3D\nobject detection models which utilize both 2D RGB image data and 3D data from a\nLIDAR sensor as inputs. In this work, we perform the first study to analyze the\nrobustness of a high-performance, open source sensor fusion model architecture\ntowards adversarial attacks and challenge the popular belief that the use of\nadditional sensors automatically mitigate the risk of adversarial attacks. We\nfind that despite the use of a LIDAR sensor, the model is vulnerable to our\npurposefully crafted image-based adversarial attacks including disappearance,\nuniversal patch, and spoofing. After identifying the underlying reason, we\nexplore some potential defenses and provide some recommendations for improved\nsensor fusion models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Won Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Alfred Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Z. Morley Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Heatmaps to Structural Explanations of Image Classifiers. (arXiv:2109.06365v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06365","description":"<p>This paper summarizes our endeavors in the past few years in terms of\nexplaining image classifiers, with the aim of including negative results and\ninsights we have gained. The paper starts with describing the explainable\nneural network (XNN), which attempts to extract and visualize several\nhigh-level concepts purely from the deep network, without relying on human\nlinguistic concepts. This helps users understand network classifications that\nare less intuitive and substantially improves user performance on a difficult\nfine-grained classification task of discriminating among different species of\nseagulls.\n</p>\n<p>Realizing that an important missing piece is a reliable heatmap visualization\ntool, we have developed I-GOS and iGOS++ utilizing integrated gradients to\navoid local optima in heatmap generation, which improved the performance across\nall resolutions. During the development of those visualizations, we realized\nthat for a significant number of images, the classifier has multiple different\npaths to reach a confident prediction. This has lead to our recent development\nof structured attention graphs (SAGs), an approach that utilizes beam search to\nlocate multiple coarse heatmaps for a single image, and compactly visualizes a\nset of heatmaps by capturing how different combinations of image regions impact\nthe confidence of a classifier.\n</p>\n<p>Through the research process, we have learned much about insights in building\ndeep network explanations, the existence and frequency of multiple\nexplanations, and various tricks of the trade that make explanations work. In\nthis paper, we attempt to share those insights and opinions with the readers\nwith the hope that some of them will be informative for future researchers on\nexplainable deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fuxin_L/0/1/0/all/0/1\">Li Fuxin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhongang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorram_S/0/1/0/all/0/1\">Saeed Khorram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shitole_V/0/1/0/all/0/1\">Vivswan Shitole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadepalli_P/0/1/0/all/0/1\">Prasad Tadepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1\">Minsuk Kahng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1\">Alan Fern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaPruner: Adaptive Channel Pruning and Effective Weights Inheritance. (arXiv:2109.06397v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06397","description":"<p>Channel pruning is one of the major compression approaches for deep neural\nnetworks. While previous pruning methods have mostly focused on identifying\nunimportant channels, channel pruning is considered as a special case of neural\narchitecture search in recent years. However, existing methods are either\ncomplicated or prone to sub-optimal pruning. In this paper, we propose a\npruning framework that adaptively determines the number of each layer's\nchannels as well as the wights inheritance criteria for sub-network. Firstly,\nevaluate the importance of each block in the network based on the mean of the\nscaling parameters of the BN layers. Secondly, use the bisection method to\nquickly find the compact sub-network satisfying the budget. Finally, adaptively\nand efficiently choose the weight inheritance criterion that fits the current\narchitecture and fine-tune the pruned network to recover performance. AdaPruner\nallows to obtain pruned network quickly, accurately and efficiently, taking\ninto account both the structure and initialization weights. We prune the\ncurrently popular CNN models (VGG, ResNet, MobileNetV2) on different image\nclassification datasets, and the experimental results demonstrate the\neffectiveness of our proposed method. On ImageNet, we reduce 32.8% FLOPs of\nMobileNetV2 with only 0.62% decrease for top-1 accuracy, which exceeds all\nprevious state-of-the-art channel pruning methods. The code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangcheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jian Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hongyi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos. (arXiv:2109.06398v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06398","description":"<p>We address the problem of temporal sentence localization in videos (TSLV).\nTraditional methods follow a top-down framework which localizes the target\nsegment with pre-defined segment proposals. Although they have achieved decent\nperformance, the proposals are handcrafted and redundant. Recently, bottom-up\nframework attracts increasing attention due to its superior efficiency. It\ndirectly predicts the probabilities for each frame as a boundary. However, the\nperformance of bottom-up model is inferior to the top-down counterpart as it\nfails to exploit the segment-level interaction. In this paper, we propose an\nAdaptive Proposal Generation Network (APGN) to maintain the segment-level\ninteraction while speeding up the efficiency. Specifically, we first perform a\nforeground-background classification upon the video and regress on the\nforeground frames to adaptively generate proposals. In this way, the\nhandcrafted proposal design is discarded and the redundant proposals are\ndecreased. Then, a proposal consolidation module is further developed to\nenhance the semantic of the generated proposals. Finally, we locate the target\nmoments with these generated proposals following the top-down framework.\nExtensive experiments on three challenging benchmarks show that our proposed\nAPGN significantly outperforms previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding. (arXiv:2109.06400v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06400","description":"<p>A key solution to temporal sentence grounding (TSG) exists in how to learn\neffective alignment between vision and language features extracted from an\nuntrimmed video and a sentence description. Existing methods mainly leverage\nvanilla soft attention to perform the alignment in a single-step process.\nHowever, such single-step attention is insufficient in practice, since\ncomplicated relations between inter- and intra-modality are usually obtained\nthrough multi-step reasoning. In this paper, we propose an Iterative Alignment\nNetwork (IA-Net) for TSG task, which iteratively interacts inter- and\nintra-modal features within multiple steps for more accurate grounding.\nSpecifically, during the iterative reasoning process, we pad multi-modal\nfeatures with learnable parameters to alleviate the nowhere-to-attend problem\nof non-matched frame-word pairs, and enhance the basic co-attention mechanism\nin a parallel manner. To further calibrate the misaligned attention caused by\neach reasoning step, we also devise a calibration module following each\nattention module to refine the alignment knowledge. With such iterative\nalignment scheme, our IA-Net can robustly capture the fine-grained relations\nbetween vision and language domains step-by-step for progressively reasoning\nthe temporal boundaries. Extensive experiments conducted on three challenging\nbenchmarks demonstrate that our proposed model performs better than the\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camera-Tracklet-Aware Contrastive Learning for Unsupervised Vehicle Re-Identification. (arXiv:2109.06401v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06401","description":"<p>Recently, vehicle re-identification methods based on deep learning constitute\nremarkable achievement. However, this achievement requires large-scale and\nwell-annotated datasets. In constructing the dataset, assigning globally\navailable identities (Ids) to vehicles captured from a great number of cameras\nis labour-intensive, because it needs to consider their subtle appearance\ndifferences or viewpoint variations. In this paper, we propose\ncamera-tracklet-aware contrastive learning (CTACL) using the multi-camera\ntracklet information without vehicle identity labels. The proposed CTACL\ndivides an unlabelled domain, i.e., entire vehicle images, into multiple\ncamera-level subdomains and conducts contrastive learning within and beyond the\nsubdomains. The positive and negative samples for contrastive learning are\ndefined using tracklet Ids of each camera. Additionally, the domain adaptation\nacross camera networks is introduced to improve the generalisation performance\nof learnt representations and alleviate the performance degradation resulted\nfrom the domain gap between the subdomains. We demonstrate the effectiveness of\nour approach on video-based and image-based vehicle Re-ID datasets.\nExperimental results show that the proposed method outperforms the recent\nstate-of-the-art unsupervised vehicle Re-ID methods. The source code for this\npaper is publicly available on\n`https://github.com/andreYoo/CTAM-CTACL-VVReID.git'.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jongmin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minkyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Hyeontaek Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-Net MLSys: Designing COVID-Net for the Clinical Workflow. (arXiv:2109.06421v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06421","description":"<p>As the COVID-19 pandemic continues to devastate globally, one promising field\nof research is machine learning-driven computer vision to streamline various\nparts of the COVID-19 clinical workflow. These machine learning methods are\ntypically stand-alone models designed without consideration for the integration\nnecessary for real-world application workflows. In this study, we take a\nmachine learning and systems (MLSys) perspective to design a system for\nCOVID-19 patient screening with the clinical workflow in mind. The COVID-Net\nsystem is comprised of the continuously evolving COVIDx dataset, COVID-Net deep\nneural network for COVID-19 patient detection, and COVID-Net S deep neural\nnetworks for disease severity scoring for COVID-19 positive patient cases. The\ndeep neural networks within the COVID-Net system possess state-of-the-art\nperformance, and are designed to be integrated within a user interface (UI) for\nclinical decision support with automatic report generation to assist clinicians\nin their treatment decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chung_A/0/1/0/all/0/1\">Audrey G. Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Terhljan_N/0/1/0/all/0/1\">Naomi Terhljan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1\">Alexander MacLean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aboutalebi_H/0/1/0/all/0/1\">Hossein Aboutalebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Surana_S/0/1/0/all/0/1\">Siddharth Surana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1\">Andy Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Region Domain Adaptation for Class-level Alignment. (arXiv:2109.06422v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06422","description":"<p>Semantic segmentation requires a lot of training data, which necessitates\ncostly annotation. There have been many studies on unsupervised domain\nadaptation (UDA) from one domain to another, e.g., from computer graphics to\nreal images. However, there is still a gap in accuracy between UDA and\nsupervised training on native domain data. It is arguably attributable to\nclass-level misalignment between the source and target domain data. To cope\nwith this, we propose a method that applies adversarial training to align two\nfeature distributions in the target domain. It uses a self-training framework\nto split the image into two regions (i.e., trusted and untrusted), which form\ntwo distributions to align in the feature space. We term this approach\ncross-region adaptation (CRA) to distinguish from the previous methods of\naligning different domain distributions, which we call cross-domain adaptation\n(CDA). CRA can be applied after any CDA method. Experimental results show that\nthis always improves the accuracy of the combined CDA method, having updated\nthe state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Few-shot Segmentation by Redifinition of the Roles of Multi-level CNN Features. (arXiv:2109.06432v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06432","description":"<p>This study is concerned with few-shot segmentation, i.e., segmenting the\nregion of an unseen object class in a query image, given support image(s) of\nits instances. The current methods rely on the pretrained CNN features of the\nsupport and query images. The key to good performance depends on the proper\nfusion of their mid-level and high-level features; the former contains\nshape-oriented information, while the latter has class-oriented information.\nCurrent state-of-the-art methods follow the approach of Tian et al., which\ngives the mid-level features the primary role and the high-level features the\nsecondary role. In this paper, we reinterpret this widely employed approach by\nredifining the roles of the multi-level features; we swap the primary and\nsecondary roles. Specifically, we regard that the current methods improve the\ninitial estimate generated from the high-level features using the mid-level\nfeatures. This reinterpretation suggests a new application of the current\nmethods: to apply the same network multiple times to iteratively update the\nestimate of the object's region, starting from its initial estimate. Our\nexperiments show that this method is effective and has updated the previous\nstate-of-the-art on COCO-20$^i$ in the 1-shot and 5-shot settings and on\nPASCAL-5$^i$ in the 1-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tesla-Rapture: A Lightweight Gesture Recognition System from mmWave Radar Point Clouds. (arXiv:2109.06448v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06448","description":"<p>We present Tesla-Rapture, a gesture recognition interface for point clouds\ngenerated by mmWave Radars. State of the art gesture recognition models are\neither too resource consuming or not sufficiently accurate for integration into\nreal-life scenarios using wearable or constrained equipment such as IoT devices\n(e.g. Raspberry PI), XR hardware (e.g. HoloLens), or smart-phones. To tackle\nthis issue, we developed Tesla, a Message Passing Neural Network (MPNN) graph\nconvolution approach for mmWave radar point clouds. The model outperforms the\nstate of the art on two datasets in terms of accuracy while reducing the\ncomputational complexity and, hence, the execution time. In particular, the\napproach, is able to predict a gesture almost 8 times faster than the most\naccurate competitor. Our performance evaluation in different scenarios\n(environments, angles, distances) shows that Tesla generalizes well and\nimproves the accuracy up to 20% in challenging scenarios like a through-wall\nsetting and sensing at extreme angles. Utilizing Tesla, we develop\nTesla-Rapture, a real-time implementation using a mmWave Radar on a Raspberry\nPI 4 and evaluate its accuracy and time-complexity. We also publish the source\ncode, the trained models, and the implementation of the model for embedded\ndevices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salami_D/0/1/0/all/0/1\">Dariush Salami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasibi_R/0/1/0/all/0/1\">Ramin Hasibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palipana_S/0/1/0/all/0/1\">Sameera Palipana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1\">Petar Popovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michoel_T/0/1/0/all/0/1\">Tom Michoel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigg_S/0/1/0/all/0/1\">Stephan Sigg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spiking Neural Networks for Visual Place Recognition via Weighted Neuronal Assignments. (arXiv:2109.06452v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06452","description":"<p>Spiking neural networks (SNNs) offer both compelling potential advantages,\nincluding energy efficiency and low latencies, and challenges including the\nnon-differentiable nature of event spikes. Much of the initial research in this\narea has converted deep neural networks to equivalent SNNs, but this conversion\napproach potentially negates some of the potential advantages of SNN-based\napproaches developed from scratch. One promising area for high performance SNNs\nis template matching and image recognition. This research introduces the first\nhigh performance SNN for the Visual Place Recognition (VPR) task: given a query\nimage, the SNN has to find the closest match out of a list of reference images.\nAt the core of this new system is a novel assignment scheme that implements a\nform of ambiguity-informed salience, by up-weighting single-place-encoding\nneurons and down-weighting \"ambiguous\" neurons that respond to multiple\ndifferent reference places. In a range of experiments on the challenging Oxford\nRobotCar and Nordland datasets, we show that our SNN achieves comparable VPR\nperformance to state-of-the-art and classical techniques, and degrades\ngracefully in performance with an increasing number of reference places. Our\nresults provide a significant milestone towards SNNs that can provide robust,\nenergy-efficient and low latency robot localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussaini_S/0/1/0/all/0/1\">Somayeh Hussaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dodging Attack Using Carefully Crafted Natural Makeup. (arXiv:2109.06467v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06467","description":"<p>Deep learning face recognition models are used by state-of-the-art\nsurveillance systems to identify individuals passing through public areas\n(e.g., airports). Previous studies have demonstrated the use of adversarial\nmachine learning (AML) attacks to successfully evade identification by such\nsystems, both in the digital and physical domains. Attacks in the physical\ndomain, however, require significant manipulation to the human participant's\nface, which can raise suspicion by human observers (e.g. airport security\nofficers). In this study, we present a novel black-box AML attack which\ncarefully crafts natural makeup, which, when applied on a human participant,\nprevents the participant from being identified by facial recognition models. We\nevaluated our proposed attack against the ArcFace face recognition model, with\n20 participants in a real-world setup that includes two cameras, different\nshooting angles, and different lighting conditions. The evaluation results show\nthat in the digital domain, the face recognition system was unable to identify\nall of the participants, while in the physical domain, the face recognition\nsystem was able to identify the participants in only 1.22% of the frames\n(compared to 47.57% without makeup and 33.73% with random natural makeup),\nwhich is below a reasonable threshold of a realistic operational environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guetta_N/0/1/0/all/0/1\">Nitzan Guetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1\">Asaf Shabtai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Inderjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momiyama_S/0/1/0/all/0/1\">Satoru Momiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Space Time Recurrent Memory Network. (arXiv:2109.06474v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06474","description":"<p>We propose a novel visual memory network architecture for the learning and\ninference problem in the spatial-temporal domain. Different from the popular\ntransformers, we maintain a fixed set of memory slots in our memory network and\nexplore designs to input new information into the memory, combine the\ninformation in different memory slots and decide when to discard old memory\nslots. Finally, this architecture is benchmarked on the video object\nsegmentation and video prediction problems. Through the experiments, we show\nthat our memory architecture can achieve competitive results with\nstate-of-the-art while maintaining constant memory capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fuxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-Based Alignment of 3D Scans. (arXiv:2109.06526v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06526","description":"<p>Full 3D scanning can efficiently be obtained using structured light scanning\ncombined with a rotation stage. In this setting it is, however, necessary to\nreposition the object and scan it in different poses in order to cover the\nentire object. In this case, correspondence between the scans is lost, since\nthe object was moved. In this paper, we propose a fully automatic method for\naligning the scans of an object in two different poses. This is done by\nmatching 2D features between images from two poses and utilizing correspondence\nbetween the images and the scanned point clouds. To demonstrate the approach,\nwe present the results of scanning three dissimilar objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messer_D/0/1/0/all/0/1\">Dolores Messer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wilm_J/0/1/0/all/0/1\">Jakob Wilm</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Eiriksson_E/0/1/0/all/0/1\">Eythor R. Eiriksson</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dahl_V/0/1/0/all/0/1\">Vedrana A. Dahl</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dahl_A/0/1/0/all/0/1\">Anders B. Dahl</a> (1) ((1) Technical University of Denmark, Visual Computing, Denmark, (2) University of Southern Denmark, Maersk Mc-Kinney Moller Institute, Denmark)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3-Dimensional Deep Learning with Spatial Erasing for Unsupervised Anomaly Segmentation in Brain MRI. (arXiv:2109.06540v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06540","description":"<p>Purpose. Brain Magnetic Resonance Images (MRIs) are essential for the\ndiagnosis of neurological diseases. Recently, deep learning methods for\nunsupervised anomaly detection (UAD) have been proposed for the analysis of\nbrain MRI. These methods rely on healthy brain MRIs and eliminate the\nrequirement of pixel-wise annotated data compared to supervised deep learning.\nWhile a wide range of methods for UAD have been proposed, these methods are\nmostly 2D and only learn from MRI slices, disregarding that brain lesions are\ninherently 3D and the spatial context of MRI volumes remains unexploited.\n</p>\n<p>Methods. We investigate whether using increased spatial context by using MRI\nvolumes combined with spatial erasing leads to improved unsupervised anomaly\nsegmentation performance compared to learning from slices. We evaluate and\ncompare 2D variational autoencoder (VAE) to their 3D counterpart, propose 3D\ninput erasing, and systemically study the impact of the data set size on the\nperformance.\n</p>\n<p>Results. Using two publicly available segmentation data sets for evaluation,\n3D VAE outperform their 2D counterpart, highlighting the advantage of\nvolumetric context. Also, our 3D erasing methods allow for further performance\nimprovements. Our best performing 3D VAE with input erasing leads to an average\nDICE score of 31.40% compared to 25.76% for the 2D VAE.\n</p>\n<p>Conclusions. We propose 3D deep learning methods for UAD in brain MRI\ncombined with 3D erasing and demonstrate that 3D methods clearly outperform\ntheir 2D counterpart for anomaly segmentation. Also, our spatial erasing method\nallows for further performance improvements and reduces the requirement for\nlarge data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bengs_M/0/1/0/all/0/1\">Marcel Bengs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Behrendt_F/0/1/0/all/0/1\">Finn Behrendt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kruger_J/0/1/0/all/0/1\">Julia Kr&#xfc;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Opfer_R/0/1/0/all/0/1\">Roland Opfer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1\">Alexander Schlaefer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Features Contrastive Networks for Unsupervised Domain Adaptation. (arXiv:2109.06543v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06543","description":"<p>Unsupervised domain adaptation aims to train a model from the labeled source\ndomain to make predictions on the unlabeled target domain when the data\ndistribution of the two domains is different. As a result, it needs to reduce\nthe data distribution difference between the two domains to improve the model's\ngeneralization ability. Existing methods tend to align the two domains directly\nat the domain-level, or perform class-level domain alignment based on deep\nfeature. The former ignores the relationship between the various classes in the\ntwo domains, which may cause serious negative transfer, the latter alleviates\nit by introducing pseudo-labels of the target domain, but it does not consider\nthe importance of performing class-level alignment on shallow feature\nrepresentations. In this paper, we develop this work on the method of\nclass-level alignment. The proposed method reduces the difference between two\ndomains dramaticlly by aligning multi-level features. In the case that the two\ndomains share the label space, the class-level alignment is implemented by\nintroducing Multi-Level Feature Contrastive Networks (MLFCNet). In practice,\nsince the categories of samples in target domain are unavailable, we\niteratively use clustering algorithm to obtain the pseudo-labels, and then\nminimize Multi-Level Contrastive Discrepancy (MLCD) loss to achieve more\naccurate class-level alignment. Experiments on three real-world benchmarks\nImageCLEF-DA, Office-31 and Office-Home demonstrate that MLFCNet compares\nfavorably against the existing state-of-the-art domain adaptation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Le Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jieren Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Ke Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qiaobo Da</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Input Strategies for Medulloblastoma Tumor Classification using Deep Transfer Learning. (arXiv:2109.06547v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06547","description":"<p>Medulloblastoma (MB) is a primary central nervous system tumor and the most\ncommon malignant brain cancer among children. Neuropathologists perform\nmicroscopic inspection of histopathological tissue slides under a microscope to\nassess the severity of the tumor. This is a time-consuming task and often\ninfused with observer variability. Recently, pre-trained convolutional neural\nnetworks (CNN) have shown promising results for MB subtype classification.\nTypically, high-resolution images are divided into smaller tiles for\nclassification, while the size of the tiles has not been systematically\nevaluated. We study the impact of tile size and input strategy and classify the\ntwo major histopathological subtypes-Classic and Demoplastic/Nodular. To this\nend, we use recently proposed EfficientNets and evaluate tiles with increasing\nsize combined with various downsampling scales. Our results demonstrate using\nlarge input tiles pixels followed by intermediate downsampling and patch\ncropping significantly improves MB classification performance. Our\ntop-performing method achieves the AUC-ROC value of 90.90\\% compared to 84.53\\%\nusing the previous approach with smaller input tiles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bengs_M/0/1/0/all/0/1\">Marcel Bengs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pant_S/0/1/0/all/0/1\">Satish Pant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bockmayr_M/0/1/0/all/0/1\">Michael Bockmayr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schuller_U/0/1/0/all/0/1\">Ulrich Sch&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1\">Alexander Schlaefer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Deep Unfolding Network with 3D-CNN Prior for Snapshot Compressive Imaging. (arXiv:2109.06548v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06548","description":"<p>Snapshot compressive imaging (SCI) aims to record three-dimensional signals\nvia a two-dimensional camera. For the sake of building a fast and accurate SCI\nrecovery algorithm, we incorporate the interpretability of model-based methods\nand the speed of learning-based ones and present a novel dense deep unfolding\nnetwork (DUN) with 3D-CNN prior for SCI, where each phase is unrolled from an\niteration of Half-Quadratic Splitting (HQS). To better exploit the\nspatial-temporal correlation among frames and address the problem of\ninformation loss between adjacent phases in existing DUNs, we propose to adopt\nthe 3D-CNN prior in our proximal mapping module and develop a novel dense\nfeature map (DFM) strategy, respectively. Besides, in order to promote network\nrobustness, we further propose a dense feature map adaption (DFMA) module to\nallow inter-phase information to fuse adaptively. All the parameters are\nlearned in an end-to-end fashion. Extensive experiments on simulation data and\nreal data verify the superiority of our method. The source code is available at\nhttps://github.com/jianzhangcs/SCI3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuoyuan Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mou_C/0/1/0/all/0/1\">Chong Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Attribution of Multivariate Time Series using Counterfactual Reasoning. (arXiv:2109.06562v1 [cs.LG])","link":"http://arxiv.org/abs/2109.06562","description":"<p>There are numerous methods for detecting anomalies in time series, but that\nis only the first step to understanding them. We strive to exceed this by\nexplaining those anomalies. Thus we develop a novel attribution scheme for\nmultivariate time series relying on counterfactual reasoning. We aim to answer\nthe counterfactual question of would the anomalous event have occurred if the\nsubset of the involved variables had been more similarly distributed to the\ndata outside of the anomalous interval. Specifically, we detect anomalous\nintervals using the Maximally Divergent Interval (MDI) algorithm, replace a\nsubset of variables with their in-distribution values within the detected\ninterval and observe if the interval has become less anomalous, by re-scoring\nit with MDI. We evaluate our method on multivariate temporal and\nspatio-temporal data and confirm the accuracy of our anomaly attribution of\nmultiple well-understood extreme climate events such as heatwaves and\nhurricanes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trifunov_V/0/1/0/all/0/1\">Violeta Teodora Trifunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shadaydeh_M/0/1/0/all/0/1\">Maha Shadaydeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semantic Indexing Structure for Image Retrieval. (arXiv:2109.06583v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06583","description":"<p>In large-scale image retrieval, many indexing methods have been proposed to\nnarrow down the searching scope of retrieval. The features extracted from\nimages usually are of high dimensions or unfixed sizes due to the existence of\nkey points. Most of existing index structures suffer from the dimension curse,\nthe unfixed feature size and/or the loss of semantic similarity. In this paper\na new classification-based indexing structure, called Semantic Indexing\nStructure (SIS), is proposed, in which we utilize the semantic categories\nrather than clustering centers to create database partitions, such that the\nproposed index SIS can be combined with feature extractors without the\nrestriction of dimensions. Besides, it is observed that the size of each\nsemantic partition is positively correlated with the semantic distribution of\ndatabase. Along this way, we found that when the partition number is normalized\nto five, the proposed algorithm performed very well in all the tests. Compared\nwith state-of-the-art models, SIS achieves outstanding performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingzhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zepeng Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lizhong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Fidelity GAN Inversion for Image Attribute Editing. (arXiv:2109.06590v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06590","description":"<p>We present a novel high-fidelity generative adversarial network (GAN)\ninversion framework that enables attribute editing with image-specific details\nwell-preserved (e.g., background, appearance and illumination). We first\nformulate GAN inversion as a lossy data compression problem and carefully\ndiscuss the Rate-Distortion-Edit trade-off. Due to this trade-off, previous\nworks fail to achieve high-fidelity reconstruction while keeping compelling\nediting ability with a low bit-rate latent code only. In this work, we propose\na distortion consultation approach that employs the distortion map as a\nreference for reconstruction. In the distortion consultation inversion (DCI),\nthe distortion map is first projected to a high-rate latent map, which then\ncomplements the basic low-rate latent code with (lost) details via consultation\nfusion. To achieve high-fidelity editing, we propose an adaptive distortion\nalignment (ADA) module with a self-supervised training scheme. Extensive\nexperiments in the face and car domains show a clear improvement in terms of\nboth inversion and editing quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling Network Guided Cross-Entropy Method for Unsupervised Point Cloud Registration. (arXiv:2109.06619v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06619","description":"<p>In this paper, by modeling the point cloud registration task as a Markov\ndecision process, we propose an end-to-end deep model embedded with the\ncross-entropy method (CEM) for unsupervised 3D registration. Our model consists\nof a sampling network module and a differentiable CEM module. In our sampling\nnetwork module, given a pair of point clouds, the sampling network learns a\nprior sampling distribution over the transformation space. The learned sampling\ndistribution can be used as a \"good\" initialization of the differentiable CEM\nmodule. In our differentiable CEM module, we first propose a maximum consensus\ncriterion based alignment metric as the reward function for the point cloud\nregistration task. Based on the reward function, for each state, we then\nconstruct a fused score function to evaluate the sampled transformations, where\nwe weight the current and future rewards of the transformations. Particularly,\nthe future rewards of the sampled transforms are obtained by performing the\niterative closest point (ICP) algorithm on the transformed state. By selecting\nthe top-k transformations with the highest scores, we iteratively update the\nsampling distribution. Furthermore, in order to make the CEM differentiable, we\nuse the sparsemax function to replace the hard top-$k$ selection. Finally, we\nformulate a Geman-McClure estimator based loss to train our end-to-end\nregistration model. Extensive experimental results demonstrate the good\nregistration performance of our method on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yaqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianjun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Attentive Graph Learning for Image Restoration. (arXiv:2109.06620v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06620","description":"<p>Non-local self-similarity in natural images has been verified to be an\neffective prior for image restoration. However, most existing deep non-local\nmethods assign a fixed number of neighbors for each query item, neglecting the\ndynamics of non-local correlations. Moreover, the non-local correlations are\nusually based on pixels, prone to be biased due to image degradation. To\nrectify these weaknesses, in this paper, we propose a dynamic attentive graph\nlearning model (DAGL) to explore the dynamic non-local property on patch level\nfor image restoration. Specifically, we propose an improved graph model to\nperform patch-wise graph convolution with a dynamic and adaptive number of\nneighbors for each node. In this way, image content can adaptively balance\nover-smooth and over-sharp artifacts through the number of its connected\nneighbors, and the patch-wise non-local correlations can enhance the message\npassing process. Experimental results on various image restoration tasks:\nsynthetic image denoising, real image denoising, image demosaicing, and\ncompression artifact reduction show that our DAGL can produce state-of-the-art\nresults with superior accuracy and visual quality. The source code is available\nat https://github.com/jianzhangcs/DAGL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1\">Chong Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuoyuan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Font Reconstruction with Dual Latent Manifolds. (arXiv:2109.06627v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06627","description":"<p>We propose a deep generative model that performs typography analysis and font\nreconstruction by learning disentangled manifolds of both font style and\ncharacter shape. Our approach enables us to massively scale up the number of\ncharacter types we can effectively model compared to previous methods.\nSpecifically, we infer separate latent variables representing character and\nfont via a pair of inference networks which take as input sets of glyphs that\neither all share a character type, or belong to the same font. This design\nallows our model to generalize to characters that were not observed during\ntraining time, an important task in light of the relative sparsity of most\nfonts. We also put forward a new loss, adapted from prior work that measures\nlikelihood using an adaptive distribution in a projected space, resulting in\nmore natural images without requiring a discriminator. We evaluate on the task\nof font reconstruction over various datasets representing character types of\nmany languages, and compare favorably to modern style transfer systems\naccording to both automatic and manually-evaluated metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivatsan_N/0/1/0/all/0/1\">Nikita Srivatsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Si Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-World Active Learning with Stacking Ensemble for Self-Driving Cars. (arXiv:2109.06628v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06628","description":"<p>The environments, in which autonomous cars act, are high-risky, dynamic, and\nfull of uncertainty, demanding a continuous update of their sensory information\nand knowledge bases. The frequency of facing an unknown object is too high\nmaking hard the usage of Artificial Intelligence (AI) classical classification\nmodels that usually rely on the close-world assumption. This problem of\nclassifying objects in this domain is better faced with and open-world AI\napproach. We propose an algorithm to identify not only all the known entities\nthat may appear in front of the car, but also to detect and learn the classes\nof those unknown objects that may be rare to stand on an highway (e.g., a lost\nbox from a truck). Our approach relies on the DOC algorithm from Lei Shu et.\nal. as well as on the Query-by-Committee algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vieira_P/0/1/0/all/0/1\">Paulo R. Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felix_P/0/1/0/all/0/1\">Pedro D. F&#xe9;lix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macedo_L/0/1/0/all/0/1\">Luis Macedo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigation of condominium building collapse in Surfside, Florida: a video feature tracking approach. (arXiv:2109.06629v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06629","description":"<p>On June 24, 2021, a 12-story condominium building (Champlain Towers South) in\nSurfside, Florida partially collapsed, resulting in one of the deadliest\nbuilding collapses in United States history with 98 people are confirmed dead.\nWe analyze this collapse event using a video clip that is publicly available\nfrom social media. We apply computer vision algorithms to corroborate new\ninformation from the video clip that may not be readily interpreted by human\neyes. By comparing the differential features against different video frames,\nour method can quantify the falling structural components by intuitively\nshowing the directions and magnitudes of their movements. We demonstrate the\npotential of this video processing methodology in investigations of\ncatastrophic structural failures and hope our results would serve as the basis\nfor further investigations of this and other structure collapse events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangxiong Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Convolutional Generative Modeling for Artificial Microstructure Development of Aluminum-Silicon Alloy. (arXiv:2109.06635v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06635","description":"<p>Machine learning which is a sub-domain of an Artificial Intelligence which is\nfinding various applications in manufacturing and material science sectors. In\nthe present study, Deep Generative Modeling which a type of unsupervised\nmachine learning technique has been adapted for the constructing the artificial\nmicrostructure of Aluminium-Silicon alloy. Deep Generative Adversarial Networks\nhas been used for developing the artificial microstructure of the given\nmicrostructure image dataset. The results obtained showed that the developed\nmodels had learnt to replicate the lining near the certain images of the\nmicrostructures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mishra_A/0/1/0/all/0/1\">Akshansh Mishra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pathak_T/0/1/0/all/0/1\">Tarushi Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Representation Learning for Video Advertisement Content Structuring. (arXiv:2109.06637v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06637","description":"<p>Video advertisement content structuring aims to segment a given video\nadvertisement and label each segment on various dimensions, such as\npresentation form, scene, and style. Different from real-life videos, video\nadvertisements contain sufficient and useful multi-modal content like caption\nand speech, which provides crucial video semantics and would enhance the\nstructuring process. In this paper, we propose a multi-modal encoder to learn\nmulti-modal representation from video advertisements by interacting between\nvideo-audio and text. Based on multi-modal representation, we then apply\nBoundary-Matching Network to generate temporal proposals. To make the proposals\nmore accurate, we refine generated proposals by scene-guided alignment and\nre-ranking. Finally, we incorporate proposal located embeddings into the\nintroduced multi-modal encoder to capture temporal relationships between local\nfeatures of each proposal and global features of the whole video for\nclassification. Experimental results show that our method achieves\nsignificantly improvement compared with several baselines and Rank 1 on the\ntask of Multi-modal Ads Video Understanding in ACM Multimedia 2021 Grand\nChallenge. Ablation study further shows that leveraging multi-modal content\nlike caption and speech in video advertisements significantly improve the\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhaoyang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Discrete Wavelet Pooling (LDW-Pooling) For Convolutional Networks. (arXiv:2109.06638v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06638","description":"<p>Pooling is a simple but essential layer in modern deep CNN architectures for\nfeature aggregation and extraction. Typical CNN design focuses on the conv\nlayers and activation functions, while leaving the pooling layers with fewer\noptions. We introduce the Learning Discrete Wavelet Pooling (LDW-Pooling) that\ncan be applied universally to replace standard pooling operations to better\nextract features with improved accuracy and efficiency. Motivated from the\nwavelet theory, we adopt the low-pass (L) and high-pass (H) filters\nhorizontally and vertically for pooling on a 2D feature map. Feature signals\nare decomposed into four (LL, LH, HL, HH) subbands to retain features better\nand avoid information dropping. The wavelet transform ensures features after\npooling can be fully preserved and recovered. We next adopt an energy-based\nattention learning to fine-select crucial and representative features.\nLDW-Pooling is effective and efficient when compared with other\nstate-of-the-art pooling techniques such as WaveletPooling and LiftPooling.\nExtensive experimental validation shows that LDW-Pooling can be applied to a\nwide range of standard CNN architectures and consistently outperform standard\n(max, mean, mixed, and stochastic) pooling operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping-Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bor-Shiun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lipeng Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CropDefender: deep watermark which is more convenient to train and more robust against cropping. (arXiv:2109.06651v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06651","description":"<p>Digital image watermarking, which is a technique for invisibly embedding\ninformation into an image, is used in fields such as property rights\nprotection. In recent years, some research has proposed the use of neural\nnetworks to add watermarks to natural images. We take StegaStamp as an example\nfor our research. Whether facing traditional image editing methods, such as\nbrightness, contrast, saturation adjustment, or style change like 1-bit\nconversion, GAN, StegaStamp has robustness far beyond traditional watermarking\ntechniques, but it still has two drawbacks: it is vulnerable to cropping and is\nhard to train. We found that the causes of vulnerability to cropping is not the\nloss of information on the edge, but the movement of watermark position. By\nexplicitly introducing the perturbation of cropping into the training, the\ncropping resistance is significantly improved. For the problem of difficult\ntraining, we introduce instance normalization to solve the vanishing gradient,\nset losses' weights as learnable parameters to reduce the number of\nhyperparameters, and use sigmoid to restrict pixel values of the generated\nimage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiayu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuchen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Changhao Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation by Maximizing Population Correlation with Neural Architecture Search. (arXiv:2109.06652v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06652","description":"<p>In Domain Adaptation (DA), where the feature distributions of the source and\ntarget domains are different, various distance-based methods have been proposed\nto minimize the discrepancy between the source and target domains to handle the\ndomain shift. In this paper, we propose a new similarity function, which is\ncalled Population Correlation (PC), to measure the domain discrepancy for DA.\nBase on the PC function, we propose a new method called Domain Adaptation by\nMaximizing Population Correlation (DAMPC) to learn a domain-invariant feature\nrepresentation for DA. Moreover, most existing DA methods use hand-crafted\nbottleneck networks, which may limit the capacity and flexibility of the\ncorresponding model. Therefore, we further propose a method called DAMPC with\nNeural Architecture Search (DAMPC-NAS) to search the optimal network\narchitecture for DAMPC. Experiments on several benchmark datasets, including\nOffice-31, Office-Home, and VisDA-2017, show that the proposed DAMPC-NAS method\nachieves better results than state-of-the-art DA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhixiong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying partial mouse brain microscopy images from Allen reference atlas using a contrastively learned semantic space. (arXiv:2109.06662v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06662","description":"<p>Precise identification of mouse brain microscopy images is a crucial first\nstep when anatomical structures in the mouse brain are to be registered to a\nreference atlas. Practitioners usually rely on manual comparison of images or\ntools that assume the presence of complete images. This work explores Siamese\nNetworks as the method for finding corresponding 2D reference atlas plates for\ngiven partial 2D mouse brain images. Siamese networks are a class of\nconvolutional neural networks (CNNs) that use weight-shared paths to obtain low\ndimensional embeddings of pairs of input images. The correspondence between the\npartial mouse brain image and reference atlas plate is determined based on the\ndistance between low dimensional embeddings of brain slices and atlas plates\nthat are obtained from Siamese networks using contrastive learning. Experiments\nshowed that Siamese CNNs can precisely identify brain slices using the Allen\nmouse brain atlas when training and testing images come from the same source.\nThey achieved TOP-1 and TOP-5 accuracy of 25% and 100%, respectively, taking\nonly 7.2 seconds to identify 29 images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antanavicius_J/0/1/0/all/0/1\">Justinas Antanavicius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_R/0/1/0/all/0/1\">Roberto Leiras Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvan_R/0/1/0/all/0/1\">Raghavendra Selvan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Insect-Inspired Randomly, Weighted Neural Network with Random Fourier Features For Neuro-Symbolic Relational Learning. (arXiv:2109.06663v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06663","description":"<p>Insects, such as fruit flies and honey bees, can solve simple associative\nlearning tasks and learn abstract concepts such as \"sameness\" and \"difference\",\nwhich is viewed as a higher-order cognitive function and typically thought to\ndepend on top-down neocortical processing. Empirical research with fruit flies\nstrongly supports that a randomized representational architecture is used in\nolfactory processing in insect brains. Based on these results, we propose a\nRandomly Weighted Feature Network (RWFN) that incorporates randomly drawn,\nuntrained weights in an encoder that uses an adapted linear model as a decoder.\nThe randomized projections between input neurons and higher-order processing\ncenters in the input brain is mimicked in RWFN by a single-hidden-layer neural\nnetwork that specially structures latent representations in the hidden layer\nusing random Fourier features that better represent complex relationships\nbetween inputs using kernel approximation. Because of this special\nrepresentation, RWFNs can effectively learn the degree of relationship among\ninputs by training only a linear decoder model. We compare the performance of\nRWFNs to LTNs for Semantic Image Interpretation (SII) tasks that have been used\nas a representative example of how LTNs utilize reasoning over first-order\nlogic to surpass the performance of solely data-driven methods. We demonstrate\nthat compared to LTNs, RWFNs can achieve better or similar performance for both\nobject classification and detection of the part-of relations between objects in\nSII tasks while using much far fewer learnable parameters (1:62 ratio) and a\nfaster learning process (1:2 ratio of running speed). Furthermore, we show that\nbecause the randomized weights do not depend on the data, several decoders can\nshare a single randomized encoder, giving RWFNs a unique economy of spatial\nscale for simultaneous classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jinyung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Resolution Image Harmonization via Collaborative Dual Transformations. (arXiv:2109.06671v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06671","description":"<p>Given a composite image, image harmonization aims to adjust the foreground to\nmake it compatible with the background. High-resolution image harmonization is\nin high demand, but still remains unexplored. Conventional image harmonization\nmethods learn global RGB-to-RGB transformation which could effortlessly scale\nto high resolution, but ignore diverse local context. Recent deep learning\nmethods learn the dense pixel-to-pixel transformation which could generate\nharmonious outputs, but are highly constrained in low resolution. In this work,\nwe propose a high-resolution image harmonization network with Collaborative\nDual Transformation (CDTNet) to combine pixel-to-pixel transformation and\nRGB-to-RGB transformation coherently in an end-to-end framework. Our CDTNet\nconsists of a low-resolution generator for pixel-to-pixel transformation, a\ncolor mapping module for RGB-to-RGB transformation, and a refinement module to\ntake advantage of both. Extensive experiments on high-resolution image\nharmonization dataset demonstrate that our CDTNet strikes a good balance\nbetween efficiency and effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1\">Wenyan Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xinhao Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xuesong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qihao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Luminance Attentive Networks for HDR Image and Panorama Reconstruction. (arXiv:2109.06688v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06688","description":"<p>It is very challenging to reconstruct a high dynamic range (HDR) from a low\ndynamic range (LDR) image as an ill-posed problem. This paper proposes a\nluminance attentive network named LANet for HDR reconstruction from a single\nLDR image. Our method is based on two fundamental observations: (1) HDR images\nstored in relative luminance are scale-invariant, which means the HDR images\nwill hold the same information when multiplied by any positive real number.\nBased on this observation, we propose a novel normalization method called \" HDR\ncalibration \" for HDR images stored in relative luminance, calibrating HDR\nimages into a similar luminance scale according to the LDR images. (2) The main\ndifference between HDR images and LDR images is in under-/over-exposed areas,\nespecially those highlighted. Following this observation, we propose a\nluminance attention module with a two-stream structure for LANet to pay more\nattention to the under-/over-exposed areas. In addition, we propose an extended\nnetwork called panoLANet for HDR panorama reconstruction from an LDR panorama\nand build a dualnet structure for panoLANet to solve the distortion problem\ncaused by the equirectangular panorama. Extensive experiments show that our\nproposed approach LANet can reconstruct visually convincing HDR images and\ndemonstrate its superiority over state-of-the-art approaches in terms of all\nmetrics in inverse tone mapping. The image-based lighting application with our\nproposed panoLANet also demonstrates that our method can simulate natural scene\nlighting using only LDR panorama. Our source code is available at\nhttps://github.com/LWT3437/LANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hanning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LRWR: Large-Scale Benchmark for Lip Reading in Russian language. (arXiv:2109.06692v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06692","description":"<p>Lipreading, also known as visual speech recognition, aims to identify the\nspeech content from videos by analyzing the visual deformations of lips and\nnearby areas. One of the significant obstacles for research in this field is\nthe lack of proper datasets for a wide variety of languages: so far, these\nmethods have been focused only on English or Chinese. In this paper, we\nintroduce a naturally distributed large-scale benchmark for lipreading in\nRussian language, named LRWR, which contains 235 classes and 135 speakers. We\nprovide a detailed description of the dataset collection pipeline and dataset\nstatistics. We also present a comprehensive comparison of the current popular\nlipreading methods on LRWR and conduct a detailed analysis of their\nperformance. The results demonstrate the differences between the benchmarked\nlanguages and provide several promising directions for lipreading models\nfinetuning. Thanks to our findings, we also achieved new state-of-the-art\nresults on the LRW benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Egorov_E/0/1/0/all/0/1\">Evgeniy Egorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostyumov_V/0/1/0/all/0/1\">Vasily Kostyumov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konyk_M/0/1/0/all/0/1\">Mikhail Konyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_S/0/1/0/all/0/1\">Sergey Kolesnikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImUnity: a generalizable VAE-GAN solution for multicenter MR image harmonization. (arXiv:2109.06756v1 [eess.IV])","link":"http://arxiv.org/abs/2109.06756","description":"<p>ImUnity is an original deep-learning model designed for efficient and\nflexible MR image harmonization. A VAE-GAN network, coupled with a confusion\nmodule and an optional biological preservation module, uses multiple 2D-slices\ntaken from different anatomical locations in each subject of the training\ndatabase, as well as image contrast transformations for its self-supervised\ntraining. It eventually generates 'corrected' MR images that can be used for\nvarious multi-center population studies. Using 3 open source databases (ABIDE,\nOASIS and SRPBS), which contain MR images from multiple acquisition scanner\ntypes or vendors and a large range of subjects ages, we show that ImUnity: (1)\noutperforms state-of-the-art methods in terms of quality of images generated\nusing traveling subjects; (2) removes sites or scanner biases while improving\npatients classification; (3) harmonizes data coming from new sites or scanners\nwithout the need for an additional fine-tuning and (4) allows the selection of\nmultiple MR reconstructed images according to the desired applications. Tested\nhere on T1-weighted images, ImUnity could be used to harmonize other types of\nmedical images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cackowski_S/0/1/0/all/0/1\">Stenzel Cackowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barbier_E/0/1/0/all/0/1\">Emmanuel L. Barbier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dojat_M/0/1/0/all/0/1\">Michel Dojat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christen_T/0/1/0/all/0/1\">Thomas Christen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionHint: Self-Supervised Monocular Visual Odometrywith Motion Constraints. (arXiv:2109.06768v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06768","description":"<p>We present a novel self-supervised algorithmnamedMotionHintfor monocular\nvisual odometry (VO) that takes motion constraints into account. A key aspect\nof ourapproach is to use an appropriate motion model that can help existing\nself-supervised monocular VO (SSM-VO) algorithms to overcome issues related to\nthe local minima within their self-supervised loss functions. The motion model\nis expressed with a neural network named PPnet. It is trained to coarsely\npredict the next pose of the camera and the uncertainty of this prediction. Our\nself-supervised approach combines the original loss and the motion loss, which\nis the weighted difference between the prediction and the generated ego-motion.\nTaking two existing SSM-VO systems as our baseline, we evaluate our MotionHint\nalgorithm on the standard KITTI and EuRoC benchmark. Experimental results show\nthat our MotionHint algorithm can be easily applied to existing open-source\nstate-of-the-art SSM-VO systems to greatly improve the performance on KITTI\ndataset by reducing the resulting ATE by up to 28.73%. For EuRoc dataset, our\nmethod can extract the motion model.But due to the poor performance of the\nbaseline methods, MotionHint cannot significantly improve their results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Approach for Masking Fetal Gender in Ultrasound Images. (arXiv:2109.06790v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06790","description":"<p>Ultrasound (US) imaging is highly effective with regards to both cost and\nversatility in real-time diagnosis; however, determination of fetal gender by\nUS scan in the early stages of pregnancy is also a cause of sex-selective\nabortion. This work proposes a deep learning object detection approach to\naccurately mask fetal gender in US images in order to increase the\naccessibility of the technology. We demonstrate how the YOLOv5L architecture\nexhibits superior performance relative to other object detection models on this\ntask. Our model achieves 45.8% AP[0.5:0.95], 92% F1-score and 0.006 False\nPositive Per Image rate on our test set. Furthermore, we introduce a bounding\nbox delay rule based on frame-to-frame structural similarity to reduce the\nfalse negative rate by 85%, further improving masking reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borundiya_A/0/1/0/all/0/1\">Amit Borundiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navruzyan_A/0/1/0/all/0/1\">Arshak Navruzyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igoschev_D/0/1/0/all/0/1\">Dennis Igoschev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oughali_F/0/1/0/all/0/1\">Feras C. Oughali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupuleti_H/0/1/0/all/0/1\">Hemanth Pasupuleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuller_M/0/1/0/all/0/1\">Mike Fuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanigicherla_V/0/1/0/all/0/1\">Vinay Kanigicherla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_T/0/1/0/all/0/1\">T S Aniruddha Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaurasia_R/0/1/0/all/0/1\">Rishabh Chaurasia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sonali Vinod Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic hippocampal surface generation via 3D U-net and active shape modeling with hybrid particle swarm optimization. (arXiv:2109.06817v1 [cs.NE])","link":"http://arxiv.org/abs/2109.06817","description":"<p>In this paper, we proposed and validated a fully automatic pipeline for\nhippocampal surface generation via 3D U-net coupled with active shape modeling\n(ASM). Principally, the proposed pipeline consisted of three steps. In the\nbeginning, for each magnetic resonance image, a 3D U-net was employed to obtain\nthe automatic hippocampus segmentation at each hemisphere. Secondly, ASM was\nperformed on a group of pre-obtained template surfaces to generate mean shape\nand shape variation parameters through principal component analysis.\nUltimately, hybrid particle swarm optimization was utilized to search for the\noptimal shape variation parameters that best match the segmentation. The\nhippocampal surface was then generated from the mean shape and the shape\nvariation parameters. The proposed pipeline was observed to provide hippocampal\nsurfaces at both hemispheres with high accuracy, correct anatomical topology,\nand sufficient smoothness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1\">Pinyuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Class Meta-Learning: Towards Generalizable Few-Shot Open-Set Classification. (arXiv:2109.06859v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06859","description":"<p>Real-world classification tasks are frequently required to work in an\nopen-set setting. This is especially challenging for few-shot learning problems\ndue to the small sample size for each known category, which prevents existing\nopen-set methods from working effectively; however, most multiclass few-shot\nmethods are limited to closed-set scenarios. In this work, we address the\nproblem of few-shot open-set classification by first proposing methods for\nfew-shot one-class classification and then extending them to few-shot\nmulticlass open-set classification. We introduce two independent few-shot\none-class classification methods: Meta Binary Cross-Entropy (Meta-BCE), which\nlearns a separate feature representation for one-class classification, and\nOne-Class Meta-Learning (OCML), which learns to generate one-class classifiers\ngiven standard multiclass feature representation. Both methods can augment any\nexisting few-shot learning method without requiring retraining to work in a\nfew-shot multiclass open-set setting without degrading its closed-set\nperformance. We demonstrate the benefits and drawbacks of both methods in\ndifferent problem settings and evaluate them on three standard benchmark\ndatasets, miniImageNet, tieredImageNet, and Caltech-UCSD-Birds-200-2011, where\nthey surpass the state-of-the-art methods in the few-shot multiclass open-set\nand few-shot one-class tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kozerawski_J/0/1/0/all/0/1\">Jedrzej Kozerawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turk_M/0/1/0/all/0/1\">Matthew Turk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning. (arXiv:2109.06860v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06860","description":"<p>Commonsense is defined as the knowledge that is shared by everyone. However,\ncertain types of commonsense knowledge are correlated with culture and\ngeographic locations and they are only shared locally. For example, the\nscenarios of wedding ceremonies vary across regions due to different customs\ninfluenced by historical and religious factors. Such regional characteristics,\nhowever, are generally omitted in prior work. In this paper, we construct a\nGeo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test\nvision-and-language models' ability to understand cultural and\ngeo-location-specific commonsense. In particular, we study two state-of-the-art\nVision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard\nmultimodal commonsense benchmark with images primarily from Western regions. We\nthen evaluate how well the trained models can generalize to answering the\nquestions in GD-VCR. We find that the performance of both models for\nnon-Western regions including East Asia, South Asia, and Africa is\nsignificantly lower than that for Western region. We analyze the reasons behind\nthe performance disparity and find that the performance gap is larger on QA\npairs that: 1) are concerned with culture-related scenarios, e.g., weddings,\nreligious activities, and festivals; 2) require high-level geo-diverse\ncommonsense reasoning rather than low-order perception and recognition. Dataset\nand code are released at https://github.com/WadeYin9712/GD-VCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing License Plates in Real-Time. (arXiv:1906.04376v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1906.04376","description":"<p>License plate detection and recognition (LPDR) is of growing importance for\nenabling intelligent transportation and ensuring the security and safety of the\ncities. However, LPDR faces a big challenge in a practical environment. The\nlicense plates can have extremely diverse sizes, fonts and colors, and the\nplate images are usually of poor quality caused by skewed capturing angles,\nuneven lighting, occlusion, and blurring. In applications such as surveillance,\nit often requires fast processing. To enable real-time and accurate license\nplate recognition, in this work, we propose a set of techniques: 1) a contour\nreconstruction method along with edge-detection to quickly detect the candidate\nplates; 2) a simple zero-one-alternation scheme to effectively remove the fake\ntop and bottom borders around plates to facilitate more accurate segmentation\nof characters on plates; 3) a set of techniques to augment the training data,\nincorporate SIFT features into the CNN network, and exploit transfer learning\nto obtain the initial parameters for more effective training; and 4) a\ntwo-phase verification procedure to determine the correct plate at low cost, a\nstatistical filtering in the plate detection stage to quickly remove unwanted\ncandidates, and the accurate CR results after the CR process to perform further\nplate verification without additional processing. We implement a complete LPDR\nsystem based on our algorithms. The experimental results demonstrate that our\nsystem can accurately recognize license plate in real-time. Additionally, it\nworks robustly under various levels of illumination and noise, and in the\npresence of car movement. Compared to peer schemes, our system is not only\namong the most accurate ones but is also the fastest, and can be easily applied\nto other scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Graph Transformer Self-Attention Networks. (arXiv:1909.11855v11 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1909.11855","description":"<p>The transformer has been extensively used in research domains such as\ncomputer vision, image processing, and natural language processing. The\ntransformer, however, has not been actively used in graph neural networks. To\nthis end, we introduce a transformer-based advanced GNN model, named UGformer,\nto learn graph representations. In particular, given an input graph, we present\ntwo UGformer variants. The first variant is to leverage the transformer on a\nset of sampled neighbors for each node, while the second is to leverage the\ntransformer directly on the input graph. Experimental results demonstrate that\nour UGformer achieves state-of-the-art accuracies on well-known benchmark\ndatasets for graph classification and inductive text classification. The code\nis available on Github:\n\\url{https://github.com/daiquocnguyen/Graph-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Binary-Ternary Quantization. (arXiv:1909.12205v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1909.12205","description":"<p>Neural network models are resource hungry. It is difficult to deploy such\ndeep networks on devices with limited resources, like smart wearables,\ncellphones, drones, and autonomous vehicles. Low bit quantization such as\nbinary and ternary quantization is a common approach to alleviate this resource\nrequirements. Ternary quantization provides a more flexible model and\noutperforms binary quantization in terms of accuracy, however doubles the\nmemory footprint and increases the computational cost. Contrary to these\napproaches, mixed quantized models allow a trade-off between accuracy and\nmemory footprint. In such models, quantization depth is often chosen manually,\nor is tuned using a separate optimization routine. The latter requires training\na quantized network multiple times. Here, we propose an adaptive combination of\nbinary and ternary quantization, namely Smart Quantization (SQ), in which the\nquantization depth is modified directly via a regularization function, so that\nthe model is trained only once. Our experimental results show that the proposed\nmethod adapts quantization depth successfully while keeping the model accuracy\nhigh on MNIST and CIFAR10 benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razani_R/0/1/0/all/0/1\">Ryan Razani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_G/0/1/0/all/0/1\">Gr&#xe9;goire Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_E/0/1/0/all/0/1\">Eyy&#xfc;b Sari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization. (arXiv:2008.08170v4 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2008.08170","description":"<p>In the paper, we propose a class of accelerated zeroth-order and first-order\nmomentum methods for both nonconvex mini-optimization and minimax-optimization.\nSpecifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM)\nmethod to solve stochastic mini-optimization problems. We prove that the\nAcc-ZOM method achieves a lower query complexity of\n$\\tilde{O}(d^{3/4}\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point,\nwhich improves the best known result by a factor of $O(d^{1/4})$ where $d$\ndenotes the parameter dimension. In particular, the Acc-ZOM does not need large\nbatches that are required in the existing zeroth-order stochastic algorithms.\nAt the same time, we propose an accelerated zeroth-order momentum descent\nascent (Acc-ZOMDA) method for black-box minimax-optimization. We prove that the\nAcc-ZOMDA method reaches the best known query complexity of\n$\\tilde{O}((d_1+d_2)\\kappa_y^{3}\\epsilon^{-3})$ without large batches for\nfinding an $\\epsilon$-stationary point, where $d_1$ and $d_2$ denote dimensions\nof optimization parameters and $\\kappa_y$ is condition number. Moreover, we\npropose an accelerated first-order momentum descent ascent (Acc-MDA) method for\nsolving white-box minimax problems, and prove that it achieves a lower gradient\ncomplexity of $\\tilde{O}(\\kappa_y^{2.5}\\epsilon^{-3})$ given batch size\n$b=\\kappa_y^{4}$ for finding an $\\epsilon$-stationary point, which improves the\nbest known result by a factor of $O(\\kappa_y^{1/2})$. Extensive experimental\nresults on the black-box adversarial attack to deep neural networks (DNNs) and\npoisoning attack demonstrate the efficiency of our algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gao_S/0/1/0/all/0/1\">Shangqian Gao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual News: Benchmark and Challenges in News Image Captioning. (arXiv:2010.03743v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.03743","description":"<p>We propose Visual News Captioner, an entity-aware model for the task of news\nimage captioning. We also introduce Visual News, a large-scale benchmark\nconsisting of more than one million news images along with associated news\narticles, image captions, author information, and other metadata. Unlike the\nstandard image captioning task, news images depict situations where people,\nlocations, and events are of paramount importance. Our proposed method can\neffectively combine visual and textual features to generate captions with\nricher information such as events and entities. More specifically, built upon\nthe Transformer architecture, our model is further equipped with novel\nmulti-modal feature fusion techniques and attention mechanisms, which are\ndesigned to generate named entities more accurately. Our method utilizes much\nfewer parameters while achieving slightly better prediction results than\ncompeting methods. Our larger and more diverse Visual News dataset further\nhighlights the remaining challenges in captioning news images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fuxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IMU-Assisted Learning of Single-View Rolling Shutter Correction. (arXiv:2011.03106v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.03106","description":"<p>Rolling shutter distortion is highly undesirable for photography and computer\nvision algorithms (e.g., visual SLAM) because pixels can be potentially\ncaptured at different times and poses. In this paper, we propose a deep neural\nnetwork to predict depth and row-wise pose from a single image for rolling\nshutter correction. Our contribution in this work is to incorporate inertial\nmeasurement unit (IMU) data into the pose refinement process, which, compared\nto the state-of-the-art, greatly enhances the pose prediction. The improved\naccuracy and robustness make it possible for numerous vision algorithms to use\nimagery captured by rolling shutter cameras and produce highly accurate\nresults. We also extend a dataset to have real rolling shutter images, IMU\ndata, depth maps, camera poses, and corresponding global shutter images for\nrolling shutter correction training. We demonstrate the efficacy of the\nproposed method by evaluating the performance of Direct Sparse Odometry (DSO)\nalgorithm on rolling shutter imagery corrected using the proposed approach.\nResults show marked improvements of the DSO algorithm over using uncorrected\nimagery, validating the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_J/0/1/0/all/0/1\">Jiawei Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Jahidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattar_J/0/1/0/all/0/1\">Junaed Sattar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-covariant and scale-invariant Gaussian derivative networks. (arXiv:2011.14759v9 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14759","description":"<p>This paper presents a hybrid approach between scale-space theory and deep\nlearning, where a deep learning architecture is constructed by coupling\nparameterized scale-space operations in cascade. By sharing the learnt\nparameters between multiple scale channels, and by using the transformation\nproperties of the scale-space primitives under scaling transformations, the\nresulting network becomes provably scale covariant. By in addition performing\nmax pooling over the multiple scale channels, a resulting network architecture\nfor image classification also becomes provably scale invariant. We investigate\nthe performance of such networks on the MNISTLargeScale dataset, which contains\nrescaled images from original MNIST over a factor of 4 concerning training data\nand over a factor of 16 concerning testing data. It is demonstrated that the\nresulting approach allows for scale generalization, enabling good performance\nfor classifying patterns at scales not present in the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1\">Tony Lindeberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image. (arXiv:2012.09855v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.09855","description":"<p>We introduce the problem of perpetual view generation - long-range generation\nof novel views corresponding to an arbitrarily long camera trajectory given a\nsingle image. This is a challenging problem that goes far beyond the\ncapabilities of current view synthesis methods, which quickly degenerate when\npresented with large camera motions. Methods for video generation also have\nlimited ability to produce long sequences and are often agnostic to scene\ngeometry. We take a hybrid approach that integrates both geometry and image\nsynthesis in an iterative `\\emph{render}, \\emph{refine} and \\emph{repeat}'\nframework, allowing for long-range generation that cover large distances after\nhundreds of frames. Our approach can be trained from a set of monocular video\nsequences. We propose a dataset of aerial footage of coastal scenes, and\ncompare our method with recent view synthesis and conditional video generation\nbaselines, showing that it can generate plausible scenes for much longer time\nhorizons over large camera trajectories compared to existing methods. Project\npage at https://infinite-nature.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andrew Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1\">Richard Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1\">Ameesh Makadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-line-of-Sight Imaging via Neural Transient Fields. (arXiv:2101.00373v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.00373","description":"<p>We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging.\nPrevious solutions have sought to explicitly recover the 3D geometry (e.g., as\npoint clouds) or voxel density (e.g., within a pre-defined volume) of the\nhidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF)\napproach, we use a multi-layer perceptron (MLP) to represent the neural\ntransient field or NeTF. However, NeTF measures the transient over spherical\nwavefronts rather than the radiance along lines. We therefore formulate a\nspherical volume NeTF reconstruction pipeline, applicable to both confocal and\nnon-confocal setups. Compared with NeRF, NeTF samples a much sparser set of\nviewpoints (scanning spots) and the sampling is highly uneven. We thus\nintroduce a Monte Carlo technique to improve the robustness in the\nreconstruction. Comprehensive experiments on synthetic and real datasets\ndemonstrate NeTF provides higher quality reconstruction and preserves fine\ndetails largely missing in the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Siyuan Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Z/0/1/0/all/0/1\">Zhengqing Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ruiqian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_T/0/1/0/all/0/1\">Tian Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study and Analysis on Open-Set Semi-Supervised Learning. (arXiv:2101.08237v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.08237","description":"<p>Pseudo-labeling (PL) and Data Augmentation-based Consistency Training (DACT)\nare two approaches widely used in Semi-Supervised Learning (SSL) methods. These\nmethods exhibit great power in many machine learning tasks by utilizing\nunlabeled data for efficient training. But in a more realistic setting (termed\nas open-set SSL), where unlabeled dataset contains out-of-distribution (OOD)\nsamples, the traditional SSL methods suffer severe performance degradation.\nRecent approaches mitigate the negative influence of OOD samples by filtering\nthem out from the unlabeled data. However, it is not clear whether directly\nremoving the OOD samples is the best choice. Furthermore, why PL and DACT could\nperform differently in open-set SSL remains a mystery. In this paper, we\nthoroughly analyze various SSL methods (PL and DACT) on open-set SSL and\ndiscuss pros and cons of these two approaches separately. Based on our\nanalysis, we propose Style Disturbance to improve traditional SSL methods on\nopen-set SSL and experimentally show our approach can achieve state-of-the-art\nresults on various datasets by utilizing OOD samples properly. We believe our\nstudy can bring new insights for SSL research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huixiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanxu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transductive Zero-Shot Learning by Decoupled Feature Generation. (arXiv:2102.03266v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03266","description":"<p>In this paper, we address zero-shot learning (ZSL), the problem of\nrecognizing categories for which no labeled visual data are available during\ntraining. We focus on the transductive setting, in which unlabelled visual data\nfrom unseen classes is available. State-of-the-art paradigms in ZSL typically\nexploit generative adversarial networks to synthesize visual features from\nsemantic attributes. We posit that the main limitation of these approaches is\nto adopt a single model to face two problems: 1) generating realistic visual\nfeatures, and 2) translating semantic attributes into visual cues. Differently,\nwe propose to decouple such tasks, solving them separately. In particular, we\ntrain an unconditional generator to solely capture the complexity of the\ndistribution of visual data and we subsequently pair it with a conditional\ngenerator devoted to enrich the prior knowledge of the data distribution with\nthe semantic content of the class embeddings. We present a detailed ablation\nstudy to dissect the effect of our proposed decoupling approach, while\ndemonstrating its superiority over the related state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marmoreo_F/0/1/0/all/0/1\">Federico Marmoreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavazza_J/0/1/0/all/0/1\">Jacopo Cavazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murino_V/0/1/0/all/0/1\">Vittorio Murino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-set Distances for Learning Representations of 3D Point Clouds. (arXiv:2102.04014v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.04014","description":"<p>Learning an effective representation of 3D point clouds requires a good\nmetric to measure the discrepancy between two 3D point sets, which is\nnon-trivial due to their irregularity. Most of the previous works resort to\nusing the Chamfer discrepancy or Earth Mover's distance, but those metrics are\neither ineffective in measuring the differences between point clouds or\ncomputationally expensive. In this paper, we conduct a systematic study with\nextensive experiments on distance metrics for 3D point clouds. From this study,\nwe propose to use sliced Wasserstein distance and its variants for learning\nrepresentations of 3D point clouds. In addition, we introduce a new algorithm\nto estimate sliced Wasserstein distance that guarantees that the estimated\nvalue is close enough to the true one. Experiments show that the sliced\nWasserstein distance and its variants allow the neural network to learn a more\nefficient representation compared to the Chamfer discrepancy. We demonstrate\nthe efficiency of the sliced Wasserstein metric and its variants on several\ntasks in 3D computer vision including training a point cloud autoencoder,\ngenerative modeling, transfer learning, and point cloud registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Trung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quang-Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tam Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Tung Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep-Learning Approach For Direct Whole-Heart Mesh Reconstruction. (arXiv:2102.07899v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.07899","description":"<p>Automated construction of surface geometries of cardiac structures from\nvolumetric medical images is important for a number of clinical applications.\nWhile deep-learning-based approaches have demonstrated promising reconstruction\nprecision, these approaches have mostly focused on voxel-wise segmentation\nfollowed by surface reconstruction and post-processing techniques. However,\nsuch approaches suffer from a number of limitations including disconnected\nregions or incorrect surface topology due to erroneous segmentation and\nstair-case artifacts due to limited segmentation resolution. We propose a novel\ndeep-learning-based approach that directly predicts whole heart surface meshes\nfrom volumetric CT and MR image data. Our approach leverages a graph\nconvolutional neural network to predict deformation on mesh vertices from a\npre-defined mesh template to reconstruct multiple anatomical structures in a 3D\nimage volume. Our method demonstrated promising performance of generating whole\nheart reconstructions with as good or better accuracy than prior\ndeep-learning-based methods on both CT and MR data. Furthermore, by deforming a\ntemplate mesh, our method can generate whole heart geometries with better\nanatomical consistency and produce high-resolution geometries from lower\nresolution input image data. Our method was also able to produce temporally\nconsistent surface mesh predictions for heart motion from CT or MR cine\nsequences, and therefore can potentially be applied for efficiently\nconstructing 4D whole heart dynamics. Our code and pre-trained networks are\navailable at https://github.com/fkong7/MeshDeformNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kong_F/0/1/0/all/0/1\">Fanwei Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_N/0/1/0/all/0/1\">Nathan Wilson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shadden_S/0/1/0/all/0/1\">Shawn C. Shadden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Scene Flow from Point Clouds in the Real World. (arXiv:2103.01306v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01306","description":"<p>Autonomous vehicles operate in highly dynamic environments necessitating an\naccurate assessment of which aspects of a scene are moving and where they are\nmoving to. A popular approach to 3D motion estimation, termed scene flow, is to\nemploy 3D point cloud data from consecutive LiDAR scans, although such\napproaches have been limited by the small size of real-world, annotated LiDAR\ndata. In this work, we introduce a new large-scale dataset for scene flow\nestimation derived from corresponding tracked 3D objects, which is\n$\\sim$1,000$\\times$ larger than previous real-world datasets in terms of the\nnumber of annotated frames. We demonstrate how previous works were bounded\nbased on the amount of real LiDAR data available, suggesting that larger\ndatasets are required to achieve state-of-the-art predictive performance.\nFurthermore, we show how previous heuristics for operating on point clouds such\nas down-sampling heavily degrade performance, motivating a new class of models\nthat are tractable on the full point cloud. To address this issue, we introduce\nthe FastFlow3D architecture which provides real time inference on the full\npoint cloud. Additionally, we design human-interpretable metrics that better\ncapture real world aspects by accounting for ego-motion and providing\nbreakdowns per object type. We hope that this dataset may provide new\nopportunities for developing real world scene flow systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jund_P/0/1/0/all/0/1\">Philipp Jund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweeney_C/0/1/0/all/0/1\">Chris Sweeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdo_N/0/1/0/all/0/1\">Nichola Abdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Recent Advances in Deep Learning for Audio-Visual Emotion Recognition. (arXiv:2103.09154v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09154","description":"<p>Emotional expressions are the behaviors that communicate our emotional state\nor attitude to others. They are expressed through verbal and non-verbal\ncommunication. Complex human behavior can be understood by studying physical\nfeatures from multiple modalities; mainly facial, vocal and physical gestures.\nRecently, spontaneous multi-modal emotion recognition has been extensively\nstudied for human behavior analysis. In this paper, we propose a new deep\nlearning-based approach for audio-visual emotion recognition. Our approach\nleverages recent advances in deep learning like knowledge distillation and\nhigh-performing deep architectures. The deep feature representations of the\naudio and visual modalities are fused based on a model-level fusion strategy. A\nrecurrent neural network is then used to capture the temporal dynamics. Our\nproposed approach substantially outperforms state-of-the-art approaches in\npredicting valence on the RECOLA dataset. Moreover, our proposed visual facial\nexpression feature extraction network outperforms state-of-the-art results on\nthe AffectNet and Google Facial Expression Comparison datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schoneveld_L/0/1/0/all/0/1\">Liam Schoneveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Othmani_A/0/1/0/all/0/1\">Alice Othmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelkawy_H/0/1/0/all/0/1\">Hazem Abdelkawy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning without Seeing nor Knowing: Towards Open Zero-Shot Learning. (arXiv:2103.12437v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12437","description":"<p>In Generalized Zero-Shot Learning (GZSL), unseen categories (for which no\nvisual data are available at training time) can be predicted by leveraging\ntheir class embeddings (e.g., a list of attributes describing them) together\nwith a complementary pool of seen classes (paired with both visual data and\nclass embeddings). Despite GZSL is arguably challenging, we posit that knowing\nin advance the class embeddings, especially for unseen categories, is an actual\nlimit of the applicability of GZSL towards real-world scenarios. To relax this\nassumption, we propose Open Zero-Shot Learning (OZSL) to extend GZSL towards\nthe open-world settings. We formalize OZSL as the problem of recognizing seen\nand unseen classes (as in GZSL) while also rejecting instances from unknown\ncategories, for which neither visual data nor class embeddings are provided. We\nformalize the OZSL problem introducing evaluation protocols, error metrics and\nbenchmark datasets. We also suggest to tackle the OZSL problem by proposing the\nidea of performing unknown feature generation (instead of only unseen features\ngeneration as done in GZSL). We achieve this by optimizing a generative process\nto sample unknown class embeddings as complementary to the seen and the unseen.\nWe intend these results to be the ground to foster future research, extending\nthe standard closed-world zero-shot learning (GZSL) with the novel open-world\ncounterpart (OZSL).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marmoreo_F/0/1/0/all/0/1\">Federico Marmoreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrazco_J/0/1/0/all/0/1\">Julio Ivan Davila Carrazco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murino_V/0/1/0/all/0/1\">Vittorio Murino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavazza_J/0/1/0/all/0/1\">Jacopo Cavazza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D2C-SR: A Divergence to Convergence Approach for Real-World Image Super-Resolution. (arXiv:2103.14373v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14373","description":"<p>In this paper, we present D2C-SR, a novel framework for the task of\nreal-world image super-resolution. As an ill-posed problem, the key challenge\nin super-resolution related tasks is there can be multiple predictions for a\ngiven low-resolution input. Most classical deep learning based approaches\nignored the fundamental fact and lack explicit modeling of the underlying\nhigh-frequency distribution which leads to blurred results. Recently, some\nmethods of GAN-based or learning super-resolution space can generate simulated\ntextures but do not promise the accuracy of the textures which have low\nquantitative performance. Rethinking both, we learn the distribution of\nunderlying high-frequency details in a discrete form and propose a two-stage\npipeline: divergence stage to convergence stage. At divergence stage, we\npropose a tree-based structure deep network as our divergence backbone.\nDivergence loss is proposed to encourage the generated results from the\ntree-based network to diverge into possible high-frequency representations,\nwhich is our way of discretely modeling the underlying high-frequency\ndistribution. At convergence stage, we assign spatial weights to fuse these\ndivergent predictions to obtain the final output with more accurate details.\nOur approach provides a convenient end-to-end manner to inference. We conduct\nevaluations on several real-world benchmarks, including a new proposed\nD2CRealSR dataset with x8 scaling factor. Our experiments demonstrate that\nD2C-SR achieves better accuracy and visual improvements against\nstate-of-the-art methods, with a significantly less parameters number.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_L/0/1/0/all/0/1\">Lanpeng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Deep Neural Networks for Supervised Learning Single-View Depth. (arXiv:2104.14202v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14202","description":"<p>Uncertainty quantification is a key aspect of robotic perception, as\noverconfident or point estimators can lead to collisions and damages to the\nenvironment and the robot. In this paper, we evaluate scalable approaches to\nuncertainty quantification in single-view supervised depth learning,\nspecifically MC dropout and deep ensembles. For MC dropout,in particular, we\nexplore deeply the effect of the dropout at different levels in the\narchitecture. We demonstrate that adding dropout in the encoder offer better\nresults than adding it in the decoder, the latest being the usual approach in\nthe literature for similar problems. We also show the use of depth uncertainty\nin the application of pseudo-RGBD ICP and demonstrate its potential for\nimproving the accuracy in such a task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Puigvert_J/0/1/0/all/0/1\">Javier Rodr&#xed;guez-Puigvert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Cantin_R/0/1/0/all/0/1\">Rub&#xe9;n Mart&#xed;nez-Cant&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NURBS-Diff: A differentiable programming module for NURBS. (arXiv:2104.14547v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.14547","description":"<p>Boundary representations (B-reps) using Non-Uniform Rational B-splines\n(NURBS) are the de facto standard used in CAD, but their utility in deep\nlearning-based approaches is not well researched. We propose a differentiable\nNURBS module to integrate the NURBS representation of CAD models with deep\nlearning methods. We mathematically define the derivatives of the NURBS curves\nor surfaces with respect to the input parameters. These derivatives are used to\ndefine an approximate Jacobian that can be used to perform the \"backward\"\nevaluation used while training deep learning models. We have implemented our\nNURBS module using GPU-accelerated algorithms and integrated it with PyTorch, a\npopular deep learning framework. We demonstrate the efficacy of our NURBS\nmodule in performing CAD operations such as curve or surface fitting and\nsurface offsetting. Further, we show its utility in deep learning for\nunsupervised point cloud reconstruction. These examples show that our module\nperforms better for certain deep learning frameworks and can be directly\nintegrated with any deep-learning framework requiring NURBS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Anjana Deva Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balu_A/0/1/0/all/0/1\">Aditya Balu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Harshil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Soumik Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Adarsh Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-augmented Spatio-Temporal Segmentation for Land Cover Mapping. (arXiv:2105.02963v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02963","description":"<p>The availability of massive earth observing satellite data provide huge\nopportunities for land use and land cover mapping. However, such mapping effort\nis challenging due to the existence of various land cover classes, noisy data,\nand the lack of proper labels. Also, each land cover class typically has its\nown unique temporal pattern and can be identified only during certain periods.\nIn this article, we introduce a novel architecture that incorporates the UNet\nstructure with Bidirectional LSTM and Attention mechanism to jointly exploit\nthe spatial and temporal nature of satellite data and to better identify the\nunique temporal patterns of each land cover. We evaluate this method for\nmapping crops in multiple regions over the world. We compare our method with\nother state-of-the-art methods both quantitatively and qualitatively on two\nreal-world datasets which involve multiple land cover classes. We also\nvisualise the attention weights to study its effectiveness in mitigating noise\nand identifying discriminative time period.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravirathinam_P/0/1/0/all/0/1\">Praveen Ravirathinam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal-Spatial Feature Pyramid for Video Saliency Detection. (arXiv:2105.04213v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04213","description":"<p>Multi-level features are important for saliency detection. Better combination\nand use of multi-level features with time information can greatly improve the\naccuracy of the video saliency model. In order to fully combine multi-level\nfeatures and make it serve the video saliency model, we propose a 3D fully\nconvolutional encoder-decoder architecture for video saliency detection, which\ncombines scale, space and time information for video saliency modeling. The\nencoder extracts multi-scale temporal-spatial features from the input\ncontinuous video frames, and then constructs temporal-spatial feature pyramid\nthrough temporal-spatial convolution and top-down feature integration. The\ndecoder performs hierarchical decoding of temporal-spatial features from\ndifferent scales, and finally produces a saliency map from the integration of\nmultiple video frames. Our model is simple yet effective, and can run in real\ntime. We perform abundant experiments, and the results indicate that the\nwell-designed structure can improve the precision of video saliency detection\nsignificantly. Experimental results on three purely visual video saliency\nbenchmarks and six audio-video saliency benchmarks demonstrate that our method\noutperforms the existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qinyao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shiping Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis. (arXiv:2106.03776v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03776","description":"<p>Background modeling is a promising research area in video analysis with a\nvariety of video surveillance applications. Recent years have witnessed the\nproliferation of deep neural networks via effective learning-based approaches\nin motion analysis. However, these techniques only provide a limited\ndescription of the observed scenes' insufficient properties where a\nsingle-valued mapping is learned to approximate the temporal conditional\naverages of the target background. On the other hand, statistical learning in\nimagery domains has become one of the most prevalent approaches with high\nadaptation to dynamic context transformation, notably Gaussian Mixture Models,\ncombined with a foreground extraction step. In this work, we propose a novel,\ntwo-stage method of change detection with two convolutional neural networks.\nThe first architecture is grounded on the unsupervised Gaussian mixtures\nstatistical learning to describe the scenes' salient features. The second one\nimplements a light-weight pipeline of foreground detection. Our two-stage\nframework contains approximately 3.5K parameters in total but still maintains\nrapid convergence to intricate motion patterns. Our experiments on publicly\navailable datasets show that our proposed networks are not only capable of\ngeneralizing regions of moving objects in unseen cases with promising results\nbut also are competitive in performance efficiency and effectiveness regarding\nforeground segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuong_N/0/1/0/all/0/1\">Nguyen-Tien Cuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hung Ngoc Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1\">Nhat Minh Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_P/0/1/0/all/0/1\">Phuong Hoai Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Synh Viet-Uyen Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse PointPillars: Maintaining and Exploiting Input Sparsity to Improve Runtime on Embedded Systems. (arXiv:2106.06882v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06882","description":"<p>Bird's Eye View (BEV) is a popular representation for processing 3D point\nclouds, and by its nature is fundamentally sparse. Motivated by the\ncomputational limitations of mobile robot platforms, we take a fast,\nhigh-performance BEV 3D object detector - PointPillars - and modify its\nbackbone to maintain and exploit this input sparsity, leading to decreased\nruntimes. We present results on KITTI, a canonical 3D detection dataset, and\nMatterport-Chair, a novel Matterport3D-derived chair detection dataset from\nscenes in real furnished homes. We evaluate runtime characteristics using a\ndesktop GPU, an embedded ML accelerator, and a robot CPU, demonstrating that\nour method results in significant runtime decreases (2x or more) for embedded\nsystems with only a modest decrease in detection quality. Our work represents a\nnew approach for practitioners to optimize models for embedded systems by\nmaintaining and exploiting input sparsity throughout their entire pipeline to\nreduce runtime and resource usage while preserving detection performance. All\nmodels, weights, experimental configurations, and datasets used are publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vedder_K/0/1/0/all/0/1\">Kyle Vedder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1\">Eric Eaton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAR Image Change Detection Based on Multiscale Capsule Network. (arXiv:2106.06896v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06896","description":"<p>Traditional change detection methods based on convolutional neural networks\n(CNNs) face the challenges of speckle noise and deformation sensitivity for\nsynthetic aperture radar images. To mitigate these issues, we proposed a\nMultiscale Capsule Network (Ms-CapsNet) to extract the discriminative\ninformation between the changed and unchanged pixels. On the one hand, the\ncapsule module is employed to exploit the spatial relationship of features.\nTherefore, equivariant properties can be achieved by aggregating the features\nfrom different positions. On the other hand, an adaptive fusion convolution\n(AFC) module is designed for the proposed Ms-CapsNet. Higher semantic features\ncan be captured for the primary capsules. Feature extracted by the AFC module\nsignificantly improves the robustness to speckle noise. The effectiveness of\nthe proposed Ms-CapsNet is verified on three real SAR datasets. The comparison\nexperiments with four state-of-the-art methods demonstrated the efficiency of\nthe proposed method. Our codes are available at\nhttps://github.com/summitgao/SAR_CD_MS_CapsNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Heng-Chao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI. (arXiv:2106.11731v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.11731","description":"<p>UK Biobank (UKB) conducts large-scale examinations of more than half a\nmillion volunteers, collecting health-related information on genetics,\nlifestyle, blood biochemistry, and more. Medical imaging of 100,000 subjects,\nwith 70,000 follow-up sessions, enables measurements of organs, muscle, and\nbody composition. With up to 170,000 mounting MR images, various methodologies\nare accordingly engaged in large-scale image analysis. This work presents an\nexperimental inference engine that can automatically predict a comprehensive\nprofile of subject metadata from UKB neck-to-knee body MRI. It was evaluated in\ncross-validation for baseline characteristics such as age, height, weight, and\nsex, but also measurements of body composition, organ volumes, and abstract\nproperties like grip strength, pulse rate, and type 2 diabetic status. It\npredicted subsequently released test data covering twelve body composition\nmetrics with a 3% median error. The proposed system can automatically analyze\none thousand subjects within ten minutes, providing individual confidence\nintervals. The underlying methodology utilizes convolutional neural networks\nfor image-based mean-variance regression on two-dimensional representations of\nthe MRI data. This work aims to make the proposed system available for free to\nresearchers, who can use it to obtain fast and fully-automated estimates of 72\ndifferent measurements immediately upon release of new UKB image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1\">Taro Langner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mora_A/0/1/0/all/0/1\">Andr&#xe9;s Mart&#xed;nez Mora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1\">Robin Strand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1\">H&#xe5;kan Ahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1\">Joel Kullberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Accurate Road Crack Detection Based on Adaptive Cost-Sensitive Loss Function. (arXiv:2106.15510v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15510","description":"<p>Numerous detection problems in computer vision, including road crack\ndetection, suffer from exceedingly foreground-background imbalance.\nFortunately, modification of loss function appears to solve this puzzle once\nand for all. In this paper, we propose a pixel-based adaptive weighted\ncross-entropy loss in conjunction with Jaccard distance to facilitate\nhigh-quality pixel-level road crack detection. Our work profoundly demonstrates\nthe influence of loss functions on detection outcomes, and sheds light on the\nsophisticated consecutive improvements in the realm of crack detection.\nSpecifically, to verify the effectiveness of the proposed loss, we conduct\nextensive experiments on four public databases, i.e., CrackForest, AigleRN,\nCrack360, and BJN260. Compared with the vanilla weighted cross-entropy, the\nproposed loss significantly speeds up the training process while retaining the\ntest accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingjie Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiquan Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening. (arXiv:2108.03815v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03815","description":"<p>Anomaly detection plays a pivotal role in numerous real-world scenarios, such\nas industrial automation and manufacturing intelligence. Recently, variational\ninference-based anomaly analysis has attracted researchers' and developers'\nattention. It aims to model the defect-free distribution so that anomalies can\nbe classified as out-of-distribution samples. Nevertheless, there are two\ndisturbing factors that need us to prioritize: (i) the simplistic prior latent\ndistribution inducing limited expressive capability; (ii) the strong\nprobability distance notion results in collapsed features. In this paper, we\npropose a novel Patch-wise Wasserstein AutoEncoder (P-WAE) architecture to\nalleviate those challenges. In particular, a patch-wise variational inference\nmodel coupled with solving the jigsaw puzzle is designed, which is a simple yet\neffective way to increase the expressiveness of the latent manifold. This makes\nusing the model on high-dimensional practical data possible. In addition, we\nleverage a weaker measure, sliced-Wasserstein distance, to achieve the\nequilibrium between the reconstruction fidelity and generalized\nrepresentations. Comprehensive experiments, conducted on the MVTec AD dataset,\ndemonstrate the superior performance of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-repository of screening mammography classifiers. (arXiv:2108.04800v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.04800","description":"<p>Artificial intelligence (AI) is showing promise in improving clinical\ndiagnosis. In breast cancer screening, several recent studies show that AI has\nthe potential to improve radiologists' accuracy, subsequently helping in early\ncancer diagnosis and reducing unnecessary workup. As the number of proposed\nmodels and their complexity grows, it is becoming increasingly difficult to\nre-implement them in order to reproduce the results and to compare different\napproaches. To enable reproducibility of research in this application area and\nto enable comparison between different methods, we release a meta-repository\ncontaining deep learning models for classification of screening mammograms.\nThis meta-repository creates a framework that enables the evaluation of machine\nlearning models on any private or public screening mammography data set. At its\ninception, our meta-repository contains five state-of-the-art models with\nopen-source implementations and cross-platform compatibility. We compare their\nperformance on six international data sets: two New York University breast\ncancer screening data sets, DDSM, INbreast, OPTIMAM and Chinese Mammography\nDatabase. Our framework has a flexible design that can be generalized to other\nmedical image analysis tasks. The meta-repository is available at\nhttps://www.github.com/nyukat/mammography_metarepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1\">Benjamin Stadnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1\">Jan Witowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1\">Vishwaesh Rajiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Objective for Novel Class Discovery. (arXiv:2108.08536v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08536","description":"<p>In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims\nat inferring novel object categories in an unlabeled set by leveraging from\nprior knowledge of a labeled set containing different, but related classes.\nExisting approaches tackle this problem by considering multiple objective\nfunctions, usually involving specialized loss terms for the labeled and the\nunlabeled samples respectively, and often requiring auxiliary regularization\nterms. In this paper, we depart from this traditional scheme and introduce a\nUNified Objective function (UNO) for discovering novel classes, with the\nexplicit purpose of favoring synergy between supervised and unsupervised\nlearning. Using a multi-view self-labeling strategy, we generate pseudo-labels\nthat can be treated homogeneously with ground truth labels. This leads to a\nsingle classification objective operating on both known and unknown classes.\nDespite its simplicity, UNO outperforms the state of the art by a significant\nmargin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The\nproject page is available at: https://ncd-uno.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Representation of Geometric Primitives for Graph-SLAM Optimization Using Decomposed Quadrics. (arXiv:2108.08957v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.08957","description":"<p>In Simultaneous Localization And Mapping (SLAM) problems, high-level\nlandmarks have the potential to build compact and informative maps compared to\ntraditional point-based landmarks. In this work, we focus on the\nparameterization of frequently used geometric primitives including points,\nlines, planes, ellipsoids, cylinders, and cones. We first present a unified\nrepresentation based on quadrics, leading to a consistent and concise\nformulation. Then we further study a decomposed model of quadrics that\ndiscloses the symmetric and degenerated properties of a primitive. Based on the\ndecomposition, we develop geometrically meaningful quadrics factors in the\nsettings of a graph-SLAM problem. Then in simulation experiments, it is shown\nthat the decomposed formulation has better efficiency and robustness to\nobservation noises than baseline parameterizations. Finally, in real-world\nexperiments, the proposed back-end framework is demonstrated to be capable of\nbuilding compact and regularized maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhen_W/0/1/0/all/0/1\">Weikun Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yaoyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding the Generative Capability of Adversarially Robust Classifiers. (arXiv:2108.09093v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.09093","description":"<p>Recently, some works found an interesting phenomenon that adversarially\nrobust classifiers can generate good images comparable to generative models. We\ninvestigate this phenomenon from an energy perspective and provide a novel\nexplanation. We reformulate adversarial example generation, adversarial\ntraining, and image generation in terms of an energy function. We find that\nadversarial training contributes to obtaining an energy function that is flat\nand has low energy around the real data, which is the key for generative\ncapability. Based on our new understanding, we further propose a better\nadversarial training method, Joint Energy Adversarial Training (JEAT), which\ncan generate high-quality images and achieve new state-of-the-art robustness\nunder a wide range of attacks. The Inception Score of the images (CIFAR-10)\ngenerated by JEAT is 8.80, much better than original robust classifiers (7.50).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiacheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiacheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rongxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection and Localization of Multiple Image Splicing Using MobileNet V1. (arXiv:2108.09674v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09674","description":"<p>In modern society, digital images have become a prominent source of\ninformation and medium of communication. They can, however, be simply altered\nusing freely available image editing software. Two or more images are combined\nto generate a new image that can transmit information across social media\nplatforms to influence the people in the society. This information may have\nboth positive and negative consequences. Hence there is a need to develop a\ntechnique that will detect and locates a multiple image splicing forgery in an\nimage. This research work proposes multiple image splicing forgery detection\nusing Mask R-CNN, with a backbone as a MobileNet V1. It also calculates the\npercentage score of a forged region of multiple spliced images. The comparative\nanalysis of the proposed work with the variants of ResNet is performed. The\nproposed model is trained and tested using our MISD (Multiple Image Splicing\nDataset), and it is observed that the proposed model outperforms the variants\nof ResNet models (ResNet 51,101 and 151).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadam_K/0/1/0/all/0/1\">Kalyani Kadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahirrao_D/0/1/0/all/0/1\">Dr. Swati Ahirrao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_D/0/1/0/all/0/1\">Dr. Ketan Kotecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Sayan Sahu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes. (arXiv:2109.03585v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03585","description":"<p>This paper considers matching images of low-light scenes, aiming to widen the\nfrontier of SfM and visual SLAM applications. Recent image sensors can record\nthe brightness of scenes with more than eight-bit precision, available in their\nRAW-format image. We are interested in making full use of such high-precision\ninformation to match extremely low-light scene images that conventional methods\ncannot handle. For extreme low-light scenes, even if some of their brightness\ninformation exists in the RAW format images' low bits, the standard raw image\nprocessing on cameras fails to utilize them properly. As was recently shown by\nChen et al., CNNs can learn to produce images with a natural appearance from\nsuch RAW-format images. To consider if and how well we can utilize such\ninformation stored in RAW-format images for image matching, we have created a\nnew dataset named MID (matching in the dark). Using it, we experimentally\nevaluated combinations of eight image-enhancing methods and eleven image\nmatching methods consisting of classical/neural local descriptors and\nclassical/neural initial point-matching methods. The results show the advantage\nof using the RAW-format images and the strengths and weaknesses of the above\ncomponent methods. They also imply there is room for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenzheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimobayashi_N/0/1/0/all/0/1\">Noriyuki Shimobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruta_D/0/1/0/all/0/1\">Daisuke Maruta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised clothing change adaptive person ReID. (arXiv:2109.03702v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03702","description":"<p>Clothing changes and lack of data labels are both crucial challenges in\nperson ReID. For the former challenge, people may occur multiple times at\ndifferent locations wearing different clothing. However, most of the current\nperson ReID research works focus on the benchmarks in which a person's clothing\nis kept the same all the time. For the last challenge, some researchers try to\nmake model learn information from a labeled dataset as a source to an unlabeled\ndataset. Whereas purely unsupervised training is less used. In this paper, we\naim to solve both problems at the same time. We design a novel unsupervised\nmodel, Sync-Person-Cloud ReID, to solve the unsupervised clothing change person\nReID problem. We developer a purely unsupervised clothing change person ReID\npipeline with person sync augmentation operation and same person feature\nrestriction. The person sync augmentation is to supply additional same person\nresources. These same person's resources can be used as part supervised input\nby same person feature restriction. The extensive experiments on clothing\nchange ReID datasets show the out-performance of our methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Congzhentao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard YiDa Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully Sampled Multi-Coil MRI Data. (arXiv:2109.03812v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.03812","description":"<p>Improving speed and image quality of Magnetic Resonance Imaging (MRI) via\nnovel reconstruction approaches remains one of the highest impact applications\nfor deep learning in medical imaging. The fastMRI dataset, unique in that it\ncontains large volumes of raw MRI data, has enabled significant advances in\naccelerating MRI using deep learning-based reconstruction methods. While the\nimpact of the fastMRI dataset on the field of medical imaging is unquestioned,\nthe dataset currently lacks clinical expert pathology annotations, critical to\naddressing clinically relevant reconstruction frameworks and exploring\nimportant questions regarding rendering of specific pathology using such novel\napproaches. This work introduces fastMRI+, which consists of 16154\nsubspecialist expert bounding box annotations and 13 study-level labels for 22\ndifferent pathology categories on the fastMRI knee dataset, and 7570\nsubspecialist expert bounding box annotations and 643 study-level labels for 30\ndifferent pathology categories for the fastMRI brain dataset. The fastMRI+\ndataset is open access and aims to support further research and advancement of\nmedical imaging in MRI reconstruction and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_R/0/1/0/all/0/1\">Ruiyang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1\">Burhaneddin Yaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stewart_R/0/1/0/all/0/1\">Russell Stewart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dixon_A/0/1/0/all/0/1\">Austin Dixon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knoll_F/0/1/0/all/0/1\">Florian Knoll</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengnan Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lui_Y/0/1/0/all/0/1\">Yvonne W. Lui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_M/0/1/0/all/0/1\">Michael S. Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving gas bubbles ascending in liquid metal from low-SNR neutron radiography images. (arXiv:2109.04883v2 [physics.flu-dyn] UPDATED)","link":"http://arxiv.org/abs/2109.04883","description":"<p>We demonstrate a new image processing methodology for resolving gas bubbles\ntravelling through liquid metal from dynamic neutron radiography images with\nintrinsically low signal-to-noise ratio. Image pre-processing, denoising and\nbubble segmentation are described in detail, with practical recommendations.\nExperimental validation is presented - stationary and moving reference bodies\nwith neutron-transparent cavities are radiographed with imaging conditions\nsimilar to the cases with bubbles in liquid metal. The new methods are applied\nto our experimental data from previous and recent imaging campaigns, and the\nperformance of the methods proposed in this paper is compared against our\npreviously developed methods. Significant improvements are observed as well as\nthe capacity to reliably extract physically meaningful information from\nmeasurements performed under highly adverse imaging conditions. The showcased\nimage processing solution and separate elements thereof are readily extendable\nbeyond the present application, and have been made open-source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Birjukovs_M/0/1/0/all/0/1\">Mihails Birjukovs</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Trtik_P/0/1/0/all/0/1\">Pavel Trtik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kaestner_A/0/1/0/all/0/1\">Anders Kaestner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hovind_J/0/1/0/all/0/1\">Jan Hovind</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Klevs_M/0/1/0/all/0/1\">Martins Klevs</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gawryluk_D/0/1/0/all/0/1\">Dariusz Jakub Gawryluk</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Thomsen_K/0/1/0/all/0/1\">Knud Thomsen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jakovics_A/0/1/0/all/0/1\">Andris Jakovics</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A semi-supervised self-training method to develop assistive intelligence for segmenting multiclass bridge elements from inspection videos. (arXiv:2109.05078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05078","description":"<p>Bridge inspection is an important step in preserving and rehabilitating\ntransportation infrastructure for extending their service lives. The\nadvancement of mobile robotic technology allows the rapid collection of a large\namount of inspection video data. However, the data are mainly images of complex\nscenes, wherein a bridge of various structural elements mix with a cluttered\nbackground. Assisting bridge inspectors in extracting structural elements of\nbridges from the big complex video data, and sorting them out by classes, will\nprepare inspectors for the element-wise inspection to determine the condition\nof bridges. This paper is motivated to develop an assistive intelligence model\nfor segmenting multiclass bridge elements from inspection videos captured by an\naerial inspection platform. With a small initial training dataset labeled by\ninspectors, a Mask Region-based Convolutional Neural Network (Mask R-CNN)\npre-trained on a large public dataset was transferred to the new task of\nmulticlass bridge element segmentation. Besides, the temporal coherence\nanalysis attempts to recover false negatives and identify the weakness that the\nneural network can learn to improve. Furthermore, a semi-supervised\nself-training (S$^3$T) method was developed to engage experienced inspectors in\nrefining the network iteratively. Quantitative and qualitative results from\nevaluating the developed deep neural network demonstrate that the proposed\nmethod can utilize a small amount of time and guidance from experienced\ninspectors (3.58 hours for labeling 66 images) to build the network of\nexcellent performance (91.8% precision, 93.6% recall, and 92.7% f1-score).\nImportantly, the paper illustrates an approach to leveraging the domain\nknowledge and experiences of bridge professionals into computational\nintelligence models to efficiently adapt the models to varied bridges in the\nNational Bridge Inventory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaozheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Genda Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Re-parameterization Residual Attention Network For Nonhomogeneous Image Dehazing. (arXiv:2109.05479v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.05479","description":"<p>This paper proposes an end-to-end Efficient Re-parameterizationResidual\nAttention Network(ERRA-Net) to directly restore the nonhomogeneous hazy image.\nThe contribution of this paper mainly has the following three aspects: 1) A\nnovel Multi-branch Attention (MA) block. The spatial attention mechanism better\nreconstructs high-frequency features, and the channel attention mechanism\ntreats the features of different channels differently. Multi-branch structure\ndramatically improves the representation ability of the model and can be\nchanged into a single path structure after re-parameterization to speed up the\nprocess of inference. Local Residual Connection allows the low-frequency\ninformation in the nonhomogeneous area to pass through the block without\nprocessing so that the block can focus on detailed features. 2) A lightweight\nnetwork structure. We use cascaded MA blocks to extract high-frequency features\nstep by step, and the Multi-layer attention fusion tail combines the shallow\nand deep features of the model to get the residual of the clean image finally.\n3)We propose two novel loss functions to help reconstruct the hazy image\nColorAttenuation loss and Laplace Pyramid loss. ERRA-Net has an impressive\nspeed, processing 1200x1600 HD quality images with an average runtime of 166.11\nfps. Extensive evaluations demonstrate that ERSANet performs favorably against\nthe SOTA approaches on the real-world hazy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1\">ErKang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">XinRui Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Separated Curve Rendering Network for Efficient and High-Resolution Image Harmonization. (arXiv:2109.05750v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05750","description":"<p>Image harmonization aims to modify the color of the composited region with\nrespect to the specific background. Previous works model this task as a\npixel-wise image-to-image translation using UNet family structures. However,\nthe model size and computational cost limit the performability of their models\non edge devices and higher-resolution images. To this end, we propose a novel\nspatial-separated curve rendering network (S$^2$CRNet) for efficient and\nhigh-resolution image harmonization for the first time. In S$^2$CRNet, we\nfirstly extract the spatial-separated embeddings from the thumbnails of the\nmasked foreground and background individually. Then, we design a curve\nrendering module (CRM), which learns and combines the spatial-specific\nknowledge using linear layers to generate the parameters of the pixel-wise\ncurve mapping in the foreground region. Finally, we directly render the\noriginal high-resolution images using the learned color curve. Besides, we also\nmake two extensions of the proposed framework via the Cascaded-CRM and\nSemantic-CRM for cascaded refinement and semantic guidance, respectively.\nExperiments show that the proposed method reduces more than 90% parameters\ncompared with previous methods but still achieves the state-of-the-art\nperformance on both synthesized iHarmony4 and real-world DIH test set.\nMoreover, our method can work smoothly on higher resolution images in real-time\nwhich is more than 10$\\times$ faster than the existing methods. The code and\npre-trained models will be made available and released at\nhttps://github.com/stefanLeong/S2CRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingtang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1\">Xiaodong Cun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color. (arXiv:2109.06129v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06129","description":"<p>Pretrained language models have been shown to encode relational information,\nsuch as the relations between entities or concepts in knowledge-bases --\n(Paris, Capital, France). However, simple relations of this type can often be\nrecovered heuristically and the extent to which models implicitly reflect\ntopological structure that is grounded in world, such as perceptual structure,\nis unknown. To explore this question, we conduct a thorough case study on\ncolor. Namely, we employ a dataset of monolexemic color terms and color chips\nrepresented in CIELAB, a color space with a perceptually meaningful distance\nmetric.\n</p>\n<p>Using two methods of evaluating the structural alignment of colors in this\nspace with text-derived color term representations, we find significant\ncorrespondence. Analyzing the differences in alignment across the color\nspectrum, we find that warmer colors are, on average, better aligned to the\nperceptual color space than cooler ones, suggesting an intriguing connection to\nfindings from recent work on efficient communication in color naming. Further\nanalysis suggests that differences in alignment are, in part, mediated by\ncollocationality and differences in syntactic usage, posing questions as to the\nrelationship between color perception and usage and context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdou_M/0/1/0/all/0/1\">Mostafa Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulmizev_A/0/1/0/all/0/1\">Artur Kulmizev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1\">Stella Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}