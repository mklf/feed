{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02394","description":"<p>Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1\">Ashwin Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakhotiya_Y/0/1/0/all/0/1\">Yash Jakhotiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pir\\'a: A Bilingual Portuguese-English Dataset for Question-Answering about the Ocean. (arXiv:2202.02398v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02398","description":"<p>Current research in natural language processing is highly dependent on\ncarefully produced corpora. Most existing resources focus on English; some\nresources focus on languages such as Chinese and French; few resources deal\nwith more than one language. This paper presents the Pir\\'a dataset, a large\nset of questions and answers about the ocean and the Brazilian coast both in\nPortuguese and English. Pir\\'a is, to the best of our knowledge, the first QA\ndataset with supporting texts in Portuguese, and, perhaps more importantly, the\nfirst bilingual QA dataset that includes this language. The Pir\\'a dataset\nconsists of 2261 properly curated question/answer (QA) sets in both languages.\nThe QA sets were manually created based on two corpora: abstracts related to\nthe Brazilian coast and excerpts of United Nation reports about the ocean. The\nQA sets were validated in a peer-review process with the dataset contributors.\nWe discuss some of the advantages as well as limitations of Pir\\'a, as this new\nresource can support a set of tasks in NLP such as question-answering,\ninformation retrieval, and machine translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paschoal_A/0/1/0/all/0/1\">Andr&#xe9; F. A. Paschoal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirozelli_P/0/1/0/all/0/1\">Paulo Pirozelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freire_V/0/1/0/all/0/1\">Valdinei Freire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delgado_K/0/1/0/all/0/1\">Karina V. Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peres_S/0/1/0/all/0/1\">Sarajane M. Peres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_M/0/1/0/all/0/1\">Marcos M. Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakasato_F/0/1/0/all/0/1\">Fl&#xe1;vio Nakasato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Andr&#xe9; S. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandao_A/0/1/0/all/0/1\">Anarosa A. F. Brand&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Anna H. R. Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozman_F/0/1/0/all/0/1\">Fabio G. Cozman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers and the representation of biomedical background knowledge. (arXiv:2202.02432v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02432","description":"<p>BioBERT and BioMegatron are Transformers models adapted for the biomedical\ndomain based on publicly available biomedical corpora. As such, they have the\npotential to encode large-scale biological knowledge. We investigate the\nencoding and representation of biological knowledge in these models, and its\npotential utility to support inference in cancer precision medicine - namely,\nthe interpretation of the clinical significance of genomic alterations. We\ncompare the performance of different transformer baselines; we use probing to\ndetermine the consistency of encodings for distinct entities; and we use\nclustering methods to compare and contrast the internal properties of the\nembeddings for genes, variants, drugs and diseases. We show that these models\ndo indeed encode biological knowledge, although some of this is lost in\nfine-tuning for specific tasks. Finally, we analyse how the models behave with\nregard to biases and imbalances in the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wysocki_O/0/1/0/all/0/1\">Oskar Wysocki</a> (1,2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a> (1,2), <a href=\"http://arxiv.org/find/cs/1/au:+ORegan_P/0/1/0/all/0/1\">Paul O&#x27;Regan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wysocka_M/0/1/0/all/0/1\">Magdalena Wysocka</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">D&#xf3;nal Landers</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a> (1,2,3) ((1) Department of Computer Science, The University of Manchester, (2) digital Experimental Cancer Medicine Team, Cancer Biomarker Centre, CRUK Manchester Institute, University of Manchester, (3) Idiap Research Institute)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Logic Analogy Learning. (arXiv:2202.02436v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02436","description":"<p>Letter-string analogy is an important analogy learning task which seems to be\neasy for humans but very challenging for machines. The main idea behind current\napproaches to solving letter-string analogies is to design heuristic rules for\nextracting analogy structures and constructing analogy mappings. However, one\nkey problem is that it is difficult to build a comprehensive and exhaustive set\nof analogy structures which can fully describe the subtlety of analogies. This\nproblem makes current approaches unable to handle complicated letter-string\nanalogy problems. In this paper, we propose Neural logic analogy learning\n(Noan), which is a dynamic neural architecture driven by differentiable logic\nreasoning to solve analogy problems. Each analogy problem is converted into\nlogical expressions consisting of logical variables and basic logical\noperations (AND, OR, and NOT). More specifically, Noan learns the logical\nvariables as vector embeddings and learns each logical operation as a neural\nmodule. In this way, the model builds computational graph integrating neural\nnetwork with logical reasoning to capture the internal logical structure of the\ninput letter strings. The analogy learning problem then becomes a True/False\nevaluation problem of the logical expressions. Experiments show that our\nmachine learning-based Noan approach outperforms state-of-the-art approaches on\nstandard letter-string analogy benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yujia Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Similarity Computing Model Based on Multi Model Fine-Grained Nonlinear Fusion. (arXiv:2202.02476v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02476","description":"<p>Natural language processing (NLP) task has achieved excellent performance in\nmany fields, including semantic understanding, automatic summarization, image\nrecognition and so on. However, most of the neural network models for NLP\nextract the text in a fine-grained way, which is not conducive to grasp the\nmeaning of the text from a global perspective. To alleviate the problem, the\ncombination of the traditional statistical method and deep learning model as\nwell as a novel model based on multi model nonlinear fusion are proposed in\nthis paper. The model uses the Jaccard coefficient based on part of speech,\nTerm Frequency-Inverse Document Frequency (TF-IDF) and word2vec-CNN algorithm\nto measure the similarity of sentences respectively. According to the\ncalculation accuracy of each model, the normalized weight coefficient is\nobtained and the calculation results are compared. The weighted vector is input\ninto the fully connected neural network to give the final classification\nresults. As a result, the statistical sentence similarity evaluation algorithm\nreduces the granularity of feature extraction, so it can grasp the sentence\nfeatures globally. Experimental results show that the matching of sentence\nsimilarity calculation method based on multi model nonlinear fusion is 84%, and\nthe F1 value of the model is 75%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peiying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xingzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chunxiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A simple language-agnostic yet very strong baseline system for hate speech and offensive content identification. (arXiv:2202.02511v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02511","description":"<p>For automatically identifying hate speech and offensive content in tweets, a\nsystem based on a classical supervised algorithm only fed with character\nn-grams, and thus completely language-agnostic, is proposed by the SATLab team.\nAfter its optimization in terms of the feature weighting and the classifier\nparameters, it reached, in the multilingual HASOC 2021 challenge, a medium\nperformance level in English, the language for which it is easy to develop deep\nlearning approaches relying on many external linguistic resources, but a far\nbetter level for the two less resourced language, Hindi and Marathi. It ends\neven first when performances are averaged over the three tasks in these\nlanguages, outperforming many deep learning approaches. These performances\nsuggest that it is an interesting reference level to evaluate the benefits of\nusing more complex approaches such as deep learning or taking into account\ncomplementary resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Automated Sarcasm Detection on Twitter. (arXiv:2202.02516v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02516","description":"<p>Automatic sarcasm detection is a growing field in computer science. Short\ntext messages are increasingly used for communication, especially over social\nmedia platforms such as Twitter. Due to insufficient or missing context,\nunidentified sarcasm in these messages can invert the meaning of a statement,\nleading to confusion and communication failures. This paper covers a variety of\ncurrent methods used for sarcasm detection, including detection by context,\nposting history and machine learning models. Additionally, a shift towards deep\nlearning methods is observable, likely due to the benefit of using a model with\ninduced instead of discrete features combined with the innovation of\ntransformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moores_B/0/1/0/all/0/1\">Bleau Moores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1\">Vijay Mago</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEAPMood: Light and Efficient Architecture to Predict Mood with Genetic Algorithm driven Hyperparameter Tuning. (arXiv:2202.02522v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02522","description":"<p>Accurate and automatic detection of mood serves as a building block for use\ncases like user profiling which in turn power applications such as advertising,\nrecommendation systems, and many more. One primary source indicative of an\nindividual's mood is textual data. While there has been extensive research on\nemotion recognition, the field of mood prediction has been barely explored. In\naddition, very little work is done in the area of on-device inferencing, which\nis highly important from the user privacy point of view. In this paper, we\npropose for the first time, an on-device deep learning approach for mood\nprediction from textual data, LEAPMood. We use a novel on-device\ndeployment-focused objective function for hyperparameter tuning based on the\nGenetic Algorithm (GA) and optimize the parameters concerning both performance\nand size. LEAPMood consists of Emotion Recognition in Conversion (ERC) as the\nfirst building block followed by mood prediction using K-means clustering. We\nshow that using a combination of character embedding, phonetic hashing, and\nattention along with Conditional Random Fields (CRF), results in a performance\nclosely comparable to that of the current State-Of-the-Art with a significant\nreduction in model size (&gt; 90%) for the task of ERC. We achieve a Micro F1\nscore of 62.05% with a memory footprint of a mere 1.67MB on the DailyDialog\ndataset. Furthermore, we curate a dataset for the task of mood prediction\nachieving a Macro F1-score of 72.12% with LEAPMood.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_H/0/1/0/all/0/1\">Harichandana B S S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-based Sentiment Analysis through EDU-level Attentions. (arXiv:2202.02535v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02535","description":"<p>A sentence may express sentiments on multiple aspects. When these aspects are\nassociated with different sentiment polarities, a model's accuracy is often\nadversely affected. We observe that multiple aspects in such hard sentences are\nmostly expressed through multiple clauses, or formally known as elementary\ndiscourse units (EDUs), and one EDU tends to express a single aspect with\nunitary sentiment towards that aspect. In this paper, we propose to consider\nEDU boundaries in sentence modeling, with attentions at both word and EDU\nlevels. Specifically, we highlight sentiment-bearing words in EDU through\nword-level sparse attention. Then at EDU level, we force the model to attend to\nthe right EDU for the right aspect, by using EDU-level sparse attention and\northogonal regularization. Experiments on three benchmark datasets show that\nour simple EDU-Attention model outperforms state-of-the-art baselines. Because\nEDU can be automatically segmented with high accuracy, our model can be applied\nto sentences directly without the need of manual EDU boundary annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimization of a Real-Time Wavelet-Based Algorithm for Improving Speech Intelligibility. (arXiv:2202.02545v1 [cs.SD])","link":"http://arxiv.org/abs/2202.02545","description":"<p>The optimization of a wavelet-based algorithm to improve speech\nintelligibility is reported. The discrete-time speech signal is split into\nfrequency sub-bands via a multi-level discrete wavelet transform. Various gains\nare applied to the sub-band signals before they are recombined to form a\nmodified version of the speech. The sub-band gains are adjusted while keeping\nthe overall signal energy unchanged, and the speech intelligibility under\nvarious background interference and simulated hearing loss conditions is\nenhanced and evaluated objectively and quantitatively using Google\nSpeech-to-Text transcription. For English and Chinese noise-free speech,\noverall intelligibility is improved, and the transcription accuracy can be\nincreased by as much as 80 percentage points by reallocating the spectral\nenergy toward the mid-frequency sub-bands, effectively increasing the\nconsonant-vowel intensity ratio. This is reasonable since the consonants are\nrelatively weak and of short duration, which are therefore the most likely to\nbecome indistinguishable in the presence of background noise or high-frequency\nhearing impairment. For speech already corrupted by noise, improving\nintelligibility is challenging but still realizable. The proposed algorithm is\nimplementable for real-time signal processing and comparatively simpler than\nprevious algorithms. Potential applications include speech enhancement, hearing\naids, machine listening, and a better understanding of speech intelligibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1\">Tianqu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_A/0/1/0/all/0/1\">Anh-Dung Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tianyuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yijia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_K/0/1/0/all/0/1\">Kevin Chau</a> (Hong Kong University of Science and Technology)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LST: Lexicon-Guided Self-Training for Few-Shot Text Classification. (arXiv:2202.02566v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02566","description":"<p>Self-training provides an effective means of using an extremely small amount\nof labeled data to create pseudo-labels for unlabeled data. Many\nstate-of-the-art self-training approaches hinge on different regularization\nmethods to prevent overfitting and improve generalization. Yet they still rely\nheavily on predictions initially trained with the limited labeled data as\npseudo-labels and are likely to put overconfident label belief on erroneous\nclasses depending on the first prediction. To tackle this issue in text\nclassification, we introduce LST, a simple self-training method that uses a\nlexicon to guide the pseudo-labeling mechanism in a linguistically-enriched\nmanner. We consistently refine the lexicon by predicting confidence of the\nunseen data to teach pseudo-labels better in the training iterations. We\ndemonstrate that this simple yet well-crafted lexical knowledge achieves\n1.0-2.0% better performance on 30 labeled samples per class for five benchmark\ndatasets than the current state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hazel Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1\">Jaeman Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yo-Sub Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Fine-Tuning of Transformer-Based Language Models for Named Entity Recognition. (arXiv:2202.02617v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02617","description":"<p>The current standard approach for fine-tuning transformer-based language\nmodels includes a fixed number of training epochs and a linear learning rate\nschedule. In order to obtain a near-optimal model for the given downstream\ntask, a search in optimization hyperparameter space is usually required. In\nparticular, the number of training epochs needs to be adjusted to the dataset\nsize. In this paper, we introduce adaptive fine-tuning, which is an alternative\napproach that uses early stopping and a custom learning rate schedule to\ndynamically adjust the number of training epochs to the dataset size. For the\nexample use case of named entity recognition, we show that our approach not\nonly makes hyperparameter search with respect to the number of training epochs\nredundant, but also leads to improved results in terms of performance,\nstability and efficiency. This holds true especially for small datasets, where\nwe outperform the state-of-the-art fine-tuning method by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stollenwerk_F/0/1/0/all/0/1\">Felix Stollenwerk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Probabilistic Models in Text Classification via Active Learning. (arXiv:2202.02629v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02629","description":"<p>When using text data, social scientists often classify documents in order to\nuse the resulting document labels as an outcome or predictor. Since it is\nprohibitively costly to label a large number of documents manually, automated\ntext classification has become a standard tool. However, current approaches for\ntext classification do not take advantage of all the data at one's disposal. We\npropose a fast new model for text classification that combines information from\nboth labeled and unlabeled data with an active learning component, where a\nhuman iteratively labels documents that the algorithm is least certain about.\nUsing text data from Wikipedia discussion pages, BBC News articles, historical\nUS Supreme Court opinions, and human rights abuse allegations, we show that by\nintroducing information about the structure of unlabeled data and iteratively\nlabeling uncertain documents, our model improves performance relative to\nclassifiers that (a) only use information from labeled data and (b) randomly\ndecide which documents to label at the cost of manually labelling a small\nnumber of documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bosley_M/0/1/0/all/0/1\">Mitchell Bosley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzushima_S/0/1/0/all/0/1\">Saki Kuzushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enamorado_T/0/1/0/all/0/1\">Ted Enamorado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiraito_Y/0/1/0/all/0/1\">Yuki Shiraito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Hate Speech and Offensive Content Detection using Modified Cross-entropy Loss. (arXiv:2202.02635v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02635","description":"<p>The number of increased social media users has led to a lot of people\nmisusing these platforms to spread offensive content and use hate speech.\nManual tracking the vast amount of posts is impractical so it is necessary to\ndevise automated methods to identify them quickly. Large language models are\ntrained on a lot of data and they also make use of contextual embeddings. We\nfine-tune the large language models to help in our task. The data is also quite\nunbalanced; so we used a modified cross-entropy loss to tackle the issue. We\nobserved that using a model which is fine-tuned in hindi corpora performs\nbetter. Our team (HNLP) achieved the macro F1-scores of 0.808, 0.639 in English\nSubtask A and English Subtask B respectively. For Hindi Subtask A, Hindi\nSubtask B our team achieved macro F1-scores of 0.737, 0.443 respectively in\nHASOC 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Arka Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankhala_P/0/1/0/all/0/1\">Priyanshu Sankhala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification on Sentence Embeddings for Legal Assistance. (arXiv:2202.02639v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02639","description":"<p>Legal proceedings take plenty of time and also cost a lot. The lawyers have\nto do a lot of work in order to identify the different sections of prior cases\nand statutes. The paper tries to solve the first tasks in AILA2021 (Artificial\nIntelligence for Legal Assistance) that will be held in FIRE2021 (Forum for\nInformation Retrieval Evaluation). The task is to semantically segment the\ndocument into different assigned one of the 7 predefined labels or \"rhetorical\nroles.\" The paper uses BERT to obtain the sentence embeddings from a sentence,\nand then a linear classifier is used to output the final prediction. The\nexperiments show that when more weightage is assigned to the class with the\nhighest frequency, the results are better than those when more weightage is\ngiven to the class with a lower frequency. In task 1, the team legalNLP\nobtained a F1 score of 0.22.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Arka Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RerrFact: Reduced Evidence Retrieval Representations for Scientific Claim Verification. (arXiv:2202.02646v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02646","description":"<p>Exponential growth in digital information outlets and the race to publish has\nmade scientific misinformation more prevalent than ever. However, the task to\nfact-verify a given scientific claim is not straightforward even for\nresearchers. Scientific claim verification requires in-depth knowledge and\ngreat labor from domain experts to substantiate supporting and refuting\nevidence from credible scientific sources. The SciFact dataset and\ncorresponding task provide a benchmarking leaderboard to the community to\ndevelop automatic scientific claim verification systems via extracting and\nassimilating relevant evidence rationales from source abstracts. In this work,\nwe propose a modular approach that sequentially carries out binary\nclassification for every prediction subtask as in the SciFact leaderboard. Our\nsimple classifier-based approach uses reduced abstract representations to\nretrieve relevant abstracts. These are further used to train the relevant\nrationale-selection model. Finally, we carry out two-step stance predictions\nthat first differentiate non-relevant rationales and then identify supporting\nor refuting rationales for a given claim. Experimentally, our system RerrFact\nwith no fine-tuning, simple design, and a fraction of model parameters fairs\ncompetitively on the leaderboard against large-scale, modular, and joint\nmodeling approaches. We make our codebase available at\nhttps://github.com/ashishrana160796/RerrFact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Ashish Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_D/0/1/0/all/0/1\">Deepanshu Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Muskaan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_T/0/1/0/all/0/1\">Tirthankar Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Harpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_P/0/1/0/all/0/1\">Prashant Singh Rana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics, Rules of Engagement, and AI: Neural Narrative Mapping Using Large Transformer Language Models. (arXiv:2202.02647v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02647","description":"<p>The problem of determining if a military unit has correctly understood an\norder and is properly executing on it is one that has bedeviled military\nplanners throughout history. The advent of advanced language models such as\nOpenAI's GPT-series offers new possibilities for addressing this problem. This\npaper presents a mechanism to harness the narrative output of large language\nmodels and produce diagrams or \"maps\" of the relationships that are latent in\nthe weights of such models as the GPT-3. The resulting \"Neural Narrative Maps\"\n(NNMs), are intended to provide insight into the organization of information,\nopinion, and belief in the model, which in turn provide means to understand\nintent and response in the context of physical distance. This paper discusses\nthe problem of mapping information spaces in general, and then presents a\nconcrete implementation of this concept in the context of OpenAI's GPT-3\nlanguage model for determining if a subordinate is following a commander's\nintent in a high-risk situation. The subordinate's locations within the NNM\nallow a novel capability to evaluate the intent of the subordinate with respect\nto the commander. We show that is is possible not only to determine if they are\nnearby in narrative space, but also how they are oriented, and what\n\"trajectory\" they are on. Our results show that our method is able to produce\nhigh-quality maps, and demonstrate new ways of evaluating intent more\ngenerally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_P/0/1/0/all/0/1\">Philip Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dant_A/0/1/0/all/0/1\">Aaron Dant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbluth_D/0/1/0/all/0/1\">David Rosenbluth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models. (arXiv:2202.02664v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02664","description":"<p>Recent research has shown the existence of significant redundancy in large\nTransformer models. One can prune the redundant parameters without\nsignificantly sacrificing the generalization performance. However, we question\nwhether the redundant parameters could have contributed more if they were\nproperly trained. To answer this question, we propose a novel training strategy\nthat encourages all parameters to be trained sufficiently. Specifically, we\nadaptively adjust the learning rate for each parameter according to its\nsensitivity, a robust gradient-based measure reflecting this parameter's\ncontribution to the model performance. A parameter with low sensitivity is\nredundant, and we improve its fitting by increasing its learning rate. In\ncontrast, a parameter with high sensitivity is well-trained, and we regularize\nit by decreasing its learning rate to prevent further overfitting. We conduct\nextensive experiments on natural language understanding, neural machine\ntranslation, and image classification to demonstrate the effectiveness of the\nproposed schedule. Analysis shows that the proposed schedule indeed reduces the\nredundancy and improves generalization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Effective is Incongruity? Implications for Code-mix Sarcasm Detection. (arXiv:2202.02702v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02702","description":"<p>The presence of sarcasm in conversational systems and social media like\nchatbots, Facebook, Twitter, etc. poses several challenges for downstream NLP\ntasks. This is attributed to the fact that the intended meaning of a sarcastic\ntext is contrary to what is expressed. Further, the use of code-mix language to\nexpress sarcasm is increasing day by day. Current NLP techniques for code-mix\ndata have limited success due to the use of different lexicon, syntax, and\nscarcity of labeled corpora. To solve the joint problem of code-mixing and\nsarcasm detection, we propose the idea of capturing incongruity through\nsub-word level embeddings learned via fastText. Empirical results shows that\nour proposed model achieves F1-score on code-mix Hinglish dataset comparable to\npretrained multilingual models while training 10x faster and using a lower\nmemory footprint\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Aditya Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurya_C/0/1/0/all/0/1\">Chandresh Kumar Maurya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. (arXiv:2202.02842v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02842","description":"<p>The search for effective and robust generalization metrics has been the focus\nof recent theoretical and empirical work.\n</p>\n<p>In this paper, we discuss the performance of natural language processing\n(NLP) models, and we evaluate various existing and novel generalization\nmetrics.\n</p>\n<p>Compared to prior studies, we\n</p>\n<p>(i) focus on NLP instead of computer vision (CV),\n</p>\n<p>(ii) focus on generalization metrics that predict test error instead of the\ngeneralization gap,\n</p>\n<p>(iii) focus on generalization metrics that do not need the access to data,\nand\n</p>\n<p>(iv) focus on the heavy-tail (HT) phenomenon that has received comparatively\nless attention in the study of deep neural networks (NNs).\n</p>\n<p>We extend recent HT-based work which focuses on power law (PL) distributions,\nand we study exponential (EXP) and exponentially truncated power law (E-TPL)\nfitting to the empirical spectral densities (ESDs) of weight matrices.\n</p>\n<p>Our detailed empirical studies show that\n</p>\n<p>(i) \\emph{shape metrics}, or the metrics obtained from fitting the shape of\nthe ESDs, perform uniformly better at predicting generalization performance\nthan \\emph{scale metrics} commonly studied in the literature, as measured by\nthe \\emph{average} rank correlations with the generalization performance for\nall of our experiments;\n</p>\n<p>(ii) among forty generalization metrics studied in our paper, the\n\\RANDDISTANCE metric, a new shape metric invented in this paper that measures\nthe distance between empirical eigenvalues of weight matrices and those of\nrandomly initialized weight matrices, achieves the highest worst-case rank\ncorrelation with generalization performance under a variety of training\nsettings; and\n</p>\n<p>(iii) among the three HT distributions considered in our paper, the E-TPL\nfitting of ESDs performs the most robustly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaoqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theisen_R/0/1/0/all/0/1\">Ryan Theisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgkinson_L/0/1/0/all/0/1\">Liam Hodgkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1\">Charles H. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Design as Information Renormalization. (arXiv:1708.01525v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1708.01525","description":"<p>Here we consider some well-known facts in syntax from a physics perspective,\nallowing us to establish equivalences between both fields with many\nconsequences. Mainly, we observe that the operation MERGE, put forward by N.\nChomsky in 1995, can be interpreted as a physical information coarse-graining.\nThus, MERGE in linguistics entails information renormalization in physics,\naccording to different time scales. We make this point mathematically formal in\nterms of language models. In this setting, MERGE amounts to a probability\ntensor implementing a coarse-graining, akin to a probabilistic context-free\ngrammar. The probability vectors of meaningful sentences are given by\nstochastic tensor networks (TN) built from diagonal tensors and which are\nmostly loop-free, such as Tree Tensor Networks and Matrix Product States, thus\nbeing computationally very efficient to manipulate. We show that this implies\nthe polynomially-decaying (long-range) correlations experimentally observed in\nlanguage, and also provides arguments in favour of certain types of neural\nnetworks for language processing. Moreover, we show how to obtain such language\nmodels from quantum states that can be efficiently prepared on a quantum\ncomputer, and use this to find bounds on the perplexity of the probability\ndistribution of words in a sentence. Implications of our results are discussed\nacross several ambits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gallego_A/0/1/0/all/0/1\">Angel J. Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orus_R/0/1/0/all/0/1\">Roman Orus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNISON: Unpaired Cross-lingual Image Captioning. (arXiv:2010.01288v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.01288","description":"<p>Image captioning has emerged as an interesting research field in recent years\ndue to its broad application scenarios. The traditional paradigm of image\ncaptioning relies on paired image-caption datasets to train the model in a\nsupervised manner. However, creating such paired datasets for every target\nlanguage is prohibitively expensive, which hinders the extensibility of\ncaptioning technology and deprives a large part of the world population of its\nbenefit. In this work, we present a novel unpaired cross-lingual method to\ngenerate image captions without relying on any caption corpus in the source or\nthe target language. Specifically, our method consists of two phases: (i) a\ncross-lingual auto-encoding process, which utilizing a sentence parallel\n(bitext) corpus to learn the mapping from the source to the target language in\nthe scene graph encoding space and decode sentences in the target language, and\n(ii) a cross-modal unsupervised feature mapping, which seeks to map the encoded\nscene graph features from image modality to language modality. We verify the\neffectiveness of our proposed method on the Chinese image caption generation\ntask. The comparisons against several existing methods demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip L. H. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Analyses of Social Biases in Wikipedia Bios. (arXiv:2101.00078v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00078","description":"<p>Social biases on Wikipedia, a widely-read global platform, could greatly\ninfluence public opinion. While prior research has examined man/woman gender\nbias in biography articles, possible influences of other demographic attributes\nlimit conclusions. In this work, we present a methodology for analyzing\nWikipedia pages about people that isolates dimensions of interest (e.g.,\ngender), from other attributes (e.g., occupation). Given a target corpus for\nanalysis (e.g.~biographies about women), we present a method for constructing a\ncomparison corpus that matches the target corpus in as many attributes as\npossible, except the target one. We develop evaluation metrics to measure how\nwell the comparison corpus aligns with the target corpus and then examine how\narticles about gender and racial minorities (cis. women, non-binary people,\ntransgender women, and transgender men; African American, Asian American, and\nHispanic/Latinx American people) differ from other articles. In addition to\nidentifying suspect social biases, our results show that failing to control for\ncovariates can result in different conclusions and veil biases. Our\ncontributions include methodology that facilitates further analyses of bias in\nWikipedia articles, findings that can aid Wikipedia editors in reducing biases,\nand a framework and evaluation metrics to guide future work in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Z. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Alignment with Parallel Documents Facilitates Biomedical Machine Translation. (arXiv:2104.08588v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08588","description":"<p>Objective: Today's neural machine translation (NMT) can achieve near\nhuman-level translation quality and greatly facilitates international\ncommunications, but the lack of parallel corpora poses a key problem to the\ndevelopment of translation systems for highly specialized domains, such as\nbiomedicine. This work presents an unsupervised algorithm for deriving parallel\ncorpora from document-level translations by using sentence alignment and\nexplores how training materials affect the performance of biomedical NMT\nsystems. Materials and Methods: Document-level translations are mixed to train\nbilingual word embeddings (BWEs) for the evaluation of cross-lingual word\nsimilarity, and sentence distance is defined by combining semantic and\npositional similarities of the sentences. The alignment of sentences is\nformulated as an extended earth mover's distance problem. A Chinese-English\nbiomedical parallel corpus is derived with the proposed algorithm using\nbilingual articles from UpToDate and translations of PubMed abstracts, which is\nthen used for the training and evaluation of NMT. Results: On two manually\naligned translation datasets, the proposed algorithm achieved accurate sentence\nalignment in the 1-to-1 cases and outperformed competing algorithms in the\nmany-to-many cases. The NMT model fine-tuned on biomedical data significantly\nimproved the in-domain translation quality (zh-en: +17.72 BLEU; en-zh: +17.02\nBLEU). Both the size of the training data and the combination of different\ncorpora can significantly affect the model's performance. Conclusion: The\nproposed algorithm relaxes the assumption for sentence alignment and\neffectively generates accurate translation pairs that facilitate training high\nquality biomedical NMT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1\">Huaiyuan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-performance symbolic-numerics via multiple dispatch. (arXiv:2105.03949v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03949","description":"<p>As mathematical computing becomes more democratized in high-level languages,\nhigh-performance symbolic-numeric systems are necessary for domain scientists\nand engineers to get the best performance out of their machine without deep\nknowledge of code optimization. Naturally, users need different term types\neither to have different algebraic properties for them, or to use efficient\ndata structures. To this end, we developed Symbolics.jl, an extendable symbolic\nsystem which uses dynamic multiple dispatch to change behavior depending on the\ndomain needs. In this work we detail an underlying abstract term interface\nwhich allows for speed without sacrificing generality. We show that by\nformalizing a generic API on actions independent of implementation, we can\nretroactively add optimized data structures to our system without changing the\npre-existing term rewriters. We showcase how this can be used to optimize term\nconstruction and give a 113x acceleration on general symbolic transformations.\nFurther, we show that such a generic API allows for complementary\nterm-rewriting implementations. We demonstrate the ability to swap between\nclassical term-rewriting simplifiers and e-graph-based term-rewriting\nsimplifiers. We showcase an e-graph ruleset which minimizes the number of CPU\ncycles during expression evaluation, and demonstrate how it simplifies a\nreal-world reaction-network simulation to halve the runtime. Additionally, we\nshow a reaction-diffusion partial differential equation solver which is able to\nbe automatically converted into symbolic expressions via multiple dispatch\ntracing, which is subsequently accelerated and parallelized to give a 157x\nsimulation speedup. Together, this presents Symbolics.jl as a next-generation\nsymbolic-numeric computing environment geared towards modeling and simulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shashi Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yingbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheli_A/0/1/0/all/0/1\">Alessandro Cheli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwozdz_M/0/1/0/all/0/1\">Maja Gwozdz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1\">Viral B. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edelman_A/0/1/0/all/0/1\">Alan Edelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rackauckas_C/0/1/0/all/0/1\">Christopher Rackauckas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Explanation of Dialogue Response Generation. (arXiv:2106.06528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06528","description":"<p>In comparison to the interpretation of classification models, the explanation\nof sequence generation models is also an important problem, however it has seen\nlittle attention. In this work, we study model-agnostic explanations of a\nrepresentative text generation task -- dialogue response generation. Dialog\nresponse generation is challenging with its open-ended sentences and multiple\nacceptable responses. To gain insights into the reasoning process of a\ngeneration model, we propose a new method, local explanation of response\ngeneration (LERG) that regards the explanations as the mutual interaction of\nsegments in input and output sentences. LERG views the sequence prediction as\nuncertainty estimation of a human response and then creates explanations by\nperturbing the input and calculating the certainty change over the human\nresponse. We show that LERG adheres to desired properties of explanations for\ntext generation including unbiased approximation, consistency and cause\nidentification. Empirically, our results show that our method consistently\nimproves other widely used methods on proposed automatic- and human- evaluation\nmetrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can\nextract both explicit and implicit relations between input and output segments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1\">Connor Pryor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1\">Lise Getoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.12479","description":"<p>Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case.\nThe breakthroughs in the field are extremely task and domain-specific. Vision\nand language are dealt with in separate manners, using separate methods and\ndifferent datasets. Current text classification methods, mostly rely on\nobtaining contextual embeddings for input text samples, then training a\nclassifier on the embedded dataset. Transfer learning in Language-related tasks\nin general, is heavily used in obtaining the contextual text embeddings for the\ninput samples. In this work, we propose to use the knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. A data transformation technique is used to\ncreate a new image dataset, where each image represents a sentence embedding\nfrom the last six layers of BERT, projected on a 2D plane using a t-SNE based\nmethod. We trained five models containing early layers sliced from vision\nmodels which are pretrained on ImageNet, on the created image dataset for the\nIMDB dataset embedded with the last six layers of BERT. Despite the challenges\nposed by the very different datasets, experimental results achieved by this\napproach which links large pretrained models on both language and vision, are\nvery promising, without employing compute resources. Specifically, Sentiment\nAnalysis is achieved by five different models on the same image dataset\nobtained after BERT embeddings are transformed into gray scale images.\n</p>\n<p>Index Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image\nclassification, Natural Language Processing, t-SNE, text classification,\nTransfer Learning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07445","description":"<p>Transformer-based pre-trained language models like BERT and its variants have\nrecently achieved promising performance in various natural language processing\n(NLP) tasks. However, the conventional paradigm constructs the backbone by\npurely stacking the manually designed global self-attention layers, introducing\ninductive bias and thus leads to sub-optimal. In this work, we make the first\nattempt to automatically discover novel pre-trained language model (PLM)\nbackbone on a flexible search space containing the most fundamental operations\nfrom scratch. Specifically, we propose a well-designed search space which (i)\ncontains primitive math operations in the intra-layer level to explore novel\nattention structures, and (ii) leverages convolution blocks to be the\nsupplementary for attentions in the inter-layer level to better learn local\ndependency. To enhance the efficiency for finding promising architectures, we\npropose an Operation-Priority Neural Architecture Search (OP-NAS) algorithm,\nwhich optimizes both the search algorithm and evaluation of candidate models.\nSpecifically, we propose Operation-Priority (OP) evolution strategy to\nfacilitate model search via balancing exploration and exploitation.\nFurthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for\nfast model evaluation. Extensive experiments show that the searched\narchitecture (named AutoBERT-Zero) significantly outperforms BERT and its\nvariants of different model capacities in various downstream tasks, proving the\narchitecture's transfer and scaling abilities. Remarkably, AutoBERT-Zero-base\noutperforms RoBERTa-base (using much more data) and BERT-large (with much\nlarger model size) by 2.4 and 1.4 higher score on GLUE test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Han Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip L.H. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Adapter Based Pre-Training for Efficient and Scalable Self-Supervised Speech Representation Learning. (arXiv:2107.13530v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2107.13530","description":"<p>We present a method for transferring pre-trained self-supervised (SSL) speech\nrepresentations to multiple languages. There is an abundance of unannotated\nspeech, so creating self-supervised representations from raw audio and\nfine-tuning on small annotated datasets is a promising direction to build\nspeech recognition systems. SSL models generally perform SSL on raw audio in a\npre-training phase and then fine-tune on a small fraction of annotated data.\nSuch models have produced state of the art results for ASR. However, these\nmodels are very expensive to pre-train. We use an existing wav2vec 2.0 model\nand tackle the problem of learning new language representations while utilizing\nexisting model knowledge. Crucially we do so without catastrophic forgetting of\nthe existing language representation. We use adapter modules to speed up\npre-training a new language task. Our model can decrease pre-training times by\n32% when learning a new language task, and learn this new audio-language\nrepresentation without forgetting previous language representation. We evaluate\nby applying these language representations to automatic speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kessler_S/0/1/0/all/0/1\">Samuel Kessler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_B/0/1/0/all/0/1\">Bethan Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Low-Resource Machine Translation. (arXiv:2109.00486v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00486","description":"<p>We present a survey covering the state of the art in low-resource machine\ntranslation research. There are currently around 7000 languages spoken in the\nworld and almost all language pairs lack significant resources for training\nmachine translation models. There has been increasing interest in research\naddressing the challenge of producing useful translation models when very\nlittle translated training data is available. We present a summary of this\ntopical research field and provide a description of the techniques evaluated by\nresearchers in several recent shared tasks in low-resource MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barone_A/0/1/0/all/0/1\">Antonio Valerio Miceli Barone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helcl_J/0/1/0/all/0/1\">Jind&#x159;ich Helcl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative. (arXiv:2109.07437v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.07437","description":"<p>In most settings of practical concern, machine learning practitioners know in\nadvance what end-task they wish to boost with auxiliary tasks. However, widely\nused methods for leveraging auxiliary data like pre-training and its\ncontinued-pretraining variant are end-task agnostic: they rarely, if ever,\nexploit knowledge of the target task. We study replacing end-task agnostic\ncontinued training of pre-trained language models with end-task aware training\nof said models. We argue that for sufficiently important end-tasks, the\nbenefits of leveraging auxiliary data in a task-aware fashion can justify\nforgoing the traditional approach of obtaining generic, end-task agnostic\nrepresentations as with (continued) pre-training. On three different\nlow-resource NLP tasks from two domains, we demonstrate that multi-tasking the\nend-task and auxiliary objectives results in significantly better downstream\ntask performance than the widely-used task-agnostic continued pre-training\nparadigm of Gururangan et al. (2020). We next introduce an online meta-learning\nalgorithm that learns a set of multi-task weights to better balance among our\nmultiple auxiliary objectives, achieving further improvements on end-task\nperformance and data efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1\">Lucio M. Dery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1\">Paul Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Case for Claim Difficulty Assessment in Automatic Fact Checking. (arXiv:2109.09689v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09689","description":"<p>Fact-checking is the process of evaluating the veracity of claims (i.e.,\npurported facts). In this opinion piece, we raise an issue that has received\nlittle attention in prior work -- that some claims are far more difficult to\nfact-check than others. We discuss the implications this has for both practical\nfact-checking and research on automated fact-checking, including task\nformulation and dataset design. We report a manual analysis undertaken to\nexplore factors underlying varying claim difficulty and identify several\ndistinct types of difficulty. We motivate this new claim difficulty prediction\ntask as beneficial to both automated fact-checking and practical fact-checking\norganizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prakhar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anubrata Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLAVA: A Foundational Language And Vision Alignment Model. (arXiv:2112.04482v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04482","description":"<p>State-of-the-art vision and vision-and-language models rely on large-scale\nvisio-linguistic pretraining for obtaining good performance on a variety of\ndownstream tasks. Generally, such models are often either cross-modal\n(contrastive) or multi-modal (with earlier fusion) but not both; and they often\nonly target specific modalities or tasks. A promising direction would be to use\na single holistic universal model, as a \"foundation\", that targets all\nmodalities at once -- a true vision and language foundation model should be\ngood at vision tasks, language tasks, and cross- and multi-modal vision and\nlanguage tasks. We introduce FLAVA as such a model and demonstrate impressive\nperformance on a wide range of 35 tasks spanning these target modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1\">Guillaume Couairon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1\">Wojciech Galuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selecting Parallel In-domain Sentences for Neural Machine Translation Using Monolingual Texts. (arXiv:2112.06096v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06096","description":"<p>Continuously-growing data volumes lead to larger generic models. Specific\nuse-cases are usually left out, since generic models tend to perform poorly in\ndomain-specific cases. Our work addresses this gap with a method for selecting\nin-domain data from generic-domain (parallel text) corpora, for the task of\nmachine translation. The proposed method ranks sentences in parallel\ngeneral-domain data according to their cosine similarity with a monolingual\ndomain-specific data set. We then select the top K sentences with the highest\nsimilarity score to train a new machine translation system tuned to the\nspecific in-domain data. Our experimental results show that models trained on\nthis in-domain data outperform models trained on generic or a mixture of\ngeneric and domain data. That is, our method selects high-quality\ndomain-specific training instances at low computational cost and data size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharami_J/0/1/0/all/0/1\">Javad Pourmostafa Roshan Sharami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shterionov_D/0/1/0/all/0/1\">Dimitar Shterionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spronck_P/0/1/0/all/0/1\">Pieter Spronck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Key Multimodal Backdoors for Visual Question Answering. (arXiv:2112.07668v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07668","description":"<p>The success of deep learning has enabled advances in multimodal tasks that\nrequire non-trivial fusion of multiple input domains. Although multimodal\nmodels have shown potential in many problems, their increased complexity makes\nthem more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of\nsecurity vulnerability wherein an attacker embeds a malicious secret behavior\ninto a network (e.g. targeted misclassification) that is activated when an\nattacker-specified trigger is added to an input. In this work, we show that\nmultimodal networks are vulnerable to a novel type of attack that we refer to\nas Dual-Key Multimodal Backdoors. This attack exploits the complex fusion\nmechanisms used by state-of-the-art networks to embed backdoors that are both\neffective and stealthy. Instead of using a single trigger, the proposed attack\nembeds a trigger in each of the input modalities and activates the malicious\nbehavior only when both the triggers are present. We present an extensive study\nof multimodal backdoors on the Visual Question Answering (VQA) task with\nmultiple architectures and visual feature backbones. A major challenge in\nembedding backdoors in VQA models is that most models use visual features\nextracted from a fixed pretrained object detector. This is challenging for the\nattacker as the detector can distort or ignore the visual trigger entirely,\nwhich leads to models where backdoors are over-reliant on the language trigger.\nWe tackle this problem by proposing a visual trigger optimization strategy\ndesigned for pretrained object detectors. Through this method, we create\nDual-Key Backdoors with over a 98% attack success rate while only poisoning 1%\nof the training data. Finally, we release TrojVQA, a large collection of clean\nand trojan VQA models to enable research in defending against multimodal\nbackdoors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walmer_M/0/1/0/all/0/1\">Matthew Walmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1\">Karan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sur_I/0/1/0/all/0/1\">Indranil Sur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADBCMM : Acronym Disambiguation by Building Counterfactuals and Multilingual Mixing. (arXiv:2112.08991v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08991","description":"<p>Scientific documents often contain a large number of acronyms. Disambiguation\nof these acronyms will help researchers better understand the meaning of\nvocabulary in the documents. In the past, thanks to large amounts of data from\nEnglish literature, acronym task was mainly applied in English literature.\nHowever, for other low-resource languages, this task is difficult to obtain\ngood performance and receives less attention due to the lack of large amount of\nannotation data. To address the above issue, this paper proposes an new method\nfor acronym disambiguation, named as ADBCMM, which can significantly improve\nthe performance of low-resource languages by building counterfactuals and\nmultilingual mixing. Specifically, by balancing data bias in low-resource\nlangauge, ADBCMM will able to improve the test performance outside the data\nset. In SDU@AAAI-22 - Shared Task 2: Acronym Disambiguation, the proposed\nmethod won first place in French and Spanish. You can repeat our results here\nhttps://github.com/WENGSYX/ADBCMM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiusheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"This Must Be the Place: Predicting Engagement of Online Communities in a Large-scale Distributed Campaign. (arXiv:2201.05334v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2201.05334","description":"<p>Understanding collective decision making at a large-scale, and elucidating\nhow community organization and community dynamics shape collective behavior are\nat the heart of social science research. In this work we study the behavior of\nthousands of communities with millions of active members. We define a novel\ntask: predicting which community will undertake an unexpected, large-scale,\ndistributed campaign.\n</p>\n<p>To this end, we develop a hybrid model, combining textual cues, community\nmeta-data, and structural properties. We show how this multi-faceted model can\naccurately predict large-scale collective decision-making in a distributed\nenvironment. We demonstrate the applicability of our model through Reddit's\nr/place - a large-scale online experiment in which millions of users,\nself-organized in thousands of communities, clashed and collaborated in an\neffort to realize their agenda.\n</p>\n<p>Our hybrid model achieves a high F1 prediction score of 0.826. We find that\ncoarse meta-features are as important for prediction accuracy as fine-grained\ntextual cues, while explicit structural features play a smaller role.\nInterpreting our model, we provide and support various social insights about\nthe unique characteristics of the communities that participated in the \\r/place\nexperiment.\n</p>\n<p>Our results and analysis shed light on the complex social dynamics that drive\ncollective behavior, and on the factors that propel user coordination. The\nscale and the unique conditions of the \\rp~experiment suggest that our findings\nmay apply in broader contexts, such as online activism, (countering) the spread\nof hate speech and reducing political polarization. The broader applicability\nof the model is demonstrated through an extensive analysis of the\nWallStreetBets community, their role in r/place and four years later, in the\nGameStop short squeeze campaign of 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Israeli_A/0/1/0/all/0/1\">Abraham Israeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kremiansky_A/0/1/0/all/0/1\">Alexander Kremiansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsur_O/0/1/0/all/0/1\">Oren Tsur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Continual Learning for Spoken Keyword Spotting. (arXiv:2201.12546v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12546","description":"<p>Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. To tackle such challenges, we propose a\nprogressive continual learning strategy for small-footprint spoken keyword\nspotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a\nnetwork instantiator to generate the task-specific sub-networks for remembering\npreviously learned keywords. As a result, the PCL-KWS approach incrementally\nlearns new keywords without forgetting prior knowledge. Besides, the\nkeyword-aware network scaling mechanism of PCL-KWS constrains the growth of\nmodel parameters while achieving high performance. Experimental results show\nthat after learning five new tasks sequentially, our proposed PCL-KWS approach\narchives the new state-of-the-art performance of 92.8% average accuracy for all\nthe tasks on Google Speech Command dataset compared with other baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yizheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_N/0/1/0/all/0/1\">Nana Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training. (arXiv:2201.12723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12723","description":"<p>Vision-and-language pre-trained models (VLMs) have achieved tremendous\nsuccess in the cross-modal area, but most of them require a large amount of\nparallel image-caption data for pre-training. Collating such data is expensive\nand labor-intensive. In this work, we focus on reducing such need for\ngenerative vision-and-language pre-training (G-VLP) by taking advantage of the\nvisual pre-trained model (CLIP-ViT) as encoder and language pre-trained model\n(GPT2) as decoder. Unfortunately, GPT2 lacks a necessary cross-attention\nmodule, which hinders the direct connection of CLIP-ViT and GPT2. To remedy\nsuch defects, we conduct extensive experiments to empirically investigate how\nto design and pre-train our model. Based on our experimental results, we\npropose a novel G-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train\nit with a small-scale image-caption corpus (Visual Genome, only 110k distinct\nimages). Evaluating on the image captioning downstream tasks (MSCOCO and\nFlickr30k Captioning), VC-GPT achieves either the best or the second-best\nperformance across all evaluation metrics over the previous works which consume\naround 30 times more distinct images during cross-modal pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers. (arXiv:2202.00120v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00120","description":"<p>The ability to have the same experience for different user groups (i.e.,\naccessibility) is one of the most important characteristics of Web-based\nsystems. The same is true for Knowledge Graph Question Answering (KGQA) systems\nthat provide the access to Semantic Web data via natural language interface.\nWhile following our research agenda on the multilingual aspect of accessibility\nof KGQA systems, we identified several ongoing challenges. One of them is the\nlack of multilingual KGQA benchmarks. In this work, we extend one of the most\npopular KGQA benchmarks - QALD-9 by introducing high-quality questions'\ntranslations to 8 languages provided by native speakers, and transferring the\nSPARQL queries of QALD-9 from DBpedia to Wikidata, s.t., the usability and\nrelevance of the dataset is strongly increased. Five of the languages -\nArmenian, Ukrainian, Lithuanian, Bashkir and Belarusian - to our best knowledge\nwere never considered in KGQA research community before. The latter two of the\nlanguages are considered as \"endangered\" by UNESCO. We call the extended\ndataset QALD-9-plus and made it available online\nhttps://github.com/Perevalov/qald_9_plus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perevalov_A/0/1/0/all/0/1\">Aleksandr Perevalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diefenbach_D/0/1/0/all/0/1\">Dennis Diefenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Both_A/0/1/0/all/0/1\">Andreas Both</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Multi-Talker ASR with Token-Level Serialized Output Training. (arXiv:2202.00842v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.00842","description":"<p>This paper proposes a token-level serialized output training (t-SOT), a novel\nframework for streaming multi-talker automatic speech recognition (ASR). Unlike\nexisting streaming multi-talker ASR models using multiple output layers, the\nt-SOT model has only a single output layer that generates recognition tokens\n(e.g., words, subwords) of multiple speakers in chronological order based on\ntheir emission times. A special token that indicates the change of \"virtual\"\noutput channels is introduced to keep track of the overlapping utterances.\nCompared to the prior streaming multi-talker ASR models, the t-SOT model has\nthe advantages of less inference cost and a simpler model architecture.\nMoreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the\nt-SOT-based transformer transducer model achieves the state-of-the-art word\nerror rates by a significant margin to the prior results. For non-overlapping\nspeech, the t-SOT model is on par with a single-talker ASR model in terms of\nboth accuracy and computational cost, opening the door for deploying one model\nfor both single- and multi-talker scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Knowledge Integration in Language Models with Graph Convolutions. (arXiv:2202.00964v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00964","description":"<p>Pretrained language models (LMs) do not capture factual knowledge very well.\nThis has led to the development of a number of knowledge integration (KI)\nmethods which aim to incorporate external knowledge into pretrained LMs. Even\nthough KI methods show some performance gains over vanilla LMs, the\ninner-workings of these methods are not well-understood. For instance, it is\nunclear how and what kind of knowledge is effectively integrated into these\nmodels and if such integration may lead to catastrophic forgetting of already\nlearned knowledge. This paper revisits the KI process in these models with an\ninformation-theoretic view and shows that KI can be interpreted using a graph\nconvolution operation. We propose a probe model called \\textit{Graph\nConvolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and\nexposing what kind of knowledge is integrated into these models. We conduct\nexperiments to verify that our GCS can indeed be used to correctly interpret\nthe KI process, and we use it to analyze two well-known knowledge-enhanced LMs:\nERNIE and K-Adapter, and find that only a small amount of factual knowledge is\nintegrated in them. We stratify knowledge in terms of various relation types\nand find that ERNIE and K-Adapter integrate different kinds of knowledge to\ndifferent extent. Our analysis also shows that simply increasing the size of\nthe KI corpus may not lead to better KI; fundamental advances may be needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guoji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RescoreBERT: Discriminative Speech Recognition Rescoring with BERT. (arXiv:2202.01094v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.01094","description":"<p>Second-pass rescoring is an important component in automatic speech\nrecognition (ASR) systems that is used to improve the outputs from a first-pass\ndecoder by implementing a lattice rescoring or $n$-best re-ranking. While\npretraining with a masked language model (MLM) objective has received great\nsuccess in various natural language understanding (NLU) tasks, it has not\ngained traction as a rescoring model for ASR. Specifically, training a\nbidirectional model like BERT on a discriminative objective such as minimum WER\n(MWER) has not been explored. Here we show how to train a BERT-based rescoring\nmodel with MWER loss, to incorporate the improvements of a discriminative loss\ninto fine-tuning of deep bidirectional pretrained models for ASR. Specifically,\nwe propose a fusion strategy that incorporates the MLM into the discriminative\ntraining process to effectively distill knowledge from a pretrained model. We\nfurther propose an alternative discriminative loss. We name this approach\nRescoreBERT. On the LibriSpeech corpus, it reduces WER by 6.6%/3.4% relative on\nclean/other test sets over a BERT baseline without discriminative objective. We\nalso evaluate our method on an internal dataset from a conversational agent and\nfind that it reduces both latency and WER (by 3 to 8% relative) over an LSTM\nrescoring model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolehmainen_J/0/1/0/all/0/1\">Jari Kolehmainen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Boundary-aware Information Maximization for Self-supervised Medical Image Segmentation. (arXiv:2202.02371v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02371","description":"<p>Unsupervised pre-training has been proven as an effective approach to boost\nvarious downstream tasks given limited labeled data. Among various methods,\ncontrastive learning learns a discriminative representation by constructing\npositive and negative pairs. However, it is not trivial to build reasonable\npairs for a segmentation task in an unsupervised way. In this work, we propose\na novel unsupervised pre-training framework that avoids the drawback of\ncontrastive learning. Our framework consists of two principles: unsupervised\nover-segmentation as a pre-train task using mutual information maximization and\nboundary-aware preserving learning. Experimental results on two benchmark\nmedical segmentation datasets reveal our method's effectiveness in improving\nsegmentation performance when few annotated images are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_J/0/1/0/all/0/1\">Jizong Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Automated Tree Topology Estimation and Artery-Vein Classification. (arXiv:2202.02382v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02382","description":"<p>We present a fully automatic technique for extracting the retinal vascular\ntopology, i.e., how the different vessels are connected to each other, given a\nsingle color fundus image. Determining this connectivity is very challenging\nbecause vessels cross each other in a 2D image, obscuring their true paths. We\nvalidated the usefulness of our extraction method by using it to achieve\nstate-of-the-art results in retinal artery-vein classification.\n</p>\n<p>Our proposed approach works as follows. We first segment the retinal vessels\nusing our previously developed state-of-the-art segmentation method. Then, we\nestimate an initial graph from the extracted vessels and assign the most likely\nblood flow to each edge. We then use a handful of high-level operations (HLOs)\nto fix errors in the graph. These HLOs include detaching neighboring nodes,\nshifting the endpoints of an edge, and reversing the estimated blood flow\ndirection for a branch. We use a novel cost function to find the optimal set of\nHLO operations for a given graph. Finally, we show that our extracted vascular\nstructure is correct by propagating artery/vein labels along the branches. As\nour experiments show, our topology-based artery-vein labeling achieved\nstate-of-the-art results on multiple datasets. We also performed several\nablation studies to verify the importance of the different components of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khanal_A/0/1/0/all/0/1\">Aashis Khanal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motevali_S/0/1/0/all/0/1\">Saeid Motevali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Estrada_R/0/1/0/all/0/1\">Rolando Estrada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StandardSim: A Synthetic Dataset For Retail Environments. (arXiv:2202.02418v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02418","description":"<p>Autonomous checkout systems rely on visual and sensory inputs to carry out\nfine-grained scene understanding in retail environments. Retail environments\npresent unique challenges compared to typical indoor scenes owing to the vast\nnumber of densely packed, unique yet similar objects. The problem becomes even\nmore difficult when only RGB input is available, especially for data-hungry\ntasks such as instance segmentation. To address the lack of datasets for\nretail, we present StandardSim, a large-scale photorealistic synthetic dataset\nfeaturing annotations for semantic segmentation, instance segmentation, depth\nestimation, and object detection. Our dataset provides multiple views per\nscene, enabling multi-view representation learning. Further, we introduce a\nnovel task central to autonomous checkout called change detection, requiring\npixel-level classification of takes, puts and shifts in objects over time. We\nbenchmark widely-used models for segmentation and depth estimation on our\ndataset, show that our test set constitutes a difficult benchmark compared to\ncurrent smaller-scale datasets and that our training set provides models with\ncrucial information for autonomous checkout tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mata_C/0/1/0/all/0/1\">Cristina Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locascio_N/0/1/0/all/0/1\">Nick Locascio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_M/0/1/0/all/0/1\">Mohammed Azeem Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kihara_K/0/1/0/all/0/1\">Kenny Kihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischetti_D/0/1/0/all/0/1\">Dan Fischetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The influence of labeling techniques in classifying human manipulation movement of different speed. (arXiv:2202.02426v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02426","description":"<p>In this work, we investigate the influence of labeling methods on the\nclassification of human movements on data recorded using a marker-based motion\ncapture system. The dataset is labeled using two different approaches, one\nbased on video data of the movements, the other based on the movement\ntrajectories recorded using the motion capture system. The dataset is labeled\nusing two different approaches, one based on video data of the movements, the\nother based on the movement trajectories recorded using the motion capture\nsystem. The data was recorded from one participant performing a stacking\nscenario comprising simple arm movements at three different speeds (slow,\nnormal, fast). Machine learning algorithms that include k-Nearest Neighbor,\nRandom Forest, Extreme Gradient Boosting classifier, Convolutional Neural\nnetworks (CNN), Long Short-Term Memory networks (LSTM), and a combination of\nCNN-LSTM networks are compared on their performance in recognition of these arm\nmovements. The models were trained on actions performed on slow and normal\nspeed movements segments and generalized on actions consisting of fast-paced\nhuman movement. It was observed that all the models trained on normal-paced\ndata labeled using trajectories have almost 20% improvement in accuracy on test\ndata in comparison to the models trained on data labeled using videos of the\nperformed experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Sadique Adnan Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutzeit_L/0/1/0/all/0/1\">Lisa Gutzeit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchner_F/0/1/0/all/0/1\">Frank Kirchner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stratification of carotid atheromatous plaque using interpretable deep learning methods on B-mode ultrasound images. (arXiv:2202.02428v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02428","description":"<p>Carotid atherosclerosis is the major cause of ischemic stroke resulting in\nsignificant rates of mortality and disability annually. Early diagnosis of such\ncases is of great importance, since it enables clinicians to apply a more\neffective treatment strategy. This paper introduces an interpretable\nclassification approach of carotid ultrasound images for the risk assessment\nand stratification of patients with carotid atheromatous plaque. To address the\nhighly imbalanced distribution of patients between the symptomatic and\nasymptomatic classes (16 vs 58, respectively), an ensemble learning scheme\nbased on a sub-sampling approach was applied along with a two-phase,\ncost-sensitive strategy of learning, that uses the original and a resampled\ndata set. Convolutional Neural Networks (CNNs) were utilized for building the\nprimary models of the ensemble. A six-layer deep CNN was used to automatically\nextract features from the images, followed by a classification stage of two\nfully connected layers. The obtained results (Area Under the ROC Curve (AUC):\n73%, sensitivity: 75%, specificity: 70%) indicate that the proposed approach\nachieved acceptable discrimination performance. Finally, interpretability\nmethods were applied on the model's predictions in order to reveal insights on\nthe model's decision process as well as to enable the identification of novel\nimage biomarkers for the stratification of patients with carotid atheromatous\nplaque.Clinical Relevance-The integration of interpretability methods with deep\nlearning strategies can facilitate the identification of novel ultrasound image\nbiomarkers for the stratification of patients with carotid atheromatous plaque.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ganitidis_T/0/1/0/all/0/1\">Theofanis Ganitidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Athanasiou_M/0/1/0/all/0/1\">Maria Athanasiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalakleidi_K/0/1/0/all/0/1\">Kalliopi Dalakleidi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Melanitis_N/0/1/0/all/0/1\">Nikos Melanitis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golemati_S/0/1/0/all/0/1\">Spyretta Golemati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nikita_K/0/1/0/all/0/1\">Konstantina S Nikita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation. (arXiv:2202.02440v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02440","description":"<p>In reinforcement learning for visual navigation, it is common to develop a\nmodel for each new task, and train that model from scratch with task-specific\ninteractions in 3D environments. However, this process is expensive; massive\namounts of interactions are needed for the model to generalize well. Moreover,\nthis process is repeated whenever there is a change in the task type or the\ngoal modality. We present a unified approach to visual navigation using a novel\nmodular transfer learning model. Our model can effectively leverage its\nexperience from one source task and apply it to multiple target tasks (e.g.,\nObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch,\naudio, label). Furthermore, our model enables zero-shot experience learning,\nwhereby it can solve the target tasks without receiving any task-specific\ninteractive training. Our experiments on multiple photorealistic datasets and\nchallenging tasks show that our approach learns faster, generalizes better, and\noutperforms SoTA models by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Halah_Z/0/1/0/all/0/1\">Ziad Al-Halah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh K. Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Method for Functional Assessment of Retinal Models. (arXiv:2202.02443v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02443","description":"<p>Challenges in the field of retinal prostheses motivate the development of\nretinal models to accurately simulate Retinal Ganglion Cells (RGCs) responses.\nThe goal of retinal prostheses is to enable blind individuals to solve complex,\nreallife visual tasks. In this paper, we introduce the functional assessment\n(FA) of retinal models, which describes the concept of evaluating the\nperformance of retinal models on visual understanding tasks. We present a\nmachine learning method for FA: we feed traditional machine learning\nclassifiers with RGC responses generated by retinal models, to solve object and\ndigit recognition tasks (CIFAR-10, MNIST, Fashion MNIST, Imagenette). We\nexamined critical FA aspects, including how the performance of FA depends on\nthe task, how to optimally feed RGC responses to the classifiers and how the\nnumber of output neurons correlates with the model's accuracy. To increase the\nnumber of output neurons, we manipulated input images - by splitting and then\nfeeding them to the retinal model and we found that image splitting does not\nsignificantly improve the model's accuracy. We also show that differences in\nthe structure of datasets result in largely divergent performance of the\nretinal model (MNIST and Fashion MNIST exceeded 80% accuracy, while CIFAR-10\nand Imagenette achieved ~40%). Furthermore, retinal models which perform better\nin standard evaluation, i.e. more accurately predict RGC response, perform\nbetter in FA as well. However, unlike standard evaluation, FA results can be\nstraightforwardly interpreted in the context of comparing the quality of visual\nperception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Papadopoulos_N/0/1/0/all/0/1\">Nikolas Papadopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Melanitis_N/0/1/0/all/0/1\">Nikos Melanitis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lozano_A/0/1/0/all/0/1\">Antonio Lozano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soto_Sanchez_C/0/1/0/all/0/1\">Cristina Soto-Sanchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_E/0/1/0/all/0/1\">Eduardo Fernandez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nikita_K/0/1/0/all/0/1\">Konstantina S Nikita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spelunking the Deep: Guaranteed Queries for General Neural Implicit Surfaces. (arXiv:2202.02444v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02444","description":"<p>Neural implicit representations, which encode a surface as the level set of a\nneural network applied to spatial coordinates, have proven to be remarkably\neffective for optimizing, compressing, and generating 3D geometry. Although\nthese representations are easy to fit, it is not clear how to best evaluate\ngeometric queries on the shape, such as intersecting against a ray or finding a\nclosest point. The predominant approach is to encourage the network to have a\nsigned distance property. However, this property typically holds only\napproximately, leading to robustness issues, and holds only at the conclusion\nof training, inhibiting the use of queries in loss functions. Instead, this\nwork presents a new approach to perform queries directly on general neural\nimplicit functions for a wide range of existing architectures. Our key tool is\nthe application of range analysis to neural networks, using automatic\narithmetic rules to bound the output of a network over a region; we conduct a\nstudy of range analysis on neural networks, and identify variants of affine\narithmetic which are highly effective. We use the resulting bounds to develop\ngeometric queries including ray casting, intersection testing, constructing\nspatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating\nbulk properties, and more. Our queries can be efficiently evaluated on GPUs,\nand offer concrete accuracy guarantees even on randomly-initialized networks,\nenabling their use in training objectives and beyond. We also show a\npreliminary application to inverse rendering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharp_N/0/1/0/all/0/1\">Nicholas Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1\">Alec Jacobson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Learning as Cluster-induced Voronoi Diagrams: A Geometric Approach. (arXiv:2202.02471v1 [cs.LG])","link":"http://arxiv.org/abs/2202.02471","description":"<p>Few-shot learning (FSL) is the process of rapid generalization from abundant\nbase samples to inadequate novel samples. Despite extensive research in recent\nyears, FSL is still not yet able to generate satisfactory solutions for a wide\nrange of real-world applications. To confront this challenge, we study the FSL\nproblem from a geometric point of view in this paper. One observation is that\nthe widely embraced ProtoNet model is essentially a Voronoi Diagram (VD) in the\nfeature space. We retrofit it by making use of a recent advance in\ncomputational geometry called Cluster-induced Voronoi Diagram (CIVD). Starting\nfrom the simplest nearest neighbor model, CIVD gradually incorporates\ncluster-to-point and then cluster-to-cluster relationships for space\nsubdivision, which is used to improve the accuracy and robustness at multiple\nstages of FSL. Specifically, we use CIVD (1) to integrate parametric and\nnonparametric few-shot classifiers; (2) to combine feature representation and\nsurrogate representation; (3) and to leverage feature-level,\ntransformation-level, and geometry-level heterogeneities for a better ensemble.\nOur CIVD-based workflow enables us to achieve new state-of-the-art results on\nmini-ImageNet, CUB, and tiered-ImagenNet datasets, with ${\\sim}2\\%{-}5\\%$\nimprovements upon the next best. To summarize, CIVD provides a mathematically\nelegant and geometrically interpretable framework that compensates for extreme\ndata insufficiency, prevents overfitting, and allows for fast geometric\nensemble for thousands of individual VD. These together make FSL stronger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chunwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhui Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification. (arXiv:2202.02472v1 [eess.SP])","link":"http://arxiv.org/abs/2202.02472","description":"<p>Deep learning (DL) has been widely investigated in a vast majority of\napplications in electroencephalography (EEG)-based brain-computer interfaces\n(BCIs), especially for motor imagery (MI) classification in the past five\nyears. The mainstream DL methodology for the MI-EEG classification exploits the\ntemporospatial patterns of EEG signals using convolutional neural networks\n(CNNs), which have been particularly successful in visual images. However,\nsince the statistical characteristics of visual images may not benefit EEG\nsignals, a natural question that arises is whether there exists an alternative\nnetwork architecture despite CNNs to extract features for the MI-EEG\nclassification. To address this question, we propose a novel geometric deep\nlearning (GDL) framework called Tensor-CSPNet to characterize EEG signals on\nsymmetric positive definite (SPD) manifolds and exploit the\ntemporo-spatio-frequential patterns using deep neural networks on SPD\nmanifolds. Meanwhile, many experiences of successful MI-EEG classifiers have\nbeen integrated into the Tensor-CSPNet framework to make it more efficient. In\nthe experiments, Tensor-CSPNet attains or slightly outperforms the current\nstate-of-the-art performance on the cross-validation and holdout scenarios of\ntwo MI-EEG datasets. The visualization and interpretability analyses also\nexhibit its validity for the MI-EEG classification. To conclude, we provide a\nfeasible answer to the question by generalizing the previous DL methodologies\non SPD manifolds, which indicates the start of a specific class from the GDL\nmethodology for the MI-EEG classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ju_C/0/1/0/all/0/1\">Ce Ju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Challenges of Class Imbalance and Scale Variation in Object Detection in Aerial Images. (arXiv:2202.02489v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02489","description":"<p>While object detection is a common problem in computer vision, it is even\nmore challenging when dealing with aerial satellite images. The variety in\nobject scales and orientations can make them difficult to identify. In\naddition, there can be large amounts of densely packed small objects such as\ncars. In this project, we propose a few changes to the Faster-RCNN\narchitecture. First, we experiment with different backbones to extract better\nfeatures. We also modify the data augmentations and generated anchor sizes for\nregion proposals in order to better handle small objects. Finally, we\ninvestigate the effects of different loss functions. Our proposed design\nachieves an improvement of 4.7 mAP over the baseline which used a vanilla\nFaster R-CNN with a ResNet-101 FPN backbone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elhagry_A/0/1/0/all/0/1\">Ahmed Elhagry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_M/0/1/0/all/0/1\">Mohamed Saeed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Detector with Robust Classifier. (arXiv:2202.02503v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02503","description":"<p>Deep neural network (DNN) models are wellknown to easily misclassify\nprediction results by using input images with small perturbations, called\nadversarial examples. In this paper, we propose a novel adversarial detector,\nwhich consists of a robust classifier and a plain one, to highly detect\nadversarial examples. The proposed adversarial detector is carried out in\naccordance with the logits of plain and robust classifiers. In an experiment,\nthe proposed detector is demonstrated to outperform a state-of-the-art detector\nwithout any robust classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osakabe_T/0/1/0/all/0/1\">Takayuki Osakabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aprilpyone_M/0/1/0/all/0/1\">Maungmaung Aprilpyone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1\">Sayaka Shiota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Reversible Steganography with Uncertainty-Aware Predictive Analytics. (arXiv:2202.02518v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02518","description":"<p>Artificial neural networks have advanced the frontiers of reversible\nsteganography. The core strength of neural networks is the ability to render\naccurate predictions for a bewildering variety of data. Residual modulation is\nrecognised as the most advanced reversible steganographic algorithm for digital\nimages and the pivot of which is the predictive module. The function of this\nmodule is to predict pixel intensity given some pixel-wise contextual\ninformation. This task can be perceived as a low-level vision problem and hence\nneural networks for addressing a similar class of problems can be deployed. On\ntop of the prior art, this paper analyses the predictive uncertainty and endows\nthe predictive module with the option to abstain when encountering a high level\nof uncertainty. Uncertainty analysis can be formulated as a pixel-level binary\nclassification problem and tackled by both supervised and unsupervised\nlearning. In contrast to handcrafted statistical analytics, learning-based\nanalytics can learn to follow some general statistical principles and\nsimultaneously adapt to a specific predictor. Experimental results show that\nsteganographic performance can be remarkably improved by adaptively filtering\nout the unpredictable regions with the learning-based uncertainty analysers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Ching-Chun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sisheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative study of 3D object detection frameworks based on LiDAR data and sensor fusion techniques. (arXiv:2202.02521v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02521","description":"<p>Estimating and understanding the surroundings of the vehicle precisely forms\nthe basic and crucial step for the autonomous vehicle. The perception system\nplays a significant role in providing an accurate interpretation of a vehicle's\nenvironment in real-time. Generally, the perception system involves various\nsubsystems such as localization, obstacle (static and dynamic) detection, and\navoidance, mapping systems, and others. For perceiving the environment, these\nvehicles will be equipped with various exteroceptive (both passive and active)\nsensors in particular cameras, Radars, LiDARs, and others. These systems are\nequipped with deep learning techniques that transform the huge amount of data\nfrom the sensors into semantic information on which the object detection and\nlocalization tasks are performed. For numerous driving tasks, to provide\naccurate results, the location and depth information of a particular object is\nnecessary. 3D object detection methods, by utilizing the additional pose data\nfrom the sensors such as LiDARs, stereo cameras, provides information on the\nsize and location of the object. Based on recent research, 3D object detection\nframeworks performing object detection and localization on LiDAR data and\nsensor fusion techniques show significant improvement in their performance. In\nthis work, a comparative study of the effect of using LiDAR data for object\ndetection frameworks and the performance improvement seen by using sensor\nfusion techniques are performed. Along with discussing various state-of-the-art\nmethods in both the cases, performing experimental analysis, and providing\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venugopala_S/0/1/0/all/0/1\">Sreenivasa Hikkal Venugopala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrivPAS: A real time Privacy-Preserving AI System and applied ethics. (arXiv:2202.02524v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02524","description":"<p>With 3.78 billion social media users worldwide in 2021 (48% of the human\npopulation), almost 3 billion images are shared daily. At the same time, a\nconsistent evolution of smartphone cameras has led to a photography explosion\nwith 85% of all new pictures being captured using smartphones. However, lately,\nthere has been an increased discussion of privacy concerns when a person being\nphotographed is unaware of the picture being taken or has reservations about\nthe same being shared. These privacy violations are amplified for people with\ndisabilities, who may find it challenging to raise dissent even if they are\naware. Such unauthorized image captures may also be misused to gain sympathy by\nthird-party organizations, leading to a privacy breach. Privacy for people with\ndisabilities has so far received comparatively less attention from the AI\ncommunity. This motivates us to work towards a solution to generate\nprivacy-conscious cues for raising awareness in smartphone users of any\nsensitivity in their viewfinder content. To this end, we introduce PrivPAS (A\nreal time Privacy-Preserving AI System) a novel framework to identify sensitive\ncontent. Additionally, we curate and annotate a dataset to identify and\nlocalize accessibility markers and classify whether an image is sensitive to a\nfeatured subject with a disability. We demonstrate that the proposed\nlightweight architecture, with a memory footprint of a mere 8.49MB, achieves a\nhigh mAP of 89.52% on resource-constrained devices. Furthermore, our pipeline,\ntrained on face anonymized data, achieves an F1-score of 73.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_H/0/1/0/all/0/1\">Harichandana B S S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Vibhav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sourav Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramena_G/0/1/0/all/0/1\">Gopi Ramena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_S/0/1/0/all/0/1\">Sumit Kumar andd Barath Raj Kandur Raja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning on 3D Point Clouds by Clustering and Contrasting. (arXiv:2202.02543v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02543","description":"<p>Learning from unlabeled or partially labeled data to alleviate human labeling\nremains a challenging research topic in 3D modeling. Along this line,\nunsupervised representation learning is a promising direction to auto-extract\nfeatures without human intervention. This paper proposes a general unsupervised\napproach, named \\textbf{ConClu}, to perform the learning of point-wise and\nglobal features by jointly leveraging point-level clustering and instance-level\ncontrasting. Specifically, for one thing, we design an Expectation-Maximization\n(EM) like soft clustering algorithm that provides local supervision to extract\ndiscriminating local features based on optimal transport. We show that this\ncriterion extends standard cross-entropy minimization to an optimal transport\nproblem, which we solve efficiently using a fast variant of the Sinkhorn-Knopp\nalgorithm. For another, we provide an instance-level contrasting method to\nlearn the global geometry, which is formulated by maximizing the similarity\nbetween two augmentations of one point cloud. Experimental evaluations on\ndownstream applications such as 3D object classification and semantic\nsegmentation demonstrate the effectiveness of our framework and show that it\ncan outperform state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Guofeng Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Litao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEVO: Depth-Event Camera Visual Odometry in Challenging Conditions. (arXiv:2202.02556v1 [cs.RO])","link":"http://arxiv.org/abs/2202.02556","description":"<p>We present a novel real-time visual odometry framework for a stereo setup of\na depth and high-resolution event camera. Our framework balances accuracy and\nrobustness against computational efficiency towards strong performance in\nchallenging scenarios. We extend conventional edge-based semi-dense visual\nodometry towards time-surface maps obtained from event streams. Semi-dense\ndepth maps are generated by warping the corresponding depth values of the\nextrinsically calibrated depth camera. The tracking module updates the camera\npose through efficient, geometric semi-dense 3D-2D edge alignment. Our approach\nis validated on both public and self-collected datasets captured under various\nconditions. We show that the proposed method performs comparable to\nstate-of-the-art RGB-D camera-based alternatives in regular conditions, and\neventually outperforms in challenging conditions such as high dynamics or low\nillumination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yi-Fan Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Catch Me if You Can: A Novel Task for Detection of Covert Geo-Locations (CGL). (arXiv:2202.02567v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02567","description":"<p>Most visual scene understanding tasks in the field of computer vision involve\nidentification of the objects present in the scene. Image regions like\nhideouts, turns, &amp; other obscured regions of the scene also contain crucial\ninformation, for specific surveillance tasks. Task proposed in this paper\ninvolves the design of an intelligent visual aid for identification of such\nlocations in an image, which has either the potential to create an imminent\nthreat from an adversary or appear as the target zones needing further\ninvestigation. Covert places (CGL) for hiding behind an occluding object are\nconcealed 3D locations, not detectable from the viewpoint (camera). Hence this\ninvolves delineating specific image regions around the projections of outer\nboundary of the occluding objects, as places to be accessed around the\npotential hideouts. CGL detection finds applications in military\ncounter-insurgency operations, surveillance with path planning for an\nexploratory robot. Given an RGB image, the goal is to identify all CGLs in the\n2D scene. Identification of such regions would require knowledge about the 3D\nboundaries of obscuring items (pillars, furniture), their spatial location with\nrespect to the neighboring regions of the scene. We propose this as a novel\ntask, termed Covert Geo-Location (CGL) Detection. Classification of any region\nof an image as a CGL (as boundary sub-segments of an occluding object that\nconceals the hideout) requires examining the 3D relation between boundaries of\noccluding objects and their neighborhoods &amp; surroundings. Our method\nsuccessfully extracts relevant depth features from a single RGB image and\nquantitatively yields significant improvement over existing object detection\nand segmentation models adapted and trained for CGL detection. We also\nintroduce a novel hand-annotated CGL detection dataset containing 1.5K\nreal-world images for experimentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_B/0/1/0/all/0/1\">Binoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sukhendu Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIS-iTrack: Visual Intention through Gaze Tracking using Low-Cost Webcam. (arXiv:2202.02587v1 [cs.HC])","link":"http://arxiv.org/abs/2202.02587","description":"<p>Human intention is an internal, mental characterization for acquiring desired\ninformation. From interactive interfaces containing either textual or graphical\ninformation, intention to perceive desired information is subjective and\nstrongly connected with eye gaze. In this work, we determine such intention by\nanalyzing real-time eye gaze data with a low-cost regular webcam. We extracted\nunique features (e.g., Fixation Count, Eye Movement Ratio) from the eye gaze\ndata of 31 participants to generate a dataset containing 124 samples of visual\nintention for perceiving textual or graphical information, labeled as either\nTEXT or IMAGE, having 48.39% and 51.61% distribution, respectively. Using this\ndataset, we analyzed 5 classifiers, including Support Vector Machine (SVM)\n(Accuracy: 92.19%). Using the trained SVM, we investigated the variation of\nvisual intention among 30 participants, distributed in 3 age groups, and found\nout that young users were more leaned towards graphical contents whereas older\nadults felt more interested in textual ones. This finding suggests that\nreal-time eye gaze data can be a potential source of identifying visual\nintention, analyzing which intention aware interactive interfaces can be\ndesigned and developed to facilitate human cognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabab_S/0/1/0/all/0/1\">Shahed Anzarus Sabab</a> (1, 2, 3, 4, and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohammad Ridwan Kabir</a> (1, 2, and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1\">Sayed Rizban Hussain</a> (1, 2, and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a> (1, 2, and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Kamrul Hasan</a> (1, 2, and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Rubaiyeat_H/0/1/0/all/0/1\">Husne Ara Rubaiyeat</a> (6) ((1) Systems and Software Lab (SSL), (2) Department of Computer Science and Engineering, (3) Islamic University of Technology (IUT), Gazipur, Bangladesh, (4) Department of Computer Science, (5) University of Manitoba, Winnipeg, Canada, (6) National University, Bangladesh.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory Defense: More Robust Classification via a Memory-Masking Autoencoder. (arXiv:2202.02595v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02595","description":"<p>Many deep neural networks are susceptible to minute perturbations of images\nthat have been carefully crafted to cause misclassification. Ideally, a robust\nclassifier would be immune to small variations in input images, and a number of\ndefensive approaches have been created as a result. One method would be to\ndiscern a latent representation which could ignore small changes to the input.\nHowever, typical autoencoders easily mingle inter-class latent representations\nwhen there are strong similarities between classes, making it harder for a\ndecoder to accurately project the image back to the original high-dimensional\nspace. We propose a novel framework, Memory Defense, an augmented classifier\nwith a memory-masking autoencoder to counter this challenge. By masking other\nclasses, the autoencoder learns class-specific independent latent\nrepresentations. We test the model's robustness against four widely used\nattacks. Experiments on the Fashion-MNIST &amp; CIFAR-10 datasets demonstrate the\nsuperiority of our model. We make available our source code at GitHub\nrepository: https://github.com/eashanadhikarla/MemDefense\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhikarla_E/0/1/0/all/0/1\">Eashan Adhikarla</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dan Luo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1\">Brian D. Davison</a> (1) ((1) Lehigh University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROMNet: Renovate the Old Memories. (arXiv:2202.02606v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02606","description":"<p>Renovating the memories in old photos is an intriguing research topic in\ncomputer vision fields. These legacy images often suffer from severe and\ncommingled degradations such as cracks, noise, and color-fading, while lack of\nlarge-scale paired old photo datasets makes this restoration task very\nchallenging. In this work, we present a novel reference-based end-to-end\nlearning framework that can jointly repair and colorize the degraded legacy\npictures. Specifically, the proposed framework consists of three modules: a\nrestoration sub-network for degradation restoration, a similarity sub-network\nfor color histogram matching and transfer, and a colorization subnet that\nlearns to predict the chroma elements of the images conditioned on chromatic\nreference signals. The whole system takes advantage of the color histogram\npriors in a given reference image, which vastly reduces the dependency on\nlarge-scale training data. Apart from the proposed method, we also create, to\nour knowledge, the first public and real-world old photo dataset with paired\nground truth for evaluating old photo restoration models, wherein each old\nphoto is paired with a manually restored pristine image by PhotoShop experts.\nOur extensive experiments conducted on both synthetic and real-world datasets\ndemonstrate that our method significantly outperforms state-of-the-arts both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyu Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinlong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zibo Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+YU_H/0/1/0/all/0/1\">Hongkai YU</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSSIM: a structural similarity index for floating-point data. (arXiv:2202.02616v1 [stat.CO])","link":"http://arxiv.org/abs/2202.02616","description":"<p>Data visualization is a critical component in terms of interacting with\nfloating-point output data from large model simulation codes. Indeed,\npostprocessing analysis workflows on simulation data often generate a large\nnumber of images from the raw data, many of which are then compared to each\nother or to specified reference images. In this image-comparison scenario,\nimage quality assessment (IQA) measures are quite useful, and the Structural\nSimilarity Index (SSIM) continues to be a popular choice. However, generating\nlarge numbers of images can be costly, and plot-specific (but data independent)\nchoices can affect the SSIM value. A natural question is whether we can apply\nthe SSIM directly to the floating-point simulation data and obtain an\nindication of whether differences in the data are likely to impact a visual\nassessment, effectively bypassing the creation of a specific set of images from\nthe data. To this end, we propose an alternative to the popular SSIM that can\nbe applied directly to the floating point data, which we refer to as the Data\nSSIM (DSSIM). While we demonstrate the usefulness of the DSSIM in the context\nof evaluating differences due to lossy compression on large volumes of\nsimulation data from a popular climate model, the DSSIM may prove useful for\nmany other applications involving simulation or image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Baker_A/0/1/0/all/0/1\">Allison H. Baker</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pinard_A/0/1/0/all/0/1\">Alexander Pinard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hammerling_D/0/1/0/all/0/1\">Dorit M. Hammerling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework. (arXiv:2202.02626v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02626","description":"<p>Deep neural network models are used today in various applications of\nartificial intelligence, the strengthening of which, in the face of adversarial\nattacks is of particular importance. An appropriate solution to adversarial\nattacks is adversarial training, which reaches a trade-off between robustness\nand generalization. This paper introduces a novel framework (Layer\nSustainability Analysis (LSA)) for the analysis of layer vulnerability in a\ngiven neural network in the scenario of adversarial attacks. LSA can be a\nhelpful toolkit to assess deep neural networks and to extend the adversarial\ntraining approaches towards improving the sustainability of model layers via\nlayer monitoring and analysis. The LSA framework identifies a list of Most\nVulnerable Layers (MVL list) of a given network. The relative error, as a\ncomparison measure, is used to evaluate representation sustainability of each\nlayer against adversarial attack inputs. The proposed approach for obtaining\nrobust neural networks to fend off adversarial attacks is based on a layer-wise\nregularization (LR) over LSA proposal(s) for adversarial training (AT); i.e.\nthe AT-LR procedure. AT-LR could be used with any benchmark adversarial attack\nto reduce the vulnerability of network layers and to improve conventional\nadversarial training approaches. The proposed idea performs well theoretically\nand experimentally for state-of-the-art multilayer perceptron and convolutional\nneural network architectures. Compared with the AT-LR and its corresponding\nbase adversarial training, the classification accuracy of more significant\nperturbations increased by 16.35%, 21.79%, and 10.730% on Moon, MNIST, and\nCIFAR-10 benchmark datasets in comparison with the AT-LR and its corresponding\nbase adversarial training, respectively. The LSA framework is available and\npublished at https://github.com/khalooei/LSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalooei_M/0/1/0/all/0/1\">Mohammad Khalooei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homayounpour_M/0/1/0/all/0/1\">Mohammad Mehdi Homayounpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirmazlaghani_M/0/1/0/all/0/1\">Maryam Amirmazlaghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training. (arXiv:2202.02643v1 [cs.LG])","link":"http://arxiv.org/abs/2202.02643","description":"<p>Random pruning is arguably the most naive way to attain sparsity in neural\nnetworks, but has been deemed uncompetitive by either post-training pruning or\nsparse training. In this paper, we focus on sparse training and highlight a\nperhaps counter-intuitive finding, that random pruning at initialization can be\nquite powerful for the sparse training of modern neural networks. Without any\ndelicate pruning criteria or carefully pursued sparsity structures, we\nempirically demonstrate that sparsely training a randomly pruned network from\nscratch can match the performance of its dense equivalent. There are two key\nfactors that contribute to this revival: (i) the network sizes matter: as the\noriginal dense networks grow wider and deeper, the performance of training a\nrandomly pruned sparse network will quickly grow to matching that of its dense\nequivalent, even at high sparsity ratios; (ii) appropriate layer-wise sparsity\nratios can be pre-chosen for sparse training, which shows to be another\nimportant performance booster. Simple as it looks, a randomly pruned subnetwork\nof Wide ResNet-50 can be sparsely trained to outperforming a dense Wide\nResNet-50, on ImageNet. We also observed such randomly pruned networks\noutperform dense counterparts in other favorable aspects, such as\nout-of-distribution detection, uncertainty estimation, and adversarial\nrobustness. Overall, our results strongly suggest there is larger-than-expected\nroom for sparse training at scale, and the benefits of sparsity might be more\nuniversal beyond carefully designed pruning. Our source code can be found at\nhttps://github.com/VITA-Group/Random_Pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey of top-down approaches for human pose estimation. (arXiv:2202.02656v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02656","description":"<p>Human pose estimation in two-dimensional images videos has been a hot topic\nin the computer vision problem recently due to its vast benefits and potential\napplications for improving human life, such as behaviors recognition, motion\ncapture and augmented reality, training robots, and movement tracking. Many\nstate-of-the-art methods implemented with Deep Learning have addressed several\nchallenges and brought tremendous remarkable results in the field of human pose\nestimation. Approaches are classified into two kinds: the two-step framework\n(top-down approach) and the part-based framework (bottom-up approach). While\nthe two-step framework first incorporates a person detector and then estimates\nthe pose within each box independently, detecting all body parts in the image\nand associating parts belonging to distinct persons is conducted in the\npart-based framework. This paper aims to provide newcomers with an extensive\nreview of deep learning methods-based 2D images for recognizing the pose of\npeople, which only focuses on top-down approaches since 2016. The discussion\nthrough this paper presents significant detectors and estimators depending on\nmathematical background, the challenges and limitations, benchmark datasets,\nevaluation metrics, and comparison between methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kresovic_M/0/1/0/all/0/1\">Milan Kresovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiDAR dataset distillation within bayesian active learning framework: Understanding the effect of data augmentation. (arXiv:2202.02661v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02661","description":"<p>Autonomous driving (AD) datasets have progressively grown in size in the past\nfew years to enable better deep representation learning. Active learning (AL)\nhas re-gained attention recently to address reduction of annotation costs and\ndataset size. AL has remained relatively unexplored for AD datasets, especially\non point cloud data from LiDARs. This paper performs a principled evaluation of\nAL based dataset distillation on (1/4th) of the large Semantic-KITTI dataset.\nFurther on, the gains in model performance due to data augmentation (DA) are\ndemonstrated across different subsets of the AL loop. We also demonstrate how\nDA improves the selection of informative samples to annotate. We observe that\ndata augmentation achieves full dataset accuracy using only 60\\% of samples\nfrom the selected dataset configuration. This provides faster training time and\nsubsequent gains in annotation costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duong_N/0/1/0/all/0/1\">Ngoc Phuong Anh Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almin_A/0/1/0/all/0/1\">Alexandre Almin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemarie_L/0/1/0/all/0/1\">L&#xe9;o Lemari&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiran_B/0/1/0/all/0/1\">B Ravi Kiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulation-to-Reality domain adaptation for offline 3D object annotation on pointclouds with correlation alignment. (arXiv:2202.02666v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02666","description":"<p>Annotating objects with 3D bounding boxes in LiDAR pointclouds is a costly\nhuman driven process in an autonomous driving perception system. In this paper,\nwe present a method to semi-automatically annotate real-world pointclouds\ncollected by deployment vehicles using simulated data. We train a 3D object\ndetector model on labeled simulated data from CARLA jointly with real world\npointclouds from our target vehicle. The supervised object detection loss is\naugmented with a CORAL loss term to reduce the distance between labeled\nsimulated and unlabeled real pointcloud feature representations. The goal here\nis to learn representations that are invariant to simulated (labeled) and\nreal-world (unlabeled) target domains. We also provide an updated survey on\ndomain adaptation methods for pointclouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weishuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiran_B/0/1/0/all/0/1\">B Ravi Kiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_T/0/1/0/all/0/1\">Thomas Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazouz_Y/0/1/0/all/0/1\">Yanis Mazouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steger_T/0/1/0/all/0/1\">Theo Steger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SRPCN: Structure Retrieval based Point Completion Network. (arXiv:2202.02669v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02669","description":"<p>Given partial objects and some complete ones as references, point cloud\ncompletion aims to recover authentic shapes. However, existing methods pay\nlittle attention to general shapes, which leads to the poor authenticity of\ncompletion results. Besides, the missing patterns are diverse in reality, but\nexisting methods can only handle fixed ones, which means a poor generalization\nability. Considering that a partial point cloud is a subset of the\ncorresponding complete one, we regard them as different samples of the same\ndistribution and propose Structure Retrieval based Point Completion Network\n(SRPCN). It first uses k-means clustering to extract structure points and\ndisperses them into distributions, and then KL Divergence is used as a metric\nto find the complete structure point cloud that best matches the input in a\ndatabase. Finally, a PCN-like decoder network is adopted to generate the final\nresults based on the retrieved structure point clouds. As structure plays an\nimportant role in describing the general shape of an object and the proposed\nstructure retrieval method is robust to missing patterns, experiments show that\nour method can generate more authentic results and has a stronger\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Ximing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Cheng Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyper-Convolutions via Implicit Kernels for Medical Imaging. (arXiv:2202.02701v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02701","description":"<p>The convolutional neural network (CNN) is one of the most commonly used\narchitectures for computer vision tasks. The key building block of a CNN is the\nconvolutional kernel that aggregates information from the pixel neighborhood\nand shares weights across all pixels. A standard CNN's capacity, and thus its\nperformance, is directly related to the number of learnable kernel weights,\nwhich is determined by the number of channels and the kernel size (support). In\nthis paper, we present the \\textit{hyper-convolution}, a novel building block\nthat implicitly encodes the convolutional kernel using spatial coordinates.\nHyper-convolutions decouple kernel size from the total number of learnable\nparameters, enabling a more flexible architecture design. We demonstrate in our\nexperiments that replacing regular convolutions with hyper-convolutions can\nimprove performance with less parameters, and increase robustness against\nnoise. We provide our code here:\n\\emph{https://github.com/tym002/Hyper-Convolution}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_T/0/1/0/all/0/1\">Tianyu Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1\">Alan Q. Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Sensor Fusion for Auto Driving Perception: A Survey. (arXiv:2202.02703v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02703","description":"<p>Multi-modal fusion is a fundamental task for the perception of an autonomous\ndriving system, which has recently intrigued many researchers. However,\nachieving a rather good performance is not an easy task due to the noisy raw\ndata, underutilized information, and the misalignment of multi-modal sensors.\nIn this paper, we provide a literature review of the existing multi-modal-based\nmethods for perception tasks in autonomous driving. Generally, we make a\ndetailed analysis including over 50 papers leveraging perception sensors\nincluding LiDAR and camera trying to solve object detection and semantic\nsegmentation tasks. Different from traditional fusion methodology for\ncategorizing fusion models, we propose an innovative way that divides them into\ntwo major classes, four minor classes by a more reasonable taxonomy in the view\nof the fusion stage. Moreover, we dive deep into the current fusion methods,\nfocusing on the remaining problems and open-up discussions on the potential\nresearch opportunities. In conclusion, what we expect to do in this paper is to\npresent a new taxonomy of multi-modal fusion methods for the autonomous driving\nperception tasks and provoke thoughts of the fusion-based techniques in the\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Keli Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Botian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Portrait Segmentation Using Deep Learning. (arXiv:2202.02705v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02705","description":"<p>A portrait is a painting, drawing, photograph, or engraving of a person,\nespecially one depicting only the face or head and shoulders. In the digital\nworld the portrait of a person is captured by having the person as a subject in\nthe image and capturing the image of the person such that the background is\nblurred. DSLRs generally do it by reducing the aperture to focus on very close\nregions of interest and automatically blur the background. In this paper I have\ncome up with a novel approach to replicate the portrait mode from DSLR using\nany smartphone to generate high quality portrait images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+and_S/0/1/0/all/0/1\">Sumedh Vilas Datar and</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernal_J/0/1/0/all/0/1\">Jesus Gonzales Bernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEAT: Face Editing with Attention. (arXiv:2202.02713v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02713","description":"<p>Employing the latent space of pretrained generators has recently been shown\nto be an effective means for GAN-based face manipulation. The success of this\napproach heavily relies on the innate disentanglement of the latent space axes\nof the generator. However, face manipulation often intends to affect local\nregions only, while common generators do not tend to have the necessary spatial\ndisentanglement. In this paper, we build on the StyleGAN generator, and present\na method that explicitly encourages face manipulation to focus on the intended\nregions by incorporating learned attention maps. During the generation of the\nedited image, the attention map serves as a mask that guides a blending between\nthe original features and the modified ones. The guidance for the latent space\nedits is achieved by employing CLIP, which has recently been shown to be\neffective for text-driven edits. We perform extensive experiments and show that\nour method can perform disentangled and controllable face manipulations based\non text descriptions by attending to the relevant regions only. Both\nqualitative and quantitative experimental results demonstrate the superiority\nof our method for facial region editing over alternative methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xianxu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1\">Or Patashnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing variational generation through self-decomposition. (arXiv:2202.02738v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02738","description":"<p>In this article we introduce the notion of Split Variational Autoencoder\n(SVAE), whose output $\\hat{x}$ is obtained as a weighted sum $\\sigma \\odot\n\\hat{x_1} + (1-\\sigma) \\odot \\hat{x_2}$ of two generated images\n$\\hat{x_1},\\hat{x_2}$, and $\\sigma$ is a learned compositional map. The network\nis trained as a usual Variational Autoencoder with a negative loglikelihood\nloss between training and reconstructed images. The decomposition is\nnondeterministic, but follows two main schemes, that we may roughly categorize\nas either \"syntactic\" or \"semantic\". In the first case, the map tends to\nexploit the strong correlation between adjacent pixels, splitting the image in\ntwo complementary high frequency sub-images. In the second case, the map\ntypically focuses on the contours of objects, splitting the image in\ninteresting variations of its content, with more marked and distinctive\nfeatures. In this case, the Fr\\'echet Inception Distance (FID) of $\\hat{x_1}$\nand $\\hat{x_2}$ is usually lower (hence better) than that of $\\hat{x}$, that\nclearly suffers from being the average of the formers. In a sense, a SVAE\nforces the Variational Autoencoder to {\\em make choices}, in contrast with its\nintrinsic tendency to average between alternatives with the aim to minimize the\nreconstruction loss towards a specific sample. According to the FID metric, our\ntechnique, tested on typical datasets such as Mnist, Cifar10 and Celeba, allows\nus to outperform all previous purely variational architectures (not relying on\nnormalization flows).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asperti_A/0/1/0/all/0/1\">Andrea Asperti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugo_L/0/1/0/all/0/1\">Laura Bugo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippini_D/0/1/0/all/0/1\">Daniele Filippini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Smart Gaze based Annotation of Histopathology Images for Training of Deep Convolutional Neural Networks. (arXiv:2202.02764v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02764","description":"<p>Unavailability of large training datasets is a bottleneck that needs to be\novercome to realize the true potential of deep learning in histopathology\napplications. Although slide digitization via whole slide imaging scanners has\nincreased the speed of data acquisition, labeling of virtual slides requires a\nsubstantial time investment from pathologists. Eye gaze annotations have the\npotential to speed up the slide labeling process. This work explores the\nviability and timing comparisons of eye gaze labeling compared to conventional\nmanual labeling for training object detectors. Challenges associated with gaze\nbased labeling and methods to refine the coarse data annotations for subsequent\nobject detection are also discussed. Results demonstrate that gaze tracking\nbased labeling can save valuable pathologist time and delivers good performance\nwhen employed for training a deep object detector. Using the task of\nlocalization of Keratin Pearls in cases of oral squamous cell carcinoma as a\ntest case, we compare the performance gap between deep object detectors trained\nusing hand-labelled and gaze-labelled data. On average, compared to\n`Bounding-box' based hand-labeling, gaze-labeling required $57.6\\%$ less time\nper label and compared to `Freehand' labeling, gaze-labeling required on\naverage $85\\%$ less time per label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mariam_K/0/1/0/all/0/1\">Komal Mariam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afzal_O/0/1/0/all/0/1\">Osama Mohammed Afzal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_W/0/1/0/all/0/1\">Wajahat Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Javed_M/0/1/0/all/0/1\">Muhammad Umar Javed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiyani_A/0/1/0/all/0/1\">Amber Kiyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khurram_S/0/1/0/all/0/1\">Syed Ali Khurram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_H/0/1/0/all/0/1\">Hassan Aqeel Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Features with Parameter-Free Layers. (arXiv:2202.02777v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02777","description":"<p>Trainable layers such as convolutional building blocks are the standard\nnetwork design choices by learning parameters to capture the global context\nthrough successive spatial operations. When designing an efficient network,\ntrainable layers such as the depthwise convolution is the source of efficiency\nin the number of parameters and FLOPs, but there was little improvement to the\nmodel speed in practice. This paper argues that simple built-in parameter-free\noperations can be a favorable alternative to the efficient trainable layers\nreplacing spatial operations in a network architecture. We aim to break the\nstereotype of organizing the spatial operations of building blocks into\ntrainable layers. Extensive experimental analyses based on layer-level studies\nwith fully-trained models and neural architecture searches are provided to\ninvestigate whether parameter-free operations such as the max-pool are\nfunctional. The studies eventually give us a simple yet effective idea for\nredesigning network architectures, where the parameter-free operations are\nheavily used as the main building block without sacrificing the model accuracy\nas much. Experimental results on the ImageNet dataset demonstrate that the\nnetwork architectures with parameter-free operations could enjoy the advantages\nof further efficiency in terms of model speed, the number of the parameters,\nand FLOPs. Code and ImageNet pretrained models are available at\nhttps://github.com/naver-ai/PfLayer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">YoungJoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-domain Unsupervised Image-to-Image Translation with Appearance Adaptive Convolution. (arXiv:2202.02779v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02779","description":"<p>Over the past few years, image-to-image (I2I) translation methods have been\nproposed to translate a given image into diverse outputs. Despite the\nimpressive results, they mainly focus on the I2I translation between two\ndomains, so the multi-domain I2I translation still remains a challenge. To\naddress this problem, we propose a novel multi-domain unsupervised\nimage-to-image translation (MDUIT) framework that leverages the decomposed\ncontent feature and appearance adaptive convolution to translate an image into\na target appearance while preserving the given geometric content. We also\nexploit a contrast learning objective, which improves the disentanglement\nability and effectively utilizes multi-domain image data in the training\nprocess by pairing the semantically similar images. This allows our method to\nlearn the diverse mappings between multiple visual domains with only a single\nframework. We show that the proposed method produces visually diverse and\nplausible results in multiple domains compared to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Somi Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy awareness in low precision neural networks. (arXiv:2202.02783v1 [cs.LG])","link":"http://arxiv.org/abs/2202.02783","description":"<p>Power consumption is a major obstacle in the deployment of deep neural\nnetworks (DNNs) on end devices. Existing approaches for reducing power\nconsumption rely on quite general principles, including avoidance of\nmultiplication operations and aggressive quantization of weights and\nactivations. However, these methods do not take into account the precise power\nconsumed by each module in the network, and are therefore not optimal. In this\npaper we develop accurate power consumption models for all arithmetic\noperations in the DNN, under various working conditions. We reveal several\nimportant factors that have been overlooked to date. Based on our analysis, we\npresent PANN (power-aware neural network), a simple approach for approximating\nany full-precision network by a low-power fixed-precision variant. Our method\ncan be applied to a pre-trained network, and can also be used during training\nto achieve improved performance. In contrast to previous methods, PANN incurs\nonly a minor degradation in accuracy w.r.t. the full-precision version of the\nnetwork, even when working at the power-budget of a 2-bit quantized variant. In\naddition, our scheme enables to seamlessly traverse the power-accuracy\ntrade-off at deployment time, which is a major advantage over existing\nquantization methods that are constrained to specific bit widths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eliezer_N/0/1/0/all/0/1\">Nurit Spingarn Eliezer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banner_R/0/1/0/all/0/1\">Ron Banner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffer_E/0/1/0/all/0/1\">Elad Hoffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Yaakov_H/0/1/0/all/0/1\">Hilla Ben-Yaakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaeli_T/0/1/0/all/0/1\">Tomer Michaeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLPanoDepth: Global-to-Local Panoramic Depth Estimation. (arXiv:2202.02796v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02796","description":"<p>In this paper, we propose a learning-based method for predicting dense depth\nvalues of a scene from a monocular omnidirectional image. An omnidirectional\nimage has a full field-of-view, providing much more complete descriptions of\nthe scene than perspective images. However, fully-convolutional networks that\nmost current solutions rely on fail to capture rich global contexts from the\npanorama. To address this issue and also the distortion of equirectangular\nprojection in the panorama, we propose Cubemap Vision Transformers (CViT), a\nnew transformer-based architecture that can model long-range dependencies and\nextract distortion-free global features from the panorama. We show that cubemap\nvision transformers have a global receptive field at every stage and can\nprovide globally coherent predictions for spherical signals. To preserve\nimportant local features, we further design a convolution-based branch in our\npipeline (dubbed GLPanoDepth) and fuse global features from cubemap vision\ntransformers at multiple scales. This global-to-local strategy allows us to\nfully exploit useful global and local features in the panorama, achieving\nstate-of-the-art performance in panoramic depth estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiayang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shuichang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haoyu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-confidence Samples Matter for Domain Adaptation. (arXiv:2202.02802v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02802","description":"<p>Domain adaptation (DA) aims to transfer knowledge from a label-rich source\ndomain to a related but label-scarce target domain. The conventional DA\nstrategy is to align the feature distributions of the two domains. Recently,\nincreasing researches have focused on self-training or other semi-supervised\nalgorithms to explore the data structure of the target domain. However, the\nbulk of them depend largely on confident samples in order to build reliable\npseudo labels, prototypes or cluster centers. Representing the target data\nstructure in such a way would overlook the huge low-confidence samples,\nresulting in sub-optimal transferability that is biased towards the samples\nsimilar to the source domain. To overcome this issue, we propose a novel\ncontrastive learning method by processing low-confidence samples, which\nencourages the model to make use of the target data structure through the\ninstance discrimination process. To be specific, we create positive and\nnegative pairs only using low-confidence samples, and then re-represent the\noriginal features with the classifier weights rather than directly utilizing\nthem, which can better encode the task-specific semantic information.\nFurthermore, we combine cross-domain mixup to augment the proposed contrastive\nloss. Consequently, the domain gap can be well bridged through contrastive\nlearning of intermediate representations across domains. We evaluate the\nproposed method in both unsupervised and semi-supervised DA settings, and\nextensive experimental results on benchmarks reveal that our method is\neffective and achieves state-of-the-art performance. The code can be found in\nhttps://github.com/zhyx12/MixLRCo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Coding for Compressed Video Understanding: A New Framework and Benchmark. (arXiv:2202.02813v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02813","description":"<p>Most video understanding methods are learned on high-quality videos. However,\nin most real-world scenarios, the videos are first compressed before the\ntransportation and then decompressed for understanding. The decompressed videos\nare degraded in terms of perceptual quality, which may degenerate the\ndownstream tasks. To address this issue, we propose the first coding framework\nfor compressed video understanding, where another learnable perceptual\nbitstream is introduced and simultaneously transported with the video\nbitstream. With the sophisticatedly designed optimization target and network\narchitectures, this new stream largely boosts the perceptual quality of the\ndecoded videos yet with a small bit cost. Our framework can enjoy the best of\nboth two worlds, (1) highly efficient content-coding of industrial video codec\nand (2) flexible perceptual-coding of neural networks (NNs). Finally, we build\na rigorous benchmark for compressed video understanding over four different\ncompression levels, six large-scale datasets, and two popular tasks. The\nproposed Dual-bitstream Perceptual Video Coding framework Dual-PVC consistently\ndemonstrates significantly stronger performances than the baseline codec under\nthe same bitrate level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block shuffling learning for Deepfake Detection. (arXiv:2202.02819v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02819","description":"<p>Although the deepfake detection based on convolutional neural network has\nachieved good results, the detection results show that these detectors show\nobvious performance degradation when the input images undergo some common\ntransformations (like resizing, blurring), which indicates that the\ngeneralization ability of the detector is insufficient. In this paper, we\npropose a novel block shuffling learning method to solve this problem.\nSpecifically, we divide the images into blocks and then introduce the random\nshuffling to intra-block and inter-block. Intra-block shuffling increases the\nrobustness of the detector and we also propose an adversarial loss algorithm to\novercome the over-fitting problem brought by the noise introduced by shuffling.\nMoreover, we encourage the detector to focus on finding differences among the\nlocal features through inter-block shuffling, and reconstruct the spatial\nlayout of the blocks to model the semantic associations between them.\nEspecially, our method can be easily integrated with various CNN models.\nExtensive experiments show that our proposed method achieves state-of-the-art\nperformance in forgery face detection, including good generalization ability in\nthe face of common image transformations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sitong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhichao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Siqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Liang Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02832","description":"<p>Convolutional Neural Networks have demonstrated human-level performance in\nthe classification of melanoma and other skin lesions, but evident performance\ndisparities between differing skin tones should be addressed before widespread\ndeployment. In this work, we utilise a modified variational autoencoder to\nuncover skin tone bias in datasets commonly used as benchmarks. We propose an\nefficient yet effective algorithm for automatically labelling the skin tone of\nlesion images, and use this to annotate the benchmark ISIC dataset. We\nsubsequently use two leading bias unlearning techniques to mitigate skin tone\nbias. Our experimental results provide evidence that our skin tone detection\nalgorithm outperforms existing solutions and that unlearning skin tone improves\ngeneralisation and can reduce the performance disparity between melanoma\ndetection in lighter and darker skin tones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bevan_P/0/1/0/all/0/1\">Peter J. Bevan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CheXstray: Real-time Multi-Modal Data Concordance for Drift Detection in Medical Imaging AI. (arXiv:2202.02833v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02833","description":"<p>Rapidly expanding Clinical AI applications worldwide have the potential to\nimpact to all areas of medical practice. Medical imaging applications\nconstitute a vast majority of approved clinical AI applications. Though\nhealthcare systems are eager to adopt AI solutions a fundamental question\nremains: \\textit{what happens after the AI model goes into production?} We use\nthe CheXpert and PadChest public datasets to build and test a medical imaging\nAI drift monitoring workflow that tracks data and model drift without\ncontemporaneous ground truth. We simulate drift in multiple experiments to\ncompare model performance with our novel multi-modal drift metric, which uses\nDICOM metadata, image appearance representation from a variational autoencoder\n(VAE), and model output probabilities as input. Through experimentation, we\ndemonstrate a strong proxy for ground truth performance using unsupervised\ndistributional shifts in relevant metadata, predicted probabilities, and VAE\nlatent representation. Our key contributions include (1) proof-of-concept for\nmedical imaging drift detection including use of VAE and domain specific\nstatistical methods (2) a multi-modal methodology for measuring and unifying\ndrift metrics (3) new insights into the challenges and solutions for observing\ndeployed medical imaging AI (4) creation of open-source tools enabling others\nto easily run their own workflows or scenarios. This work has important\nimplications for addressing the translation gap related to continuous medical\nimaging AI model monitoring in dynamic healthcare environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Soin_A/0/1/0/all/0/1\">Arjun Soin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merkow_J/0/1/0/all/0/1\">Jameson Merkow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_J/0/1/0/all/0/1\">Jin Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_J/0/1/0/all/0/1\">Joesph Paul Cohen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saligrama_S/0/1/0/all/0/1\">Smitha Saligrama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaiser_S/0/1/0/all/0/1\">Stephen Kaiser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borg_S/0/1/0/all/0/1\">Steven Borg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tarapov_I/0/1/0/all/0/1\">Ivan Tarapov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P Lungren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Deblurring with an Adaptive Network. (arXiv:1903.11394v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1903.11394","description":"<p>In this paper, we address the problem of dynamic scene deblurring in the\npresence of motion blur. Restoration of images affected by severe blur\nnecessitates a network design with a large receptive field, which existing\nnetworks attempt to achieve through simple increment in the number of generic\nconvolution layers, kernel-size, or the scales at which the image is processed.\nHowever, increasing the network capacity in this manner comes at the expense of\nincrease in model size and inference speed, and ignoring the non-uniform nature\nof blur. We present a new architecture composed of spatially adaptive residual\nlearning modules that implicitly discover the spatially varying shifts\nresponsible for non-uniform blur in the input image and learn to modulate the\nfilters. This capability is complemented by a self-attentive module which\ncaptures non-local relationships among the intermediate features and enhances\nthe receptive field. We then incorporate a spatiotemporal recurrent module in\nthe design to also facilitate efficient video deblurring. Our networks can\nimplicitly model the spatially-varying deblurring process, while dispensing\nwith multi-scale processing and large filters entirely. Extensive qualitative\nand quantitative comparisons with prior art on benchmark dynamic scene\ndeblurring datasets clearly demonstrate the superiority of the proposed\nnetworks via reduction in model-size and significant improvements in accuracy\nand speed, enabling almost real-time deblurring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Planar Geometry and Image Recovery from Motion-Blur. (arXiv:1904.03710v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.03710","description":"<p>Existing works on motion deblurring either ignore the effects of\ndepth-dependent blur or work with the assumption of a multi-layered scene\nwherein each layer is modeled in the form of fronto-parallel plane. In this\nwork, we consider the case of 3D scenes with piecewise planar structure i.e., a\nscene that can be modeled as a combination of multiple planes with arbitrary\norientations. We first propose an approach for estimation of normal of a planar\nscene from a single motion blurred observation. We then develop an algorithm\nfor automatic recovery of number of planes, the parameters corresponding to\neach plane, and camera motion from a single motion blurred image of a\nmultiplanar 3D scene. Finally, we propose a first-of-its-kind approach to\nrecover the planar geometry and latent image of the scene by adopting an\nalternating minimization framework built on our findings. Experiments on\nsynthetic and real data reveal that our proposed method achieves\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasu_S/0/1/0/all/0/1\">Subeesh Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">M. Purnachandra Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Deepfakes Creation and Detection: A Survey. (arXiv:1909.11573v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.11573","description":"<p>Deep learning has been successfully applied to solve various complex problems\nranging from big data analytics to computer vision and human-level control.\nDeep learning advances however have also been employed to create software that\ncan cause threats to privacy, democracy and national security. One of those\ndeep learning-powered applications recently emerged is deepfake. Deepfake\nalgorithms can create fake images and videos that humans cannot distinguish\nthem from authentic ones. The proposal of technologies that can automatically\ndetect and assess the integrity of digital visual media is therefore\nindispensable. This paper presents a survey of algorithms used to create\ndeepfakes and, more importantly, methods proposed to detect deepfakes in the\nliterature to date. We present extensive discussions on challenges, research\ntrends and directions related to deepfake technologies. By reviewing the\nbackground of deepfakes and state-of-the-art deepfake detection methods, this\nstudy provides a comprehensive overview of deepfake techniques and facilitates\nthe development of new and more robust methods to deal with the increasingly\nchallenging deepfakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Thi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quoc Viet Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dung Tien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_The_T/0/1/0/all/0/1\">Thien Huynh-The</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Tam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quoc-Viet Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong M. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FISR: Deep Joint Frame Interpolation and Super-Resolution with a Multi-scale Temporal Loss. (arXiv:1912.07213v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.07213","description":"<p>Super-resolution (SR) has been widely used to convert low-resolution legacy\nvideos to high-resolution (HR) ones, to suit the increasing resolution of\ndisplays (e.g. UHD TVs). However, it becomes easier for humans to notice motion\nartifacts (e.g. motion judder) in HR videos being rendered on larger-sized\ndisplay devices. Thus, broadcasting standards support higher frame rates for\nUHD (Ultra High Definition) videos (4K@60 fps, 8K@120 fps), meaning that\napplying SR only is insufficient to produce genuine high quality videos. Hence,\nto up-convert legacy videos for realistic applications, not only SR but also\nvideo frame interpolation (VFI) is necessitated. In this paper, we first\npropose a joint VFI-SR framework for up-scaling the spatio-temporal resolution\nof videos from 2K 30 fps to 4K 60 fps. For this, we propose a novel training\nscheme with a multi-scale temporal loss that imposes temporal regularization on\nthe input video sequence, which can be applied to any general video-related\ntask. The proposed structure is analyzed in depth with extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo Ye Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jihyong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Munchurl Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedOCR: Communication-Efficient Federated Learning for Scene Text Recognition. (arXiv:2007.11462v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.11462","description":"<p>While scene text recognition techniques have been widely used in commercial\napplications, data privacy has rarely been taken into account by this research\ncommunity. Most existing algorithms have assumed a set of shared or centralized\ntraining data. However, in practice, data may be distributed on different local\ndevices that can not be centralized to share due to the privacy restrictions.\nIn this paper, we study how to make use of decentralized datasets for training\na robust scene text recognizer while keeping them stay on local devices. To the\nbest of our knowledge, we propose the first framework leveraging federated\nlearning for scene text recognition, which is trained with decentralized\ndatasets collaboratively. Hence we name it FedOCR. To make FedCOR fairly\nsuitable to be deployed on end devices, we make two improvements including\nusing lightweight models and hashing techniques. We argue that both are crucial\nfor FedOCR in terms of the communication efficiency of federated learning. The\nsimulations on decentralized datasets show that the proposed FedOCR achieves\ncompetitive results to the models that are trained with centralized data, with\nfewer communication costs and higher-level privacy-preserving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compensation Tracker: Reprocessing Lost Object for Multi-Object Tracking. (arXiv:2008.12052v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.12052","description":"<p>Tracking by detection paradigm is one of the most popular object tracking\nmethods. However, it is very dependent on the performance of the detector. When\nthe detector has a behavior of missing detection, the tracking result will be\ndirectly affected. In this paper, we analyze the phenomenon of the lost\ntracking object in real-time tracking model on MOT2020 dataset. Based on simple\nand traditional methods, we propose a compensation tracker to further alleviate\nthe lost tracking problem caused by missing detection. It consists of a motion\ncompensation module and an object selection module. The proposed method not\nonly can re-track missing tracking objects from lost objects, but also does not\nrequire additional networks so as to maintain speed-accuracy trade-off of the\nreal-time model. Our method only needs to be embedded into the tracker to work\nwithout re-training the network. Experiments show that the compensation tracker\ncan efficaciously improve the performance of the model and reduce identity\nswitches. With limited costs, the compensation tracker successfully enhances\nthe baseline tracking performance by a large margin and reaches 66% of MOTA and\n67% of IDF1 on MOT2020 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhibo Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Ensemble Robustness by Collaboratively Promoting and Demoting Adversarial Robustness. (arXiv:2009.09612v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.09612","description":"<p>Ensemble-based adversarial training is a principled approach to achieve\nrobustness against adversarial attacks. An important technique of this approach\nis to control the transferability of adversarial examples among ensemble\nmembers. We propose in this work a simple yet effective strategy to collaborate\namong committee models of an ensemble model. This is achieved via the secure\nand insecure sets defined for each model member on a given sample, hence help\nus to quantify and regularize the transferability. Consequently, our proposed\nframework provides the flexibility to reduce the adversarial transferability as\nwell as to promote the diversity of ensemble members, which are two crucial\nfactors for better robustness in our ensemble approach. We conduct extensive\nand comprehensive experiments to demonstrate that our proposed method\noutperforms the state-of-the-art ensemble baselines, at the same time can\ndetect a wide range of adversarial examples with a nearly perfect accuracy. Our\ncode is available at:\nhttps://github.com/tuananhbui89/Crossing-Collaborative-Ensemble.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_A/0/1/0/all/0/1\">Anh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montague_P/0/1/0/all/0/1\">Paul Montague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+deVel_O/0/1/0/all/0/1\">Olivier deVel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_T/0/1/0/all/0/1\">Tamas Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real-Time Predictive Pedestrian Collision Warning Service for Cooperative Intelligent Transportation Systems Using 3D Pose Estimation. (arXiv:2009.10868v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.10868","description":"<p>Minimizing traffic accidents between vehicles and pedestrians is one of the\nprimary research goals in intelligent transportation systems. To achieve the\ngoal, pedestrian orientation recognition and prediction of pedestrian's\ncrossing or not-crossing intention play a central role. Contemporary approaches\ndo not guarantee satisfactory performance due to limited field-of-view, lack of\ngeneralization, and high computational complexity. To overcome these\nlimitations, we propose a real-time predictive pedestrian collision warning\nservice (P2CWS) for two tasks: pedestrian orientation recognition (100.53 FPS)\nand intention prediction (35.76 FPS). Our framework obtains satisfying\ngeneralization over multiple sites because of the proposed site-independent\nfeatures. At the center of the feature extraction lies 3D pose estimation. The\n3D pose analysis enables robust and accurate recognition of pedestrian\norientations and prediction of intentions over multiple sites. The proposed\nvision framework realizes 89.3% accuracy in the behavior recognition task on\nthe TUD dataset without any training process and 91.28% accuracy in intention\nprediction on our dataset achieving new state-of-the-art performance. To\ncontribute to the corresponding research community, we make our source codes\npublic which are available at https://github.com/Uehwan/VisionForPedestrian\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_U/0/1/0/all/0/1\">Ue-Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ka_D/0/1/0/all/0/1\">Dongho Ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Hwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging 2D and 3D Segmentation Networks for Computation Efficient Volumetric Medical Image Segmentation: An Empirical Study of 2.5D Solutions. (arXiv:2010.06163v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.06163","description":"<p>Recently, deep convolutional neural networks have achieved great success for\nmedical image segmentation. However, unlike segmentation of natural images,\nmost medical images such as MRI and CT are volumetric data. In order to make\nfull use of volumetric information, 3D CNNs are widely used. However, 3D CNNs\nsuffer from higher inference time and computation cost, which hinders their\nfurther clinical applications. Additionally, with the increased number of\nparameters, the risk of overfitting is higher, especially for medical images\nwhere data and annotations are expensive to acquire. To issue this problem,\nmany 2.5D segmentation methods have been proposed to make use of volumetric\nspatial information with less computation cost. Despite these works lead to\nimprovements on a variety of segmentation tasks, to the best of our knowledge,\nthere has not previously been a large-scale empirical comparison of these\nmethods. In this paper, we aim to present a review of the latest developments\nof 2.5D methods for volumetric medical image segmentation. Additionally, to\ncompare the performance and effectiveness of these methods, we provide an\nempirical study of these methods on three representative segmentation tasks\ninvolving different modalities and targets. Our experimental results highlight\nthat 3D CNNs may not always be the best choice. Despite all these 2.5D methods\ncan bring performance gains to 2D baseline, not all the methods hold the\nbenefits on different datasets. We hope the results and conclusions of our\nstudy will prove useful for the community on exploring and developing efficient\nvolumetric medical image segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_Q/0/1/0/all/0/1\">Qingcheng Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_L/0/1/0/all/0/1\">Le Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jicong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Regularization Prediction in Diffeomorphic Image Registration. (arXiv:2011.14229v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.14229","description":"<p>This paper presents a predictive model for estimating regularization\nparameters of diffeomorphic image registration. We introduce a novel framework\nthat automatically determines the parameters controlling the smoothness of\ndiffeomorphic transformations. Our method significantly reduces the effort of\nparameter tuning, which is time and labor-consuming. To achieve the goal, we\ndevelop a predictive model based on deep convolutional neural networks (CNN)\nthat learns the mapping between pairwise images and the regularization\nparameter of image registration. In contrast to previous methods that estimate\nsuch parameters in a high-dimensional image space, our model is built in an\nefficient bandlimited space with much lower dimensions. We demonstrate the\neffectiveness of our model on both 2D synthetic data and 3D real brain images.\nExperimental results show that our model not only predicts appropriate\nregularization parameters for image registration, but also improving the\nnetwork training in terms of time and memory efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense outlier detection and open-set recognition based on training with noisy negative images. (arXiv:2101.09193v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.09193","description":"<p>Deep convolutional models often produce inadequate predictions for inputs\nforeign to the training distribution. Consequently, the problem of detecting\noutlier images has recently been receiving a lot of attention. Unlike most\nprevious work, we address this problem in the dense prediction context in order\nto be able to locate outlier objects in front of in-distribution background.\nOur approach is based on two reasonable assumptions. First, we assume that the\ninlier dataset is related to some narrow application field (e.g.~road driving).\nSecond, we assume that there exists a general-purpose dataset which is much\nmore diverse than the inlier dataset (e.g.~ImageNet-1k). We consider pixels\nfrom the general-purpose dataset as noisy negative training samples since most\n(but not all) of them are outliers. We encourage the model to recognize borders\nbetween known and unknown by pasting jittered negative patches over inlier\ntraining images. Our experiments target two dense open-set recognition\nbenchmarks (WildDash 1 and Fishyscapes) and one dense open-set recognition\ndataset (StreetHazard). Extensive performance evaluation indicates competitive\npotential of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevandic_P/0/1/0/all/0/1\">Petra Bevandi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreso_I/0/1/0/all/0/1\">Ivan Kre&#x161;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1\">Marin Or&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kanerva++: extending The Kanerva Machine with differentiable, locally block allocated latent memory. (arXiv:2103.03905v3 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2103.03905","description":"<p>Episodic and semantic memory are critical components of the human memory\nmodel. The theory of complementary learning systems (McClelland et al., 1995)\nsuggests that the compressed representation produced by a serial event\n(episodic memory) is later restructured to build a more generalized form of\nreusable knowledge (semantic memory). In this work we develop a new principled\nBayesian memory allocation scheme that bridges the gap between episodic and\nsemantic memory via a hierarchical latent variable model. We take inspiration\nfrom traditional heap allocation and extend the idea of locally contiguous\nmemory to the Kanerva Machine, enabling a novel differentiable block allocated\nlatent memory. In contrast to the Kanerva Machine, we simplify the process of\nmemory writing by treating it as a fully feed forward deterministic process,\nrelying on the stochasticity of the read key distribution to disperse\ninformation within the memory. We demonstrate that this allocation scheme\nimproves performance in memory conditional image generation, resulting in new\nstate-of-the-art conditional likelihood values on binarized MNIST (&lt;=41.58\nnats/image) , binarized Omniglot (&lt;=66.24 nats/image), as well as presenting\ncompetitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32x32.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1\">Jason Ramapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1\">Alexandros Kalousis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised and self-adaptative techniques for cross-domain person re-identification. (arXiv:2103.11520v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11520","description":"<p>Person Re-Identification (ReID) across non-overlapping cameras is a\nchallenging task and, for this reason, most works in the prior art rely on\nsupervised feature learning from a labeled dataset to match the same person in\ndifferent views. However, it demands the time-consuming task of labeling the\nacquired data, prohibiting its fast deployment, specially in forensic\nscenarios. Unsupervised Domain Adaptation (UDA) emerges as a promising\nalternative, as it performs feature-learning adaptation from a model trained on\na source to a target domain without identity-label annotation. However, most\nUDA-based algorithms rely upon a complex loss function with several\nhyper-parameters, which hinders the generalization to different scenarios.\nMoreover, as UDA depends on the translation between domains, it is important to\nselect the most reliable data from the unseen domain, thus avoiding error\npropagation caused by noisy examples on the target data -- an often overlooked\nproblem. In this sense, we propose a novel UDA-based ReID method that optimizes\na simple loss function with only one hyper-parameter and that takes advantage\nof triplets of samples created by a new offline strategy based on the diversity\nof cameras within a cluster. This new strategy adapts the model and also\nregularizes it, avoiding overfitting on the target domain. We also introduce a\nnew self-ensembling strategy, in which weights from different iterations are\naggregated to create a final model combining knowledge from distinct moments of\nthe adaptation. For evaluation, we consider three well-known deep learning\narchitectures and combine them for final decision-making. The proposed method\ndoes not use person re-ranking nor any label on the target domain, and\noutperforms the state of the art, with a much simpler setup, on the Market to\nDuke, the challenging Market1501 to MSMT17, and Duke to MSMT17 adaptation\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertocco_G/0/1/0/all/0/1\">Gabriel Bertocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andalo_F/0/1/0/all/0/1\">Fernanda Andal&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Image Aesthetic Assessment. (arXiv:2103.11616v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11616","description":"<p>Automatic image aesthetics assessment is a computer vision problem dealing\nwith categorizing images into different aesthetic levels. The categorization is\nusually done by analyzing an input image and computing some measure of the\ndegree to which the image adheres to the fundamental principles of photography\nsuch as balance, rhythm, harmony, contrast, unity, look, feel, tone and\ntexture. Due to its diverse applications in many areas, automatic image\naesthetic assessment has gained significant research attention in recent years.\nThis article presents a review of the contemporary automatic image aesthetics\nassessment techniques. Many traditional hand-crafted and deep learning-based\napproaches are reviewed, and critical problem aspects are discussed, including\nwhy some features or models perform better than others and the limitations. A\ncomparison of the quantitative results of different methods is also provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1\">Abbas Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanwal_S/0/1/0/all/0/1\">Saira Kanwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahir_M/0/1/0/all/0/1\">Muhammad Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1\">Muhammad Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzair_M/0/1/0/all/0/1\">Muhammad Uzair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_M/0/1/0/all/0/1\">Mohammad Khalid Imam Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullah_H/0/1/0/all/0/1\">Habib Ullah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eigenbackground Revisited: Can We Model the Background with Eigenvectors?. (arXiv:2104.11379v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11379","description":"<p>Using dominant eigenvectors for background modeling (usually known as\nEigenbackground) is a common technique in the literature. However, its results\nsuffer from noticeable artifacts. Thus have been many attempts to reduce the\nartifacts by making some improvements/enhancement in the Eigenbackground\nalgorithm.\n</p>\n<p>In this paper, we show the main problem of the Eigenbackground is in its own\ncore and in fact, it is not a good idea to use strongest eigenvectors for\nmodeling the background. Instead, we propose an alternative solution by\nexploiting the weakest eigenvectors (which are usually thrown away and treated\nas garbage data) for background modeling. MATLAB codes are available at\n\\url{https://github.com/mamintoosi/Eigenbackground-Revisited}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amintoosi_M/0/1/0/all/0/1\">Mahmood Amintoosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farbiz_F/0/1/0/all/0/1\">Farzam Farbiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras. (arXiv:2106.04477v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04477","description":"<p>Synthesizing novel views of dynamic humans from stationary monocular cameras\nis a specialized but desirable setup. This is particularly attractive as it\ndoes not require static scenes, controlled environments, or specialized capture\nhardware. In contrast to techniques that exploit multi-view observations, the\nproblem of modeling a dynamic scene from a single view is significantly more\nunder-constrained and ill-posed. In this paper, we introduce Neural Motion\nConsensus Flow (MoCo-Flow), a representation that models dynamic humans in\nstationary monocular cameras using a 4D continuous time-variant function. We\nlearn the proposed representation by optimizing for a dynamic scene that\nminimizes the total rendering error, over all the observed images. At the heart\nof our work lies a carefully designed optimization scheme, which includes a\ndedicated initialization step and is constrained by a motion consensus\nregularization on the estimated motion flow. We extensively evaluate MoCo-Flow\non several datasets that contain human motions of varying complexity, and\ncompare, both qualitatively and quantitatively, to several baselines and\nablated variations of our methods, showing the efficacy and merits of the\nproposed approach. Pretrained model, code, and data will be released for\nresearch purposes upon paper acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuelin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. (arXiv:2106.10404v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.10404","description":"<p>Works on lottery ticket hypothesis (LTH) and single-shot network pruning\n(SNIP) have raised a lot of attention currently on post-training pruning\n(iterative magnitude pruning), and before-training pruning (pruning at\ninitialization). The former method suffers from an extremely large computation\ncost and the latter usually struggles with insufficient performance. In\ncomparison, during-training pruning, a class of pruning methods that\nsimultaneously enjoys the training/inference efficiency and the comparable\nperformance, temporarily, has been less explored. To better understand\nduring-training pruning, we quantitatively study the effect of pruning\nthroughout training from the perspective of pruning plasticity (the ability of\nthe pruned networks to recover the original performance). Pruning plasticity\ncan help explain several other empirical observations about neural network\npruning in literature. We further find that pruning plasticity can be\nsubstantially improved by injecting a brain-inspired mechanism called\nneuroregeneration, i.e., to regenerate the same number of connections as\npruned. We design a novel gradual magnitude pruning (GMP) method, named gradual\npruning with zero-cost neuroregeneration (\\textbf{GraNet}), that advances state\nof the art. Perhaps most impressively, its sparse-to-sparse version for the\nfirst time boosts the sparse-to-sparse training performance over various\ndense-to-sparse methods with ResNet-50 on ImageNet without extending the\ntraining time. We release all codes in\nhttps://github.com/Shiweiliuiiiiiii/GraNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1\">Zahra Atashgahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_H/0/1/0/all/0/1\">Huanyu Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity. (arXiv:2106.14568v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14568","description":"<p>The success of deep ensembles on improving predictive performance,\nuncertainty estimation, and out-of-distribution robustness has been extensively\nstudied in the machine learning literature. Albeit the promising results,\nnaively training multiple deep neural networks and combining their predictions\nat inference leads to prohibitive computational costs and memory requirements.\nRecently proposed efficient ensemble approaches reach the performance of the\ntraditional deep ensembles with significantly lower costs. However, the\ntraining resources required by these approaches are still at least the same as\ntraining a single dense model. In this work, we draw a unique connection\nbetween sparse neural network training and deep ensembles, yielding a novel\nefficient ensemble learning framework called FreeTickets. Instead of training\nmultiple dense networks and averaging them, we directly train sparse\nsubnetworks from scratch and extract diverse yet accurate subnetworks during\nthis efficient, sparse-to-sparse training. Our framework, FreeTickets, is\ndefined as the ensemble of these relatively cheap sparse subnetworks. Despite\nbeing an ensemble method, FreeTickets has even fewer parameters and training\nFLOPs than a single dense model. This seemingly counter-intuitive outcome is\ndue to the ultra training/inference efficiency of dynamic sparse training.\nFreeTickets surpasses the dense baseline in all the following criteria:\nprediction accuracy, uncertainty estimation, out-of-distribution (OoD)\nrobustness, as well as efficiency for both training and inference.\nImpressively, FreeTickets outperforms the naive deep ensemble with ResNet50 on\nImageNet using around only 1/5 of the training FLOPs required by the latter. We\nhave released our source code at https://github.com/VITA-Group/FreeTickets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1\">Zahra Atashgahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokar_G/0/1/0/all/0/1\">Ghada Sokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_E/0/1/0/all/0/1\">Elena Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional GANs with Auxiliary Discriminative Classifier. (arXiv:2107.10060v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10060","description":"<p>Conditional generative models aim to learn the underlying joint distribution\nof data and labels, and thus realize conditional generation. Among them,\nauxiliary classifier generative adversarial networks (AC-GAN) have been widely\nused, but suffer from the problem of low intra-class diversity on generated\nsamples. The fundamental reason pointed out in this paper is that the\nclassifier of AC-GAN is generator-agnostic, which therefore cannot provide\ninformative guidance to the generator to approximate the target distribution,\nresulting in minimization of conditional entropy that decreases the intra-class\ndiversity. Motivated by this, we propose a novel conditional GAN with auxiliary\n\\textit{discriminative} classifier (ADC-GAN) to resolve the above problem.\nSpecifically, the proposed auxiliary \\textit{discriminative} classifier becomes\ngenerator-aware by recognizing the labels of the real data and the generated\ndata \\textit{discriminatively}. Our theoretical analysis reveals that the\ngenerator can faithfully replicate the target distribution even without the\noriginal discriminator, making the proposed ADC-GAN robust to the value of\ncoefficient hyper-parameter and the selection of GAN loss, and being stable\nduring the training process. Extensive experimental results on synthetic and\nreal-world datasets demonstrate the superiority of ADC-GAN on conditional\ngenerative modeling compared with state-of-the-art classifier-based and\nprojection-based cGANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Siyuan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoshuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Image Segmentation using 3D Convolutional Neural Networks: A Review. (arXiv:2108.08467v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.08467","description":"<p>Computer-aided medical image analysis plays a significant role in assisting\nmedical practitioners for expert clinical diagnosis and deciding the optimal\ntreatment plan. At present, convolutional neural networks (CNN) are the\npreferred choice for medical image analysis. In addition, with the rapid\nadvancements in three-dimensional (3D) imaging systems and the availability of\nexcellent hardware and software support to process large volumes of data, 3D\ndeep learning methods are gaining popularity in medical image analysis. Here,\nwe present an extensive review of the recently evolved 3D deep learning methods\nin medical image segmentation. Furthermore, the research gaps and future\ndirections in 3D medical image segmentation are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Niyas_S/0/1/0/all/0/1\">S Niyas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1\">M Anand Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Prompt for Vision-Language Models. (arXiv:2109.01134v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01134","description":"<p>Large pre-trained vision-language models like CLIP have shown great potential\nin learning representations that are transferable across a wide range of\ndownstream tasks. Different from the traditional representation learning that\nis based mostly on discretized labels, vision-language pre-training aligns\nimages and texts in a common feature space, which allows zero-shot transfer to\nany downstream task via \\emph{prompting}, i.e., classification weights are\nsynthesized from natural language describing classes of interest. In this work,\nwe show that a major challenge for deploying such models in practice is prompt\nengineering, which requires domain expertise and is extremely time-consuming --\none needs to spend a significant amount of time on words tuning since a slight\nchange in wording could have a huge impact on performance. Inspired by recent\nadvances in prompt learning research in natural language processing (NLP), we\npropose \\emph{Context Optimization (CoOp)}, a simple approach specifically for\nadapting CLIP-like vision-language models for downstream image recognition.\nConcretely, CoOp models a prompt's context words with learnable vectors while\nthe entire pre-trained parameters are kept fixed. To handle different image\nrecognition tasks, we provide two implementations of CoOp: unified context and\nclass-specific context. Through extensive experiments on 11 datasets, we\ndemonstrate that CoOp requires as few as one or two shots to beat hand-crafted\nprompts with a decent margin and is able to gain significant improvements when\nusing more shots, e.g., with 16 shots the average gain is around 15\\% (with the\nhighest reaching over 45\\%). Despite being a learning-based approach, CoOp\nachieves superb domain generalization performance compared with the zero-shot\nmodel using hand-crafted prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UMPNet: Universal Manipulation Policy Network for Articulated Objects. (arXiv:2109.05668v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05668","description":"<p>We introduce the Universal Manipulation Policy Network (UMPNet) -- a single\nimage-based policy network that infers closed-loop action sequences for\nmanipulating arbitrary articulated objects. To infer a wide range of action\ntrajectories, the policy supports 6DoF action representation and varying\ntrajectory length. To handle a diverse set of objects, the policy learns from\nobjects with different articulation structures and generalizes to unseen\nobjects or categories. The policy is trained with self-guided exploration\nwithout any human demonstrations, scripted policy, or pre-defined goal\nconditions. To support effective multi-step interaction, we introduce a novel\nArrow-of-Time action attribute that indicates whether an action will change the\nobject state back to the past or forward into the future. With the\nArrow-of-Time inference at each interaction step, the learned policy is able to\nselect actions that consistently lead towards or away from a given state,\nthereby, enabling both effective state exploration and goal-conditioned\nmanipulation. Video is available at https://youtu.be/KqlvcL9RqKM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhanpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-stream CNN with Learnable Architecture for Multi-source Remote Sensing Data. (arXiv:2109.06094v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06094","description":"<p>In this paper, we propose an efficient and generalizable framework based on\ndeep convolutional neural network (CNN) for multi-source remote sensing data\njoint classification. While recent methods are mostly based on multi-stream\narchitectures, we use group convolution to construct equivalent network\narchitectures efficiently within a single-stream network. We further adopt and\nimprove dynamic grouping convolution (DGConv) to make group convolution\nhyperparameters, and thus the overall network architecture, learnable during\nnetwork training. The proposed method therefore can theoretically adjust any\nmodern CNN models to any multi-source remote sensing data set, and can\npotentially avoid sub-optimal solutions caused by manually decided architecture\nhyperparameters. In the experiments, the proposed method is applied to ResNet\nand UNet, and the adjusted networks are verified on three very diverse\nbenchmark data sets (i.e., Houston2018 data, Berlin data, and MUUFL data).\nExperimental results demonstrate the effectiveness of the proposed\nsingle-stream CNNs, and in particular ResNet18-DGConv improves the\nstate-of-the-art classification overall accuracy (OA) on HS-SAR Berlin data set\nfrom $62.23\\%$ to $68.21\\%$. In the experiments we have two interesting\nfindings. First, using DGConv generally reduces test OA variance. Second,\nmulti-stream is harmful to model performance if imposed to the first few\nlayers, but becomes beneficial if applied to deeper layers. Altogether, the\nfindings imply that multi-stream architecture, instead of being a strictly\nnecessary component in deep learning models for multi-source remote sensing\ndata, essentially plays the role of model regularizer. Our code is publicly\navailable at https://github.com/yyyyangyi/Multi-source-RS-DGConv. We hope our\nwork can inspire novel research in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Daoye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_T/0/1/0/all/0/1\">Tengteng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiangyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Fuhu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chengqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the Timing of Camera Movements From the Kinematics of Instruments in Robotic-Assisted Surgery Using Artificial Neural Networks. (arXiv:2109.11192v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.11192","description":"<p>Robotic-assisted surgeries benefit both surgeons and patients, however,\nsurgeons frequently need to adjust the endoscopic camera to achieve good\nviewpoints. Simultaneously controlling the camera and the surgical instruments\nis impossible, and consequentially, these camera adjustments repeatedly\ninterrupt the surgery. Autonomous camera control could help overcome this\nchallenge, but most existing systems are reactive, e.g., by having the camera\nfollow the surgical instruments. We propose a predictive approach for\nanticipating when camera movements will occur using artificial neural networks.\nWe used the kinematic data of the surgical instruments, which were recorded\nduring robotic-assisted surgical training on porcine models. We split the data\ninto segments, and labeled each either as a segment that immediately precedes a\ncamera movement, or one that does not. Due to the large class imbalance, we\ntrained an ensemble of networks, each on a balanced sub-set of the training\ndata. We found that the instruments' kinematic data can be used to predict when\ncamera movements will occur, and evaluated the performance on different segment\ndurations and ensemble sizes. We also studied how much in advance an upcoming\ncamera movement can be predicted, and found that predicting a camera movement\n0.25, 0.5, and 1 second before they occurred achieved 98%, 94%, and 84%\naccuracy relative to the prediction of an imminent camera movement. This\nindicates that camera movement events can be predicted early enough to leave\ntime for computing and executing an autonomous camera movement and suggests\nthat an autonomous camera controller for RAMIS may one day be feasible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kossowsky_H/0/1/0/all/0/1\">Hanna Kossowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nisky_I/0/1/0/all/0/1\">Ilana Nisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12925","description":"<p>Due to its role in many computer vision tasks, image matching has been\nsubjected to an active investigation by researchers, which has lead to better\nand more discriminant feature descriptors and to more robust matching\nstrategies, also thanks to the advent of the deep learning and the increased\ncomputational power of the modern hardware. Despite of these achievements, the\nkeypoint extraction process at the base of the image matching pipeline has not\nseen equivalent progresses. This paper presents HarrisZ$^+$, an upgrade to the\nHarrisZ corner detector, optimized to synergically take advance of the recent\nimprovements of the other steps of the image matching pipeline. HarrisZ$^+$\ndoes not only consists of a tuning of the setup parameters, but introduces\nfurther refinements to the selection criteria delineated by HarrisZ, so\nproviding more, yet discriminative, keypoints, which are better distributed on\nthe image and with higher localization accuracy. The image matching pipeline\nincluding HarrisZ$^+$, together with the other modern components, obtained in\ndifferent recent matching benchmarks state-of-the-art results among the classic\nimage matching pipelines. These results are quite close to those obtained by\nthe more recent fully deep end-to-end trainable approaches and show that there\nis still a proper margin of improvement that can be granted by the research in\nclassic image matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1\">Fabio Bellavia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_D/0/1/0/all/0/1\">Dmytro Mishkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Dense Crowd Trajectory Forecasting. (arXiv:2109.14128v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14128","description":"<p>Accurate, long-term forecasting of human pedestrian trajectories in highly\ndynamic and interactive scenes is a long-standing challenge. Recent advances in\nusing data-driven approaches have achieved significant improvements in terms of\nprediction accuracy. However, the lack of group-aware analysis has limited the\nperformance of forecasting models. This is especially apparent in highly\npopulated scenes, where pedestrians are moving in groups and the interactions\nbetween groups are extremely complex and dynamic. In this paper, we present\nGrouptron, a multi-scale dynamic forecasting framework that leverages\npedestrian group detection and utilizes individual-level, group-level, and\nscene-level information for better understanding and representation of the\nscenes. Our approach employs spatio-temporal clustering algorithms to identify\npedestrian groups, creates spatio-temporal graphs at the individual, group, and\nscene levels. It then uses graph neural networks to encode dynamics at\ndifferent scales and incorporates encoding across different scales for\ntrajectory prediction. We carried out extensive comparisons and ablation\nexperiments to demonstrate the effectiveness of our approach. Our method\nachieves a 9.3% decrease in final displacement error (FDE) compared with\nstate-of-the-art methods on ETH/UCY benchmark datasets, and a 16.1% decrease in\nFDE in more crowded scenes where extensive human group interactions are more\nfrequently present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Rui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhuo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Unlearning of Backdoors via Implicit Hypergradient. (arXiv:2110.03735v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03735","description":"<p>We propose a minimax formulation for removing backdoors from a given poisoned\nmodel based on a small set of clean data. This formulation encompasses much of\nprior work on backdoor removal. We propose the Implicit Bacdoor Adversarial\nUnlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which\nbreaks down the minimax into separate inner and outer problems, our algorithm\nutilizes the implicit hypergradient to account for the interdependence between\ninner and outer optimization. We theoretically analyze its convergence and the\ngeneralizability of the robustness gained by solving minimax on clean data to\nunseen test data. In our evaluation, we compare I-BAU with six state-of-art\nbackdoor defenses on seven backdoor attacks over two datasets and various\nattack settings, including the common setting where the attacker targets one\nclass as well as important but underexplored settings where multiple classes\nare targeted. I-BAU's performance is comparable to and most often significantly\nbetter than the best baseline. Particularly, its performance is more robust to\nthe variation on triggers, attack settings, poison ratio, and clean data size.\nMoreover, I-BAU requires less computation to take effect; particularly, it is\nmore than $13\\times$ faster than the most efficient baseline in the\nsingle-target attack setting. Furthermore, it can remain effective in the\nextreme case where the defender can only access 100 clean samples -- a setting\nwhere all the baselines fail to produce acceptable results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Won Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Z. Morley Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying the Transferability of Adversarial Attacks in Computer Networks. (arXiv:2110.04488v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2110.04488","description":"<p>Convolutional Neural Networks (CNNs) models are one of the most frequently\nused deep learning networks and are extensively used in both academia and\nindustry. Recent studies demonstrated that adversarial attacks against such\nmodels can maintain their effectiveness even when used on models other than the\none targeted by the attacker. This major property is known as transferability\nand makes CNNs ill-suited for security applications. In this paper, we provide\nthe first comprehensive study which assesses the robustness of CNN-based models\nfor computer networks against adversarial transferability. Furthermore, we\ninvestigate whether the transferability property issue holds in computer\nnetworks applications. In our experiments, we first consider five different\nattacks: the Iterative Fast Gradient Method (I-FGSM), the Jacobian-based\nSaliency Map (JSMA), the Limited-memory Broyden letcher Goldfarb Shanno BFGS\n(L-BFGS), the Projected Gradient Descent (PGD), and the DeepFool attack. Then,\nwe perform these attacks against three well-known datasets: the Network-based\nDetection of IoT (N-BaIoT) dataset and the Domain Generating Algorithms (DGA)\ndataset, and RIPE Atlas dataset. Our experimental results show clearly that the\ntransferability happens in specific use cases for the I-FGSM, the JSMA, and the\nLBFGS attack. In such scenarios, the attack success rate on the target network\nrange from 63.00\\% to 100\\%. Finally, we suggest two shielding strategies to\nhinder the attack transferability, by considering the Most Powerful Attacks\n(MPAs), and the mismatch LSTM architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekdad_Y/0/1/0/all/0/1\">Yassine Mekdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berenjestanaki_M/0/1/0/all/0/1\">Mohammad Hajian Berenjestanaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergougui_A/0/1/0/all/0/1\">Abdeslam EL Fergougui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v2 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2110.06986","description":"<p>In this paper, we propose a new deep unfolding neural network based on the\nADMM algorithm for analysis Compressed Sensing. The proposed network jointly\nlearns a redundant analysis operator for sparsification and reconstructs the\nsignal of interest. We compare our proposed network with a state-of-the-art\nunfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we\nconsider not only image, but also speech datasets as test examples.\nComputational experiments demonstrate that our proposed network outperforms the\nstate-of-the-art deep unfolding network, consistently for both real-world image\nand speech datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kouni_V/0/1/0/all/0/1\">Vasiliki Kouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandropoulos_G/0/1/0/all/0/1\">George C. Alexandropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?. (arXiv:2110.07472v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.07472","description":"<p>Equivariance has emerged as a desirable property of representations of\nobjects subject to identity-preserving transformations that constitute a group,\nsuch as translations and rotations. However, the expressivity of a\nrepresentation constrained by group equivariance is still not fully understood.\nWe address this gap by providing a generalization of Cover's Function Counting\nTheorem that quantifies the number of linearly separable and group-invariant\nbinary dichotomies that can be assigned to equivariant representations of\nobjects. We find that the fraction of separable dichotomies is determined by\nthe dimension of the space that is fixed by the group action. We show how this\nrelation extends to operations such as convolutions, element-wise\nnonlinearities, and global and local pooling. While other operations do not\nchange the fraction of separable dichotomies, local pooling decreases the\nfraction, despite being a highly nonlinear operation. Finally, we test our\ntheory on intermediate representations of randomly initialized and fully\ntrained convolutional neural networks and find perfect agreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farrell_M/0/1/0/all/0/1\">Matthew Farrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordelon_B/0/1/0/all/0/1\">Blake Bordelon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_S/0/1/0/all/0/1\">Shubhendu Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pehlevan_C/0/1/0/all/0/1\">Cengiz Pehlevan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TorchEsegeta: Framework for Interpretability and Explainability of Image-based Deep Learning Models. (arXiv:2110.08429v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08429","description":"<p>Clinicians are often very sceptical about applying automatic image processing\napproaches, especially deep learning based methods, in practice. One main\nreason for this is the black-box nature of these approaches and the inherent\nproblem of missing insights of the automatically derived decisions. In order to\nincrease trust in these methods, this paper presents approaches that help to\ninterpret and explain the results of deep learning algorithms by depicting the\nanatomical areas which influence the decision of the algorithm most. Moreover,\nthis research presents a unified framework, TorchEsegeta, for applying various\ninterpretability and explainability techniques for deep learning models and\ngenerate visual interpretations and explanations for clinicians to corroborate\ntheir clinical findings. In addition, this will aid in gaining confidence in\nsuch methods. The framework builds on existing interpretability and\nexplainability techniques that are currently focusing on classification models,\nextending them to segmentation tasks. In addition, these methods have been\nadapted to 3D models for volumetric analysis. The proposed framework provides\nmethods to quantitatively compare visual explanations using infidelity and\nsensitivity metrics. This framework can be used by data scientists to perform\npost-hoc interpretations and explanations of their models, develop more\nexplainable tools and present the findings to clinicians to increase their\nfaith in such models. The proposed framework was evaluated based on a use case\nscenario of vessel segmentation models trained on Time-of-fight (TOF) Magnetic\nResonance Angiogram (MRA) images of the human brain. Quantitative and\nqualitative results of a comparative study of different models and\ninterpretability methods are presented. Furthermore, this paper provides an\nextensive overview of several existing interpretability and explainability\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Arnab Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_C/0/1/0/all/0/1\">Chirag Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_B/0/1/0/all/0/1\">Budhaditya Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vipinraj_M/0/1/0/all/0/1\">Manish Vipinraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1\">Aniruddh Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rajatha Nagaraja Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarasaen_C/0/1/0/all/0/1\">Chompunuch Sarasaen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying How Self-Supervised Features Improve Training from Noisy Labels. (arXiv:2110.09022v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09022","description":"<p>The advancement of self-supervised learning (SSL) motivates researchers to\napply SSL on other tasks such as learning with noisy labels. Recent literature\nindicates that methods built on SSL features can substantially improve the\nperformance of learning with noisy labels. Nonetheless, the deeper reasons why\n(and how) SSL features benefit the training from noisy labels are less\nunderstood. In this paper, we study why and how self-supervised features help\nnetworks resist label noise using both theoretical analyses and numerical\nexperiments. Our results explain when and why fixing the SSL encoder helps\nconverge to a better optimum, and why an unfixed encoder is unstable but tends\nto achieve a better best-epoch accuracy in more challenging noise settings.\nFurther, we provide insights for how knowledge distilled from SSL features can\ncompromise between a fixed encoder and an unfixed encoder. We hope our work\nprovides a better understanding for learning with noisy labels from the\nperspective of self-supervised learning and can potentially serve as a\nguideline for further research. Code is available at\ngithub.com/UCSC-REAL/SelfSup_NoisyLabel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaowei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v7 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09113","description":"<p>Salt and pepper noise removal is a common inverse problem in image\nprocessing. Traditional denoising methods have two limitations. First, noise\ncharacteristics are often not described accurately. For example, the noise\nlocation information is often ignored and the sparsity of the salt and pepper\nnoise is often described by L1 norm, which cannot illustrate the sparse\nvariables clearly. Second, conventional methods separate the contaminated image\ninto a recovered image and a noise part, thus resulting in recovering an image\nwith unsatisfied smooth parts and detail parts. In this study, we introduce a\nnoise detection strategy to determine the position of the noise, and a\nnon-convex sparsity regularization depicted by Lp quasi-norm is employed to\ndescribe the sparsity of the noise, thereby addressing the first limitation.\nThe morphological component analysis framework with stationary Framelet\ntransform is adopted to decompose the processed image into cartoon, texture,\nand noise parts to resolve the second limitation. Then, the alternating\ndirection method of multipliers (ADMM) is employed to solve the proposed model.\nFinally, experiments are conducted to verify the proposed method and compare it\nwith some current state-of-the-art denoising methods. The experimental results\nshow that the proposed method can remove salt and pepper noise while preserving\nthe details of the processed image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yingpin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huiying Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jianhua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Chaoqun Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanping Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"csBoundary: City-scale Road-boundary Detection in Aerial Images for High-definition Maps. (arXiv:2111.06020v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06020","description":"<p>High-Definition (HD) maps can provide precise geometric and semantic\ninformation of static traffic environments for autonomous driving.\nRoad-boundary is one of the most important information contained in HD maps\nsince it distinguishes between road areas and off-road areas, which can guide\nvehicles to drive within road areas. But it is labor-intensive to annotate road\nboundaries for HD maps at the city scale. To enable automatic HD map\nannotation, current work uses semantic segmentation or iterative graph growing\nfor road-boundary detection. However, the former could not ensure topological\ncorrectness since it works at the pixel level, while the latter suffers from\ninefficiency and drifting issues. To provide a solution to the aforementioned\nproblems, in this letter, we propose a novel system termed csBoundary to\nautomatically detect road boundaries at the city scale for HD map annotation.\nOur network takes as input an aerial image patch, and directly infers the\ncontinuous road-boundary graph (i.e., vertices and edges) from this image. To\ngenerate the city-scale road-boundary graph, we stitch the obtained graphs from\nall the image patches. Our csBoundary is evaluated and compared on a public\nbenchmark dataset. The results demonstrate our superiority. The accompanied\ndemonstration video is available at our project page\n\\url{https://sites.google.com/view/csboundary/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Lu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangcheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lujia Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPF6D: Masked Pyramid Fusion 6D Pose Estimation. (arXiv:2111.09378v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09378","description":"<p>Object pose estimation has multiple important applications, such as robotic\ngrasping and augmented reality. We present a new method to estimate the 6D pose\nof objects that improves upon the accuracy of current proposals and can still\nbe used in real-time. Our method uses RGB-D data as input to segment objects\nand estimate their pose. It uses a neural network with multiple heads to\nidentify the objects in the scene, generate the appropriate masks and estimate\nthe values of the translation vectors and the quaternion that represents the\nobjects' rotation. These heads leverage a pyramid architecture used during\nfeature extraction and feature fusion. We conduct an empirical evaluation using\nthe two most common datasets in the area, and compare against state-of-the-art\napproaches, illustrating the capabilities of MPF6D. Our method can be used in\nreal-time with its low inference time and high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_N/0/1/0/all/0/1\">Nuno Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandre_L/0/1/0/all/0/1\">Lu&#xed;s A. Alexandre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALIKE: Accurate and Lightweight Keypoint Detection and Descriptor Extraction. (arXiv:2112.02906v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02906","description":"<p>Existing methods detect the keypoints in a non-differentiable way, therefore\nthey can not directly optimize the position of keypoints through\nback-propagation. To address this issue, we present a partially differentiable\nkeypoint detection module, which outputs accurate sub-pixel keypoints. The\nreprojection loss is then proposed to directly optimize these sub-pixel\nkeypoints, and the dispersity peak loss is presented for accurate keypoints\nregularization. We also extract the descriptors in a sub-pixel way, and they\nare trained with the stable neural reprojection error loss. Moreover, a\nlightweight network is designed for keypoint detection and descriptor\nextraction, which can run at 95 frames per second for 640x480 images on a\ncommercial GPU. On homography estimation, camera pose estimation, and visual\n(re-)localization tasks, the proposed method achieves equivalent performance\nwith the state-of-the-art approaches, while greatly reduces the inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jinyu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peter C. Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLAVA: A Foundational Language And Vision Alignment Model. (arXiv:2112.04482v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04482","description":"<p>State-of-the-art vision and vision-and-language models rely on large-scale\nvisio-linguistic pretraining for obtaining good performance on a variety of\ndownstream tasks. Generally, such models are often either cross-modal\n(contrastive) or multi-modal (with earlier fusion) but not both; and they often\nonly target specific modalities or tasks. A promising direction would be to use\na single holistic universal model, as a \"foundation\", that targets all\nmodalities at once -- a true vision and language foundation model should be\ngood at vision tasks, language tasks, and cross- and multi-modal vision and\nlanguage tasks. We introduce FLAVA as such a model and demonstrate impressive\nperformance on a wide range of 35 tasks spanning these target modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1\">Guillaume Couairon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1\">Wojciech Galuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Atlas Building with Deep Registration Priors. (arXiv:2112.06406v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06406","description":"<p>Registration-based atlas building often poses computational challenges in\nhigh-dimensional image spaces. In this paper, we introduce a novel hybrid atlas\nbuilding algorithm that fast estimates atlas from large-scale image datasets\nwith much reduced computational cost. In contrast to previous approaches that\niteratively perform registration tasks between an estimated atlas and\nindividual images, we propose to use learned priors of registration from\npre-trained neural networks. This newly developed hybrid framework features\nseveral advantages of (i) providing an efficient way of atlas building without\nlosing the quality of results, and (ii) offering flexibility in utilizing a\nwide variety of deep learning based registration methods. We demonstrate the\neffectiveness of this proposed model on 3D brain magnetic resonance imaging\n(MRI) scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guixu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yaxin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chaomin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Key Multimodal Backdoors for Visual Question Answering. (arXiv:2112.07668v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07668","description":"<p>The success of deep learning has enabled advances in multimodal tasks that\nrequire non-trivial fusion of multiple input domains. Although multimodal\nmodels have shown potential in many problems, their increased complexity makes\nthem more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of\nsecurity vulnerability wherein an attacker embeds a malicious secret behavior\ninto a network (e.g. targeted misclassification) that is activated when an\nattacker-specified trigger is added to an input. In this work, we show that\nmultimodal networks are vulnerable to a novel type of attack that we refer to\nas Dual-Key Multimodal Backdoors. This attack exploits the complex fusion\nmechanisms used by state-of-the-art networks to embed backdoors that are both\neffective and stealthy. Instead of using a single trigger, the proposed attack\nembeds a trigger in each of the input modalities and activates the malicious\nbehavior only when both the triggers are present. We present an extensive study\nof multimodal backdoors on the Visual Question Answering (VQA) task with\nmultiple architectures and visual feature backbones. A major challenge in\nembedding backdoors in VQA models is that most models use visual features\nextracted from a fixed pretrained object detector. This is challenging for the\nattacker as the detector can distort or ignore the visual trigger entirely,\nwhich leads to models where backdoors are over-reliant on the language trigger.\nWe tackle this problem by proposing a visual trigger optimization strategy\ndesigned for pretrained object detectors. Through this method, we create\nDual-Key Backdoors with over a 98% attack success rate while only poisoning 1%\nof the training data. Finally, we release TrojVQA, a large collection of clean\nand trojan VQA models to enable research in defending against multimodal\nbackdoors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walmer_M/0/1/0/all/0/1\">Matthew Walmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1\">Karan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sur_I/0/1/0/all/0/1\">Indranil Sur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Image Visual Question Answering. (arXiv:2112.13706v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13706","description":"<p>While a lot of work has been done on developing models to tackle the problem\nof Visual Question Answering, the ability of these models to relate the\nquestion to the image features still remain less explored. We present an\nempirical study of different feature extraction methods with different loss\nfunctions. We propose New dataset for the task of Visual Question Answering\nwith multiple image inputs having only one ground truth, and benchmark our\nresults on them. Our final model utilising Resnet + RCNN image features and\nBert embeddings, inspired from stacked attention network gives 39% word\naccuracy and 99% image accuracy on CLEVER+TinyImagenet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raj_H/0/1/0/all/0/1\">Harsh Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadhania_J/0/1/0/all/0/1\">Janhavi Dadhania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_A/0/1/0/all/0/1\">Akhilesh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KJ_P/0/1/0/all/0/1\">Prabuchandran KJ</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Band Wi-Fi Sensing with Matched Feature Granularity. (arXiv:2112.14006v2 [cs.NI] UPDATED)","link":"http://arxiv.org/abs/2112.14006","description":"<p>Complementary to the fine-grained channel state information (CSI) from the\nphysical layer and coarse-grained received signal strength indicator (RSSI)\nmeasurements, the mid-grained spatial beam attributes (e.g., beam SNR) that are\navailable at millimeter-wave (mmWave) bands during the mandatory beam training\nphase can be repurposed for Wi-Fi sensing applications. In this paper, we\npropose a multi-band Wi-Fi fusion method for Wi-Fi sensing that hierarchically\nfuses the features from both the fine-grained CSI at sub-6 GHz and the\nmid-grained beam SNR at 60 GHz in a granularity matching framework. The\ngranularity matching is realized by pairing two feature maps from the CSI and\nbeam SNR at different granularity levels and linearly combining all paired\nfeature maps into a fused feature map with learnable weights. To further\naddress the issue of limited labeled training data, we propose an\nautoencoder-based multi-band Wi-Fi fusion network that can be pre-trained in an\nunsupervised fashion. Once the autoencoder-based fusion network is pre-trained,\nwe detach the decoders and append multi-task sensing heads to the fused feature\nmap by fine-tuning the fusion block and re-training the multi-task heads from\nthe scratch. The multi-band Wi-Fi fusion framework is thoroughly validated by\nin-house experimental Wi-Fi sensing datasets spanning three tasks: 1) pose\nrecognition; 2) occupancy sensing; and 3) indoor localization. Comparison to\nfour baseline methods (i.e., CSI-only, beam SNR-only, input fusion, and feature\nfusion) demonstrates the granularity matching improves the multi-task sensing\nperformance. Quantitative performance is evaluated as a function of the number\nof labeled training data, latent space dimension, and fine-tuning learning\nrates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianyuan Yu</a>, Pu (Perry) <a href=\"http://arxiv.org/find/cs/1/au:+Wang/0/1/0/all/0/1\">Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koike_Akino_T/0/1/0/all/0/1\">Toshiaki Koike-Akino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlik_P/0/1/0/all/0/1\">Philip V. Orlik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buehrer_R/0/1/0/all/0/1\">R. Michael Buehrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RePaint: Inpainting using Denoising Diffusion Probabilistic Models. (arXiv:2201.09865v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09865","description":"<p>Free-form inpainting is the task of adding new content to an image in the\nregions specified by an arbitrary binary mask. Most existing approaches train\nfor a certain distribution of masks, which limits their generalization\ncapabilities to unseen mask types. Furthermore, training with pixel-wise and\nperceptual losses often leads to simple textural extensions towards the missing\nareas instead of semantically meaningful generation. In this work, we propose\nRePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting\napproach that is applicable to even extreme masks. We employ a pretrained\nunconditional DDPM as the generative prior. To condition the generation\nprocess, we only alter the reverse diffusion iterations by sampling the\nunmasked regions using the given image information. Since this technique does\nnot modify or condition the original DDPM network itself, the model produces\nhigh-quality and diverse output images for any inpainting form. We validate our\nmethod for both faces and general-purpose image inpainting using standard and\nextreme masks.\n</p>\n<p>RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for\nat least five out of six mask distributions.\n</p>\n<p>Github Repository: git.io/RePaint\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugmayr_A/0/1/0/all/0/1\">Andreas Lugmayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_A/0/1/0/all/0/1\">Andres Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projective Urban Texturing. (arXiv:2201.10938v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10938","description":"<p>This paper proposes a method for automatic generation of textures for 3D city\nmeshes in immersive urban environments. Many recent pipelines capture or\nsynthesize large quantities of city geometry using scanners or procedural\nmodeling pipelines. Such geometry is intricate and realistic, however the\ngeneration of photo-realistic textures for such large scenes remains a problem.\nWe propose to generate textures for input target 3D meshes driven by the\ntextural style present in readily available datasets of panoramic photos\ncapturing urban environments. Re-targeting such 2D datasets to 3D geometry is\nchallenging because the underlying shape, size, and layout of the urban\nstructures in the photos do not correspond to the ones in the target meshes.\nPhotos also often have objects (e.g., trees, vehicles) that may not even be\npresent in the target geometry. To address these issues we present a method,\ncalled Projective Urban Texturing (PUT), which re-targets textural style from\nreal-world panoramic images to unseen urban meshes. PUT relies on contrastive\nand adversarial training of a neural architecture designed for unpaired\nimage-to-texture translation. The generated textures are stored in a texture\natlas applied to the target 3D mesh geometry. To promote texture consistency,\nPUT employs an iterative procedure in which texture synthesis is conditioned on\npreviously generated, adjacent textures. We demonstrate both quantitative and\nqualitative evaluation of the generated textures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgiou_Y/0/1/0/all/0/1\">Yiangos Georgiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averkiou_M/0/1/0/all/0/1\">Melinos Averkiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1\">Tom Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1\">Evangelos Kalogerakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Diffusion Restoration Models. (arXiv:2201.11793v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.11793","description":"<p>Many interesting tasks in image restoration can be cast as linear inverse\nproblems. A recent family of approaches for solving these problems uses\nstochastic algorithms that sample from the posterior distribution of natural\nimages given the measurements. However, efficient solutions often require\nproblem-specific supervised training to model the posterior, whereas\nunsupervised methods that are not problem-specific typically rely on\ninefficient iterative methods. This work addresses these issues by introducing\nDenoising Diffusion Restoration Models (DDRM), an efficient, unsupervised\nposterior sampling method. Motivated by variational inference, DDRM takes\nadvantage of a pre-trained denoising diffusion generative model for solving any\nlinear inverse problem. We demonstrate DDRM's versatility on several image\ndatasets for super-resolution, deblurring, inpainting, and colorization under\nvarious amounts of measurement noise. DDRM outperforms the current leading\nunsupervised methods on the diverse ImageNet dataset in reconstruction quality,\nperceptual quality, and runtime, being 5x faster than the nearest competitor.\nDDRM also generalizes well for natural images out of the distribution of the\nobserved ImageNet training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kawar_B/0/1/0/all/0/1\">Bahjat Kawar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixing Implicit and Explicit Deep Learning with Skip DEQs and Infinite Time Neural ODEs (Continuous DEQs). (arXiv:2201.12240v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12240","description":"<p>Implicit deep learning architectures, like Neural ODEs and Deep Equilibrium\nModels (DEQs), separate the definition of a layer from the description of its\nsolution process. While implicit layers allow features such as depth to adapt\nto new scenarios and inputs automatically, this adaptivity makes its\ncomputational expense challenging to predict. Numerous authors have noted that\nimplicit layer techniques can be more computationally intensive than explicit\nlayer methods. In this manuscript, we address the question: is there a way to\nsimultaneously achieve the robustness of implicit layers while allowing the\nreduced computational expense of an explicit layer? To solve this we develop\nSkip DEQ, an implicit-explicit (IMEX) layer that simultaneously trains an\nexplicit prediction followed by an implicit correction. We show that training\nthis explicit layer is free and even decreases the training time by 2.5x and\nprediction time by 3.4x. We then further increase the \"implicitness\" of the DEQ\nby redefining the method in terms of an infinite time neural ODE which\nparadoxically decreases the training cost over a standard neural ODE by not\nrequiring backpropagation through time. We demonstrate how the resulting\nContinuous Skip DEQ architecture trains more robustly than the original DEQ\nwhile achieving faster training and prediction times. Together, this manuscript\nshows how bridging the dichotomy of implicit and explicit deep learning can\ncombine the advantages of both techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1\">Avik Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edelman_A/0/1/0/all/0/1\">Alan Edelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rackauckas_C/0/1/0/all/0/1\">Christopher Rackauckas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training. (arXiv:2201.12723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12723","description":"<p>Vision-and-language pre-trained models (VLMs) have achieved tremendous\nsuccess in the cross-modal area, but most of them require a large amount of\nparallel image-caption data for pre-training. Collating such data is expensive\nand labor-intensive. In this work, we focus on reducing such need for\ngenerative vision-and-language pre-training (G-VLP) by taking advantage of the\nvisual pre-trained model (CLIP-ViT) as encoder and language pre-trained model\n(GPT2) as decoder. Unfortunately, GPT2 lacks a necessary cross-attention\nmodule, which hinders the direct connection of CLIP-ViT and GPT2. To remedy\nsuch defects, we conduct extensive experiments to empirically investigate how\nto design and pre-train our model. Based on our experimental results, we\npropose a novel G-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train\nit with a small-scale image-caption corpus (Visual Genome, only 110k distinct\nimages). Evaluating on the image captioning downstream tasks (MSCOCO and\nFlickr30k Captioning), VC-GPT achieves either the best or the second-best\nperformance across all evaluation metrics over the previous works which consume\naround 30 times more distinct images during cross-modal pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Novelty Search with a Surrogate Model to Engineer Meta-Diversity in Ensembles of Classifiers. (arXiv:2201.12896v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12896","description":"<p>Using Neuroevolution combined with Novelty Search to promote behavioural\ndiversity is capable of constructing high-performing ensembles for\nclassification. However, using gradient descent to train evolved architectures\nduring the search can be computationally prohibitive. Here we propose a method\nto overcome this limitation by using a surrogate model which estimates the\nbehavioural distance between two neural network architectures required to\ncalculate the sparseness term in Novelty Search. We demonstrate a speedup of 10\ntimes over previous work and significantly improve on previous reported results\non three benchmark datasets from Computer Vision -- CIFAR-10, CIFAR-100, and\nSVHN. This results from the expanded architecture search space facilitated by\nusing a surrogate. Our method represents an improved paradigm for implementing\nhorizontal scaling of learning algorithms by making an explicit search for\ndiversity considerably more tractable for the same bounded resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_R/0/1/0/all/0/1\">Rui P. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hart_E/0/1/0/all/0/1\">Emma Hart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurka_D/0/1/0/all/0/1\">David Burth Kurka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitt_J/0/1/0/all/0/1\">Jeremy V. Pitt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filtering In Neural Implicit Functions. (arXiv:2201.13013v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13013","description":"<p>Neural implicit functions are highly effective for representing many kinds of\ndata, including images and 3D shapes. However, the implicit functions learned\nby neural networks usually include over-smoothed patches or noisy artifacts\ninto the results if the data has many scales of details or a wide range of\nfrequencies. Adapting the result containing both noise and over-smoothed\nregions usually suffers from either over smoothing or noisy issues. To overcome\nthis challenge, we propose a new framework, coined FINN, that integrates a\nfiltering module into the neural network to perform data generation while\nfiltering artifacts. The filtering module has a smoothing operator that acts on\nthe intermediate results of the network and a recovering operator that brings\ndistinct details from the input back to the regions overly smoothed. The\nproposed method significantly alleviates over smoothing or noisy issues. We\ndemonstrate the advantage of the FINN on the image regression task, considering\nboth real-world and synthetic images, and showcases significant improvement on\nboth quantitative and qualitative results compared to state-of-the-art methods.\nMoreover, FINN yields better performance in both convergence speed and network\nstability. Source code is available at https://github.com/yixin26/FINN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yixin Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.13392","description":"<p>The mortality of lung cancer has ranked high among cancers for many years.\nEarly detection of lung cancer is critical for disease prevention, cure, and\nmortality rate reduction. However, existing detection methods on pulmonary\nnodules introduce an excessive number of false positive proposals in order to\nachieve high sensitivity, which is not practical in clinical situations. In\nthis paper, we propose the multi-head detection and spatial\nsqueeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to\naid doctors in the early diagnosis of lung cancers. Specifically, we first\nintroduce multi-head detectors and skip connections to customize for the\nvariety of nodules in sizes, shapes and types and capture multi-scale features.\nThen, we implement a spatial attention module to enable the network to focus on\ndifferent regions differently inspired by how experienced clinicians screen CT\nimages, which results in fewer false positive proposals. Lastly, we present a\nlightweight but effective false positive reduction module with the Linear\nRegression model to cut down the number of false positive proposals, without\nany constraints on the front network. Extensive experimental results compared\nwith the state-of-the-art models have shown the superiority of the MHSnet in\nterms of the average FROC, sensitivity and especially false discovery rate\n(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%\nand 28.33% decrease in terms of false discovery rate and average candidates per\nscan). The false positive reduction module significantly decreases the average\nnumber of candidates generated per scan by 68.11% and the false discovery rate\nby 13.48%, which is promising to reduce distracted proposals for the downstream\ntasks based on the detection results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Juanyun Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Minghao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayin Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1\">Yanbo Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diao_Z/0/1/0/all/0/1\">Zhaoqi Diao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1\">Xinliang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jianyu Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_J/0/1/0/all/0/1\">Jian You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_A/0/1/0/all/0/1\">Airu Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_X/0/1/0/all/0/1\">Xiangcheng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1\">Jinsheng Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1\">Hua Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}