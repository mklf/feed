{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CompactIE: Compact Facts in Open Information Extraction. (arXiv:2205.02880v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02880","description":"<p>A major drawback of modern neural OpenIE systems and benchmarks is that they\nprioritize high coverage of information in extractions over compactness of\ntheir constituents. This severely limits the usefulness of OpenIE extractions\nin many downstream tasks. The utility of extractions can be improved if\nextractions are compact and share constituents. To this end, we study the\nproblem of identifying compact extractions with neural-based methods. We\npropose CompactIE, an OpenIE system that uses a novel pipelined approach to\nproduce compact extractions with overlapping constituents. It first detects\nconstituents of the extractions and then links them to build extractions. We\ntrain our system on compact extractions obtained by processing existing\nbenchmarks. Our experiments on CaRB and Wire57 datasets indicate that CompactIE\nfinds 1.5x-2x more compact extractions than previous systems, with high\nprecision, establishing a new state-of-the-art performance in OpenIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayat_F/0/1/0/all/0/1\">Farima Fatahi Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadish_H/0/1/0/all/0/1\">H.V. Jagadish</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology Reuse: the Real Test of Ontological Design. (arXiv:2205.02892v1 [cs.SE])","link":"http://arxiv.org/abs/2205.02892","description":"<p>Reusing ontologies in practice is still very challenging, especially when\nmultiple ontologies are involved. Moreover, despite recent advances, systematic\nontology quality assurance remains a difficult problem. In this work, the\nquality of thirty biomedical ontologies, and the Computer Science Ontology, are\ninvestigated from the perspective of a practical use case. Special scrutiny is\ngiven to cross-ontology references, which are vital for combining ontologies.\nDiverse methods to detect the issues are proposed, including natural language\nprocessing and network analysis. Moreover, several suggestions for improving\nontologies and their quality assurance processes are presented. It is argued\nthat while the advancing automatic tools for ontology quality assurance are\ncrucial for ontology improvement, they will not solve the problem entirely. It\nis ontology reuse that is the ultimate method for continuously verifying and\nimproving ontology quality, as well as for guiding its future development. Many\nissues can be found and fixed only through practical and diverse ontology reuse\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sowinski_P/0/1/0/all/0/1\">Piotr Sowinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasielewska_Michniewska_K/0/1/0/all/0/1\">Katarzyna Wasielewska-Michniewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganzha_M/0/1/0/all/0/1\">Maria Ganzha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paprzycki_M/0/1/0/all/0/1\">Marcin Paprzycki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badica_C/0/1/0/all/0/1\">Costin Badica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Model Cards: A Human-Centered Approach to Model Documentation. (arXiv:2205.02894v1 [cs.HC])","link":"http://arxiv.org/abs/2205.02894","description":"<p>Deep learning models for natural language processing (NLP) are increasingly\nadopted and deployed by analysts without formal training in NLP or machine\nlearning (ML). However, the documentation intended to convey the model's\ndetails and appropriate use is tailored primarily to individuals with ML or NLP\nexpertise. To address this gap, we conduct a design inquiry into interactive\nmodel cards, which augment traditionally static model cards with affordances\nfor exploring model documentation and interacting with the models themselves.\nOur investigation consists of an initial conceptual study with experts in ML,\nNLP, and AI Ethics, followed by a separate evaluative study with non-expert\nanalysts who use ML models in their work. Using a semi-structured interview\nformat coupled with a think-aloud protocol, we collected feedback from a total\nof 30 participants who engaged with different versions of standard and\ninteractive model cards. Through a thematic analysis of the collected data, we\nidentified several conceptual dimensions that summarize the strengths and\nlimitations of standard and interactive model cards, including: stakeholders;\ndesign; guidance; understandability &amp; interpretability; sensemaking &amp;\nskepticism; and trust &amp; safety. Our findings demonstrate the importance of\ncarefully considered design and interactivity for orienting and supporting\nnon-expert analysts using deep learning models, along with a need for\nconsideration of broader sociotechnical contexts and organizational dynamics.\nWe have also identified design elements, such as language, visual cues, and\nwarnings, among others, that support interactivity and make non-interactive\ncontent accessible. We summarize our findings as design guidelines and discuss\ntheir implications for a human-centered approach towards AI/ML documentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crisan_A/0/1/0/all/0/1\">Anamaria Crisan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drouhard_M/0/1/0/all/0/1\">Margaret Drouhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Rajani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Analysis of Daily Dialog Data using Polite Emotional Dialogue Acts. (arXiv:2205.02921v1 [cs.CL])","link":"http://arxiv.org/abs/2205.02921","description":"<p>Many socio-linguistic cues are used in conversational analysis, such as\nemotion, sentiment, and dialogue acts. One of the fundamental cues is\npoliteness, which linguistically possesses properties such as social manners\nuseful in conversational analysis. This article presents findings of polite\nemotional dialogue act associations, where we can correlate the relationships\nbetween the socio-linguistic cues. We confirm our hypothesis that the\nutterances with the emotion classes Anger and Disgust are more likely to be\nimpolite. At the same time, Happiness and Sadness are more likely to be polite.\nA less expectable phenomenon occurs with dialogue acts Inform and Commissive\nwhich contain more polite utterances than Question and Directive. Finally, we\nconclude on the future work of these findings to extend the learning of social\nbehaviours using politeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bothe_C/0/1/0/all/0/1\">Chandrakant Bothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Propaganda Techniques in Visuo-Lingual Metaphor in Memes. (arXiv:2205.02937v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02937","description":"<p>The exponential rise of social media networks has allowed the production,\ndistribution, and consumption of data at a phenomenal rate. Moreover, the\nsocial media revolution has brought a unique phenomenon to social media\nplatforms called Internet memes. Internet memes are one of the most popular\ncontents used on social media, and they can be in the form of images with a\nwitty, catchy, or satirical text description. In this paper, we are dealing\nwith propaganda that is often seen in Internet memes in recent times.\nPropaganda is communication, which frequently includes psychological and\nrhetorical techniques to manipulate or influence an audience to act or respond\nas the propagandist wants. To detect propaganda in Internet memes, we propose a\nmultimodal deep learning fusion system that fuses the text and image feature\nrepresentations and outperforms individual models based solely on either text\nor image modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gundapu_S/0/1/0/all/0/1\">Sunil Gundapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining the Effectiveness of Multi-Task Learning for Efficient Knowledge Extraction from Spine MRI Reports. (arXiv:2205.02979v1 [cs.LG])","link":"http://arxiv.org/abs/2205.02979","description":"<p>Pretrained Transformer based models finetuned on domain specific corpora have\nchanged the landscape of NLP. However, training or fine-tuning these models for\nindividual tasks can be time consuming and resource intensive. Thus, a lot of\ncurrent research is focused on using transformers for multi-task learning\n(Raffel et al.,2020) and how to group the tasks to help a multi-task model to\nlearn effective representations that can be shared across tasks (Standley et\nal., 2020; Fifty et al., 2021). In this work, we show that a single\nmulti-tasking model can match the performance of task specific models when the\ntask specific models show similar representations across all of their hidden\nlayers and their gradients are aligned, i.e. their gradients follow the same\ndirection. We hypothesize that the above observations explain the effectiveness\nof multi-task learning. We validate our observations on our internal\nradiologist-annotated datasets on the cervical and lumbar spine. Our method is\nsimple and intuitive, and can be used in a wide range of NLP problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sehanobish_A/0/1/0/all/0/1\">Arijit Sehanobish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandora_M/0/1/0/all/0/1\">McCullen Sandora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_N/0/1/0/all/0/1\">Nabila Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawar_J/0/1/0/all/0/1\">Jayashri Pawar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_D/0/1/0/all/0/1\">Danielle Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anasuya Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_M/0/1/0/all/0/1\">Murray Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_R/0/1/0/all/0/1\">Richard Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odry_B/0/1/0/all/0/1\">Benjamin Odry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vianu_R/0/1/0/all/0/1\">Ron Vianu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Noisy Label Correction for Fine-Grained Entity Typing. (arXiv:2205.03011v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03011","description":"<p>Fine-grained entity typing (FET) aims to assign proper semantic types to\nentity mentions according to their context, which is a fundamental task in\nvarious entity-leveraging applications. Current FET systems usually establish\non large-scale weakly-supervised/distantly annotation data, which may contain\nabundant noise and thus severely hinder the performance of the FET task.\nAlthough previous studies have made great success in automatically identifying\nthe noisy labels in FET, they usually rely on some auxiliary resources which\nmay be unavailable in real-world applications (e.g. pre-defined hierarchical\ntype structures, human-annotated subsets). In this paper, we propose a novel\napproach to automatically correct noisy labels for FET without external\nresources. Specifically, it first identifies the potentially noisy labels by\nestimating the posterior probability of a label being positive or negative\naccording to the logits output by the model, and then relabel candidate noisy\nlabels by training a robust model over the remaining clean labels. Experiments\non two popular benchmarks prove the effectiveness of our method. Our source\ncode can be obtained from \\url{https://github.com/CCIIPLab/DenoiseFET}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiran Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feida Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aksharantar: Towards building open transliteration tools for the next billion users. (arXiv:2205.03018v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03018","description":"<p>We introduce Aksharantar, the largest publicly available transliteration\ndataset for 21 Indic languages containing 26 million transliteration pairs. We\nbuild this dataset by mining transliteration pairs from large monolingual and\nparallel corpora, as well as collecting transliterations from human annotators\nto ensure diversity of words and representation of low-resource languages. We\nintroduce a new, large, diverse testset for Indic language transliteration\ncontaining 103k words pairs spanning 19 languages that enables fine-grained\nanalysis of transliteration models.\n</p>\n<p>We train the IndicXlit model on the Aksharantar training set. IndicXlit is a\nsingle transformer-based multilingual transliteration model for roman to Indic\nscript conversion supporting 21 Indic languages. It achieves state-of-the art\nresults on the Dakshina testset, and establishes strong baselines on the\nAksharantar testset released along with this work.\n</p>\n<p>All the datasets and models are publicly available at\nhttps://indicnlp.ai4bharat.org/aksharantar. We hope the availability of these\nlarge-scale, open resources will spur innovation for Indic language\ntransliteration and downstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhani_Y/0/1/0/all/0/1\">Yash Madhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthan_S/0/1/0/all/0/1\">Sushane Parthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedekar_P/0/1/0/all/0/1\">Priyanka Bedekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_R/0/1/0/all/0/1\">Ruchi Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_V/0/1/0/all/0/1\">Vivek Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hearing voices at the National Library -- a speech corpus and acoustic model for the Swedish language. (arXiv:2205.03026v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03026","description":"<p>This paper explains our work in developing new acoustic models for automated\nspeech recognition (ASR) at KBLab, the infrastructure for data-driven research\nat the National Library of Sweden (KB). We evaluate different approaches for a\nviable speech-to-text pipeline for audiovisual resources in Swedish, using the\nwav2vec 2.0 architecture in combination with speech corpuses created from KB's\ncollections. These approaches include pretraining an acoustic model for Swedish\nfrom the ground up, and fine-tuning existing monolingual and multilingual\nmodels. The collections-based corpuses we use have been sampled from millions\nof hours of speech, with a conscious attempt to balance regional dialects to\nproduce a more representative, and thus more democratic, model. The acoustic\nmodel this enabled, \"VoxRex\", outperforms existing models for Swedish ASR. We\nalso evaluate combining this model with various pretrained language models,\nwhich further enhanced performance. We conclude by highlighting the potential\nof such technology for cultural heritage institutions with vast collections of\npreviously unlabelled audiovisual data. Our models are released for further\nexploration and research here: https://huggingface.co/KBLab.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malmsten_M/0/1/0/all/0/1\">Martin Malmsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffenden_C/0/1/0/all/0/1\">Chris Haffenden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borjeson_L/0/1/0/all/0/1\">Love B&#xf6;rjeson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Multi-Domain False News and Underlying User Effects on Chinese Weibo. (arXiv:2205.03068v1 [cs.SI])","link":"http://arxiv.org/abs/2205.03068","description":"<p>False news that spreads on social media has proliferated over the past years\nand has led to multi-aspect threats in the real world. While there are studies\nof false news on specific domains (like politics or health care), little work\nis found comparing false news across domains. In this article, we investigate\nfalse news across nine domains on Weibo, the largest Twitter-like social media\nplatform in China, from 2009 to 2019. The newly collected data comprise 44,728\nposts in the nine domains, published by 40,215 users, and reposted over 3.4\nmillion times. Based on the distributions and spreads of the multi-domain\ndataset, we observe that false news in domains that are close to daily life\nlike health and medicine generated more posts but diffused less effectively\nthan those in other domains like politics, and that political false news had\nthe most effective capacity for diffusion. The widely diffused false news posts\non Weibo were associated strongly with certain types of users -- by gender,\nage, etc. Further, these posts provoked strong emotions in the reposts and\ndiffused further with the active engagement of false-news starters. Our\nfindings have the potential to help design false news detection systems in\nsuspicious news discovery, veracity prediction, and display and explanation.\nThe comparison of the findings on Weibo with those of existing work\ndemonstrates nuanced patterns, suggesting the need for more research on data\nfrom diverse platforms, countries, or languages to tackle the global issue of\nfalse news. The code and new anonymized dataset are available at\nhttps://github.com/ICTMCG/Characterizing-Weibo-Multi-Domain-False-News.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_H/0/1/0/all/0/1\">H. Russell Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering. (arXiv:2205.03071v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03071","description":"<p>Extractive Question Answering (EQA) is one of the most important tasks in\nMachine Reading Comprehension (MRC), which can be solved by fine-tuning the\nspan selecting heads of Pre-trained Language Models (PLMs). However, most\nexisting approaches for MRC may perform poorly in the few-shot learning\nscenario. To solve this issue, we propose a novel framework named Knowledge\nEnhanced Contrastive Prompt-tuning (KECP). Instead of adding pointer heads to\nPLMs, we introduce a seminal paradigm for EQA that transform the task into a\nnon-autoregressive Masked Language Modeling (MLM) generation problem.\nSimultaneously, rich semantics from the external knowledge base (KB) and the\npassage context are support for enhancing the representations of the query. In\naddition, to boost the performance of PLMs, we jointly train the model by the\nMLM and contrastive learning objectives. Experiments on multiple benchmarks\ndemonstrate that our method consistently outperforms state-of-the-art\napproaches in few-shot settings by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qiuhui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QLEVR: A Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning. (arXiv:2205.03075v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03075","description":"<p>Synthetic datasets have successfully been used to probe visual\nquestion-answering datasets for their reasoning abilities. CLEVR\n(johnson2017clevr), for example, tests a range of visual reasoning abilities.\nThe questions in CLEVR focus on comparisons of shapes, colors, and sizes,\nnumerical reasoning, and existence claims. This paper introduces a minimally\nbiased, diagnostic visual question-answering dataset, QLEVR, that goes beyond\nexistential and numerical quantification and focus on more complex quantifiers\nand their combinations, e.g., asking whether there are more than two red balls\nthat are smaller than at least three blue balls in an image. We describe how\nthe dataset was created and present a first evaluation of state-of-the-art\nvisual question-answering models, showing that QLEVR presents a formidable\nchallenge to our current models. Code and Dataset are available at\nhttps://github.com/zechenli03/QLEVR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning with Noisy User Feedback. (arXiv:2205.03092v1 [cs.LG])","link":"http://arxiv.org/abs/2205.03092","description":"<p>Machine Learning (ML) systems are getting increasingly popular, and drive\nmore and more applications and services in our daily life. This has led to\ngrowing concerns over user privacy, since human interaction data typically\nneeds to be transmitted to the cloud in order to train and improve such\nsystems. Federated learning (FL) has recently emerged as a method for training\nML models on edge devices using sensitive user data and is seen as a way to\nmitigate concerns over data privacy. However, since ML models are most commonly\ntrained with label supervision, we need a way to extract labels on edge to make\nFL viable. In this work, we propose a strategy for training FL models using\npositive and negative user feedback. We also design a novel framework to study\ndifferent noise patterns in user feedback, and explore how well standard\nnoise-robust objectives can help mitigate this noise when training models in a\nfederated setting. We evaluate our proposed training setup through detailed\nexperiments on two text classification datasets and analyze the effects of\nvarying levels of user reliability and feedback noise on model performance. We\nshow that our method improves substantially over a self-training baseline,\nachieving performance closer to models trained with full supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rahul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishna_A/0/1/0/all/0/1\">Anil Ramakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacLaughlin_A/0/1/0/all/0/1\">Ansel MacLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majmudar_J/0/1/0/all/0/1\">Jimit Majmudar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1\">Clement Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emp-RFT: Empathetic Response Generation via Recognizing Feature Transitions between Utterances. (arXiv:2205.03112v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03112","description":"<p>Each utterance in multi-turn empathetic dialogues has features such as\nemotion, keywords, and utterance-level meaning. Feature transitions between\nutterances occur naturally. However, existing approaches fail to perceive the\ntransitions because they extract features for the context at the coarse-grained\nlevel. To solve the above issue, we propose a novel approach of recognizing\nfeature transitions between utterances, which helps understand the dialogue\nflow and better grasp the features of utterance that needs attention. Also, we\nintroduce a response generation strategy to help focus on emotion and keywords\nrelated to appropriate features when generating responses. Experimental results\nshow that our approach outperforms baselines and especially, achieves\nsignificant improvements on multi-turn dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wongyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1\">Youbin Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyong-Ho Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic Fake News Detection Based on Deep Contextualized Embedding Models. (arXiv:2205.03114v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03114","description":"<p>Social media is becoming a source of news for many people due to its ease and\nfreedom of use. As a result, fake news has been spreading quickly and easily\nregardless of its credibility, especially in the last decade. Fake news\npublishers take advantage of critical situations such as the Covid-19 pandemic\nand the American presidential elections to affect societies negatively. Fake\nnews can seriously impact society in many fields including politics, finance,\nsports, etc. Many studies have been conducted to help detect fake news in\nEnglish, but research conducted on fake news detection in the Arabic language\nis scarce. Our contribution is twofold: first, we have constructed a large and\ndiverse Arabic fake news dataset. Second, we have developed and evaluated\ntransformer-based classifiers to identify fake news while utilizing eight\nstate-of-the-art Arabic contextualized embedding models. The majority of these\nmodels had not been previously used for Arabic fake news detection. We conduct\na thorough analysis of the state-of-the-art Arabic contextualized embedding\nmodels as well as comparison with similar fake news detection systems.\nExperimental results confirm that these state-of-the-art models are robust,\nwith accuracy exceeding 98%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nassif_A/0/1/0/all/0/1\">Ali Bou Nassif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elnagar_A/0/1/0/all/0/1\">Ashraf Elnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgendy_O/0/1/0/all/0/1\">Omar Elgendy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afadar_Y/0/1/0/all/0/1\">Yaman Afadar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Domain Gap for Stance Detection for the Zulu language. (arXiv:2205.03153v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03153","description":"<p>Misinformation has become a major concern in recent last years given its\nspread across our information sources. In the past years, many NLP tasks have\nbeen introduced in this area, with some systems reaching good results on\nEnglish language datasets. Existing AI based approaches for fighting\nmisinformation in literature suggest automatic stance detection as an integral\nfirst step to success. Our paper aims at utilizing this progress made for\nEnglish to transfers that knowledge into other languages, which is a\nnon-trivial task due to the domain gap between English and the target\nlanguages. We propose a black-box non-intrusive method that utilizes techniques\nfrom Domain Adaptation to reduce the domain gap, without requiring any human\nexpertise in the target language, by leveraging low-quality data in both a\nsupervised and unsupervised manner. This allows us to rapidly achieve similar\nresults for stance detection for the Zulu language, the target language in this\nwork, as are found for English. We also provide a stance detection dataset in\nthe Zulu language. Our experimental results show that by leveraging English\ndatasets and machine translation we can increase performances on both English\ndata along with other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dlamini_G/0/1/0/all/0/1\">Gcinizwe Dlamini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkouch_I/0/1/0/all/0/1\">Imad Eddine Ibrahim Bekkouch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Adil Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review on Text-Based Emotion Detection -- Techniques, Applications, Datasets, and Future Directions. (arXiv:2205.03235v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03235","description":"<p>Artificial Intelligence (AI) has been used for processing data to make\ndecisions, interact with humans, and understand their feelings and emotions.\nWith the advent of the internet, people share and express their thoughts on\nday-to-day activities and global and local events through text messaging\napplications. Hence, it is essential for machines to understand emotions in\nopinions, feedback, and textual dialogues to provide emotionally aware\nresponses to users in today's online world. The field of text-based emotion\ndetection (TBED) is advancing to provide automated solutions to various\napplications, such as businesses, and finances, to name a few. TBED has gained\na lot of attention in recent times. The paper presents a systematic literature\nreview of the existing literature published between 2005 to 2021 in TBED. This\nreview has meticulously examined 63 research papers from IEEE, Science Direct,\nScopus, and Web of Science databases to address four primary research\nquestions. It also reviews the different applications of TBED across various\nresearch domains and highlights its use. An overview of various emotion models,\ntechniques, feature extraction methods, datasets, and research challenges with\nfuture directions has also been represented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kusal_S/0/1/0/all/0/1\">Sheetal Kusal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Shruti Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudrie_J/0/1/0/all/0/1\">Jyoti Choudrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_K/0/1/0/all/0/1\">Ketan Kotecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vora_D/0/1/0/all/0/1\">Deepali Vora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_I/0/1/0/all/0/1\">Ilias Pappas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collective Relevance Labeling for Passage Retrieval. (arXiv:2205.03273v1 [cs.IR])","link":"http://arxiv.org/abs/2205.03273","description":"<p>Deep learning for Information Retrieval (IR) requires a large amount of\nhigh-quality query-document relevance labels, but such labels are inherently\nsparse. Label smoothing redistributes some observed probability mass over\nunobserved instances, often uniformly, uninformed of the true distribution. In\ncontrast, we propose knowledge distillation for informed labeling, without\nincurring high computation overheads at evaluation time. Our contribution is\ndesigning a simple but efficient teacher model which utilizes collective\nknowledge, to outperform state-of-the-arts distilled from a more complex\nteacher model. Specifically, we train up to x8 faster than the state-of-the-art\nteacher, while distilling the rankings better. Our code is publicly available\nat https://github.com/jihyukkim-nlp/CollectiveKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihyuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsso Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers. (arXiv:2205.03286v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03286","description":"<p>There has been a growing interest in interpreting the underlying dynamics of\nTransformers. While self-attention patterns were initially deemed as the\nprimary option, recent studies have shown that integrating other components can\nyield more accurate explanations. This paper introduces a novel token\nattribution analysis method that incorporates all the components in the encoder\nblock and aggregates this throughout layers. Through extensive quantitative and\nqualitative experiments, we demonstrate that our method can produce faithful\nand meaningful global token attributions. Our experiments reveal that\nincorporating almost every encoder component results in increasingly more\naccurate analysis in both local (single layer) and global (the whole model)\nsettings. Our global attribution analysis significantly outperforms previous\nmethods on various tasks regarding correlation with gradient-based saliency\nscores. Our code is freely available at\nhttps://github.com/mohsenfayyaz/GlobEnc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modarressi_A/0/1/0/all/0/1\">Ali Modarressi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media. (arXiv:2205.03296v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03296","description":"<p>Building models to detect vaccine attitudes on social media is challenging\nbecause of the composite, often intricate aspects involved, and the limited\navailability of annotated data. Existing approaches have relied heavily on\nsupervised training that requires abundant annotations and pre-defined aspect\ncategories. Instead, with the aim of leveraging the large amount of unannotated\ndata now available on vaccination, we propose a novel semi-supervised approach\nfor vaccine attitude detection, called VADet. A variational autoencoding\narchitecture based on language models is employed to learn from unlabelled data\nthe topical information of the domain. Then, the model is fine-tuned with a few\nmanually annotated examples of user attitudes. We validate the effectiveness of\nVADet on our annotated data and also on an existing vaccination corpus\nannotated with opinions on vaccines. Our results show that VADet is able to\nlearn disentangled stance and aspect topics, and outperforms existing\naspect-based sentiment analysis models on both stance detection and tweet\nclustering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lixing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection. (arXiv:2205.03302v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03302","description":"<p>We present a novel feature attribution method for explaining text\nclassifiers, and analyze it in the context of hate speech detection. Although\nfeature attribution models usually provide a single importance score for each\ntoken, we instead provide two complementary and theoretically-grounded scores\n-- necessity and sufficiency -- resulting in more informative explanations. We\npropose a transparent method that calculates these values by generating\nexplicit perturbations of the input text, allowing the importance scores\nthemselves to be explainable. We employ our method to explain the predictions\nof different hate speech detection models on the same set of curated examples\nfrom a test suite, and show that different values of necessity and sufficiency\nfor identity terms correspond to different kinds of false positive errors,\nexposing sources of classifier bias against marginalized groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balkir_E/0/1/0/all/0/1\">Esma Balkir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1\">Kathleen C. Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Humor and Sarcasm for Improving Political Parody Detection. (arXiv:2205.03313v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03313","description":"<p>Parody is a figurative device used for mimicking entities for comedic or\ncritical purposes. Parody is intentionally humorous and often involves sarcasm.\nThis paper explores jointly modelling these figurative tropes with the goal of\nimproving performance of political parody detection in tweets. To this end, we\npresent a multi-encoder model that combines three parallel encoders to enrich\nparody-specific representations with humor and sarcasm information. Experiments\non a publicly available data set of political parody tweets demonstrate that\nour approach outperforms previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiao Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_D/0/1/0/all/0/1\">Danae S&#xe1;nchez Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preotiuc_Pietro_D/0/1/0/all/0/1\">Daniel Preo&#x163;iuc-Pietro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Example-Based Machine Translation from Text to a Hierarchical Representation of Sign Language. (arXiv:2205.03314v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03314","description":"<p>This article presents an original method for Text-to-Sign Translation. It\ncompensates data scarcity using a domain-specific parallel corpus of alignments\nbetween text and hierarchical formal descriptions of Sign Language videos in\nAZee. Based on the detection of similarities present in the source text, the\nproposed algorithm recursively exploits matches and substitutions of aligned\nsegments to build multiple candidate translations for a novel statement. This\nhelps preserving Sign Language structures as much as possible before falling\nback on literal translations too quickly, in a generative way. The resulting\ntranslations are in the form of AZee expressions, designed to be used as input\nto avatar synthesis systems. We present a test set tailored to showcase its\npotential for expressiveness and generation of idiomatic target language, and\nobserved limitations. This work finally opens prospects on how to evaluate\ntranslation and linguistic aspects, such as accuracy and grammatical fluency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertin_Lemee_E/0/1/0/all/0/1\">&#xc9;lise Bertin-Lem&#xe9;e</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braffort_A/0/1/0/all/0/1\">Annelies Braffort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Challant_C/0/1/0/all/0/1\">Camille Challant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danet_C/0/1/0/all/0/1\">Claire Danet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filhol_M/0/1/0/all/0/1\">Michael Filhol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Synthesis and Fusion and their Impact on Machine Translation. (arXiv:2205.03369v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03369","description":"<p>Theoretical work in morphological typology offers the possibility of\nmeasuring morphological diversity on a continuous scale. However, literature in\nNatural Language Processing (NLP) typically labels a whole language with a\nstrict type of morphology, e.g. fusional or agglutinative. In this work, we\npropose to reduce the rigidity of such claims, by quantifying morphological\ntypology at the word and segment level. We consider Payne (2017)'s approach to\nclassify morphology using two indices: synthesis (e.g. analytic to\npolysynthetic) and fusion (agglutinative to fusional). For computing synthesis,\nwe test unsupervised and supervised morphological segmentation methods for\nEnglish, German and Turkish, whereas for fusion, we propose a semi-automatic\nmethod using Spanish as a case study. Then, we analyse the relationship between\nmachine translation quality and the degree of synthesis and fusion at word\n(nouns and verbs for English-Turkish, and verbs in English-Spanish) and segment\nlevel (previous language pairs plus English-German in both directions). We\ncomplement the word-level analysis with human evaluation, and overall, we\nobserve a consistent impact of both indexes on machine translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oncevay_A/0/1/0/all/0/1\">Arturo Oncevay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkel_N/0/1/0/all/0/1\">Niels van Berkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bjerva_J/0/1/0/all/0/1\">Johannes Bjerva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Unreliability of Explanations in Few-Shot In-Context Learning. (arXiv:2205.03401v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03401","description":"<p>How can prompting a large language model like GPT-3 with explanations improve\nin-context learning? We focus specifically on two NLP tasks that involve\nreasoning over text, namely question answering and natural language inference.\nIncluding explanations in the prompt and having the model generate them does\nnot consistently improve performance in the settings we study, contrary to\nrecent results on symbolic reasoning tasks (Nye et al., 2021; Wei et al.,\n2022). Despite careful prompting, explanations generated by GPT-3 may not even\nbe factually grounded in the input, even on simple tasks with straightforward\nextractive explanations. However, these flawed explanations can still be useful\nas a way to verify GPT-3's predictions post-hoc. Through analysis in three\nsettings, we show that explanations judged as good by humans--those that are\nlogically consistent with the input and the prediction--usually indicate more\naccurate predictions. Following these observations, we present a framework for\ncalibrating model predictions based on the reliability of the explanations. Our\nframework trains calibrators using automatically extracted scores that\napproximately assess the reliability of explanations, which helps improve\nperformance across three different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data Cartography based MixUp for Pre-trained Language Models. (arXiv:2205.03403v1 [cs.CL])","link":"http://arxiv.org/abs/2205.03403","description":"<p>MixUp is a data augmentation strategy where additional samples are generated\nduring training by combining random pairs of training samples and their labels.\nHowever, selecting random pairs is not potentially an optimal choice. In this\nwork, we propose TDMixUp, a novel MixUp strategy that leverages Training\nDynamics and allows more informative samples to be combined for generating new\ndata samples. Our proposed TDMixUp first measures confidence, variability,\n(Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al.,\n2020) to identify the characteristics of training samples (e.g., as\neasy-to-learn or ambiguous samples), and then interpolates these characterized\nsamples. We empirically validate that our method not only achieves competitive\nperformance using a smaller subset of the training data compared with strong\nbaselines, but also yields lower expected calibration error on the pre-trained\nlanguage model, BERT, on both in-domain and out-of-domain settings in a wide\nrange of NLP tasks. We publicly release our code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seo Yeon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adjusting for Confounders with Text: Challenges and an Empirical Evaluation Framework for Causal Inference. (arXiv:2009.09961v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.09961","description":"<p>Causal inference studies using textual social media data can provide\nactionable insights on human behavior. Making accurate causal inferences with\ntext requires controlling for confounding which could otherwise impart bias.\nRecently, many different methods for adjusting for confounders have been\nproposed, and we show that these existing methods disagree with one another on\ntwo datasets inspired by previous social media studies. Evaluating causal\nmethods is challenging, as ground truth counterfactuals are almost never\navailable. Presently, no empirical evaluation framework for causal methods\nusing text exists, and as such, practitioners must select their methods without\nguidance. We contribute the first such framework, which consists of five tasks\ndrawn from real world studies. Our framework enables the evaluation of any\ncasual inference method using text. Across 648 experiments and two datasets, we\nevaluate every commonly used causal inference method and identify their\nstrengths and weaknesses to inform social media researchers seeking to use such\nmethods, and guide future improvements. We make all tasks, data, and models\npublic to inform applications and encourage additional research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weld_G/0/1/0/all/0/1\">Galen Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glenski_M/0/1/0/all/0/1\">Maria Glenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbour_D/0/1/0/all/0/1\">David Arbour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Ryan Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks. (arXiv:2104.08815v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08815","description":"<p>Increasing concerns and regulations about data privacy and sparsity\nnecessitate the study of privacy-preserving, decentralized learning methods for\nnatural language processing (NLP) tasks. Federated learning (FL) provides\npromising approaches for a large number of clients (e.g., personal devices or\norganizations) to collaboratively learn a shared global model to benefit all\nclients while allowing users to keep their data locally. Despite interest in\nstudying FL methods for NLP tasks, a systematic comparison and analysis is\nlacking in the literature. Herein, we present the FedNLP, a benchmarking\nframework for evaluating federated learning methods on four different task\nformulations: text classification, sequence tagging, question answering, and\nseq2seq. We propose a universal interface between Transformer-based language\nmodels (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under\nvarious non-IID partitioning strategies. Our extensive experiments with FedNLP\nprovide empirical comparisons between FL methods and helps us better understand\nthe inherent challenges of this direction. The comprehensive analysis points to\nintriguing and exciting future research aimed at developing FL methods for NLP\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zihang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (arXiv:2108.05921v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05921","description":"<p>Detecting online hate is a complex task, and low-performing models have\nharmful consequences when used for sensitive applications such as content\nmoderation. Emoji-based hate is an emerging challenge for automated detection.\nWe present HatemojiCheck, a test suite of 3,930 short-form statements that\nallows us to evaluate performance on hateful language expressed with emoji.\nUsing the test suite, we expose weaknesses in existing hate detection models.\nTo address these weaknesses, we create the HatemojiBuild dataset using a\nhuman-and-model-in-the-loop approach. Models built with these 5,912 adversarial\nexamples perform substantially better at detecting emoji-based hate, while\nretaining strong performance on text-only hate. Both HatemojiCheck and\nHatemojiBuild are made publicly available. See our Github Repository\n(https://github.com/HannahKirk/Hatemoji). HatemojiCheck, HatemojiBuild, and the\nfinal Hatemoji Model are also available on HuggingFace\n(https://huggingface.co/datasets/HannahRoseKirk/).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertram Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling. (arXiv:2109.13023v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13023","description":"<p>Few-Shot Sequence Labeling (FSSL) is a canonical paradigm for the tagging\nmodels, e.g., named entity recognition and slot filling, to generalize on an\nemerging, resource-scarce domain. Recently, the metric-based meta-learning\nframework has been recognized as a promising approach for FSSL. However, most\nprior works assign a label to each token based on the token-level similarities,\nwhich ignores the integrality of named entities or slots. To this end, in this\npaper, we propose ESD, an Enhanced Span-based Decomposition method for FSSL.\nESD formulates FSSL as a span-level matching problem between test query and\nsupporting instances. Specifically, ESD decomposes the span matching problem\ninto a series of span-level procedures, mainly including enhanced span\nrepresentation, class prototype aggregation and span conflicts resolution.\nExtensive experiments show that ESD achieves the new state-of-the-art results\non two popular FSSL benchmarks, FewNERD and SNIPS, and is proven to be more\nrobust in the nested and noisy tagging scenarios. Our code is available at\nhttps://github.com/Wangpeiyi9979/ESD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PARE: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction. (arXiv:2110.07415v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07415","description":"<p>Neural models for distantly supervised relation extraction (DS-RE) encode\neach sentence in an entity-pair bag separately. These are then aggregated for\nbag-level relation prediction. Since, at encoding time, these approaches do not\nallow information to flow from other sentences in the bag, we believe that they\ndo not utilize the available bag data to the fullest. In response, we explore a\nsimple baseline approach (PARE) in which all sentences of a bag are\nconcatenated into a passage of sentences, and encoded jointly using BERT. The\ncontextual embeddings of tokens are aggregated using attention with the\ncandidate relation as query -- this summary of whole passage predicts the\ncandidate relation. We find that our simple baseline solution outperforms\nexisting state-of-the-art DS-RE models in both monolingual and multilingual\nDS-RE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rathore_V/0/1/0/all/0/1\">Vipul Rathore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badola_K/0/1/0/all/0/1\">Kartikeya Badola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding. (arXiv:2110.14170v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.14170","description":"<p>Knowledge graphs (KGs) consisting of a large number of triples have become\nwidespread recently, and many knowledge graph embedding (KGE) methods are\nproposed to embed entities and relations of a KG into continuous vector spaces.\nSuch embedding methods simplify the operations of conducting various in-KG\ntasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering).\nThey can be viewed as general solutions for representing KGs. However, existing\nKGE methods are not applicable to inductive settings, where a model trained on\nsource KGs will be tested on target KGs with entities unseen during model\ntraining. Existing works focusing on KGs in inductive settings can only solve\nthe inductive relation prediction task. They can not handle other out-of-KG\ntasks as general as KGE methods since they don't produce embeddings for\nentities. In this paper, to achieve inductive knowledge graph embedding, we\npropose a model MorsE, which does not learn embeddings for entities but learns\ntransferable meta-knowledge that can be used to produce entity embeddings. Such\nmeta-knowledge is modeled by entity-independent modules and learned by\nmeta-learning. Experimental results show that our model significantly\noutperforms corresponding baselines for in-KG and out-of-KG tasks in inductive\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zonggang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offense Detection in Dravidian Languages using Code-Mixing Index based Focal Loss. (arXiv:2111.06916v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06916","description":"<p>Over the past decade, we have seen exponential growth in online content\nfueled by social media platforms. Data generation of this scale comes with the\ncaveat of insurmountable offensive content in it. The complexity of identifying\noffensive content is exacerbated by the usage of multiple modalities (image,\nlanguage, etc.), code-mixed language and more. Moreover, even after careful\nsampling and annotation of offensive content, there will always exist a\nsignificant class imbalance between offensive and non-offensive content. In\nthis paper, we introduce a novel Code-Mixing Index (CMI) based focal loss which\ncircumvents two challenges (1) code-mixing in languages (2) class imbalance\nproblem for Dravidian language offense detection. We also replace the\nconventional dot product-based classifier with the cosine-based classifier\nwhich results in a boost in performance. Further, we use multilingual models\nthat help transfer characteristics learnt across languages to work effectively\nwith low resourced languages. It is also important to note that our model\nhandles instances of mixed script (say usage of Latin and Dravidian-Tamil\nscript) as well. To summarize, our model can handle offensive language\ndetection in a low-resource, class imbalanced, multilingual and code-mixed\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tula_D/0/1/0/all/0/1\">Debapriya Tula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MS_S/0/1/0/all/0/1\">Shreyas MS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_V/0/1/0/all/0/1\">Viswanatha Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1\">Pranjal Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potluri_P/0/1/0/all/0/1\">Prathyush Potluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukumaran_R/0/1/0/all/0/1\">Rohan Sukumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Context Question Answering via Supervised Contrastive Learning. (arXiv:2112.08777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08777","description":"<p>Long-context question answering (QA) tasks require reasoning over a long\ndocument or multiple documents. Addressing these tasks often benefits from\nidentifying a set of evidence spans (e.g., sentences), which provide supporting\nevidence for answering the question. In this work, we propose a novel method\nfor equipping long-context QA models with an additional sequence-level\nobjective for better identification of the supporting evidence. We achieve this\nvia an additional contrastive supervision signal in finetuning, where the model\nis encouraged to explicitly discriminate supporting evidence sentences from\nnegative ones by maximizing question-evidence similarity. The proposed\nadditional loss exhibits consistent improvements on three different strong\nlong-context transformer models, across two challenging question answering\nbenchmarks -- HotpotQA and QAsper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1\">Jacob Goldberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering. (arXiv:2204.04179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04179","description":"<p>Content-based collaborative filtering (CCF) predicts user-item interactions\nbased on both users' interaction history and items' content information.\nRecently, pre-trained language models (PLM) have been used to extract\nhigh-quality item encodings for CCF. However, it is resource-intensive to train\na PLM-based CCF model in an end-to-end (E2E) manner, since optimization\ninvolves back-propagating through every content encoding within a given user\ninteraction sequence. To tackle this issue, we propose GRAM (GRadient\nAccumulation for Multi-modality in CCF), which exploits the fact that a given\nitem often appears multiple times within a batch of interaction histories.\nSpecifically, Single-step GRAM aggregates each item encoding's gradients for\nback-propagation, with theoretic equivalence to the standard E2E training. As\nan extension of Single-step GRAM, we propose Multi-step GRAM, which increases\nthe gradient update latency, achieving a further speedup with drastically less\nGPU memory. GRAM significantly improves training efficiency (up to 146x) on\nfive datasets from two task domains of Knowledge Tracing and News\nRecommendation. Our code is available at https://github.com/yoonseok312/GRAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yoonseok Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyu Seok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsam Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Juneyoung Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Climate and Weather: Inspecting Depression Detection via Emotion Recognition. (arXiv:2204.14099v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14099","description":"<p>Automatic depression detection has attracted increasing amount of attention\nbut remains a challenging task. Psychological research suggests that depressive\nmood is closely related with emotion expression and perception, which motivates\nthe investigation of whether knowledge of emotion recognition can be\ntransferred for depression detection. This paper uses pretrained features\nextracted from the emotion recognition model for depression detection, further\nfuses emotion modality with audio and text to form multimodal depression\ndetection. The proposed emotion transfer improves depression detection\nperformance on DAIC-WOZ as well as increases the training stability. The\nanalysis of how the emotion expressed by depressed individuals is further\nperceived provides clues for further understanding of the relationship between\ndepression and emotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation. (arXiv:2205.01133v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01133","description":"<p>Multi-modal Machine Translation (MMT) enables the use of visual information\nto enhance the quality of translations. The visual information can serve as a\nvaluable piece of context information to decrease the ambiguity of input\nsentences. Despite the increasing popularity of such a technique, good and\nsizeable datasets are scarce, limiting the full extent of their potential.\nHausa, a Chadic language, is a member of the Afro-Asiatic language family. It\nis estimated that about 100 to 150 million people speak the language, with more\nthan 80 million indigenous speakers. This is more than any of the other Chadic\nlanguages. Despite a large number of speakers, the Hausa language is considered\nlow-resource in natural language processing (NLP). This is due to the absence\nof sufficient resources to implement most NLP tasks. While some datasets exist,\nthey are either scarce, machine-generated, or in the religious domain.\nTherefore, there is a need to create training and evaluation data for\nimplementing machine learning tasks and bridging the research gap in the\nlanguage. This work presents the Hausa Visual Genome (HaVG), a dataset that\ncontains the description of an image or a section within the image in Hausa and\nits equivalent in English. To prepare the dataset, we started by translating\nthe English description of the images in the Hindi Visual Genome (HVG) into\nHausa automatically. Afterward, the synthetic Hausa data was carefully\npost-edited considering the respective images. The dataset comprises 32,923\nimages and their descriptions that are divided into training, development,\ntest, and challenge test set. The Hausa Visual Genome is the first dataset of\nits kind and can be used for Hausa-English machine translation, multi-modal\nresearch, and image description, among various other natural language\nprocessing and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Satya Ranjan Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawud_M/0/1/0/all/0/1\">Musa Abdullahi Dawud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parida_S/0/1/0/all/0/1\">Shantipriya Parida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Sa&#x27;id Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_S/0/1/0/all/0/1\">Subhadarshi Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galadanci_B/0/1/0/all/0/1\">Bashir Shehu Galadanci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1\">Bello Shehu Bello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducibility Beyond the Research Community: Experience from NLP Beginners. (arXiv:2205.02182v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02182","description":"<p>As NLP research attracts public attention and excitement, it becomes\nincreasingly important for it to be accessible to a broad audience. As the\nresearch community works to democratize NLP, it remains unclear whether\nbeginners to the field can easily apply the latest developments. To understand\ntheir needs, we conducted a study with 93 students in an introductory NLP\ncourse, where students reproduced results of recent NLP papers. Surprisingly,\nour results suggest that their technical skill (i.e., programming experience)\nhas limited impact on their effort spent completing the exercise. Instead, we\nfind accessibility efforts by research authors to be key to a successful\nexperience, including thorough documentation and easy access to required models\nand datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Keunwoo Peter Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction. (arXiv:2205.02225v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02225","description":"<p>Unsupervised relation extraction aims to extract the relationship between\nentities from natural language sentences without prior information on\nrelational scope or distribution. Existing works either utilize self-supervised\nschemes to refine relational feature signals by iteratively leveraging adaptive\nclustering and classification that provoke gradual drift problems, or adopt\ninstance-wise contrastive learning which unreasonably pushes apart those\nsentence pairs that are semantically similar. To overcome these defects, we\npropose a novel contrastive learning framework named HiURE, which has the\ncapability to derive hierarchical signals from relational feature space using\ncross hierarchy attention and effectively optimize relation representation of\nsentences under exemplar-wise contrastive learning. Experimental results on two\npublic datasets demonstrate the advanced effectiveness and robustness of HiURE\non unsupervised relation extraction when compared with state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu`ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User-Driven Research of Medical Note Generation Software. (arXiv:2205.02549v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2205.02549","description":"<p>A growing body of work uses Natural Language Processing (NLP) methods to\nautomatically generate medical notes from audio recordings of doctor-patient\nconsultations. However, there are very few studies on how such systems could be\nused in clinical practice, how clinicians would adjust to using them, or how\nsystem design should be influenced by such considerations. In this paper, we\npresent three rounds of user studies, carried out in the context of developing\na medical note generation system. We present, analyse and discuss the\nparticipating clinicians' impressions and views of how the system ought to be\nadapted to be of value to them. Next, we describe a three-week test run of the\nsystem in a live telehealth clinical practice. Major findings include (i) the\nemergence of five different note-taking behaviours; (ii) the importance of the\nsystem generating notes in real time during the consultation; and (iii) the\nidentification of a number of clinical use cases that could prove challenging\nfor automatic note generation systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knoll_T/0/1/0/all/0/1\">Tom Knoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moramarco_F/0/1/0/all/0/1\">Francesco Moramarco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korfiatis_A/0/1/0/all/0/1\">Alex Papadopoulos Korfiatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1\">Rachel Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruffini_C/0/1/0/all/0/1\">Claudia Ruffini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_M/0/1/0/all/0/1\">Mark Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perstl_C/0/1/0/all/0/1\">Christian Perstl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1\">Ehud Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1\">Anya Belz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savkov_A/0/1/0/all/0/1\">Aleksandar Savkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"GAN Inversion for Data Augmentation to Improve Colonoscopy Lesion Classification. (arXiv:2205.02840v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02840","description":"<p>A major challenge in applying deep learning to medical imaging is the paucity\nof annotated data. This study demonstrates that synthetic colonoscopy images\ngenerated by Generative Adversarial Network (GAN) inversion can be used as\ntraining data to improve the lesion classification performance of deep learning\nmodels. This approach inverts pairs of images with the same label to a\nsemantically rich &amp; disentangled latent space and manipulates latent\nrepresentations to produce new synthetic images with the same label. We perform\nimage modality translation (style transfer) between white light and narrowband\nimaging (NBI). We also generate realistic-looking synthetic lesion images by\ninterpolating between original training images to increase the variety of\nlesion shapes in the training dataset. We show that these approaches outperform\ncomparative colonoscopy data augmentation techniques without the need to\nre-train multiple generative models. This approach also leverages information\nfrom datasets that may not have been designed for the specific colonoscopy\ndownstream task. E.g. using a bowel prep grading dataset for a polyp\nclassification task. Our experiments show this approach can perform multiple\ncolonoscopy data augmentations, which improve the downstream polyp\nclassification performance over baseline and comparison methods by up to 6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Golhar_M/0/1/0/all/0/1\">Mayank Golhar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bobrow_T/0/1/0/all/0/1\">Taylor L. Bobrow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ngamruengphong_S/0/1/0/all/0/1\">Saowanee Ngamruengphong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Durr_N/0/1/0/all/0/1\">Nicholas J. Durr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Transfer Learning for Chest Radiograph Clinical Report Generation with Modified Transformer Architectures. (arXiv:2205.02841v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02841","description":"<p>The image captioning task is increasingly prevalent in artificial\nintelligence applications for medicine. One important application is clinical\nreport generation from chest radiographs. The clinical writing of unstructured\nreports is time consuming and error-prone. An automated system would improve\nstandardization, error reduction, time consumption, and medical accessibility.\nIn this paper we demonstrate the importance of domain specific pre-training and\npropose a modified transformer architecture for the medical image captioning\ntask. To accomplish this, we train a series of modified transformers to\ngenerate clinical reports from chest radiograph image input. These modified\ntransformers include: a meshed-memory augmented transformer architecture with\nvisual extractor using ImageNet pre-trained weights, a meshed-memory augmented\ntransformer architecture with visual extractor using CheXpert pre-trained\nweights, and a meshed-memory augmented transformer whose encoder is passed the\nconcatenated embeddings using both ImageNet pre-trained weights and CheXpert\npre-trained weights. We use BLEU(1-4), ROUGE-L, CIDEr, and the clinical\nCheXbert F1 scores to validate our models and demonstrate competitive scores\nwith state of the art models. We provide evidence that ImageNet pre-training is\nill-suited for the medical image captioning task, especially for less frequent\nconditions (eg: enlarged cardiomediastinum, lung lesion, pneumothorax).\nFurthermore, we demonstrate that the double feature model improves performance\nfor specific medical conditions (edema, consolidation, pneumothorax, support\ndevices) and overall CheXbert F1 score, and should be further developed in\nfuture work. Such a double feature model, including both ImageNet pre-training\nas well as domain specific pre-training, could be used in a wide range of image\ncaptioning models in medicine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vendrow_E/0/1/0/all/0/1\">Edward Vendrow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonfeld_E/0/1/0/all/0/1\">Ethan Schonfeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InvNorm: Domain Generalization for Object Detection in Gastrointestinal Endoscopy. (arXiv:2205.02842v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02842","description":"<p>Domain Generalization is a challenging topic in computer vision, especially\nin Gastrointestinal Endoscopy image analysis. Due to several device limitations\nand ethical reasons, current open-source datasets are typically collected on a\nlimited number of patients using the same brand of sensors. Different brands of\ndevices and individual differences will significantly affect the model's\ngeneralizability. Therefore, to address the generalization problem in\nGI(Gastrointestinal) endoscopy, we propose a multi-domain GI dataset and a\nlight, plug-in block called InvNorm(Invertible Normalization), which could\nachieve a better generalization performance in any structure. Previous\nDG(Domain Generalization) methods fail to achieve invertible transformation,\nwhich would lead to some misleading augmentation. Moreover, these models would\nbe more likely to lead to medical ethics issues. Our method utilizes\nnormalizing flow to achieve invertible and explainable style normalization to\naddress the problem. The effectiveness of InvNorm is demonstrated on a wide\nrange of tasks, including GI recognition, GI object detection, and natural\nimage recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_W/0/1/0/all/0/1\">Weichen Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanbo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_K/0/1/0/all/0/1\">Kunpeng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yongxin Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Network Based Synthetic Learning and a Novel Domain Relevant Loss Term for Spine Radiographs. (arXiv:2205.02843v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02843","description":"<p>Problem: There is a lack of big data for the training of deep learning models\nin medicine, characterized by the time cost of data collection and privacy\nconcerns. Generative adversarial networks (GANs) offer both the potential to\ngenerate new data, as well as to use this newly generated data, without\ninclusion of patients' real data, for downstream applications.\n</p>\n<p>Approach: A series of GANs were trained and applied for a downstream computer\nvision spine radiograph abnormality classification task. Separate classifiers\nwere trained with either access or no access to the original imaging. Trained\nGANs included a conditional StyleGAN2 with adaptive discriminator augmentation,\na conditional StyleGAN2 with adaptive discriminator augmentation to generate\nspine radiographs conditional on lesion type, and using a novel clinical loss\nterm for the generator a StyleGAN2 with adaptive discriminator augmentation\nconditional on abnormality (SpineGAN). Finally, a differential privacy imposed\nStyleGAN2 with adaptive discriminator augmentation conditional on abnormality\nwas trained and an ablation study was performed on its differential privacy\nimpositions.\n</p>\n<p>Key Results: We accomplish GAN generation of synthetic spine radiographs\nwithout meaningful input for the first time from a literature review. We\nfurther demonstrate the success of synthetic learning for the spine domain with\na downstream clinical classification task (AUC of 0.830 using synthetic data\ncompared to AUC of 0.886 using the real data). Importantly, the introduction of\na new clinical loss term for the generator was found to increase generation\nrecall as well as accelerate model training. Lastly, we demonstrate that, in a\nlimited size medical dataset, differential privacy impositions severely impede\nGAN training, finding that this is specifically due to the requirement for\ngradient perturbation with noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schonfeld_E/0/1/0/all/0/1\">Ethan Schonfeld</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeravagu_A/0/1/0/all/0/1\">Anand Veeravagu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariant Content Synergistic Learning for Domain Generalization of Medical Image Segmentation. (arXiv:2205.02845v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02845","description":"<p>While achieving remarkable success for medical image segmentation, deep\nconvolution neural networks (DCNNs) often fail to maintain their robustness\nwhen confronting test data with the novel distribution. To address such a\ndrawback, the inductive bias of DCNNs is recently well-recognized.\nSpecifically, DCNNs exhibit an inductive bias towards image style (e.g.,\nsuperficial texture) rather than invariant content (e.g., object shapes). In\nthis paper, we propose a method, named Invariant Content Synergistic Learning\n(ICSL), to improve the generalization ability of DCNNs on unseen datasets by\ncontrolling the inductive bias. First, ICSL mixes the style of training\ninstances to perturb the training distribution. That is to say, more diverse\ndomains or styles would be made available for training DCNNs. Based on the\nperturbed distribution, we carefully design a dual-branches invariant content\nsynergistic learning strategy to prevent style-biased predictions and focus\nmore on the invariant content. Extensive experimental results on two typical\nmedical image segmentation tasks show that our approach performs better than\nstate-of-the-art domain generalization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kang_Y/0/1/0/all/0/1\">Yuxin Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hansheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xuan Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dongqing Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Feihong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1\">Jun Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation with Super Images: A New 2D Perspective on 3D Medical Image Analysis. (arXiv:2205.02847v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02847","description":"<p>Deep learning is showing an increasing number of audience in medical imaging\nresearch. In the segmentation task of medical images, we oftentimes rely on\nvolumetric data, and thus require the use of 3D architectures which are praised\nfor their ability to capture more features from the depth dimension. Yet, these\narchitectures are generally more ineffective in time and compute compared to\ntheir 2D counterpart on account of 3D convolutions, max pooling,\nup-convolutions, and other operations used in these networks. Moreover, there\nare limited to no 3D pretrained model weights, and pretraining is generally\nchallenging. To alleviate these issues, we propose to cast volumetric data to\n2D super images and use 2D networks for the segmentation task. The method\nprocesses the 3D image by stitching slices side-by-side to generate a super\nresolution image. While the depth information is lost, we expect that deep\nneural networks can still capture and learn these features. Our goal in this\nwork is to introduce a new perspective when dealing with volumetric data, and\ntest our hypothesis using vanilla networks. We hope that this approach, while\nachieving close enough results to 3D networks using only 2D counterparts, can\nattract more related research in the future, especially in medical image\nanalysis since volumetric data is comparably limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sobirov_I/0/1/0/all/0/1\">Ikboljon Sobirov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saeed_N/0/1/0/all/0/1\">Numan Saeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Brains: Subvolume Recombination for Data Augmentation in Large Vessel Occlusion Detection. (arXiv:2205.02848v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02848","description":"<p>Ischemic strokes are often caused by large vessel occlusions (LVOs), which\ncan be visualized and diagnosed with Computed Tomography Angiography scans. As\ntime is brain, a fast, accurate and automated diagnosis of these scans is\ndesirable. Human readers compare the left and right hemispheres in their\nassessment of strokes. A large training data set is required for a standard\ndeep learning-based model to learn this strategy from data. As labeled medical\ndata in this field is rare, other approaches need to be developed. To both\ninclude the prior knowledge of side comparison and increase the amount of\ntraining data, we propose an augmentation method that generates artificial\ntraining samples by recombining vessel tree segmentations of the hemispheres or\nhemisphere subregions from different patients. The subregions cover vessels\ncommonly affected by LVOs, namely the internal carotid artery (ICA) and middle\ncerebral artery (MCA). In line with the augmentation scheme, we use a\n3D-DenseNet fed with task-specific input, fostering a side-by-side comparison\nbetween the hemispheres. Furthermore, we propose an extension of that\narchitecture to process the individual hemisphere subregions. All\nconfigurations predict the presence of an LVO, its side, and the affected\nsubregion. We show the effect of recombination as an augmentation strategy in a\n5-fold cross validated ablation study. We enhanced the AUC for patient-wise\nclassification regarding the presence of an LVO of all investigated\narchitectures. For one variant, the proposed method improved the AUC from 0.73\nwithout augmentation to 0.89. The best configuration detects LVOs with an AUC\nof 0.91, LVOs in the ICA with an AUC of 0.96, and in the MCA with 0.91 while\naccurately predicting the affected side.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thamm_F/0/1/0/all/0/1\">Florian Thamm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taubmann_O/0/1/0/all/0/1\">Oliver Taubmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jurgens_M/0/1/0/all/0/1\">Markus J&#xfc;rgens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thamm_A/0/1/0/all/0/1\">Aleksandra Thamm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1\">Felix Denzinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rist_L/0/1/0/all/0/1\">Leonhard Rist</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ditt_H/0/1/0/all/0/1\">Hendrik Ditt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching. (arXiv:2205.02849v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02849","description":"<p>This paper tackles the challenge of forensic medical image matching (FMIM)\nusing deep neural networks (DNNs). FMIM is a particular case of content-based\nimage retrieval (CBIR). The main challenge in FMIM compared to the general case\nof CBIR, is that the subject to whom a query image belongs may be affected by\naging and progressive degenerative disorders, making it difficult to match data\non a subject level. CBIR with DNNs is generally solved by minimizing a ranking\nloss, such as Triplet loss (TL), computed on image representations extracted by\na DNN from the original data. TL, in particular, operates on triplets: anchor,\npositive (similar to anchor) and negative (dissimilar to anchor). Although TL\nhas been shown to perform well in many CBIR tasks, it still has limitations,\nwhich we identify and analyze in this work. In this paper, we introduce (i) the\nAdaTriplet loss -- an extension of TL whose gradients adapt to different\ndifficulty levels of negative samples, and (ii) the AutoMargin method -- a\ntechnique to adjust hyperparameters of margin-based losses such as TL and our\nproposed loss dynamically. Our results are evaluated on two large-scale\nbenchmarks for FMIM based on the Osteoarthritis Initiative and Chest X-ray-14\ndatasets. The codes allowing replication of this study have been made publicly\navailable at \\url{https://github.com/Oulu-IMEDS/AdaTriplet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy Hoang Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiulpin_A/0/1/0/all/0/1\">Aleksei Tiulpin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Reinforcement Learning Framework for Rapid Diagnosis of Whole Slide Pathological Images. (arXiv:2205.02850v1 [eess.IV])","link":"http://arxiv.org/abs/2205.02850","description":"<p>The deep neural network is a research hotspot for histopathological image\nanalysis, which can improve the efficiency and accuracy of diagnosis for\npathologists or be used for disease screening. The whole slide pathological\nimage can reach one gigapixel and contains abundant tissue feature information,\nwhich needs to be divided into a lot of patches in the training and inference\nstages. This will lead to a long convergence time and large memory consumption.\nFurthermore, well-annotated data sets are also in short supply in the field of\ndigital pathology. Inspired by the pathologist's clinical diagnosis process, we\npropose a weakly supervised deep reinforcement learning framework, which can\ngreatly reduce the time required for network inference. We use neural network\nto construct the search model and decision model of reinforcement learning\nagent respectively. The search model predicts the next action through the image\nfeatures of different magnifications in the current field of view, and the\ndecision model is used to return the predicted probability of the current field\nof view image. In addition, an expert-guided model is constructed by\nmulti-instance learning, which not only provides rewards for search model, but\nalso guides decision model learning by the knowledge distillation method.\nExperimental results show that our proposed method can achieve fast inference\nand accurate prediction of whole slide images without any pixel-level\nannotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_T/0/1/0/all/0/1\">Tingting Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+chen_W/0/1/0/all/0/1\">Weixing chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shuqin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quan_H/0/1/0/all/0/1\">Hao Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_Q/0/1/0/all/0/1\">Qun Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nan_T/0/1/0/all/0/1\">Tianhang Nan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Song Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xinghua Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_X/0/1/0/all/0/1\">Xiaoyu Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Context for Deep Object Detectors. (arXiv:2205.02887v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02887","description":"<p>Which object detector is suitable for your context sensitive task? Deep\nobject detectors exploit scene context for recognition differently. In this\npaper, we group object detectors into 3 categories in terms of context use: no\ncontext by cropping the input (RCNN), partial context by cropping the\nfeaturemap (two-stage methods) and full context without any cropping\n(single-stage methods). We systematically evaluate the effect of context for\neach deep detector category. We create a fully controlled dataset for varying\ncontext and investigate the context for deep detectors. We also evaluate\ngradually removing the background context and the foreground object on MS COCO.\nWe demonstrate that single-stage and two-stage object detectors can and will\nuse the context by virtue of their large receptive field. Thus, choosing the\nbest object detector may depend on the application context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kayhan_O/0/1/0/all/0/1\">Osman Semih Kayhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Jacobian Fields: Learning Intrinsic Mappings of Arbitrary Meshes. (arXiv:2205.02904v1 [cs.GR])","link":"http://arxiv.org/abs/2205.02904","description":"<p>This paper introduces a framework designed to accurately predict piecewise\nlinear mappings of arbitrary meshes via a neural network, enabling training and\nevaluating over heterogeneous collections of meshes that do not share a\ntriangulation, as well as producing highly detail-preserving maps whose\naccuracy exceeds current state of the art. The framework is based on reducing\nthe neural aspect to a prediction of a matrix for a single given point,\nconditioned on a global shape descriptor. The field of matrices is then\nprojected onto the tangent bundle of the given mesh, and used as candidate\njacobians for the predicted map. The map is computed by a standard Poisson\nsolve, implemented as a differentiable layer with cached pre-factorization for\nefficient training. This construction is agnostic to the triangulation of the\ninput, thereby enabling applications on datasets with varying triangulations.\nAt the same time, by operating in the intrinsic gradient domain of each\nindividual mesh, it allows the framework to predict highly-accurate mappings.\nWe validate these properties by conducting experiments over a broad range of\nscenarios, from semantic ones such as morphing, registration, and deformation\ntransfer, to optimization-based ones, such as emulating elastic deformations\nand contact correction, as well as being the first work, to our knowledge, to\ntackle the task of learning to compute UV parameterizations of arbitrary\nmeshes. The results exhibit the high accuracy of the method as well as its\nversatility, as it is readily applied to the above scenarios without any\nchanges to the framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kunal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir G. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1\">Thibault Groueix</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Representative Samples for Few-Shot Classification. (arXiv:2205.02918v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02918","description":"<p>Few-shot learning (FSL) aims to learn new categories with a few visual\nsamples per class. Few-shot class representations are often biased due to data\nscarcity. To mitigate this issue, we propose to generate visual samples based\non semantic embeddings using a conditional variational autoencoder (CVAE)\nmodel. We train this CVAE model on base classes and use it to generate features\nfor novel classes. More importantly, we guide this VAE to strictly generate\nrepresentative samples by removing non-representative samples from the base\ntraining set when training the CVAE model. We show that this training scheme\nenhances the representativeness of the generated samples and therefore,\nimproves the few-shot classification results. Experimental results show that\nour method improves three FSL baseline methods by substantial margins,\nachieving state-of-the-art few-shot classification performance on miniImageNet\nand tieredImageNet datasets for both 1-shot and 5-shot settings. Code is\navailable at: https://github.com/cvlab-stonybrook/fsl-rsvae.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hieu Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FisheyeDistill: Self-Supervised Monocular Depth Estimation with Ordinal Distillation for Fisheye Cameras. (arXiv:2205.02930v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02930","description":"<p>In this paper, we deal with the problem of monocular depth estimation for\nfisheye cameras in a self-supervised manner. A known issue of self-supervised\ndepth estimation is that it suffers in low-light/over-exposure conditions and\nin large homogeneous regions. To tackle this issue, we propose a novel ordinal\ndistillation loss that distills the ordinal information from a large teacher\nmodel. Such a teacher model, since having been trained on a large amount of\ndiverse data, can capture the depth ordering information well, but lacks in\npreserving accurate scene geometry. Combined with self-supervised losses, we\nshow that our model can not only generate reasonable depth maps in challenging\nenvironments but also better recover the scene geometry. We further leverage\nthe fisheye cameras of an AR-Glasses device to collect an indoor dataset to\nfacilitate evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_N/0/1/0/all/0/1\">Nitin Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Urban Water Consumption using Remotely Sensed Data. (arXiv:2205.02932v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02932","description":"<p>Urban metabolism is an active field of research that deals with the\nestimation of emissions and resource consumption from urban regions. The\nanalysis could be carried out through a manual surveyor by the implementation\nof elegant machine learning algorithms. In this exploratory work, we estimate\nthe water consumption by the buildings in the region captured by satellite\nimagery. To this end, we break our analysis into three parts: i) Identification\nof building pixels, given a satellite image, followed by ii) identification of\nthe building type (residential/non-residential) from the building pixels, and\nfinally iii) using the building pixels along with their type to estimate the\nwater consumption using the average per unit area consumption for different\nbuilding types as obtained from municipal surveys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Shaswat Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijay_A/0/1/0/all/0/1\">Anirudh Vijay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_S/0/1/0/all/0/1\">Shailesh Deshpande</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biometric Signature Verification Using Recurrent Neural Networks. (arXiv:2205.02934v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02934","description":"<p>Architectures based on Recurrent Neural Networks (RNNs) have been\nsuccessfully applied to many different tasks such as speech or handwriting\nrecognition with state-of-the-art results. The main contribution of this work\nis to analyse the feasibility of RNNs for on-line signature verification in\nreal practical scenarios. We have considered a system based on Long Short-Term\nMemory (LSTM) with a Siamese architecture whose goal is to learn a similarity\nmetric from pairs of signatures. For the experimental work, the BiosecurID\ndatabase comprised of 400 users and 4 separated acquisition sessions are\nconsidered. Our proposed LSTM RNN system has outperformed the results of recent\npublished works on the BiosecurID benchmark in figures ranging from 17.76% to\n28.00% relative verification performance improvement for skilled forgeries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Propaganda Techniques in Visuo-Lingual Metaphor in Memes. (arXiv:2205.02937v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02937","description":"<p>The exponential rise of social media networks has allowed the production,\ndistribution, and consumption of data at a phenomenal rate. Moreover, the\nsocial media revolution has brought a unique phenomenon to social media\nplatforms called Internet memes. Internet memes are one of the most popular\ncontents used on social media, and they can be in the form of images with a\nwitty, catchy, or satirical text description. In this paper, we are dealing\nwith propaganda that is often seen in Internet memes in recent times.\nPropaganda is communication, which frequently includes psychological and\nrhetorical techniques to manipulate or influence an audience to act or respond\nas the propagandist wants. To detect propaganda in Internet memes, we propose a\nmultimodal deep learning fusion system that fuses the text and image feature\nrepresentations and outperforms individual models based solely on either text\nor image modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gundapu_S/0/1/0/all/0/1\">Sunil Gundapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Immiscible Color Flows in Optimal Transport Networks for Image Classification. (arXiv:2205.02938v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02938","description":"<p>In classification tasks, it is crucial to meaningfully exploit information\ncontained in data. Here, we propose a physics-inspired dynamical system that\nadapts Optimal Transport principles to effectively leverage color distributions\nof images. Our dynamics regulates immiscible fluxes of colors traveling on a\nnetwork built from images. Instead of aggregating colors together, it treats\nthem as different commodities that interact with a shared capacity on edges.\nOur method outperforms competitor algorithms on image classification tasks in\ndatasets where color information matters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lonardi_A/0/1/0/all/0/1\">Alessandro Lonardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baptista_D/0/1/0/all/0/1\">Diego Baptista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacco_C/0/1/0/all/0/1\">Caterina De Bacco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-Augmented Visual-Inertial SLAM with Planar Constraints. (arXiv:2205.02940v1 [cs.RO])","link":"http://arxiv.org/abs/2205.02940","description":"<p>We present a robust visual-inertial SLAM system that combines the benefits of\nConvolutional Neural Networks (CNNs) and planar constraints. Our system\nleverages a CNN to predict the depth map and the corresponding uncertainty map\nfor each image. The CNN depth effectively bootstraps the back-end optimization\nof SLAM and meanwhile the CNN uncertainty adaptively weighs the contribution of\neach feature point to the back-end optimization. Given the gravity direction\nfrom the inertial sensor, we further present a fast plane detection method that\ndetects horizontal planes via one-point RANSAC and vertical planes via\ntwo-point RANSAC. Those stably detected planes are in turn used to regularize\nthe back-end optimization of SLAM. We evaluate our system on a public dataset,\n\\ie, EuRoC, and demonstrate improved results over a state-of-the-art SLAM\nsystem, \\ie, ORB-SLAM3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn-to-Race Challenge 2022: Benchmarking Safe Learning and Cross-domain Generalisation in Autonomous Racing. (arXiv:2205.02953v1 [cs.RO])","link":"http://arxiv.org/abs/2205.02953","description":"<p>We present the results of our autonomous racing virtual challenge, based on\nthe newly-released Learn-to-Race (L2R) simulation framework, which seeks to\nencourage interdisciplinary research in autonomous driving and to help advance\nthe state of the art on a realistic benchmark. Analogous to racing being used\nto test cutting-edge vehicles, we envision autonomous racing to serve as a\nparticularly challenging proving ground for autonomous agents as: (i) they need\nto make sub-second, safety-critical decisions in a complex, fast-changing\nenvironment; and (ii) both perception and control must be robust to\ndistribution shifts, novel road features, and unseen obstacles. Thus, the main\ngoal of the challenge is to evaluate the joint safety, performance, and\ngeneralisation capabilities of reinforcement learning agents on multi-modal\nperception, through a two-stage process. In the first stage of the challenge,\nwe evaluate an autonomous agent's ability to drive as fast as possible, while\nadhering to safety constraints. In the second stage, we additionally require\nthe agent to adapt to an unseen racetrack through safe exploration. In this\npaper, we describe the new L2R Task 2.0 benchmark, with refined metrics and\nbaseline approaches. We also provide an overview of deployment, evaluation, and\nrankings for the inaugural instance of the L2R Autonomous Racing Virtual\nChallenge (supported by Carnegie Mellon University, Arrival Ltd., AICrowd,\nAmazon Web Services, and Honda Research), which officially used the new L2R\nTask 2.0 benchmark and received over 20,100 views, 437 active participants, 46\nteams, and 733 model submissions -- from 88 unique institutions, in 28\ndifferent countries. Finally, we release leaderboard results from the challenge\nand provide description of the two top-ranking approaches in cross-domain model\ntransfer, across multiple sensor configurations and simulated races.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kathpal_S/0/1/0/all/0/1\">Sidharth Kathpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poonganam_J/0/1/0/all/0/1\">Jyotish Poonganam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivani_A/0/1/0/all/0/1\">Ayush Shivani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genc_S/0/1/0/all/0/1\">Sahika Genc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_I/0/1/0/all/0/1\">Ivan Zhukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumskoy_M/0/1/0/all/0/1\">Max Kumskoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Graph Expansion for Semantics-Guided Image Outpainting. (arXiv:2205.02958v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02958","description":"<p>In this paper, we address the task of semantics-guided image outpainting,\nwhich is to complete an image by generating semantically practical content.\nDifferent from most existing image outpainting works, we approach the above\ntask by understanding and completing image semantics at the scene graph level.\nIn particular, we propose a novel network of Scene Graph Transformer (SGT),\nwhich is designed to take node and edge features as inputs for modeling the\nassociated structural information. To better understand and process graph-based\ninputs, our SGT uniquely performs feature attention at both node and edge\nlevels. While the former views edges as relationship regularization, the latter\nobserves the co-occurrence of nodes for guiding the attention process. We\ndemonstrate that, given a partial input image with its layout and scene graph,\nour SGT can be applied for scene graph expansion and its conversion to a\ncomplete layout. Following state-of-the-art layout-to-image conversions works,\nthe task of image outpainting can be completed with sufficient and practical\nsemantics introduced. Extensive experiments are conducted on the datasets of\nMS-COCO and Visual Genome, which quantitatively and qualitatively confirm the\neffectiveness of our proposed SGT and outpainting frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chiao-An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheng-Yo Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wan-Cyuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Fu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Meng-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Chiang Frank Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximate Convex Decomposition for 3D Meshes with Collision-Aware Concavity and Tree Search. (arXiv:2205.02961v1 [cs.GR])","link":"http://arxiv.org/abs/2205.02961","description":"<p>Approximate convex decomposition aims to decompose a 3D shape into a set of\nalmost convex components, whose convex hulls can then be used to represent the\ninput shape. It thus enables efficient geometry processing algorithms\nspecifically designed for convex shapes and has been widely used in game\nengines, physics simulations, and animation. While prior works can capture the\nglobal structure of input shapes, they may fail to preserve fine-grained\ndetails (e.g., filling a toaster's slots), which are critical for retaining the\nfunctionality of objects in interactive environments. In this paper, we propose\na novel method that addresses the limitations of existing approaches from three\nperspectives: (a) We introduce a novel collision-aware concavity metric that\nexamines the distance between a shape and its convex hull from both the\nboundary and the interior. The proposed concavity preserves collision\nconditions and is more robust to detect various approximation errors. (b) We\ndecompose shapes by directly cutting meshes with 3D planes. It ensures\ngenerated convex hulls are intersection-free and avoids voxelization errors.\n(c) Instead of using a one-step greedy strategy, we propose employing a\nmulti-step tree search to determine the cutting planes, which leads to a\nglobally better solution and avoids unnecessary cuttings. Through extensive\nevaluation on a large-scale articulated object dataset, we show that our method\ngenerates decompositions closer to the original shape with fewer components. It\nthus supports delicate and efficient object interaction in downstream\napplications. We will release our implementation to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinyue Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Scale Transfer Learning for Differentially Private Image Classification. (arXiv:2205.02973v1 [cs.LG])","link":"http://arxiv.org/abs/2205.02973","description":"<p>Differential Privacy (DP) provides a formal framework for training machine\nlearning models with individual example level privacy. Training models with DP\nprotects the model against leakage of sensitive data in a potentially\nadversarial setting. In the field of deep learning, Differentially Private\nStochastic Gradient Descent (DP-SGD) has emerged as a popular private training\nalgorithm. Private training using DP-SGD protects against leakage by injecting\nnoise into individual example gradients, such that the trained model weights\nbecome nearly independent of the use any particular training example. While\nthis result is quite appealing, the computational cost of training large-scale\nmodels with DP-SGD is substantially higher than non-private training. This is\nfurther exacerbated by the fact that increasing the number of parameters leads\nto larger degradation in utility with DP. In this work, we zoom in on the\nImageNet dataset and demonstrate that similar to the non-private case,\npre-training over-parameterized models on a large public dataset can lead to\nsubstantial gains when the model is finetuned privately. Moreover, by\nsystematically comparing private and non-private models across a range of huge\nbatch sizes, we find that similar to non-private setting, choice of optimizer\ncan further improve performance substantially with DP. By switching from DP-SGD\nto DP-LAMB we saw improvement of up to 20$\\%$ points (absolute). Finally, we\nshow that finetuning just the last layer for a \\emph{single step} in the full\nbatch setting leads to both SOTA results of 81.7 $\\%$ under a wide privacy\nbudget range of $\\epsilon \\in [4, 10]$ and $\\delta$ = $10^{-6}$ while\nminimizing the computational overhead substantially.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1\">Abhradeep Thakurta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1\">Alexey Kurakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate and Edit Your Own Character in a Canonical View. (arXiv:2205.02974v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02974","description":"<p>Recently, synthesizing personalized characters from a single user-given\nportrait has received remarkable attention as a drastic popularization of\nsocial media and the metaverse. The input image is not always in frontal view,\nthus it is important to acquire or predict canonical view for 3D modeling or\nother applications. Although the progress of generative models enables the\nstylization of a portrait, obtaining the stylized image in canonical view is\nstill a challenging task. There have been several studies on face\nfrontalization but their performance significantly decreases when input is not\nin the real image domain, e.g., cartoon or painting. Stylizing after\nfrontalization also results in degenerated output. In this paper, we propose a\nnovel and unified framework which generates stylized portraits in canonical\nview. With a proposed latent mapper, we analyze and discover frontalization\nmapping in a latent space of StyleGAN to stylize and frontalize at once. In\naddition, our model can be trained with unlabelled 2D image sets, without any\n3D supervision. The effectiveness of our method is demonstrated by experimental\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1\">Jeong-gi Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1\">Dongsik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">David Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Point Cloud Registration based on Evolutionary Multitasking with Bi-Channel Knowledge Sharing Mechanism. (arXiv:2205.02996v1 [cs.CV])","link":"http://arxiv.org/abs/2205.02996","description":"<p>Registration of multi-view point clouds is fundamental in 3D reconstruction.\nSince there are close connections between point clouds captured from different\nviewpoints, registration performance can be enhanced if these connections be\nharnessed properly. Therefore, this paper models the registration problem as\nmulti-task optimization, and proposes a novel bi-channel knowledge sharing\nmechanism for effective and efficient problem solving. The modeling of\nmulti-view point cloud registration as multi-task optimization are twofold. By\nsimultaneously considering the local accuracy of two point clouds as well as\nthe global consistency posed by all the point clouds involved, a fitness\nfunction with an adaptive threshold is derived. Also a framework of the\nco-evolutionary search process is defined for the concurrent optimization of\nmultiple fitness functions belonging to related tasks. To enhance solution\nquality and convergence speed, the proposed bi-channel knowledge sharing\nmechanism plays its role. The intra-task knowledge sharing introduces aiding\ntasks that are much simpler to solve, and useful information is shared within\ntasks, accelerating the search process. The inter-task knowledge sharing\nexplores commonalities buried among tasks, aiming to prevent tasks from getting\nstuck to local optima. Comprehensive experiments conducted on model object as\nwell as scene point clouds show the efficacy of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Maoguo Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zedong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1\">Qiguang Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenping Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Pretraining for Semi-Supervised Learning in the Low-Label Regime. (arXiv:2205.03001v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03001","description":"<p>Semi-supervised learning (SSL) addresses the lack of labeled data by\nexploiting large unlabeled data through pseudolabeling. However, in the\nextremely low-label regime, pseudo labels could be incorrect, a.k.a. the\nconfirmation bias, and the pseudo labels will in turn harm the network\ntraining. Recent studies combined finetuning (FT) from pretrained weights with\nSSL to mitigate the challenges and claimed superior results in the low-label\nregime. In this work, we first show that the better pretrained weights brought\nin by FT account for the state-of-the-art performance, and importantly that\nthey are universally helpful to off-the-shelf semi-supervised learners. We\nfurther argue that direct finetuning from pretrained weights is suboptimal due\nto covariate shift and propose a contrastive target pretraining step to adapt\nmodel weights towards target dataset. We carried out extensive experiments on\nboth classification and segmentation tasks by doing target pretraining then\nfollowed by semi-supervised finetuning. The promising results validate the\nefficacy of target pretraining for SSL, in particular in the low-label regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jingyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Lile Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Manh Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kangkang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazici_Y/0/1/0/all/0/1\">Yasin Yazici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan Sheng Foo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fingerprint Detection Method by Fingerprint Ridge Orientation Check. (arXiv:2205.03019v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03019","description":"<p>Fingerprints are popular among the biometric based systems due to ease of\nacquisition, uniqueness and availability. Nowadays it is used in smart phone\nsecurity, digital payment and digital locker. Fingerprint recognition\ntechnology has been studied for a long time, and its recognition rate has\nrecently risen to a high level. In particular, with the introduction of Deep\nNeural Network technologies, the recognition rate that could not be reached\nbefore was reached. In this paper, we propose a fingerprint detection algorithm\nused in a fingerprint recognition system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+JuSong_K/0/1/0/all/0/1\">Kim JuSong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IlYong_R/0/1/0/all/0/1\">Ri IlYong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantification of Robotic Surgeries with Vision-Based Deep Learning. (arXiv:2205.03028v1 [cs.RO])","link":"http://arxiv.org/abs/2205.03028","description":"<p>Surgery is a high-stakes domain where surgeons must navigate critical\nanatomical structures and actively avoid potential complications while\nachieving the main task at hand. Such surgical activity has been shown to\naffect long-term patient outcomes. To better understand this relationship,\nwhose mechanics remain unknown for the majority of surgical procedures, we\nhypothesize that the core elements of surgery must first be quantified in a\nreliable, objective, and scalable manner. We believe this is a prerequisite for\nthe provision of surgical feedback and modulation of surgeon performance in\npursuit of improved patient outcomes. To holistically quantify surgeries, we\npropose a unified deep learning framework, entitled Roboformer, which operates\nexclusively on videos recorded during surgery to independently achieve multiple\ntasks: surgical phase recognition (the what of surgery), gesture classification\nand skills assessment (the how of surgery). We validated our framework on four\nvideo-based datasets of two commonly-encountered types of steps (dissection and\nsuturing) within minimally-invasive robotic surgeries. We demonstrated that our\nframework can generalize well to unseen videos, surgeons, medical centres, and\nsurgical procedures. We also found that our framework, which naturally lends\nitself to explainable findings, identified relevant information when achieving\na particular task. These findings are likely to instill surgeons with more\nconfidence in our framework's behaviour, increasing the likelihood of clinical\nadoption, and thus paving the way for more targeted surgical feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiyasseh_D/0/1/0/all/0/1\">Dani Kiyasseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Runzhuo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_T/0/1/0/all/0/1\">Taseen F. Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1\">Jessica Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_C/0/1/0/all/0/1\">Christian Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Animashree Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_A/0/1/0/all/0/1\">Andrew J. Hung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Level Decoupled Transformer for Video Captioning. (arXiv:2205.03039v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03039","description":"<p>Video captioning aims to understand the spatio-temporal semantic concept of\nthe video and generate descriptive sentences. The de-facto approach to this\ntask dictates a text generator to learn from \\textit{offline-extracted} motion\nor appearance features from \\textit{pre-trained} vision models. However, these\nmethods may suffer from the so-called \\textbf{\\textit{\"couple\"}} drawbacks on\nboth \\textit{video spatio-temporal representation} and \\textit{sentence\ngeneration}. For the former, \\textbf{\\textit{\"couple\"}} means learning\nspatio-temporal representation in a single model(3DCNN), resulting the problems\nnamed \\emph{disconnection in task/pre-train domain} and \\emph{hard for\nend-to-end training}. As for the latter, \\textbf{\\textit{\"couple\"}} means\ntreating the generation of visual semantic and syntax-related words equally. To\nthis end, we present $\\mathcal{D}^{2}$ - a dual-level decoupled transformer\npipeline to solve the above drawbacks: \\emph{(i)} for video spatio-temporal\nrepresentation, we decouple the process of it into\n\"first-spatial-then-temporal\" paradigm, releasing the potential of using\ndedicated model(\\textit{e.g.} image-text pre-training) to connect the\npre-training and downstream tasks, and makes the entire model end-to-end\ntrainable. \\emph{(ii)} for sentence generation, we propose \\emph{Syntax-Aware\nDecoder} to dynamically measure the contribution of visual semantic and\nsyntax-related words. Extensive experiments on three widely-used benchmarks\n(MSVD, MSR-VTT and VATEX) have shown great potential of the proposed\n$\\mathcal{D}^{2}$ and surpassed the previous methods by a large margin in the\ntask of video captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xinglin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suo_W/0/1/0/all/0/1\">Wei Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mengyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tiezheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Object Detection via Prototypical Task Correlation Guided Gating Mechanism. (arXiv:2205.03055v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03055","description":"<p>Continual learning is a challenging real-world problem for constructing a\nmature AI system when data are provided in a streaming fashion. Despite recent\nprogress in continual classification, the researches of continual object\ndetection are impeded by the diverse sizes and numbers of objects in each\nimage. Different from previous works that tune the whole network for all tasks,\nin this work, we present a simple and flexible framework for continual object\ndetection via pRotOtypical taSk corrElaTion guided gaTing mechAnism (ROSETTA).\nConcretely, a unified framework is shared by all tasks while task-aware gates\nare introduced to automatically select sub-models for specific tasks. In this\nway, various knowledge can be successively memorized by storing their\ncorresponding sub-model weights in this system. To make ROSETTA automatically\ndetermine which experience is available and useful, a prototypical task\ncorrelation guided Gating Diversity Controller(GDC) is introduced to adaptively\nadjust the diversity of gates for the new task based on class-specific\nprototypes. GDC module computes class-to-class correlation matrix to depict the\ncross-task correlation, and hereby activates more exclusive gates for the new\ntask if a significant domain gap is observed. Comprehensive experiments on\nCOCO-VOC, KITTI-Kitchen, class-incremental detection on VOC and sequential\nlearning of four tasks show that ROSETTA yields state-of-the-art performance on\nboth task-based and class-based continual object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Binbin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xinchi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Han Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gengwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Data-Uploading for Full-Quantum Classification. (arXiv:2205.03057v1 [quant-ph])","link":"http://arxiv.org/abs/2205.03057","description":"<p>The data representation in a machine-learning model strongly influences its\nperformance. This becomes even more important for quantum machine learning\nmodels implemented on noisy intermediate scale quantum (NISQ) devices. Encoding\nhigh dimensional data into a quantum circuit for a NISQ device without any loss\nof information is not trivial and brings a lot of challenges. While simple\nencoding schemes (like single qubit rotational gates to encode high dimensional\ndata) often lead to information loss within the circuit, complex encoding\nschemes with entanglement and data re-uploading lead to an increase in the\nencoding gate count. This is not well-suited for NISQ devices. This work\nproposes 'incremental data-uploading', a novel encoding pattern for high\ndimensional data that tackles these challenges. We spread the encoding gates\nfor the feature vector of a given data point throughout the quantum circuit\nwith parameterized gates in between them. This encoding pattern results in a\nbetter representation of data in the quantum circuit with a minimal\npre-processing requirement. We show the efficiency of our encoding pattern on a\nclassification task using the MNIST and Fashion-MNIST datasets, and compare\ndifferent encoding methods via classification accuracy and the effective\ndimension of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Periyasamy_M/0/1/0/all/0/1\">Maniraman Periyasamy</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Meyer_N/0/1/0/all/0/1\">Nico Meyer</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ufrecht_C/0/1/0/all/0/1\">Christian Ufrecht</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Scherer_D/0/1/0/all/0/1\">Daniel D. Scherer</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Plinge_A/0/1/0/all/0/1\">Axel Plinge</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Mutschler_C/0/1/0/all/0/1\">Christopher Mutschler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QLEVR: A Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning. (arXiv:2205.03075v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03075","description":"<p>Synthetic datasets have successfully been used to probe visual\nquestion-answering datasets for their reasoning abilities. CLEVR\n(johnson2017clevr), for example, tests a range of visual reasoning abilities.\nThe questions in CLEVR focus on comparisons of shapes, colors, and sizes,\nnumerical reasoning, and existence claims. This paper introduces a minimally\nbiased, diagnostic visual question-answering dataset, QLEVR, that goes beyond\nexistential and numerical quantification and focus on more complex quantifiers\nand their combinations, e.g., asking whether there are more than two red balls\nthat are smaller than at least three blue balls in an image. We describe how\nthe dataset was created and present a first evaluation of state-of-the-art\nvisual question-answering models, showing that QLEVR presents a formidable\nchallenge to our current models. Code and Dataset are available at\nhttps://github.com/zechenli03/QLEVR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crop Type Identification for Smallholding Farms: Analyzing Spatial, Temporal and Spectral Resolutions in Satellite Imagery. (arXiv:2205.03104v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03104","description":"<p>The integration of the modern Machine Learning (ML) models into remote\nsensing and agriculture has expanded the scope of the application of satellite\nimages in the agriculture domain. In this paper, we present how the accuracy of\ncrop type identification improves as we move from\nmedium-spatiotemporal-resolution (MSTR) to high-spatiotemporal-resolution\n(HSTR) satellite images. We further demonstrate that high spectral resolution\nin satellite imagery can improve prediction performance for low spatial and\ntemporal resolutions (LSTR) images. The F1-score is increased by 7% when using\nmultispectral data of MSTR images as compared to the best results obtained from\nHSTR images. Similarly, when crop season based time series of multispectral\ndata is used we observe an increase of 1.2% in the F1-score. The outcome\nmotivates further advancements in the field of synthetic band generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sani_D/0/1/0/all/0/1\">Depanshu Sani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahato_S/0/1/0/all/0/1\">Sandeep Mahato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirohi_P/0/1/0/all/0/1\">Parichya Sirohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1\">Saket Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1\">Gaurav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devshali_C/0/1/0/all/0/1\">Charu Chandra Devshali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_T/0/1/0/all/0/1\">T. Jayaraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Dropout for Uncertainty Estimation. (arXiv:2205.03109v1 [cs.LG])","link":"http://arxiv.org/abs/2205.03109","description":"<p>Uncertainty quantification in a neural network is one of the most discussed\ntopics for safety-critical applications. Though Neural Networks (NNs) have\nachieved state-of-the-art performance for many applications, they still provide\nunreliable point predictions, which lack information about uncertainty\nestimates. Among various methods to enable neural networks to estimate\nuncertainty, Monte Carlo (MC) dropout has gained much popularity in a short\nperiod due to its simplicity. In this study, we present a new version of the\ntraditional dropout layer where we are able to fix the number of dropout\nconfigurations. As such, each layer can take and apply the new dropout layer in\nthe MC method to quantify the uncertainty associated with NN predictions. We\nconduct experiments on both toy and realistic datasets and compare the results\nwith the MC method using the traditional dropout layer. Performance analysis\nutilizing uncertainty evaluation metrics corroborates that our dropout layer\noffers better performance in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mehedi Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_I/0/1/0/all/0/1\">Ibrahim Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">Ashikur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A High-Accuracy Unsupervised Person Re-identification Method Using Auxiliary Information Mined from Datasets. (arXiv:2205.03124v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03124","description":"<p>Supervised person re-identification methods rely heavily on high-quality\ncross-camera training label. This significantly hinders the deployment of re-ID\nmodels in real-world applications. The unsupervised person re-ID methods can\nreduce the cost of data annotation, but their performance is still far lower\nthan the supervised ones. In this paper, we make full use of the auxiliary\ninformation mined from the datasets for multi-modal feature learning, including\ncamera information, temporal information and spatial information. By analyzing\nthe style bias of cameras, the characteristics of pedestrians' motion\ntrajectories and the positions of camera network, this paper designs three\nmodules: Time-Overlapping Constraint (TOC), Spatio-Temporal Similarity (STS)\nand Same-Camera Penalty (SCP) to exploit the auxiliary information. Auxiliary\ninformation can improve the model performance and inference accuracy by\nconstructing association constraints or fusing with visual features. In\naddition, this paper proposes three effective training tricks, including\nRestricted Label Smoothing Cross Entropy Loss (RLSCE), Weight Adaptive Triplet\nLoss (WATL) and Dynamic Training Iterations (DTI). The tricks achieve mAP of\n72.4% and 81.1% on MARS and DukeMTMC-VideoReID, respectively. Combined with\nauxiliary information exploiting modules, our methods achieve mAP of 89.9% on\nDukeMTMC, where TOC, STS and SCP all contributed considerable performance\nimprovements. The method proposed by this paper outperforms most existing\nunsupervised re-ID methods and narrows the gap between unsupervised and\nsupervised re-ID methods. Our code is at\nhttps://github.com/tenghehan/AuxUSLReID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teng_H/0/1/0/all/0/1\">Hehan Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BDIS: Bayesian Dense Inverse Searching Method for Real-Time Stereo Surgical Image Matching. (arXiv:2205.03133v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03133","description":"<p>In stereoscope-based Minimally Invasive Surgeries (MIS), dense stereo\nmatching plays an indispensable role in 3D shape recovery, AR, VR, and\nnavigation tasks. Although numerous Deep Neural Network (DNN) approaches are\nproposed, the conventional prior-free approaches are still popular in the\nindustry because of the lack of open-source annotated data set and the\nlimitation of the task-specific pre-trained DNNs. Among the prior-free stereo\nmatching algorithms, there is no successful real-time algorithm in none GPU\nenvironment for MIS. This paper proposes the first CPU-level real-time\nprior-free stereo matching algorithm for general MIS tasks. We achieve an\naverage 17 Hz on 640*480 images with a single-core CPU (i5-9400) for surgical\nimages. Meanwhile, it achieves slightly better accuracy than the popular ELAS.\nThe patch-based fast disparity searching algorithm is adopted for the rectified\nstereo images. A coarse-to-fine Bayesian probability and a spatial Gaussian\nmixed model were proposed to evaluate the patch probability at different\nscales. An optional probability density function estimation algorithm was\nadopted to quantify the prediction variance. Extensive experiments demonstrated\nthe proposed method's capability to handle ambiguities introduced by the\ntextureless surfaces and the photometric inconsistency from the non-Lambertian\nreflectance and dark illumination. The estimated probability managed to balance\nthe confidences of the patches for stereo images at different scales. It has\nsimilar or higher accuracy and fewer outliers than the baseline ELAS in MIS,\nwhile it is 4-5 times faster. The code and the synthetic data sets are\navailable at https://github.com/JingweiSong/BDIS-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiuchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised 3D Point Cloud Segmentation via Multi-Prototype Learning. (arXiv:2205.03137v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03137","description":"<p>Addressing the annotation challenge in 3D Point Cloud segmentation has\ninspired research into weakly supervised learning. Existing approaches mainly\nfocus on exploiting manifold and pseudo-labeling to make use of large unlabeled\ndata points. A fundamental challenge here lies in the large intra-class\nvariations of local geometric structure, resulting in subclasses within a\nsemantic class. In this work, we leverage this intuition and opt for\nmaintaining an individual classifier for each subclass. Technically, we design\na multi-prototype classifier, each prototype serves as the classifier weights\nfor one subclass. To enable effective updating of multi-prototype classifier\nweights, we propose two constraints respectively for updating the prototypes\nw.r.t. all point features and for encouraging the learning of diverse\nprototypes. Experiments on weakly supervised 3D point cloud segmentation tasks\nvalidate the efficacy of proposed method in particular at low-label regime. Our\nhypothesis is also verified given the consistent discovery of semantic\nsubclasses at no cost of additional annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yongyi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-CLOP: CLIP-Guided Collage and Photomontage. (arXiv:2205.03146v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03146","description":"<p>The unabated mystique of large-scale neural networks, such as the CLIP dual\nimage-and-text encoder, popularized automatically generated art. Increasingly\nmore sophisticated generators enhanced the artworks' realism and visual\nappearance, and creative prompt engineering enabled stylistic expression.\nGuided by an artist-in-the-loop ideal, we design a gradient-based generator to\nproduce collages. It requires the human artist to curate libraries of image\npatches and to describe (with prompts) the whole image composition, with the\noption to manually adjust the patches' positions during generation, thereby\nallowing humans to reclaim some control of the process and achieve greater\ncreative freedom. We explore the aesthetic potentials of high-resolution\ncollages, and provide an open-source Google Colab as an artistic tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirowski_P/0/1/0/all/0/1\">Piotr Mirowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banarse_D/0/1/0/all/0/1\">Dylan Banarse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1\">Simon Osindero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_C/0/1/0/all/0/1\">Chrisantha Fernando</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Easy to Hard: Learning Language-guided Curriculum for Visual Question Answering on Remote Sensing Data. (arXiv:2205.03147v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03147","description":"<p>Visual question answering (VQA) for remote sensing scene has great potential\nin intelligent human-computer interaction system. Although VQA in computer\nvision has been widely researched, VQA for remote sensing data (RSVQA) is still\nin its infancy. There are two characteristics that need to be specially\nconsidered for the RSVQA task. 1) No object annotations are available in RSVQA\ndatasets, which makes it difficult for models to exploit informative region\nrepresentation; 2) There are questions with clearly different difficulty levels\nfor each image in the RSVQA task. Directly training a model with questions in a\nrandom order may confuse the model and limit the performance. To address these\ntwo problems, in this paper, a multi-level visual feature learning method is\nproposed to jointly extract language-guided holistic and regional image\nfeatures. Besides, a self-paced curriculum learning (SPCL)-based VQA model is\ndeveloped to train networks with samples in an easy-to-hard way. To be more\nspecific, a language-guided SPCL method with a soft weighting strategy is\nexplored in this work. The proposed model is evaluated on three public\ndatasets, and extensive experimental results show that the proposed RSVQA\nframework can achieve promising performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhenghang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating and Explaining the Frequency Bias in Image Classification. (arXiv:2205.03154v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03154","description":"<p>CNNs exhibit many behaviors different from humans, one of which is the\ncapability of employing high-frequency components. This paper discusses the\nfrequency bias phenomenon in image classification tasks: the high-frequency\ncomponents are actually much less exploited than the low- and mid-frequency\ncomponents. We first investigate the frequency bias phenomenon by presenting\ntwo observations on feature discrimination and learning priority. Furthermore,\nwe hypothesize that (i) the spectral density, (ii) class consistency directly\naffect the frequency bias. Specifically, our investigations verify that the\nspectral density of datasets mainly affects the learning priority, while the\nclass consistency mainly affects the feature discrimination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">ZhiYu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">YiFei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">JiTao Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics-Guided Moving Object Segmentation with 3D LiDAR. (arXiv:2205.03186v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03186","description":"<p>Moving object segmentation (MOS) is a task to distinguish moving objects,\ne.g., moving vehicles and pedestrians, from the surrounding static environment.\nThe segmentation accuracy of MOS can have an influence on odometry, map\nconstruction, and planning tasks. In this paper, we propose a semantics-guided\nconvolutional neural network for moving object segmentation. The network takes\nsequential LiDAR range images as inputs. Instead of segmenting the moving\nobjects directly, the network conducts single-scan-based semantic segmentation\nand multiple-scan-based moving object segmentation in turn. The semantic\nsegmentation module provides semantic priors for the MOS module, where we\npropose an adjacent scan association (ASA) module to convert the semantic\nfeatures of adjacent scans into the same coordinate system to fully exploit the\ncross-scan semantic features. Finally, by analyzing the difference between the\ntransformed features, reliable MOS result can be obtained quickly. Experimental\nresults on the SemanticKITTI MOS dataset proves the effectiveness of our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuo Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Suling Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1\">Hui Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Atlas-powered deep learning (ADL) -- application to diffusion weighted MRI. (arXiv:2205.03210v1 [physics.med-ph])","link":"http://arxiv.org/abs/2205.03210","description":"<p>Deep learning has a great potential for estimating biomarkers in diffusion\nweighted magnetic resonance imaging (dMRI). Atlases, on the other hand, are a\nunique tool for modeling the spatio-temporal variability of biomarkers. In this\npaper, we propose the first framework to exploit both deep learning and atlases\nfor biomarker estimation in dMRI. Our framework relies on non-linear diffusion\ntensor registration to compute biomarker atlases and to estimate atlas\nreliability maps. We also use nonlinear tensor registration to align the atlas\nto a subject and to estimate the error of this alignment. We use the biomarker\natlas, atlas reliability map, and alignment error map, in addition to the dMRI\nsignal, as inputs to a deep learning model for biomarker estimation. We use our\nframework to estimate fractional anisotropy and neurite orientation dispersion\nfrom down-sampled dMRI data on a test cohort of 70 newborn subjects. Results\nshow that our method significantly outperforms standard estimation methods as\nwell as recent deep learning techniques. Our method is also more robust to\nstronger measurement down-sampling factors. Our study shows that the advantages\nof deep learning and atlases can be synergistically combined to achieve\nunprecedented accuracy in biomarker estimation from dMRI data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Karimi_D/0/1/0/all/0/1\">Davood Karimi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gholipour_A/0/1/0/all/0/1\">Ali Gholipour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Future Occupancy Grids in Dynamic Environment with Spatio-Temporal Learning. (arXiv:2205.03212v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03212","description":"<p>Reliably predicting future occupancy of highly dynamic urban environments is\nan important precursor for safe autonomous navigation. Common challenges in the\nprediction include forecasting the relative position of other vehicles,\nmodelling the dynamics of vehicles subjected to different traffic conditions,\nand vanishing surrounding objects. To tackle these challenges, we propose a\nspatio-temporal prediction network pipeline that takes the past information\nfrom the environment and semantic labels separately for generating future\noccupancy predictions. Compared to the current SOTA, our approach predicts\noccupancy for a longer horizon of 3 seconds and in a relatively complex\nenvironment from the nuScenes dataset. Our experimental results demonstrate the\nability of spatio-temporal networks to understand scene dynamics without the\nneed for HD-Maps and explicit modeling dynamic objects. We publicly release our\noccupancy grid dataset based on nuScenes to support further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mann_K/0/1/0/all/0/1\">Khushdeep Singh Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomy_A/0/1/0/all/0/1\">Abhishek Tomy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paigwar_A/0/1/0/all/0/1\">Anshul Paigwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renzaglia_A/0/1/0/all/0/1\">Alessandro Renzaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laugier_C/0/1/0/all/0/1\">Christian Laugier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forget Less, Count Better: A Domain-Incremental Self-Distillation Learning Benchmark for Lifelong Crowd Counting. (arXiv:2205.03307v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03307","description":"<p>Crowd Counting has important applications in public safety and pandemic\ncontrol. A robust and practical crowd counting system has to be capable of\ncontinuously learning with the new-coming domain data in real-world scenarios\ninstead of fitting one domain only. Off-the-shelf methods have some drawbacks\nto handle multiple domains. 1) The models will achieve limited performance\n(even drop dramatically) among old domains after training images from new\ndomains due to the discrepancies of intrinsic data distributions from various\ndomains, which is called catastrophic forgetting. 2) The well-trained model in\na specific domain achieves imperfect performance among other unseen domains\nbecause of the domain shift. 3) It leads to linearly-increased storage overhead\neither mixing all the data for training or simply training dozens of separate\nmodels for different domains when new ones are available. To overcome these\nissues, we investigate a new task of crowd counting under the incremental\ndomains training setting, namely, Lifelong Crowd Counting. It aims at\nalleviating the catastrophic forgetting and improving the generalization\nability using a single model updated by the incremental domains. To be more\nspecific, we propose a self-distillation learning framework as a\nbenchmark~(Forget Less, Count Better, FLCB) for lifelong crowd counting, which\nhelps the model sustainably leverage previous meaningful knowledge for better\ncrowd counting to mitigate the forgetting when the new data arrive. Meanwhile,\na new quantitative metric, normalized backward transfer~(nBwT), is developed to\nevaluate the forgetting degree of the model in the lifelong learning process.\nExtensive experimental results demonstrate the superiority of our proposed\nbenchmark in achieving a low catastrophic forgetting degree and strong\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiaqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yanyun Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junping Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Distribution Learning. (arXiv:2205.03340v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03340","description":"<p>We present prompt distribution learning for effectively adapting a\npre-trained vision-language model to address downstream recognition tasks. Our\nmethod not only learns low-bias prompts from a few samples but also captures\nthe distribution of diverse prompts to handle the varying visual\nrepresentations. In this way, we provide high-quality task-related content for\nfacilitating recognition. This prompt distribution learning is realized by an\nefficient approach that learns the output embeddings of prompts instead of the\ninput embeddings. Thus, we can employ a Gaussian distribution to model them\neffectively and derive a surrogate loss for efficient training. Extensive\nexperiments on 12 datasets demonstrate that our method consistently and\nsignificantly outperforms existing methods. For example, with 1 sample per\ncategory, it relatively improves the average result by 9.1% compared to\nhuman-crafted prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yonggang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yajing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask AET with Orthogonal Tangent Regularity for Dark Object Detection. (arXiv:2205.03346v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03346","description":"<p>Dark environment becomes a challenge for computer vision algorithms owing to\ninsufficient photons and undesirable noise. To enhance object detection in a\ndark environment, we propose a novel multitask auto encoding transformation\n(MAET) model which is able to explore the intrinsic pattern behind illumination\ntranslation. In a self-supervision manner, the MAET learns the intrinsic visual\nstructure by encoding and decoding the realistic illumination-degrading\ntransformation considering the physical noise model and image signal processing\n(ISP).\n</p>\n<p>Based on this representation, we achieve the object detection task by\ndecoding the bounding box coordinates and classes. To avoid the\nover-entanglement of two tasks, our MAET disentangles the object and degrading\nfeatures by imposing an orthogonal tangent regularity. This forms a parametric\nmanifold along which multitask predictions can be geometrically formulated by\nmaximizing the orthogonality between the tangents along the outputs of\nrespective tasks. Our framework can be implemented based on the mainstream\nobject detection architecture and directly trained end-to-end using normal\ntarget detection datasets, such as VOC and COCO. We have achieved the\nstate-of-the-art performance using synthetic and real-world datasets. Code is\navailable at https://github.com/cuiziteng/MAET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Ziteng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1\">Lin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shaodi You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zenghui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All Grains, One Scheme (AGOS): Learning Multi-grain Instance Representation for Aerial Scene Classification. (arXiv:2205.03371v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03371","description":"<p>Aerial scene classification remains challenging as: 1) the size of key\nobjects in determining the scene scheme varies greatly; 2) many objects\nirrelevant to the scene scheme are often flooded in the image. Hence, how to\neffectively perceive the region of interests (RoIs) from a variety of sizes and\nbuild more discriminative representation from such complicated object\ndistribution is vital to understand an aerial scene. In this paper, we propose\na novel all grains, one scheme (AGOS) framework to tackle these challenges. To\nthe best of our knowledge, it is the first work to extend the classic multiple\ninstance learning into multi-grain formulation. Specially, it consists of a\nmulti-grain perception module (MGP), a multi-branch multi-instance\nrepresentation module (MBMIR) and a self-aligned semantic fusion (SSF) module.\nFirstly, our MGP preserves the differential dilated convolutional features from\nthe backbone, which magnifies the discriminative information from multi-grains.\nThen, our MBMIR highlights the key instances in the multi-grain representation\nunder the MIL formulation. Finally, our SSF allows our framework to learn the\nsame scene scheme from multi-grain instance representations and fuses them, so\nthat the entire framework is optimized as a whole. Notably, our AGOS is\nflexible and can be easily adapted to existing CNNs in a plug-and-play manner.\nExtensive experiments on UCM, AID and NWPU benchmarks demonstrate that our AGOS\nachieves a comparable performance against the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_Q/0/1/0/all/0/1\">Qi Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Beichen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-mode Tensor Train Factorization with Spatial-spectral Regularization for Remote Sensing Images Recovery. (arXiv:2205.03380v1 [eess.IV])","link":"http://arxiv.org/abs/2205.03380","description":"<p>Tensor train (TT) factorization and corresponding TT rank, which can well\nexpress the low-rankness and mode correlations of higher-order tensors, have\nattracted much attention in recent years. However, TT factorization based\nmethods are generally not sufficient to characterize low-rankness along each\nmode of third-order tensor. Inspired by this, we generalize the tensor train\nfactorization to the mode-k tensor train factorization and introduce a\ncorresponding multi-mode tensor train (MTT) rank. Then, we proposed a novel\nlow-MTT-rank tensor completion model via multi-mode TT factorization and\nspatial-spectral smoothness regularization. To tackle the proposed model, we\ndevelop an efficient proximal alternating minimization (PAM) algorithm.\nExtensive numerical experiment results on visual data demonstrate that the\nproposed MTTD3R method outperforms compared methods in terms of visual and\nquantitative measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_G/0/1/0/all/0/1\">Gaohang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_S/0/1/0/all/0/1\">Shaochun Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_L/0/1/0/all/0/1\">Liqun Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINI: Mining Implicit Novel Instances for Few-Shot Object Detection. (arXiv:2205.03381v1 [cs.CV])","link":"http://arxiv.org/abs/2205.03381","description":"<p>Learning from a few training samples is a desirable ability of an object\ndetector, inspiring the explorations of Few-Shot Object Detection (FSOD). Most\nexisting approaches employ a pretrain-transfer paradigm. The model is first\npre-trained on base classes with abundant data and then transferred to novel\nclasses with a few annotated samples. Despite the substantial progress, the\nFSOD performance is still far behind satisfactory. During pre-training, due to\nthe co-occurrence between base and novel classes, the model is learned to treat\nthe co-occurred novel classes as backgrounds. During transferring, given scarce\nsamples of novel classes, the model suffers from learning discriminative\nfeatures to distinguish novel instances from backgrounds and base classes. To\novercome the obstacles, we propose a novel framework, Mining Implicit Novel\nInstances (MINI), to mine the implicit novel instances as auxiliary training\nsamples, which widely exist in abundant base data but are not annotated. MINI\ncomprises an offline mining mechanism and an online mining mechanism. The\noffline mining mechanism leverages a self-supervised discriminative model to\ncollaboratively mine implicit novel instances with a trained FSOD network.\nTaking the mined novel instances as auxiliary training samples, the online\nmining mechanism takes a teacher-student framework to simultaneously update the\nFSOD network and the mined implicit novel instances on the fly. Extensive\nexperiments on PASCAL VOC and MS-COCO datasets show MINI achieves new\nstate-of-the-art performance on any shot and split. The significant performance\nimprovements demonstrate the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Domain-Independent Feature Decomposition Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2003.09869v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.09869","description":"<p>Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal\nretrieval task for searching natural images given free-hand sketches under the\nzero-shot scenario. Most existing methods solve this problem by simultaneously\nprojecting visual features and semantic supervision into a low-dimensional\ncommon space for efficient retrieval. However, such low-dimensional projection\ndestroys the completeness of semantic knowledge in original semantic space, so\nthat it is unable to transfer useful knowledge well when learning semantic from\ndifferent modalities. Moreover, the domain information and semantic information\nare entangled in visual features, which is not conducive for cross-modal\nmatching since it will hinder the reduction of domain gap between sketch and\nimage. In this paper, we propose a Progressive Domain-independent Feature\nDecomposition (PDFD) network for ZS-SBIR. Specifically, with the supervision of\noriginal semantic knowledge, PDFD decomposes visual features into domain\nfeatures and semantic ones, and then the semantic features are projected into\ncommon space as retrieval features for ZS-SBIR. The progressive projection\nstrategy maintains strong semantic supervision. Besides, to guarantee the\nretrieval features to capture clean and complete semantic information, the\ncross-reconstruction loss is introduced to encourage that any combinations of\nretrieval features and domain features can reconstruct the visual features.\nExtensive experiments demonstrate the superiority of our PDFD over\nstate-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinxun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muli Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Quantification for Hyperspectral Image Denoising Frameworks based on Low-rank Matrix Approximation. (arXiv:2004.10959v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2004.10959","description":"<p>Sliding-window based low-rank matrix approximation (LRMA) is a technique\nwidely used in hyperspectral images (HSIs) denoising or completion. However,\nthe uncertainty quantification of the restored HSI has not been addressed to\ndate. Accurate uncertainty quantification of the denoised HSI facilitates to\napplications such as multi-source or multi-scale data fusion, data\nassimilation, and product uncertainty quantification, since these applications\nrequire an accurate approach to describe the statistical distributions of the\ninput data. Therefore, we propose a prior-free closed-form element-wise\nuncertainty quantification method for LRMA-based HSI restoration. Our\nclosed-form algorithm overcomes the difficulty of the HSI patch mixing problem\ncaused by the sliding-window strategy used in the conventional LRMA process.\nThe proposed approach only requires the uncertainty of the observed HSI and\nprovides the uncertainty result relatively rapidly and with similar\ncomputational complexity as the LRMA technique. We conduct extensive\nexperiments to validate the estimation accuracy of the proposed closed-form\nuncertainty approach. The method is robust to at least 10% random impulse noise\nat the cost of 10-20% of additional processing time compared to the LRMA. The\nexperiments indicate that the proposed closed-form uncertainty quantification\nmethod is more applicable to real-world applications than the baseline Monte\nCarlo test, which is computationally expensive. The code is available in the\nattachment and will be released after the acceptance of this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jingwei Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_S/0/1/0/all/0/1\">Shaobo Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_M/0/1/0/all/0/1\">Mitesh Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction. (arXiv:2105.09711v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09711","description":"<p>Joint relation modeling is a curial component in human motion prediction.\nMost existing methods tend to design skeletal-based graphs to build the\nrelations among joints, where local interactions between joint pairs are well\nlearned. However, the global coordination of all joints, which reflects human\nmotion's balance property, is usually weakened because it is learned from part\nto whole progressively and asynchronously. Thus, the final predicted motions\nare sometimes unnatural. To tackle this issue, we learn a medium, called\nbalance attractor (BA), from the spatiotemporal features of motion to\ncharacterize the global motion features, which is subsequently used to build\nnew joint relations. Through the BA, all joints are related synchronously, and\nthus the global coordination of all joints can be better learned. Based on the\nBA, we propose our framework, referred to Attractor-Guided Neural Network,\nmainly including Attractor-Based Joint Relation Extractor (AJRE) and\nMulti-timescale Dynamics Extractor (MTDE). The AJRE mainly includes Global\nCoordination Extractor (GCE) and Local Interaction Extractor (LIE). The former\npresents the global coordination of all joints, and the latter encodes local\ninteractions between joint pairs. The MTDE is designed to extract dynamic\ninformation from raw position information for effective prediction. Extensive\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods in both short and long-term predictions in H3.6M, CMU-Mocap, and 3DPW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1\">Pengxiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Adversarial Driving Scenario Generation with Explicit Knowledge Integration. (arXiv:2106.04066v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04066","description":"<p>Generating adversarial scenarios, which have the potential to fail autonomous\ndriving systems, provides an effective way to improve the robustness. Extending\npurely data-driven generative models, recent specialized models satisfy\nadditional controllable requirements such as embedding a traffic sign in a\ndriving scene by manipulating patterns implicitly in the neuron level. In this\npaper, we introduce a method to incorporate domain knowledge explicitly in the\ngeneration process to achieve the Semantically Adversarial Generation (SAG). To\nbe consistent with the composition of driving scenes, we first categorize the\nknowledge into two types, the property of objects and the relationship among\nobjects. We then propose a tree-structured variational auto-encoder (T-VAE) to\nlearn hierarchical scene representation. By imposing semantic rules on the\nproperties of nodes and edges in the tree structure, explicit knowledge\nintegration enables controllable generation. We construct a synthetic example\nto illustrate the controllability and explainability of our method in a\nsuccinct setting. We further extend to realistic environments for autonomous\nvehicles: our method efficiently identifies adversarial driving scenes against\ndifferent state-of-the-art 3D point cloud segmentation models and satisfies the\ntraffic rules specified as the explicit knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haohong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1\">Kim Ji Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Total Recall in Industrial Anomaly Detection. (arXiv:2106.08265v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08265","description":"<p>Being able to spot defective parts is a critical component in large-scale\nindustrial manufacturing. A particular challenge that we address in this work\nis the cold-start problem: fit a model using nominal (non-defective) example\nimages only. While handcrafted solutions per class are possible, the goal is to\nbuild systems that work well simultaneously on many different tasks\nautomatically. The best performing approaches combine embeddings from ImageNet\nmodels with an outlier detection model. In this paper, we extend on this line\nof work and propose \\textbf{PatchCore}, which uses a maximally representative\nmemory bank of nominal patch-features. PatchCore offers competitive inference\ntimes while achieving state-of-the-art performance for both detection and\nlocalization. On the challenging, widely used MVTec AD benchmark PatchCore\nachieves an image-level anomaly detection AUROC score of up to $99.6\\%$, more\nthan halving the error compared to the next best competitor. We further report\ncompetitive results on two additional datasets and also find competitive\nresults in the few samples regime.\\freefootnote{$^*$ Work done during a\nresearch internship at Amazon AWS.} Code:\ngithub.com/amazon-research/patchcore-inspection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pemula_L/0/1/0/all/0/1\">Latha Pemula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zepeda_J/0/1/0/all/0/1\">Joaquin Zepeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05087","description":"<p>Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory\nfunctionality and is receiving increasing attention during the COVID-19\npandemic. Clinical findings show that it is possible for COVID-19 patients to\nhave significantly low SpO$_2$ before any obvious symptoms. The prevalence of\ncameras has motivated researchers to investigate methods for monitoring SpO$_2$\nusing videos. Most prior schemes involving smartphones are contact-based: They\nrequire a fingertip to cover the phone's camera and the nearby light source to\ncapture re-emitted light from the illuminated tissue. In this paper, we propose\nthe first convolutional neural network based noncontact SpO$_2$ estimation\nscheme using smartphone cameras. The scheme analyzes the videos of a\nparticipant's hand for physiological sensing, which is convenient and\ncomfortable, and can protect their privacy and allow for keeping face masks on.\nWe design our neural network architectures inspired by the optophysiological\nmodels for SpO$_2$ measurement and demonstrate the explainability by\nvisualizing the weights for channel combination. Our proposed models outperform\nthe state-of-the-art model that is designed for contact-based SpO$_2$\nmeasurement, showing the potential of our proposed method to contribute to\npublic health. We also analyze the impact of skin type and the side of a hand\non SpO$_2$ estimation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Joshua Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Chau-Wai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Optimal Conformal Classifiers. (arXiv:2110.09192v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09192","description":"<p>Modern deep learning based classifiers show very high accuracy on test data\nbut this does not provide sufficient guarantees for safe deployment, especially\nin high-stake AI applications such as medical diagnosis. Usually, predictions\nare obtained without a reliable uncertainty estimate or a formal guarantee.\nConformal prediction (CP) addresses these issues by using the classifier's\npredictions, e.g., its probability estimates, to predict confidence sets\ncontaining the true class with a user-specified probability. However, using CP\nas a separate processing step after training prevents the underlying model from\nadapting to the prediction of confidence sets. Thus, this paper explores\nstrategies to differentiate through CP during training with the goal of\ntraining model with the conformal wrapper end-to-end. In our approach,\nconformal training (ConfTr), we specifically \"simulate\" conformalization on\nmini-batches during training. Compared to standard training, ConfTr reduces the\naverage confidence set size (inefficiency) of state-of-the-art CP methods\napplied after training. Moreover, it allows to \"shape\" the confidence sets\npredicted at test time, which is difficult for standard CP. On experiments with\nseveral datasets, we show ConfTr can influence how inefficiency is distributed\nacross classes, or guide the composition of confidence sets in terms of the\nincluded classes, while retaining the guarantees offered by CP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1\">David Stutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy/0/1/0/all/0/1\">Krishnamurthy</a> (Dj) <a href=\"http://arxiv.org/find/cs/1/au:+Dvijotham/0/1/0/all/0/1\">Dvijotham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cemgil_A/0/1/0/all/0/1\">Ali Taylan Cemgil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAGLETS: A System for Automatic Semi-Supervised Learning with Auxiliary Data. (arXiv:2111.04798v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.04798","description":"<p>Machine learning practitioners often have access to a spectrum of data:\nlabeled data for the target task (which is often limited), unlabeled data, and\nauxiliary data, the many available labeled datasets for other tasks. We\ndescribe TAGLETS, a system built to study techniques for automatically\nexploiting all three types of data and creating high-quality, servable\nclassifiers. The key components of TAGLETS are: (1) auxiliary data organized\naccording to a knowledge graph, (2) modules encapsulating different methods for\nexploiting auxiliary and unlabeled data, and (3) a distillation stage in which\nthe ensembled modules are combined into a servable model. We compare TAGLETS\nwith state-of-the-art transfer learning and semi-supervised learning methods on\nfour image classification tasks. Our study covers a range of settings, varying\nthe amount of labeled data and the semantic relatedness of the auxiliary data\nto the target task. We find that the intelligent incorporation of auxiliary and\nunlabeled data into multiple learning techniques enables TAGLETS to match-and\nmost often significantly surpass-these alternatives. TAGLETS is available as an\nopen-source system at github.com/BatsResearch/taglets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piriyakulkij_W/0/1/0/all/0/1\">Wasu Piriyakulkij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menghini_C/0/1/0/all/0/1\">Cristina Menghini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briden_R/0/1/0/all/0/1\">Ross Briden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jeffrey Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raisi_E/0/1/0/all/0/1\">Elaheh Raisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CYBORG: Blending Human Saliency Into the Loss Improves Deep Learning. (arXiv:2112.00686v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00686","description":"<p>Can deep learning models achieve greater generalization if their training is\nguided by reference to human perceptual abilities? And how can we implement\nthis in a practical manner? This paper proposes a training strategy to ConveY\nBrain Oversight to Raise Generalization (CYBORG). This new approach\nincorporates human-annotated saliency maps into a CYBORG loss function that\nguides the model's learning towards features from image regions that humans\nfind salient for the task. The Class Activation Mapping (CAM) mechanism is used\nto probe the model's current saliency in each training batch, juxtapose this\nmodel saliency with human saliency, and penalize large differences. Results on\nthe task of synthetic face detection, selected to illustrate the effectiveness\nof the approach, show that CYBORG leads to significant improvement in accuracy\non unseen samples consisting of face images generated from six Generative\nAdversarial Networks across multiple classification network architectures. We\nalso show that scaling to even seven times the training data with standard loss\ncannot beat CYBORG accuracy. As a side effect, we observe that the addition of\nexplicit region annotation to the task of synthetic face detection increased\nhuman classification performance. This work opens a new area of research on how\nto incorporate human visual saliency into loss functions in practice. All data,\ncode and pre-trained models used in this work are offered with this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boyd_A/0/1/0/all/0/1\">Aidan Boyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinsley_P/0/1/0/all/0/1\">Patrick Tinsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin Bowyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1\">Adam Czajka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Echocardiography Segmentation with Enforced Temporal Consistency. (arXiv:2112.02102v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.02102","description":"<p>Convolutional neural networks (CNN) have demonstrated their ability to\nsegment 2D cardiac ultrasound images. However, despite recent successes\naccording to which the intra-observer variability on end-diastole and\nend-systole images has been reached, CNNs still struggle to leverage temporal\ninformation to provide accurate and temporally consistent segmentation maps\nacross the whole cycle. Such consistency is required to accurately describe the\ncardiac function, a necessary step in diagnosing many cardiovascular diseases.\nIn this paper, we propose a framework to learn the 2D+time apical long-axis\ncardiac shape such that the segmented sequences can benefit from temporal and\nanatomical consistency constraints. Our method is a post-processing that takes\nas input segmented echocardiographic sequences produced by any state-of-the-art\nmethod and processes it in two steps to (i) identify spatio-temporal\ninconsistencies according to the overall dynamics of the cardiac sequence and\n(ii) correct the inconsistencies. The identification and correction of cardiac\ninconsistencies relies on a constrained autoencoder trained to learn a\nphysiologically interpretable embedding of cardiac shapes, where we can both\ndetect and fix anomalies. We tested our framework on 98 full-cycle sequences\nfrom the CAMUS dataset, which are available alongside this paper. Our temporal\nregularization method not only improves the accuracy of the segmentation across\nthe whole sequences, but also enforces temporal and anatomical consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Painchaud_N/0/1/0/all/0/1\">Nathan Painchaud</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duchateau_N/0/1/0/all/0/1\">Nicolas Duchateau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bernard_O/0/1/0/all/0/1\">Olivier Bernard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPICT: Deep Progressive Image Compression Using Trit-Planes. (arXiv:2112.06334v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.06334","description":"<p>We propose the deep progressive image compression using trit-planes (DPICT)\nalgorithm, which is the first learning-based codec supporting fine granular\nscalability (FGS). First, we transform an image into a latent tensor using an\nanalysis network. Then, we represent the latent tensor in ternary digits\n(trits) and encode it into a compressed bitstream trit-plane by trit-plane in\nthe decreasing order of significance. Moreover, within each trit-plane, we sort\nthe trits according to their rate-distortion priorities and transmit more\nimportant information first. Since the compression network is less optimized\nfor the cases of using fewer trit-planes, we develop a postprocessing network\nfor refining reconstructed images at low rates. Experimental results show that\nDPICT outperforms conventional progressive codecs significantly, while enabling\nFGS transmission. Codes are available at\nhttps://github.com/jaehanlee-mcl/DPICT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jae-Han Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jeon_S/0/1/0/all/0/1\">Seungmin Jeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_K/0/1/0/all/0/1\">Kwang Pyo Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Youngo Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Image Denoising Algorithm Using Concepts of Quantum Many-Body Theory. (arXiv:2112.09254v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.09254","description":"<p>Sparse representation of real-life images is a very effective approach in\nimaging applications, such as denoising. In recent years, with the growth of\ncomputing power, data-driven strategies exploiting the redundancy within\npatches extracted from one or several images to increase sparsity have become\nmore prominent. This paper presents a novel image denoising algorithm\nexploiting such an image-dependent basis inspired by the quantum many-body\ntheory. Based on patch analysis, the similarity measures in a local image\nneighborhood are formalized through a term akin to interaction in quantum\nmechanics that can efficiently preserve the local structures of real images.\nThe versatile nature of this adaptive basis extends the scope of its\napplication to image-independent or image-dependent noise scenarios without any\nadjustment. We carry out a rigorous comparison with contemporary methods to\ndemonstrate the denoising capability of the proposed algorithm regardless of\nthe image characteristics, noise statistics and intensity. We illustrate the\nproperties of the hyperparameters and their respective effects on the denoising\nperformance, together with automated rules of selecting their values close to\nthe optimal one in experimental setups with ground truth not available.\nFinally, we show the ability of our approach to deal with practical images\ndenoising problems such as medical ultrasound image despeckling applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dutta_S/0/1/0/all/0/1\">Sayantan Dutta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basarab_A/0/1/0/all/0/1\">Adrian Basarab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgeot_B/0/1/0/all/0/1\">Bertrand Georgeot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kouame_D/0/1/0/all/0/1\">Denis Kouam&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Visible: A Survey on Cross-spectral Face Recognition. (arXiv:2201.04435v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04435","description":"<p>Cross-spectral face recognition (CFR) refers to recognizing individuals using\nface images stemming from different spectral bands, such as infrared vs.\nvisible. While CFR is inherently more challenging than classical face\nrecognition due to significant variation in facial appearance caused by the\nmodality gap, it is useful in many scenarios including night-vision biometrics\nand detecting presentation attacks. Recent advances in convolutional neural\nnetworks (CNNs) have resulted in significant improvement in the performance of\nCFR systems. Given these developments, the contributions of this survey are\nthree-fold. First, we provide an overview of CFR, by formalizing the CFR\nproblem and presenting related applications. Secondly, we discuss the\nappropriate spectral bands for face recognition and discuss recent CFR methods,\nplacing emphasis on deep neural networks. In particular we describe techniques\nthat have been proposed to extract and compare heterogeneous features emerging\nfrom different spectral bands. We also discuss the datasets that have been used\nfor evaluating CFR methods. Finally, we discuss the challenges and future lines\nof research on this topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anghelone_D/0/1/0/all/0/1\">David Anghelone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cunjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Arun Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields. (arXiv:2201.09968v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09968","description":"<p>High-resolution optical satellite sensors, combined with dense stereo\nalgorithms, have made it possible to reconstruct 3D city models from space.\nHowever, these models are, in practice, rather noisy and tend to miss small\ngeometric features that are clearly visible in the images. We argue that one\nreason for the limited quality may be a too early, heuristic reduction of the\ntriangulated 3D point cloud to an explicit height field or surface mesh. To\nmake full use of the point cloud and the underlying images, we introduce\nImpliCity, a neural representation of the 3D scene as an implicit, continuous\noccupancy field, driven by learned embeddings of the point cloud and a stereo\npair of ortho-photos. We show that this representation enables the extraction\nof high-quality DSMs: with image resolution 0.5$\\,$m, ImpliCity reaches a\nmedian height error of $\\approx\\,$0.7$\\,$m and outperforms competing methods,\nespecially w.r.t. building reconstruction, featuring intricate roof details,\nsmooth surfaces, and straight, regular outlines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stucker_C/0/1/0/all/0/1\">Corinne Stucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_B/0/1/0/all/0/1\">Bingxin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yuanwen Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armeni_I/0/1/0/all/0/1\">Iro Armeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration. (arXiv:2201.12716v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2201.12716","description":"<p>Promising results have been achieved recently in category-level manipulation\nthat generalizes across object instances. Nevertheless, it often requires\nexpensive real-world data collection and manual specification of semantic\nkeypoints for each object category and task. Additionally, coarse keypoint\npredictions and ignoring intermediate action sequences hinder adoption in\ncomplex manipulation tasks beyond pick-and-place. This work proposes a novel,\ncategory-level manipulation framework that leverages an object-centric,\ncategory-level representation and model-free 6 DoF motion tracking. The\ncanonical object representation is learned solely in simulation and then used\nto parse a category-level, task trajectory from a single demonstration video.\nThe demonstration is reprojected to a target trajectory tailored to a novel\nobject via the canonical representation. During execution, the manipulation\nhorizon is decomposed into longrange, collision-free motion and last-inch\nmanipulation. For the latter part, a category-level behavior cloning (CatBC)\nmethod leverages motion tracking to perform closed-loop control. CatBC follows\nthe target trajectory, projected from the demonstration and anchored to a\ndynamically selected category-level coordinate frame. The frame is\nautomatically selected along the manipulation horizon by a local attention\nmechanism. This framework allows to teach different manipulation strategies by\nsolely providing a single demonstration, without complicated manual\nprogramming. Extensive experiments demonstrate its efficacy in a range of\nchallenging industrial tasks in highprecision assembly, which involve learning\ncomplex, long-horizon policies. The process exhibits robustness against\nuncertainty due to dynamics as well as generalization across object instances\nand scene configurations. The supplementary video is available at\nhttps://www.youtube.com/watch?v=WAr8ZY3mYyw\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_W/0/1/0/all/0/1\">Wenzhao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaal_S/0/1/0/all/0/1\">Stefan Schaal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04533","description":"<p>Emerging Metaverse applications demand reliable, accurate, and photorealistic\nreproductions of human hands to perform sophisticated operations as if in the\nphysical world. While real human hand represents one of the most intricate\ncoordination between bones, muscle, tendon, and skin, state-of-the-art\ntechniques unanimously focus on modeling only the skeleton of the hand. In this\npaper, we present NIMBLE, a novel parametric hand model that includes the\nmissing key components, bringing 3D hand model to a new level of realism. We\nfirst annotate muscles, bones and skins on the recent Magnetic Resonance\nImaging hand (MRI-Hand) dataset and then register a volumetric template hand\nonto individual poses and subjects within the dataset. NIMBLE consists of 20\nbones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin\nmesh. Via iterative shape registration and parameter learning, it further\nproduces shape blend shapes, pose blend shapes, and a joint regressor. We\ndemonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.\nBy enforcing the inner bones and muscles to match anatomic and kinematic rules,\nNIMBLE can animate 3D hands to new poses at unprecedented realism. To model the\nappearance of skin, we further construct a photometric HandStage to acquire\nhigh-quality textures and normal maps to model wrinkles and palm print.\nFinally, NIMBLE also benefits learning-based hand pose and shape estimation by\neither synthesizing rich data or acting directly as a differentiable layer in\nthe inference network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zesong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingwenqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Beneath the Ink: Synthetic Data and Tattoo Removal with Application to Face Recognition. (arXiv:2202.05297v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05297","description":"<p>Systems that analyse faces have seen significant improvements in recent years\nand are today used in numerous application scenarios. However, these systems\nhave been found to be negatively affected by facial alterations such as\ntattoos. To better understand and mitigate the effect of facial tattoos in\nfacial analysis systems, large datasets of images of individuals with and\nwithout tattoos are needed. To this end, we propose a generator for\nautomatically adding realistic tattoos to facial images. Moreover, we\ndemonstrate the feasibility of the generation by using a deep learning-based\nmodel for removing tattoos from face images. The experimental results show that\nit is possible to remove facial tattoos from real images without degrading the\nquality of the image. Additionally, we show that it is possible to improve face\nrecognition accuracy by using the proposed deep learning-based tattoo removal\nbefore extracting and comparing facial features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibsen_M/0/1/0/all/0/1\">Mathias Ibsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images. (arXiv:2202.07983v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.07983","description":"<p>Age-related macular degeneration (AMD) is the leading cause of visual\nimpairment among elderly in the world. Early detection of AMD is of great\nimportance, as the vision loss caused by this disease is irreversible and\npermanent. Color fundus photography is the most cost-effective imaging modality\nto screen for retinal disorders. Cutting edge deep learning based algorithms\nhave been recently developed for automatically detecting AMD from fundus\nimages. However, there are still lack of a comprehensive annotated dataset and\nstandard evaluation benchmarks. To deal with this issue, we set up the\nAutomatic Detection challenge on Age-related Macular degeneration (ADAM), which\nwas held as a satellite event of the ISBI 2020 conference. The ADAM challenge\nconsisted of four tasks which cover the main aspects of detecting and\ncharacterizing AMD from fundus images, including detection of AMD, detection\nand segmentation of optic disc, localization of fovea, and detection and\nsegmentation of lesions. As part of the challenge, we have released a\ncomprehensive dataset of 1200 fundus images with AMD diagnostic labels,\npixel-wise segmentation masks for both optic disc and AMD-related lesions\n(drusen, exudates, hemorrhages and scars, among others), as well as the\ncoordinates corresponding to the location of the macular fovea. A uniform\nevaluation framework has been built to make a fair comparison of different\nmodels using this dataset. During the challenge, 610 results were submitted for\nonline evaluation, with 11 teams finally participating in the onsite challenge.\nThis paper introduces the challenge, the dataset and the evaluation methods, as\nwell as summarizes the participating methods and analyzes their results for\neach task. In particular, we observed that the ensembling strategy and the\nincorporation of clinical domain knowledge were the key to improve the\nperformance of the deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1\">Xingxing Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1\">Fengbin Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Son_J/0/1/0/all/0/1\">Jaemin Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sunho Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quellec_G/0/1/0/all/0/1\">Gwenole Quellec</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matta_S/0/1/0/all/0/1\">Sarah Matta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shankaranarayana_S/0/1/0/all/0/1\">Sharath M Shankaranarayana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chuen-heng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg A. Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Yen Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1\">Chih-Chung Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1\">Hai Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_B/0/1/0/all/0/1\">Baiying Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Innani_S/0/1/0/all/0/1\">Shubham Innani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_K/0/1/0/all/0/1\">Kang Dang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1\">Wenxiu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamble_R/0/1/0/all/0/1\">Ravi Kamble</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singhal_N/0/1/0/all/0/1\">Nitin Singhal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Ching-Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lo_S/0/1/0/all/0/1\">Shih-Chang Lo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orlando_J/0/1/0/all/0/1\">Jos&#xe9; Ignacio Orlando</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiulan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+group_iChallenge_AMD_study/0/1/0/all/0/1\">iChallenge-AMD study group</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Adversarial Robustness Through Robust Representation Matching. (arXiv:2202.09994v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.09994","description":"<p>With the widespread use of machine learning, concerns over its security and\nreliability have become prevalent. As such, many have developed defenses to\nharden neural networks against adversarial examples, imperceptibly perturbed\ninputs that are reliably misclassified. Adversarial training in which\nadversarial examples are generated and used during training is one of the few\nknown defenses able to reliably withstand such attacks against neural networks.\nHowever, adversarial training imposes a significant training overhead and\nscales poorly with model complexity and input dimension. In this paper, we\npropose Robust Representation Matching (RRM), a low-cost method to transfer the\nrobustness of an adversarially trained model to a new model being trained for\nthe same task irrespective of architectural differences. Inspired by\nstudent-teacher learning, our method introduces a novel training loss that\nencourages the student to learn the teacher's robust representations. Compared\nto prior works, RRM is superior with respect to both model performance and\nadversarial training time. On CIFAR-10, RRM trains a robust model $\\sim\n1.8\\times$ faster than the state-of-the-art. Furthermore, RRM remains effective\non higher-dimensional datasets. On Restricted-ImageNet, RRM trains a ResNet50\nmodel $\\sim 18\\times$ faster than standard adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaishnavi_P/0/1/0/all/0/1\">Pratik Vaishnavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eykholt_K/0/1/0/all/0/1\">Kevin Eykholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1\">Amir Rahmati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values. (arXiv:2203.01993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01993","description":"<p>We present Polarity Sampling, a theoretically justified plug-and-play method\nfor controlling the generation quality and diversity of pre-trained deep\ngenerative networks DGNs). Leveraging the fact that DGNs are, or can be\napproximated by, continuous piecewise affine splines, we derive the analytical\nDGN output space distribution as a function of the product of the DGN's\nJacobian singular values raised to a power $\\rho$. We dub $\\rho$ the\n$\\textbf{polarity}$ parameter and prove that $\\rho$ focuses the DGN sampling on\nthe modes ($\\rho &lt; 0$) or anti-modes ($\\rho &gt; 0$) of the DGN output-space\ndistribution. We demonstrate that nonzero polarity values achieve a better\nprecision-recall (quality-diversity) Pareto frontier than standard methods,\nsuch as truncation, for a number of state-of-the-art DGNs. We also present\nquantitative and qualitative results on the improvement of overall generation\nquality (e.g., in terms of the Frechet Inception Distance) for a number of\nstate-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different\nconditional and unconditional image generation tasks. In particular, Polarity\nSampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to\nFID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and StyleGAN3 on the\nAFHQv2 Dataset to FID 3.95. Demo: bit.ly/polarity-samp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1\">Ahmed Imtiaz Humayun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AEGNN: Asynchronous Event-based Graph Neural Networks. (arXiv:2203.17149v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17149","description":"<p>The best performing learning algorithms devised for event cameras work by\nfirst converting events into dense representations that are then processed\nusing standard CNNs. However, these steps discard both the sparsity and high\ntemporal resolution of events, leading to high computational burden and\nlatency. For this reason, recent works have adopted Graph Neural Networks\n(GNNs), which process events as \"static\" spatio-temporal graphs, which are\ninherently \"sparse\". We take this trend one step further by introducing\nAsynchronous, Event-based Graph Neural Networks (AEGNNs), a novel\nevent-processing paradigm that generalizes standard GNNs to process events as\n\"evolving\" spatio-temporal graphs. AEGNNs follow efficient update rules that\nrestrict recomputation of network activations only to the nodes affected by\neach new event, thereby significantly reducing both computation and latency for\nevent-by-event processing. AEGNNs are easily trained on synchronous inputs and\ncan be converted to efficient, \"asynchronous\" networks at test time. We\nthoroughly validate our method on object classification and detection tasks,\nwhere we show an up to a 200-fold reduction in computational complexity\n(FLOPs), with similar or even better performance than state-of-the-art\nasynchronous methods. This reduction in computation directly translates to an\n8-fold reduction in computational latency when compared to standard GNNs, which\nopens the door to low-latency event-based processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_S/0/1/0/all/0/1\">Simon Schaefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SymForce: Symbolic Computation and Code Generation for Robotics. (arXiv:2204.07889v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.07889","description":"<p>We present SymForce, a library for fast symbolic computation, code\ngeneration, and nonlinear optimization for robotics applications like computer\nvision, motion planning, and controls. SymForce combines the development speed\nand flexibility of symbolic math with the performance of autogenerated, highly\noptimized code in C++ or any target runtime language. SymForce provides\ngeometry and camera types, Lie group operations, and branchless singularity\nhandling for creating and analyzing complex symbolic expressions in Python,\nbuilt on top of SymPy. Generated functions can be integrated as factors into\nour tangent-space nonlinear optimizer, which is highly optimized for real-time\nproduction use. We introduce novel methods to automatically compute\ntangent-space Jacobians, eliminating the need for bug-prone handwritten\nderivatives. This workflow enables faster runtime code, faster development\ntime, and fewer lines of handwritten code versus the state-of-the-art. Our\nexperiments demonstrate that our approach can yield order of magnitude speedups\non computational tasks core to robotics. Code is available at\nhttps://github.com/symforce-org/symforce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martiros_H/0/1/0/all/0/1\">Hayk Martiros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1\">Aaron Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucki_N/0/1/0/all/0/1\">Nathan Bucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solliday_B/0/1/0/all/0/1\">Bradley Solliday</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_R/0/1/0/all/0/1\">Ryan Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jack Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tung Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattison_D/0/1/0/all/0/1\">Dominic Pattison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Harrison Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomic_T/0/1/0/all/0/1\">Teo Tomic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_P/0/1/0/all/0/1\">Peter Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_G/0/1/0/all/0/1\">Gareth Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanderMey_J/0/1/0/all/0/1\">Josiah VanderMey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Alvin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Samuel Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtz_K/0/1/0/all/0/1\">Kristen Holtz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Working memory inspired hierarchical video decomposition with transformative representations. (arXiv:2204.10105v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10105","description":"<p>Video decomposition is very important to extract moving foreground objects\nfrom complex backgrounds in computer vision, machine learning, and medical\nimaging, e.g., extracting moving contrast-filled vessels from the complex and\nnoisy backgrounds of X-ray coronary angiography (XCA). However, the challenges\ncaused by dynamic backgrounds, overlapping heterogeneous environments and\ncomplex noises still exist in video decomposition. To solve these problems,\nthis study is the first to introduce a flexible visual working memory model in\nvideo decomposition tasks to provide interpretable and high-performance\nhierarchical deep architecture, integrating the transformative representations\nbetween sensory and control layers from the perspective of visual and cognitive\nneuroscience. Specifically, robust PCA unrolling networks acting as a\nstructure-regularized sensor layer decompose XCA into sparse/low-rank\nstructured representations to separate moving contrast-filled vessels from\nnoisy and complex backgrounds. Then, patch recurrent convolutional LSTM\nnetworks with a backprojection module embody unstructured random\nrepresentations of the control layer in working memory, recurrently projecting\nspatiotemporally decomposed nonlocal patches into orthogonal subspaces for\nheterogeneous vessel retrieval and interference suppression. This video\ndecomposition deep architecture effectively restores the heterogeneous profiles\nof intensity and the geometries of moving objects against the complex\nbackground interferences. Experiments show that the proposed method\nsignificantly outperforms state-of-the-art methods in accurate moving\ncontrast-filled vessel extraction with excellent flexibility and computational\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Binjie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Haohao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yueqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Song Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poly-CAM: High resolution class activation map for convolutional neural networks. (arXiv:2204.13359v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13359","description":"<p>The need for Explainable AI is increasing with the development of deep\nlearning. The saliency maps derived from convolutional neural networks\ngenerally fail in localizing with accuracy the image features justifying the\nnetwork prediction. This is because those maps are either low-resolution as for\nCAM [Zhou et al., 2016], or smooth as for perturbation-based methods [Zeiler\nand Fergus, 2014], or do correspond to a large number of widespread peaky spots\nas for gradient-based approaches [Sundararajan et al., 2017, Smilkov et al.,\n2017]. In contrast, our work proposes to combine the information from earlier\nnetwork layers with the one from later layers to produce a high resolution\nClass Activation Map that is competitive with the previous art in term of\ninsertion-deletion faithfulness metrics, while outperforming it in term of\nprecision of class-specific features localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Englebert_A/0/1/0/all/0/1\">Alexandre Englebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornu_O/0/1/0/all/0/1\">Olivier Cornu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vleeschouwer_C/0/1/0/all/0/1\">Christophe De Vleeschouwer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual networks based 3D Multi-Person Pose Estimation from Monocular Video. (arXiv:2205.00748v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00748","description":"<p>Monocular 3D human pose estimation has made progress in recent years. Most of\nthe methods focus on single persons, which estimate the poses in the\nperson-centric coordinates, i.e., the coordinates based on the center of the\ntarget person. Hence, these methods are inapplicable for multi-person 3D pose\nestimation, where the absolute coordinates (e.g., the camera coordinates) are\nrequired. Moreover, multi-person pose estimation is more challenging than\nsingle pose estimation, due to inter-person occlusion and close human\ninteractions. Existing top-down multi-person methods rely on human detection\n(i.e., top-down approach), and thus suffer from the detection errors and cannot\nproduce reliable pose estimation in multi-person scenes. Meanwhile, existing\nbottom-up methods that do not use human detection are not affected by detection\nerrors, but since they process all persons in a scene at once, they are prone\nto errors, particularly for persons in small scales. To address all these\nchallenges, we propose the integration of top-down and bottom-up approaches to\nexploit their strengths. Our top-down network estimates human joints from all\npersons instead of one in an image patch, making it robust to possible\nerroneous bounding boxes. Our bottom-up network incorporates human-detection\nbased normalized heatmaps, allowing the network to be more robust in handling\nscale variations. Finally, the estimated 3D poses from the top-down and\nbottom-up networks are fed into our integration network for final 3D poses. To\naddress the common gaps between training and testing data, we do optimization\nduring the test time, by refining the estimated 3D human poses using high-order\ntemporal constraint, re-projection loss, and bone length regularizations. Our\nevaluations demonstrate the effectiveness of the proposed method. Code and\nmodels are available: https://github.com/3dpose/3D-Multi-Person-Pose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Robby T. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation. (arXiv:2205.01133v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01133","description":"<p>Multi-modal Machine Translation (MMT) enables the use of visual information\nto enhance the quality of translations. The visual information can serve as a\nvaluable piece of context information to decrease the ambiguity of input\nsentences. Despite the increasing popularity of such a technique, good and\nsizeable datasets are scarce, limiting the full extent of their potential.\nHausa, a Chadic language, is a member of the Afro-Asiatic language family. It\nis estimated that about 100 to 150 million people speak the language, with more\nthan 80 million indigenous speakers. This is more than any of the other Chadic\nlanguages. Despite a large number of speakers, the Hausa language is considered\nlow-resource in natural language processing (NLP). This is due to the absence\nof sufficient resources to implement most NLP tasks. While some datasets exist,\nthey are either scarce, machine-generated, or in the religious domain.\nTherefore, there is a need to create training and evaluation data for\nimplementing machine learning tasks and bridging the research gap in the\nlanguage. This work presents the Hausa Visual Genome (HaVG), a dataset that\ncontains the description of an image or a section within the image in Hausa and\nits equivalent in English. To prepare the dataset, we started by translating\nthe English description of the images in the Hindi Visual Genome (HVG) into\nHausa automatically. Afterward, the synthetic Hausa data was carefully\npost-edited considering the respective images. The dataset comprises 32,923\nimages and their descriptions that are divided into training, development,\ntest, and challenge test set. The Hausa Visual Genome is the first dataset of\nits kind and can be used for Hausa-English machine translation, multi-modal\nresearch, and image description, among various other natural language\nprocessing and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Satya Ranjan Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawud_M/0/1/0/all/0/1\">Musa Abdullahi Dawud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parida_S/0/1/0/all/0/1\">Shantipriya Parida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Sa&#x27;id Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_S/0/1/0/all/0/1\">Subhadarshi Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galadanci_B/0/1/0/all/0/1\">Bashir Shehu Galadanci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1\">Bello Shehu Bello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion Networks. (arXiv:2205.01355v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2205.01355","description":"<p>We present a learning algorithm that uses bone-driven motion networks to\npredict the deformation of loose-fitting garment meshes at interactive rates.\nGiven a garment, we generate a simulation database and extract virtual bones\nfrom simulated mesh sequences using skin decomposition. At runtime, we\nseparately compute low- and high-frequency deformations in a sequential manner.\nThe low-frequency deformations are predicted by transferring body motions to\nvirtual bones' motions, and the high-frequency deformations are estimated\nleveraging the global information of virtual bones' motions and local\ninformation extracted from low-frequency meshes. In addition, our method can\nestimate garment deformations caused by variations of the simulation parameters\n(e.g., fabric's bending stiffness) using an RBF kernel ensembling trained\nnetworks for different sets of simulation parameters. Through extensive\ncomparisons, we show that our method outperforms state-of-the-art methods in\nterms of prediction accuracy of mesh deformations by about 20% in RMSE and 10%\nin Hausdorff distance and STED. The code and data are available at\nhttps://github.com/non-void/VirtualBones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_J/0/1/0/all/0/1\">Jiaming Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Dongxue Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1\">Tianjia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaogang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuroevolutionary Multi-objective approaches to Trajectory Prediction in Autonomous Vehicles. (arXiv:2205.02105v3 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2205.02105","description":"<p>The incentive for using Evolutionary Algorithms (EAs) for the automated\noptimization and training of deep neural networks (DNNs), a process referred to\nas neuroevolution, has gained momentum in recent years. The configuration and\ntraining of these networks can be posed as optimization problems. Indeed, most\nof the recent works on neuroevolution have focused their attention on\nsingle-objective optimization. Moreover, from the little research that has been\ndone at the intersection of neuroevolution and evolutionary multi-objective\noptimization (EMO), all the research that has been carried out has focused\npredominantly on the use of one type of DNN: convolutional neural networks\n(CNNs), using well-established standard benchmark problems such as MNIST. In\nthis work, we make a leap in the understanding of these two areas\n(neuroevolution and EMO), regarded in this work as neuroevolutionary\nmulti-objective, by using and studying a rich DNN composed of a CNN and\nLong-short Term Memory network. Moreover, we use a robust and challenging\nvehicle trajectory prediction problem. By using the well-known Non-dominated\nSorting Genetic Algorithm-II, we study the effects of five different\nobjectives, tested in categories of three, allowing us to show how these\nobjectives have either a positive or detrimental effect in neuroevolution for\ntrajectory prediction in autonomous vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stapleton_F/0/1/0/all/0/1\">Fergal Stapleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galvan_E/0/1/0/all/0/1\">Edgar Galv&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Octree Graph Networks for Learning Adaptive Volumetric Shape Representations. (arXiv:2205.02825v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02825","description":"<p>We present an adaptive deep representation of volumetric fields of 3D shapes\nand an efficient approach to learn this deep representation for high-quality 3D\nshape reconstruction and auto-encoding. Our method encodes the volumetric field\nof a 3D shape with an adaptive feature volume organized by an octree and\napplies a compact multilayer perceptron network for mapping the features to the\nfield value at each 3D position. An encoder-decoder network is designed to\nlearn the adaptive feature volume based on the graph convolutions over the dual\ngraph of octree nodes. The core of our network is a new graph convolution\noperator defined over a regular grid of features fused from irregular\nneighboring octree nodes at different levels, which not only reduces the\ncomputational and memory cost of the convolutions over irregular neighboring\noctree nodes, but also improves the performance of feature learning. Our method\neffectively encodes shape details, enables fast 3D shape reconstruction, and\nexhibits good generality for modeling 3D shapes out of training categories. We\nevaluate our method on a set of reconstruction tasks of 3D shapes and scenes\nand validate its superiority over other existing approaches. Our code, data,\nand trained models are available at https://wang-ps.github.io/dualocnn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng-Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}