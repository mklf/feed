{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-31T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Who is we? Disambiguating the referents of first person plural pronouns in parliamentary debates. (arXiv:2205.14182v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14182","description":"<p>This paper investigates the use of first person plural pronouns as a\nrhetorical device in political speeches. We present an annotation schema for\ndisambiguating pronoun references and use our schema to create an annotated\ncorpus of debates from the German Bundestag. We then use our corpus to learn to\nautomatically resolve pronoun referents in parliamentary debates. We explore\nthe use of data augmentation with weak supervision to further expand our corpus\nand report preliminary results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehbein_I/0/1/0/all/0/1\">Ines Rehbein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruppenhofer_J/0/1/0/all/0/1\">Josef Ruppenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1\">Julian Bernauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion-LM Improves Controllable Text Generation. (arXiv:2205.14217v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14217","description":"<p>Controlling the behavior of language models (LMs) without re-training is a\nmajor open problem in natural language generation. While recent works have\ndemonstrated successes on controlling simple sentence attributes (e.g.,\nsentiment), there has been little progress on complex, fine-grained controls\n(e.g., syntactic structure). To address this challenge, we develop a new\nnon-autoregressive language model based on continuous diffusions that we call\nDiffusion-LM. Building upon the recent successes of diffusion models in\ncontinuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian\nvectors into word vectors, yielding a sequence of intermediate latent\nvariables. The continuous, hierarchical nature of these intermediate variables\nenables a simple gradient-based algorithm to perform complex, controllable\ngeneration tasks. We demonstrate successful control of Diffusion-LM for six\nchallenging fine-grained control tasks, significantly outperforming prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lisa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulrajani_I/0/1/0/all/0/1\">Ishaan Gulrajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori B. Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Text Generation with Neurally-Decomposed Oracle. (arXiv:2205.14219v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14219","description":"<p>We propose a general and efficient framework to control auto-regressive\ngeneration models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained\nbase language model and a sequence-level boolean oracle function, we propose to\ndecompose the oracle function into token-level guidance to steer the base model\nin text generation. Specifically, the token-level guidance is approximated by a\nneural model trained with examples sampled from the base model, demanding no\nadditional auxiliary labeled data. We present the closed-form optimal solution\nto incorporate the token-level guidance into the base model for controllable\ngeneration. We further provide a theoretical analysis of how the approximation\nquality of NADO affects the controllable generation results. Experiments\nconducted on two applications: (1) text generation with lexical constraints and\n(2) machine translation with formality control demonstrate that our framework\nefficiently guides the base model towards the given oracle while maintaining\nhigh generation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1\">Tao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sidi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Light-Weight Answer Text Retrieval in Dialogue Systems. (arXiv:2205.14226v1 [cs.IR])","link":"http://arxiv.org/abs/2205.14226","description":"<p>Dialogue systems can benefit from being able to search through a corpus of\ntext to find information relevant to user requests, especially when\nencountering a request for which no manually curated response is available. The\nstate-of-the-art technology for neural dense retrieval or re-ranking involves\ndeep learning models with hundreds of millions of parameters. However, it is\ndifficult and expensive to get such models to operate at an industrial scale,\nespecially for cloud services that often need to support a big number of\nindividually customized dialogue systems, each with its own text corpus. We\nreport our work on enabling advanced neural dense retrieval systems to operate\neffectively at scale on relatively inexpensive hardware. We compare with\nleading alternative industrial solutions and show that we can provide a\nsolution that is effective, fast, and cost-efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Hui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Siva Sankalp Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murdock_J/0/1/0/all/0/1\">J. William Murdock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potdar_S/0/1/0/all/0/1\">Saloni Potdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Conditional Hidden Markov Model for Weakly Supervised Named Entity Recognition. (arXiv:2205.14228v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14228","description":"<p>Weakly supervised named entity recognition methods train label models to\naggregate the token annotations of multiple noisy labeling functions (LFs)\nwithout seeing any manually annotated labels. To work well, the label model\nneeds to contextually identify and emphasize well-performed LFs while\ndown-weighting the under-performers. However, evaluating the LFs is challenging\ndue to the lack of ground truths. To address this issue, we propose the sparse\nconditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire\nemission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating\nits diagonal elements, which are considered as the reliability scores of the\nLFs. The sparse scores are then expanded to the full-fledged emission matrix\nwith pre-defined expansion functions. We also augment the emission with\nweighted XOR scores, which track the probabilities of an LF observing incorrect\nentities. Sparse-CHMM is optimized through unsupervised learning with a\nthree-stage training pipeline that reduces the training difficulty and prevents\nthe model from falling into local optima. Compared with the baselines in the\nWrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on\nfive comprehensive datasets. Experiments show that each component of\nSparse-CHMM is effective, and the estimated LF reliabilities strongly correlate\nwith true LF F1 scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Against Stealthy Backdoor Attacks. (arXiv:2205.14246v1 [cs.CR])","link":"http://arxiv.org/abs/2205.14246","description":"<p>Defenses against security threats have been an interest of recent studies.\nRecent works have shown that it is not difficult to attack a natural language\nprocessing (NLP) model while defending against them is still a cat-mouse game.\nBackdoor attacks are one such attack where a neural network is made to perform\nin a certain way on specific samples containing some triggers while achieving\nnormal results on other samples. In this work, we present a few defense\nstrategies that can be useful to counter against such an attack. We show that\nour defense methodologies significantly decrease the performance on the\nattacked inputs while maintaining similar performance on benign inputs. We also\nshow that some of our defenses have very less runtime and also maintain\nsimilarity with the original inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_S/0/1/0/all/0/1\">Sangeet Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_A/0/1/0/all/0/1\">Abhinav Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidaralli_A/0/1/0/all/0/1\">Abhijith Srinivas Bidaralli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised models of audio effectively explain human cortical responses to speech. (arXiv:2205.14252v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14252","description":"<p>Self-supervised language models are very effective at predicting high-level\ncortical responses during language comprehension. However, the best current\nmodels of lower-level auditory processing in the human brain rely on either\nhand-constructed acoustic filters or representations from supervised audio\nneural networks. In this work, we capitalize on the progress of self-supervised\nspeech representation learning (SSL) to create new state-of-the-art models of\nthe human auditory system. Compared against acoustic baselines, phonemic\nfeatures, and supervised models, representations from the middle layers of\nself-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently\nyield the best prediction performance for fMRI recordings within the auditory\ncortex (AC). Brain areas involved in low-level auditory processing exhibit a\npreference for earlier SSL model layers, whereas higher-level semantic areas\nprefer later layers. We show that these trends are due to the models' ability\nto encode information at multiple linguistic levels (acoustic, phonetic, and\nlexical) along their representation depth. Overall, these results show that\nself-supervised models effectively capture the hierarchy of information\nrelevant to different stages of speech processing in human cortex.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1\">Aditya R. Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shailee Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander G. Huth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Subgoal Planning with Language Models. (arXiv:2205.14288v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14288","description":"<p>Pre-trained large language models have shown successful progress in many\nlanguage understanding benchmarks. This work explores the capability of these\nmodels to predict actionable plans in real-world environments. Given a text\ninstruction, we show that language priors encoded in pre-trained language\nmodels allow us to infer fine-grained subgoal sequences. In contrast to recent\nmethods which make strong assumptions about subgoal supervision, our\nexperiments show that language models can infer detailed subgoal sequences from\nfew training sequences without any fine-tuning. We further propose a simple\nstrategy to re-rank language model predictions based on interaction and\nfeedback from the environment. Combined with pre-trained navigation and visual\nreasoning components, our approach demonstrates competitive performance on\nsubgoal prediction and task completion in the ALFRED benchmark compared to\nprior methods that assume more subgoal supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximate Conditional Coverage via Neural Model Approximations. (arXiv:2205.14310v1 [cs.LG])","link":"http://arxiv.org/abs/2205.14310","description":"<p>Constructing reliable prediction sets is an obstacle for applications of\nneural models: Distribution-free conditional coverage is theoretically\nimpossible, and the exchangeability assumption underpinning the coverage\nguarantees of standard split-conformal approaches is violated on domain shifts.\nGiven these challenges, we propose and analyze a data-driven procedure for\nobtaining empirically reliable approximate conditional coverage, calculating\nunique quantile thresholds for each label for each test point. We achieve this\nvia the strong signals for prediction reliability from KNN-based model\napproximations over the training set and approximations over constrained\nsamples from the held-out calibration set. We demonstrate the potential for\nsubstantial (and otherwise unknowable) under-coverage with split-conformal\nalternatives with marginal coverage guarantees when not taking these distances\nand constraints into account with protein secondary structure prediction,\ngrammatical error detection, sentiment classification, and fact verification,\ncovering supervised sequence labeling, zero-shot sequence labeling (i.e.,\nfeature detection), document classification (with sparsity/interpretability\nconstraints), and retrieval-classification, including class-imbalanced and\ndomain-shifted settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmaltz_A/0/1/0/all/0/1\">Allen Schmaltz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasooly_D/0/1/0/all/0/1\">Danielle Rasooly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Activation Network For Low Resource Multilingual Speech Recognition. (arXiv:2205.14326v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14326","description":"<p>Low resource automatic speech recognition (ASR) is a useful but thorny task,\nsince deep learning ASR models usually need huge amounts of training data. The\nexisting models mostly established a bottleneck (BN) layer by pre-training on a\nlarge source language, and transferring to the low resource target language. In\nthis work, we introduced an adaptive activation network to the upper layers of\nASR model, and applied different activation functions to different languages.\nWe also proposed two approaches to train the model: (1) cross-lingual learning,\nreplacing the activation function from source language to target language, (2)\nmultilingual learning, jointly training the Connectionist Temporal\nClassification (CTC) loss of each language and the relevance of different\nlanguages. Our experiments on IARPA Babel datasets demonstrated that our\napproaches outperform the from-scratch training and traditional bottleneck\nfeature based methods. In addition, combining the cross-lingual learning and\nmultilingual learning together could further improve the performance of\nmultilingual speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhenpeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Augmentation Based Unsupervised Learning for Keyword Spotting. (arXiv:2205.14329v1 [cs.SD])","link":"http://arxiv.org/abs/2205.14329","description":"<p>In this paper, we investigated a speech augmentation based unsupervised\nlearning approach for keyword spotting (KWS) task. KWS is a useful speech\napplication, yet also heavily depends on the labeled data. We designed a\nCNN-Attention architecture to conduct the KWS task. CNN layers focus on the\nlocal acoustic features, and attention layers model the long-time dependency.\nTo improve the robustness of KWS model, we also proposed an unsupervised\nlearning method. The unsupervised loss is based on the similarity between the\noriginal and augmented speech features, as well as the audio reconstructing\ninformation. Two speech augmentation methods are explored in the unsupervised\nlearning: speed and intensity. The experiments on Google Speech Commands V2\nDataset demonstrated that our CNN-Attention model has competitive results.\nMoreover, the augmentation based unsupervised learning could further improve\nthe classification accuracy of KWS task. In our experiments, with augmentation\nbased unsupervised learning, our KWS model achieves better performance than\nother unsupervised methods, such as CPC, APC, and MPC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haobin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Reference Is Not Enough: Diverse Distillation with Reference Selection for Non-Autoregressive Translation. (arXiv:2205.14333v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14333","description":"<p>Non-autoregressive neural machine translation (NAT) suffers from the\nmulti-modality problem: the source sentence may have multiple correct\ntranslations, but the loss function is calculated only according to the\nreference sentence. Sequence-level knowledge distillation makes the target more\ndeterministic by replacing the target with the output from an autoregressive\nmodel. However, the multi-modality problem in the distilled dataset is still\nnonnegligible. Furthermore, learning from a specific teacher limits the upper\nbound of the model capability, restricting the potential of NAT models. In this\npaper, we argue that one reference is not enough and propose diverse\ndistillation with reference selection (DDRS) for NAT. Specifically, we first\npropose a method called SeedDiv for diverse machine translation, which enables\nus to generate a dataset containing multiple high-quality reference\ntranslations for each source sentence. During the training, we compare the NAT\noutput with all references and select the one that best fits the NAT output to\ntrain the model. Experiments on widely-used machine translation benchmarks\ndemonstrate the effectiveness of DDRS, which achieves 29.82 BLEU with only one\ndecoding pass on WMT14 En-De, improving the state-of-the-art performance for\nNAT by over 1 BLEU. Source code: https://github.com/ictnlp/DDRS-NAT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuanfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Models to Express Their Uncertainty in Words. (arXiv:2205.14334v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14334","description":"<p>We show that a GPT-3 model can learn to express uncertainty about its own\nanswers in natural language -- without use of model logits. When given a\nquestion, the model generates both an answer and a level of confidence (e.g.\n\"90% confidence\" or \"high confidence\"). These levels map to probabilities that\nare well calibrated. The model also remains moderately calibrated under\ndistribution shift, and is sensitive to uncertainty in its own answers, rather\nthan imitating human examples. To our knowledge, this is the first time a model\nhas been shown to express calibrated uncertainty about its own answers in\nnatural language. For testing calibration, we introduce the CalibratedMath\nsuite of tasks. We compare the calibration of uncertainty expressed in words\n(\"verbalized probability\") to uncertainty extracted from model logits. Both\nkinds of uncertainty are capable of generalizing calibration under distribution\nshift. We also provide evidence that GPT-3's ability to generalize calibration\ndepends on pre-trained latent representations that correlate with epistemic\nuncertainty over its answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephanie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction. (arXiv:2205.14393v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14393","description":"<p>Compared with traditional sentence-level relation extraction, document-level\nrelation extraction is a more challenging task where an entity in a document\nmay be mentioned multiple times and associated with multiple relations.\nHowever, most methods of document-level relation extraction do not distinguish\nbetween mention-level features and entity-level features, and just apply simple\npooling operation for aggregating mention-level features into entity-level\nfeatures. As a result, the distinct semantics between the different mentions of\nan entity are overlooked. To address this problem, we propose RSMAN in this\npaper which performs selective attentions over different entity mentions with\nrespect to candidate relations. In this manner, the flexible and\nrelation-specific representations of entities are obtained which indeed benefit\nrelation classification. Our extensive experiments upon two benchmark datasets\nshow that our RSMAN can bring significant improvements for some backbone models\nto achieve state-of-the-art performance, especially when an entity have\nmultiple mentions in the document.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiaxin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shuyu Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset. (arXiv:2205.14462v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14462","description":"<p>As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Faiyaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shifath_S/0/1/0/all/0/1\">S.M. Sadiq-Ur-Rahman Shifath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization. (arXiv:2205.14521v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14521","description":"<p>Text summarization aims to generate a short summary for an input text. In\nthis work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS)\napproach, which does not require parallel data for training. Our NAUS first\nperforms edit-based search towards a heuristically defined score, and generates\na summary as pseudo-groundtruth. Then, we train an encoder-only\nnon-autoregressive Transformer based on the search result. We also propose a\ndynamic programming approach for length-control decoding, which is important\nfor the summarization task. Experiments on two datasets show that NAUS achieves\nstate-of-the-art performance for unsupervised summarization, yet largely\nimproving inference efficiency. Further, our algorithm is able to perform\nexplicit length-transfer summary generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Puyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Character-Level Length-Control Algorithm for Non-Autoregressive Sentence Summarization. (arXiv:2205.14522v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14522","description":"<p>Sentence summarization aims at compressing a long sentence into a short one\nthat keeps the main gist, and has extensive real-world applications such as\nheadline generation. In previous work, researchers have developed various\napproaches to improve the ROUGE score, which is the main evaluation metric for\nsummarization, whereas controlling the summary length has not drawn much\nattention. In our work, we address a new problem of explicit character-level\nlength control for summarization, and propose a dynamic programming algorithm\nbased on the Connectionist Temporal Classification (CTC) model. Results show\nthat our approach not only achieves higher ROUGE scores but also yields more\ncomplete sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Puyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoDisc: Automatic Distillation Schedule for Large Language Model Compression. (arXiv:2205.14570v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14570","description":"<p>Driven by the teacher-student paradigm, knowledge distillation is one of the\nde facto ways for language model compression. Recent studies have uncovered\nthat conventional distillation is less effective when facing a large capacity\ngap between the teacher and the student, and introduced teacher assistant-based\ndistillation to bridge the gap. As a connection, the scale and the performance\nof the teacher assistant is crucial for transferring the knowledge from the\nteacher to the student. However, existing teacher assistant-based methods\nmanually select the scale of the teacher assistant, which fails to identify the\nteacher assistant with the optimal scale-performance tradeoff. To this end, we\npropose an Automatic Distillation Schedule (AutoDisc) for large language model\ncompression. In particular, AutoDisc first specifies a set of teacher assistant\ncandidates at different scales with gridding and pruning, and then optimizes\nall candidates in an once-for-all optimization with two approximations. The\nbest teacher assistant scale is automatically selected according to the\nscale-performance tradeoff. AutoDisc is evaluated with an extensive set of\nexperiments on a language understanding benchmark GLUE. Experimental results\ndemonstrate the improved performance and applicability of our AutoDisc. We\nfurther apply AutoDisc on a language model with over one billion parameters and\nshow the scalability of AutoDisc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiahao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Locality and Isotropy in Dialogue Modeling. (arXiv:2205.14583v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14583","description":"<p>Existing dialogue modeling methods have achieved promising performance on\nvarious dialogue tasks with the aid of Transformer and the large-scale\npre-trained language models. However, some recent studies revealed that the\ncontext representations produced by these methods suffer the problem of\nanisotropy. In this paper, we find that the generated representations are also\nnot conversational, losing the conversation structure information during the\ncontext modeling stage. To this end, we identify two properties in dialogue\nmodeling, i.e., locality and isotropy, and present a simple method for dialogue\nrepresentation calibration, namely SimDRC, to build isotropic and\nconversational feature spaces. Experimental results show that our approach\nsignificantly outperforms the current state-of-the-art models on three dialogue\ntasks across the automatic and human evaluation metrics. More in-depth analyses\nfurther confirm the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haochen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1\">Mingjie Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shaoqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor Prediction: A Topic Modeling Approach. (arXiv:2205.14631v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14631","description":"<p>Networks of documents connected by hyperlinks, such as Wikipedia, are\nubiquitous. Hyperlinks are inserted by the authors to enrich the text and\nfacilitate the navigation through the network. However, authors tend to insert\nonly a fraction of the relevant hyperlinks, mainly because this is a time\nconsuming task. In this paper we address an annotation, which we refer to as\nanchor prediction. Even though it is conceptually close to link prediction or\nentity linking, it is a different task that require developing a specific\nmethod to solve it. Given a source document and a target document, this task\nconsists in automatically identifying anchors in the source document, i.e words\nor terms that should carry a hyperlink pointing towards the target document. We\npropose a contextualized relational topic model, CRTM, that models directed\nlinks between documents as a function of the local context of the anchor in the\nsource document and the whole content of the target document. The model can be\nused to predict anchors in a source document, given the target document,\nwithout relying on a dictionary of previously seen mention or title, nor any\nexternal knowledge graph. Authors can benefit from CRTM, by letting it\nautomatically suggest hyperlinks, given a new document and the set of target\ndocument to connect to. It can also benefit to readers, by dynamically\ninserting hyperlinks between the documents they're reading. Experiments\nconducted on several Wikipedia corpora (in English, Italian and German)\nhighlight the practical usefulness of anchor prediction and demonstrate the\nrelevancy of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_J/0/1/0/all/0/1\">Jean Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guille_A/0/1/0/all/0/1\">Adrien Guille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacques_J/0/1/0/all/0/1\">Julien Jacques</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SFE-AI at SemEval-2022 Task 11: Low-Resource Named Entity Recognition using Large Pre-trained Language Models. (arXiv:2205.14660v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14660","description":"<p>Large scale pre-training models have been widely used in named entity\nrecognition (NER) tasks. However, model ensemble through parameter averaging or\nvoting can not give full play to the differentiation advantages of different\nmodels, especially in the open domain. This paper describes our NER system in\nthe SemEval 2022 task11: MultiCoNER. We proposed an effective system to\nadaptively ensemble pre-trained language models by a Transformer layer. By\nassigning different weights to each model for different inputs, we adopted the\nTransformer layer to integrate the advantages of diverse models effectively.\nExperimental results show that our method achieves superior performances in\nFarsi and Dutch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Changyu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yixuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qizhi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaopeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiandi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1\">Qifeng Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoNT: Contrastive Neural Text Generation. (arXiv:2205.14690v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14690","description":"<p>Recently, contrastive learning attracts increasing interests in neural text\ngeneration as a new solution to alleviate the exposure bias problem. It\nintroduces a sequence-level training signal which is crucial to generation\ntasks that always rely on auto-regressive decoding. However, previous methods\nusing contrastive learning in neural text generation usually lead to inferior\nperformance. In this paper, we analyse the underlying reasons and propose a new\nContrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks\nthat prevent contrastive learning from being widely adopted in generation tasks\nfrom three aspects -- the construction of contrastive examples, the choice of\nthe contrastive loss, and the strategy in decoding. We validate CoNT on five\ngeneration tasks with ten benchmarks, including machine translation,\nsummarization, code comment generation, data-to-text generation and commonsense\ngeneration. Experimental results show that CoNT clearly outperforms the\nconventional training framework on all the ten benchmarks with a convincing\nmargin. Especially, CoNT surpasses previous the most competitive contrastive\nlearning method for text generation, by 1.50 BLEU on machine translation and\n1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art\non summarization, code comment generation (without external data) and\ndata-to-text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1\">Kai Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VD-PCR: Improving Visual Dialog with Pronoun Coreference Resolution. (arXiv:2205.14693v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14693","description":"<p>The visual dialog task requires an AI agent to interact with humans in\nmulti-round dialogs based on a visual environment. As a common linguistic\nphenomenon, pronouns are often used in dialogs to improve the communication\nefficiency. As a result, resolving pronouns (i.e., grounding pronouns to the\nnoun phrases they refer to) is an essential step towards understanding dialogs.\nIn this paper, we propose VD-PCR, a novel framework to improve Visual Dialog\nunderstanding with Pronoun Coreference Resolution in both implicit and explicit\nways. First, to implicitly help models understand pronouns, we design novel\nmethods to perform the joint training of the pronoun coreference resolution and\nvisual dialog tasks. Second, after observing that the coreference relationship\nof pronouns and their referents indicates the relevance between dialog rounds,\nwe propose to explicitly prune the irrelevant history rounds in visual dialog\nmodels' input. With pruned input, the models can focus on relevant dialog\nhistory and ignore the distraction in the irrelevant one. With the proposed\nimplicit and explicit methods, VD-PCR achieves state-of-the-art experimental\nresults on the VisDial dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xintong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Ruixin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14704","description":"<p>Prompt learning approaches have made waves in natural language processing by\ninducing better few-shot performance while they still follow a parametric-based\nlearning paradigm; the oblivion and rote memorization problems in learning may\nencounter unstable generalization issues. Specifically, vanilla prompt learning\nmay struggle to utilize atypical instances by rote during fully-supervised\ntraining or overfit shallow patterns with low-shot data. To alleviate such\nlimitations, we develop RetroPrompt with the motivation of decoupling knowledge\nfrom memorization to help the model strike a balance between generalization and\nmemorization. In contrast with vanilla prompt learning, RetroPrompt constructs\nan open-book knowledge-store from training instances and implements a retrieval\nmechanism during the process of input, training and inference, thus equipping\nthe model with the ability to retrieve related contexts from the training\ncorpus as cues for enhancement. Extensive experiments demonstrate that\nRetroPrompt can obtain better performance in both few-shot and zero-shot\nsettings. Besides, we further illustrate that our proposed RetroPrompt can\nyield better generalization abilities with new datasets. Detailed analysis of\nmemorization indeed reveals RetroPrompt can reduce the reliance of language\nmodels on memorization; thus, improving generalization for downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What are People Talking about in #BackLivesMatter and #StopAsianHate? Exploring and Categorizing Twitter Topics Emerging in Online Social Movements through the Latent Dirichlet Allocation Model. (arXiv:2205.14725v1 [cs.IR])","link":"http://arxiv.org/abs/2205.14725","description":"<p>Minority groups have been using social media to organize social movements\nthat create profound social impacts. Black Lives Matter (BLM) and Stop Asian\nHate (SAH) are two successful social movements that have spread on Twitter that\npromote protests and activities against racism and increase the public's\nawareness of other social challenges that minority groups face. However,\nprevious studies have mostly conducted qualitative analyses of tweets or\ninterviews with users, which may not comprehensively and validly represent all\ntweets. Very few studies have explored the Twitter topics within BLM and SAH\ndialogs in a rigorous, quantified and data-centered approach. Therefore, in\nthis research, we adopted a mixed-methods approach to comprehensively analyze\nBLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation\nmodel to understand the top high-level words and topics and (2) open-coding\nanalysis to identify specific themes across the tweets. We collected more than\none million tweets with the #blacklivesmatter and #stopasianhate hashtags and\ncompared their topics. Our findings revealed that the tweets discussed a\nvariety of influential topics in depth, and social justice, social movements,\nand emotional sentiments were common topics in both movements, though with\nunique subtopics for each movement. Our study contributes to the topic analysis\nof social movements on social media platforms in particular and the literature\non the interplay of AI, ethics, and society in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiayi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bei_R/0/1/0/all/0/1\">Rongqi Bei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luyao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI. (arXiv:2205.14727v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14727","description":"<p>Human language expression is based on the subjective construal of the\nsituation instead of the objective truth conditions, which means that speakers'\npersonalities and emotions after cognitive processing have an important\ninfluence on conversation. However, most existing datasets for conversational\nAI ignore human personalities and emotions, or only consider part of them. It's\ndifficult for dialogue systems to understand speakers' personalities and\nemotions although large-scale pre-training language models have been widely\nused. In order to consider both personalities and emotions in the process of\nconversation generation, we propose CPED, a large-scale Chinese personalized\nand emotional dialogue dataset, which consists of multi-source knowledge\nrelated to empathy and personal characteristic. These knowledge covers gender,\nBig Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPED\ncontains more than 12K dialogues of 392 speakers from 40 TV shows. We release\nthe textual dataset with audio features and video features according to the\ncopyright claims, privacy issues, terms of service of video platforms. We\nprovide detailed description of the CPED construction process and introduce\nthree tasks for conversational AI, including personality recognition, emotion\nrecognition in conversations as well as personalized and emotional conversation\ngeneration. Finally, we provide baseline systems for these tasks and consider\nthe function of speakers' personalities and emotions on conversation. Our\nmotivation is to propose a dataset to be widely adopted by the NLP community as\na new open benchmark for conversational AI research. The full dataset is\navailable at https://github.com/scutcyr/CPED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yirong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Weiquan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xiaofen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jianxin Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tie_Q/0/1/0/all/0/1\">Qianfeng Tie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangmin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaNLP: Marathi Natural Language Processing Datasets, Models, and Library. (arXiv:2205.14728v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14728","description":"<p>Despite being the third most popular language in India, the Marathi language\nlacks useful NLP resources. Moreover, popular NLP libraries do not have support\nfor the Marathi language. With L3Cube-MahaNLP, we aim to build resources and a\nlibrary for Marathi natural language processing. We present datasets and\ntransformer models for supervised tasks like sentiment analysis, named entity\nrecognition, and hate speech detection. We have also published a monolingual\nMarathi corpus for unsupervised language modeling tasks. Overall we present\nMahaCorpus, MahaSent, MahaNER, and MahaHate datasets and their corresponding\nMahaBERT models fine-tuned on these datasets. We aim to move ahead of benchmark\ndatasets and prepare useful resources for Marathi. The resources are available\nat https://github.com/l3cube-pune/MarathiNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition. (arXiv:2205.14748v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14748","description":"<p>We propose novel AI-empowered chat bots for learning as conversation where a\nuser does not read a passage but gains information and knowledge through\nconversation with a teacher bot. Our information-acquisition-oriented dialogue\nsystem employs a novel adaptation of reinforced self-play so that the system\ncan be transferred to various domains without in-domain dialogue data, and can\ncarry out conversations both informative and attentive to users. Our extensive\nsubjective and objective evaluations on three large public data corpora\ndemonstrate the effectiveness of our system to deliver knowledge-intensive and\nattentive conversations and help end users substantially gain knowledge without\nreading passages. Our code and datasets are publicly available for follow-up\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Pengshan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Hui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PKUSEG: A Toolkit for Multi-Domain Chinese Word Segmentation. (arXiv:1906.11455v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.11455","description":"<p>Chinese word segmentation (CWS) is a fundamental step of Chinese natural\nlanguage processing. In this paper, we build a new toolkit, named PKUSEG, for\nmulti-domain word segmentation. Unlike existing single-model toolkits, PKUSEG\ntargets multi-domain word segmentation and provides separate models for\ndifferent domains, such as web, medicine, and tourism. Besides, due to the lack\nof labeled data in many domains, we propose a domain adaptation paradigm to\nintroduce cross-domain semantic knowledge via a translation system. Through\nthis method, we generate synthetic data using a large amount of unlabeled data\nin the target domain and then obtain a word segmentation model for the target\ndomain. We also further refine the performance of the default model with the\nhelp of synthetic data. Experiments show that PKUSEG achieves high performance\non multiple domains. The new toolkit also supports POS tagging and model\ntraining to adapt to various application scenarios. The toolkit is now freely\nand publicly available for the usage of research and industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ruixuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InsNet: An Efficient, Flexible, and Performant Insertion-based Text Generation Model. (arXiv:2102.11008v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.11008","description":"<p>We propose InsNet, an expressive insertion-based text generator with\nefficient training and flexible decoding (parallel or sequential). Unlike most\nexisting insertion-based text generation works that require re-encoding of the\ncontext after each insertion operation and thus are inefficient to train,\nInsNet only requires one pass of context encoding for the entire insertion\nsequence during training by introducing a novel insertion-oriented position\nencoding to enable computation sharing. Experiments on two unsupervised\nlexically constrained text generation datasets and three machine translation\ndatasets demonstrate InsNet's advantages over previous insertion-based methods\nin terms of training speed, inference efficiency, and generation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sidi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1\">Tao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling the dynamics of language change: logistic regression, Piotrowski's law, and a handful of examples in Polish. (arXiv:2104.06324v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06324","description":"<p>The study discusses modeling diachronic processes by logistic regression. The\nphenomenon of nonlinear changes in language was first observed by Raimund\nPiotrowski (hence labelled as Piotrowski's law), even if actual linguistic\nevidence usually speaks against using the notion of a \"law\" in this context. In\nour study, we apply logistic regression models to 9 changes which occurred\nbetween 15th and 18th century in the Polish language. The attested course of\nthe majority of these changes closely follow the expected values, which proves\nthat the language change might indeed resemble a nonlinear phase change\nscenario. We also extend the original Piotrowski's approach by proposing\npolynomial logistic regression for these cases which can hardly be described by\nits standard version. Also, we propose to consider individual language change\ncases jointly, in order to inspect their possible collinearity or, more likely,\ntheir different dynamics in the function of time. Last but not least, we\nevaluate our results by testing the influence of the subcorpus size on the\nmodel's goodness-of-fit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorski_R/0/1/0/all/0/1\">Rafa&#x142; L. G&#xf3;rski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eder_M/0/1/0/all/0/1\">Maciej Eder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Answers with Entailment Trees. (arXiv:2104.08661v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08661","description":"<p>Our goal, in the context of open-domain textual question-answering (QA), is\nto explain answers by showing the line of reasoning from what is known to the\nanswer, rather than simply showing a fragment of textual evidence (a\n\"rationale'\"). If this could be done, new opportunities for understanding and\ndebugging the system's reasoning become possible. Our approach is to generate\nexplanations in the form of entailment trees, namely a tree of multipremise\nentailment steps from facts that are known, through intermediate conclusions,\nto the hypothesis of interest (namely the question + answer). To train a model\nwith this skill, we created ENTAILMENTBANK, the first dataset to contain\nmultistep entailment trees. Given a hypothesis (question + answer), we define\nthree increasingly difficult explanation tasks: generate a valid entailment\ntree given (a) all relevant sentences (b) all relevant and some irrelevant\nsentences, or (c) a corpus. We show that a strong language model can partially\nsolve these tasks, in particular when the relevant sentences are included in\nthe input (e.g., 35% of trees for (a) are perfect), and with indications of\ngeneralization to other domains. This work is significant as it provides a new\ntype of dataset (multistep entailments) and baselines, offering a new avenue\nfor the community to generate richer, more systematic explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_B/0/1/0/all/0/1\">Bhavana Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhengnan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_H/0/1/0/all/0/1\">Hannah Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pipatanangkura_L/0/1/0/all/0/1\">Leighanna Pipatanangkura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing and Mitigating Interference in Neural Architecture Search. (arXiv:2108.12821v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12821","description":"<p>Weight sharing is a popular approach to reduce the cost of neural\narchitecture search (NAS) by reusing the weights of shared operators from\npreviously trained child models. However, the rank correlation between the\nestimated accuracy and ground truth accuracy of those child models is low due\nto the interference among different child models caused by weight sharing. In\nthis paper, we investigate the interference issue by sampling different child\nmodels and calculating the gradient similarity of shared operators, and\nobserve: 1) the interference on a shared operator between two child models is\npositively correlated with the number of different operators; 2) the\ninterference is smaller when the inputs and outputs of the shared operator are\nmore similar. Inspired by these two observations, we propose two approaches to\nmitigate the interference: 1) MAGIC-T: rather than randomly sampling child\nmodels for optimization, we propose a gradual modification scheme by modifying\none operator between adjacent optimization steps to minimize the interference\non the shared operators; 2) MAGIC-A: forcing the inputs and outputs of the\noperator across all child models to be similar to reduce the interference.\nExperiments on a BERT search space verify that mitigating interference via each\nof our proposed methods improves the rank correlation of super-pet and\ncombining both methods can achieve better results. Our discovered architecture\noutperforms RoBERTa$_{\\rm base}$ by 1.1 and 0.6 points and ELECTRA$_{\\rm base}$\nby 1.6 and 1.1 points on the dev and test set of GLUE benchmark. Extensive\nresults on the BERT compression, reading comprehension and ImageNet task\ndemonstrate the effectiveness and generality of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paradigm Shift in Natural Language Processing. (arXiv:2109.12575v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12575","description":"<p>In the era of deep learning, modeling for most NLP tasks has converged to\nseveral mainstream paradigms. For example, we usually adopt the sequence\nlabeling paradigm to solve a bundle of tasks such as POS-tagging, NER,\nChunking, and adopt the classification paradigm to solve tasks like sentiment\nanalysis. With the rapid progress of pre-trained language models, recent years\nhave observed a rising trend of Paradigm Shift, which is solving one NLP task\nby reformulating it as another one. Paradigm shift has achieved great success\non many tasks, becoming a promising way to improve model performance. Moreover,\nsome of these paradigms have shown great potential to unify a large number of\nNLP tasks, making it possible to build a single model to handle diverse tasks.\nIn this paper, we review such phenomenon of paradigm shifts in recent years,\nhighlighting several paradigms that have the potential to solve different NLP\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00269","description":"<p>Pre-trained models learn informative representations on large-scale training\ndata through a self-supervised or supervised learning method, which has\nachieved promising performance in natural language processing (NLP), computer\nvision (CV), and cross-modal fields after fine-tuning. These models, however,\nsuffer from poor robustness and lack of interpretability. Pre-trained models\nwith knowledge injection, which we call knowledge enhanced pre-trained models\n(KEPTMs), possess deep understanding and logical reasoning and introduce\ninterpretability. In this survey, we provide a comprehensive overview of KEPTMs\nin NLP and CV. We first introduce the progress of pre-trained models and\nknowledge representation learning. Then we systematically categorize existing\nKEPTMs from three different perspectives. Finally, we outline some potential\ndirections of KEPTMs for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Gang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinghui Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v12 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devising aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Semantic Parsing with Language Models Trained On Code. (arXiv:2112.08696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08696","description":"<p>Large language models can perform semantic parsing with little training data,\nwhen prompted with in-context examples. It has been shown that this can be\nimproved by formulating the problem as paraphrasing into canonical utterances,\nwhich casts the underlying meaning representation into a controlled natural\nlanguage-like representation. Intuitively, such models can more easily output\ncanonical utterances as they are closer to the natural language used for\npre-training. Recently, models also pre-trained on code, like OpenAI Codex,\nhave risen in prominence. For semantic parsing tasks where we map natural\nlanguage into code, such models may prove more adept at it. In this paper, we\ntest this hypothesis and find that Codex performs better on such tasks than\nequivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that\nunlike GPT-3, Codex performs similarly when targeting meaning representations\ndirectly, perhaps because meaning representations are structured similar to\ncode in these datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1\">Richard Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Dense Information Retrieval with Contrastive Learning. (arXiv:2112.09118v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2112.09118","description":"<p>Recently, information retrieval has seen the emergence of dense retrievers,\nbased on neural networks, as an alternative to classical sparse methods based\non term-frequency. These models have obtained state-of-the-art results on\ndatasets and tasks where large training sets are available. However, they do\nnot transfer well to new applications with no training data, and are\noutperformed by unsupervised term-frequency methods such as BM25. In this work,\nwe explore the limits of contrastive learning as a way to train unsupervised\ndense retrievers and show that it leads to strong performance in various\nretrieval settings. On the BEIR benchmark our unsupervised model outperforms\nBM25 on 11 out of 15 datasets for the Recall@100 metric. When used as\npre-training before fine-tuning, either on a few thousands in-domain examples\nor on the large MS MARCO dataset, our contrastive model leads to improvements\non the BEIR benchmark. Finally, we evaluate our approach for multi-lingual\nretrieval, where training data is even scarcer than for English, and show that\nour approach leads to strong unsupervised performance. Our model also exhibits\nstrong cross-lingual transfer when fine-tuned on supervised English data only\nand evaluated on low resources language such as Swahili. We show that our\nunsupervised models can perform cross-lingual retrieval between different\nscripts, such as retrieving English documents from Arabic queries, which would\nnot be possible with term matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izacard_G/0/1/0/all/0/1\">Gautier Izacard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_L/0/1/0/all/0/1\">Lucas Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1\">Edouard Grave</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v5 [q-bio.BM] UPDATED)","link":"http://arxiv.org/abs/2201.11147","description":"<p>Self-supervised protein language models have proved their effectiveness in\nlearning the proteins representations. With the increasing computational power,\ncurrent protein language models pre-trained with millions of diverse sequences\ncan advance the parameter scale from million-level to billion-level and achieve\nremarkable improvement. However, those prevailing approaches rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured\nknowledge facts for better protein representations. We argue that informative\nbiology knowledge in KGs can enhance protein representation with external\nknowledge. In this work, we propose OntoProtein, the first general framework\nthat makes use of structure in GO (Gene Ontology) into protein pre-training\nmodels. We construct a novel large-scale knowledge graph that consists of GO\nand its related proteins, and gene annotation texts or protein sequences\ndescribe all nodes in the graph. We propose novel contrastive learning with\nknowledge-aware negative sampling to jointly optimize the knowledge graph and\nprotein embedding during pre-training. Experimental results show that\nOntoProtein can surpass state-of-the-art methods with pre-trained protein\nlanguage models in TAPE benchmark and yield better performance compared with\nbaselines in protein-protein interaction and protein function prediction. Code\nand datasets are available in https://github.com/zjunlp/OntoProtein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hong_H/0/1/0/all/0/1\">Haosen Hong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lian_J/0/1/0/all/0/1\">Jiazhang Lian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Contrastive Framework for Neural Text Generation. (arXiv:2202.06417v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06417","description":"<p>Text generation is of great importance to many natural language processing\napplications. However, maximization-based decoding methods (e.g. beam search)\nof neural language models often lead to degenerate solutions -- the generated\ntext is unnatural and contains undesirable repetitions. Existing approaches\nintroduce stochasticity via sampling or modify training objectives to decrease\nprobabilities of certain tokens (e.g., unlikelihood training). However, they\noften lead to solutions that lack coherence. In this work, we show that an\nunderlying reason for model degeneration is the anisotropic distribution of\ntoken representations. We present a contrastive solution: (i) SimCTG, a\ncontrastive training objective to calibrate the model's representation space,\nand (ii) a decoding method -- contrastive search -- to encourage diversity\nwhile maintaining coherence in the generated text. Extensive experiments and\nanalyses on three benchmarks from two languages demonstrate that our proposed\napproach outperforms state-of-the-art text generation methods as evaluated by\nboth human and automatic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\rm{C {\\small IS}}^2$: A Simplified Commonsense Inference Evaluation for Story Prose. (arXiv:2202.07880v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07880","description":"<p>Transformers have been showing near-human performance on a variety of tasks,\nbut they are not without their limitations. We discuss the issue of conflating\nresults of transformers that are instructed to do multiple tasks\nsimultaneously. In particular, we focus on the domain of commonsense reasoning\nwithin story prose, which we call contextual commonsense inference (CCI). We\nlook at the GLUCOSE (Mostafazadeh et al. 2020) dataset and task for predicting\nimplicit commonsense inferences between story sentences. Since the GLUCOSE task\nsimultaneously generates sentences and predicts the CCI relation, there is a\nconflation in the results. Is the model really measuring CCI or is its ability\nto generate grammatical text carrying the results? In this paper, we introduce\nthe task contextual commonsense inference in sentence selection ($\\rm{C {\\small\nIS}}^2$), a simplified task that avoids conflation by eliminating language\ngeneration altogether. Our findings emphasize the necessity of future work to\ndisentangle language generation from the desired NLP tasks at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition. (arXiv:2203.06925v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06925","description":"<p>Named Entity Recognition task is one of the core tasks of information\nextraction.Word ambiguity and word abbreviation are important reasons for the\nlow recognition rate of named entities. In this paper, we propose a novel named\nentity recognition model WCL-BBCD (Word Contrastive Learning with\nBERT-BiLSTM-CRF-DBpedia) incorporating the idea of contrastive learning. The\nmodel first trains the sentence pairs in the text, calculate similarity between\nwords in sentence pairs by cosine similarity, and fine-tunes the BERT model\nused for the named entity recognition task through the similarity, so as to\nalleviate word ambiguity. Then, the fine-tuned BERT model is combined with the\nBiLSTM-CRF model to perform the named entity recognition task. Finally, the\nrecognition results are corrected in combination with prior knowledge such as\nknowledge graphs, so as to alleviate the recognition caused by word\nabbreviations low-rate problem. Experimental results show that our model\noutperforms other similar model methods on the CoNLL-2003 English dataset and\nOntoNotes V5 English dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Renjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jian Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tianxiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianjun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Semantic Aware Pre-training for Few-shot Text Classification. (arXiv:2204.07128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07128","description":"<p>In text classification tasks, useful information is encoded in the label\nnames. Label semantic aware systems have leveraged this information for\nimproved text classification performance during fine-tuning and prediction.\nHowever, use of label-semantics during pre-training has not been extensively\nexplored. We therefore propose Label Semantic Aware Pre-training (LSAP) to\nimprove the generalization and data efficiency of text classification systems.\nLSAP incorporates label semantics into pre-trained generative models (T5 in our\ncase) by performing secondary pre-training on labeled sentences from a variety\nof domains. As domain-general pre-training requires large amounts of data, we\ndevelop a filtering and labeling pipeline to automatically create\nsentence-label pairs from unlabeled text. We perform experiments on intent\n(ATIS, Snips, TOPv2) and topic classification (AG News, Yahoo! Answers). LSAP\nobtains significant accuracy improvements over state-of-the-art models for\nfew-shot text classification while maintaining performance comparable to state\nof the art in high-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krone_J/0/1/0/all/0/1\">Jason Krone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1\">Salvatore Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding. (arXiv:2204.10050v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10050","description":"<p>This paper presents the shared task on Multilingual Idiomaticity Detection\nand Sentence Embedding, which consists of two subtasks: (a) a binary\nclassification task aimed at identifying whether a sentence contains an\nidiomatic expression, and (b) a task based on semantic text similarity which\nrequires the model to adequately represent potentially idiomatic expressions in\ncontext. Each subtask includes different settings regarding the amount of\ntraining data. Besides the task description, this paper introduces the datasets\nin English, Portuguese, and Galician and their annotation procedure, the\nevaluation metrics, and a summary of the participant systems and their results.\nThe task had close to 100 registered participants organised into twenty five\nteams making over 650 and 150 submissions in the practice and evaluation phases\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gow_Smith_E/0/1/0/all/0/1\">Edward Gow-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1\">Marcos Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idiart_M/0/1/0/all/0/1\">Marco Idiart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1\">Aline Villavicencio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training. (arXiv:2204.11218v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11218","description":"<p>Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained\nlanguage models (PLMs) like BERT contain matching subnetworks that have similar\ntransfer learning performance as the original PLM. These subnetworks are found\nusing magnitude-based pruning. In this paper, we find that the BERT subnetworks\nhave even more potential than these studies have shown. Firstly, we discover\nthat the success of magnitude pruning can be attributed to the preserved\npre-training performance, which correlates with the downstream transferability.\nInspired by this, we propose to directly optimize the subnetwork structure\ntowards the pre-training objectives, which can better preserve the pre-training\nperformance. Specifically, we train binary masks over model weights on the\npre-training tasks, with the aim of preserving the universal transferability of\nthe subnetwork, which is agnostic to any specific downstream tasks. We then\nfine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The\nresults show that, compared with magnitude pruning, mask training can\neffectively find BERT subnetworks with improved overall performance on\ndownstream tasks. Moreover, our method is also more efficient in searching\nsubnetworks and more advantageous when fine-tuning within a certain range of\ndata scarcity. Our code is available at https://github.com/llyx97/TAMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1\">Peng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02357","description":"<p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual\nknowledge, have recently been successfully applied to tasks such as information\nretrieval, question answering, and recommendation system. Since most MKGs are\nfar from complete, extensive knowledge graph completion studies have been\nproposed focusing on the multimodal entity, relation extraction and link\nprediction. However, different tasks and modalities require changes to the\nmodel architecture, and not all images/objects are relevant to text input,\nwhich hinders the applicability to diverse real-world scenarios. In this paper,\nwe propose a hybrid transformer with multi-level fusion to address those\nissues. Specifically, we leverage a hybrid transformer architecture with\nunified input-output for diverse multimodal knowledge graph completion tasks.\nMoreover, we propose multi-level fusion, which integrates visual and text\nrepresentation via coarse-grained prefix-guided interaction and fine-grained\ncorrelation-aware fusion modules. We conduct extensive experiments to validate\nthat our MKGformer can obtain SOTA performance on four datasets of multimodal\nlink prediction, multimodal RE, and multimodal NER. Code is available in\nhttps://github.com/zjunlp/MKGformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting and Understanding Harmful Memes: A Survey. (arXiv:2205.04274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.04274","description":"<p>The automatic identification of harmful content online is of major concern\nfor social media platforms, policymakers, and society. Researchers have studied\ntextual, visual, and audio content, but typically in isolation. Yet, harmful\ncontent often combines multiple modalities, as in the case of memes, which are\nof particular interest due to their viral nature. With this in mind, here we\noffer a comprehensive survey with a focus on harmful memes. Based on a\nsystematic analysis of recent literature, we first propose a new typology of\nharmful memes, and then we highlight and summarize the relevant state of the\nart. One interesting finding is that many types of harmful memes are not really\nstudied, e.g., such featuring self-harm and extremism, partly due to the lack\nof suitable datasets. We further find that existing datasets mostly capture\nmulti-class scenarios, which are not inclusive of the affective spectrum that\nmemes can represent. Another observation is that memes can propagate globally\nthrough repackaging in different languages and that they can also be\nmultilingual, blending different cultures. We conclude by highlighting several\nchallenges related to multimodal semiotics, technological constraints, and\nnon-trivial social engagement, and we present several open-ended aspects such\nas delineating online harm and empirically examining related frameworks and\nassistive interventions, which we believe will motivate and drive future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Extra-Textual Attributes about Dialogue Participants -- A Case Study of English-to-Polish Neural Machine Translation. (arXiv:2205.04747v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.04747","description":"<p>Unlike English, morphologically rich languages can reveal characteristics of\nspeakers or their conversational partners, such as gender and number, via\npronouns, morphological endings of words and syntax. When translating from\nEnglish to such languages, a machine translation model needs to opt for a\ncertain interpretation of textual context, which may lead to serious\ntranslation errors if extra-textual information is unavailable. We investigate\nthis challenge in the English-to-Polish language direction. We focus on the\nunderresearched problem of utilising external metadata in automatic translation\nof TV dialogue, proposing a case study where a wide range of approaches for\ncontrolling attributes in translation is employed in a multi-attribute\nscenario. The best model achieves an improvement of +5.81 chrF++/+6.03 BLEU,\nwith other models achieving competitive performance. We additionally contribute\na novel attribute-annotated dataset of Polish TV dialogue and a morphological\nanalysis script used to evaluate attribute control in models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vincent_S/0/1/0/all/0/1\">Sebastian T. Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Distributional Properties Drive Emergent In-Context Learning in Transformers. (arXiv:2205.05055v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.05055","description":"<p>Large transformer-based models are able to perform in-context few-shot\nlearning, without being explicitly trained for it. This observation raises the\nquestion: what aspects of the training regime lead to this emergent behavior?\nHere, we show that this behavior is driven by the distributions of the training\ndata itself. In-context learning emerges when the training data exhibits\nparticular distributional properties such as burstiness (items appear in\nclusters rather than being uniformly distributed over time) and having large\nnumbers of rarely occurring classes. In-context learning also emerges more\nstrongly when item meanings or interpretations are dynamic rather than fixed.\nThese properties are exemplified by natural language, but are also inherent to\nnaturalistic data in a wide range of other domains. They also depart\nsignificantly from the uniform, i.i.d. training distributions typically used\nfor standard supervised learning. In our initial experiments, we found that\nin-context learning traded off against more conventional weight-based learning,\nand models were unable to achieve both simultaneously. However, our later\nexperiments uncovered that the two modes of learning could co-exist in a single\nmodel when it was trained on data following a skewed Zipfian distribution --\nanother common property of naturalistic data, including language. In further\nexperiments, we found that naturalistic data distributions were only able to\nelicit in-context learning in transformers, and not in recurrent models. In\nsum, our findings indicate how the transformer architecture works together with\nparticular properties of the training data to drive the intriguing emergent\nin-context learning behaviour of large language models, and how future work\nmight encourage both in-context and in-weights learning in domains beyond\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1\">Pierre H. Richemond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">Jay McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Tips from Song Reviews: A New Dataset and Framework. (arXiv:2205.06985v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.06985","description":"<p>Reviews of songs play an important role in online music service platforms.\nPrior research shows that users can make quicker and more informed decisions\nwhen presented with meaningful song reviews. However, reviews of music songs\nare generally long in length and most of them are non-informative for users. It\nis difficult for users to efficiently grasp meaningful messages for making\ndecisions. To solve this problem, one practical strategy is to provide tips,\ni.e., short, concise, empathetic, and self-contained descriptions about songs.\nTips are produced from song reviews and should express non-trivial insights\nabout the songs. To the best of our knowledge, no prior studies have explored\nthe tip generation task in music domain. In this paper, we create a dataset\nnamed MTips for the task and propose a framework named GENTMS for automatically\ngenerating tips from song reviews. The dataset involves 8,003 Chinese\ntips/non-tips from 128 songs which are distributed in five different song\ngenres. Experimental results show that GENTMS achieves top-10 precision at\n85.56%, outperforming the baseline models by at least 3.34%. Besides, to\nsimulate the practical usage of our proposed framework, we also experiment with\npreviously-unseen songs, during which GENTMS also achieves the best performance\nwith top-10 precision at 78.89% on average. The results demonstrate the\neffectiveness of the proposed framework in tip generation of the music domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_J/0/1/0/all/0/1\">Jingya Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cuiyun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yupan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lanjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Plagiarism in Introductory Programming Course Assignments. (arXiv:2205.08520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.08520","description":"<p>Measuring plagiarism in programming assignments is an essential task to the\neducational procedure. This paper discusses the methods of plagiarism and its\ndetection in introductory programming course assignments written in C++. A\nsmall corpus of assignments is made publically available. A general framework\nto compute the similarity between a solution pair is developed that uses the\nthree token-based similarity methods as features and predicts if the solution\nis plagiarized. The importance of each feature is also measured, which in\nreturn ranks the effectiveness of each method in use. Finally, the artificially\ngenerated dataset improves the results compared to the original data. We\nachieved an F1 score of 0.955 and 0.971 on original and synthetic datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayoun_M/0/1/0/all/0/1\">Muhammad Humayoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashmi_M/0/1/0/all/0/1\">Muhammad Adnan Hashmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Ali Hanzala Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling. (arXiv:2205.12986v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12986","description":"<p>Sentence scoring aims at measuring the likelihood score of a sentence and is\nwidely used in many natural language processing scenarios, like reranking,\nwhich is to select the best sentence from multiple candidates. Previous works\non sentence scoring mainly adopted either causal language modeling (CLM) like\nGPT or masked language modeling (MLM) like BERT, which have some limitations:\n1) CLM only utilizes unidirectional information for the probability estimation\nof a sentence without considering bidirectional context, which affects the\nscoring quality; 2) MLM can only estimate the probability of partial tokens at\na time and thus requires multiple forward passes to estimate the probability of\nthe whole sentence, which incurs large computation and time cost. In this\npaper, we propose \\textit{Transcormer} -- a Transformer model with a novel\n\\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,\nour SLM adopts a triple-stream self-attention mechanism to estimate the\nprobability of all tokens in a sentence with bidirectional context and only\nrequires a single forward pass. SLM can avoid the limitations of CLM (only\nunidirectional context) and MLM (multiple forward passes) and inherit their\nadvantages, and thus achieve high effectiveness and efficiency in scoring.\nExperimental results on multiple tasks demonstrate that our method achieves\nbetter performance than other language modelings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Multiscale Voxel Based Decoding For Enhanced Natural Image Reconstruction From Brain Activity. (arXiv:2205.14177v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14177","description":"<p>Reconstructing perceived images from human brain activity monitored by\nfunctional magnetic resonance imaging (fMRI) is hard, especially for natural\nimages. Existing methods often result in blurry and unintelligible\nreconstructions with low fidelity. In this study, we present a novel approach\nfor enhanced image reconstruction, in which existing methods for object\ndecoding and image reconstruction are merged together. This is achieved by\nconditioning the reconstructed image to its decoded image category using a\nclass-conditional generative adversarial network and neural style transfer. The\nresults indicate that our approach improves the semantic similarity of the\nreconstructed images and can be used as a general framework for enhanced image\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halac_M/0/1/0/all/0/1\">Mali Halac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isik_M/0/1/0/all/0/1\">Murat Isik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayaz_H/0/1/0/all/0/1\">Hasan Ayaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anup Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised learning of features and object boundaries from local prediction. (arXiv:2205.14195v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14195","description":"<p>A visual system has to learn both which features to extract from images and\nhow to group locations into (proto-)objects. Those two aspects are usually\ndealt with separately, although predictability is discussed as a cue for both.\nTo incorporate features and boundaries into the same model, we model a layer of\nfeature maps with a pairwise Markov random field model in which each factor is\npaired with an additional binary variable, which switches the factor on or off.\nUsing one of two contrastive learning objectives, we can learn both the\nfeatures and the parameters of the Markov random field factors from images\nwithout further supervision signals. The features learned by shallow neural\nnetworks based on this loss are local averages, opponent colors, and Gabor-like\nstripe patterns. Furthermore, we can infer connectivity between locations by\ninferring the switch variables. Contours inferred from this connectivity\nperform quite well on the Berkeley segmentation database (BSDS500) without any\ntraining on contours. Thus, computing predictions across space aids both\nsegmentation and feature learning, and models trained to optimize these\npredictions show similarities to the human visual system. We speculate that\nretinotopic visual cortex might implement such predictions over space through\nlateral connections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schutt_H/0/1/0/all/0/1\">Heiko H. Sch&#xfc;tt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei Ji Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Masked Autoencoders Learn Transferable Representations. (arXiv:2205.14204v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14204","description":"<p>Building scalable models to learn from diverse, multimodal data remains an\nopen challenge. For vision-language data, the dominant approaches are based on\ncontrastive learning objectives that train a separate encoder for each\nmodality. While effective, contrastive learning approaches introduce sampling\nbias depending on the data augmentations used, which can degrade performance on\ndownstream tasks. Moreover, these methods are limited to paired image-text\ndata, and cannot leverage widely-available unpaired data. In this paper, we\ninvestigate whether a large multimodal model trained purely via masked token\nprediction, without using modality-specific encoders or contrastive learning,\ncan learn transferable representations for downstream tasks. We propose a\nsimple and scalable network architecture, the Multimodal Masked Autoencoder\n(M3AE), which learns a unified encoder for both vision and language data via\nmasked token prediction. We provide an empirical study of M3AE trained on a\nlarge-scale image-text dataset, and find that M3AE is able to learn\ngeneralizable representations that transfer well to downstream tasks.\nSurprisingly, we find that M3AE benefits from a higher text mask ratio\n(50-90%), in contrast to BERT whose standard masking ratio is 15%, due to the\njoint training of two data modalities. We also provide qualitative analysis\nshowing that the learned representation incorporates meaningful information\nfrom both image and language. Lastly, we demonstrate the scalability of M3AE\nwith larger model size and training time, and its flexibility to train on both\npaired image-text data as well as unpaired data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xinyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lisa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurams_D/0/1/0/all/0/1\">Dale Schuurams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exemplar Free Class Agnostic Counting. (arXiv:2205.14212v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14212","description":"<p>We tackle the task of Class Agnostic Counting, which aims to count objects in\na novel object category at test time without any access to labeled training\ndata for that category. All previous class agnostic counting methods cannot\nwork in a fully automated setting, and require computationally expensive test\ntime adaptation. To address these challenges, we propose a visual counter which\noperates in a fully automated setting and does not require any test time\nadaptation. Our proposed approach first identifies exemplars from repeating\nobjects in an image, and then counts the repeating objects. We propose a novel\nregion proposal network for identifying the exemplars. After identifying the\nexemplars, we obtain the corresponding count by using a density estimation\nbased Visual Counter. We evaluate our proposed approach on FSC-147 dataset, and\nshow that it achieves superior performance compared to the existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_V/0/1/0/all/0/1\">Viresh Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1\">Minh Hoai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction. (arXiv:2205.14230v1 [cs.LG])","link":"http://arxiv.org/abs/2205.14230","description":"<p>Predicting the trajectories of surrounding objects is a critical task in\nself-driving and many other autonomous systems. Recent works demonstrate that\nadversarial attacks on trajectory prediction, where small crafted perturbations\nare introduced to history trajectories, may significantly mislead the\nprediction of future trajectories and ultimately induce unsafe planning.\nHowever, few works have addressed enhancing the robustness of this important\nsafety-critical task. In this paper, we present the first adversarial training\nmethod for trajectory prediction. Compared with typical adversarial training on\nimage tasks, our work is challenged by more random inputs with rich context,\nand a lack of class labels. To address these challenges, we propose a method\nbased on a semi-supervised adversarial autoencoder that models disentangled\nsemantic features with domain knowledge and provides additional latent labels\nfor the adversarial training. Extensive experiments with different types of\nattacks demonstrate that our semi-supervised semantics-guided adversarial\ntraining method can effectively mitigate the impact of adversarial attacks and\ngenerally improve the system's adversarial robustness to a variety of attacks,\nincluding unseen ones. We believe that such semantics-guided architecture and\nadvancement in robust generalization is an important step for developing robust\nprediction models and enabling safe decision making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1\">Ruochen Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangguo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">Takami Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Alfred Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Keypoint Matching using Graph Neural Networks. (arXiv:2205.14275v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14275","description":"<p>Image matching is a key component of many tasks in computer vision and its\nmain objective is to find correspondences between features extracted from\ndifferent natural images. When images are represented as graphs, image matching\nboils down to the problem of graph matching which has been studied intensively\nin the past. In recent years, graph neural networks have shown great potential\nin the graph matching task, and have also been applied to image matching. In\nthis paper, we propose a graph neural network for the problem of image\nmatching. The proposed method first generates initial soft correspondences\nbetween keypoints using localized node embeddings and then iteratively refines\nthe initial correspondences using a series of graph neural network layers. We\nevaluate our method on natural image datasets with keypoint annotations and\nshow that, in comparison to a state-of-the-art model, our method speeds up\ninference times without sacrificing prediction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nancy Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolentzos_G/0/1/0/all/0/1\">Giannis Nikolentzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bostrom_H/0/1/0/all/0/1\">Henrik Bostr&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Object Placement Assessment. (arXiv:2205.14280v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14280","description":"<p>Object placement assessment (OPA) aims to predict the rationality score of a\ncomposite image in terms of the placement (e.g., scale, location) of inserted\nforeground object. However, given a pair of scaled foreground and background,\nto enumerate all the reasonable locations, existing OPA model needs to place\nthe foreground at each location on the background and pass the obtained\ncomposite image through the model one at a time, which is very time-consuming.\nIn this work, we investigate a new task named as fast OPA. Specifically,\nprovided with a scaled foreground and a background, we only pass them through\nthe model once and predict the rationality scores for all locations. To\naccomplish this task, we propose a pioneering fast OPA model with several\ninnovations (i.e., foreground dynamic filter, background prior transfer, and\ncomposite feature mimicking) to bridge the performance gap between slow OPA\nmodel and fast OPA model. Extensive experiments on OPA dataset show that our\nproposed fast OPA model performs on par with slow OPA model but runs\nsignificantly faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangtong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Lip Region-of-Interest Sufficient for Lipreading?. (arXiv:2205.14295v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14295","description":"<p>Lip region-of-interest (ROI) is conventionally used for visual input in the\nlipreading task. Few works have adopted the entire face as visual input because\nlip-excluded parts of the face are usually considered to be redundant and\nirrelevant to visual speech recognition. However, faces contain much more\ndetailed information than lips, such as speakers' head pose, emotion, identity\netc. We argue that such information might benefit visual speech recognition if\na powerful feature extractor employing the entire face is trained. In this\nwork, we propose to adopt the entire face for lipreading with self-supervised\nlearning. AV-HuBERT, an audio-visual multi-modal self-supervised learning\nframework, was adopted in our experiments. Our experimental results showed that\nadopting the entire face achieved 16% relative word error rate (WER) reduction\non the lipreading task, compared with the baseline method using lip as visual\ninput. Without self-supervised pretraining, the model with face input achieved\na higher WER than that using lip input in the case of limited training data (30\nhours), while a slightly lower WER when using large amount of training data\n(433 hours).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing-Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1\">Gen-Shun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake It Till You Make It: Near-Distribution Novelty Detection by Score-Based Generative Models. (arXiv:2205.14297v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14297","description":"<p>We aim for image-based novelty detection. Despite considerable progress,\nexisting models either fail or face a dramatic drop under the so-called\n``near-distribution\" setting, where the differences between normal and\nanomalous samples are subtle. We first demonstrate existing methods experience\nup to 20\\% decrease in performance in the near-distribution setting. Next, we\npropose to exploit a score-based generative model to produce synthetic\nnear-distribution anomalous data. Our model is then fine-tuned to distinguish\nsuch data from the normal samples. We provide a quantitative as well as\nqualitative evaluation of this strategy, and compare the results with a variety\nof GAN-based models. Effectiveness of our method for both the near-distribution\nand standard novelty detection is assessed through extensive experiments on\ndatasets in diverse applications such as medical images, object classification,\nand quality control. This reveals that our method considerably improves over\nexisting models, and consistently decreases the gap between the\nnear-distribution and standard novelty detection performance. Overall, our\nmethod improves the near-distribution novelty detection by 6% and passes the\nstate-of-the-art by 1% to 5% across nine novelty detection benchmarks. The code\nrepository is available at https://github.com/rohban-lab/FITYMI\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_H/0/1/0/all/0/1\">Hossein Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahabi_S/0/1/0/all/0/1\">Sajjad Shahabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning with Label Noise: A Hierarchical Approach. (arXiv:2205.14299v1 [cs.LG])","link":"http://arxiv.org/abs/2205.14299","description":"<p>Deep neural networks are susceptible to label noise. Existing methods to\nimprove robustness, such as meta-learning and regularization, usually require\nsignificant change to the network architecture or careful tuning of the\noptimization procedure. In this work, we propose a simple hierarchical approach\nthat incorporates a label hierarchy when training the deep learning models. Our\napproach requires no change of the network architecture or the optimization\nprocedure. We investigate our hierarchical network through a wide range of\nsimulated and real datasets and various label noise types. Our hierarchical\napproach improves upon regular deep neural networks in learning with label\nnoise. Combining our hierarchical approach with pre-trained models achieves\nstate-of-the-art performance in real-world noisy datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1\">Ningyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1\">Cong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1\">Hayden S. Helm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lytvynets_K/0/1/0/all/0/1\">Kate Lytvynets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Fake News Detection via CLIP-Guided Learning. (arXiv:2205.14304v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14304","description":"<p>Multimodal fake news detection has attracted many research interests in\nsocial forensics. Many existing approaches introduce tailored attention\nmechanisms to guide the fusion of unimodal features. However, how the\nsimilarity of these features is calculated and how it will affect the\ndecision-making process in FND are still open questions. Besides, the potential\nof pretrained multi-modal feature learning models in fake news detection has\nnot been well exploited. This paper proposes a FND-CLIP framework, i.e., a\nmultimodal Fake News Detection network based on Contrastive Language-Image\nPretraining (CLIP). Given a targeted multimodal news, we extract the deep\nrepresentations from the image and text using a ResNet-based encoder, a\nBERT-based encoder and two pair-wise CLIP encoders. The multimodal feature is a\nconcatenation of the CLIP-generated features weighted by the standardized\ncross-modal similarity of the two modalities. The extracted features are\nfurther processed for redundancy reduction before feeding them into the final\nclassifier. We introduce a modality-wise attention module to adaptively\nreweight and aggregate the features. We have conducted extensive experiments on\ntypical fake news datasets. The results indicate that the proposed framework\nhas a better capability in mining crucial features for fake news detection. The\nproposed FND-CLIP can achieve better performances than previous works, i.e.,\n0.7\\%, 6.8\\% and 1.3\\% improvements in overall accuracy on Weibo, Politifact\nand Gossipcop, respectively. Besides, we justify that CLIP-based learning can\nallow better flexibility on multimodal feature selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Molecular Image Recognition: A Graph Generation Approach. (arXiv:2205.14311v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14311","description":"<p>Molecular image recognition is a fundamental task in information extraction\nfrom chemistry literature. Previous data-driven models formulate it as an\nimage-to-sequence task, to generate a sequential representation of the molecule\n(e.g. SMILES string) from its graphical representation. Although they perform\nadequately on certain benchmarks, these models are not robust in real-world\nsituations, where molecular images differ in style, quality, and chemical\npatterns. In this paper, we propose a novel graph generation approach that\nexplicitly predicts atoms and bonds, along with their geometric layouts, to\nconstruct the molecular graph. We develop data augmentation strategies for\nmolecules and images to increase the robustness of our model against domain\nshifts. Our model is flexible to incorporate chemistry constraints, and\nproduces more interpretable predictions than SMILES. In experiments on both\nsynthetic and realistic molecular images, our model significantly outperforms\nprevious models, achieving 84-93% accuracy on five benchmarks. We also conduct\nhuman evaluation and show that our model reduces the time for a chemist to\nextract molecular structures from images by roughly 50%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yujie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengkai Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coley_C/0/1/0/all/0/1\">Connor W. Coley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WT-MVSNet: Window-based Transformers for Multi-view Stereo. (arXiv:2205.14319v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14319","description":"<p>Recently, Transformers were shown to enhance the performance of multi-view\nstereo by enabling long-range feature interaction. In this work, we propose\nWindow-based Transformers (WT) for local feature matching and global feature\naggregation in multi-view stereo. We introduce a Window-based Epipolar\nTransformer (WET) which reduces matching redundancy by using epipolar\nconstraints. Since point-to-line matching is sensitive to erroneous camera pose\nand calibration, we match windows near the epipolar lines. A second Shifted WT\nis employed for aggregating global information within cost volume. We present a\nnovel Cost Transformer (CT) to replace 3D convolutions for cost volume\nregularization. In order to better constrain the estimated depth maps from\nmultiple views, we further design a novel geometric consistency loss (Geo Loss)\nwhich punishes unreliable areas where multi-view consistency is not satisfied.\nOur WT multi-view stereo method (WT-MVSNet) achieves state-of-the-art\nperformance across multiple datasets and ranks $1^{st}$ on Tanks and Temples\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jinli Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yikang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavit_Y/0/1/0/all/0/1\">Yoli Shavit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shihao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wensen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo. (arXiv:2205.14320v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14320","description":"<p>In this paper, we present a learning-based approach for multi-view stereo\n(MVS), i.e., estimate the depth map of a reference frame using posed multi-view\nimages. Our core idea lies in leveraging a \"learning-to-optimize\" paradigm to\niteratively index a plane-sweeping cost volume and regress the depth map via a\nconvolutional Gated Recurrent Unit (GRU). Since the cost volume plays a\nparamount role in encoding the multi-view geometry, we aim to improve its\nconstruction both in pixel- and frame- levels. In the pixel level, we propose\nto break the symmetry of the Siamese network (which is typically used in MVS to\nextract image features) by introducing a transformer block to the reference\nimage (but not to the source images). Such an asymmetric volume allows the\nnetwork to extract global features from the reference image to predict its\ndepth map. In view of the inaccuracy of poses between reference and source\nimages, we propose to incorporate a residual pose network to make corrections\nto the relative poses, which essentially rectifies the cost volume in the\nframe-level. We conduct extensive experiments on real-world MVS datasets and\nshow that our method achieves state-of-the-art performance in terms of both\nwithin-dataset evaluation and cross-dataset generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Changjiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point RCNN: An Angle-Free Framework for Rotated Object Detection. (arXiv:2205.14328v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14328","description":"<p>Rotated object detection in aerial images is still challenging due to\narbitrary orientations, large scale and aspect ratio variations, and extreme\ndensity of objects. Existing state-of-the-art rotated object detection methods\nmainly rely on angle-based detectors. However, angle regression can easily\nsuffer from the long-standing boundary problem. To tackle this problem, we\npropose a purely angle-free framework for rotated object detection, called\nPoint RCNN, which mainly consists of PointRPN and PointReg. In particular,\nPointRPN generates accurate rotated RoIs (RRoIs) by converting the learned\nrepresentative points with a coarse-to-fine manner, which is motivated by\nRepPoints. Based on the learned RRoIs, PointReg performs corner points\nrefinement for more accurate detection. In addition, aerial images are often\nseverely unbalanced in categories, and existing methods almost ignore this\nissue. In this paper, we also experimentally verify that re-sampling the images\nof the rare categories will stabilize training and further improve the\ndetection performance. Experiments demonstrate that our Point RCNN achieves the\nnew state-of-the-art detection performance on commonly used aerial datasets,\nincluding DOTA-v1.0, DOTA-v1.5, and HRSC2016.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chaohui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Point-Based Radiance Fields for Efficient View Synthesis. (arXiv:2205.14330v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14330","description":"<p>We propose a differentiable rendering algorithm for efficient novel view\nsynthesis. By departing from volume-based representations in favor of a learned\npoint representation, we improve on existing methods more than an order of\nmagnitude in memory and runtime, both in training and inference. The method\nbegins with a uniformly-sampled random point cloud and learns per-point\nposition and view-dependent appearance, using a differentiable splat-based\nrenderer to evolve the model to match a set of input images. Our method is up\nto 300x faster than NeRF in both training and inference, with only a marginal\nsacrifice in quality, while using less than 10~MB of memory for a static scene.\nFor dynamic scenes, our method trains two orders of magnitude faster than\nSTNeRF and renders at near interactive rate, while maintaining high image\nquality and temporal coherence even without imposing any temporal-coherency\nregularizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1\">Szymon Rusinkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V4D: Voxel for 4D Novel View Synthesis. (arXiv:2205.14332v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14332","description":"<p>Neural radiance fields have made a remarkable breakthrough in the novel view\nsynthesis task at the 3D static scene. However, for the 4D circumstance (e.g.,\ndynamic scene), the performance of the existing method is still limited by the\ncapacity of the neural network, typically in a multilayer perceptron network\n(MLP). In this paper, we present the method to model the 4D neural radiance\nfield by the 3D voxel, short as V4D, where the 3D voxel has two formats. The\nfirst one is to regularly model the bounded 3D space and then use the sampled\nlocal 3D feature with the time index to model the density field and the texture\nfield. The second one is in look-up tables (LUTs) format that is for the\npixel-level refinement, where the pseudo-surface produced by the volume\nrendering is utilized as the guidance information to learn a 2D pixel-level\nrefinement mapping. The proposed LUTs-based refinement module achieves the\nperformance gain with a little computational cost and could serve as the\nplug-and-play module in the novel view synthesis task. Moreover, we propose a\nmore effective conditional positional encoding toward the 4D data that achieves\nperformance gain with negligible computational burdens. Extensive experiments\ndemonstrate that the proposed method achieves state-of-the-art performance by a\nlarge margin. At last, the proposed V4D is also a computational-friendly method\nin both the training and testing phase, where we achieve 2 times faster in the\ntraining phase and 10 times faster in the inference phase compared with the\nstate-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Wanshui Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1\">Naoto Yokoya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-wise Masked Autoencoders for Fast Pre-training. (arXiv:2205.14338v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14338","description":"<p>Self-supervised pre-training for images without labels has recently achieved\npromising performance in image classification. The success of transformer-based\nmethods, ViT and MAE, draws the community's attention to the design of backbone\narchitecture and self-supervised task. In this work, we show that current\nmasked image encoding models learn the underlying relationship between all\nobjects in the whole scene, instead of a single object representation.\nTherefore, those methods bring a lot of compute time for self-supervised\npre-training. To solve this issue, we introduce a novel object selection and\ndivision strategy to drop non-object patches for learning object-wise\nrepresentations by selective reconstruction with interested region masks. We\nrefer to this method ObjMAE. Extensive experiments on four commonly-used\ndatasets demonstrate the effectiveness of our model in reducing the compute\ncost by 72% while achieving competitive performance. Furthermore, we\ninvestigate the inter-object and intra-object relationship and find that the\nlatter is crucial for self-supervised pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiantao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimation of 3D Body Shape and Clothing Measurements from Frontal- and Side-view Images. (arXiv:2205.14347v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14347","description":"<p>The estimation of 3D human body shape and clothing measurements is crucial\nfor virtual try-on and size recommendation problems in the fashion industry but\nhas always been a challenging problem due to several conditions, such as lack\nof publicly available realistic datasets, ambiguity in multiple camera\nresolutions, and the undefinable human shape space. Existing works proposed\nvarious solutions to these problems but could not succeed in the industry\nadaptation because of complexity and restrictions. To solve the complexity and\nchallenges, in this paper, we propose a simple yet effective architecture to\nestimate both shape and measures from frontal- and side-view images. We utilize\nsilhouette segmentation from the two multi-view images and implement an\nauto-encoder network to learn low-dimensional features from segmented\nsilhouettes. Then, we adopt a kernel-based regularized regression module to\nestimate the body shape and measurements. The experimental results show that\nthe proposed method provides competitive results on the synthetic dataset,\nNOMO-3d-400-scans Dataset, and RGB Images of humans captured in different\ncameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thota_K/0/1/0/all/0/1\">Kundan Sai Prabhu Thota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1\">Sungho Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1\">Paul Lukowicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning with Multi-query Transformer for Dense Prediction. (arXiv:2205.14354v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14354","description":"<p>Previous multi-task dense prediction studies developed complex pipelines such\nas multi-modal distillations in multiple stages or searching for task\nrelational contexts for each task. The core insight beyond these methods is to\nmaximize the mutual effects between each task. Inspired by the recent\nquery-based Transformers, we propose a simpler pipeline named Multi-Query\nTransformer (MQTransformer) that is equipped with multiple queries from\ndifferent tasks to facilitate the reasoning among multiple tasks and simplify\nthe cross task pipeline. Instead of modeling the dense per-pixel context among\ndifferent tasks, we seek a task-specific proxy to perform cross-task reasoning\nvia multiple queries where each query encodes the task-related context. The\nMQTransformer is composed of three key components: shared encoder, cross task\nattention and shared decoder. We first model each task with a task-relevant and\nscale-aware query, and then both the image feature output by the feature\nextractor and the task-relevant query feature are fed into the shared encoder,\nthus encoding the query feature from the image feature. Secondly, we design a\ncross task attention module to reason the dependencies among multiple tasks and\nfeature scales from two perspectives including different tasks of the same\nscale and different scales of the same task. Then we use a shared decoder to\ngradually refine the image features with the reasoned query features from\ndifferent tasks. Extensive experiment results on two dense prediction datasets\n(NYUD-v2 and PASCAL-Context) show that the proposed method is an effective\napproach and achieves the state-of-the-art result. Code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haobo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Facial Expression Recognition by A Semi-Supervised Progressive Teacher. (arXiv:2205.14361v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14361","description":"<p>In this paper, we aim to improve the performance of in-the-wild Facial\nExpression Recognition (FER) by exploiting semi-supervised learning.\nLarge-scale labeled data and deep learning methods have greatly improved the\nperformance of image recognition. However, the performance of FER is still not\nideal due to the lack of training data and incorrect annotations (e.g., label\nnoises). Among existing in-the-wild FER datasets, reliable ones contain\ninsufficient data to train robust deep models while large-scale ones are\nannotated in lower quality. To address this problem, we propose a\nsemi-supervised learning algorithm named Progressive Teacher (PT) to utilize\nreliable FER datasets as well as large-scale unlabeled expression images for\neffective training. On the one hand, PT introduces semi-supervised learning\nmethod to relieve the shortage of data in FER. On the other hand, it selects\nuseful labeled training samples automatically and progressively to alleviate\nlabel noise. PT uses selected clean labeled data for computing the supervised\nclassification loss and unlabeled data for unsupervised consistency loss.\nExperiments on widely-used databases RAF-DB and FERPlus validate the\neffectiveness of our method, which achieves state-of-the-art performance with\naccuracy of 89.57% on RAF-DB. Additionally, when the synthetic noise rate\nreaches even 30%, the performance of our PT algorithm only degrades by 4.37%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14375","description":"<p>Gains in the ability to generalize on image analysis tasks for neural\nnetworks have come at the cost of increased number of parameters and layers,\ndataset sizes, training and test computations, and GPU RAM. We introduce a new\narchitecture -- WaveMix-Lite -- that can generalize on par with contemporary\ntransformers and convolutional neural networks (CNNs) while needing fewer\nresources. WaveMix-Lite uses 2D-discrete wavelet transform to efficiently mix\nspatial information from pixels. WaveMix-Lite seems to be a versatile and\nscalable architectural framework that can be used for multiple vision tasks,\nsuch as image classification and semantic segmentation, without requiring\nsignificant architectural changes, unlike transformers and CNNs. It is able to\nmeet or exceed several accuracy benchmarks while training on a single GPU. For\ninstance, it achieves state-of-the-art accuracy on five EMNIST datasets,\noutperforms CNNs and transformers in ImageNet-1K (64$\\times$64 images), and\nachieves an mIoU of 75.32 % on Cityscapes validation set, while using less than\none-fifth the number parameters and half the GPU RAM of comparable CNNs or\ntransformers. Our experiments show that while the convolutional elements of\nneural architectures exploit the shift-invariance property of images, new types\nof layers (e.g., wavelet transform) can exploit additional properties of\nimages, such as scale-invariance and finite spatial extents of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_K/0/1/0/all/0/1\">Kavitha Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Quality of Pose-varied Face Restoration with Local Weak Feature Sensing and GAN Prior. (arXiv:2205.14377v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14377","description":"<p>Facial semantic guidance (facial landmarks, facial parsing maps, facial\nheatmaps, etc.) and facial generative adversarial networks (GAN) prior have\nbeen widely used in blind face restoration (BFR) in recent years. Although\nexisting BFR methods have achieved good performance in ordinary cases, these\nsolutions have limited resilience when applied to face images with serious\ndegradation and pose-varied (look up, look down, laugh, etc.) in real-world\nscenarios. In this work, we propose a well-designed blind face restoration\nnetwork with generative facial prior. The proposed network is mainly comprised\nof an asymmetric codec and StyleGAN2 prior network. In the asymmetric codec, we\nadopt a mixed multi-path residual block (MMRB) to gradually extract weak\ntexture features of input images, which can improve the texture integrity and\nauthenticity of our networks. Furthermore, the MMRB block can also be\nplug-and-play in any other network. Besides, a novel self-supervised training\nstrategy is specially designed for face restoration tasks to fit the\ndistribution closer to the target and maintain training stability. Extensive\nexperiments over synthetic and real-world datasets demonstrate that our model\nachieves superior performance to the prior art for face restoration and face\nsuper-resolution tasks and can tackle seriously degraded face images in diverse\nposes and expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Renhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Bin Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training. (arXiv:2205.14401v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14401","description":"<p>Masked Autoencoders (MAE) have shown great potentials in self-supervised\npre-training for language and 2D image transformers. However, it still remains\nan open question on how to exploit masked autoencoding for learning 3D\nrepresentations of irregular point clouds. In this paper, we propose\nPoint-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical\nself-supervised learning of 3D point clouds. Unlike the standard transformer in\nMAE, we modify the encoder and decoder into pyramid architectures to\nprogressively model spatial geometries and capture both fine-grained and\nhigh-level semantics of 3D shapes. For the encoder that downsamples point\ntokens by stages, we design a multi-scale masking strategy to generate\nconsistent visible regions across scales, and adopt a local spatial\nself-attention mechanism to focus on neighboring patterns. By multi-scale token\npropagation, the lightweight decoder gradually upsamples point tokens with\ncomplementary skip connections from the encoder, which further promotes the\nreconstruction from a global-to-local perspective. Extensive experiments\ndemonstrate the state-of-the-art performance of Point-M2AE for 3D\nrepresentation learning. With a frozen encoder after pre-training, Point-M2AE\nachieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some\nfully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves\n86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely\nbenefits the few-shot classification, part segmentation and 3D object detection\nwith the hierarchical pre-training scheme. Code will be available at\nhttps://github.com/ZrrSkywalker/Point-M2AE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rongyao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strengthening Skeletal Action Recognizers via Leveraging Temporal Patterns. (arXiv:2205.14405v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14405","description":"<p>Skeleton sequences are compact and lightweight. Numerous skeleton-based\naction recognizers have been proposed to classify human behaviors. In this\nwork, we aim to incorporate components that are compatible with existing models\nand further improve their accuracy. To this end, we design two temporal\naccessories: discrete cosine encoding (DCE) and chronological loss (CRL). DCE\nfacilitates models to analyze motion patterns from the frequency domain and\nmeanwhile alleviates the influence of signal noise. CRL guides networks to\nexplicitly capture the sequence's chronological order. These two components\nconsistently endow many recently-proposed action recognizers with accuracy\nboosts, achieving new state-of-the-art (SOTA) accuracy on two large benchmark\ndatasets (NTU60 and NTU120).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Generation for Satellite Image Classification Using Self-Supervised Representation Learning. (arXiv:2205.14418v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14418","description":"<p>Supervised deep neural networks are the-state-of-the-art for many tasks in\nthe remote sensing domain, against the fact that such techniques require the\ndataset consisting of pairs of input and label, which are rare and expensive to\ncollect in term of both manpower and resources. On the other hand, there are\nabundance of raw satellite images available both for commercial and academic\npurposes. Hence, in this work, we tackle the insufficient labeled data problem\nin satellite image classification task by introducing the process based on the\nself-supervised learning technique to create the synthetic labels for satellite\nimage patches. These synthetic labels can be used as the training dataset for\nthe existing supervised learning techniques. In our experiments, we show that\nthe models trained on the synthetic labels give similar performance to the\nmodels trained on the real labels. And in the process of creating the synthetic\nlabels, we also obtain the visual representation vectors that are versatile and\nknowledge transferable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gulyanon_S/0/1/0/all/0/1\">Sarun Gulyanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Limprasert_W/0/1/0/all/0/1\">Wasit Limprasert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Songmuang_P/0/1/0/all/0/1\">Pokpong Songmuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kongkachandra_R/0/1/0/all/0/1\">Rachada Kongkachandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looks Like Magic: Transfer Learning in GANs to Generate New Card Illustrations. (arXiv:2205.14442v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14442","description":"<p>In this paper, we propose MAGICSTYLEGAN and MAGICSTYLEGAN-ADA - both\nincarnations of the state-of-the-art models StyleGan2 and StyleGan2 ADA - to\nexperiment with their capacity of transfer learning into a rather different\ndomain: creating new illustrations for the vast universe of the game \"Magic:\nThe Gathering\" cards. This is a challenging task especially due to the variety\nof elements present in these illustrations, such as humans, creatures,\nartifacts, and landscapes - not to mention the plethora of art styles of the\nimages made by various artists throughout the years. To solve the task at hand,\nwe introduced a novel dataset, named MTG, with thousands of illustration from\ndiverse card types and rich in metadata. The resulting set is a dataset\ncomposed by a myriad of both realistic and fantasy-like illustrations.\nAlthough, to investigate effects of diversity we also introduced subsets that\ncontain specific types of concepts, such as forests, islands, faces, and\nhumans. We show that simpler models, such as DCGANs, are not able to learn to\ngenerate proper illustrations in any setting. On the other side, we train\ninstances of MAGICSTYLEGAN using all proposed subsets, being able to generate\nhigh quality illustrations. We perform experiments to understand how well\npre-trained features from StyleGan2 can be transferred towards the target\ndomain. We show that in well trained models we can find particular instances of\nnoise vector that realistically represent real images from the dataset.\nMoreover, we provide both quantitative and qualitative studies to support our\nclaims, and that demonstrate that MAGICSTYLEGAN is the state-of-the-art\napproach for generating Magic illustrations. Finally, this paper highlights\nsome emerging properties regarding transfer learning in GANs, which is still a\nsomehow under-explored field in generative learning research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venturelli_M/0/1/0/all/0/1\">Matheus K. Venturelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_P/0/1/0/all/0/1\">Pedro H. Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehrmann_J/0/1/0/all/0/1\">J&#xf4;natas Wehrmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Self-supervised Lightweight Vision Transformers. (arXiv:2205.14443v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14443","description":"<p>Self-supervised learning on large-scale Vision Transformers (ViTs) as\npre-training methods has achieved promising downstream performance. Yet, how\nsuch pre-training paradigms promote lightweight ViTs' performance is\nconsiderably less studied. In this work, we mainly produce recipes for\npre-training high-performance lightweight ViTs using\nmasked-image-modeling-based MAE, namely MAE-lite, which achieves 78.4% top-1\naccuracy on ImageNet with ViT-Tiny (5.7M). Furthermore, we develop and\nbenchmark other fully-supervised and self-supervised pre-training counterparts,\ne.g., contrastive-learning-based MoCo-v3, on both ImageNet and other\nclassification tasks. We analyze and clearly show the effect of such\npre-training, and reveal that properly-learned lower layers of the pre-trained\nmodels matter more than higher ones in data-sufficient downstream tasks.\nFinally, by further comparing with the pre-trained representations of the\nup-scaled models, a distillation strategy during pre-training is developed to\nimprove the pre-trained representations as well, leading to further downstream\nperformance improvement. The code and models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaoru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Superordinate Abstraction for Robust Concept Learning. (arXiv:2205.14444v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14444","description":"<p>Concept learning constructs visual representations that are connected to\nlinguistic semantics, which is fundamental to vision-language tasks. Although\npromising progress has been made, existing concept learners are still\nvulnerable to attribute perturbations and out-of-distribution compositions\nduring inference. We ascribe the bottleneck to a failure of exploring the\nintrinsic semantic hierarchy of visual concepts, e.g. \\{red, blue,...\\} $\\in$\n`color' subspace yet cube $\\in$ `shape'. In this paper, we propose a visual\nsuperordinate abstraction framework for explicitly modeling semantic-aware\nvisual subspaces (i.e. visual superordinates). With only natural visual\nquestion answering data, our model first acquires the semantic hierarchy from a\nlinguistic view, and then explores mutually exclusive visual superordinates\nunder the guidance of linguistic hierarchy. In addition, a quasi-center visual\nconcept clustering and a superordinate shortcut learning schemes are proposed\nto enhance the discrimination and independence of concepts within each visual\nsuperordinate. Experiments demonstrate the superiority of the proposed\nframework under diverse settings, which increases the overall answering\naccuracy relatively by 7.5\\% on reasoning with perturbations and 15.6\\% on\ncompositional generalization tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Transformer: A Framework Beyond the Trade-off between Accuracy and Diversity for Image Captioning. (arXiv:2205.14458v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14458","description":"<p>Accuracy and Diversity are two essential metrizable manifestations in\ngenerating natural and semantically correct captions. Many efforts have been\nmade to enhance one of them with another decayed due to the trade-off gap.\nHowever, compromise does not make the progress. Decayed diversity makes the\ncaptioner a repeater, and decayed accuracy makes it a fake advisor. In this\nwork, we exploit a novel Variational Transformer framework to improve accuracy\nand diversity simultaneously. To ensure accuracy, we introduce the \"Invisible\nInformation Prior\" along with the \"Auto-selectable GMM\" to instruct the encoder\nto learn the precise language information and object relation in different\nscenes. To ensure diversity, we propose the \"Range-Median Reward\" baseline to\nretain more diverse candidates with higher rewards during the RL-based training\nprocess. Experiments show that our method achieves the simultaneous promotion\nof accuracy (CIDEr) and diversity (self-CIDEr), up to 1.1 and 4.8 percent,\ncompared with the baseline. Also, our method outperforms others under the newly\nproposed measurement of the trade-off gap, with at least 3.55 percent\npromotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longzhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_S/0/1/0/all/0/1\">Shaohua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yitao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CyCLIP: Cyclic Contrastive Language-Image Pretraining. (arXiv:2205.14459v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14459","description":"<p>Recent advances in contrastive representation learning over paired image-text\ndata have led to models such as CLIP that achieve state-of-the-art performance\nfor zero-shot classification and distributional robustness. Such models\ntypically require joint reasoning in the image and text representation spaces\nfor downstream inference tasks. Contrary to prior beliefs, we demonstrate that\nthe image and text representations learned via a standard contrastive objective\nare not interchangeable and can lead to inconsistent downstream predictions. To\nmitigate this issue, we formalize consistency and propose CyCLIP, a framework\nfor contrastive representation learning that explicitly optimizes for the\nlearned representations to be geometrically consistent in the image and text\nspace. In particular, we show that consistent representations can be learned by\nexplicitly symmetrizing (a) the similarity between the two mismatched\nimage-text pairs (cross-modal consistency); and (b) the similarity between the\nimage-image pair and the text-text pair (in-modal consistency). Empirically, we\nshow that the improved consistency in CyCLIP translates to significant gains\nover CLIP, with gains ranging from 10%-24% for zero-shot classification\naccuracy on standard benchmarks (CIFAR-10, CIFAR-100, ImageNet1K) and 10%-27%\nfor robustness to various natural distribution shifts. The code is available at\nhttps://github.com/goel-shashank/CyCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Shashank Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1\">Hritik Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Ryan A. Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinay_V/0/1/0/all/0/1\">Vishwa Vinay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors. (arXiv:2205.14467v1 [cs.LG])","link":"http://arxiv.org/abs/2205.14467","description":"<p>Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an\nunlabeled target domain supervised by a black-box predictor trained on a source\ndomain. It does not require access to both the source-domain data and the\npredictor parameters, thus addressing the data privacy and portability issues\nof standard domain adaptation. Existing DABP approaches mostly rely on model\ndistillation from the black-box predictor, \\emph{i.e.}, training the model with\nits noisy target-domain predictions, which however inevitably introduces the\nconfirmation bias accumulated from the prediction noises. To mitigate such\nbias, we propose a new method, named BETA, to incorporate knowledge\ndistillation and noisy label learning into one coherent framework. This is\nenabled by a new divide-to-adapt strategy. BETA divides the target domain into\nan easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain. Then\nit deploys mutually-teaching twin networks to filter the predictor errors for\neach other and improve them progressively, from the easy to hard subdomains. As\nsuch, BETA effectively purifies the noisy labels and reduces error\naccumulation. We theoretically show that the target error of BETA is minimized\nby decreasing the noise ratio of the subdomains. Extensive experiments\ndemonstrate BETA outperforms existing methods on all DABP benchmarks, and is\neven comparable with the standard domain adaptation methods that use the\nsource-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lihua Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptually Optimized Color Selection for Visualization. (arXiv:2205.14472v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14472","description":"<p>We propose an approach, called the Equilibrium Distribution Model (EDM), for\nautomatically selecting colors with optimum perceptual contrast for scientific\nvisualization. Given any number of features that need to be emphasized in a\nvisualization task, our approach derives evenly distributed points in the\nCIELAB color space to assign colors to the features so that the minimum\nEuclidean Distance among the colors are optimized. Our approach can assign\ncolors with high perceptual contrast even for very high numbers of features,\nwhere other color selection methods typically fail. We compare our approach\nwith the widely used Harmonic color selection scheme and demonstrate that while\nthe harmonic scheme can achieve reasonable color contrast for visualizing up to\n20 different features, our Equilibrium scheme provides significantly better\ncontrast and achieves perceptible contrast for visualizing even up to 100\nunique features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhrajyoti Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingliana_J/0/1/0/all/0/1\">John Dingliana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepRM: Deep Recurrent Matching for 6D Pose Refinement. (arXiv:2205.14474v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14474","description":"<p>Precise 6D pose estimation of rigid objects from RGB images is a critical but\nchallenging task in robotics and augmented reality. To address this problem, we\npropose DeepRM, a novel recurrent network architecture for 6D pose refinement.\nDeepRM leverages initial coarse pose estimates to render synthetic images of\ntarget objects. The rendered images are then matched with the observed images\nto predict a rigid transform for updating the previous pose estimate. This\nprocess is repeated to incrementally refine the estimate at each iteration.\nLSTM units are used to propagate information through each refinement step,\nsignificantly improving overall performance. In contrast to many 2-stage\nPerspective-n-Point based solutions, DeepRM is trained end-to-end, and uses a\nscalable backbone that can be tuned via a single parameter for accuracy and\nefficiency. During training, a multi-scale optical flow head is added to\npredict the optical flow between the observed and synthetic images. Optical\nflow prediction stabilizes the training process, and enforces the learning of\nfeatures that are relevant to the task of pose estimation. Our results\ndemonstrate that DeepRM achieves state-of-the-art performance on two widely\naccepted challenging datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avery_A/0/1/0/all/0/1\">Alexander Avery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savakis_A/0/1/0/all/0/1\">Andreas Savakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDMLP: Image Classification from Scratch on Small Datasets with MLP. (arXiv:2205.14477v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14477","description":"<p>The attention mechanism has become a go-to technique for natural language\nprocessing and computer vision tasks. Recently, the MLP-Mixer and other\nMLP-based architectures, based simply on multi-layer perceptrons (MLPs), are\nalso powerful compared to CNNs and attention techniques and raises a new\nresearch direction. However, the high capability of the MLP-based networks\nseverely relies on large volume of training data, and lacks of explanation\nability compared to the Vision Transformer (ViT) or ConvNets. When trained on\nsmall datasets, they usually achieved inferior results than ConvNets. To\nresolve it, we present (i) multi-dimensional MLP (MDMLP), a conceptually simple\nand lightweight MLP-based architecture yet achieves SOTA when training from\nscratch on small-size datasets; (ii) multi-dimension MLP Attention Tool\n(MDAttnTool), a novel and efficient attention mechanism based on MLPs. Even\nwithout strong data augmentation, MDMLP achieves 90.90% accuracy on CIFAR10\nwith only 0.3M parameters, while the well-known MLP-Mixer achieves 85.45% with\n17.1M parameters. In addition, the lightweight MDAttnTool highlights objects in\nimages, indicating its explanation power. Our code is available at\nhttps://github.com/Amoza-Theodore/MDMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tian Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Chongyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New High-Performance Approach to Approximate Pattern-Matching for Plagiarism Detection in Blockchain-Based Non-Fungible Tokens (NFTs). (arXiv:2205.14492v1 [cs.FL])","link":"http://arxiv.org/abs/2205.14492","description":"<p>We are presenting a fast and innovative approach to performing approximate\npattern-matching for plagiarism detection, using an NDFA-based approach that\nsignificantly enhances performance compared to other existing similarity\nmeasures. We outline the advantages of our approach in the context of\nblockchain-based non-fungible tokens (NFTs). We present, formalize, discuss and\ntest our proposed approach in several real-world scenarios and with different\nsimilarity measures commonly used in plagiarism detection, and observe\nsignificant throughput enhancements throughout the entire spectrum of tests,\nwith little to no compromises on the accuracy of the detection process overall.\nWe conclude that our approach is suitable and adequate to perform approximate\npattern-matching for plagiarism detection, and outline research directions for\nfuture improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pungila_C/0/1/0/all/0/1\">Ciprian Pungila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galis_D/0/1/0/all/0/1\">Darius Galis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negru_V/0/1/0/all/0/1\">Viorel Negru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BadDet: Backdoor Attacks on Object Detection. (arXiv:2205.14497v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14497","description":"<p>Deep learning models have been deployed in numerous real-world applications\nsuch as autonomous driving and surveillance. However, these models are\nvulnerable in adversarial environments. Backdoor attack is emerging as a severe\nsecurity threat which injects a backdoor trigger into a small portion of\ntraining data such that the trained model behaves normally on benign inputs but\ngives incorrect predictions when the specific trigger appears. While most\nresearch in backdoor attacks focuses on image classification, backdoor attacks\non object detection have not been explored but are of equal importance. Object\ndetection has been adopted as an important module in various security-sensitive\napplications such as autonomous driving. Therefore, backdoor attacks on object\ndetection could pose severe threats to human lives and properties. We propose\nfour kinds of backdoor attacks for object detection task: 1) Object Generation\nAttack: a trigger can falsely generate an object of the target class; 2)\nRegional Misclassification Attack: a trigger can change the prediction of a\nsurrounding object to the target class; 3) Global Misclassification Attack: a\nsingle trigger can change the predictions of all objects in an image to the\ntarget class; and 4) Object Disappearance Attack: a trigger can make the\ndetector fail to detect the object of the target class. We develop appropriate\nmetrics to evaluate the four backdoor attacks on object detection. We perform\nexperiments using two typical object detection models -- Faster-RCNN and YOLOv3\non different datasets. More crucially, we demonstrate that even fine-tuning on\nanother benign dataset cannot remove the backdoor hidden in the object\ndetection model. To defend against these backdoor attacks, we propose Detector\nCleanse, an entropy-based run-time detection framework to identify poisoned\ntesting samples for any deployed object detector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Shih-Han Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners. (arXiv:2205.14540v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14540","description":"<p>Self-supervised Masked Autoencoders (MAE) are emerging as a new pre-training\nparadigm in computer vision. MAE learns semantics implicitly via reconstructing\nlocal patches, requiring thousands of pre-training epochs to achieve favorable\nperformance. This paper incorporates explicit supervision, i.e., golden labels,\ninto the MAE framework. The proposed Supervised MAE (SupMAE) only exploits a\nvisible subset of image patches for classification, unlike the standard\nsupervised pre-training where all image patches are used. SupMAE is efficient\nand can achieve comparable performance with MAE using only 30% compute when\nevaluated on ImageNet with the ViT-B/16 model. Detailed ablation studies are\nconducted to verify the proposed components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1\">Diana Marculescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Missing Invariance Principle Found -- the Reciprocal Twin of Invariant Risk Minimization. (arXiv:2205.14546v1 [cs.LG])","link":"http://arxiv.org/abs/2205.14546","description":"<p>Machine learning models often generalize poorly to out-of-distribution (OOD)\ndata as a result of relying on features that are spuriously correlated with the\nlabel during training. Recently, the technique of Invariant Risk Minimization\n(IRM) was proposed to learn predictors that only use invariant features by\nconserving the feature-conditioned class expectation $\\mathbb{E}_e[y|f(x)]$\nacross environments. However, more recent studies have demonstrated that IRM\ncan fail in various task settings. Here, we identify a fundamental flaw of IRM\nformulation that causes the failure. We then introduce a complementary notion\nof invariance, MRI, that is based on conserving the class-conditioned feature\nexpectation $\\mathbb{E}_e[f(x)|y]$ across environments, that corrects for the\nflaw in IRM. Further, we introduce a simplified, practical version of the MRI\nformulation called as MRI-v1. We note that this constraint is convex which\nconfers it with an advantage over the practical version of IRM, IRM-v1, which\nimposes non-convex constraints. We prove that in a general linear problem\nsetting, MRI-v1 can guarantee invariant predictors given sufficient\nenvironments. We also empirically demonstrate that MRI strongly out-performs\nIRM and consistently achieves near-optimal OOD generalization in image-based\nnonlinear problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huh_D/0/1/0/all/0/1\">Dongsung Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baidya_A/0/1/0/all/0/1\">Avinash Baidya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Super-resolution with An Enhanced Group Convolutional Neural Network. (arXiv:2205.14548v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14548","description":"<p>CNNs with strong learning abilities are widely chosen to resolve\nsuper-resolution problem. However, CNNs depend on deeper network architectures\nto improve performance of image super-resolution, which may increase\ncomputational cost in general. In this paper, we present an enhanced\nsuper-resolution group CNN (ESRGCNN) with a shallow architecture by fully\nfusing deep and wide channel features to extract more accurate low-frequency\ninformation in terms of correlations of different channels in single image\nsuper-resolution (SISR). Also, a signal enhancement operation in the ESRGCNN is\nuseful to inherit more long-distance contextual information for resolving\nlong-term dependency. An adaptive up-sampling operation is gathered into a CNN\nto obtain an image super-resolution model with low-resolution images of\ndifferent sizes. Extensive experiments report that our ESRGCNN surpasses the\nstate-of-the-arts in terms of SISR performance, complexity, execution speed,\nimage quality evaluation and visual effect in SISR. Code is found at\nhttps://github.com/hellloxiaotian/ESRGCNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chunwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProxyMix: Proxy-based Mixup Training with Label Refinery for Source-Free Domain Adaptation. (arXiv:2205.14566v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14566","description":"<p>Unsupervised domain adaptation (UDA) aims to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. Owing to privacy concerns\nand heavy data transmission, source-free UDA, exploiting the pre-trained source\nmodels instead of the raw source data for target learning, has been gaining\npopularity in recent years. Some works attempt to recover unseen source domains\nwith generative models, however introducing additional network parameters.\nOther works propose to fine-tune the source model by pseudo labels, while noisy\npseudo labels may misguide the decision boundary, leading to unsatisfied\nresults. To tackle these issues, we propose an effective method named\nProxy-based Mixup training with label refinery (ProxyMix). First of all, to\navoid additional parameters and explore the information in the source model,\nProxyMix defines the weights of the classifier as the class prototypes and then\nconstructs a class-balanced proxy source domain by the nearest neighbors of the\nprototypes to bridge the unseen source domain and the target domain. To improve\nthe reliability of pseudo labels, we further propose the frequency-weighted\naggregation strategy to generate soft pseudo labels for unlabeled target data.\nThe proposed strategy exploits the internal structure of target features, pulls\ntarget features to their semantic neighbors, and increases the weights of\nlow-frequency classes samples during gradient updating. With the proxy domain\nand the reliable pseudo labels, we employ two kinds of mixup regularization,\ni.e., inter- and intra-domain mixup, in our framework, to align the proxy and\nthe target domain, enforcing the consistency of predictions, thereby further\nmitigating the negative impacts of noisy labels. Experiments on three 2D image\nand one 3D point cloud object recognition benchmarks demonstrate that ProxyMix\nyields state-of-the-art performance for source-free UDA tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuhe Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1\">Lijun Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1\">Aihua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ComplexGen: CAD Reconstruction by B-Rep Chain Complex Generation. (arXiv:2205.14573v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14573","description":"<p>We view the reconstruction of CAD models in the boundary representation\n(B-Rep) as the detection of geometric primitives of different orders, i.e.\nvertices, edges and surface patches, and the correspondence of primitives,\nwhich are holistically modeled as a chain complex, and show that by modeling\nsuch comprehensive structures more complete and regularized reconstructions can\nbe achieved. We solve the complex generation problem in two steps. First, we\npropose a novel neural framework that consists of a sparse CNN encoder for\ninput point cloud processing and a tri-path transformer decoder for generating\ngeometric primitives and their mutual relationships with estimated\nprobabilities. Second, given the probabilistic structure predicted by the\nneural network, we recover a definite B-Rep chain complex by solving a global\noptimization maximizing the likelihood under structural validness constraints\nand applying geometric refinements. Extensive tests on large scale CAD datasets\ndemonstrate that the modeling of B-Rep chain complex structure enables more\naccurate detection for learning and more constrained reconstruction for\noptimization, leading to structurally more faithful and complete CAD B-Rep\nmodels than previous results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haoxiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-Aligned Video Raindrop Removal with Temporal Constraints. (arXiv:2205.14574v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14574","description":"<p>Existing adherent raindrop removal methods focus on the detection of the\nraindrop locations, and then use inpainting techniques or generative networks\nto recover the background behind raindrops. Yet, as adherent raindrops are\ndiverse in sizes and appearances, the detection is challenging for both single\nimage and video. Moreover, unlike rain streaks, adherent raindrops tend to\ncover the same area in several frames. Addressing these problems, our method\nemploys a two-stage video-based raindrop removal method. The first stage is the\nsingle image module, which generates initial clean results. The second stage is\nthe multiple frame module, which further refines the initial results using\ntemporal constraints, namely, by utilizing multiple input frames in our process\nand applying temporal consistency between adjacent output frames. Our single\nimage module employs a raindrop removal network to generate initial raindrop\nremoval results, and create a mask representing the differences between the\ninput and initial output. Once the masks and initial results for consecutive\nframes are obtained, our multiple-frame module aligns the frames in both the\nimage and feature levels and then obtains the clean background. Our method\ninitially employs optical flow to align the frames, and then utilizes\ndeformable convolution layers further to achieve feature-level frame alignment.\nTo remove small raindrops and recover correct backgrounds, a target frame is\npredicted from adjacent frames. A series of unsupervised losses are proposed so\nthat our second stage, which is the video raindrop removal module, can\nself-learn from video data without ground truths. Experimental results on real\nvideos demonstrate the state-of-art performance of our method both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wending Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Robby T. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-C2FT: Coarse-to-fine Transformer for Multi-view 3D Reconstruction. (arXiv:2205.14575v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14575","description":"<p>Recently, the transformer model has been successfully employed for the\nmulti-view 3D reconstruction problem. However, challenges remain on designing\nan attention mechanism to explore the multiview features and exploit their\nrelations for reinforcing the encoding-decoding modules. This paper proposes a\nnew model, namely 3D coarse-to-fine transformer (3D-C2FT), by introducing a\nnovel coarse-to-fine(C2F) attention mechanism for encoding multi-view features\nand rectifying defective 3D objects. C2F attention mechanism enables the model\nto learn multi-view information flow and synthesize 3D surface correction in a\ncoarse to fine-grained manner. The proposed model is evaluated by ShapeNet and\nMulti-view Real-life datasets. Experimental results show that 3D-C2FT achieves\nnotable results and outperforms several competing models on these datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiong_L/0/1/0/all/0/1\">Leslie Ching Ow Tiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigmund_D/0/1/0/all/0/1\">Dick Sigmund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teoh_A/0/1/0/all/0/1\">Andrew Beng Jin Teoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards an unsupervised large-scale 2D and 3D building mapping with LiDAR. (arXiv:2205.14585v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14585","description":"<p>A 2D and 3D building map provides invaluable information for understanding\nhuman activities and their impacts on Earth and its environment. Despite\nenormous efforts to improve the quality of building maps, current large-scale\nbuilding maps have lots of errors and are limited to providing only 2D building\ninformation. This study presents a state-of-the-art 2D and 3D building\nextraction algorithm with airborne LiDAR data that is suitable for large-scale\nbuilding mapping. Our algorithm operates in a fully unsupervised manner and\ndoes not require either any training label or training procedure. Our algorithm\nrequires only simple operations of morphological filtering and planarity-based\nfiltering but can produce an accurate 2D and 3D building map. A quantitative\nand qualitative evaluation in a large-scale dataset (-550 sqkm) of Denver and\nNew York City showed that our algorithm outperforms the deep learning-based\nMicrosoft's building mapping algorithm even without any parameter tuning. More\nextensive evaluations in different conditions of landscapes confirmed that our\nalgorithm is scalable and can be improved further with appropriate parameter\nselection. Our algorithm is more advantageous than other image-based building\nextraction algorithms in that it is more computationally efficient, more\naccurate, and more explainable. Our proposed algorithm that can produce an\naccurate large-scale 2D and 3D building map provides a great potential towards\na global-scale 2D and 3D building mapping with airborne LiDAR data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hunsoo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jinha Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Distillation with Receptive Tokens. (arXiv:2205.14589v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14589","description":"<p>Distilling from the feature maps can be fairly effective for dense prediction\ntasks since both the feature discriminability and localization priors can be\nwell transferred. However, not every pixel contributes equally to the\nperformance, and a good student should learn from what really matters to the\nteacher. In this paper, we introduce a learnable embedding dubbed receptive\ntoken to localize those pixels of interests (PoIs) in the feature map, with a\ndistillation mask generated via pixel-wise attention. Then the distillation\nwill be performed on the mask via pixel-wise reconstruction. In this way, a\ndistillation mask actually indicates a pattern of pixel dependencies within\nfeature maps of teacher. We thus adopt multiple receptive tokens to investigate\nmore sophisticated and informative pixel dependencies to further enhance the\ndistillation. To obtain a group of masks, the receptive tokens are learned via\nthe regular task loss but with teacher fixed, and we also leverage a Dice loss\nto enrich the diversity of learned masks. Our method dubbed MasKD is simple and\npractical, and needs no priors of tasks in application. Experiments show that\nour MasKD can achieve state-of-the-art performance consistently on object\ndetection and semantic segmentation benchmarks. Code is available at:\nhttps://github.com/hunto/MasKD .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jian Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiasEnsemble: Revisiting the Importance of Amplifying Bias for Debiasing. (arXiv:2205.14594v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14594","description":"<p>In image classification, \"debiasing\" aims to train a classifier to be less\nsusceptible to dataset bias, the strong correlation between peripheral\nattributes of data samples and a target class. For example, even if the frog\nclass in the dataset mainly consists of frog images with a swamp background\n(i.e., bias-aligned samples), a debiased classifier should be able to correctly\nclassify a frog at a beach (i.e., bias-conflicting samples). Recent debiasing\napproaches commonly use two components for debiasing, a biased model $f_B$ and\na debiased model $f_D$. $f_B$ is trained to focus on bias-aligned samples while\n$f_D$ is mainly trained with bias-conflicting samples by concentrating on\nsamples which $f_B$ fails to learn, leading $f_D$ to be less susceptible to the\ndataset bias. While the state-of-the-art debiasing techniques have aimed to\nbetter train $f_D$, we focus on training $f_B$, an overlooked component until\nnow. Our empirical analysis reveals that removing the bias-conflicting samples\nfrom the training set for $f_B$ is important for improving the debiasing\nperformance of $f_D$. This is due to the fact that the bias-conflicting samples\nwork as noisy samples for amplifying the bias for $f_B$. To this end, we\npropose a novel biased sample selection method BiasEnsemble which removes the\nbias-conflicting samples via leveraging additional biased models to construct a\nbias-amplified dataset for training $f_B$. Our simple yet effective approach\ncan be directly applied to existing reweighting-based debiasing approaches,\nobtaining consistent performance boost and achieving the state-of-the-art\nperformance on both synthetic and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeonghoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A General Multiple Data Augmentation Based Framework for Training Deep Neural Networks. (arXiv:2205.14606v1 [cs.NE])","link":"http://arxiv.org/abs/2205.14606","description":"<p>Deep neural networks (DNNs) often rely on massive labelled data for training,\nwhich is inaccessible in many applications. Data augmentation (DA) tackles data\nscarcity by creating new labelled data from available ones. Different DA\nmethods have different mechanisms and therefore using their generated labelled\ndata for DNN training may help improving DNN's generalisation to different\ndegrees. Combining multiple DA methods, namely multi-DA, for DNN training,\nprovides a way to boost generalisation. Among existing multi-DA based DNN\ntraining methods, those relying on knowledge distillation (KD) have received\ngreat attention. They leverage knowledge transfer to utilise the labelled data\nsets created by multiple DA methods instead of directly combining them for\ntraining DNNs. However, existing KD-based methods can only utilise certain\ntypes of DA methods, incapable of utilising the advantages of arbitrary DA\nmethods. We propose a general multi-DA based DNN training framework capable to\nuse arbitrary DA methods. To train a DNN, our framework replicates a certain\nportion in the latter part of the DNN into multiple copies, leading to multiple\nDNNs with shared blocks in their former parts and independent blocks in their\nlatter parts. Each of these DNNs is associated with a unique DA and a newly\ndevised loss that allows comprehensively learning from the data generated by\nall DA methods and the outputs from all DNNs in an online and adaptive way. The\noverall loss, i.e., the sum of each DNN's loss, is used for training the DNN.\nEventually, one of the DNNs with the best validation performance is chosen for\ninference. We implement the proposed framework by using three distinct DA\nmethods and apply it for training representative DNNs. Experiments on the\npopular benchmarks of image classification demonstrate the superiority of our\nmethod to several existing single-DA and multi-DA based training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Binyan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1\">A. K. Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation. (arXiv:2205.14620v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14620","description":"<p>Prevailing video frame interpolation algorithms, that generate the\nintermediate frames from consecutive inputs, typically rely on complex model\narchitectures with heavy parameters or large delay, hindering them from diverse\nreal-time applications. In this work, we devise an efficient encoder-decoder\nbased network, termed IFRNet, for fast intermediate frame synthesizing. It\nfirst extracts pyramid features from given inputs, and then refines the\nbilateral intermediate flow fields together with a powerful intermediate\nfeature until generating the desired output. The gradually refined intermediate\nfeature can not only facilitate intermediate flow estimation, but also\ncompensate for contextual details, making IFRNet do not need additional\nsynthesis or refinement module. To fully release its potential, we further\npropose a novel task-oriented optical flow distillation loss to focus on\nlearning the useful teacher knowledge towards frame synthesizing. Meanwhile, a\nnew geometry consistency regularization term is imposed on the gradually\nrefined intermediate features to keep better structure layout. Experiments on\nvarious benchmarks demonstrate the excellent performance and fast inference\nspeed of proposed approaches. Code is available at\nhttps://github.com/ltkong218/IFRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingtong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Boyuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Donghao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wenqing Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SKFlow: Learning Optical Flow with Super Kernels. (arXiv:2205.14623v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14623","description":"<p>Optical flow estimation is a classical yet challenging task in computer\nvision. One of the essential factors in accurately predicting optical flow is\nto alleviate occlusions between frames. However, it is still a thorny problem\nfor current top-performing optical flow estimation methods due to insufficient\nlocal evidence to model occluded areas. In this paper, we propose Super Kernel\nFlow Network (SKFlow), a CNN architecture to ameliorate the impacts of\nocclusions on optical flow estimation. SKFlow benefits from the super kernels\nwhich bring enlarged receptive fields to complement the absent matching\ninformation and recover the occluded motions. We present efficient super kernel\ndesigns by utilizing conical connections and hybrid depth-wise convolutions.\nExtensive experiments demonstrate the effectiveness of SKFlow on multiple\nbenchmarks, especially in the occluded areas. Without pre-trained backbones on\nImageNet and with modest increase in computation, SKFlow achieves compelling\nperformance and ranks $\\textbf{1st}$ among current published methods on Sintel\nbenchmark. On the challenging Sintel final pass test set, SKFlow attains the\naverage end-point error of $2.23$, which surpasses the best published result\n$2.47$ by $9.72\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shangkun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cervical Glandular Cell Detection from Whole Slide Image with Out-Of-Distribution Data. (arXiv:2205.14625v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14625","description":"<p>Cervical glandular cell (GC) detection is a key step in computer-aided\ndiagnosis for cervical adenocarcinomas screening. It is challenging to\naccurately recognize GCs in cervical smears in which squamous cells are the\nmajor. Widely existing Out-Of-Distribution (OOD) data in the entire smear leads\ndecreasing reliability of machine learning system for GC detection. Although,\nthe State-Of-The-Art (SOTA) deep learning model can outperform pathologists in\npreselected regions of interest, the mass False Positive (FP) prediction with\nhigh probability is still unsolved when facing such gigapixel whole slide\nimage. This paper proposed a novel PolarNet based on the morphological prior\nknowledge of GC trying to solve the FP problem via a self-attention mechanism\nin eight-neighbor. It estimates the polar orientation of nucleus of GC. As a\nplugin module, PolarNet can guide the deep feature and predicted confidence of\ngeneral object detection models. In experiments, we discovered that general\nmodels based on four different frameworks can reject FP in small image set and\nincrease the mean of average precision (mAP) by $\\text{0.007}\\sim\\text{0.015}$\nin average, where the highest exceeds the recent cervical cell detection model\n0.037. By plugging PolarNet, the deployed C++ program improved by 8.8\\% on\naccuracy of top-20 GC detection from external WSIs, while sacrificing 14.4 s of\ncomputational time. Code is available in\n\\href{https://github.com/Chrisa142857/PolarNet-GCdet}{https://github.com/Chrisa142857/PolarNet-GCdet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shenghua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shaoqun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superclass Adversarial Attack. (arXiv:2205.14629v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14629","description":"<p>Adversarial attacks have only focused on changing the predictions of the\nclassifier, but their danger greatly depends on how the class is mistaken. For\nexample, when an automatic driving system mistakes a Persian cat for a Siamese\ncat, it is hardly a problem. However, if it mistakes a cat for a 120km/h\nminimum speed sign, serious problems can arise. As a stepping stone to more\nthreatening adversarial attacks, we consider the superclass adversarial attack,\nwhich causes misclassification of not only fine classes, but also superclasses.\nWe conducted the first comprehensive analysis of superclass adversarial attacks\n(an existing and 19 new methods) in terms of accuracy, speed, and stability,\nand identified several strategies to achieve better performance. Although this\nstudy is aimed at superclass misclassification, the findings can be applied to\nother problem settings involving multiple classes, such as top-k and\nmulti-label classification attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumano_S/0/1/0/all/0/1\">Soichiro Kumano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kera_H/0/1/0/all/0/1\">Hiroshi Kera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamasaki_T/0/1/0/all/0/1\">Toshihiko Yamasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceiving the Invisible: Proposal-Free Amodal Panoptic Segmentation. (arXiv:2205.14637v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14637","description":"<p>Amodal panoptic segmentation aims to connect the perception of the world to\nits cognitive understanding. It entails simultaneously predicting the semantic\nlabels of visible scene regions and the entire shape of traffic participant\ninstances, including regions that may be occluded. In this work, we formulate a\nproposal-free framework that tackles this task as a multi-label and multi-class\nproblem by first assigning the amodal masks to different layers according to\ntheir relative occlusion order and then employing amodal instance regression on\neach layer independently while learning background semantics. We propose the\n\\net architecture that incorporates a shared backbone and an asymmetrical\ndual-decoder consisting of several modules to facilitate within-scale and\ncross-scale feature aggregations, bilateral feature propagation between\ndecoders, and integration of global instance-level and local pixel-level\nocclusion reasoning. Further, we propose the amodal mask refiner that resolves\nthe ambiguity in complex occlusion scenarios by explicitly leveraging the\nembedding of unoccluded instance masks. Extensive evaluation on the BDD100K-APS\nand KITTI-360-APS datasets demonstrate that our approach set the new\nstate-of-the-art on both benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1\">Rohit Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Micro-Expression Recognition Based on Attribute Information Embedding and Cross-modal Contrastive Learning. (arXiv:2205.14643v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14643","description":"<p>Facial micro-expressions recognition has attracted much attention recently.\nMicro-expressions have the characteristics of short duration and low intensity,\nand it is difficult to train a high-performance classifier with the limited\nnumber of existing micro-expressions. Therefore, recognizing micro-expressions\nis a challenge task. In this paper, we propose a micro-expression recognition\nmethod based on attribute information embedding and cross-modal contrastive\nlearning. We use 3D CNN to extract RGB features and FLOW features of\nmicro-expression sequences and fuse them, and use BERT network to extract text\ninformation in Facial Action Coding System. Through cross-modal contrastive\nloss, we embed attribute information in the visual network, thereby improving\nthe representation ability of micro-expression recognition in the case of\nlimited samples. We conduct extensive experiments in CASME II and MMEW\ndatabases, and the accuracy is 77.82% and 71.04%, respectively. The comparative\nexperiments show that this method has better recognition effect than other\nmethods for micro-expression recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yanxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianbo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhangcheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COFS: Controllable Furniture layout Synthesis. (arXiv:2205.14657v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14657","description":"<p>Scalable generation of furniture layouts is essential for many applications\nin virtual reality, augmented reality, game development and synthetic data\ngeneration. Many existing methods tackle this problem as a sequence generation\nproblem which imposes a specific ordering on the elements of the layout making\nsuch methods impractical for interactive editing or scene completion.\nAdditionally, most methods focus on generating layouts unconditionally and\noffer minimal control over the generated layouts. We propose COFS, an\narchitecture based on standard transformer architecture blocks from language\nmodeling. The proposed model is invariant to object order by design, removing\nthe unnatural requirement of specifying an object generation order.\nFurthermore, the model allows for user interaction at multiple levels enabling\nfine grained control over the generation process. Our model consistently\noutperforms other methods which we verify by performing quantitative\nevaluations. Our method is also faster to train and sample from, compared to\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Para_W/0/1/0/all/0/1\">Wamiq Reyaz Para</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glance to Count: Learning to Rank with Anchors for Weakly-supervised Crowd Counting. (arXiv:2205.14659v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14659","description":"<p>Crowd image is arguably one of the most laborious data to annotate. In this\npaper, we devote to reduce the massive demand of densely labeled crowd data,\nand propose a novel weakly-supervised setting, in which we leverage the binary\nranking of two images with high-contrast crowd counts as training guidance. To\nenable training under this new setting, we convert the crowd count regression\nproblem to a ranking potential prediction problem. In particular, we tailor a\nSiamese Ranking Network that predicts the potential scores of two images\nindicating the ordering of the counts. Hence, the ultimate goal is to assign\nappropriate potentials for all the crowd images to ensure their orderings obey\nthe ranking labels. On the other hand, potentials reveal the relative crowd\nsizes but cannot yield an exact crowd count. We resolve this problem by\nintroducing \"anchors\" during the inference stage. Concretely, anchors are a few\nimages with count labels used for referencing the corresponding counts from\npotential scores by a simple linear mapping function. We conduct extensive\nexperiments to study various combinations of supervision, and we show that the\nproposed method outperforms existing weakly-supervised methods without\nadditional labeling effort by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zheng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Liangyu Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency Map Based Data Augmentation. (arXiv:2205.14686v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14686","description":"<p>Data augmentation is a commonly applied technique with two seemingly related\nadvantages. With this method one can increase the size of the training set\ngenerating new samples and also increase the invariance of the network against\nthe applied transformations. Unfortunately all images contain both relevant and\nirrelevant features for classification therefore this invariance has to be\nclass specific. In this paper we will present a new method which uses saliency\nmaps to restrict the invariance of neural networks to certain regions,\nproviding higher test accuracy in classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_afandi_J/0/1/0/all/0/1\">Jalal Al-afandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magyar_B/0/1/0/all/0/1\">B&#xe1;lint Magyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_A/0/1/0/all/0/1\">Andr&#xe1;s Horv&#xe1;th</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition. (arXiv:2205.14756v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14756","description":"<p>Vision Transformer (ViT) has achieved remarkable performance in many vision\ntasks. However, ViT is inferior to convolutional neural networks (CNNs) when\ntargeting high-resolution mobile vision applications. The key computational\nbottleneck of ViT is the softmax attention module which has quadratic\ncomputational complexity with the input resolution. It is essential to reduce\nthe cost of ViT to deploy it on edge devices. Existing methods (e.g., Swin,\nPVT) restrict the softmax attention within local windows or reduce the\nresolution of key/value tensors to reduce the cost, which sacrifices ViT's core\nadvantages on global feature extractions. In this work, we present\nEfficientViT, an efficient ViT architecture for high-resolution low-computation\nvisual recognition. Instead of restricting the softmax attention, we propose to\nreplace softmax attention with linear attention while enhancing its local\nfeature extraction ability with depthwise convolution. EfficientViT maintains\nglobal and local feature extraction capability while enjoying linear\ncomputational complexity. Extensive experiments on COCO object detection and\nCityscapes semantic segmentation demonstrate the effectiveness of our method.\nOn the COCO dataset, EfficientViT achieves 42.6 AP with 4.4G MACs, surpassing\nEfficientDet-D1 by 2.4 AP while having 27.9% fewer MACs. On Cityscapes,\nEfficientViT reaches 78.7 mIoU with 19.1G MACs, outperforming SegFormer by 2.5\nmIoU while requiring less than 1/3 the computational cost. On Qualcomm\nSnapdragon 855 CPU, EfficientViT is 3x faster than EfficientNet while achieving\nhigher ImageNet accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Han Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Uncertainty Quantification for Segmentation with Multiple Annotations. (arXiv:1907.01949v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1907.01949","description":"<p>The accurate estimation of predictive uncertainty carries importance in\nmedical scenarios such as lung node segmentation. Unfortunately, most existing\nworks on predictive uncertainty do not return calibrated uncertainty estimates,\nwhich could be used in practice. In this work we exploit multi-grader\nannotation variability as a source of 'groundtruth' aleatoric uncertainty,\nwhich can be treated as a target in a supervised learning problem. We combine\nthis groundtruth uncertainty with a Probabilistic U-Net and test on the\nLIDC-IDRI lung nodule CT dataset and MICCAI2012 prostate MRI dataset. We find\nthat we are able to improve predictive uncertainty estimates. We also find that\nwe can improve sample accuracy and sample diversity. In real-world\napplications, our method could inform doctors about the confidence of the\nsegmentation results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worrall_D/0/1/0/all/0/1\">Daniel Worrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knegt_S/0/1/0/all/0/1\">Stefan Knegt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1\">Bas Veeling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huisman_H/0/1/0/all/0/1\">Henkjan Huisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recovery of Future Data via Convolution Nuclear Norm Minimization. (arXiv:1909.03889v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1909.03889","description":"<p>This paper studies the problem of time series forecasting (TSF) from the\nperspective of compressed sensing. First of all, we convert TSF into a more\ninclusive problem called tensor completion with arbitrary sampling (TCAS),\nwhich is to restore a tensor from a subset of its entries sampled in an\narbitrary manner. While it is known that, in the framework of Tucker\nlow-rankness, it is theoretically impossible to identify the target tensor\nbased on some arbitrarily selected entries, in this work we shall show that\nTCAS is indeed tackleable in the light of a new concept called convolutional\nlow-rankness, which is a generalization of the well-known Fourier sparsity.\nThen we introduce a convex program termed Convolution Nuclear Norm Minimization\n(CNNM), and we prove that CNNM succeeds in solving TCAS as long as a sampling\ncondition--which depends on the convolution rank of the target tensor--is\nobeyed. This theory provides a meaningful answer to the fundamental question of\nwhat is the minimum sampling size needed for making a given number of\nforecasts. Experiments on univariate time series, images and videos show\nencouraging results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangcan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L6DNet: Light 6 DoF Network for Robust and Precise Object Pose Estimation with Small Datasets. (arXiv:2002.00911v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.00911","description":"<p>Estimating the 3D pose of an object is a challenging task that can be\nconsidered within augmented reality or robotic applications. In this paper, we\npropose a novel approach to perform 6 DoF object pose estimation from a single\nRGB-D image. We adopt a hybrid pipeline in two stages: data-driven and\ngeometric respectively. The data-driven step consists of a classification CNN\nto estimate the object 2D location in the image from local patches, followed by\na regression CNN trained to predict the 3D location of a set of keypoints in\nthe camera coordinate system. To extract the pose information, the geometric\nstep consists in aligning the 3D points in the camera coordinate system with\nthe corresponding 3D points in world coordinate system by minimizing a\nregistration error, thus computing the pose. Our experiments on the standard\ndataset LineMod show that our approach is more robust and accurate than\nstate-of-the-art methods. The approach is also validated to achieve a 6 DoF\npositioning task by visual servoing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mathieu Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacete_A/0/1/0/all/0/1\">Amine Kacete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murienne_A/0/1/0/all/0/1\">Albert Murienne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchand_E/0/1/0/all/0/1\">Eric Marchand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bayesian approach to tissue-fraction estimation for oncological PET segmentation. (arXiv:2003.00317v3 [physics.med-ph] UPDATED)","link":"http://arxiv.org/abs/2003.00317","description":"<p>Tumor segmentation in oncological PET is challenging, a major reason being\nthe partial-volume effects that arise due to low system resolution and finite\nvoxel size. The latter results in tissue-fraction effects, i.e. voxels contain\na mixture of tissue classes. Conventional segmentation methods are typically\ndesigned to assign each voxel in the image as belonging to a certain tissue\nclass. Thus, these methods are inherently limited in modeling tissue-fraction\neffects. To address the challenge of accounting for partial-volume effects, and\nin particular, tissue-fraction effects, we propose a Bayesian approach to\ntissue-fraction estimation for oncological PET segmentation. Specifically, this\nBayesian approach estimates the posterior mean of fractional volume that the\ntumor occupies within each voxel of the image. The proposed method, implemented\nusing a deep-learning-based technique, was first evaluated using clinically\nrealistic 2-D simulation studies with known ground truth, in the context of\nsegmenting the primary tumor in PET images of patients with lung cancer. The\nevaluation studies demonstrated that the method accurately estimated the\ntumor-fraction areas and significantly outperformed widely used conventional\nPET segmentation methods, including a U-net-based method, on the task of\nsegmenting the tumor. In addition, the proposed method was relatively\ninsensitive to partial-volume effects and yielded reliable tumor segmentation\nfor different clinical-scanner configurations. The method was then evaluated\nusing clinical images of patients with stage IIB/III non-small cell lung cancer\nfrom ACRIN 6668/RTOG 0235 multi-center clinical trial. Here, the results showed\nthat the proposed method significantly outperformed all other considered\nmethods and yielded accurate tumor segmentation on patient images with Dice\nsimilarity coefficient (DSC) of 0.82 (95 % CI: [0.78, 0.86]).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Liu_Z/0/1/0/all/0/1\">Ziping Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mhlanga_J/0/1/0/all/0/1\">Joyce C. Mhlanga</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Laforest_R/0/1/0/all/0/1\">Richard Laforest</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Derenoncourt_P/0/1/0/all/0/1\">Paul-Robert Derenoncourt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Siegel_B/0/1/0/all/0/1\">Barry A. Siegel</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jha_A/0/1/0/all/0/1\">Abhinav K. Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedBoosting: Federated Learning with Gradient Protected Boosting for Text Recognition. (arXiv:2007.07296v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.07296","description":"<p>Typical machine learning approaches require centralized data for model\ntraining, which may not be possible where restrictions on data sharing are in\nplace due to, for instance, privacy and gradient protection. The recently\nproposed Federated Learning (FL) framework allows learning a shared model\ncollaboratively without data being centralized or shared among data owners.\nHowever, we show in this paper that the generalization ability of the joint\nmodel is poor on Non-Independent and Non-Identically Distributed (Non-IID)\ndata, particularly when the Federated Averaging (FedAvg) strategy is used due\nto the weight divergence phenomenon. Hence, we propose a novel boosting\nalgorithm for FL to address both the generalization and gradient leakage\nissues, as well as achieve faster convergence in gradient-based optimization.\nIn addition, a secure gradient sharing protocol using Homomorphic Encryption\n(HE) and Differential Privacy (DP) is introduced to defend against gradient\nleakage attack and avoid pairwise encryption that is not scalable. We\ndemonstrate the proposed Federated Boosting (FedBoosting) method achieves\nnoticeable improvements in both prediction accuracy and run-time efficiency in\na visual text recognition task on public benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingjing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xianghua Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoke Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yichuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape Generation. (arXiv:2008.05440v4 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2008.05440","description":"<p>D shape generation is a fundamental operation in computer graphics. While\nsignificant progress has been made, especially with recent deep generative\nmodels, it remains a challenge to synthesize high-quality shapes with rich\ngeometric details and complex structure, in a controllable manner. To tackle\nthis, we introduce DSG-Net, a deep neural network that learns a disentangled\nstructured and geometric mesh representation for 3D shapes, where two key\naspects of shapes, geometry, and structure, are encoded in a synergistic manner\nto ensure plausibility of the generated shapes, while also being disentangled\nas much as possible. This supports a range of novel shape generation\napplications with disentangled control, such as interpolation of structure\n(geometry) while keeping geometry (structure) unchanged. To achieve this, we\nsimultaneously learn structure and geometry through variational autoencoders\n(VAEs) in a hierarchical manner for both, with bijective mappings at each\nlevel. In this manner, we effectively encode geometry and structure in separate\nlatent spaces, while ensuring their compatibility: the structure is used to\nguide the geometry and vice versa. At the leaf level, the part geometry is\nrepresented using a conditional part VAE, to encode high-quality geometric\ndetails, guided by the structure context as the condition. Our method not only\nsupports controllable generation applications but also produces high-quality\nsynthesized shapes, outperforming state-of-the-art methods. The code has been\nreleased at https://github.com/IGLICT/DSG-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Few-Shot Generalization with Attributes. (arXiv:2012.05895v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.05895","description":"<p>Despite impressive progress in deep learning, generalizing far beyond the\ntraining distribution is an important open challenge. In this work, we consider\nfew-shot classification, and aim to shed light on what makes some novel classes\neasier to learn than others, and what types of learned representations\ngeneralize better. To this end, we define a new paradigm in terms of attributes\n-- simple building blocks of which concepts are formed -- as a means of\nquantifying the degree of relatedness of different concepts. Our empirical\nanalysis reveals that supervised learning generalizes poorly to new attributes,\nbut a combination of self-supervised pretraining with supervised finetuning\nleads to stronger generalization. The benefit of self-supervised pretraining\nand supervised finetuning is further investigated through controlled\nexperiments using random splits of the attribute space, and we find that\npredictability of test attributes provides an informative estimate of a model's\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triantafillou_E/0/1/0/all/0/1\">Eleni Triantafillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuan-Chieh Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1\">James Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snell_J/0/1/0/all/0/1\">Jake Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1\">Xaq Pitkow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1\">Andreas S. Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Momentum-Contrastive Pre-Training. (arXiv:2012.13154v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13154","description":"<p>Recently proposed adversarial self-supervised learning methods usually\nrequire big batches and long training epochs to extract robust features, which\nwill bring heavy computational overhead on platforms with limited resources. In\norder to help the network learn more powerful feature representations in\nsmaller batches and fewer epochs, this paper proposes a novel adversarial\nmomentum contrastive learning method, which introduces two memory banks\ncorresponding to clean samples and adversarial samples, respectively. These\nmemory banks can be dynamically incorporated into the training process to track\ninvariant features among historical mini-batches. Compared with the previous\nadversarial pre-training model, our method achieves superior performance with\nsmaller batch size and less training epochs. In addition, the model outperforms\nsome state-of-the-art supervised defensive methods on multiple benchmark\ndatasets after being fine-tuned on downstream classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-point dimensionality reduction to improve projection layout reliability. (arXiv:2101.06224v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06224","description":"<p>In ordinary Dimensionality Reduction (DR), each data instance in a high\ndimensional space (original space), or on a distance matrix denoting original\nspace distances, is mapped to (projected onto) one point in a low dimensional\nspace (visual space), building a layout of projected points trying to preserve\nas much as possible some property of data such as distances, neighbourhood\nrelationships, and/or topology structures, with the ultimate goal of\napproximating semantic properties of data with preserved geometric properties\nor topology structures in visual space. In this paper, the concept of\nMulti-point Dimensionality Reduction is elaborated on where each data instance\ncan be mapped to (projected onto) possibly more than one point in visual space\nby providing the first general solution (algorithm) for it as a move in the\ndirection of improving reliablity, usability and interpretability of\ndimensionality reduction. Furthermore by allowing the points in visual space to\nbe split into two layers while maintaining the possibility of having more than\none projection (mapping) per data instance , the benefit of separating more\nreliable points from less reliable points is dicussed notwithstanding the\neffort to improve less reliable points. The proposed solution (algorithm) in\nthis paper, named Layered Vertex Splitting Data Embedding (LVSDE), is built\nupon and extends a combination of ordinary DR and graph drawing techniques. On\nthe empirical side, this paper shows that the particular proposed algorithm\n(LVSDE) practically outperforms popular ordinary DR methods visually\n(semantics, group separation, subgroup detection or combinational group\ndetection) in a way that is easily explainable and performs in close proximity\nto top on most of the data sets studied in the paper in a quantitative analysis\nbased on KNN classification accuracy. [Abstract truncated for length]\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barahimi_F/0/1/0/all/0/1\">Farshad Barahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DivSwapper: Towards Diversified Patch-based Arbitrary Style Transfer. (arXiv:2101.06381v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06381","description":"<p>Gram-based and patch-based approaches are two important research lines of\nstyle transfer. Recent diversified Gram-based methods have been able to produce\nmultiple and diverse stylized outputs for the same content and style images.\nHowever, as another widespread research interest, the diversity of patch-based\nmethods remains challenging due to the stereotyped style swapping process based\non nearest patch matching. To resolve this dilemma, in this paper, we dive into\nthe crux of existing patch-based methods and propose a universal and efficient\nmodule, termed DivSwapper, for diversified patch-based arbitrary style\ntransfer. The key insight is to use an essential intuition that neural patches\nwith higher activation values could contribute more to diversity. Our\nDivSwapper is plug-and-play and can be easily integrated into existing\npatch-based and Gram-based methods to generate diverse results for arbitrary\nstyles. We conduct theoretical analyses and extensive experiments to\ndemonstrate the effectiveness of our method, and compared with state-of-the-art\nalgorithms, it shows superiority in diversity, quality, and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haibo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Z/0/1/0/all/0/1\">Zhiwen Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1\">Wei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dongming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepRA: Predicting Joint Damage From Radiographs Using CNN with Attention. (arXiv:2102.06982v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06982","description":"<p>Joint damage in Rheumatoid Arthritis (RA) is assessed by manually inspecting\nand grading radiographs of hands and feet. This is a tedious task which\nrequires trained experts whose subjective assessment leads to low inter-rater\nagreement. An algorithm which can automatically predict the joint level damage\nin hands and feet can help optimize this process, which will eventually aid the\ndoctors in better patient care and research. In this paper, we propose a\ntwo-staged approach which amalgamates object detection and convolution neural\nnetworks with attention which can efficiently and accurately predict the\noverall and joint level narrowing and erosion from patients radiographs. This\napproach has been evaluated on hands and feet radiographs of patients suffering\nfrom RA and has achieved a weighted root mean squared error (RMSE) of 1.358 and\n1.404 in predicting joint level narrowing and erosion Sharp van der Heijde\n(SvH) scores which is 31% and 19% improvement with respect to the baseline SvH\nscores, respectively. The proposed approach achieved a weighted absolute error\nof 1.456 in predicting the overall damage in hands and feet radiographs for the\npatients which is a 79% improvement as compared to the baseline. Our method\nalso provides an inherent capability to provide explanations for model\npredictions using attention weights, which is essential given the black box\nnature of deep learning models. The proposed approach was developed during the\nRA2 Dream Challenge hosted by Dream Challenges and secured 4th and 8th position\nin predicting overall and joint level narrowing and erosion SvH scores from\nradiographs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_N/0/1/0/all/0/1\">Neelambuj Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Image Generation by Conditioning Variational Auto-Encoders. (arXiv:2102.12037v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.12037","description":"<p>We present a conditional variational auto-encoder (VAE) which, to avoid the\nsubstantial cost of training from scratch, uses an architecture and training\nobjective capable of leveraging a foundation model in the form of a pretrained\nunconditional VAE. To train the conditional VAE, we only need to train an\nartifact to perform amortized inference over the unconditional VAE's latent\nvariables given a conditioning input. We demonstrate our approach on tasks\nincluding image inpainting, for which it outperforms state-of-the-art GAN-based\napproaches at faithfully representing the inherent uncertainty. We conclude by\ndescribing a possible application of our inpainting model, in which it is used\nto perform Bayesian experimental design for the purpose of guiding a sensor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harvey_W/0/1/0/all/0/1\">William Harvey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naderiparizi_S/0/1/0/all/0/1\">Saeid Naderiparizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09124","description":"<p>While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4\\%/16.3\\% of\nResNet-101/ResNet-50. Code is available at https://github.\ncom/Yuting-Gao/DisCo-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Map-Based Navigation Using Semantic Segmentation and Pattern Matching. (arXiv:2107.00689v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00689","description":"<p>This paper proposes a novel approach to map-based navigation system for\nunmanned aircraft. The proposed system attempts label-to-label matching, not\nimage-to-image matching, between aerial images and a map database. The ground\nobjects can be labelled by deep learning approaches and the configuration of\nthe objects is used to find the corresponding location in the map database. The\nuse of the deep learning technique as a tool for extracting high-level features\nreduces the image-based localization problem to a pattern matching problem.\nThis paper proposes a pattern matching algorithm that does not require altitude\ninformation or a camera model to estimate the absolute horizontal position. The\nfeasibility analysis with simulated images shows the proposed map-based\nnavigation can be realized with the proposed pattern matching algorithm and it\nis able to provide positions given the labelled objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11845","description":"<p>In this paper, we are concerned with image classification with deep\nconvolutional neural networks (CNNs). We focus on the following question: given\na set of candidate CNN models, how to select the right one with the best\ngeneralization property for the current task? Current model selection methods\nall require access to a batch of labeled data for computing a pre-specified\nperformance metric, such as the cross-entropy loss, the classification error\nrate and the negative log-likelihood. In many practical cases, labels are not\navailable in time as labeling itself is a time-consuming and expensive task. To\nthis end, we propose an approach to CNN model selection using only unlabeled\ndata. We develop this method based on a principle termed consistent relative\nconfidence. Experimental results on benchmark datasets demonstrate the\neffectiveness and efficiency of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite Relaxations and Scalable Global Optimization. (arXiv:2109.03349v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03349","description":"<p>We propose the first general and scalable framework to design certifiable\nalgorithms for robust geometric perception in the presence of outliers. Our\nfirst contribution is to show that estimation using common robust costs, such\nas truncated least squares (TLS), maximum consensus, Geman-McClure, Tukey's\nbiweight, among others, can be reformulated as polynomial optimization problems\n(POPs). By focusing on the TLS cost, our second contribution is to exploit\nsparsity in the POP and propose a sparse semidefinite programming (SDP)\nrelaxation that is much smaller than the standard Lasserre's hierarchy while\npreserving empirical exactness, i.e., the SDP recovers the optimizer of the\nnonconvex POP with an optimality certificate. Our third contribution is to\nsolve the SDP relaxations at an unprecedented scale and accuracy by presenting\nSTRIDE, a solver that blends global descent on the convex SDP with fast local\nsearch on the nonconvex POP. Our fourth contribution is an evaluation of the\nproposed framework on six geometric perception problems including single and\nmultiple rotation averaging, point cloud and mesh registration, absolute pose\nestimation, and category-level object pose and shape estimation. Our\nexperiments demonstrate that (i) our sparse SDP relaxation is empirically exact\nwith up to 60%-90% outliers across applications; (ii) while still being far\nfrom real-time, STRIDE is up to 100 times faster than existing SDP solvers on\nmedium-scale problems, and is the only solver that can solve large-scale SDPs\nwith hundreds of thousands of constraints to high accuracy; (iii) STRIDE\nsafeguards existing fast heuristics for robust estimation (e.g., RANSAC or\nGraduated Non-Convexity), i.e., it certifies global optimality if the heuristic\nestimates are optimal, or detects and allows escaping local optima when the\nheuristic estimates are suboptimal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?. (arXiv:2109.05422v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05422","description":"<p>Transformers have sprung up in the field of computer vision. In this work, we\nexplore whether the core self-attention module in Transformer is the key to\nachieving excellent performance in image recognition. To this end, we build an\nattention-free network called sMLPNet based on the existing MLP-based vision\nmodels. Specifically, we replace the MLP module in the token-mixing step with a\nnovel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along\nthe axial directions and the parameters are shared among rows or columns. By\nsparse connection and weight sharing, sMLP module significantly reduces the\nnumber of model parameters and computational complexity, avoiding the common\nover-fitting problem that plagues the performance of MLP-like models. When only\ntrained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1\naccuracy with only 24M parameters, which is much better than most CNNs and\nvision Transformers under the same model size constraint. When scaling up to\n66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the\nstate-of-the-art Swin Transformer. The success of sMLPNet suggests that the\nself-attention mechanism is not necessarily a silver bullet in computer vision.\nThe code and models are publicly available at\nhttps://github.com/microsoft/SPACH\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chuanxin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yucheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wenxuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Unsupervised Learning of Visual Representations and Categories. (arXiv:2109.05675v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05675","description":"<p>Real world learning scenarios involve a nonstationary distribution of classes\nwith sequential dependencies among the samples, in contrast to the standard\nmachine learning formulation of drawing samples independently from a fixed,\ntypically uniform distribution. Furthermore, real world interactions demand\nlearning on-the-fly from few or no class labels. In this work, we propose an\nunsupervised model that simultaneously performs online visual representation\nlearning and few-shot learning of new categories without relying on any class\nlabels. Our model is a prototype-based memory network with a control component\nthat determines when to form a new class prototype. We formulate it as an\nonline mixture model, where components are created with only a single new\nexample, and assignments do not have to be balanced, which permits an\napproximation to natural imbalanced distributions from uncurated raw data.\nLearning includes a contrastive loss that encourages different views of the\nsame image to be assigned to the same prototype. The result is a mechanism that\nforms categorical representations of objects in nonstationary environments.\nExperiments show that our method can learn from an online stream of visual\ninput data and its learned representations are significantly better at category\nrecognition compared to state-of-the-art self-supervised learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_T/0/1/0/all/0/1\">Tyler R. Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iuzzolino_M/0/1/0/all/0/1\">Michael L. Iuzzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAFNe: A One-Stage Anchor-Free Approach for Oriented Object Detection. (arXiv:2109.06148v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06148","description":"<p>We present DAFNe, a Dense one-stage Anchor-Free deep Network for oriented\nobject detection. As a one-stage model, it performs bounding box predictions on\na dense grid over the input image, being architecturally simpler in design, as\nwell as easier to optimize than its two-stage counterparts. Furthermore, as an\nanchor-free model, it reduces the prediction complexity by refraining from\nemploying bounding box anchors. With DAFNe we introduce an orientation-aware\ngeneralization of the center-ness function for arbitrarily oriented bounding\nboxes to down-weight low-quality predictions and a center-to-corner bounding\nbox prediction strategy that improves object localization performance. Our\nexperiments show that DAFNe outperforms all previous one-stage anchor-free\nmodels on DOTA 1.0, DOTA 1.5, and UCAS-AOD and is on par with the best models\non HRSC2016.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_S/0/1/0/all/0/1\">Steven Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1\">Fabrizio Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Dense Video Grounding via Parallel Regression. (arXiv:2109.11265v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11265","description":"<p>Video grounding aims to localize the corresponding video moment in an\nuntrimmed video given a language query. Existing methods often address this\ntask in an indirect way, by casting it as a proposal-and-match or\nfusion-and-detection problem. Solving these surrogate problems often requires\nsophisticated label assignment during training and hand-crafted removal of\nnear-duplicate results. Meanwhile, existing works typically focus on sparse\nvideo grounding with a single sentence as input, which could result in\nambiguous localization due to its unclear description. In this paper, we tackle\na new problem of dense video grounding, by simultaneously localizing multiple\nmoments with a paragraph as input. From a perspective on video grounding as\nlanguage conditioned regression, we present an end-to-end parallel decoding\nparadigm by re-purposing a Transformer-alike architecture (PRVG). The key\ndesign in our PRVG is to use languages as queries, and directly regress the\nmoment boundaries based on language-modulated visual representations. Thanks to\nits simplicity in design, our PRVG framework can be applied in different\ntesting schemes (sparse or dense grounding) and allows for efficient inference\nwithout any post-processing technique. In addition, we devise a robust\nproposal-level attention loss to guide the training of PRVG, which is invariant\nto moment duration and contributes to model convergence. We perform experiments\non two video grounding benchmarks of ActivityNet Captions and TACoS,\ndemonstrating that our PRVG can significantly outperform previous methods. We\nalso perform in-depth studies to investigate the effectiveness of parallel\nregression paradigm on video grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Fengyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Camera Human Motion Transfer by Time Series Analysis. (arXiv:2109.14174v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14174","description":"<p>Along with advances in optical sensors is the increasingly common practice of\nbuilding an imaging system with heterogeneous cameras. While high-resolution\n(HR) video acquisition and analysis benefit from hybrid sensors, the intrinsic\ncharacteristics of multiple cameras lead to a challenging motion transfer\nproblem. In this paper, we propose an algorithm using time series analysis for\nmotion transfer among multiple cameras. Specifically, we first identify\nseasonality in the motion data, and then build an additive time series model to\nextract patterns that could be transferred across different cameras. Our\napproach has a complete and clear mathematical formulation, and the algorithm\nis also efficient and interpretable. Through the experiment on real-world data,\nwe demonstrate the effectiveness of our method. Furthermore, our motion\ntransfer algorithm could combine with and facilitate downstream tasks, e.g.,\nenhancing pose estimation on low-resolution (LR) videos with inherent patterns\nextracted from HR ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_E/0/1/0/all/0/1\">Edmund Y. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sharpness-aware Minimization for Improved Training of Neural Networks. (arXiv:2110.03141v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2110.03141","description":"<p>Overparametrized Deep Neural Networks (DNNs) often achieve astounding\nperformances, but may potentially result in severe generalization error.\nRecently, the relation between the sharpness of the loss landscape and the\ngeneralization error has been established by Foret et al. (2020), in which the\nSharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the\ngeneralization. Unfortunately, SAM s computational cost is roughly double that\nof base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus\nproposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM s\nefficiency at no cost to its generalization performance. ESAM includes two\nnovel and efficient training strategies-StochasticWeight Perturbation and\nSharpness-Sensitive Data Selection. In the former, the sharpness measure is\napproximated by perturbing a stochastically chosen set of weights in each\niteration; in the latter, the SAM loss is optimized using only a judiciously\nselected subset of data that is sensitive to the sharpness. We provide\ntheoretical explanations as to why these strategies perform well. We also show,\nvia extensive experiments on the CIFAR and ImageNet datasets, that ESAM\nenhances the efficiency over SAM from requiring 100% extra computations to 40%\nvis-a-vis base optimizers, while test accuracies are preserved or even\nimproved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanshu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_L/0/1/0/all/0/1\">Liangli Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Molecular Graph Representation with 3D Geometry. (arXiv:2110.07728v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.07728","description":"<p>Molecular graph representation learning is a fundamental problem in modern\ndrug and material discovery. Molecular graphs are typically modeled by their 2D\ntopological structures, but it has been recently discovered that 3D geometric\ninformation plays a more vital role in predicting molecular functionalities.\nHowever, the lack of 3D information in real-world scenarios has significantly\nimpeded the learning of geometric graph representation. To cope with this\nchallenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework\nwhere self-supervised learning (SSL) is performed by leveraging the\ncorrespondence and consistency between 2D topological structures and 3D\ngeometric views. GraphMVP effectively learns a 2D molecular graph encoder that\nis enhanced by richer and more discriminative 3D geometry. We further provide\ntheoretical insights to justify the effectiveness of GraphMVP. Finally,\ncomprehensive experiments show that GraphMVP can consistently outperform\nexisting graph SSL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-free unsupervised domain adaptation for cross-modality abdominal multi-organ segmentation. (arXiv:2111.12221v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12221","description":"<p>Domain adaptation is crucial for transferring the knowledge from the source\nlabeled CT dataset to the target unlabeled MR dataset in abdominal multi-organ\nsegmentation. Meanwhile, it is highly desirable to avoid the high annotation\ncost related to the target dataset and protect the source dataset privacy.\nTherefore, we propose an effective source-free unsupervised domain adaptation\nmethod for cross-modality abdominal multi-organ segmentation without source\ndataset access. The proposed framework comprises two stages. In the first\nstage, the feature map statistics-guided model adaptation combined with entropy\nminimization is developed to help the top segmentation network reliably segment\nthe target images. The pseudo-labels output from the top segmentation network\nare used to guide the style compensation network to generate source-like\nimages. The pseudo-labels output from the middle segmentation network is used\nto supervise the learning progress of the desired model (bottom segmentation\nnetwork). In the second stage, the circular learning and pixel-adaptive mask\nrefinement are used to further improve the desired model performance. With this\napproach, we achieved satisfactory abdominal multi-organ segmentation\nperformance, outperforming the existing state-of-the-art domain adaptation\nmethods. The proposed approach can be easily extended to situations in which\ntarget annotation data exist. With only one labeled MR volume, the performance\ncan be levelled with that of supervised learning. Furthermore, the proposed\napproach is proven to be effective for source-free unsupervised domain\nadaptation in reverse direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu-Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weitian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hallucinated Neural Radiance Fields in the Wild. (arXiv:2111.15246v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15246","description":"<p>Neural Radiance Fields (NeRF) has recently gained popularity for its\nimpressive novel view synthesis ability. This paper studies the problem of\nhallucinated NeRF: i.e., recovering a realistic NeRF at a different time of day\nfrom a group of tourism images. Existing solutions adopt NeRF with a\ncontrollable appearance embedding to render novel views under various\nconditions, but they cannot render view-consistent images with an unseen\nappearance. To solve this problem, we present an end-to-end framework for\nconstructing a hallucinated NeRF, dubbed as Ha-NeRF. Specifically, we propose\nan appearance hallucination module to handle time-varying appearances and\ntransfer them to novel views. Considering the complex occlusions of tourism\nimages, we introduce an anti-occlusion module to decompose the static subjects\nfor visibility accurately. Experimental results on synthetic data and real\ntourism photo collections demonstrate that our method can hallucinate the\ndesired appearances and render occlusion-free images from different views. The\nproject and supplementary materials are available at\nhttps://rover-xingyu.github.io/Ha-NeRF/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Ying Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trimap-guided Feature Mining and Fusion Network for Natural Image Matting. (arXiv:2112.00510v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00510","description":"<p>Utilizing trimap guidance and fusing multi-level features are two important\nissues for trimap-based matting with pixel-level prediction. To utilize trimap\nguidance, most existing approaches simply concatenate trimaps and images\ntogether to feed a deep network or apply an extra network to extract more\ntrimap guidance, which meets the conflict between efficiency and effectiveness.\nFor emerging content-based feature fusion, most existing matting methods only\nfocus on local features which lack the guidance of a global feature with strong\nsemantic information related to the interesting object. In this paper, we\npropose a trimap-guided feature mining and fusion network consisting of our\ntrimap-guided non-background multi-scale pooling (TMP) module and global-local\ncontext-aware fusion (GLF) modules. Considering that trimap provides strong\nsemantic guidance, our TMP module focuses effective feature mining on\ninteresting objects under the guidance of trimap without extra parameters.\nFurthermore, our GLF modules use global semantic information of interesting\nobjects mined by our TMP module to guide an effective global-local\ncontext-aware multi-level feature fusion. In addition, we build a common\ninteresting object matting (CIOM) dataset to advance high-quality image\nmatting. Particularly, results on the Composition-1k and our CIOM show that our\nTMFNet achieves 13% and 25% relative improvement on SAD, respectively, against\na strong baseline with fewer parameters and 14% fewer FLOPs. Experimental\nresults on the Composition-1k test set, Alphamatting benchmark, and our CIOM\ntest set demonstrate that our method outperforms state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weihao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhaozhi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaoyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongtao Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Retinex Fusion for Adaptive Infrared and Visible Image Super-resolution Fusion. (arXiv:2112.02869v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02869","description":"<p>Convolutional neural networks have turned into an illustrious tool for image\nfusion and super-resolution. However, their excellent performance cannot work\nwithout large fixed-paired datasets; and additionally, these high-demanded\nground truth data always cannot be obtained easily in fusion tasks. In this\nstudy, we show that, the structures of generative networks capture a great deal\nof image feature priors, and then these priors are sufficient to reconstruct\nhigh-quality fused super-resolution result using only low-resolution inputs. By\nthis way, we propose a novel self-supervised dataset-free method for adaptive\ninfrared (IR) and visible (VIS) image super-resolution fusion named Deep\nRetinex Fusion (DRF). The key idea of DRF is first generating component priors\nwhich are disentangled from physical model using our designed generative\nnetworks ZipperNet, LightingNet and AdjustingNet, then combining these priors\nwhich captured by networks via adaptive fusion loss functions based on Retinex\ntheory, and finally reconstructing the super-resolution fusion results.\nFurthermore, in order to verify the effectiveness of our reported DRF, both\nqualitative and quantitative experiments via comparing with other\nstate-of-the-art methods are performed using different test sets. These results\nprove that, comparing with large datasets trained methods, DRF which works\nwithout any dataset achieves the best super-resolution fusion performance; and\nmore importantly, DRF can adaptively balance IR and VIS information and has\ngood noise immunity. DRF codes are open source available at\nhttps://github.com/GuYuanjie/Deep-Retinex-fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuanjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhibo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hailun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shouyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-FasterSeg: An Efficient Semantic Segmentation Network Based on Neural Architecture Search. (arXiv:2112.07918v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07918","description":"<p>Image semantic segmentation technology is one of the key technologies for\nintelligent systems to understand natural scenes. As one of the important\nresearch directions in the field of visual intelligence, this technology has\nbroad application scenarios in the fields of mobile robots, drones, smart\ndriving, and smart security. However, in the actual application of mobile\nrobots, problems such as inaccurate segmentation semantic label prediction and\nloss of edge information of segmented objects and background may occur. This\npaper proposes an improved structure of a semantic segmentation network based\non a deep learning network that combines self-attention neural network and\nneural network architecture search methods. First, a neural network search\nmethod NAS (Neural Architecture Search) is used to find a semantic segmentation\nnetwork with multiple resolution branches. In the search process, combine the\nself-attention network structure module to adjust the searched neural network\nstructure, and then combine the semantic segmentation network searched by\ndifferent branches to form a fast semantic segmentation network structure, and\ninput the picture into the network structure to get the final forecast result.\nThe experimental results on the Cityscapes dataset show that the accuracy of\nthe algorithm is 69.8%, and the segmentation speed is 48/s. It achieves a good\nbalance between real-time and accuracy, can optimize edge segmentation, and has\na better performance in complex scenes. Good robustness is suitable for\npractical application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junjun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1\">Huiyu Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qingwu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xilin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoman Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point2Cyl: Reverse Engineering 3D Objects from Point Clouds to Extrusion Cylinders. (arXiv:2112.09329v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09329","description":"<p>We propose Point2Cyl, a supervised network transforming a raw 3D point cloud\nto a set of extrusion cylinders. Reverse engineering from a raw geometry to a\nCAD model is an essential task to enable manipulation of the 3D data in shape\nediting software and thus expand their usages in many downstream applications.\nParticularly, the form of CAD models having a sequence of extrusion cylinders\n-- a 2D sketch plus an extrusion axis and range -- and their boolean\ncombinations is not only widely used in the CAD community/software but also has\ngreat expressivity of shapes, compared to having limited types of primitives\n(e.g., planes, spheres, and cylinders). In this work, we introduce a neural\nnetwork that solves the extrusion cylinder decomposition problem in a\ngeometry-grounded way by first learning underlying geometric proxies.\nPrecisely, our approach first predicts per-point segmentation, base/barrel\nlabels and normals, then estimates for the underlying extrusion parameters in\ndifferentiable and closed-form formulations. Our experiments show that our\napproach demonstrates the best performance on two recent CAD datasets, Fusion\nGallery and DeepCAD, and we further showcase our approach on reverse\nengineering and editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uy_M/0/1/0/all/0/1\">Mikaela Angelina Uy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-yu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Purvi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph Lambourne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flow-Guided Sparse Transformer for Video Deblurring. (arXiv:2201.01893v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.01893","description":"<p>Exploiting similar and sharper scene patches in spatio-temporal neighborhoods\nis critical for video deblurring. However, CNN-based methods show limitations\nin capturing long-range dependencies and modeling non-local self-similarity. In\nthis paper, we propose a novel framework, Flow-Guided Sparse Transformer\n(FGST), for video deblurring. In FGST, we customize a self-attention module,\nFlow-Guided Sparse Window-based Multi-head Self-Attention (FGSW-MSA). For each\n$query$ element on the blurry reference frame, FGSW-MSA enjoys the guidance of\nthe estimated optical flow to globally sample spatially sparse yet highly\nrelated $key$ elements corresponding to the same scene patch in neighboring\nframes. Besides, we present a Recurrent Embedding (RE) mechanism to transfer\ninformation from past frames and strengthen long-range temporal dependencies.\nComprehensive experiments demonstrate that our proposed FGST outperforms\nstate-of-the-art (SOTA) methods on both DVD and GOPRO datasets and even yields\nmore visually pleasing results in real video deblurring. Code and pre-trained\nmodels are publicly available at https://github.com/linjing7/VR-Baseline\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_X/0/1/0/all/0/1\">Xueyi Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World. (arXiv:2201.08619v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08619","description":"<p>Deep learning models have been shown to be vulnerable to recent backdoor\nattacks. A backdoored model behaves normally for inputs containing no\nattacker-secretly-chosen trigger and maliciously for inputs with the trigger.\nTo date, backdoor attacks and countermeasures mainly focus on image\nclassification tasks. And most of them are implemented in the digital world\nwith digital triggers. Besides the classification tasks, object detection\nsystems are also considered as one of the basic foundations of computer vision\ntasks. However, there is no investigation and understanding of the backdoor\nvulnerability of the object detector, even in the digital world with digital\ntriggers. For the first time, this work demonstrates that existing object\ndetectors are inherently susceptible to physical backdoor attacks. We use a\nnatural T-shirt bought from a market as a trigger to enable the cloaking\neffect--the person bounding-box disappears in front of the object detector. We\nshow that such a backdoor can be implanted from two exploitable attack\nscenarios into the object detector, which is outsourced or fine-tuned through a\npretrained model. We have extensively evaluated three popular object detection\nalgorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building\nupon 19 videos shot in real-world scenes, we confirm that the backdoor attack\nis robust against various factors: movement, distance, angle, non-rigid\ndeformation, and lighting. Specifically, the attack success rate (ASR) in most\nvideos is 100% or close to it, while the clean data accuracy of the backdoored\nmodel is the same as its clean counterpart. The latter implies that it is\ninfeasible to detect the backdoor behavior merely through a validation set. The\naveraged ASR still remains sufficiently high to be 78% in the transfer learning\nattack scenarios evaluated on CenterNet. See the demo video on\nhttps://youtu.be/Q3HOF4OobbY.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hua Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yansong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1\">Alsharif Abuadbba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1\">Anmin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyoungshick Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Sarawi_S/0/1/0/all/0/1\">Said F. Al-Sarawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1\">Nepal Surya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_D/0/1/0/all/0/1\">Derek Abbott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Sketch Based Image Retrieval using Graph Transformer. (arXiv:2201.10185v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10185","description":"<p>The performance of a zero-shot sketch-based image retrieval (ZS-SBIR) task is\nprimarily affected by two challenges. The substantial domain gap between image\nand sketch features needs to be bridged, while at the same time the side\ninformation has to be chosen tactfully. Existing literature has shown that\nvarying the semantic side information greatly affects the performance of\nZS-SBIR. To this end, we propose a novel graph transformer based zero-shot\nsketch-based image retrieval (GTZSR) framework for solving ZS-SBIR tasks which\nuses a novel graph transformer to preserve the topology of the classes in the\nsemantic space and propagates the context-graph of the classes within the\nembedding features of the visual space. To bridge the domain gap between the\nvisual features, we propose minimizing the Wasserstein distance between images\nand sketches in a learned domain-shared space. We also propose a novel\ncompatibility loss that further aligns the two visual domains by bridging the\ndomain gap of one class with respect to the domain gap of all other classes in\nthe training set. Experimental results obtained on the extended Sketchy,\nTU-Berlin, and QuickDraw datasets exhibit sharp improvements over the existing\nstate-of-the-art methods in both ZS-SBIR and generalized ZS-SBIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sumrit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_U/0/1/0/all/0/1\">Ushasi Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Image Deblurring: A Survey. (arXiv:2201.10700v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10700","description":"<p>Image deblurring is a classic problem in low-level computer vision with the\naim to recover a sharp image from a blurred input image. Advances in deep\nlearning have led to significant progress in solving this problem, and a large\nnumber of deblurring networks have been proposed. This paper presents a\ncomprehensive and timely survey of recently published deep-learning based image\ndeblurring approaches, aiming to serve the community as a useful literature\nreview. We start by discussing common causes of image blur, introduce benchmark\ndatasets and performance metrics, and summarize different problem formulations.\nNext, we present a taxonomy of methods using convolutional neural networks\n(CNN) based on architecture, loss function, and application, offering a\ndetailed review and comparison. In addition, we discuss some domain-specific\ndeblurring applications including face images, text, and stereo image pairs. We\nconclude by discussing key challenges and future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Sheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenger_B/0/1/0/all/0/1\">Bjorn Stenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Tasks Siamese Transformer Framework for Building Damage Assessment. (arXiv:2201.10953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10953","description":"<p>Accurate and fine-grained information about the extent of damage to buildings\nis essential for humanitarian relief and disaster response. However, as the\nmost commonly used architecture in remote sensing interpretation tasks,\nConvolutional Neural Networks (CNNs) have limited ability to model the\nnon-local relationship between pixels. Recently, Transformer architecture first\nproposed for modeling long-range dependency in natural language processing has\nshown promising results in computer vision tasks. Considering the frontier\nadvances of Transformer architecture in the computer vision field, in this\npaper, we present the first attempt at designing a Transformer-based damage\nassessment architecture (DamFormer). In DamFormer, a siamese Transformer\nencoder is first constructed to extract non-local and representative deep\nfeatures from input multitemporal image-pairs. Then, a multitemporal fusion\nmodule is designed to fuse information for downstream tasks. Finally, a\nlightweight dual-tasks decoder aggregates multi-level features for final\nprediction. To the best of our knowledge, it is the first time that such a deep\nTransformer-based network is proposed for multitemporal remote sensing\ninterpretation tasks. The experimental results on the large-scale damage\nassessment dataset xBD demonstrate the potential of the Transformer-based\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemni_E/0/1/0/all/0/1\">Edoardo Nemni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallecorsa_S/0/1/0/all/0/1\">Sofia Vallecorsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bromley_L/0/1/0/all/0/1\">Lars Bromley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Compose Diversified Prompts for Image Emotion Classification. (arXiv:2201.10963v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10963","description":"<p>Contrastive Language-Image Pre-training (CLIP) represents the latest\nincarnation of pre-trained vision-language models. Although CLIP has recently\nshown its superior power on a wide range of downstream vision-language tasks\nlike Visual Question Answering, it is still underexplored for Image Emotion\nClassification (IEC). Adapting CLIP to the IEC task has three significant\nchallenges, tremendous training objective gap between pretraining and IEC,\nshared suboptimal and invariant prompts for all instances. In this paper, we\npropose a general framework that shows how CLIP can be effectively applied to\nIEC. We first introduce a prompt tuning method that mimics the pretraining\nobjective of CLIP and thus can leverage the rich image and text semantics\nentailed in CLIP. Then we automatically compose instance-specific prompts by\nconditioning them on the categories and image contents of instances,\ndiversifying prompts and avoiding suboptimal problems. Evaluations on six\nwidely-used affective datasets demonstrate that our proposed method outperforms\nthe state-of-the-art methods to a large margin (i.e., up to 9.29% accuracy gain\non EmotionROI dataset) on IEC tasks, with only a few parameters trained. Our\ncodes will be publicly available for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Sinuo Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lifang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Ge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lehao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_M/0/1/0/all/0/1\">Meng Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Ye Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Invariant Representation Learning from EEG with Private Encoders. (arXiv:2201.11613v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.11613","description":"<p>Deep learning based electroencephalography (EEG) signal processing methods\nare known to suffer from poor test-time generalization due to the changes in\ndata distribution. This becomes a more challenging problem when\nprivacy-preserving representation learning is of interest such as in clinical\nsettings. To that end, we propose a multi-source learning architecture where we\nextract domain-invariant representations from dataset-specific private\nencoders. Our model utilizes a maximum-mean-discrepancy (MMD) based domain\nalignment approach to impose domain-invariance for encoded representations,\nwhich outperforms state-of-the-art approaches in EEG-based emotion\nclassification. Furthermore, representations learned in our pipeline preserve\ndomain privacy as dataset-specific private encoding alleviates the need for\nconventional, centralized EEG-based deep neural network training approaches\nwith shared parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bethge_D/0/1/0/all/0/1\">David Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallgarten_P/0/1/0/all/0/1\">Philipp Hallgarten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosse_Puppendahl_T/0/1/0/all/0/1\">Tobias Grosse-Puppendahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kari_M/0/1/0/all/0/1\">Mohamed Kari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikut_R/0/1/0/all/0/1\">Ralf Mikut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_A/0/1/0/all/0/1\">Albrecht Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozdenizci_O/0/1/0/all/0/1\">Ozan &#xd6;zdenizci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The KFIoU Loss for Rotated Object Detection. (arXiv:2201.12558v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12558","description":"<p>Differing from the well-developed horizontal object detection area whereby\nthe computing-friendly IoU based loss is readily adopted and well fits with the\ndetection metrics. In contrast, rotation detectors often involve a more\ncomplicated loss based on SkewIoU which is unfriendly to gradient-based\ntraining. In this paper, we argue that one effective alternative is to devise\nan approximate loss who can achieve trend-level alignment with SkewIoU loss\ninstead of the strict value-level identity. Specifically, we model the objects\nas Gaussian distribution and adopt Kalman filter to inherently mimic the\nmechanism of SkewIoU by its definition, and show its alignment with the SkewIoU\nat trend-level. This is in contrast to recent Gaussian modeling based rotation\ndetectors e.g. GWD, KLD that involves a human-specified distribution distance\nmetric which requires additional hyperparameter tuning. The resulting new loss\ncalled KFIoU is easier to implement and works better compared with exact\nSkewIoU, thanks to its full differentiability and ability to handle the\nnon-overlapping cases. We further extend our technique to the 3-D case which\nalso suffers from the same issues as 2-D detection. Extensive results on\nvarious public datasets (2-D/3-D, aerial/text/face images) with different base\ndetectors show the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gefan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jirui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Autoencoder for Self-Supervised Representation Learning. (arXiv:2202.03026v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03026","description":"<p>We present a novel masked image modeling (MIM) approach, context autoencoder\n(CAE), for self-supervised representation pretraining. The goal is to pretrain\nan encoder by solving the pretext task: estimate the masked patches from the\nvisible patches in an image. Our approach first feeds the visible patches into\nthe encoder, extracting the representations. Then, we make predictions from\nvisible patches to masked patches in the encoded representation space. We\nintroduce an alignment constraint, encouraging that the representations for\nmasked patches, predicted from the encoded representations of visible patches,\nare aligned with the masked patch presentations computed from the encoder. In\nother words, the predicted representations are expected to lie in the encoded\nrepresentation space, which empirically shows the benefit to representation\nlearning. Last, the predicted masked patch representations are mapped to the\ntargets of the pretext task through a decoder. In comparison to previous MIM\nmethods (e.g., BEiT) that couple the encoding and pretext task completion\nroles, our approach benefits the separation of the representation learning\n(encoding) role and the pretext task completion role, improving the\nrepresentation learning capacity and accordingly helping more on downstream\ntasks. In addition, we present the explanations about why contrastive\npretraining and supervised pretraining perform similarly and why MIM\npotentially performs better. We demonstrate the effectiveness of our CAE\nthrough superior transfer performance in downstream tasks: semantic\nsegmentation, and object detection and instance segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1\">Ying Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shumin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-to-Real Domain Adaptation for Lane Detection and Classification in Autonomous Driving. (arXiv:2202.07133v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07133","description":"<p>While supervised detection and classification frameworks in autonomous\ndriving require large labelled datasets to converge, Unsupervised Domain\nAdaptation (UDA) approaches, facilitated by synthetic data generated from\nphoto-real simulated environments, are considered low-cost and less\ntime-consuming solutions. In this paper, we propose UDA schemes using\nadversarial discriminative and generative methods for lane detection and\nclassification applications in autonomous driving. We also present Simulanes\ndataset generator to create a synthetic dataset that is naturalistic utilizing\nCARLA's vast traffic scenarios and weather conditions. The proposed UDA\nframeworks take the synthesized dataset with labels as the source domain,\nwhereas the target domain is the unlabelled real-world data. Using adversarial\ngenerative and feature discriminators, the learnt models are tuned to predict\nthe lane location and class in the target domain. The proposed techniques are\nevaluated using both real-world and our synthetic datasets. The results\nmanifest that the proposed methods have shown superiority over other baseline\nschemes in terms of detection and classification accuracy and consistency. The\nablation study reveals that the size of the simulation dataset plays important\nroles in the classification performance of the proposed methods. Our UDA\nframeworks are available at https://github.com/anita-hu/sim2real-lane-detection\nand our dataset generator is released at https://github.com/anita-hu/simulanes\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudson_S/0/1/0/all/0/1\">Sinclair Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethier_M/0/1/0/all/0/1\">Martin Ethier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Sharman_M/0/1/0/all/0/1\">Mohammad Al-Sharman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayside_D/0/1/0/all/0/1\">Derek Rayside</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melek_W/0/1/0/all/0/1\">William Melek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lie Point Symmetry Data Augmentation for Neural PDE Solvers. (arXiv:2202.07643v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.07643","description":"<p>Neural networks are increasingly being used to solve partial differential\nequations (PDEs), replacing slower numerical solvers. However, a critical issue\nis that neural PDE solvers require high-quality ground truth data, which\nusually must come from the very solvers they are designed to replace. Thus, we\nare presented with a proverbial chicken-and-egg problem. In this paper, we\npresent a method, which can partially alleviate this problem, by improving\nneural PDE solver sample complexity -- Lie point symmetry data augmentation\n(LPSDA). In the context of PDEs, it turns out that we are able to\nquantitatively derive an exhaustive list of data transformations, based on the\nLie point symmetry group of the PDEs in question, something not possible in\nother application areas. We present this framework and demonstrate how it can\neasily be deployed to improve neural PDE solver sample complexity by an order\nof magnitude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1\">Johannes Brandstetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worrall_D/0/1/0/all/0/1\">Daniel E. Worrall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSegNet: Operational Segmentation Network for COVID-19 Detection using Chest X-ray Images. (arXiv:2202.10185v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.10185","description":"<p>Coronavirus disease 2019 (COVID-19) has been diagnosed automatically using\nMachine Learning algorithms over chest X-ray (CXR) images. However, most of the\nearlier studies used Deep Learning models over scarce datasets bearing the risk\nof overfitting. Additionally, previous studies have revealed the fact that deep\nnetworks are not reliable for classification since their decisions may\noriginate from irrelevant areas on the CXRs. Therefore, in this study, we\npropose Operational Segmentation Network (OSegNet) that performs detection by\nsegmenting COVID-19 pneumonia for a reliable diagnosis. To address the data\nscarcity encountered in training and especially in evaluation, this study\nextends the largest COVID-19 CXR dataset: QaTa-COV19 with 121,378 CXRs\nincluding 9258 COVID-19 samples with their corresponding ground-truth\nsegmentation masks that are publicly shared with the research community.\nConsequently, OSegNet has achieved a detection performance with the highest\naccuracy of 99.65% among the state-of-the-art deep models with 98.09%\nprecision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Degerli_A/0/1/0/all/0/1\">Aysen Degerli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Muhammad E. H. Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TwistSLAM: Constrained SLAM in Dynamic Environment. (arXiv:2202.12384v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.12384","description":"<p>Classical visual simultaneous localization and mapping (SLAM) algorithms\nusually assume the environment to be rigid. This assumption limits the\napplicability of those algorithms as they are unable to accurately estimate the\ncamera poses and world structure in real life scenes containing moving objects\n(e.g. cars, bikes, pedestrians, etc.). To tackle this issue, we propose\nTwistSLAM: a semantic, dynamic and stereo SLAM system that can track dynamic\nobjects in the environment. Our algorithm creates clusters of points according\nto their semantic class. Thanks to the definition of inter-cluster constraints\nmodeled by mechanical joints (function of the semantic class), a novel\nconstrained bundle adjustment is then able to jointly estimate both poses and\nvelocities of moving objects along with the classical world structure and\ncamera trajectory. We evaluate our approach on several sequences from the\npublic KITTI dataset and demonstrate quantitatively that it improves camera and\nobject tracking compared to state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mathieu Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchand_E/0/1/0/all/0/1\">Eric Marchand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacete_A/0/1/0/all/0/1\">Amine Kacete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royan_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Royan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods. (arXiv:2203.02486v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02486","description":"<p>In many object recognition applications, the set of possible categories is an\nopen set, and the deployed recognition system will encounter novel objects\nbelonging to categories unseen during training. Detecting such \"novel category\"\nobjects is usually formulated as an anomaly detection problem. Anomaly\ndetection algorithms for feature-vector data identify anomalies as outliers,\nbut outlier detection has not worked well in deep learning. Instead, methods\nbased on the computed logits of visual object classifiers give state-of-the-art\nperformance. This paper proposes the Familiarity Hypothesis that these methods\nsucceed because they are detecting the absence of familiar learned features\nrather than the presence of novelty. The paper reviews evidence from the\nliterature and presents additional evidence from our own experiments that\nprovide strong support for this hypothesis. The paper concludes with a\ndiscussion of whether familiarity detection is an inevitable consequence of\nrepresentation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dietterich_T/0/1/0/all/0/1\">Thomas G. Dietterich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyer_A/0/1/0/all/0/1\">Alexander Guyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN. (arXiv:2203.02762v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2203.02762","description":"<p>The research topic of sketch-to-portrait generation has witnessed a boost of\nprogress with deep learning techniques. The recently proposed StyleGAN\narchitectures achieve state-of-the-art generation ability but the original\nStyleGAN is not friendly for sketch-based creation due to its unconditional\ngeneration nature. To address this issue, we propose a direct conditioning\nstrategy to better preserve the spatial information under the StyleGAN\nframework. Specifically, we introduce Spatially Conditioned StyleGAN\n(SC-StyleGAN for short), which explicitly injects spatial constraints to the\noriginal StyleGAN generation process. We explore two input modalities, sketches\nand semantic maps, which together allow users to express desired generation\nresults more precisely and easily. Based on SC-StyleGAN, we present\nDrawingInStyles, a novel drawing interface for non-professional users to easily\nproduce high-quality, photo-realistic face images with precise control, either\nfrom scratch or editing existing ones. Qualitative and quantitative evaluations\nshow the superior generation ability of our method to existing and alternative\nsolutions. The usability and expressiveness of our system are confirmed by a\nuser study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Wanchao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shu-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do We Really Need a Learnable Classifier at the End of Deep Neural Network?. (arXiv:2203.09081v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.09081","description":"<p>Modern deep neural networks for classification usually jointly learn a\nbackbone for representation and a linear classifier to output the logit of each\nclass. A recent study has shown a phenomenon called neural collapse that the\nwithin-class means of features and the classifier vectors converge to the\nvertices of a simplex equiangular tight frame (ETF) at the terminal phase of\ntraining on a balanced dataset. Since the ETF geometric structure maximally\nseparates the pair-wise angles of all classes in the classifier, it is natural\nto raise the question, why do we spend an effort to learn a classifier when we\nknow its optimal geometric structure? In this paper, we study the potential of\nlearning a neural network for classification with the classifier randomly\ninitialized as an ETF and fixed during training. Our analytical work based on\nthe layer-peeled model indicates that the feature learning with a fixed ETF\nclassifier naturally leads to the neural collapse state even when the dataset\nis imbalanced among classes. We further show that in this case the cross\nentropy (CE) loss is not necessary and can be replaced by a simple squared loss\nthat shares the same global optimality but enjoys a better convergence\nproperty. Our experimental results show that our method is able to bring\nsignificant improvements with faster convergence on multiple imbalanced\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09481","description":"<p>Denoising diffusion probabilistic models are a promising new class of\ngenerative models that mark a milestone in high-quality image generation. This\npaper showcases their ability to sequentially generate video, surpassing prior\nmethods in perceptual and probabilistic forecasting metrics. We propose an\nautoregressive, end-to-end optimized video diffusion model inspired by recent\nadvances in neural video compression. The model successively generates future\nframes by correcting a deterministic next-frame prediction using a stochastic\nresidual generated by an inverse diffusion process. We compare this approach\nagainst five baselines on four datasets involving natural and simulation-based\nvideos. We find significant improvements in terms of perceptual quality for all\ndatasets. Furthermore, by introducing a scalable version of the Continuous\nRanked Probability Score (CRPS) applicable to video, we show that our model\nalso outperforms existing approaches in their probabilistic frame forecasting\nability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prakhar Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Inversion for Nonlinear Imaging Models using Deep Generative Priors. (arXiv:2203.10078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10078","description":"<p>Most modern imaging systems incorporate a computational pipeline to infer the\nimage of interest from acquired measurements. The Bayesian approach for solving\nsuch ill-posed inverse problems involves the characterization of the posterior\ndistribution of the image. This depends on the model of the imaging system and\nprior knowledge on the image of interest. In this work, we present a Bayesian\nreconstruction framework for nonlinear imaging models, where the prior\nknowledge on the image is specified by a deep generative model. We develop a\ntractable posterior sampling scheme based on the Metropolis-adjusted Langevin\nalgorithm (MALA) for the class of nonlinear inverse problems where the forward\nmodel has a neural-network-like structure. This class includes most practical\nimaging modalities. We introduce the notion of augmented deep generative priors\nin order to suitably handle quantitative image recovery. We illustrate the\nadvantages of our framework by applying it to two nonlinear imaging\nmodalities-phase retrieval and optical diffraction tomography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohra_P/0/1/0/all/0/1\">Pakshal Bohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thanh-an Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jonathan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unser_M/0/1/0/all/0/1\">Michael Unser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.12023","description":"<p>Many promising applications of supervised machine learning face hurdles in\nthe acquisition of labeled data in sufficient quantity and quality, creating an\nexpensive bottleneck. To overcome such limitations, techniques that do not\ndepend on ground truth labels have been studied, including weak supervision and\ngenerative modeling. While these techniques would seem to be usable in concert,\nimproving one another, how to build an interface between them is not\nwell-understood. In this work, we propose a model fusing programmatic weak\nsupervision and generative adversarial networks and provide theoretical\njustification motivating this fusion. The proposed approach captures discrete\nlatent variables in the data alongside the weak supervision derived label\nestimate. Alignment of the two allows for better modeling of sample-dependent\naccuracies of the weak supervision sources, improving the estimate of\nunobserved labels. It is the first approach to enable data augmentation through\nweakly supervised synthetic images and pseudolabels. Additionally, its learned\nlatent variables can be inspected qualitatively. The model outperforms baseline\nweak supervision label models on a number of multiclass image classification\ndatasets, improves the quality of generated images, and further improves\nend-model performance through data augmentation with synthetic samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1\">Nicholas Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1\">Frederic Sala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1\">Artur Dubrawski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction. (arXiv:2203.12997v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12997","description":"<p>Dimensionality reduction is crucial both for visualization and preprocessing\nhigh dimensional data for machine learning. We introduce a novel method based\non a hierarchy built on 1-nearest neighbor graphs in the original space which\nis used to preserve the grouping properties of the data distribution on\nmultiple levels. The core of the proposal is an optimization-free projection\nthat is competitive with the latest versions of t-SNE and UMAP in performance\nand visualization quality while being an order of magnitude faster in run-time.\nFurthermore, its interpretable mechanics, the ability to project new data, and\nthe natural separation of data clusters in visualizations make it a general\npurpose unsupervised dimension reduction technique. In the paper, we argue\nabout the soundness of the proposed method and evaluate it on a diverse\ncollection of datasets with sizes varying from 1K to 11M samples and dimensions\nfrom 28 to 16K. We perform comparisons with other state-of-the-art methods on\nmultiple metrics and target dimensions highlighting its efficiency and\nperformance. Code is available at https://github.com/koulakis/h-nne\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarfraz_M/0/1/0/all/0/1\">M. Saquib Sarfraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koulakis_M/0/1/0/all/0/1\">Marios Koulakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13310","description":"<p>Monocular 3D object detection has long been a challenging task in autonomous\ndriving, which requires to decode 3D predictions solely from a single 2D image.\nMost existing methods follow conventional 2D object detectors to localize\nobjects based on their centers, and predict 3D attributes by neighboring\nfeatures around the centers. However, only using local features is insufficient\nto understand the scene-level 3D spatial structures and ignore the inter-object\ndepth relations from contextual cues. In this paper, we introduce a novel\nframework for Monocular DEtection with a depth-guided TRansformer, named\nMonoDETR. The vanilla transformer is modified to be depth-aware and the whole\ndetection process is then guided by depth. Specifically, we represent 3D object\ncandidates as a set of queries and adopt an attention-based depth encoder to\nproduce non-local depth embeddings of the input image. Then, we propose a\ndepth-guided decoder with depth cross-attention modules to conduct both\ninter-query and query-scene depth feature interactions. In this way, each\nobject query estimates its 3D attributes adaptively from the depth-guided\nregions from the image and is no longer constrained to use only neighboring\nvisual features. MonoDETR is an end-to-end network without extra data or NMS\npost-processing and achieves state-of-the-art performance on KITTI benchmark\nwith significant gains. Extensive ablation studies demonstrate the\neffectiveness of our approach and its potential to serve as a transformer\nbaseline for future monocular 3D object detection research. Code is available\nat https://github.com/ZrrSkywalker/MonoDETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuanzhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Min-Max Similarity: A Contrastive Learning Based Semi-Supervised Learning Network for Surgical Tools Segmentation. (arXiv:2203.15177v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15177","description":"<p>Segmentation of images is a popular topic in medical AI. This is mainly due\nto the difficulty to obtain a significant number of pixel-level annotated data\nto train a neural network. To address this issue, we proposed a semi-supervised\nsegmentation network based on contrastive learning. In contrast to the previous\nstate-of-the-art, we introduce a contrastive learning form of dual-view\ntraining by employing classifiers and projectors to build all-negative, and\npositive and negative feature pairs respectively to formulate the learning\nproblem as solving min-max similarity problem. The all-negative pairs are used\nto supervise the networks learning from different views and make sure to\ncapture general features, and the consistency of unlabeled predictions is\nmeasured by pixel-wise contrastive loss between positive and negative pairs. To\nquantitative and qualitative evaluate our proposed method, we test it on two\npublic endoscopy surgical tool segmentation datasets and one cochlear implant\nsurgery dataset which we manually annotate the cochlear implant in surgical\nvideos. The segmentation performance (dice coefficients) indicates that our\nproposed method outperforms state-of-the-art semi-supervised and fully\nsupervised segmentation algorithms consistently. The code is publicly available\nat: https://github.com/AngeLouCN/Min_Max_Similarity\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1\">Ange Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tawfik_K/0/1/0/all/0/1\">Kareem Tawfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziteng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">Jack Noble</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex-Valued Autoencoders for Object Discovery. (arXiv:2204.02075v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.02075","description":"<p>Object-centric representations form the basis of human perception, and enable\nus to reason about the world and to systematically generalize to new settings.\nCurrently, most works on unsupervised object discovery focus on slot-based\napproaches, which explicitly separate the latent representations of individual\nobjects. While the result is easily interpretable, it usually requires the\ndesign of involved architectures. In contrast to this, we propose a\ncomparatively simple approach - the Complex AutoEncoder (CAE) - that creates\ndistributed object-centric representations. Following a coding scheme theorized\nto underlie object representations in biological neurons, its complex-valued\nactivations represent two messages: their magnitudes express the presence of a\nfeature, while the relative phase differences between neurons express which\nfeatures should be bound together to create joint object representations. In\ncontrast to previous approaches using complex-valued activations for object\ndiscovery, we present a fully unsupervised approach that is trained end-to-end\n- resulting in significant improvements in performance and efficiency on simple\nmulti-object datasets. Further, we show that the CAE achieves competitive or\nbetter unsupervised object discovery performance compared to a state-of-the-art\nslot-based approach while being up to 100 times faster to train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Sindy L&#xf6;we</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Maja Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Different Losses for Deep Learning Image Colorization. (arXiv:2204.02980v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02980","description":"<p>Image colorization aims to add color information to a grayscale image in a\nrealistic way. Recent methods mostly rely on deep learning strategies. While\nlearning to automatically colorize an image, one can define well-suited\nobjective functions related to the desired color output. Some of them are based\non a specific type of error between the predicted image and ground truth one,\nwhile other losses rely on the comparison of perceptual properties. But, is the\nchoice of the objective function that crucial, i.e., does it play an important\nrole in the results? In this chapter, we aim to answer this question by\nanalyzing the impact of the loss function on the estimated colorization\nresults. To that goal, we review the different losses and evaluation metrics\nthat are used in the literature. We then train a baseline network with several\nof the reviewed objective functions: classic L1 and L2 losses, as well as more\ncomplex combinations such as Wasserstein GAN and VGG-based LPIPS loss.\nQuantitative results show that the models trained with VGG-based LPIPS provide\noverall slightly better results for most evaluation metrics. Qualitative\nresults exhibit more vivid colors when with Wasserstein GAN plus the L2 loss or\nagain with the VGG-based LPIPS. Finally, the convenience of quantitative user\nstudies is also discussed to overcome the difficulty of properly assessing on\ncolorized images, notably for the case of old archive photographs where no\nground truth is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ballester_C/0/1/0/all/0/1\">Coloma Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugeau_A/0/1/0/all/0/1\">Aur&#xe9;lie Bugeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrillo_H/0/1/0/all/0/1\">Hernan Carrillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_M/0/1/0/all/0/1\">Micha&#xeb;l Cl&#xe9;ment</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giraud_R/0/1/0/all/0/1\">R&#xe9;mi Giraud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raad_L/0/1/0/all/0/1\">Lara Raad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitoria_P/0/1/0/all/0/1\">Patricia Vitoria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pin the Memory: Learning to Generalize Semantic Segmentation. (arXiv:2204.03609v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03609","description":"<p>The rise of deep neural networks has led to several breakthroughs for\nsemantic segmentation. In spite of this, a model trained on source domain often\nfails to work properly in new challenging domains, that is directly concerned\nwith the generalization capability of the model. In this paper, we present a\nnovel memory-guided domain generalization method for semantic segmentation\nbased on meta-learning framework. Especially, our method abstracts the\nconceptual knowledge of semantic classes into categorical memory which is\nconstant beyond the domains. Upon the meta-learning concept, we repeatedly\ntrain memory-guided networks and simulate virtual test to 1) learn how to\nmemorize a domain-agnostic and distinct information of classes and 2) offer an\nexternally settled memory as a class-guidance to reduce the ambiguity of\nrepresentation in the test data of arbitrary unseen domain. To this end, we\nalso propose memory divergence and feature cohesion losses, which encourage to\nlearn memory reading and update processes for category-aware domain\ngeneralization. Extensive experiments for semantic segmentation demonstrate the\nsuperior generalization capability of our method over state-of-the-art works on\nvarious benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dongbo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transparent Shape from Single Polarization Images. (arXiv:2204.06331v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06331","description":"<p>This paper presents a data-driven approach for transparent shape from\npolarization. Due to the inherent high transmittance, the previous shape from\npolarization(SfP) methods based on specular reflection model have difficulty in\nestimating transparent shape, and the lack of datasets for transparent SfP also\nlimits the application of the data-driven approach. Hence, we construct the\ntransparent SfP dataset which consists of both synthetic and real-world\ndatasets. To determine the reliability of the physics-based reflection model,\nwe define the physics-based prior confidence by exploiting the inherent fault\nof polarization information, then we propose a multi-branch fusion network to\nembed the confidence. Experimental results show that our approach outperforms\nother SfP methods. Compared with the previous method, the mean and median\nangular error of our approach are reduced from $19.00^\\circ$ and $14.91^\\circ$\nto $16.72^\\circ$ and $13.36^\\circ$, and the accuracy $11.25^\\circ, 22.5^\\circ,\n30^\\circ$ are improved from $38.36\\%, 77.36\\%, 87.48\\%$ to $45.51\\%, 78.86\\%,\n89.98\\%$, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Mingqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Chongkun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junnan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueqian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation. (arXiv:2204.09903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09903","description":"<p>Few-shot segmentation, which aims to segment unseen-class objects given only\na handful of densely labeled samples, has received widespread attention from\nthe community. Existing approaches typically follow the prototype learning\nparadigm to perform meta-inference, which fails to fully exploit the underlying\ninformation from support image-mask pairs, resulting in various segmentation\nfailures, e.g., incomplete objects, ambiguous boundaries, and distractor\nactivation. To this end, we propose a simple yet versatile framework in the\nspirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is\nfirst implemented on the annotated support image, and then the coarse\nsegmentation mask is divided into multiple regions with different properties.\nLeveraging effective masked average pooling operations, a series of\nsupport-induced proxies are thus derived, each playing a specific role in\nconquering the above challenges. Moreover, we devise a unique parallel decoder\nstructure that integrates proxies with similar attributes to boost the\ndiscrimination power. Our proposed approach, named divide-and-conquer proxies\n(DCP), allows for the development of appropriate and reliable information as a\nguide at the \"episode\" level, not just about the object cues themselves.\nExtensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of\nDCP over conventional prototype-based approaches (up to 5~10% on average),\nwhich also establishes a new state-of-the-art. Code is available at\ngithub.com/chunbolang/DCP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Chunbo Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_B/0/1/0/all/0/1\">Binfei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Estimation with Simplified Transformer. (arXiv:2204.13791v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13791","description":"<p>Transformer and its variants have shown state-of-the-art results in many\nvision tasks recently, ranging from image classification to dense prediction.\nDespite of their success, limited work has been reported on improving the model\nefficiency for deployment in latency-critical applications, such as autonomous\ndriving and robotic navigation. In this paper, we aim at improving upon the\nexisting transformers in vision, and propose a method for self-supervised\nmonocular Depth Estimation with Simplified Transformer (DEST), which is\nefficient and particularly suitable for deployment on GPU-based platforms.\nThrough strategic design choices, our model leads to significant reduction in\nmodel size, complexity, as well as inference latency, while achieving superior\naccuracy as compared to state-of-the-art. We also show that our design\ngeneralize well to other dense prediction task without bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">John Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Le An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_A/0/1/0/all/0/1\">Anurag Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1\">Jinkyu Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Su Inn Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining. (arXiv:2204.14095v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.14095","description":"<p>Large-scale vision-language pre-training has achieved promising results on\ndownstream tasks. Existing methods highly rely on the assumption that the\nimage-text pairs crawled from the Internet are in perfect one-to-one\ncorrespondence. However, in real scenarios, this assumption can be difficult to\nhold: the text description, obtained by crawling the affiliated metadata of the\nimage, often suffers from the semantic mismatch and the mutual compatibility.\nTo address these issues, we introduce PyramidCLIP, which constructs an input\npyramid with different semantic levels for each modality, and aligns visual\nelements and linguistic elements in the form of hierarchy via peer-level\nsemantics alignment and cross-level relation alignment. Furthermore, we soften\nthe loss of negative samples (unpaired samples) so as to weaken the strict\nconstraint during the pre-training stage, thus mitigating the risk of forcing\nthe model to distinguish compatible negative pairs. Experiments on five\ndownstream tasks demonstrate the effectiveness of the proposed PyramidCLIP. In\nparticular, with the same amount of 15 million pre-training image-text pairs,\nPyramidCLIP exceeds CLIP on ImageNet zero-shot classification top-1 accuracy by\n10.6%/13.2%/10.0% with ResNet50/ViT-B32/ViT-B16 based image encoder\nrespectively. When scaling to larger datasets, PyramidCLIP achieves the\nstate-of-the-art results on several downstream tasks. In particular, the\nresults of PyramidCLIP-ResNet50 trained on 143M image-text pairs surpass that\nof CLIP using 400M data on ImageNet zero-shot classification task,\nsignificantly improving the data efficiency of CLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion Networks. (arXiv:2205.01355v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2205.01355","description":"<p>We present a learning algorithm that uses bone-driven motion networks to\npredict the deformation of loose-fitting garment meshes at interactive rates.\nGiven a garment, we generate a simulation database and extract virtual bones\nfrom simulated mesh sequences using skin decomposition. At runtime, we\nseparately compute low- and high-frequency deformations in a sequential manner.\nThe low-frequency deformations are predicted by transferring body motions to\nvirtual bones' motions, and the high-frequency deformations are estimated\nleveraging the global information of virtual bones' motions and local\ninformation extracted from low-frequency meshes. In addition, our method can\nestimate garment deformations caused by variations of the simulation parameters\n(e.g., fabric's bending stiffness) using an RBF kernel ensembling trained\nnetworks for different sets of simulation parameters. Through extensive\ncomparisons, we show that our method outperforms state-of-the-art methods in\nterms of prediction accuracy of mesh deformations by about 20% in RMSE and 10%\nin Hausdorff distance and STED. The code and data are available at\nhttps://github.com/non-void/VirtualBones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_J/0/1/0/all/0/1\">Jiaming Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Dongxue Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1\">Tianjia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaogang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting and Understanding Harmful Memes: A Survey. (arXiv:2205.04274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.04274","description":"<p>The automatic identification of harmful content online is of major concern\nfor social media platforms, policymakers, and society. Researchers have studied\ntextual, visual, and audio content, but typically in isolation. Yet, harmful\ncontent often combines multiple modalities, as in the case of memes, which are\nof particular interest due to their viral nature. With this in mind, here we\noffer a comprehensive survey with a focus on harmful memes. Based on a\nsystematic analysis of recent literature, we first propose a new typology of\nharmful memes, and then we highlight and summarize the relevant state of the\nart. One interesting finding is that many types of harmful memes are not really\nstudied, e.g., such featuring self-harm and extremism, partly due to the lack\nof suitable datasets. We further find that existing datasets mostly capture\nmulti-class scenarios, which are not inclusive of the affective spectrum that\nmemes can represent. Another observation is that memes can propagate globally\nthrough repackaging in different languages and that they can also be\nmultilingual, blending different cultures. We conclude by highlighting several\nchallenges related to multimodal semiotics, technological constraints, and\nnon-trivial social engagement, and we present several open-ended aspects such\nas delineating online harm and empirically examining related frameworks and\nassistive interventions, which we believe will motivate and drive future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-textures, a self-supervised hard clustering deep learning algorithm for satellite image segmentation. (arXiv:2205.08671v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08671","description":"<p>Deep learning self-supervised algorithms that can segment an image in a fixed\nnumber of hard labels such as the k-means algorithm and relying only on deep\nlearning techniques are still lacking. Here, we introduce the k-textures\nalgorithm which provides self-supervised segmentation of a 4-band image\n(RGB-NIR) for a $k$ number of classes. An example of its application on high\nresolution Planet satellite imagery is given. Our algorithm shows that discrete\nsearch is feasible using convolutional neural networks (CNN) and gradient\ndescent. The model detects $k$ hard clustering classes represented in the model\nas $k$ discrete binary masks and their associated $k$ independently generated\ntextures, that combined are a simulation of the original image. The similarity\nloss is the mean squared error between the features of the original and the\nsimulated image, both extracted from the penultimate convolutional block of\nKeras 'imagenet' pretrained VGG-16 model and a custom feature extractor made\nwith Planet data. The main advances of the k-textures model are: first, the $k$\ndiscrete binary masks are obtained inside the model using gradient descent. The\nmodel allows for the generation of discrete binary masks using a novel method\nusing a hard sigmoid activation function. Second, it provides hard clustering\nclasses -- each pixels has only one class. Finally, in comparison to k-means,\nwhere each pixel is considered independently, here, contextual information is\nalso considered and each class is not associated only to similar values in the\ncolor channels but also to a texture. Our approach is designed to ease the\nproduction of training samples for satellite image segmentation and the\nk-textures architecture could be adapted to support different number of bands\nand for more complex tasks, such as object self-segmentation. The model codes\nand weights are available at https://doi.org/10.5281/zenodo.6359859\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_F/0/1/0/all/0/1\">Fabien H. Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalagnol_R/0/1/0/all/0/1\">Ricardo Dalagnol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_A/0/1/0/all/0/1\">Alber H. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirye_M/0/1/0/all/0/1\">Mayumi C.M. Hirye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favrichon_S/0/1/0/all/0/1\">Samuel Favrichon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jake H. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauceri_S/0/1/0/all/0/1\">Steffen Mauceri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saatchi_S/0/1/0/all/0/1\">Sassan Saatchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation. (arXiv:2205.09853v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09853","description":"<p>Video prediction is a challenging task. The quality of video frames from\ncurrent state-of-the-art (SOTA) generative models tends to be poor and\ngeneralization beyond the training data is difficult. Furthermore, existing\nprediction frameworks are typically not capable of simultaneously handling\nother video-related tasks such as unconditional generation or interpolation. In\nthis work, we devise a general-purpose framework called Masked Conditional\nVideo Diffusion (MCVD) for all of these video synthesis tasks using a\nprobabilistic conditional score-based denoising diffusion model, conditioned on\npast and/or future frames. We train the model in a manner where we randomly and\nindependently mask all the past frames or all the future frames. This novel but\nstraightforward setup allows us to train a single model that is capable of\nexecuting a broad range of video tasks, specifically: future/past prediction --\nwhen only future/past frames are masked; unconditional generation -- when both\npast and future frames are masked; and interpolation -- when neither past nor\nfuture frames are masked. Our experiments show that this approach can generate\nhigh-quality frames for diverse types of videos. Our MCVD models are built from\nsimple non-recurrent 2D-convolutional architectures, conditioning on blocks of\nframes and generating blocks of frames. We generate videos of arbitrary lengths\nautoregressively in a block-wise manner. Our approach yields SOTA results\nacross standard video prediction and interpolation benchmarks, with computation\ntimes for training models measured in 1-12 days using $\\le$ 4 GPUs. Project\npage: https://mask-cond-video-diffusion.github.io ; Code :\nhttps://github.com/voletiv/mcvd-pytorch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jolicoeur_Martineau_A/0/1/0/all/0/1\">Alexia Jolicoeur-Martineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer based Generative Adversarial Network for Liver Segmentation. (arXiv:2205.10663v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.10663","description":"<p>Automated liver segmentation from radiology scans (CT, MRI) can improve\nsurgery and therapy planning and follow-up assessment in addition to\nconventional use for diagnosis and prognosis. Although convolutional neural\nnetworks (CNNs) have become the standard image segmentation tasks, more\nrecently this has started to change towards Transformers based architectures\nbecause Transformers are taking advantage of capturing long range dependence\nmodeling capability in signals, so called attention mechanism. In this study,\nwe propose a new segmentation approach using a hybrid approach combining the\nTransformer(s) with the Generative Adversarial Network (GAN) approach. The\npremise behind this choice is that the self-attention mechanism of the\nTransformers allows the network to aggregate the high dimensional feature and\nprovide global information modeling. This mechanism provides better\nsegmentation performance compared with traditional methods. Furthermore, we\nencode this generator into the GAN based architecture so that the discriminator\nnetwork in the GAN can classify the credibility of the generated segmentation\nmasks compared with the real masks coming from human (expert) annotations. This\nallows us to extract the high dimensional topology information in the mask for\nbiomedical image segmentation and provide more reliable segmentation results.\nOur model achieved a high dice coefficient of 0.9433, recall of 0.9515, and\nprecision of 0.9376 and outperformed other Transformer based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1\">Ugur Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antalek_M/0/1/0/all/0/1\">Matthew Antalek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1\">Elif Keles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borhani_A/0/1/0/all/0/1\">Amir Borhani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ladner_D/0/1/0/all/0/1\">Daniela Ladner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners. (arXiv:2205.10747v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10747","description":"<p>The goal of this work is to build flexible video-language models that can\ngeneralize to various video-to-text tasks from few examples, such as\ndomain-specific captioning, question answering, and future event prediction.\nExisting few-shot video-language learners focus exclusively on the encoder,\nresulting in the absence of a video-to-text decoder to handle generative tasks.\nVideo captioners have been pretrained on large-scale video-language datasets,\nbut they rely heavily on finetuning and lack the ability to generate text for\nunseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language\nLearner via Image and Language models, which demonstrates strong performance on\nfew-shot video-to-text tasks without the necessity of pretraining or finetuning\non any video datasets. We use the image-language models to translate the video\ncontent into frame captions, object, attribute, and event phrases, and compose\nthem into a temporal structure template. We then instruct a language model,\nwith a prompt containing a few in-context examples, to generate a target output\nfrom the composed content. The flexibility of prompting allows the model to\ncapture any form of text input, such as automatic speech recognition (ASR)\ntranscripts. Our experiments demonstrate the power of language models in\nunderstanding videos on a wide variety of video-language tasks, including video\ncaptioning, video question answering, video caption retrieval, and video future\nevent prediction. Especially, on video future event prediction, our few-shot\nmodel significantly outperforms state-of-the-art supervised models trained on\nlarge-scale video datasets. Code and resources are publicly available for\nresearch purposes at https://github.com/MikeWangWZHL/VidIL .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Counting with Crowd-Sourced Supervision. (arXiv:2205.11398v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11398","description":"<p>Crowd-sourcing is an increasingly popular tool for image analysis in animal\necology. Computer vision methods that can utilize crowd-sourced annotations can\nhelp scale up analysis further. In this work we study the potential to do so on\nthe challenging task of fine-grained counting. As opposed to the standard crowd\ncounting task, fine-grained counting also involves classifying attributes of\nindividuals in dense crowds. We introduce a new dataset from animal ecology to\nenable this study that contains 1.7M crowd-sourced annotations of 8\nfine-grained classes. It is the largest available dataset for fine-grained\ncounting and the first to enable the study of the task with crowd-sourced\nannotations. We introduce methods for generating aggregate \"ground truths\" from\nthe collected annotations, as well as a counting method that can utilize the\naggregate information. Our method improves results by 8% over a comparable\nbaseline, indicating the potential for algorithms to learn fine-grained\ncounting using crowd-sourced supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1\">Justin Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foley_C/0/1/0/all/0/1\">Catherine M. Foley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hart_T/0/1/0/all/0/1\">Tom Hart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Model Generalization for Monocular 3D Object Detection. (arXiv:2205.11664v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11664","description":"<p>Monocular 3D object detection (Mono3D) has achieved tremendous improvements\nwith emerging large-scale autonomous driving datasets and the rapid development\nof deep learning techniques. However, caused by severe domain gaps (e.g., the\nfield of view (FOV), pixel size, and object size among datasets), Mono3D\ndetectors have difficulty in generalization, leading to drastic performance\ndegradation on unseen domains. To solve these issues, we combine the\nposition-invariant transform and multi-scale training with the pixel-size depth\nstrategy to construct an effective unified camera-generalized paradigm (CGP).\nIt fully considers discrepancies in the FOV and pixel size of images captured\nby different cameras. Moreover, we further investigate the obstacle in\nquantitative metrics when cross-dataset inference through an exhaustive\nsystematic study. We discern that the size bias of prediction leads to a\ncolossal failure. Hence, we propose the 2D-3D geometry-consistent object\nscaling strategy (GCOS) to bridge the gap via an instance-level augment. Our\nmethod called DGMono3D achieves remarkable performance on all evaluated\ndatasets and surpasses the SoTA unsupervised domain adaptation scheme even\nwithout utilizing data on the target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet Feature Maps Compression for Image-to-Image CNNs. (arXiv:2205.12268v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12268","description":"<p>Convolutional Neural Networks (CNNs) are known for requiring extensive\ncomputational resources, and quantization is among the best and most common\nmethods for compressing them. While aggressive quantization (i.e., less than\n4-bits) performs well for classification, it may cause severe performance\ndegradation in image-to-image tasks such as semantic segmentation and depth\nestimation. In this paper, we propose Wavelet Compressed Convolution (WCC) -- a\nnovel approach for high-resolution activation maps compression integrated with\npoint-wise convolutions, which are the main computational cost of modern\narchitectures. To this end, we use an efficient and hardware-friendly\nHaar-wavelet transform, known for its effectiveness in image compression, and\ndefine the convolution on the compressed activation map. We experiment on\nvarious tasks, that benefit from high-resolution input, and by combining WCC\nwith light quantization, we achieve compression rates equivalent to 1-4bit\nactivation quantization with relatively small and much more graceful\ndegradation in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finder_S/0/1/0/all/0/1\">Shahaf E. Finder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zohav_Y/0/1/0/all/0/1\">Yair Zohav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashkenazi_M/0/1/0/all/0/1\">Maor Ashkenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1\">Eran Treister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Diffusion Models via Early Stop of the Diffusion Process. (arXiv:2205.12524v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12524","description":"<p>Denoising Diffusion Probabilistic Models (DDPMs) have achieved impressive\nperformance on various generation tasks. By modeling the reverse process of\ngradually diffusing the data distribution into a Gaussian distribution,\ngenerating a sample in DDPMs can be regarded as iteratively denoising a\nrandomly sampled Gaussian noise. However, in practice DDPMs often need hundreds\neven thousands of denoising steps to obtain a high-quality sample from the\nGaussian noise, leading to extremely low inference efficiency. In this work, we\npropose a principled acceleration strategy, referred to as Early-Stopped DDPM\n(ES-DDPM), for DDPMs. The key idea is to stop the diffusion process early where\nonly the few initial diffusing steps are considered and the reverse denoising\nprocess starts from a non-Gaussian distribution. By further adopting a powerful\npre-trained generative model, such as GAN and VAE, in ES-DDPM, sampling from\nthe target non-Gaussian distribution can be efficiently achieved by diffusing\nsamples obtained from the pre-trained generative model. In this way, the number\nof required denoising steps is significantly reduced. In the meantime, the\nsample quality of ES-DDPM also improves substantially, outperforming both the\nvanilla DDPM and the adopted pre-trained generative model. On extensive\nexperiments across CIFAR-10, CelebA, ImageNet, LSUN-Bedroom and LSUN-Cat,\nES-DDPM obtains promising acceleration effect and performance improvement over\nrepresentative baseline methods. Moreover, ES-DDPM also demonstrates several\nattractive properties, including being orthogonal to existing acceleration\nmethods, as well as simultaneously enabling both global semantic and local\npixel-level control in image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XU_X/0/1/0/all/0/1\">Xudong XU</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Ceyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12693","description":"<p>Self-supervised learning has achieved a great success in the representation\nlearning of visual and textual data. However, the current methods are mainly\nvalidated on the well-curated datasets, which do not exhibit the real-world\nlong-tailed distribution. Recent attempts to consider self-supervised\nlong-tailed learning are made by rebalancing in the loss perspective or the\nmodel perspective, resembling the paradigms in the supervised long-tailed\nlearning. Nevertheless, without the aid of labels, these explorations have not\nshown the expected significant promise due to the limitation in tail sample\ndiscovery or the heuristic structure design. Different from previous works, we\nexplore this direction from an alternative perspective, i.e., the data\nperspective, and propose a novel Boosted Contrastive Learning (BCL) method.\nSpecifically, BCL leverages the memorization effect of deep neural networks to\nautomatically drive the information discrepancy of the sample views in\ncontrastive learning, which is more efficient to enhance the long-tailed\nlearning in the label-unaware context. Extensive experiments on a range of\nbenchmark datasets demonstrate the effectiveness of BCL over several\nstate-of-the-art methods. Our code is available at\nhttps://github.com/Zhihan-Zhou/Boosted-Contrastive-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing an Efficient End-to-end Machine Learning Pipeline for Real-time Empty-shelf Detection. (arXiv:2205.13060v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.13060","description":"<p>On-Shelf Availability (OSA) of products in retail stores is a critical\nbusiness criterion in the fast moving consumer goods and retails sector. When a\nproduct is out-of-stock (OOS) and a customer cannot find it on its designed\nshelf, this motivates the customer to store-switching or buying nothing, which\ncauses fall in future sales and demands. Retailers are employing several\napproaches to detect empty shelves and ensure high OSA of products; however,\nsuch methods are generally ineffective and infeasible since they are either\nmanual, expensive or less accurate. Recently machine learning based solutions\nhave been proposed, but they suffer from high computational cost and low\naccuracy problem due to lack of large annotated datasets of on-shelf products.\nHere, we present an elegant approach for designing an end-to-end machine\nlearning (ML) pipeline for real-time empty shelf detection. Considering the\nstrong dependency between the quality of ML models and the quality of data, we\nfocus on the importance of proper data collection, cleaning and correct data\nannotation before delving into modeling. Since an empty-shelf detection\nsolution should be computationally-efficient for real-time predictions, we\nexplore different run-time optimizations to improve the model performance. Our\ndataset contains 1000 images, collected and annotated by following well-defined\nguidelines. Our low-latency model achieves a mean average F1-score of 68.5%,\nand can process up to 67 images/s on Intel Xeon Gold and up to 860 images/s on\nan A100 GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Dipendra Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahjoubfar_A/0/1/0/all/0/1\">Ata Mahjoubfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Anupama Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning. (arXiv:2205.13137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13137","description":"<p>In this study, we propose Mixed and Masked Image Modeling (MixMIM), a simple\nbut efficient MIM method that is applicable to various hierarchical Vision\nTransformers. Existing MIM methods replace a random subset of input tokens with\na special MASK symbol and aim at reconstructing original image tokens from the\ncorrupted image. However, we find that using the MASK symbol greatly slows down\nthe training and causes training-finetuning inconsistency, due to the large\nmasking ratio (e.g., 40% in BEiT). In contrast, we replace the masked tokens of\none image with visible tokens of another image, i.e., creating a mixed image.\nWe then conduct dual reconstruction to reconstruct the original two images from\nthe mixed input, which significantly improves efficiency. While MixMIM can be\napplied to various architectures, this paper explores a simpler but stronger\nhierarchical Transformer, and scales with MixMIM-B, -L, and -H. Empirical\nresults demonstrate that MixMIM can learn high-quality visual representations\nefficiently. Notably, MixMIM-B with 88M parameters achieves 85.1% top-1\naccuracy on ImageNet-1K by pretraining for 600 epochs, setting a new record for\nneural networks with comparable model sizes (e.g., ViT-B) among MIM methods.\nBesides, its transferring performances on the other 6 datasets show MixMIM has\nbetter FLOPs / performance tradeoff than previous MIM methods. Code is\navailable at https://github.com/Sense-X/MixMIM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CA-UDA: Class-Aware Unsupervised Domain Adaptation with Optimal Assignment and Pseudo-Label Refinement. (arXiv:2205.13579v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13579","description":"<p>Recent works on unsupervised domain adaptation (UDA) focus on the selection\nof good pseudo-labels as surrogates for the missing labels in the target data.\nHowever, source domain bias that deteriorates the pseudo-labels can still exist\nsince the shared network of the source and target domains are typically used\nfor the pseudo-label selections. The suboptimal feature space source-to-target\ndomain alignment can also result in unsatisfactory performance. In this paper,\nwe propose CA-UDA to improve the quality of the pseudo-labels and UDA results\nwith optimal assignment, a pseudo-label refinement strategy and class-aware\ndomain alignment. We use an auxiliary network to mitigate the source domain\nbias for pseudo-label refinement. Our intuition is that the underlying\nsemantics in the target domain can be fully exploited to help refine the\npseudo-labels that are inferred from the source features under domain shift.\nFurthermore, our optimal assignment can optimally align features in the\nsource-to-target domains and our class-aware domain alignment can\nsimultaneously close the domain gap while preserving the classification\ndecision boundaries. Extensive experiments on several benchmark datasets show\nthat our method can achieve state-of-the-art performance in the image\nclassification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TraClets: Harnessing the power of computer vision for trajectory classification. (arXiv:2205.13880v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13880","description":"<p>Due to the advent of new mobile devices and tracking sensors in recent years,\nhuge amounts of data are being produced every day. Therefore, novel\nmethodologies need to emerge that dive through this vast sea of information and\ngenerate insights and meaningful information. To this end, researchers have\ndeveloped several trajectory classification algorithms over the years that are\nable to annotate tracking data. Similarly, in this research, a novel\nmethodology is presented that exploits image representations of trajectories,\ncalled TraClets, in order to classify trajectories in an intuitive humans way,\nthrough computer vision techniques. Several real-world datasets are used to\nevaluate the proposed approach and compare its classification performance to\nother state-of-the-art trajectory classification algorithms. Experimental\nresults demonstrate that TraClets achieves a classification performance that is\ncomparable to, or in most cases, better than the state-of-the-art, acting as a\nuniversal, high-accuracy approach for trajectory classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kontopoulos_I/0/1/0/all/0/1\">Ioannis Kontopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makris_A/0/1/0/all/0/1\">Antonios Makris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tserpes_K/0/1/0/all/0/1\">Konstantinos Tserpes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogorny_V/0/1/0/all/0/1\">Vania Bogorny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharpness-Aware Training for Free. (arXiv:2205.14083v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14083","description":"<p>Modern deep neural networks (DNNs) have achieved state-of-the-art\nperformances but are typically over-parameterized. The over-parameterization\nmay result in undesirably large generalization error in the absence of other\ncustomized training strategies. Recently, a line of research under the name of\nSharpness-Aware Minimization (SAM) has shown that minimizing a sharpness\nmeasure, which reflects the geometry of the loss landscape, can significantly\nreduce the generalization error. However, SAM-like methods incur a two-fold\ncomputational overhead of the given base optimizer (e.g. SGD) for approximating\nthe sharpness measure. In this paper, we propose Sharpness-Aware Training for\nFree, or SAF, which mitigates the sharp landscape at almost zero additional\ncomputational cost over the base optimizer. Intuitively, SAF achieves this by\navoiding sudden drops in the loss in the sharp local minima throughout the\ntrajectory of the updates of the weights. Specifically, we suggest a novel\ntrajectory loss, based on the KL-divergence between the outputs of DNNs with\nthe current weights and past weights, as a replacement of the SAM's sharpness\nmeasure. This loss captures the rate of change of the training loss along the\nmodel's update trajectory. By minimizing it, SAF ensures the convergence to a\nflat minimum with improved generalization capabilities. Extensive empirical\nresults show that SAF minimizes the sharpness in the same way that SAM does,\nyielding better results on the ImageNet dataset with essentially the same\ncomputational cost as the base optimizer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Daquan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenCalib: A Multi-sensor Calibration Toolbox for Autonomous Driving. (arXiv:2205.14087v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2205.14087","description":"<p>Accurate sensor calibration is a prerequisite for multi-sensor perception and\nlocalization systems for autonomous vehicles. The intrinsic parameter\ncalibration of the sensor is to obtain the mapping relationship inside the\nsensor, and the extrinsic parameter calibration is to transform two or more\nsensors into a unified spatial coordinate system. Most sensors need to be\ncalibrated after installation to ensure the accuracy of sensor measurements. To\nthis end, we present OpenCalib, a calibration toolbox that contains a rich set\nof various sensor calibration methods. OpenCalib covers manual calibration\ntools, automatic calibration tools, factory calibration tools, and online\ncalibration tools for different application scenarios. At the same time, to\nevaluate the calibration accuracy and subsequently improve the accuracy of the\ncalibration algorithm, we released a corresponding benchmark dataset. This\npaper introduces various features and calibration methods of this toolbox. To\nour knowledge, this is the first open-sourced calibration codebase containing\nthe full set of autonomous-driving-related calibration approaches in this area.\nWe wish that the toolbox could be helpful to autonomous driving researchers. We\nhave open-sourced our code on GitHub to benefit the community. Code is\navailable at https://github.com/PJLab-ADG/SensorsCalibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guohang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuochun_L/0/1/0/all/0/1\">Liu Zhuochun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chunlei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengjin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xinyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zebin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}