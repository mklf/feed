{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"When a Computer Cracks a Joke: Automated Generation of Humorous Headlines. (arXiv:2109.08702v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08702","description":"<p>Automated news generation has become a major interest for new agencies in the\npast. Oftentimes headlines for such automatically generated news articles are\nunimaginative as they have been generated with ready-made templates. We present\na computationally creative approach for headline generation that can generate\nhumorous versions of existing headlines. We evaluate our system with human\njudges and compare the results to human authored humorous titles. The headlines\nproduced by the system are considered funny 36\\% of the time by human\nevaluators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relating Neural Text Degeneration to Exposure Bias. (arXiv:2109.08705v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08705","description":"<p>This work focuses on relating two mysteries in neural-based text generation:\nexposure bias, and text degeneration. Despite the long time since exposure bias\nwas mentioned and the numerous studies for its remedy, to our knowledge, its\nimpact on text generation has not yet been verified. Text degeneration is a\nproblem that the widely-used pre-trained language model GPT-2 was recently\nfound to suffer from (Holtzman et al., 2020). Motivated by the unknown\ncausation of the text degeneration, in this paper we attempt to relate these\ntwo mysteries. Specifically, we first qualitatively quantitatively identify\nmistakes made before text degeneration occurs. Then we investigate the\nsignificance of the mistakes by inspecting the hidden states in GPT-2. Our\nresults show that text degeneration is likely to be partly caused by exposure\nbias. We also study the self-reinforcing mechanism of text degeneration,\nexplaining why the mistakes amplify. In sum, our study provides a more concrete\nfoundation for further investigation on exposure bias and text degeneration\nproblems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1\">Ting-Rui Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-device neural speech synthesis. (arXiv:2109.08710v1 [eess.AS])","link":"http://arxiv.org/abs/2109.08710","description":"<p>Recent advances in text-to-speech (TTS) synthesis, such as Tacotron and\nWaveRNN, have made it possible to construct a fully neural network based TTS\nsystem, by coupling the two components together. Such a system is conceptually\nsimple as it only takes grapheme or phoneme input, uses Mel-spectrogram as an\nintermediate feature, and directly generates speech samples. The system\nachieves quality equal or close to natural speech. However, the high\ncomputational cost of the system and issues with robustness have limited their\nusage in real-world speech synthesis applications and products. In this paper,\nwe present key modeling improvements and optimization strategies that enable\ndeploying these models, not only on GPU servers, but also on mobile devices.\nThe proposed system can generate high-quality 24 kHz speech at 5x faster than\nreal time on server and 3x faster than real time on mobile devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Achanta_S/0/1/0/all/0/1\">Sivanand Achanta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antony_A/0/1/0/all/0/1\">Albert Antony</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golipour_L/0/1/0/all/0/1\">Ladan Golipour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangchuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raitio_T/0/1/0/all/0/1\">Tuomo Raitio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasipuram_R/0/1/0/all/0/1\">Ramya Rasipuram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rossi_F/0/1/0/all/0/1\">Francesco Rossi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jennifer Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_J/0/1/0/all/0/1\">Jaimin Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Winarsky_D/0/1/0/all/0/1\">David Winarsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hepeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back-translation for Large-Scale Multilingual Machine Translation. (arXiv:2109.08712v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08712","description":"<p>This paper illustrates our approach to the shared task on large-scale\nmultilingual machine translation in the sixth conference on machine translation\n(WMT-21). This work aims to build a single multilingual translation system with\na hypothesis that a universal cross-language representation leads to better\nmultilingual translation performance. We extend the exploration of different\nback-translation methods from bilingual translation to multilingual\ntranslation. Better performance is obtained by the constrained sampling method,\nwhich is different from the finding of the bilingual translation. Besides, we\nalso explore the effect of vocabularies and the amount of synthetic data.\nSurprisingly, the smaller size of vocabularies perform better, and the\nextensive monolingual English data offers a modest improvement. We submitted to\nboth the small tasks and achieved the second place.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Baohao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1\">Shahram Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewavitharana_S/0/1/0/all/0/1\">Sanjika Hewavitharana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08722","description":"<p>Prerequisite chain learning helps people acquire new knowledge efficiently.\nWhile people may quickly determine learning paths over concepts in a domain,\nfinding such paths in other domains can be challenging. We introduce\nDomain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this\ncross-domain prerequisite chain learning task efficiently. Our novel model\nconsists of a variational graph autoencoder (VGAE) and a domain discriminator.\nThe VGAE is trained to predict concept relations through link prediction, while\nthe domain discriminator takes both source and target domain data as input and\nis trained to predict domain labels. Most importantly, this method only needs\nsimple homogeneous graphs as input, compared with the current state-of-the-art\nmodel. We evaluate our model on the LectureBankCD dataset, and results show\nthat our model outperforms recent graph-based benchmarks while using only 1/10\nof graph scale and 1/3 computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The JHU-Microsoft Submission for WMT21 Quality Estimation Shared Task. (arXiv:2109.08724v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08724","description":"<p>This paper presents the JHU-Microsoft joint submission for WMT 2021 quality\nestimation shared task. We only participate in Task 2 (post-editing effort\nestimation) of the shared task, focusing on the target-side word-level quality\nestimation. The techniques we experimented with include Levenshtein Transformer\ntraining and data augmentation with a combination of forward, backward,\nround-trip translation, and pseudo post-editing of the MT output. We\ndemonstrate the competitiveness of our system compared to the widely adopted\nOpenKiwi-XLM baseline. Our system is also the top-ranking system on the MT MCC\nmetric for the English-German language pair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuoyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Few-Shot Intent Classification and Slot Filling. (arXiv:2109.08754v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08754","description":"<p>Intent classification (IC) and slot filling (SF) are two fundamental tasks in\nmodern Natural Language Understanding (NLU) systems. Collecting and annotating\nlarge amounts of data to train deep learning models for such systems is not\nscalable. This problem can be addressed by learning from few examples using\nfast supervised meta-learning techniques such as prototypical networks. In this\nwork, we systematically investigate how contrastive learning and unsupervised\ndata augmentation methods can benefit these existing supervised meta-learning\npipelines for jointly modelled IC/SF tasks. Through extensive experiments\nacross standard IC/SF benchmarks (SNIPS and ATIS), we show that our proposed\nsemi-supervised approaches outperform standard supervised meta-learning\nmethods: contrastive losses in conjunction with prototypical networks\nconsistently outperform the existing state-of-the-art for both IC and SF tasks,\nwhile data augmentation strategies primarily improve few-shot IC by a\nsignificant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Samyadeep Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_K/0/1/0/all/0/1\">Karine lp Kiun Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1\">Amr Sharaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1\">Alex Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohra_V/0/1/0/all/0/1\">Vishal Rohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amoake_M/0/1/0/all/0/1\">Michael Amoake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Hammamy_H/0/1/0/all/0/1\">Hazem El-Hammamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nosakhare_E/0/1/0/all/0/1\">Ehi Nosakhare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramani_V/0/1/0/all/0/1\">Vijay Ramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solar cell patent classification method based on keyword extraction and deep neural network. (arXiv:2109.08796v1 [cs.IR])","link":"http://arxiv.org/abs/2109.08796","description":"<p>With the growing impact of ESG on businesses, research related to renewable\nenergy is receiving great attention. Solar cells are one of them, and\naccordingly, it can be said that the research value of solar cell patent\nanalysis is very high. Patent documents have high research value. Being able to\naccurately analyze and classify patent documents can reveal several important\ntechnical relationships. It can also describe the business trends in that\ntechnology. And when it comes to investment, new industrial solutions will also\nbe inspired and proposed to make important decisions. Therefore, we must\ncarefully analyze patent documents and utilize the value of patents. To solve\nthe solar cell patent classification problem, we propose a keyword extraction\nmethod and a deep neural network-based solar cell patent classification method.\nFirst, solar cell patents are analyzed for pretreatment. It then uses the\nKeyBERT algorithm to extract keywords and key phrases from the patent abstract\nto construct a lexical dictionary. We then build a solar cell patent\nclassification model according to the deep neural network. Finally, we use a\ndeep neural network-based solar cell patent classification model to classify\npower patents, and the training accuracy is greater than 95%. Also, the\nvalidation accuracy is about 87.5%. It can be seen that the deep neural network\nmethod can not only realize the classification of complex and difficult solar\ncell patents, but also have a good classification effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Yongmin Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1\">Dongjin Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_T/0/1/0/all/0/1\">Tak-Sung Heo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-Beta: A Proactive Probabilistic Approach to Text Moderation. (arXiv:2109.08805v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08805","description":"<p>Text moderation for user generated content, which helps to promote healthy\ninteraction among users, has been widely studied and many machine learning\nmodels have been proposed. In this work, we explore an alternative perspective\nby augmenting reactive reviews with proactive forecasting. Specifically, we\npropose a new concept {\\it text toxicity propensity} to characterize the extent\nto which a text tends to attract toxic comments. Beta regression is then\nintroduced to do the probabilistic modeling, which is demonstrated to function\nwell in comprehensive experiments. We also propose an explanation method to\ncommunicate the model decision clearly. Both propensity scoring and\ninterpretation benefit text moderation in a novel manner. Finally, the proposed\nscaling mechanism for the linear model offers useful insights beyond this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Changwei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Pattern Pruning Using Regularization. (arXiv:2109.08814v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08814","description":"<p>Iterative Magnitude Pruning (IMP) is a network pruning method that repeats\nthe process of removing weights with the least magnitudes and retraining the\nmodel. When visualizing the weight matrices of language models pruned by IMP,\nprevious research has shown that a structured pattern emerges, wherein the\nresulting surviving weights tend to prominently cluster in a select few rows\nand columns of the matrix. Though the need for further research in utilizing\nthese structured patterns for potential performance gains has previously been\nindicated, it has yet to be thoroughly studied. We propose SPUR (Structured\nPattern pruning Using Regularization), a novel pruning mechanism that\npreemptively induces structured patterns in compression by adding a\nregularization term to the objective function in the IMP. Our results show that\nSPUR can significantly preserve model performance under high sparsity settings\nregardless of the language or the task. Our contributions are as follows: (i)\nWe propose SPUR, a network pruning mechanism that improves upon IMP regardless\nof the language or the task. (ii) We are the first to empirically verify the\nefficacy of \"structured patterns\" observed previously in pruning research.\n(iii) SPUR is a resource-efficient mechanism in that it does not require\nsignificant additional computations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dongjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Geung-Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyLex: Incoporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08818","description":"<p>Incorporating lexical knowledge into deep learning models has been proved to\nbe very effective for sequence labeling tasks. However, previous works commonly\nhave difficulty dealing with large-scale dynamic lexicons which often cause\nexcessive matching noise and problems of frequent updates. In this paper, we\npropose DyLex, a plug-in lexicon incorporation approach for BERT based sequence\nlabeling tasks. Instead of leveraging embeddings of words in the lexicon as in\nconventional methods, we adopt word-agnostic tag embeddings to avoid\nre-training the representation while updating the lexicon. Moreover, we employ\nan effective supervised lexical knowledge denoising method to smooth out\nmatching noise. Finally, we introduce a col-wise attention based knowledge\nfusion mechanism to guarantee the pluggability of the proposed framework.\nExperiments on ten datasets of three tasks show that the proposed framework\nachieves new SOTA, even with very large scale lexicons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1\">Guang-Yuan Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems. (arXiv:2109.08820v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08820","description":"<p>Most prior work on task-oriented dialogue systems is restricted to supporting\ndomain APIs. However, users may have requests that are out of the scope of\nthese APIs. This work focuses on identifying such user requests. Existing\nmethods for this task mainly rely on fine-tuning pre-trained models on large\nannotated data. We propose a novel method, REDE, based on adaptive\nrepresentation learning and density estimation. REDE can be applied to\nzero-shot cases, and quickly learns a high-performing detector with only a few\nshots by updating less than 3K parameters. We demonstrate REDE's competitive\nperformance on DSTC9 data and our newly collected test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes. (arXiv:2109.08828v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08828","description":"<p>Empathy is a complex cognitive ability based on the reasoning of others'\naffective states. In order to better understand others and express stronger\nempathy in dialogues, we argue that two issues must be tackled at the same\ntime: (i) identifying which word is the cause for the other's emotion from his\nor her utterance and (ii) reflecting those specific words in the response\ngeneration. However, previous approaches for recognizing emotion cause words in\ntext require sub-utterance level annotations, which can be demanding. Taking\ninspiration from social cognition, we leverage a generative estimator to infer\nemotion cause words from utterances with no word-level label. Also, we\nintroduce a novel method based on pragmatics to make dialogue models focus on\ntargeted words in the input during generation. Our method is applicable to any\ndialogue models with no additional training on the fly. We show our approach\nimproves multiple best-performing dialogue agents on generating more focused\nempathetic responses in terms of both automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeongchang Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MM-Deacon: Multimodal molecular domain embedding analysis via contrastive learning. (arXiv:2109.08830v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08830","description":"<p>Molecular representation learning plays an essential role in cheminformatics.\nRecently, language model-based approaches have been popular as an alternative\nto traditional expert-designed features to encode molecules. However, these\napproaches only utilize a single modality for representing molecules. Driven by\nthe fact that a given molecule can be described through different modalities\nsuch as Simplified Molecular Line Entry System (SMILES), The International\nUnion of Pure and Applied Chemistry (IUPAC), and The IUPAC International\nChemical Identifier (InChI), we propose a multimodal molecular embedding\ngeneration approach called MM-Deacon (multimodal molecular domain embedding\nanalysis via contrastive learning). MM-Deacon is trained using SMILES and IUPAC\nmolecule representations as two different modalities. First, SMILES and IUPAC\nstrings are encoded by using two different transformer-based language models\nindependently, then the contrastive loss is utilized to bring these encoded\nrepresentations from different modalities closer to each other if they belong\nto the same molecule, and to push embeddings farther from each other if they\nbelong to different molecules. We evaluate the robustness of our molecule\nembeddings on molecule clustering, cross-modal molecule search, drug similarity\nassessment and drug-drug interaction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhihui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pramod Kumar Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_R/0/1/0/all/0/1\">Robin Abraham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TVRecap: A Dataset for Generating Stories with Character Descriptions. (arXiv:2109.08833v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08833","description":"<p>We introduce TVRecap, a story generation dataset that requires generating\ndetailed TV show episode recaps from a brief summary and a set of documents\ndescribing the characters involved. Unlike other story generation datasets,\nTVRecap contains stories that are authored by professional screenwriters and\nthat feature complex interactions among multiple characters. Generating stories\nin TVRecap requires drawing relevant information from the lengthy provided\ndocuments about characters based on the brief summary. In addition, by swapping\nthe input and output, TVRecap can serve as a challenging testbed for\nabstractive summarization. We create TVRecap from fan-contributed websites,\nwhich allows us to collect 26k episode recaps with 1868.7 tokens on average.\nEmpirically, we take a hierarchical story generation approach and find that the\nneural model that uses oracle content selectors for character descriptions\ndemonstrates the best performance on automatic metrics, showing the potential\nof our dataset to inspire future research on story generation with constraints.\nQualitative analysis shows that the best-performing model sometimes generates\ncontent that is unfaithful to the short summaries, suggesting promising\ndirections for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechNAS: Towards Better Trade-off between Latency and Accuracy for Large-Scale Speaker Verification. (arXiv:2109.08839v1 [cs.SD])","link":"http://arxiv.org/abs/2109.08839","description":"<p>Recently, x-vector has been a successful and popular approach for speaker\nverification, which employs a time delay neural network (TDNN) and statistics\npooling to extract speaker characterizing embedding from variable-length\nutterances. Improvement upon the x-vector has been an active research area, and\nenormous neural networks have been elaborately designed based on the x-vector,\neg, extended TDNN (E-TDNN), factorized TDNN (F-TDNN), and densely connected\nTDNN (D-TDNN). In this work, we try to identify the optimal architectures from\na TDNN based search space employing neural architecture search (NAS), named\nSpeechNAS. Leveraging the recent advances in the speaker recognition, such as\nhigh-order statistics pooling, multi-branch mechanism, D-TDNN and angular\nadditive margin softmax (AAM) loss with a minimum hyper-spherical energy (MHE),\nSpeechNAS automatically discovers five network architectures, from SpeechNAS-1\nto SpeechNAS-5, of various numbers of parameters and GFLOPs on the large-scale\ntext-independent speaker recognition dataset VoxCeleb1. Our derived best neural\nnetwork achieves an equal error rate (EER) of 1.02% on the standard test set of\nVoxCeleb1, which surpasses previous TDNN based state-of-the-art approaches by a\nlarge margin. Code and trained weights are in\nhttps://github.com/wentaozhu/speechnas.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tianlong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jixiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Feng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Engineering for US State Legislative Hearings: Stance, Affiliation, Engagement and Absentees. (arXiv:2109.08855v1 [cs.IR])","link":"http://arxiv.org/abs/2109.08855","description":"<p>In US State government legislatures, most of the activity occurs in\ncommittees made up of lawmakers discussing bills. When analyzing, classifying\nor summarizing these committee proceedings, some important features become\nbroadly interesting. In this paper, we engineer four useful features, two\napplying to lawmakers (engagement and absence), and two to non-lawmakers\n(stance and affiliation). We propose a system to automatically track the\naffiliation of organizations in public comments and whether the organizational\nrepresentative supports or opposes the bill. The model tracking affiliation\nachieves an F1 of 0.872 while the support determination has an F1 of 0.979.\nAdditionally, a metric to compute legislator engagement and absenteeism is also\nproposed and as proof-of-concept, a list of the most and least engaged\nlegislators over one full California legislative session is presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grace_J/0/1/0/all/0/1\">Josh Grace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosmood_F/0/1/0/all/0/1\">Foaad Khosmood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emily: Developing An Emotion-affective Open-Domain Chatbot with Knowledge Graph-based Persona. (arXiv:2109.08875v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08875","description":"<p>In this paper, we describe approaches for developing Emily, an\nemotion-affective open-domain chatbot. Emily can perceive a user's negative\nemotion state and offer supports by positively converting the user's emotion\nstates. This is done by finetuning a pretrained dialogue model upon data\ncapturing dialogue contexts and desirable emotion states transition across\nturns. Emily can differentiate a general open-domain dialogue utterance with\nquestions relating to personal information. By leveraging a question-answering\napproach based on knowledge graphs to handle personal information, Emily\nmaintains personality consistency. We evaluate Emily against a few\nstate-of-the-art open-domain chatbots and show the effects of the proposed\napproaches in emotion affecting and addressing personality inconsistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiaoling Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chong Hsuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haonan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Ximing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation. (arXiv:2109.08877v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08877","description":"<p>In this paper, we provide a bilingual parallel human-to-human recommendation\ndialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging\ntask of multilingual and cross-lingual conversational recommendation. The\ndifference between DuRecDial 2.0 and existing conversational recommendation\ndatasets is that the data item (Profile, Goal, Knowledge, Context, Response) in\nDuRecDial 2.0 is annotated in two languages, both English and Chinese, while\nother datasets are built with the setting of a single language. We collect 8.2k\ndialogs aligned across English and Chinese languages (16.5k dialogs and 255k\nutterances in total) that are annotated by crowdsourced workers with strict\nquality control procedure. We then build monolingual, multilingual, and\ncross-lingual conversational recommendation baselines on DuRecDial 2.0.\nExperiment results show that the use of additional English data can bring\nperformance improvement for Chinese conversational recommendation, indicating\nthe benefits of DuRecDial 2.0. Finally, this dataset provides a challenging\ntestbed for future studies of monolingual, multilingual, and cross-lingual\nconversational recommendation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zheng-Yu Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Joint Intent Detection and Slot Filling via Higher-order Attention. (arXiv:2109.08890v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08890","description":"<p>Intent detection (ID) and Slot filling (SF) are two major tasks in spoken\nlanguage understanding (SLU). Recently, attention mechanism has been shown to\nbe effective in jointly optimizing these two tasks in an interactive manner.\nHowever, latest attention-based works concentrated only on the first-order\nattention design, while ignoring the exploration of higher-order attention\nmechanisms. In this paper, we propose a BiLinear attention block, which\nleverages bilinear pooling to simultaneously exploit both the contextual and\nchannel-wise bilinear attention distributions to capture the second-order\ninteractions between the input intent or slot features. Higher and even\ninfinity order interactions are built by stacking numerous blocks and assigning\nExponential Linear Unit (ELU) to blocks. Before the decoding stage, we\nintroduce the Dynamic Feature Fusion Layer to implicitly fuse intent and slot\ninformation in a more effective way. Technically, instead of simply\nconcatenating intent and slot features, we first compute two correlation\nmatrices to weight on two features. Furthermore, we present Higher-order\nAttention Network for the SLU tasks. Experiments on two benchmark datasets show\nthat our approach yields improvements compared with the state-of-the-art\napproach. We also provide discussion to demonstrate the effectiveness of the\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongsheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency distance minimization predicts compression. (arXiv:2109.08900v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08900","description":"<p>Dependency distance minimization (DDm) is a well-established principle of\nword order. It has been predicted theoretically that DDm implies compression,\nnamely the minimization of word lengths. This is a second order prediction\nbecause it links a principle with another principle, rather than a principle\nand a manifestation as in a first order prediction. Here we test that second\norder prediction with a parallel collection of treebanks controlling for\nannotation style with Universal Dependencies and Surface-Syntactic Universal\nDependencies. To test it, we use a recently introduced score that has many\nmathematical and statistical advantages with respect to the widely used sum of\ndependency distances. We find that the prediction is confirmed by the new score\nwhen word lengths are measured in phonemes, independently of the annotation\nstyle, but not when word lengths are measured in syllables. In contrast, one of\nthe most widely used scores, i.e. the sum of dependency distances, fails to\nconfirm that prediction, showing the weakness of raw dependency distances for\nresearch on word order. Finally, our findings expand the theory of natural\ncommunication by linking two distinct levels of organization, namely syntax\n(word order) and word internal structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Detoxification using Large Pre-trained Neural Models. (arXiv:2109.08914v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08914","description":"<p>We present two novel unsupervised methods for eliminating toxicity in text.\nOur first method combines two recent ideas: (1) guidance of the generation\nprocess with small style-conditional language models and (2) use of\nparaphrasing models to perform style transfer. We use a well-performing\nparaphraser guided by style-trained language models to keep the text content\nand remove toxicity. Our second method uses BERT to replace toxic words with\ntheir non-offensive synonyms. We make the method more flexible by enabling BERT\nto replace mask tokens with a variable number of words. Finally, we present the\nfirst large-scale comparative study of style transfer models on the task of\ntoxicity removal. We compare our models with a number of methods for style\ntransfer. The models are evaluated in a reference-free way using a combination\nof unsupervised style transfer metrics. Both methods we suggest yield new SOTA\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1\">David Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voronov_A/0/1/0/all/0/1\">Anton Voronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dementieva_D/0/1/0/all/0/1\">Daryna Dementieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logacheva_V/0/1/0/all/0/1\">Varvara Logacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozlova_O/0/1/0/all/0/1\">Olga Kozlova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semenov_N/0/1/0/all/0/1\">Nikita Semenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking the Combinatorial Generalizability of Complex Query Answering on Knowledge Graphs. (arXiv:2109.08925v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08925","description":"<p>Complex Query Answering (CQA) is an important reasoning task on knowledge\ngraphs. Current CQA learning models have been shown to be able to generalize\nfrom atomic operators to more complex formulas, which can be regarded as the\ncombinatorial generalizability. In this paper, we present EFO-1-QA, a new\ndataset to benchmark the combinatorial generalizability of CQA models by\nincluding 301 different queries types, which is 20 times larger than existing\ndatasets. Besides, our work, for the first time, provides a benchmark to\nevaluate and analyze the impact of different operators and normal forms by\nusing (a) 7 choices of the operator systems and (b) 9 forms of complex queries.\nSpecifically, we provide the detailed study of the combinatorial\ngeneralizability of two commonly used operators, i.e., projection and\nintersection, and justify the impact of the forms of queries given the\ncanonical choice of operators. Our code and data can provide an effective\npipeline to benchmark CQA models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic. (arXiv:2109.08927v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08927","description":"<p>Natural language inference (NLI) aims to determine the logical relationship\nbetween two sentences among the target labels Entailment, Contradiction, and\nNeutral. In recent years, deep learning models have become a prevailing\napproach to NLI, but they lack interpretability and explainability. In this\nwork, we address the explainability for NLI by weakly supervised logical\nreasoning, and propose an Explainable Phrasal Reasoning (EPR) approach. Our\nmodel first detects phrases as the semantic unit and aligns corresponding\nphrases. Then, the model predicts the NLI label for the aligned phrases, and\ninduces the sentence label by fuzzy logic formulas. Our EPR is almost\neverywhere differentiable and thus the system can be trained end-to-end in a\nweakly supervised manner. We annotated a corpus and developed a set of metrics\nto evaluate phrasal reasoning. Results show that our EPR yields much more\nmeaningful explanations in terms of F scores than previous studies. To the best\nof our knowledge, we are the first to develop a weakly supervised phrasal\nreasoning model for the NLI task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Atharva Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zi Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Temporal Question Answering on Knowledge Graphs. (arXiv:2109.08935v1 [cs.IR])","link":"http://arxiv.org/abs/2109.08935","description":"<p>Question answering over knowledge graphs (KG-QA) is a vital topic in IR.\nQuestions with temporal intent are a special class of practical importance, but\nhave not received much attention in research. This work presents EXAQT, the\nfirst end-to-end system for answering complex temporal questions that have\nmultiple entities and predicates, and associated temporal conditions. EXAQT\nanswers natural language questions over KGs in two stages, one geared towards\nhigh recall, the other towards precision at top ranks. The first step computes\nquestion-relevant compact subgraphs within the KG, and judiciously enhances\nthem with pertinent temporal facts, using Group Steiner Trees and fine-tuned\nBERT models. The second step constructs relational graph convolutional networks\n(R-GCNs) from the first step's output, and enhances the R-GCNs with time-aware\nentity embeddings and attention over temporal relations. We evaluate EXAQT on\nTimeQuestions, a large dataset of 16k temporal questions we compiled from a\nvariety of general purpose KG-QA benchmarks. Results show that EXAQT\noutperforms three state-of-the-art systems for answering complex questions over\nKGs, thereby justifying specialized treatment of temporal QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReaSCAN: Compositional Reasoning in Language Grounding. (arXiv:2109.08994v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08994","description":"<p>The ability to compositionally map language to referents, relations, and\nactions is an essential component of language understanding. The recent gSCAN\ndataset (Ruis et al. 2020, NeurIPS) is an inspiring attempt to assess the\ncapacity of models to learn this kind of grounding in scenarios involving\nnavigational instructions. However, we show that gSCAN's highly constrained\ndesign means that it does not require compositional interpretation and that\nmany details of its instructions and scenarios are not required for task\nsuccess. To address these limitations, we propose ReaSCAN, a benchmark dataset\nthat builds off gSCAN but requires compositional language interpretation and\nreasoning about entities and relations. We assess two models on ReaSCAN: a\nmulti-modal baseline and a state-of-the-art graph convolutional neural model.\nThese experiments show that ReaSCAN is substantially harder than gSCAN for both\nneural architectures. This suggests that ReaSCAN can serve as a valuable\nbenchmark for advancing our understanding of models' compositional\ngeneralization and reasoning capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1\">Elisa Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1\">Desmond C. Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting semantic lexicons using word embeddings and transfer learning. (arXiv:2109.09010v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09010","description":"<p>Sentiment-aware intelligent systems are essential to a wide array of\napplications including marketing, political campaigns, recommender systems,\nbehavioral economics, social psychology, and national security. These\nsentiment-aware intelligent systems are driven by language models which broadly\nfall into two paradigms: 1. Lexicon-based and 2. Contextual. Although recent\ncontextual models are increasingly dominant, we still see demand for\nlexicon-based models because of their interpretability and ease of use. For\nexample, lexicon-based models allow researchers to readily determine which\nwords and phrases contribute most to a change in measured sentiment. A\nchallenge for any lexicon-based approach is that the lexicon needs to be\nroutinely expanded with new words and expressions. Crowdsourcing annotations\nfor semantic dictionaries may be an expensive and time-consuming task. Here, we\npropose two models for predicting sentiment scores to augment semantic lexicons\nat a relatively low cost using word embeddings and transfer learning. Our first\nmodel establishes a baseline employing a simple and shallow neural network\ninitialized with pre-trained word embeddings using a non-contextual approach.\nOur second model improves upon our baseline, featuring a deep Transformer-based\nnetwork that brings to bear word definitions to estimate their lexical\npolarity. Our evaluation shows that both models are able to score new words\nwith a similar accuracy to reviewers from Amazon Mechanical Turk, but at a\nfraction of the cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oort_C/0/1/0/all/0/1\">Colin Van Oort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">Mikaela Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Machine Learning Pipeline to Examine Political Bias with Congressional Speeches. (arXiv:2109.09014v1 [cs.CY])","link":"http://arxiv.org/abs/2109.09014","description":"<p>Computational methods to model political bias in social media involve several\nchallenges due to heterogeneity, high-dimensional, multiple modalities, and the\nscale of the data. Political bias in social media has been studied in multiple\nviewpoints like media bias, political ideology, echo chambers, and\ncontroversies using machine learning pipelines. Most of the current methods\nrely heavily on the manually-labeled ground-truth data for the underlying\npolitical bias prediction tasks. Limitations of such methods include\nhuman-intensive labeling, labels related to only a specific problem, and the\ninability to determine the near future bias state of a social media\nconversation. In this work, we address such problems and give machine learning\napproaches to study political bias in two ideologically diverse social media\nforums: Gab and Twitter without the availability of human-annotated data. Our\nproposed methods exploit the use of transcripts collected from political\nspeeches in US congress to label the data and achieve the highest accuracy of\n70.5% and 65.1% in Twitter and Gab data respectively to predict political bias.\nWe also present a machine learning approach that combines features from\ncascades and text to forecast cascade's political bias with an accuracy of\nabout 85%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+hajare_P/0/1/0/all/0/1\">Prasad hajare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamal_S/0/1/0/all/0/1\">Sadia Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1\">Siddharth Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagavathi_A/0/1/0/all/0/1\">Arunkumar Bagavathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Relation-Guided Type-Sentence Alignment for Long-Tail Relation Extraction with Distant Supervision. (arXiv:2109.09036v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09036","description":"<p>Distant supervision uses triple facts in knowledge graphs to label a corpus\nfor relation extraction, leading to wrong labeling and long-tail problems. Some\nworks use the hierarchy of relations for knowledge transfer to long-tail\nrelations. However, a coarse-grained relation often implies only an attribute\n(e.g., domain or topic) of the distant fact, making it hard to discriminate\nrelations based solely on sentence semantics. One solution is resorting to\nentity types, but open questions remain about how to fully leverage the\ninformation of entity types and how to align multi-granular entity types with\nsentences. In this work, we propose a novel model to enrich\ndistantly-supervised sentences with entity types. It consists of (1) a pairwise\ntype-enriched sentence encoding module injecting both context-free and -related\nbackgrounds to alleviate sentence-level wrong labeling, and (2) a hierarchical\ntype-sentence alignment module enriching a sentence with the triple fact's\nbasic attributes to support long-tail relations. Our model achieves new\nstate-of-the-art results in overall and long-tail performance on benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Enhanced Evidence Retrieval for Counterargument Generation. (arXiv:2109.09057v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09057","description":"<p>Finding counterevidence to statements is key to many tasks, including\ncounterargument generation. We build a system that, given a statement,\nretrieves counterevidence from diverse sources on the Web. At the core of this\nsystem is a natural language inference (NLI) model that determines whether a\ncandidate sentence is valid counterevidence or not. Most NLI models to date,\nhowever, lack proper reasoning abilities necessary to find counterevidence that\ninvolves complex inference. Thus, we present a knowledge-enhanced NLI model\nthat aims to handle causality- and example-based inference by incorporating\nknowledge graphs. Our NLI model outperforms baselines for NLI tasks, especially\nfor instances that require the targeted inference. In addition, this NLI model\nfurther improves the counterevidence retrieval system, notably finding complex\ncounterevidence better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yohan Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Haneul Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_J/0/1/0/all/0/1\">JinYeong Bak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Chris Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Training with Contrastive Learning in NLP. (arXiv:2109.09075v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09075","description":"<p>For years, adversarial training has been extensively studied in natural\nlanguage processing (NLP) settings. The main goal is to make models robust so\nthat similar inputs derive in semantically similar outcomes, which is not a\ntrivial problem since there is no objective measure of semantic similarity in\nlanguage. Previous works use an external pre-trained NLP model to tackle this\nchallenge, introducing an extra training stage with huge memory consumption\nduring training. However, the recent popular approach of contrastive learning\nin language processing hints a convenient way of obtaining such similarity\nrestrictions. The main advantage of the contrastive learning approach is that\nit aims for similar data points to be mapped close to each other and further\nfrom different ones in the representation space. In this work, we propose\nadversarial training with contrastive learning (ATCL) to adversarially train a\nlanguage processing task using the benefits of contrastive learning. The core\nidea is to make linear perturbations in the embedding space of the input via\nfast gradient methods (FGM) and train the model to keep the original and\nperturbed representations close via contrastive learning. In NLP experiments,\nwe applied ATCL to language modeling and neural machine translation tasks. The\nresults show not only an improvement in the quantitative (perplexity and BLEU)\nscores when compared to the baselines, but ATCL also achieves good qualitative\nresults in the semantic level for both tasks without using a pre-trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rim_D/0/1/0/all/0/1\">Daniela N. Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_D/0/1/0/all/0/1\">DongNyeong Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study. (arXiv:2109.09105v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09105","description":"<p>Language Models (LMs) have been ubiquitously leveraged in various tasks\nincluding spoken language understanding (SLU). Spoken language requires careful\nunderstanding of speaker interactions, dialog states and speech induced\nmultimodal behaviors to generate a meaningful representation of the\nconversation.In this work, we propose to dissect SLU into three representative\nproperties:conversational(disfluency, pause, overtalk), channel(speaker-type,\nturn-tasks) andASR(insertion, deletion,substitution). We probe BERT based\nlanguage models (BERT, RoBERTa) trained on spoken transcripts to investigate\nits ability to understand multifarious properties in absence of any speech\ncues. Empirical results indicate that LM is surprisingly good at capturing\nconversational properties such as pause prediction and overtalk detection from\nlexical tokens. On the downsides, the LM scores low on turn-tasks and ASR\nerrors predictions. Additionally, pre-training the LM on spoken transcripts\nrestrain its linguistic understanding. Finally,we establish the efficacy and\ntransferability of the mentioned properties on two benchmark datasets:\nSwitchboard Dialog Act and Disfluency datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_M/0/1/0/all/0/1\">Mukuntha Narayanan Sundararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vepa_J/0/1/0/all/0/1\">Jithendra Vepa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Long-Range Language Models Actually Use Long-Range Context?. (arXiv:2109.09115v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09115","description":"<p>Language models are generally trained on short, truncated input sequences,\nwhich limits their ability to use discourse-level information present in\nlong-range context to improve their predictions. Recent efforts to improve the\nefficiency of self-attention have led to a proliferation of long-range\nTransformer language models, which can process much longer sequences than\nmodels of the past. However, the ways in which such models take advantage of\nthe long-range context remain unclear. In this paper, we perform a fine-grained\nanalysis of two long-range Transformer language models (including the\n\\emph{Routing Transformer}, which achieves state-of-the-art perplexity on the\nPG-19 long-sequence LM benchmark dataset) that accept input sequences of up to\n8K tokens. Our results reveal that providing long-range context (i.e., beyond\nthe previous 2K tokens) to these models only improves their predictions on a\nsmall set of tokens (e.g., those that can be copied from the distant context)\nand does not help at all for sentence-level prediction tasks. Finally, we\ndiscover that PG-19 contains a variety of different document types and domains,\nand that long-range context helps most for literary novels (as opposed to\ntextbooks or magazines).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattarella_Micke_A/0/1/0/all/0/1\">Andrew Mattarella-Micke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preventing Author Profiling through Zero-Shot Multilingual Back-Translation. (arXiv:2109.09133v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09133","description":"<p>Documents as short as a single sentence may inadvertently reveal sensitive\ninformation about their authors, including e.g. their gender or ethnicity.\nStyle transfer is an effective way of transforming texts in order to remove any\ninformation that enables author profiling. However, for a number of current\nstate-of-the-art approaches the improved privacy is accompanied by an\nundesirable drop in the down-stream utility of the transformed data. In this\npaper, we propose a simple, zero-shot way to effectively lower the risk of\nauthor profiling through multilingual back-translation using off-the-shelf\ntranslation models. We compare our models with five representative text style\ntransfer models on three datasets across different domains. Results from both\nan automatic and a human evaluation show that our approach achieves the best\noverall performance while requiring no training data. We are able to lower the\nadversarial prediction of gender and race by up to $22\\%$ while retaining\n$95\\%$ of the original utility on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davody_A/0/1/0/all/0/1\">Ali Davody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1\">Thomas Kleinbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition. (arXiv:2109.09161v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09161","description":"<p>Unifying acoustic and linguistic representation learning has become\nincreasingly crucial to transfer the knowledge learned on the abundance of\nhigh-resource language data for low-resource speech recognition. Existing\napproaches simply cascade pre-trained acoustic and language models to learn the\ntransfer from speech to text. However, how to solve the representation\ndiscrepancy of speech and text is unexplored, which hinders the utilization of\nacoustic and linguistic information. Moreover, previous works simply replace\nthe embedding layer of the pre-trained language model with the acoustic\nfeatures, which may cause the catastrophic forgetting problem. In this work, we\nintroduce Wav-BERT, a cooperative acoustic and linguistic representation\nlearning method to fuse and utilize the contextual information of speech and\ntext. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a\nlanguage model (BERT) into an end-to-end trainable framework. A Representation\nAggregation Module is designed to aggregate acoustic and linguistic\nrepresentation, and an Embedding Attention Module is introduced to incorporate\nacoustic information into BERT, which can effectively facilitate the\ncooperation of two pre-trained models and thus boost the representation\nlearning. Extensive experiments show that our Wav-BERT significantly\noutperforms the existing approaches and achieves state-of-the-art performance\non low-resource speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guolin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yubei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1\">Ke Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FST Morphological Analyser and Generator for Mapud\\\"ungun. (arXiv:2109.09176v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09176","description":"<p>Following the Mapuche grammar by Smeets, this article describes the main\nmorphophonological aspects of Mapud\\\"ungun, explaining what triggers them and\nthe contexts where they arise. We present a computational approach producing a\nfinite state morphological analyser (and generator) capable of classifying and\nappropriately tagging all the components (roots and suffixes) that interact in\na Mapuche word form. The bulk of the article focuses on presenting details\nabout the morphology of Mapud\\\"ungun verb and its formalisation using FOMA. A\nsystem evaluation process and its results are also present in this article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandia_A/0/1/0/all/0/1\">Andr&#xe9;s Chand&#xed;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Dynamic based data filtering may not work for NLP datasets. (arXiv:2109.09191v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09191","description":"<p>The recent increase in dataset size has brought about significant advances in\nnatural language understanding. These large datasets are usually collected\nthrough automation (search engines or web crawlers) or crowdsourcing which\ninherently introduces incorrectly labeled data. Training on these datasets\nleads to memorization and poor generalization. Thus, it is pertinent to develop\ntechniques that help in the identification and isolation of mislabelled data.\nIn this paper, we study the applicability of the Area Under the Margin (AUM)\nmetric to identify and remove/rectify mislabelled examples in NLP datasets. We\nfind that mislabelled samples can be filtered using the AUM metric in NLP\ndatasets but it also removes a significant number of correctly labeled points\nand leads to the loss of a large amount of relevant language information. We\nshow that models rely on the distributional information instead of relying on\nsyntactic and semantic representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_A/0/1/0/all/0/1\">Arka Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagar_M/0/1/0/all/0/1\">Monika Dagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prachi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_V/0/1/0/all/0/1\">Varun Menon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero-Label Language Learning. (arXiv:2109.09193v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09193","description":"<p>This paper explores zero-label learning in Natural Language Processing (NLP),\nwhereby no human-annotated data is used anywhere during training and models are\ntrained purely on synthetic data. At the core of our framework is a novel\napproach for better leveraging the powerful pretrained language models.\nSpecifically, inspired by the recent success of few-shot inference on GPT-3, we\npresent a training data creation procedure named Unsupervised Data Generation\n(UDG), which leverages few-shot prompts to synthesize high-quality training\ndata without real human annotations. Our method enables zero-label learning as\nwe train task-specific models solely on the synthetic data, yet we achieve\nbetter or comparable results from strong baseline models trained on\nhuman-labeled data. Furthermore, when mixed with labeled data, our approach\nserves as a highly effective data augmentation procedure, achieving new\nstate-of-the-art results on the SuperGLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Crowdsourcing Protocols for Evaluatingthe Factual Consistency of Summaries. (arXiv:2109.09195v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09195","description":"<p>Current pre-trained models applied to summarization are prone to factual\ninconsistencies which either misrepresent the source text or introduce\nextraneous information. Thus, comparing the factual consistency of summaries is\nnecessary as we develop improved models. However, the optimal human evaluation\nsetup for factual consistency has not been standardized. To address this issue,\nwe crowdsourced evaluations for factual consistency using the rating-based\nLikert scale and ranking-based Best-Worst Scaling protocols, on 100 articles\nfrom each of the CNN-Daily Mail and XSum datasets over four state-of-the-art\nmodels, to determine the most reliable evaluation framework. We find that\nranking-based protocols offer a more reliable measure of summary quality across\ndatasets, while the reliability of Likert ratings depends on the target dataset\nand the evaluation design. Our crowdsourcing templates and summary evaluations\nwill be publicly available to facilitate future research on factual consistency\nin summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Ziming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_G/0/1/0/all/0/1\">Griffin Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Borui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization. (arXiv:2109.09209v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09209","description":"<p>We study generating abstractive summaries that are faithful and factually\nconsistent with the given articles. A novel contrastive learning formulation is\npresented, which leverages both reference summaries, as positive training data,\nand automatically generated erroneous summaries, as negative training data, to\ntrain summarization systems that are better at distinguishing between them. We\nfurther design four types of strategies for creating negative samples, to\nresemble errors made commonly by two state-of-the-art models, BART and PEGASUS,\nfound in our new human annotations of summary errors. Experiments on XSum and\nCNN/Daily Mail show that our contrastive learning framework is robust across\ndatasets and models. It consistently produces more factual summaries than\nstrong comparisons with post error correction, entailment-based reranking, and\nunlikelihood training, according to QA-based factuality evaluation. Human\njudges echo the observation and find that our model summaries correct more\nerrors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPV at CheckThat! 2021: Mitigating Cultural Differences for Identifying Multilingual Check-worthy Claims. (arXiv:2109.09232v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09232","description":"<p>Identifying check-worthy claims is often the first step of automated\nfact-checking systems. Tackling this task in a multilingual setting has been\nunderstudied. Encoding inputs with multilingual text representations could be\none approach to solve the multilingual check-worthiness detection. However,\nthis approach could suffer if cultural bias exists within the communities on\ndetermining what is check-worthy.In this paper, we propose a language\nidentification task as an auxiliary task to mitigate unintended bias.With this\npurpose, we experiment joint training by using the datasets from CLEF-2021\nCheckThat!, that contain tweets in English, Arabic, Bulgarian, Spanish and\nTurkish. Our results show that joint training of language identification and\ncheck-worthy claim detection tasks can provide performance gains for some of\nthe selected languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1\">Ipek Baris Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Angel Felipe Magnoss&#xe3;o de Paula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified and Multilingual Author Profiling for Detecting Haters. (arXiv:2109.09233v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09233","description":"<p>This paper presents a unified user profiling framework to identify hate\nspeech spreaders by processing their tweets regardless of the language. The\nframework encodes the tweets with sentence transformers and applies an\nattention mechanism to select important tweets for learning user profiles.\nFurthermore, the attention layer helps to explain why a user is a hate speech\nspreader by producing attention weights at both token and post level. Our\nproposed model outperformed the state-of-the-art multilingual transformer\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1\">Ipek Baris Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Angel Felipe Magnoss&#xe3;o de Paula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional probing: measuring usable information beyond a baseline. (arXiv:2109.09234v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09234","description":"<p>Probing experiments investigate the extent to which neural representations\nmake properties -- like part-of-speech -- predictable. One suggests that a\nrepresentation encodes a property if probing that representation produces\nhigher accuracy than probing a baseline representation like non-contextual word\nembeddings. Instead of using baselines as a point of comparison, we're\ninterested in measuring information that is contained in the representation but\nnot in the baseline. For example, current methods can detect when a\nrepresentation is more useful than the word identity (a baseline) for\npredicting part-of-speech; however, they cannot detect when the representation\nis predictive of just the aspects of part-of-speech not explainable by the word\nidentity. In this work, we extend a theory of usable information called\n$\\mathcal{V}$-information and propose conditional probing, which explicitly\nconditions on the information in the baseline. In a case study, we find that\nafter conditioning on non-contextual word embeddings, properties like\npart-of-speech are accessible at deeper layers of a network than previously\nthought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1\">John Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MirrorWiC: On Eliciting Word-in-Context Representations from Pretrained Language Models. (arXiv:2109.09237v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09237","description":"<p>Recent work indicated that pretrained language models (PLMs) such as BERT and\nRoBERTa can be transformed into effective sentence and word encoders even via\nsimple self-supervised techniques. Inspired by this line of work, in this paper\nwe propose a fully unsupervised approach to improving word-in-context (WiC)\nrepresentations in PLMs, achieved via a simple and efficient WiC-targeted\nfine-tuning procedure: MirrorWiC. The proposed method leverages only raw texts\nsampled from Wikipedia, assuming no sense-annotated data, and learns\ncontext-aware word representations within a standard contrastive learning\nsetup. We experiment with a series of standard and comprehensive WiC benchmarks\nacross multiple languages. Our proposed fully unsupervised MirrorWiC models\nobtain substantial gains over off-the-shelf PLMs across all monolingual,\nmultilingual and cross-lingual setups. Moreover, on some standard WiC\nbenchmarks, MirrorWiC is even on-par with supervised models fine-tuned with\nin-task data and sense labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Volatility of the Political agenda in Public Opinion and News Media. (arXiv:1808.09037v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/1808.09037","description":"<p>Recent election surprises, regime changes, and political shocks indicate that\npolitical agendas have become more fast-moving and volatile. The ability to\nmeasure the complex dynamics of agenda change and capture the nature and extent\nof volatility in political systems is therefore more crucial than ever before.\nThis study proposes a definition and operationalization of volatility that\ncombines insights from political science, communications, information theory,\nand computational techniques. The proposed measures of fractionalization and\nagenda change encompass the shifting salience of issues in the agenda as a\nwhole and allow the study of agendas across different domains. We evaluate\nthese metrics and compare them to other measures such as issue-level survival\nrates and the Pedersen Index, which uses public-opinion poll data to measure\npublic agendas, as well as traditional media content to measure media agendas\nin the UK and Germany. We show how these measures complement existing\napproaches and could be employed in future agenda-setting research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camargo_C/0/1/0/all/0/1\">Chico Q. Camargo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_P/0/1/0/all/0/1\">Peter John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margetts_H/0/1/0/all/0/1\">Helen Z. Margetts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NatCat: Weakly Supervised Text Classification with Naturally Annotated Resources. (arXiv:2009.14335v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.14335","description":"<p>We describe NatCat, a large-scale resource for text classification\nconstructed from three data sources: Wikipedia, Stack Exchange, and Reddit.\nNatCat consists of document-category pairs derived from manual curation that\noccurs naturally within online communities. To demonstrate its usefulness, we\nbuild general purpose text classifiers by training on NatCat and evaluate them\non a suite of 11 text classification tasks (CatEval), reporting large\nimprovements compared to prior work. We benchmark different modeling choices\nand resource combinations and show how tasks benefit from particular NatCat\ndata sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zewei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exemplar-Controllable Paraphrasing and Translation using Bitext. (arXiv:2010.05856v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.05856","description":"<p>Most prior work on exemplar-based syntactically controlled paraphrase\ngeneration relies on automatically-constructed large-scale paraphrase datasets,\nwhich are costly to create. We sidestep this prerequisite by adapting models\nfrom prior work to be able to learn solely from bilingual text (bitext).\nDespite only using bitext for training, and in near zero-shot conditions, our\nsingle proposed model can perform four tasks: controlled paraphrase generation\nin both languages and controlled machine translation in both language\ndirections. To evaluate these tasks quantitatively, we create three novel\nevaluation datasets. Our experimental results show that our models achieve\ncompetitive results on controlled paraphrase generation and strong performance\non controlled machine translation. Analysis shows that our models learn to\ndisentangle semantics and syntax in their latent representations, but still\nsuffer from semantic drift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can images help recognize entities? A study of the role of images for Multimodal NER. (arXiv:2010.12712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12712","description":"<p>Multimodal named entity recognition (MNER) requires to bridge the gap between\nlanguage understanding and visual context. While many multimodal neural\ntechniques have been proposed to incorporate images into the MNER task, the\nmodel's ability to leverage multimodal interactions remains poorly understood.\nIn this work, we conduct in-depth analyses of existing multimodal fusion\ntechniques from different perspectives and describe the scenarios where adding\ninformation from the image does not always boost performance. We also study the\nuse of captions as a way to enrich the context for MNER. Experiments on three\ndatasets from popular social platforms expose the bottleneck of existing\nmultimodal models and the situations where using captions is beneficial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_G/0/1/0/all/0/1\">Gustavo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.08091","description":"<p>Sentiment quantification is the task of training, by means of supervised\nlearning, estimators of the relative frequency (also called ``prevalence'') of\nsentiment-related classes (such as \\textsf{Positive}, \\textsf{Neutral},\n\\textsf{Negative}) in a sample of unlabelled texts. This task is especially\nimportant when these texts are tweets, since the final goal of most sentiment\nclassification efforts carried out on Twitter data is actually quantification\n(and not the classification of individual tweets). It is well-known that\nsolving quantification by means of ``classify and count'' (i.e., by classifying\nall unlabelled items by means of a standard classifier and counting the items\nthat have been assigned to a given class) is less than optimal in terms of\naccuracy, and that more accurate quantification methods exist. Gao and\nSebastiani (2016) carried out a systematic comparison of quantification methods\non the task of tweet sentiment quantification. In hindsight, we observe that\nthe experimental protocol followed in that work was weak, and that the\nreliability of the conclusions that were drawn from the results is thus\nquestionable. We now re-evaluate those quantification methods (plus a few more\nmodern ones) on exactly the same same datasets, this time following a now\nconsolidated and much more robust experimental protocol (which also involves\nsimulating the presence, in the test data, of class prevalence values very\ndifferent from those of the training set). This experimental protocol (even\nwithout counting the newly added methods) involves a number of experiments\n5,775 times larger than that of the original study. The results of our\nexperiments are dramatically different from those obtained by Gao and\nSebastiani, and they provide a different, much more solid understanding of the\nrelative strengths and weaknesses of different sentiment quantification\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons Learned. (arXiv:2101.00133v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00133","description":"<p>We review the EfficientQA competition from NeurIPS 2020. The competition\nfocused on open-domain question answering (QA), where systems take natural\nlanguage questions as input and return natural language answers. The aim of the\ncompetition was to build systems that can predict correct answers while also\nsatisfying strict on-disk memory budgets. These memory budgets were designed to\nencourage contestants to explore the trade-off between storing retrieval\ncorpora or the parameters of learned models. In this report, we describe the\nmotivation and organization of the competition, review the best submissions,\nand analyze system predictions to inform a discussion of evaluation for\nopen-domain QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberti_C/0/1/0/all/0/1\">Chris Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palomaki_J/0/1/0/all/0/1\">Jennimaria Palomaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowski_T/0/1/0/all/0/1\">Tom Kwiatkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuttler_H/0/1/0/all/0/1\">Heinrich K&#xfc;ttler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izacard_G/0/1/0/all/0/1\">Gautier Izacard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_L/0/1/0/all/0/1\">Lucas Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1\">Edouard Grave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimaoka_S/0/1/0/all/0/1\">Sonse Shimaoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masatoshi Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyawaki_S/0/1/0/all/0/1\">Shumpei Miyawaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_S/0/1/0/all/0/1\">Shun Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_R/0/1/0/all/0/1\">Ryo Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Docekal_M/0/1/0/all/0/1\">Martin Docekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondrej_K/0/1/0/all/0/1\">Karel Ondrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Size Does Not Fit All: Finding the Optimal Subword Sizes for FastText Models across Languages. (arXiv:2102.02585v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.02585","description":"<p>Unsupervised representation learning of words from large multilingual corpora\nis useful for downstream tasks such as word sense disambiguation, semantic text\nsimilarity, and information retrieval. The representation precision of\nlog-bilinear fastText models is mostly due to their use of subword information.\nIn previous work, the optimization of fastText's subword sizes has not been\nfully explored, and non-English fastText models were trained using subword\nsizes optimized for English and German word analogy tasks. In our work, we find\nthe optimal subword sizes on the English, German, Czech, Italian, Spanish,\nFrench, Hindi, Turkish, and Russian word analogy tasks. We then propose a\nsimple n-gram coverage model and we show that it predicts better-than-default\nsubword sizes on the Spanish, French, Hindi, Turkish, and Russian word analogy\ntasks. We show that the optimization of fastText's subword sizes matters and\nresults in a 14% improvement on the Czech word analogy task. We also show that\nexpensive parameter optimization can be replaced by a simple n-gram coverage\nmodel that consistently improves the accuracy of fastText models on the word\nanalogy tasks by up to 3% compared to the default subword sizes, and that it is\nwithin 1% accuracy of the optimal subword sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ayetiran_E/0/1/0/all/0/1\">Eniafe Festus Ayetiran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bacovsky_D/0/1/0/all/0/1\">Dalibor Ba&#x10d;ovsk&#xfd;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Luptak_D/0/1/0/all/0/1\">D&#xe1;vid Lupt&#xe1;k</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a> (1) ((1) Faculty of Informatics Masaryk University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Post-Processing Technique for Improving Readability Assessment of Texts using Word Mover's Distance. (arXiv:2103.07277v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07277","description":"<p>Assessing the proper difficulty levels of reading materials or texts in\ngeneral is the first step towards effective comprehension and learning. In this\nstudy, we improve the conventional methodology of automatic readability\nassessment by incorporating the Word Mover's Distance (WMD) of ranked texts as\nan additional post-processing technique to further ground the difficulty level\ngiven by a model. Results of our experiments on three multilingual datasets in\nFilipino, German, and English show that the post-processing technique\noutperforms previous vanilla and ranking-based models using SVM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Ethel Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning Pretrained Transformers into RNNs. (arXiv:2103.13076v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.13076","description":"<p>Transformers have outperformed recurrent neural networks (RNNs) in natural\nlanguage generation. But this comes with a significant computational cost, as\nthe attention mechanism's complexity scales quadratically with sequence length.\nEfficient transformer variants have received increasing interest in recent\nworks. Among them, a linear-complexity recurrent variant has proven well suited\nfor autoregressive generation. It approximates the softmax attention with\nrandomized or heuristic feature maps, but can be difficult to train and may\nyield suboptimal accuracy. This work aims to convert a pretrained transformer\ninto its efficient recurrent counterpart, improving efficiency while\nmaintaining accuracy. Specifically, we propose a swap-then-finetune procedure:\nin an off-the-shelf pretrained transformer, we replace the softmax attention\nwith its linear-complexity recurrent alternative and then finetune. With a\nlearned feature map, our approach provides an improved tradeoff between\nefficiency and accuracy over the standard transformer and other recurrent\nvariants. We also show that the finetuning process has lower training cost\nrelative to training these recurrent variants from scratch. As many models for\nnatural language tasks are increasingly dependent on large-scale pretrained\ntransformers, this work presents a viable approach to improving inference\nefficiency without repeating the expensive pretraining process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Keyword is an Island: In search of covert associations. (arXiv:2103.17114v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.17114","description":"<p>This paper describes how corpus-assisted discourse analysis based on keyword\n(KW) identification and interpretation can benefit from employing Market basket\nanalysis (MBA) after KW extraction. MBA is a data mining technique used\noriginally in marketing that can reveal consistent associations between items\nin a shopping cart, but also between keywords in a corpus of many texts. By\nidentifying recurring associations between KWs we can compensate for the lack\nof wider context which is a major issue impeding the interpretation of isolated\nKWs (esp. when analyzing large data). To showcase the advantages of MBA in\n\"re-contextualizing\" keywords within the discourse, a pilot study on the topic\nof migration was conducted contrasting anti-system and center-right Czech\ninternet media. was conducted. The results show that MBA is useful in\nidentifying the dominant strategy of anti-system news portals: to weave in a\nconfounding ideological undercurrent and connect the concept of migrants to a\nmultitude of other topics (i.e., flooding the discourse).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cvrcek_V/0/1/0/all/0/1\">V&#xe1;clav Cvr&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_M/0/1/0/all/0/1\">Masako Ueda Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SYSML: StYlometry with Structure and Multitask Learning: Implications for Darknet Forum Migrant Analysis. (arXiv:2104.00764v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.00764","description":"<p>Darknet market forums are frequently used to exchange illegal goods and\nservices between parties who use encryption to conceal their identities. The\nTor network is used to host these markets, which guarantees additional\nanonymization from IP and location tracking, making it challenging to link\nacross malicious users using multiple accounts (sybils). Additionally, users\nmigrate to new forums when one is closed, making it difficult to link users\nacross multiple forums. We develop a novel stylometry-based multitask learning\napproach for natural language and interaction modeling using graph embeddings\nto construct low-dimensional representations of short episodes of user activity\nfor authorship attribution. We provide a comprehensive evaluation of our\nmethods across four different darknet forums demonstrating its efficacy over\nthe state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X\non Recall@10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maneriker_P/0/1/0/all/0/1\">Pranav Maneriker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuntian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGA : An Intent-Guided Authoring Assistant. (arXiv:2104.07000v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07000","description":"<p>While large-scale pretrained language models have significantly improved\nwriting assistance functionalities such as autocomplete, more complex and\ncontrollable writing assistants have yet to be explored. We leverage advances\nin language modeling to build an interactive writing assistant that generates\nand rephrases text according to fine-grained author specifications. Users\nprovide input to our Intent-Guided Assistant (IGA) in the form of text\ninterspersed with tags that correspond to specific rhetorical directives (e.g.,\nadding description or contrast, or rephrasing a particular sentence). We\nfine-tune a language model on a dataset heuristically-labeled with author\nintent, which allows IGA to fill in these tags with generated text that users\ncan subsequently edit to their liking. A series of automatic and crowdsourced\nevaluations confirm the quality of IGA's generated outputs, while a small-scale\nuser study demonstrates author preference for IGA over baseline methods in a\ncreative writing task. We release our dataset, code, and demo to spur further\nresearch into AI-assisted writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenlong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunatha_V/0/1/0/all/0/1\">Varun Manjunatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1\">Vlad Morariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1\">Balaji Vasan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Alignment-Agnostic Model for Chinese Text Error Correction. (arXiv:2104.07190v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07190","description":"<p>This paper investigates how to correct Chinese text errors with types of\nmistaken, missing and redundant characters, which is common for Chinese native\nspeakers. Most existing models based on detect-correct framework can correct\nmistaken characters errors, but they cannot deal with missing or redundant\ncharacters. The reason is that lengths of sentences before and after correction\nare not the same, leading to the inconsistence between model inputs and\noutputs. Although the Seq2Seq-based or sequence tagging methods provide\nsolutions to the problem and achieved relatively good results on English\ncontext, but they do not perform well in Chinese context according to our\nexperimental results. In our work, we propose a novel detect-correct framework\nwhich is alignment-agnostic, meaning that it can handle both text aligned and\nnon-aligned occasions, and it can also serve as a cold start model when there\nare no annotated data provided. Experimental results on three datasets\ndemonstrate that our method is effective and achieves the best performance\namong existing published models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liying Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yue Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weishun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multivalent Entailment Graphs for Question Answering. (arXiv:2104.07846v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07846","description":"<p>Drawing inferences between open-domain natural language predicates is a\nnecessity for true language understanding. There has been much progress in\nunsupervised learning of entailment graphs for this purpose. We make three\ncontributions: (1) we reinterpret the Distributional Inclusion Hypothesis to\nmodel entailment between predicates of different valencies, like DEFEAT(Biden,\nTrump) entails WIN(Biden); (2) we actualize this theory by learning\nunsupervised Multivalent Entailment Graphs of open-domain predicates; and (3)\nwe demonstrate the capabilities of these graphs on a novel question answering\ntask. We show that directional entailment is more helpful for inference than\nbidirectional similarity on questions of fine-grained semantics. We also show\nthat drawing on evidence across valencies answers more questions than by using\nonly the same valency evidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKenna_N/0/1/0/all/0/1\">Nick McKenna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1\">Liane Guillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mohammad Javad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vroe_S/0/1/0/all/0/1\">Sander Bijl de Vroe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Across Time: What Does RoBERTa Know and When?. (arXiv:2104.07885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07885","description":"<p>Models of language trained on very large corpora have been demonstrated\nuseful for NLP. As fixed artifacts, they have become the object of intense\nstudy, with many researchers \"probing\" the extent to which linguistic\nabstractions, factual and commonsense knowledge, and reasoning abilities they\nacquire and readily demonstrate. Building on this line of work, we consider a\nnew question: for types of knowledge a language model learns, when during\n(pre)training are they acquired? We plot probing performance across iterations,\nusing RoBERTa as a case study. Among our findings: linguistic knowledge is\nacquired fast, stably, and robustly across domains. Facts and commonsense are\nslower and more domain-sensitive. Reasoning abilities are, in general, not\nstably acquired. As new datasets, pretraining protocols, and probes emerge, we\nbelieve that probing-across-time analyses can help researchers understand the\ncomplex, intermingled learning that these models undergo and guide us toward\nmore efficient approaches that accomplish necessary learning faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leo Z. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Few-Shot Butlers. (arXiv:2104.07972v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07972","description":"<p>Pretrained language models demonstrate strong performance in most NLP tasks\nwhen fine-tuned on small task-specific datasets. Hence, these autoregressive\nmodels constitute ideal agents to operate in text-based environments where\nlanguage understanding and generative capabilities are essential. Nonetheless,\ncollecting expert demonstrations in such environments is a time-consuming\nendeavour. We introduce a two-stage procedure to learn from a small set of\ndemonstrations and further improve by interacting with an environment. We show\nthat language models fine-tuned with only 1.2% of the expert demonstrations and\na simple reinforcement learning algorithm achieve a 51% absolute improvement in\nsuccess rate over existing methods in the ALFWorld environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Micheli_V/0/1/0/all/0/1\">Vincent Micheli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Fran&#xe7;ois Fleuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Passage Ranking for Diverse Multi-Answer Retrieval. (arXiv:2104.08445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08445","description":"<p>We study multi-answer retrieval, an under-explored problem that requires\nretrieving passages to cover multiple distinct answers for a given question.\nThis task requires joint modeling of retrieved passages, as models should not\nrepeatedly retrieve passages containing the same answer at the cost of missing\na different valid answer. In this paper, we introduce JPR, the first joint\npassage retrieval model for multi-answer retrieval. JPR makes use of an\nautoregressive reranker that selects a sequence of passages, each conditioned\non previously selected passages. JPR is trained to select passages that cover\nnew answers at each timestep and uses a tree-decoding algorithm to enable\nflexibility in the degree of diversity. Compared to prior approaches, JPR\nachieves significantly better answer coverage on three multi-answer datasets.\nWhen combined with downstream question answering, the improved retrieval\nenables larger answer generation models since they need to consider fewer\npassages, establishing a new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Interaction Network for Question Matching. (arXiv:2104.08451v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08451","description":"<p>Impressive milestones have been achieved in text matching by adopting a\ncross-attention mechanism to capture pertinent semantic connections between two\nsentence representations. However, regular cross-attention focuses on\nword-level links between the two input sequences, neglecting the importance of\ncontextual information. We propose a context-aware interaction network (COIN)\nto properly align two sequences and infer their semantic relationship.\nSpecifically, each interaction block includes (1) a context-aware\ncross-attention mechanism to effectively integrate contextual information when\naligning two sequences, and (2) a gate fusion layer to flexibly interpolate\naligned representations. We apply multiple stacked interaction blocks to\nproduce alignments at different levels and gradually refine the attention\nresults. Experiments on two question matching datasets and detailed analyses\ndemonstrate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages with Adversarial Examples. (arXiv:2104.08639v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08639","description":"<p>Capturing word meaning in context and distinguishing between correspondences\nand variations across languages is key to building successful multilingual and\ncross-lingual text representation models. However, existing multilingual\nevaluation datasets that evaluate lexical semantics \"in-context\" have various\nlimitations. In particular, 1) their language coverage is restricted to\nhigh-resource languages and skewed in favor of only a few language families and\nareas, 2) a design that makes the task solvable via superficial cues, which\nresults in artificially inflated (and sometimes super-human) performances of\npretrained encoders, on many target languages, which limits their usefulness\nfor model probing and diagnostics, and 3) little support for cross-lingual\nevaluation. In order to address these gaps, we present AM2iCo (Adversarial and\nMultilingual Meaning in Context), a wide-coverage cross-lingual and\nmultilingual evaluation set; it aims to faithfully assess the ability of\nstate-of-the-art (SotA) representation models to understand the identity of\nword meaning in cross-lingual contexts for 14 language pairs. We conduct a\nseries of experiments in a wide range of setups and demonstrate the challenging\nnature of AM2iCo. The results reveal that current SotA pretrained encoders\nsubstantially lag behind human performance, and the largest gaps are observed\nfor low-resource languages and languages dissimilar to English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo M. Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_D/0/1/0/all/0/1\">Diana McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Answers with Entailment Trees. (arXiv:2104.08661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08661","description":"<p>Our goal, in the context of open-domain textual question-answering (QA), is\nto explain answers by showing the line of reasoning from what is known to the\nanswer, rather than simply showing a fragment of textual evidence (a\n\"rationale'\"). If this could be done, new opportunities for understanding and\ndebugging the system's reasoning become possible. Our approach is to generate\nexplanations in the form of entailment trees, namely a tree of multipremise\nentailment steps from facts that are known, through intermediate conclusions,\nto the hypothesis of interest (namely the question + answer). To train a model\nwith this skill, we created ENTAILMENTBANK, the first dataset to contain\nmultistep entailment trees. Given a hypothesis (question + answer), we define\nthree increasingly difficult explanation tasks: generate a valid entailment\ntree given (a) all relevant sentences (b) all relevant and some irrelevant\nsentences, or (c) a corpus. We show that a strong language model can partially\nsolve these tasks, in particular when the relevant sentences are included in\nthe input (e.g., 35% of trees for (a) are perfect), and with indications of\ngeneralization to other domains. This work is significant as it provides a new\ntype of dataset (multistep entailments) and baselines, offering a new avenue\nfor the community to generate richer, more systematic explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_B/0/1/0/all/0/1\">Bhavana Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhengnan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_H/0/1/0/all/0/1\">Hannah Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pipatanangkura_L/0/1/0/all/0/1\">Leighanna Pipatanangkura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting. (arXiv:2104.09691v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09691","description":"<p>Since the seminal work of Mikolov et al. (2013a) and Bojanowski et al.\n(2017), word representations of shallow log-bilinear language models have found\ntheir way into many NLP applications. Mikolov et al. (2018) introduced a\npositional log-bilinear language model, which has characteristics of an\nattention-based language model and which has reached state-of-the-art\nperformance on the intrinsic word analogy task. However, the positional model\nhas never been evaluated on qualitative criteria or extrinsic tasks and its\nspeed is impractical.\n</p>\n<p>We outline the similarities between the attention mechanism and the\npositional model, and we propose a constrained positional model, which adapts\nthe sparse attention mechanism of Dai et al. (2018). We evaluate the positional\nand constrained positional models on three novel qualitative criteria and on\nthe extrinsic language modeling task of Botha and Blunsom (2014).\n</p>\n<p>We show that the positional and constrained positional models contain\ninterpretable information about word order and outperform the subword model of\nBojanowski et al. (2017) on language modeling. We also show that the\nconstrained positional model outperforms the positional model on language\nmodeling and is twice as fast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayetiran_E/0/1/0/all/0/1\">Eniafe Festus Ayetiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRILL: Dynamic Representations for Imbalanced Lifelong Learning. (arXiv:2105.08445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08445","description":"<p>Continual or lifelong learning has been a long-standing challenge in machine\nlearning to date, especially in natural language processing (NLP). Although\nstate-of-the-art language models such as BERT have ushered in a new era in this\nfield due to their outstanding performance in multitask learning scenarios,\nthey suffer from forgetting when being exposed to a continuous stream of data\nwith shifting data distributions. In this paper, we introduce DRILL, a novel\ncontinual learning architecture for open-domain text classification. DRILL\nleverages a biologically inspired self-organizing neural architecture to\nselectively gate latent language representations from BERT in a\ntask-incremental manner. We demonstrate in our experiments that DRILL\noutperforms current methods in a realistic scenario of imbalanced,\nnon-stationary data without prior knowledge about task boundaries. To the best\nof our knowledge, DRILL is the first of its kind to use a self-organizing\nneural architecture for open-domain lifelong learning in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahrens_K/0/1/0/all/0/1\">Kyra Ahrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abawi_F/0/1/0/all/0/1\">Fares Abawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What if This Modified That? Syntactic Interventions via Counterfactual Embeddings. (arXiv:2105.14002v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14002","description":"<p>Neural language models exhibit impressive performance on a variety of tasks,\nbut their internal reasoning may be difficult to understand. Prior art aims to\nuncover meaningful properties within model representations via probes, but it\nis unclear how faithfully such probes portray information that the models\nactually use. To overcome such limitations, we propose a technique, inspired by\ncausal analysis, for generating counterfactual embeddings within models. In\nexperiments testing our technique, we produce evidence that suggests some\nBERT-based models use a tree-distance-like representation of syntax in\ndownstream prediction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1\">Mycal Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"You made me feel this way\": Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions using Speech Data. (arXiv:2106.01526v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01526","description":"<p>How romantic partners interact with each other during a conflict influences\nhow they feel at the end of the interaction and is predictive of whether the\npartners stay together in the long term. Hence understanding the emotions of\neach partner is important. Yet current approaches that are used include\nself-reports which are burdensome and hence limit the frequency of this data\ncollection. Automatic emotion prediction could address this challenge. Insights\nfrom psychology research indicate that partners' behaviors influence each\nother's emotions in conflict interaction and hence, the behavior of both\npartners could be considered to better predict each partner's emotion. However,\nit is yet to be investigated how doing so compares to only using each partner's\nown behavior in terms of emotion prediction performance. In this work, we used\nBERT to extract linguistic features (i.e., what partners said) and openSMILE to\nextract paralinguistic features (i.e., how they said it) from a data set of 368\nGerman-speaking Swiss couples (N = 736 individuals) who were videotaped during\nan 8-minutes conflict interaction in the laboratory. Based on those features,\nwe trained machine learning models to predict if partners feel positive or\nnegative after the conflict interaction. Our results show that including the\nbehavior of the other partner improves the prediction performance. Furthermore,\nfor men, considering how their female partners spoke is most important and for\nwomen considering what their male partner said is most important in getting\nbetter prediction performance. This work is a step towards automatically\nrecognizing each partners' emotion based on the behavior of both, which would\nenable a better understanding of couples in research, therapy, and the real\nworld.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1\">George Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilpert_P/0/1/0/all/0/1\">Peter Hilpert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenmann_G/0/1/0/all/0/1\">Guy Bodenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neysari_M/0/1/0/all/0/1\">Mona Neysari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowatsch_T/0/1/0/all/0/1\">Tobias Kowatsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions. (arXiv:2106.01536v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01536","description":"<p>Many processes in psychology are complex, such as dyadic interactions between\ntwo interacting partners (e.g. patient-therapist, intimate relationship\npartners). Nevertheless, many basic questions about interactions are difficult\nto investigate because dyadic processes can be within a person and between\npartners, they are based on multimodal aspects of behavior and unfold rapidly.\nCurrent analyses are mainly based on the behavioral coding method, whereby\nhuman coders annotate behavior based on a coding schema. But coding is\nlabor-intensive, expensive, slow, focuses on few modalities. Current approaches\nin psychology use LIWC for analyzing couples' interactions. However, advances\nin natural language processing such as BERT could enable the development of\nsystems to potentially automate behavioral coding, which in turn could\nsubstantially improve psychological research. In this work, we train machine\nlearning models to automatically predict positive and negative communication\nbehavioral codes of 368 German-speaking Swiss couples during an 8-minute\nconflict interaction on a fine-grained scale (10-seconds sequences) using\nlinguistic features and paralinguistic features derived with openSMILE. Our\nresults show that both simpler TF-IDF features as well as more complex BERT\nfeatures performed better than LIWC, and that adding paralinguistic features\ndid not improve the performance. These results suggest it might be time to\nconsider modern alternatives to LIWC, the de facto linguistic features in\npsychology, for prediction tasks in couples research. This work is a further\nstep towards the automated coding of couples' behavior which could enhance\ncouple research and therapy, and be utilized for other dyadic interactions as\nwell.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biggiogera_J/0/1/0/all/0/1\">Jacopo Biggiogera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1\">George Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilpert_P/0/1/0/all/0/1\">Peter Hilpert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vowels_M/0/1/0/all/0/1\">Matthew Vowels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenmann_G/0/1/0/all/0/1\">Guy Bodenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neysari_M/0/1/0/all/0/1\">Mona Neysari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbeck_F/0/1/0/all/0/1\">Fridtjof Nussbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowatsch_T/0/1/0/all/0/1\">Tobias Kowatsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Programming Puzzles. (arXiv:2106.05784v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05784","description":"<p>We introduce a new type of programming challenge called programming puzzles,\nas an objective and comprehensive evaluation of program synthesis, and release\nan open-source dataset of Python Programming Puzzles (P3). Each puzzle is\ndefined by a short Python program $f$, and the goal is to find an input $x$\nwhich makes $f$ output \"True\". The puzzles are objective in that each one is\nspecified entirely by the source code of its verifier $f$, so evaluating $f(x)$\nis all that is needed to test a candidate solution $x$. They do not require an\nanswer key or input/output examples, nor do they depend on natural language\nunderstanding. The dataset is comprehensive in that it spans problems of a\nrange of difficulties and domains, ranging from trivial string manipulation\nproblems that are immediately obvious to human programmers (but not necessarily\nto AI), to classic programming puzzles (e.g., Towers of Hanoi), to\ninterview/competitive-programming problems (e.g., dynamic programming), to\nlongstanding open problems in algorithms and mathematics (e.g., factoring). The\nobjective nature of P3 readily supports self-supervised bootstrapping. We\ndevelop baseline enumerative program synthesis and GPT-3 solvers that are\ncapable of solving easy puzzles -- even without access to any reference\nsolutions -- by learning from their own past solutions. Based on a small user\nstudy, we find puzzle difficulty to correlate between human programmers and the\nbaseline AI solvers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1\">Oleksandr Polozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1\">Adam Tauman Kalai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised and Unsupervised Sense Annotation via Translations. (arXiv:2106.06462v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06462","description":"<p>Acquisition of multilingual training data continues to be a challenge in word\nsense disambiguation (WSD). To address this problem, unsupervised approaches\nhave been proposed to automatically generate sense annotations for training\nsupervised WSD systems. We present three new methods for creating\nsense-annotated corpora which leverage translations, parallel bitexts, lexical\nresources, as well as contextual and synset embeddings. Our semi-supervised\nmethod applies machine translation to transfer existing sense annotations to\nother languages. Our two unsupervised methods refine sense annotations produced\nby a knowledge-based WSD system via lexical translations in a parallel corpus.\nWe obtain state-of-the-art results on standard WSD benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1\">Bradley Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1\">Grzegorz Kondrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yixing Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallik_A/0/1/0/all/0/1\">Arnob Mallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAS: Self-Augmented Strategy for Language Model Pre-training. (arXiv:2106.07176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07176","description":"<p>The core of a self-supervised learning method for pre-training language\nmodels includes the design of appropriate data augmentation and corresponding\npre-training task(s). Most data augmentations in language model pre-training\nare context-independent. The seminal contextualized augmentation recently\nproposed by the ELECTRA requires a separate generator, which leads to extra\ncomputation cost as well as the challenge in adjusting the capability of its\ngenerator relative to that of the other model component(s). We propose a\nself-augmented strategy (SAS) that uses a single forward pass through the model\nto augment the input data for model training in the next epoch. Essentially our\nstrategy eliminates a separate generator network and uses only one network to\ngenerate the data augmentation and undertake two pre-training tasks (the MLM\ntask and the RTD task) jointly, which naturally avoids the challenge in\nadjusting the generator's capability as well as reduces the computation cost.\nAdditionally, our SAS is a general strategy such that it can seamlessly\nincorporate many new techniques emerging recently or in the future, such as the\ndisentangled attention mechanism recently proposed by the DeBERTa model. Our\nexperiments show that our SAS is able to outperform the ELECTRA and other\nstate-of-the-art models in the GLUE tasks with the same or less computation\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ru He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Liangzhu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilateral Personalized Dialogue Generation with Dynamic Persona-Aware Fusion. (arXiv:2106.07857v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07857","description":"<p>Generating personalized responses is one of the major challenges in natural\nhuman-robot interaction. Current researches in this field mainly focus on\ngenerating responses consistent with the robot's pre-assigned persona, while\nignoring the user's persona. Such responses may be inappropriate or even\noffensive, which may lead to the bad user experience. Therefore, we propose a\nbilateral personalized dialogue generation (BPDG) method with dynamic\npersona-aware fusion via multi-task transfer learning to generate responses\nconsistent with both personas. The proposed method aims to accomplish three\nlearning tasks: 1) an encoder is trained with dialogue utterances added with\ncorresponded personalized attributes and relative position (language model\ntask), 2) a dynamic persona-aware fusion module predicts the persona presence\nto adaptively fuse the contextual and bilateral personas encodings (persona\nprediction task) and 3) a decoder generates natural, fluent and personalized\nresponses (dialogue generation task). To make the generated responses more\npersonalized and bilateral persona-consistent, the Conditional Mutual\nInformation Maximum (CMIM) criterion is adopted to select the final response\nfrom the generated candidates. The experimental results show that the proposed\nmethod outperforms several state-of-the-art methods in terms of both automatic\nand manual evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a> (Member, IEEE), <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a> (Fellow, IEEE)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Modular and Joint Approaches for Speaker-Attributed ASR on Monaural Long-Form Audio. (arXiv:2107.02852v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2107.02852","description":"<p>Speaker-attributed automatic speech recognition (SA-ASR) is a task to\nrecognize \"who spoke what\" from multi-talker recordings. An SA-ASR system\nusually consists of multiple modules such as speech separation, speaker\ndiarization and ASR. On the other hand, considering the joint optimization, an\nend-to-end (E2E) SA-ASR model has recently been proposed with promising results\non simulation data. In this paper, we present our recent study on the\ncomparison of such modular and joint approaches towards SA-ASR on real monaural\nrecordings. We develop state-of-the-art SA-ASR systems for both modular and\njoint approaches by leveraging large-scale training data, including 75 thousand\nhours of ASR training data and the VoxCeleb corpus for speaker representation\nlearning. We also propose a new pipeline that performs the E2E SA-ASR model\nafter speaker clustering. Our evaluation on the AMI meeting corpus reveals that\nafter fine-tuning with a small real data, the joint system performs 8.9--29.9%\nbetter in accuracy compared to the best modular system while the modular system\nperforms better before such fine-tuning. We also conduct various error analyses\nto show the remaining issues for the monaural SA-ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.08661","description":"<p>We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, but unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural News Recommendation with Collaborative News Encoding and Structural User Encoding. (arXiv:2109.00750v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00750","description":"<p>Automatic news recommendation has gained much attention from the academic\ncommunity and industry. Recent studies reveal that the key to this task lies\nwithin the effective representation learning of both news and users. Existing\nworks typically encode news title and content separately while neglecting their\nsemantic interaction, which is inadequate for news text comprehension. Besides,\nprevious models encode user browsing history without leveraging the structural\ncorrelation of user browsed news to reflect user interests explicitly. In this\nwork, we propose a news recommendation framework consisting of collaborative\nnews encoding (CNE) and structural user encoding (SUE) to enhance news and user\nrepresentation learning. CNE equipped with bidirectional LSTMs encodes news\ntitle and content collaboratively with cross-selection and cross-attention\nmodules to learn semantic-interactive news representations. SUE utilizes graph\nconvolutional networks to extract cluster-structural features of user history,\nfollowed by intra-cluster and inter-cluster attention modules to learn\nhierarchical user interest representations. Experiment results on the MIND\ndataset validate the effectiveness of our model to improve the performance of\nnews recommendation. Our code is released at\nhttps://github.com/Veason-silverbullet/NNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers. (arXiv:2109.00799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00799","description":"<p>Developing automatic Math Word Problem (MWP) solvers has been an interest of\nNLP researchers since the 1960s. Over the last few years, there are a growing\nnumber of datasets and deep learning-based methods proposed for effectively\nsolving MWPs. However, most existing methods are benchmarked soly on one or two\ndatasets, varying in different configurations, which leads to a lack of\nunified, standardized, fair, and comprehensive comparison between methods. This\npaper presents MWPToolkit, the first open-source framework for solving MWPs. In\nMWPToolkit, we decompose the procedure of existing MWP solvers into multiple\ncore components and decouple their models into highly reusable modules. We also\nprovide a hyper-parameter search function to boost the performance. In total,\nwe implement and compare 17 MWP solvers on 4 widely-used single equation\ngeneration benchmarks and 2 multiple equations generation benchmarks. These\nfeatures enable our MWPToolkit to be suitable for researchers to reproduce\nadvanced baseline models and develop new MWP solvers quickly. Code and\ndocuments are available at https://github.com/LYH-YF/MWPToolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yihuai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bing Tian Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection. (arXiv:2109.02051v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2109.02051","description":"<p>Many endeavors have sought to develop countermeasure techniques as\nenhancements on Automatic Speaker Verification (ASV) systems, in order to make\nthem more robust against spoof attacks. As evidenced by the latest ASVspoof\n2019 countermeasure challenge, models currently deployed for the task of ASV\nare, at their best, devoid of suitable degrees of generalization to unseen\nattacks. Upon further investigation of the proposed methods, it appears that a\nbroader three-tiered view of the proposed systems. comprised of the classifier,\nfeature extraction phase, and model loss function, may to some extent lessen\nthe problem. Accordingly, the present study proposes the Efficient Attention\nBranch Network (EABN) modular architecture with a combined loss function to\naddress the generalization problem...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_A/0/1/0/all/0/1\">Amir Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homayounpour_M/0/1/0/all/0/1\">Mohammad Mehdi Homayounpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Language Models with Plug-and-Play Large-Scale Commonsense. (arXiv:2109.02572v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02572","description":"<p>We study how to enhance language models (LMs) with textual commonsense\nknowledge. Previous work (e.g., KnowBERT) has focused on the integrating entity\nknowledge from knowledge graphs. In order to introduce the external entity\nembeddings, they learn to jointly represent the original sentences and external\nknowledge by pre-training on a large scale corpus. However, when switching to\ntextual commonsense, unlike the light entity embeddings, the encoding of\ncommonsense descriptions is heavy. Therefore, the pre-training for learning to\njointly represent the target sentence and external commonsense descriptions is\nunaffordable. On the other hand, since pre-trained LMs for representing the\ntarget sentences alone are readily available, is it feasible to introduce\ncommonsense knowledge in downstream tasks by fine-tuning them only? In this\npaper, we propose a plug-and-play method for large-scale commonsense\nintegration without pre-training. Our method is inspired by the observation\nthat in the regular fine-tuning for downstream tasks where no external\nknowledge was introduced, the variation in the parameters of the language model\nwas minor. Our method starts from a pre-trained LM that represents the target\nsentences only (e.g., BERT). We think that the pre-training for joint\nrepresentation learning can be avoided, if the joint representation reduces the\nimpact of parameters on the starting LM. Previous methods such as KnowBERT\nproposed complex modifications to the vanilla LM to introduce external\nknowledge. Our model (Cook-Transformer, COmmOnsense Knowledge-enhanced\nTransformer), on the other hand, hardly changes the vanilla LM except adding a\nknowledge token in each Transformer layer. In a variety of experiments,\nCOOK-Transformer-based BERT/RoBERTa improve their effect without any\npre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanyun Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingran Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Select One Among All? An Extensive Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding. (arXiv:2109.05696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05696","description":"<p>Knowledge Distillation (KD) is a model compression algorithm that helps\ntransfer the knowledge of a large neural network into a smaller one. Even\nthough KD has shown promise on a wide range of Natural Language Processing\n(NLP) applications, little is understood about how one KD algorithm compares to\nanother and whether these approaches can be complimentary to each other. In\nthis work, we evaluate various KD algorithms on in-domain, out-of-domain and\nadversarial testing. We propose a framework to assess the adversarial\nrobustness of multiple KD algorithms. Moreover, we introduce a new KD\nalgorithm, Combined-KD, which takes advantage of two promising approaches\n(better training scheme and more efficient data augmentation). Our extensive\nexperimental results show that Combined-KD achieves state-of-the-art results on\nthe GLUE benchmark, out-of-domain generalization, and adversarial robustness\ncompared to competitive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_A/0/1/0/all/0/1\">Aref Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pranav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text is NOT Enough: Integrating Visual Impressions into Open-domain Dialogue Generation. (arXiv:2109.05778v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05778","description":"<p>Open-domain dialogue generation in natural language processing (NLP) is by\ndefault a pure-language task, which aims to satisfy human need for daily\ncommunication on open-ended topics by producing related and informative\nresponses. In this paper, we point out that hidden images, named as visual\nimpressions (VIs), can be explored from the text-only data to enhance dialogue\nunderstanding and help generate better responses. Besides, the semantic\ndependency between an dialogue post and its response is complicated, e.g., few\nword alignments and some topic transitions. Therefore, the visual impressions\nof them are not shared, and it is more reasonable to integrate the response\nvisual impressions (RVIs) into the decoder, rather than the post visual\nimpressions (PVIs). However, both the response and its RVIs are not given\ndirectly in the test process. To handle the above issues, we propose a\nframework to explicitly construct VIs based on pure-language dialogue datasets\nand utilize them for better dialogue understanding and generation.\nSpecifically, we obtain a group of images (PVIs) for each post based on a\npre-trained word-image mapping model. These PVIs are used in a co-attention\nencoder to get a post representation with both visual and textual information.\nSince the RVIs are not provided directly during testing, we design a cascade\ndecoder that consists of two sub-decoders. The first sub-decoder predicts the\ncontent words in response, and applies the word-image mapping model to get\nthose RVIs. Then, the second sub-decoder generates the response based on the\npost and RVIs. Experimental results on two open-domain dialogue datasets show\nthat our proposed approach achieves superior performance over competitive\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haolan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?. (arXiv:2109.07102v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07102","description":"<p>There have been many efforts to try to understand what gram-matical knowledge\n(e.g., ability to understand the part of speech of a token) is encoded in large\npre-trained language models (LM). This is done through 'Edge Probing' (EP)\ntests: simple ML models that predict the grammatical properties ofa span\n(whether it has a particular part of speech) using only the LM's token\nrepresentations. However, most NLP applications use fine-tuned LMs. Here, we\nask: if a LM is fine-tuned, does the encoding of linguistic information in it\nchange, as measured by EP tests? Conducting experiments on multiple\nquestion-answering (QA) datasets, we answer that question negatively: the EP\ntest results do not change significantly when the fine-tuned QA model performs\nwell or in adversarial situations where the model is forced to learn wrong\ncorrelations. However, a critical analysis of the EP task datasets reveals that\nEP models may rely on spurious correlations to make predictions. This indicates\neven if fine-tuning changes the encoding of such knowledge, the EP tests might\nfail to measure it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Sagnik Ray Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07680","description":"<p>Identification of user's opinions from natural language text has become an\nexciting field of research due to its growing applications in the real world.\nThe research field is known as sentiment analysis and classification, where\naspect category detection (ACD) and aspect category polarity (ACP) are two\nimportant sub-tasks of aspect-based sentiment analysis. The goal in ACD is to\nspecify which aspect of the entity comes up in opinion while ACP aims to\nspecify the polarity of each aspect category from the ACD task. The previous\nworks mostly propose separate solutions for these two sub-tasks. This paper\nfocuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The\nproposed method carries out multi-label classification where four different\ndeep models were employed and comparatively evaluated to examine their\nperformance. A dataset of Persian reviews was collected from CinemaTicket\nwebsite including 2200 samples from 14 categories. The developed models were\nevaluated using the collected dataset in terms of example-based and label-based\nmetrics. The results indicate the high applicability and preference of the CNN\nand GRU models in comparison to LSTM and Bi-LSTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vazan_M/0/1/0/all/0/1\">Milad Vazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razmara_J/0/1/0/all/0/1\">Jafar Razmara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation. (arXiv:2109.07779v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07779","description":"<p>Researches on dialogue empathy aim to endow an agent with the capacity of\naccurate understanding and proper responding for emotions. Existing models for\nempathetic dialogue generation focus on the emotion flow in one direction, that\nis, from the context to response. We argue that conducting an empathetic\nconversation is a bidirectional process, where empathy occurs when the emotions\nof two interlocutors could converge on the same point, i.e., reaching an\nemotion consensus. Besides, we also find that the empathetic dialogue corpus is\nextremely limited, which further restricts the model performance. To address\nthe above issues, we propose a dual-generative model, Dual-Emp, to\nsimultaneously construct the emotion consensus and utilize some external\nunpaired data. Specifically, our model integrates a forward dialogue model, a\nbackward dialogue model, and a discrete latent variable representing the\nemotion consensus into a unified architecture. Then, to alleviate the\nconstraint of paired data, we extract unpaired emotional data from open-domain\nconversations and employ Dual-Emp to produce pseudo paired empathetic samples,\nwhich is more efficient and low-cost than the human annotation. Automatic and\nhuman evaluations demonstrate that our method outperforms competitive baselines\nin producing coherent and empathetic responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Jiao Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.08270","description":"<p>Language models (LMs) are sentence-completion engines trained on massive\ncorpora. LMs have emerged as a significant breakthrough in natural-language\nprocessing, providing capabilities that go far beyond sentence completion\nincluding question answering, summarization, and natural-language inference.\nWhile many of these capabilities have potential application to cognitive\nsystems, exploiting language models as a source of task knowledge, especially\nfor task learning, offers significant, near-term benefits. We introduce\nlanguage models and the various tasks to which they have been applied and then\nreview methods of knowledge extraction from language models. The resulting\nanalysis outlines both the challenges and opportunities for using language\nmodels as a new knowledge source for cognitive systems. It also identifies\npossible ways to improve knowledge extraction from language models using the\ncapabilities provided by cognitive systems. Central to success will be the\nability of a cognitive agent to itself learn an abstract model of the knowledge\nimplicit in the LM as well as methods to extract high-quality knowledge\neffectively and efficiently. To illustrate, we introduce a hypothetical robot\nagent and describe how language models could extend its task knowledge and\nimprove its performance and the kinds of knowledge and methods the agent can\nuse to exploit the knowledge within a language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wray_R/0/1/0/all/0/1\">Robert E. Wray, III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_J/0/1/0/all/0/1\">James R. Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_J/0/1/0/all/0/1\">John E. Laird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers. (arXiv:2109.08406v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08406","description":"<p>Despite the success of fine-tuning pretrained language encoders like BERT for\ndownstream natural language understanding (NLU) tasks, it is still poorly\nunderstood how neural networks change after fine-tuning. In this work, we use\ncentered kernel alignment (CKA), a method for comparing learned\nrepresentations, to measure the similarity of representations in task-tuned\nmodels across layers. In experiments across twelve NLU tasks, we discover a\nconsistent block diagonal structure in the similarity of representations within\nfine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of\nearlier and later layers, but not between them. The similarity of later layer\nrepresentations implies that later layers only marginally contribute to task\nperformance, and we verify in experiments that the top few layers of fine-tuned\nTransformers can be discarded without hurting performance, even with no further\ntuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haokun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Asymmetric 3D Context Fusion for Universal Lesion Detection. (arXiv:2109.08684v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08684","description":"<p>Modeling 3D context is essential for high-performance 3D medical image\nanalysis. Although 2D networks benefit from large-scale 2D supervised\npretraining, it is weak in capturing 3D context. 3D networks are strong in 3D\ncontext yet lack supervised pretraining. As an emerging technique, \\emph{3D\ncontext fusion operator}, which enables conversion from 2D pretrained networks,\nleverages the advantages of both and has achieved great success. Existing 3D\ncontext fusion operators are designed to be spatially symmetric, i.e.,\nperforming identical operations on each 2D slice like convolutions. However,\nthese operators are not truly equivariant to translation, especially when only\na few 3D slices are used as inputs. In this paper, we propose a novel\nasymmetric 3D context fusion operator (A3D), which uses different weights to\nfuse 3D context from different 2D slices. Notably, A3D is NOT\ntranslation-equivariant while it significantly outperforms existing symmetric\ncontext fusion operators without introducing large computational overhead. We\nvalidate the effectiveness of the proposed method by extensive experiments on\nDeepLesion benchmark, a large-scale public dataset for universal lesion\ndetection from computed tomography (CT). The proposed A3D consistently\noutperforms symmetric context fusion operators by considerable margins, and\nestablishes a new \\emph{state of the art} on DeepLesion. To facilitate open\nresearch, our code and model in PyTorch are available at\nhttps://github.com/M3DV/AlignShift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yi He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_K/0/1/0/all/0/1\">Kaiming Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1\">Zudi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised learning methods and applications in medical imaging analysis: A survey. (arXiv:2109.08685v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08685","description":"<p>The availability of high quality annotated medical imaging datasets is a\nmajor problem that collides with machine learning applications in the field of\nmedical imaging analysis and impedes its advancement. Self-supervised learning\nis a recent training paradigm that enables learning robust representations\nwithout the need for human annotation which can be considered as an effective\nsolution for the scarcity in annotated medical data. This article reviews the\nstate-of-the-art research directions in self-supervised learning approaches for\nimage data with concentration on their applications in the field of medical\nimaging analysis. The article covers a set of the most recent self-supervised\nlearning methods from the computer vision field as they are applicable to the\nmedical imaging analysis and categorize them as predictive, generative and\ncontrastive approaches. Moreover, the article covers (40) of the most recent\nresearches in the field of self-supervised learning in medical imaging analysis\naiming at shedding the light on the recent innovation in the field. Ultimately,\nthe article concludes with possible future research directions in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shurrab_S/0/1/0/all/0/1\">Saeed Shurrab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duwiari_R/0/1/0/all/0/1\">Rehab Duwiari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of Brain MRI using an Altruistic Harris Hawks' Optimization algorithm. (arXiv:2109.08688v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08688","description":"<p>Segmentation is an essential requirement in medicine when digital images are\nused in illness diagnosis, especially, in posterior tasks as analysis and\ndisease identification. An efficient segmentation of brain Magnetic Resonance\nImages (MRIs) is of prime concern to radiologists due to their poor\nillumination and other conditions related to de acquisition of the images.\nThresholding is a popular method for segmentation that uses the histogram of an\nimage to label different homogeneous groups of pixels into different classes.\nHowever, the computational cost increases exponentially according to the number\nof thresholds. In this paper, we perform the multi-level thresholding using an\nevolutionary metaheuristic. It is an improved version of the Harris Hawks\nOptimization (HHO) algorithm that combines the chaotic initialization and the\nconcept of altruism. Further, for fitness assignment, we use a hybrid objective\nfunction where along with the cross-entropy minimization, we apply a new\nentropy function, and leverage weights to the two objective functions to form a\nnew hybrid approach. The HHO was originally designed to solve numerical\noptimization problems. Earlier, the statistical results and comparisons have\ndemonstrated that the HHO provides very promising results compared with\nwell-established metaheuristic techniques. In this article, the altruism has\nbeen incorporated into the HHO algorithm to enhance its exploitation\ncapabilities. We evaluate the proposed method over 10 benchmark images from the\nWBA database of the Harvard Medical School and 8 benchmark images from the\nBrainweb dataset using some standard evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bandyopadhyay_R/0/1/0/all/0/1\">Rajarshi Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kundu_R/0/1/0/all/0/1\">Rohit Kundu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliva_D/0/1/0/all/0/1\">Diego Oliva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarkar_R/0/1/0/all/0/1\">Ram Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChipQA: No-Reference Video Quality Prediction via Space-Time Chips. (arXiv:2109.08726v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08726","description":"<p>We propose a new model for no-reference video quality assessment (VQA). Our\napproach uses a new idea of highly-localized space-time (ST) slices called\nSpace-Time Chips (ST Chips). ST Chips are localized cuts of video data along\ndirections that \\textit{implicitly} capture motion. We use\nperceptually-motivated bandpass and normalization models to first process the\nvideo data, and then select oriented ST Chips based on how closely they fit\nparametric models of natural video statistics. We show that the parameters that\ndescribe these statistics can be used to reliably predict the quality of\nvideos, without the need for a reference video. The proposed method implicitly\nmodels ST video naturalness, and deviations from naturalness. We train and test\nour model on several large VQA databases, and show that our model achieves\nstate-of-the-art performance at reduced cost, without requiring motion\ncomputation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ebenezer_J/0/1/0/all/0/1\">Joshua P. Ebenezer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shang_Z/0/1/0/all/0/1\">Zaixi Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1\">Hai Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sethuraman_S/0/1/0/all/0/1\">Sriram Sethuraman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised View-Invariant Human Posture Representation. (arXiv:2109.08730v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08730","description":"<p>Most recent view-invariant action recognition and performance assessment\napproaches rely on a large amount of annotated 3D skeleton data to extract\nview-invariant features. However, acquiring 3D skeleton data can be cumbersome,\nif not impractical, in in-the-wild scenarios. To overcome this problem, we\npresent a novel unsupervised approach that learns to extract view-invariant 3D\nhuman pose representation from a 2D image without using 3D joint data. Our\nmodel is trained by exploiting the intrinsic view-invariant properties of human\npose between simultaneous frames from different viewpoints and their\nequivariant properties between augmented frames from the same viewpoint. We\nevaluate the learned view-invariant pose representations for two downstream\ntasks. We perform comparative experiments that show improvements on the\nstate-of-the-art unsupervised cross-view action classification accuracy on NTU\nRGB+D by a significant margin, on both RGB and depth images. We also show the\nefficiency of transferring the learned representations from NTU RGB+D to obtain\nthe first ever unsupervised cross-view and cross-subject rank correlation\nresults on the multi-view human movement quality dataset, QMAR, and marginally\nimprove on the-state-of-the-art supervised results for this dataset. We also\ncarry out ablation studies to examine the contributions of the different\ncomponents of our proposed network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sardari_F/0/1/0/all/0/1\">Faegheh Sardari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1\">Majid Mirmehdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto White-Balance Correction for Mixed-Illuminant Scenes. (arXiv:2109.08750v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08750","description":"<p>Auto white balance (AWB) is applied by camera hardware at capture time to\nremove the color cast caused by the scene illumination. The vast majority of\nwhite-balance algorithms assume a single light source illuminates the scene;\nhowever, real scenes often have mixed lighting conditions. This paper presents\nan effective AWB method to deal with such mixed-illuminant scenes. A unique\ndeparture from conventional AWB, our method does not require illuminant\nestimation, as is the case in traditional camera AWB modules. Instead, our\nmethod proposes to render the captured scene with a small set of predefined\nwhite-balance settings. Given this set of rendered images, our method learns to\nestimate weighting maps that are used to blend the rendered images to generate\nthe final corrected image. Through extensive experiments, we show this proposed\nmethod produces promising results compared to other alternatives for single-\nand mixed-illuminant scene color correction. Our source code and trained models\nare available at https://github.com/mahmoudnafifi/mixedillWB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WiSoSuper: Benchmarking Super-Resolution Methods on Wind and Solar Data. (arXiv:2109.08770v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08770","description":"<p>The transition to green energy grids depends on detailed wind and solar\nforecasts to optimize the siting and scheduling of renewable energy generation.\nOperational forecasts from numerical weather prediction models, however, only\nhave a spatial resolution of 10 to 20-km, which leads to sub-optimal usage and\ndevelopment of renewable energy farms. Weather scientists have been developing\nsuper-resolution methods to increase the resolution, but often rely on simple\ninterpolation techniques or computationally expensive differential\nequation-based models. Recently, machine learning-based models, specifically\nthe physics-informed resolution-enhancing generative adversarial network\n(PhIREGAN), have outperformed traditional downscaling methods. We provide a\nthorough and extensible benchmark of leading deep learning-based\nsuper-resolution techniques, including the enhanced super-resolution generative\nadversarial network (ESRGAN) and an enhanced deep super-resolution (EDSR)\nnetwork, on wind and solar data. We accompany the benchmark with a novel\npublic, processed, and machine learning-ready dataset for benchmarking\nsuper-resolution methods on wind and solar data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurinchi_Vendhan_R/0/1/0/all/0/1\">Rupa Kurinchi-Vendhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Ritwik Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werner_L/0/1/0/all/0/1\">Lucien Werner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1\">Dava Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_S/0/1/0/all/0/1\">Steven Low</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locally Weighted Mean Phase Angle (LWMPA) Based Tone Mapping Quality Index (TMQI-3). (arXiv:2109.08774v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08774","description":"<p>High Dynamic Range (HDR) images are the ones that contain a greater range of\nluminosity as compared to the standard images. HDR images have a higher detail\nand clarity of structure, objects, and color, which the standard images lack.\nHDR images are useful in capturing scenes that pose high brightness, darker\nareas, and shadows, etc. An HDR image comprises multiple narrow-range-exposure\nimages combined into one high-quality image. As these HDR images cannot be\ndisplayed on standard display devices, the real challenge comes while\nconverting these HDR images to Low dynamic range (LDR) images. The conversion\nof HDR image to LDR image is performed using Tone-mapped operators (TMOs). This\nconversion results in the loss of much valuable information in structure,\ncolor, naturalness, and exposures. The loss of information in the LDR image may\nnot directly be visible to the human eye. To calculate how good an LDR image is\nafter conversion, various metrics have been proposed previously. Some are not\nnoise resilient, some work on separate color channels (Red, Green, and Blue one\nby one), and some lack capacity to identify the structure. To deal with this\nproblem, we propose a metric in this paper called the Tone Mapping Quality\nIndex (TMQI-3), which evaluates the quality of the LDR image based on its\nobjective score. TMQI-3 is noise resilient, takes account of structure and\nnaturalness, and works on all three color channels combined into one luminosity\ncomponent. This eliminates the need to use multiple metrics at the same time.\nWe compute results for several HDR and LDR images from the literature and show\nthat our quality index metric performs better than the baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hassan_I/0/1/0/all/0/1\">Inaam Ul Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haseeb_A/0/1/0/all/0/1\">Abdul Haseeb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sarwan Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small Lesion Segmentation in Brain MRIs with Subpixel Embedding. (arXiv:2109.08791v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08791","description":"<p>We present a method to segment MRI scans of the human brain into ischemic\nstroke lesion and normal tissues. We propose a neural network architecture in\nthe form of a standard encoder-decoder where predictions are guided by a\nspatial expansion embedding network. Our embedding network learns features that\ncan resolve detailed structures in the brain without the need for\nhigh-resolution training images, which are often unavailable and expensive to\nacquire. Alternatively, the encoder-decoder learns global structures by means\nof striding and max pooling. Our embedding network complements the\nencoder-decoder architecture by guiding the decoder with fine-grained details\nlost to spatial downsampling during the encoder stage. Unlike previous works,\nour decoder outputs at 2 times the input resolution, where a single pixel in\nthe input resolution is predicted by four neighboring subpixels in our output.\nTo obtain the output at the original scale, we propose a learnable downsampler\n(as opposed to hand-crafted ones e.g. bilinear) that combines subpixel\npredictions. Our approach improves the baseline architecture by approximately\n11.7% and achieves the state of the art on the ATLAS public benchmark dataset\nwith a smaller memory footprint and faster runtime than the best competing\nmethod. Our source code has been made available at:\nhttps://github.com/alexklwong/subpixel-embedding-segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Allison Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yangchao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cicek_S/0/1/0/all/0/1\">Safa Cicek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiard_A/0/1/0/all/0/1\">Alexandre Tiard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_B/0/1/0/all/0/1\">Byung-Woo Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Report on China-Spain Joint Clinical Testing for Rapid COVID-19 Risk Screening by Eye-region Manifestations. (arXiv:2109.08807v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08807","description":"<p>Background: The worldwide surge in coronavirus cases has led to the COVID-19\ntesting demand surge. Rapid, accurate, and cost-effective COVID-19 screening\ntests working at a population level are in imperative demand globally.\n</p>\n<p>Methods: Based on the eye symptoms of COVID-19, we developed and tested a\nCOVID-19 rapid prescreening model using the eye-region images captured in China\nand Spain with cellphone cameras. The convolutional neural networks\n(CNNs)-based model was trained on these eye images to complete binary\nclassification task of identifying the COVID-19 cases. The performance was\nmeasured using area under receiver-operating-characteristic curve (AUC),\nsensitivity, specificity, accuracy, and F1. The application programming\ninterface was open access.\n</p>\n<p>Findings: The multicenter study included 2436 pictures corresponding to 657\nsubjects (155 COVID-19 infection, 23.6%) in development dataset (train and\nvalidation) and 2138 pictures corresponding to 478 subjects (64 COVID-19\ninfections, 13.4%) in test dataset. The image-level performance of COVID-19\nprescreening model in the China-Spain multicenter study achieved an AUC of\n0.913 (95% CI, 0.898-0.927), with a sensitivity of 0.695 (95% CI, 0.643-0.748),\na specificity of 0.904 (95% CI, 0.891 -0.919), an accuracy of\n0.875(0.861-0.889), and a F1 of 0.611(0.568-0.655).\n</p>\n<p>Interpretation: The CNN-based model for COVID-19 rapid prescreening has\nreliable specificity and sensitivity. This system provides a low-cost, fully\nself-performed, non-invasive, real-time feedback solution for continuous\nsurveillance and large-scale rapid prescreening for COVID-19.\n</p>\n<p>Funding: This project is supported by Aimomics (Shanghai) Intelligent\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fustel_P/0/1/0/all/0/1\">Paula boned Fustel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1\">Lei Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_L/0/1/0/all/0/1\">Lijie Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Haojie Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Q/0/1/0/all/0/1\">Qiang Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rong_S/0/1/0/all/0/1\">Shisong Rong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_H/0/1/0/all/0/1\">Haicheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiao Xie Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pei_Y/0/1/0/all/0/1\">Yantao Pei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiuqi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanhua Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_H/0/1/0/all/0/1\">Hongxia Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_M/0/1/0/all/0/1\">Mengwei Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HYouTube: Video Harmonization Dataset. (arXiv:2109.08809v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08809","description":"<p>Video composition aims to generate a composite video by combining the\nforeground of one video with the background of another video, but the inserted\nforeground may be incompatible with the background in terms of color and\nillumination. Video harmonization aims to adjust the foreground of a composite\nvideo to make it compatible with the background. So far, video harmonization\nhas only received limited attention and there is no public dataset for video\nharmonization. In this work, we construct a new video harmonization dataset\nHYouTube by adjusting the foreground of real videos to create synthetic\ncomposite videos. Considering the domain gap between real composite videos and\nsynthetic composite videos, we additionally create 100 real composite videos\nvia copy-and-paste. Datasets are available at\nhttps://github.com/bcmi/Video-Harmonization-Dataset-HYouTube.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xinyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1\">Wenyan Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Homogeneous and Heterogeneous Relational Graph for Visible-infrared Person Re-identification. (arXiv:2109.08811v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08811","description":"<p>Visible-infrared person re-identification (VI Re-ID) aims to match person\nimages between the visible and infrared modalities. Existing VI Re-ID methods\nmainly focus on extracting homogeneous structural relationships from a single\nimage, while ignoring the heterogeneous correlation between cross-modality\nimages. The homogenous and heterogeneous structured relationships are crucial\nto learning effective identity representation and cross-modality matching. In\nthis paper, we separately model the homogenous structural relationship by a\nmodality-specific graph within individual modality and then mine the\nheterogeneous structural correlation in these two modality-specific graphs.\nFirst, the homogeneous structured graph (HOSG) mines one-vs.-rest relation\nbetween an arbitrary node (local feature) and all the rest nodes within a\nvisible or infrared image to learn effective identity representation. Second,\nto find cross-modality identity-consistent correspondence, the heterogeneous\ngraph alignment module (HGAM) further measures the relational edge strength by\nroute search between two-modality local node features. Third, we propose the\ncross-modality cross-correlation (CMCC) loss to extract the modality invariance\nin heterogeneous global graph representation. CMCC computes the mutual\ninformation between modalities and expels semantic redundancy. Extensive\nexperiments on SYSU-MM01 and RegDB datasets demonstrate that our method\noutperforms state-of-the-arts with a gain of 13.73\\% and 9.45\\% Rank1/mAP. The\ncode is available at\nhttps://github.com/fegnyujian/Homogeneous-and-Heterogeneous-Relational-Graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yujian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yimu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shangdong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Regrasp by Learning to Place. (arXiv:2109.08817v1 [cs.RO])","link":"http://arxiv.org/abs/2109.08817","description":"<p>In this paper, we explore whether a robot can learn to regrasp a diverse set\nof objects to achieve various desired grasp poses. Regrasping is needed\nwhenever a robot's current grasp pose fails to perform desired manipulation\ntasks. Endowing robots with such an ability has applications in many domains\nsuch as manufacturing or domestic services. Yet, it is a challenging task due\nto the large diversity of geometry in everyday objects and the high\ndimensionality of the state and action space. In this paper, we propose a\nsystem for robots to take partial point clouds of an object and the supporting\nenvironment as inputs and output a sequence of pick-and-place operations to\ntransform an initial object grasp pose to the desired object grasp poses. The\nkey technique includes a neural stable placement predictor and a regrasp graph\nbased solution through leveraging and changing the surrounding environment. We\nintroduce a new and challenging synthetic dataset for learning and evaluating\nthe proposed approach. In this dataset, we show that our system is able to\nachieve 73.3% success rate of regrasping diverse objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shuo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Lin Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adaptive Partial Domain Adaptation. (arXiv:2109.08829v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08829","description":"<p>Partial Domain adaptation (PDA) aims to solve a more practical cross-domain\nlearning problem that assumes target label space is a subset of source label\nspace. However, the mismatched label space causes significant negative\ntransfer. A traditional solution is using soft weights to increase weights of\nsource shared domain and reduce those of source outlier domain. But it still\nlearns features of outliers and leads to negative immigration. The other\nmainstream idea is to distinguish source domain into shared and outlier parts\nby hard binary weights, while it is unavailable to correct the tangled shared\nand outlier classes. In this paper, we propose an end-to-end Self-Adaptive\nPartial Domain Adaptation(SAPDA) Network. Class weights evaluation mechanism is\nintroduced to dynamically self-rectify the weights of shared, outlier and\nconfused classes, thus the higher confidence samples have the more sufficient\nweights. Meanwhile it can eliminate the negative transfer caused by the\nmismatching of label space greatly. Moreover, our strategy can efficiently\nmeasure the transferability of samples in a broader sense, so that our method\ncan achieve competitive results on unsupervised DA task likewise. A large\nnumber of experiments on multiple benchmarks have demonstrated the\neffectiveness of our SAPDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_H/0/1/0/all/0/1\">Hongya Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Haowen Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1\">Zhongliang Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_H/0/1/0/all/0/1\">Henry Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_R/0/1/0/all/0/1\">Ruping Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechNAS: Towards Better Trade-off between Latency and Accuracy for Large-Scale Speaker Verification. (arXiv:2109.08839v1 [cs.SD])","link":"http://arxiv.org/abs/2109.08839","description":"<p>Recently, x-vector has been a successful and popular approach for speaker\nverification, which employs a time delay neural network (TDNN) and statistics\npooling to extract speaker characterizing embedding from variable-length\nutterances. Improvement upon the x-vector has been an active research area, and\nenormous neural networks have been elaborately designed based on the x-vector,\neg, extended TDNN (E-TDNN), factorized TDNN (F-TDNN), and densely connected\nTDNN (D-TDNN). In this work, we try to identify the optimal architectures from\na TDNN based search space employing neural architecture search (NAS), named\nSpeechNAS. Leveraging the recent advances in the speaker recognition, such as\nhigh-order statistics pooling, multi-branch mechanism, D-TDNN and angular\nadditive margin softmax (AAM) loss with a minimum hyper-spherical energy (MHE),\nSpeechNAS automatically discovers five network architectures, from SpeechNAS-1\nto SpeechNAS-5, of various numbers of parameters and GFLOPs on the large-scale\ntext-independent speaker recognition dataset VoxCeleb1. Our derived best neural\nnetwork achieves an equal error rate (EER) of 1.02% on the standard test set of\nVoxCeleb1, which surpasses previous TDNN based state-of-the-art approaches by a\nlarge margin. Code and trained weights are in\nhttps://github.com/wentaozhu/speechnas.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tianlong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jixiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Feng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory Regulation and Alignment toward Generalizer RGB-Infrared Person. (arXiv:2109.08843v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08843","description":"<p>The domain shift, coming from unneglectable modality gap and non-overlapped\nidentity classes between training and test sets, is a major issue of\nRGB-Infrared person re-identification. A key to tackle the inherent issue --\ndomain shift -- is to enforce the data distributions of the two domains to be\nsimilar. However, RGB-IR ReID always demands discriminative features, leading\nto over-rely feature sensitivity of seen classes, \\textit{e.g.}, via\nattention-based feature alignment or metric learning. Therefore, predicting the\nunseen query category from predefined training classes may not be accurate and\nleads to a sub-optimal adversarial gradient. In this paper, we uncover it in a\nmore explainable way and propose a novel multi-granularity memory regulation\nand alignment module (MG-MRA) to solve this issue. By explicitly incorporating\na latent variable attribute, from fine-grained to coarse semantic granularity,\ninto intermediate features, our method could alleviate the over-confidence of\nthe model about discriminative features of seen classes. Moreover, instead of\nmatching discriminative features by traversing nearest neighbor, sparse\nattributes, \\textit{i.e.}, global structural pattern, are recollected with\nrespect to features and assigned to measure pair-wise image similarity in\nhashing. Extensive experiments on RegDB \\cite{RegDB} and SYSU-MM01 \\cite{SYSU}\nshow the superiority of the proposed method that outperforms existing\nstate-of-the-art methods. Our code is available in\nhttps://github.com/Chenfeng1271/MGMRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhiguo Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards High-Quality Temporal Action Detection with Sparse Proposals. (arXiv:2109.08847v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08847","description":"<p>Temporal Action Detection (TAD) is an essential and challenging topic in\nvideo understanding, aiming to localize the temporal segments containing human\naction instances and predict the action categories. The previous works greatly\nrely upon dense candidates either by designing varying anchors or enumerating\nall the combinations of boundaries on video sequences; therefore, they are\nrelated to complicated pipelines and sensitive hand-crafted designs. Recently,\nwith the resurgence of Transformer, query-based methods have tended to become\nthe rising solutions for their simplicity and flexibility. However, there still\nexists a performance gap between query-based methods and well-established\nmethods. In this paper, we identify the main challenge lies in the large\nvariants of action duration and the ambiguous boundaries for short action\ninstances; nevertheless, quadratic-computational global attention prevents\nquery-based methods to build multi-scale feature maps. Towards high-quality\ntemporal action detection, we introduce Sparse Proposals to interact with the\nhierarchical features. In our method, named SP-TAD, each proposal attends to a\nlocal segment feature in the temporal feature pyramid. The local interaction\nenables utilization of high-resolution features to preserve action instances\ndetails. Extensive experiments demonstrate the effectiveness of our method,\nespecially under high tIoU thresholds. E.g., we achieve the state-of-the-art\nperformance on THUMOS14 (45.7% on mAP@0.6, 33.4% on mAP@0.7 and 53.5% on\nmAP@Avg) and competitive results on ActivityNet-1.3 (32.99% on mAP@Avg). Code\nwill be made available at https://github.com/wjn922/SP-TAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiannan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shoufa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zihao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Composition and Attention for Unseen-Domain Generalizable Medical Image Segmentation. (arXiv:2109.08852v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08852","description":"<p>Domain generalizable model is attracting increasing attention in medical\nimage analysis since data is commonly acquired from different institutes with\nvarious imaging protocols and scanners. To tackle this challenging domain\ngeneralization problem, we propose a Domain Composition and Attention-based\nnetwork (DCA-Net) to improve the ability of domain representation and\ngeneralization. First, we present a domain composition method that represents\none certain domain by a linear combination of a set of basis representations\n(i.e., a representation bank). Second, a novel plug-and-play parallel domain\npreceptor is proposed to learn these basis representations and we introduce a\ndivergence constraint function to encourage the basis representations to be as\ndivergent as possible. Then, a domain attention module is proposed to learn the\nlinear combination coefficients of the basis representations. The result of\nlinear combination is used to calibrate the feature maps of an input image,\nwhich enables the model to generalize to different and even unseen domains. We\nvalidate our method on public prostate MRI dataset acquired from six different\ninstitutions with apparent domain shift. Experimental results show that our\nproposed model can generalize well on different and even unseen domains and it\noutperforms state-of-the-art methods on the multi-domain prostate segmentation\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gu_R/0/1/0/all/0/1\">Ran Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_W/0/1/0/all/0/1\">Wenhui Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on deep learning approaches for breast cancer diagnosis. (arXiv:2109.08853v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08853","description":"<p>Deep learning has introduced several learning-based methods to recognize\nbreast tumours and presents high applicability in breast cancer diagnostics. It\nhas presented itself as a practical installment in Computer-Aided Diagnostic\n(CAD) systems to further assist radiologists in diagnostics for different\nmodalities. A deep learning network trained on images provided by hospitals or\npublic databases can perform classification, detection, and segmentation of\nlesion types. Significant progress has been made in recognizing tumours on 2D\nimages but recognizing 3D images remains a frontier so far. The interconnection\nof deep learning networks between different fields of study help propels\ndiscoveries for more efficient, accurate, and robust networks. In this review\npaper, the following topics will be explored: (i) theory and application of\ndeep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches in breast\ntumour recognition from a performance metric perspective, and (iii) challenges\nfaced in CNN approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kwong_T/0/1/0/all/0/1\">Timothy Kwong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazaheri_S/0/1/0/all/0/1\">Samaneh Mazaheri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modern Evolution Strategies for Creativity: Fitting Concrete Images and Abstract Concepts. (arXiv:2109.08857v1 [cs.NE])","link":"http://arxiv.org/abs/2109.08857","description":"<p>Evolutionary algorithms have been used in the digital art scene since the\n1970s. A popular application of genetic algorithms is to optimize the\nprocedural placement of vector graphic primitives to resemble a given painting.\nIn recent years, deep learning-based approaches have also been proposed to\ngenerate procedural drawings, which can be optimized using gradient descent. In\nthis work, we revisit the use of evolutionary algorithms for computational\ncreativity. We find that modern evolution strategies (ES) algorithms, when\ntasked with the placement of shapes, offer large improvements in both quality\nand efficiency compared to traditional genetic algorithms, and even comparable\nto gradient-based methods. We demonstrate that ES is also well suited at\noptimizing the placement of shapes to fit the CLIP model, and can produce\ndiverse, distinct geometric abstractions that are aligned with human\ninterpretation of language. Videos and demo: https://es-clip.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingtao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_D/0/1/0/all/0/1\">David Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V-SlowFast Network for Efficient Visual Sound Separation. (arXiv:2109.08867v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08867","description":"<p>The objective of this paper is to perform visual sound separation: i) we\nstudy visual sound separation on spectrograms of different temporal\nresolutions; ii) we propose a new light yet efficient three-stream framework\nV-SlowFast that operates on Visual frame, Slow spectrogram, and Fast\nspectrogram. The Slow spectrogram captures the coarse temporal resolution while\nthe Fast spectrogram contains the fine-grained temporal resolution; iii) we\nintroduce two contrastive objectives to encourage the network to learn\ndiscriminative visual features for separating sounds; iv) we propose an\naudio-visual global attention module for audio and visual feature fusion; v)\nthe introduced V-SlowFast model outperforms previous state-of-the-art in\nsingle-frame based visual sound separation on small- and large-scale datasets:\nMUSIC-21, AVE, and VGG-Sound. We also propose a small V-SlowFast architecture\nvariant, which achieves 74.2% reduction in the number of model parameters and\n81.4% reduction in GMACs compared to the previous multi-stage models. Project\npage:\n\\href{https://ly-zhu.github.io/V-SlowFast}{https://ly-zhu.github.io/V-SlowFast}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clean-label Backdoor Attack against Deep Hashing based Retrieval. (arXiv:2109.08868v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08868","description":"<p>Deep hashing has become a popular method in large-scale image retrieval due\nto its computational and storage efficiency. However, recent works raise the\nsecurity concerns of deep hashing. Although existing works focus on the\nvulnerability of deep hashing in terms of adversarial perturbations, we\nidentify a more pressing threat, backdoor attack, when the attacker has access\nto the training data. A backdoored deep hashing model behaves normally on\noriginal query images, while returning the images with the target label when\nthe trigger presents, which makes the attack hard to be detected. In this\npaper, we uncover this security concern by utilizing clean-label data\npoisoning. To the best of our knowledge, this is the first attempt at the\nbackdoor attack against deep hashing models. To craft the poisoned images, we\nfirst generate the targeted adversarial patch as the backdoor trigger.\nFurthermore, we propose the confusing perturbations to disturb the hashing code\nlearning, such that the hashing model can learn more about the trigger. The\nconfusing perturbations are imperceptible and generated by dispersing the\nimages with the target label in the Hamming space. We have conducted extensive\nexperiments to verify the efficacy of our backdoor attack under various\nsettings. For instance, it can achieve 63% targeted mean average precision on\nImageNet under 48 bits code length with only 40 poisoned images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kuofeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiawang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongxian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastHyMix: Fast and Parameter-free Hyperspectral Image Mixed Noise Removal. (arXiv:2109.08879v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08879","description":"<p>Hyperspectral imaging with high spectral resolution plays an important role\nin finding objects, identifying materials, or detecting processes. The decrease\nof the widths of spectral bands leads to a decrease in the signal-to-noise\nratio (SNR) of measurements. The decreased SNR reduces the reliability of\nmeasured features or information extracted from HSIs. Furthermore, the image\ndegradations linked with various mechanisms also result in different types of\nnoise, such as Gaussian noise, impulse noise, deadlines, and stripes. This\npaper introduces a fast and parameter-free hyperspectral image mixed noise\nremoval method (termed FastHyMix), which characterizes the complex distribution\nof mixed noise by using a Gaussian mixture model and exploits two main\ncharacteristics of hyperspectral data, namely low-rankness in the spectral\ndomain and high correlation in the spatial domain. The Gaussian mixture model\nenables us to make a good estimation of Gaussian noise intensity and the\nlocation of sparse noise. The proposed method takes advantage of the\nlow-rankness using subspace representation and the spatial correlation of HSIs\nby adding a powerful deep image prior, which is extracted from a neural\ndenoising network. An exhaustive array of experiments and comparisons with\nstate-of-the-art denoisers were carried out. The experimental results show\nsignificant improvement in both synthetic and real datasets. A MATLAB demo of\nthis work will be available at https://github.com/LinaZhuang for the sake of\nreproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_L/0/1/0/all/0/1\">Lina Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_M/0/1/0/all/0/1\">Michael K. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational Imaging and Artificial Intelligence: The Next Revolution of Mobile Vision. (arXiv:2109.08880v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08880","description":"<p>Signal capture stands in the forefront to perceive and understand the\nenvironment and thus imaging plays the pivotal role in mobile vision. Recent\nexplosive progresses in Artificial Intelligence (AI) have shown great potential\nto develop advanced mobile platforms with new imaging devices. Traditional\nimaging systems based on the \"capturing images first and processing afterwards\"\nmechanism cannot meet this unprecedented demand. Differently, Computational\nImaging (CI) systems are designed to capture high-dimensional data in an\nencoded manner to provide more information for mobile vision systems.Thanks to\nAI, CI can now be used in real systems by integrating deep learning algorithms\ninto the mobile vision platform to achieve the closed loop of intelligent\nacquisition, processing and decision making, thus leading to the next\nrevolution of mobile vision.Starting from the history of mobile vision using\ndigital cameras, this work first introduces the advances of CI in diverse\napplications and then conducts a comprehensive review of current research\ntopics combining CI and AI. Motivated by the fact that most existing studies\nonly loosely connect CI and AI (usually using AI to improve the performance of\nCI and only limited works have deeply connected them), in this work, we propose\na framework to deeply integrate CI and AI by using the example of self-driving\nvehicles with high-speed communication, edge computing and traffic planning.\nFinally, we outlook the future of CI plus AI by investigating new materials,\nbrain science and new computing techniques to shed light on new directions of\nmobile vision systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suo_J/0/1/0/all/0/1\">Jinli Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weihang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1\">Jin Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brady_D/0/1/0/all/0/1\">David J. Brady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S$^3$VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation. (arXiv:2109.08901v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08901","description":"<p>Unsupervised domain adaptation (DA) methods have focused on achieving maximal\nperformance through aligning features from source and target domains without\nusing labeled data in the target domain. Whereas, in the real-world scenario's\nit might be feasible to get labels for a small proportion of target data. In\nthese scenarios, it is important to select maximally-informative samples to\nlabel and find an effective way to combine them with the existing knowledge\nfrom source data. Towards achieving this, we propose S$^3$VAADA which i)\nintroduces a novel submodular criterion to select a maximally informative\nsubset to label and ii) enhances a cluster-based DA procedure through novel\nimprovements to effectively utilize all the available data for improving\ngeneralization on target. Our approach consistently outperforms the competing\nstate-of-the-art approaches on datasets with varying degrees of domain shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rangwani_H/0/1/0/all/0/1\">Harsh Rangwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Arihant Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aithal_S/0/1/0/all/0/1\">Sumukh K Aithal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the rogue wave pattern triggered from Gaussian perturbations by deep learning. (arXiv:2109.08909v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08909","description":"<p>Weak Gaussian perturbations on a plane wave background could trigger lots of\nrogue waves, due to modulational instability. Numerical simulations showed that\nthese rogue waves seemed to have similar unit structure. However, to the best\nof our knowledge, there is no relative result to prove that these rogue waves\nhave the similar patterns for different perturbations, partly due to that it is\nhard to measure the rogue wave pattern automatically. In this work, we address\nthese problems from the perspective of computer vision via using deep neural\nnetworks. We propose a Rogue Wave Detection Network (RWD-Net) model to\nautomatically and accurately detect RWs on the images, which directly indicates\nthey have the similar computer vision patterns. For this purpose, we herein\nmeanwhile have designed the related dataset, termed as Rogue Wave Dataset-$10$K\n(RWD-$10$K), which has $10,191$ RW images with bounding box annotations for\neach RW unit. In our detection experiments, we get $99.29\\%$ average precision\non the test splits of the RWD-$10$K dataset. Finally, we derive our novel\nmetric, the density of RW units (DRW), to characterize the evolution of\nGaussian perturbations and obtain the statistical results on them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Liwen Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">XinHang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Delu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_L/0/1/0/all/0/1\">Liming Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Li-Chen Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Semantic Segmentation via Low-level Edge Information Transfer. (arXiv:2109.08912v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08912","description":"<p>Unsupervised domain adaptation for semantic segmentation aims to make models\ntrained on synthetic data (source domain) adapt to real images (target domain).\nPrevious feature-level adversarial learning methods only consider adapting\nmodels on the high-level semantic features. However, the large domain gap\nbetween source and target domains in the high-level semantic features makes\naccurate adaptation difficult. In this paper, we present the first attempt at\nexplicitly using low-level edge information, which has a small inter-domain\ngap, to guide the transfer of semantic information. To this end, a\nsemantic-edge domain adaptation architecture is proposed, which uses an\nindependent edge stream to process edge information, thereby generating\nhigh-quality semantic boundaries over the target domain. Then, an edge\nconsistency loss is presented to align target semantic predictions with\nproduced semantic boundaries. Moreover, we further propose two entropy\nreweighting methods for semantic adversarial learning and self-supervised\nlearning, respectively, which can further enhance the adaptation performance of\nour architecture. Comprehensive experiments on two UDA benchmark datasets\ndemonstrate the superiority of our architecture compared with state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yonghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge Prior Augmented Networks for Motion Deblurring on Naturally Blurry Images. (arXiv:2109.08915v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08915","description":"<p>Motion deblurring has witnessed rapid development in recent years, and most\nof the recent methods address it by using deep learning techniques, with the\nhelp of different kinds of prior knowledge. Concerning that deblurring is\nessentially expected to improve the image sharpness, edge information can serve\nas an important prior. However, the edge has not yet been seriously taken into\nconsideration in previous methods when designing deep models. To this end, we\npresent a novel framework that incorporates edge prior knowledge into deep\nmodels, termed Edge Prior Augmented Networks (EPAN). EPAN has a content-based\nmain branch and an edge-based auxiliary branch, which are constructed as a\nContent Deblurring Net (CDN) and an Edge Enhancement Net (EEN), respectively.\nEEN is designed to augment CDN in the deblurring process via an attentive\nfusion mechanism, where edge features are mapped as spatial masks to guide\ncontent features in a feature-based hierarchical manner. An edge-guided loss\nfunction is proposed to further regulate the optimization of EPAN by enforcing\nthe focus on edge areas. Besides, we design a dual-camera-based image capturing\nsetting to build a new dataset, Real Object Motion Blur (ROMB), with paired\nsharp and naturally blurry images of fast-moving cars, so as to better train\nmotion deblurring models and benchmark the capability of motion deblurring\nalgorithms in practice. Extensive experiments on the proposed ROMB and other\nexisting datasets demonstrate that EPAN outperforms state-of-the-art approaches\nqualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuedong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohua Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Underwater Image Enhancement Using Convolutional Neural Network. (arXiv:2109.08916v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08916","description":"<p>This work proposes a method for underwater image enhancement using the\nprinciple of histogram equalization. Since underwater images have a global\nstrong dominant colour, their colourfulness and contrast are often degraded.\nBefore applying the histogram equalisation technique on the image, the image is\nconverted from coloured image to a gray scale image for further operations.\nHistogram equalization is a technique for adjusting image intensities to\nenhance contrast. The colours of the image are retained using a convolutional\nneural network model which is trained by the datasets of underwater images to\ngive better results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yadav_A/0/1/0/all/0/1\">Anushka Yadav</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_M/0/1/0/all/0/1\">Mayank Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_G/0/1/0/all/0/1\">Ghanapriya Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Studious Approach to Semi-Supervised Learning. (arXiv:2109.08924v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08924","description":"<p>The problem of learning from few labeled examples while using large amounts\nof unlabeled data has been approached by various semi-supervised methods.\nAlthough these methods can achieve superior performance, the models are often\nnot deployable due to the large number of parameters. This paper is an ablation\nstudy of distillation in a semi-supervised setting, which not just reduces the\nnumber of parameters of the model but can achieve this while improving the\nperformance over the baseline supervised model and making it better at\ngeneralizing. After the supervised pretraining, the network is used as a\nteacher model, and a student network is trained over the soft labels that the\nteacher model generates over the entire unlabeled data. We find that the fewer\nthe labels, the more this approach benefits from a smaller student network.\nThis brings forward the potential of distillation as an effective solution to\nenhance performance in semi-supervised computer vision tasks while maintaining\ndeployability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1\">Sahil Khose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shruti Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manushree_V/0/1/0/all/0/1\">V Manushree</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Hybrid Transformer: Learning Global-local Context for Urban Sence Segmentation. (arXiv:2109.08937v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08937","description":"<p>Semantic segmentation of fine-resolution urban scene images plays a vital\nrole in extensive practical applications, such as land cover mapping, urban\nchange detection, environmental protection and economic assessment. Driven by\nrapid developments in deep learning technologies, convolutional neural networks\n(CNNs) have dominated the semantic segmentation task for many years.\nConvolutional neural networks adopt hierarchical feature representation and\nhave strong local context extraction. However, the local property of the\nconvolution layer limits the network from capturing global information that is\ncrucial for improving fine-resolution image segmentation. Recently, Transformer\ncomprise a hot topic in the computer vision domain. Vision Transformer\ndemonstrates the great capability of global information modelling, boosting\nmany vision tasks, such as image classification, object detection and\nespecially semantic segmentation. In this paper, we propose an efficient hybrid\nTransformer (EHT) for semantic segmentation of urban scene images. EHT takes\nadvantage of CNNs and Transformer, learning global-local context to strengthen\nthe feature representation. Extensive experiments demonstrate that EHT has\nhigher efficiency with competitive accuracy compared with state-of-the-art\nbenchmark methods. Specifically, the proposed EHT achieves a 67.0% mIoU on the\nUAVid test set and outperforms other lightweight models significantly. The code\nwill be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shenghui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Violence Detection in Videos. (arXiv:2109.08941v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08941","description":"<p>In the recent years, there has been a tremendous increase in the amount of\nvideo content uploaded to social networking and video sharing websites like\nFacebook and Youtube. As of result of this, the risk of children getting\nexposed to adult and violent content on the web also increased. To address this\nissue, an approach to automatically detect violent content in videos is\nproposed in this work. Here, a novel attempt is made also to detect the\ncategory of violence present in a video. A system which can automatically\ndetect violence from both Hollywood movies and videos from the web is extremely\nuseful not only in parental control but also for applications related to movie\nratings, video surveillance, genre classification and so on.\n</p>\n<p>Here, both audio and visual features are used to detect violence. MFCC\nfeatures are used as audio cues. Blood, Motion, and SentiBank features are used\nas visual cues. Binary SVM classifiers are trained on each of these features to\ndetect violence. Late fusion using a weighted sum of classification scores is\nperformed to get final classification scores for each of the violence class\ntarget by the system. To determine optimal weights for each of the violence\nclasses an approach based on grid search is employed. Publicly available\ndatasets, mainly Violent Scene Detection (VSD), are used for classifier\ntraining, weight calculation, and testing. The performance of the system is\nevaluated on two classification tasks, Multi-Class classification, and Binary\nClassification. The results obtained for Binary Classification are better than\nthe baseline results from MediaEval-2014.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tirupattur_P/0/1/0/all/0/1\">Praveen Tirupattur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulze_C/0/1/0/all/0/1\">Christian Schulze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iWave3D: End-to-end Brain Image Compression with Trainable 3-D Wavelet Transform. (arXiv:2109.08942v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08942","description":"<p>With the rapid development of whole brain imaging technology, a large number\nof brain images have been produced, which puts forward a great demand for\nefficient brain image compression methods. At present, the most commonly used\ncompression methods are all based on 3-D wavelet transform, such as JP3D.\nHowever, traditional 3-D wavelet transforms are designed manually with certain\nassumptions on the signal, but brain images are not as ideal as assumed. What's\nmore, they are not directly optimized for compression task. In order to solve\nthese problems, we propose a trainable 3-D wavelet transform based on the\nlifting scheme, in which the predict and update steps are replaced by 3-D\nconvolutional neural networks. Then the proposed transform is embedded into an\nend-to-end compression scheme called iWave3D, which is trained with a large\namount of brain images to directly minimize the rate-distortion loss.\nExperimental results demonstrate that our method outperforms JP3D significantly\nby 2.012 dB in terms of average BD-PSNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xue_D/0/1/0/all/0/1\">Dongmei Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1\">Haichuan Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold-preserved GANs. (arXiv:2109.08955v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08955","description":"<p>Generative Adversarial Networks (GANs) have been widely adopted in various\nfields. However, existing GANs generally are not able to preserve the manifold\nof data space, mainly due to the simple representation of discriminator for the\nreal/generated data. To address such open challenges, this paper proposes\nManifold-preserved GANs (MaF-GANs), which generalize Wasserstein GANs into\nhigh-dimensional form. Specifically, to improve the representation of data, the\ndiscriminator in MaF-GANs is designed to map data into a high-dimensional\nmanifold. Furthermore, to stabilize the training of MaF-GANs, an operation with\nprecise and universal solution for any K-Lipschitz continuity, called\nTopological Consistency is proposed. The effectiveness of the proposed method\nis justified by both theoretical analysis and empirical results. When adopting\nDCGAN as the backbone on CelebA (256*256), the proposed method achieved 12.43\nFID, which outperforms the state-of-the-art model like Realness GAN (23.51 FID)\nby a large margin. Code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanbang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xianxu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoqian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDTP: Semantic-aware Decoupled Transformer Pyramid for Dense Image Prediction. (arXiv:2109.08963v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08963","description":"<p>Although transformer has achieved great progress on computer vision tasks,\nthe scale variation in dense image prediction is still the key challenge. Few\neffective multi-scale techniques are applied in transformer and there are two\nmain limitations in the current methods. On one hand, self-attention module in\nvanilla transformer fails to sufficiently exploit the diversity of semantic\ninformation because of its rigid mechanism. On the other hand, it is hard to\nbuild attention and interaction among different levels due to the heavy\ncomputational burden. To alleviate this problem, we first revisit multi-scale\nproblem in dense prediction, verifying the significance of diverse semantic\nrepresentation and multi-scale interaction, and exploring the adaptation of\ntransformer to pyramidal structure. Inspired by these findings, we propose a\nnovel Semantic-aware Decoupled Transformer Pyramid (SDTP) for dense image\nprediction, consisting of Intra-level Semantic Promotion (ISP), Cross-level\nDecoupled Interaction (CDI) and Attention Refinement Function (ARF). ISP\nexplores the semantic diversity in different receptive space. CDI builds the\nglobal attention and interaction among different levels in decoupled space\nwhich also solves the problem of heavy computation. Besides, ARF is further\nadded to refine the attention in transformer. Experimental results demonstrate\nthe validity and generality of the proposed method, which outperforms the\nstate-of-the-art by a significant margin in dense image prediction tasks.\nFurthermore, the proposed components are all plug-and-play, which can be\nembedded in other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yufan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kebin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Atrial Fibrillation: A Medical and Technological Review. (arXiv:2109.08974v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08974","description":"<p>Atrial Fibrillation (AF) is the most common type of arrhythmia (Greek a-,\nloss + rhythmos, rhythm = loss of rhythm) leading to hospitalization in the\nUnited States. Though sometimes AF is asymptomatic, it increases the risk of\nstroke and heart failure in patients, in addition to lowering the\nhealth-related quality of life (HRQOL). AF-related care costs the healthcare\nsystem between $6.0 to $26 billion each year. Early detection of AF and\nclinical attention can help improve symptoms and HRQOL of the patient, as well\nas bring down the cost of care. However, the prevalent paradigm of AF detection\ndepends on electrocardiogram (ECG) recorded at a single point in time and does\nnot shed light on the relation of the symptoms with heart rhythm or AF. In the\nrecent decade, due to the democratization of health monitors and the advent of\nhigh-performing computers, Machine Learning algorithms have been proven\neffective in identifying AF, from the ECG of patients. This paper provides an\noverview of the symptoms of AF, its diagnosis, and future prospects for\nresearch in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Samayan Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahnawaz_S/0/1/0/all/0/1\">Sk Shahnawaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirLoop: Lifelong Loop Closure Detection. (arXiv:2109.08975v1 [cs.RO])","link":"http://arxiv.org/abs/2109.08975","description":"<p>Loop closure detection is an important building block that ensures the\naccuracy and robustness of simultaneous localization and mapping (SLAM)\nsystems. Due to their generalization ability, CNN-based approaches have\nreceived increasing attention. Although they normally benefit from training on\ndatasets that are diverse and reflective of the environments, new environments\noften emerge after the model is deployed. It is therefore desirable to\nincorporate the data newly collected during operation for incremental learning.\nNevertheless, simply finetuning the model on new data is infeasible since it\nmay cause the model's performance on previously learned data to degrade over\ntime, which is also known as the problem of catastrophic forgetting. In this\npaper, we present AirLoop, a method that leverages techniques from lifelong\nlearning to minimize forgetting when training loop closure detection models\nincrementally. We experimentally demonstrate the effectiveness of AirLoop on\nTartanAir, Nordland, and RobotCar datasets. To the best of our knowledge,\nAirLoop is one of the first works to achieve lifelong learning of deep loop\nclosure detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dasong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Recognition based on Retinal Bifurcations and Modified Correlation Function. (arXiv:2109.08977v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08977","description":"<p>Nowadays high security is an important issue for most of the secure places\nand recent advances increase the needs of high-security systems. Therefore,\nneeds to high security for controlling and permitting the allowable people to\nenter the high secure places, increases and extends the use of conventional\nrecognition methods. Therefore, a novel identification method using retinal\nimages is proposed in this paper. For this purpose, new mathematical functions\nare applied on corners and bifurcations. To evaluate the proposed method we use\n40 retinal images from the DRIVE database, 20 normal retinal image from STARE\ndatabase and 140 normal retinal images from local collected database and the\naccuracy rate is 99.34 percent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dehghani_A/0/1/0/all/0/1\">Amin Dehghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random Multi-Channel Image Synthesis for Multiplexed Immunofluorescence Imaging. (arXiv:2109.09004v1 [eess.IV])","link":"http://arxiv.org/abs/2109.09004","description":"<p>Multiplex immunofluorescence (MxIF) is an emerging imaging technique that\nproduces the high sensitivity and specificity of single-cell mapping. With a\ntenet of 'seeing is believing', MxIF enables iterative staining and imaging\nextensive antibodies, which provides comprehensive biomarkers to segment and\ngroup different cells on a single tissue section. However, considerable\ndepletion of the scarce tissue is inevitable from extensive rounds of staining\nand bleaching ('missing tissue'). Moreover, the immunofluorescence (IF) imaging\ncan globally fail for particular rounds ('missing stain''). In this work, we\nfocus on the 'missing stain' issue. It would be appealing to develop digital\nimage synthesis approaches to restore missing stain images without losing more\ntissue physically. Herein, we aim to develop image synthesis approaches for\neleven MxIF structural molecular markers (i.e., epithelial and stromal) on real\nsamples. We propose a novel multi-channel high-resolution image synthesis\napproach, called pixN2N-HD, to tackle possible missing stain scenarios via a\nhigh-resolution generative adversarial network (GAN). Our contribution is\nthree-fold: (1) a single deep network framework is proposed to tackle missing\nstain in MxIF; (2) the proposed 'N-to-N' strategy reduces theoretical four\nyears of computational time to 20 hours when covering all possible missing\nstains scenarios, with up to five missing stains (e.g., '(N-1)-to-1',\n'(N-2)-to-2'); and (3) this work is the first comprehensive experimental study\nof investigating cross-stain synthesis in MxIF. Our results elucidate a\npromising direction of advancing MxIF imaging with deep image synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bao_S/0/1/0/all/0/1\">Shunxing Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Ho Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiron_S/0/1/0/all/0/1\">Sophie Chiron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_I/0/1/0/all/0/1\">Ilwoo Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coburn_L/0/1/0/all/0/1\">Lori A. Coburn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_K/0/1/0/all/0/1\">Keith T. Wilson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roland_J/0/1/0/all/0/1\">Joseph T. Roland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Unreasonable Effectiveness of the Final Batch Normalization Layer. (arXiv:2109.09016v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09016","description":"<p>Early-stage disease indications are rarely recorded in real-world domains,\nsuch as Agriculture and Healthcare, and yet, their accurate identification is\ncritical in that point of time. In this type of highly imbalanced\nclassification problems, which encompass complex features, deep learning (DL)\nis much needed because of its strong detection capabilities. At the same time,\nDL is observed in practice to favor majority over minority classes and\nconsequently suffer from inaccurate detection of the targeted early-stage\nindications. In this work, we extend the study done by Kocaman et al., 2020,\nshowing that the final BN layer, when placed before the softmax output layer,\nhas a considerable impact in highly imbalanced image classification problems as\nwell as undermines the role of the softmax outputs as an uncertainty measure.\nThis current study addresses additional hypotheses and reports on the following\nfindings: (i) the performance gain after adding the final BN layer in highly\nimbalanced settings could still be achieved after removing this additional BN\nlayer in inference; (ii) there is a certain threshold for the imbalance ratio\nupon which the progress gained by the final BN layer reaches its peak; (iii)\nthe batch size also plays a role and affects the outcome of the final BN\napplication; (iv) the impact of the BN application is also reproducible on\nother datasets and when utilizing much simpler neural architectures; (v) the\nreported BN effect occurs only per a single majority class and multiple\nminority classes i.e., no improvements are evident when there are two majority\nclasses; and finally, (vi) utilizing this BN layer with sigmoid activation has\nalmost no impact when dealing with a strongly imbalanced image classification\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocaman_V/0/1/0/all/0/1\">Veysel Kocaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shir_O/0/1/0/all/0/1\">Ofer M. Shir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baeck_T/0/1/0/all/0/1\">Thomas Baeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Distribution Alignment via Adversarial Learning for Domain Adaptive Object Detection. (arXiv:2109.09033v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09033","description":"<p>Unsupervised domain adaptive object detection aims to adapt a well-trained\ndetector from its original source domain with rich labeled data to a new target\ndomain with unlabeled data. Recently, mainstream approaches perform this task\nthrough adversarial learning, yet still suffer from two limitations. First,\nthey mainly align marginal distribution by unsupervised cross-domain feature\nmatching, and ignore each feature's categorical and positional information that\ncan be exploited for conditional alignment; Second, they treat all classes as\nequally important for transferring cross-domain knowledge and ignore that\ndifferent classes usually have different transferability. In this paper, we\npropose a joint adaptive detection framework (JADF) to address the above\nchallenges. First, an end-to-end joint adversarial adaptation framework for\nobject detection is proposed, which aligns both marginal and conditional\ndistributions between domains without introducing any extra hyperparameter.\nNext, to consider the transferability of each object class, a metric for\nclass-wise transferability assessment is proposed, which is incorporated into\nthe JADF objective for domain adaptation. Further, an extended study from\nunsupervised domain adaptation (UDA) to unsupervised few-shot domain adaptation\n(UFDA) is conducted, where only a few unlabeled training images are available\nin unlabeled target domain. Extensive experiments validate that JADF is\neffective in both the UDA and UFDA settings, achieving significant performance\ngains over existing state-of-the-art cross-domain detection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruoyao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Tracking by Jointly Exploiting Frame and Event Domain. (arXiv:2109.09052v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09052","description":"<p>Inspired by the complementarity between conventional frame-based and\nbio-inspired event-based cameras, we propose a multi-modal based approach to\nfuse visual cues from the frame- and event-domain to enhance the single object\ntracking performance, especially in degraded conditions (e.g., scenes with high\ndynamic range, low light, and fast-motion objects). The proposed approach can\neffectively and adaptively combine meaningful information from both domains.\nOur approach's effectiveness is enforced by a novel designed cross-domain\nattention schemes, which can effectively enhance features based on self- and\ncross-domain attention schemes; The adaptiveness is guarded by a specially\ndesigned weighting scheme, which can adaptively balance the contribution of the\ntwo domains. To exploit event-based visual cues in single-object tracking, we\nconstruct a large-scale frame-event-based dataset, which we subsequently employ\nto train a novel frame-event fusion based model. Extensive experiments show\nthat the proposed approach outperforms state-of-the-art frame-based tracking\nmethods by at least 10.4% and 11.9% in terms of representative success rate and\nprecision rate, respectively. Besides, the effectiveness of each key component\nof our approach is evidenced by our thorough ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingkai Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaopeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology-based n-ball Concept Embeddings Informing Few-shot Image Classification. (arXiv:2109.09063v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09063","description":"<p>We propose a novel framework named ViOCE that integrates ontology-based\nbackground knowledge in the form of $n$-ball concept embeddings into a neural\nnetwork based vision architecture. The approach consists of two components -\nconverting symbolic knowledge of an ontology into continuous space by learning\nn-ball embeddings that capture properties of subsumption and disjointness, and\nguiding the training and inference of a vision model using the learnt\nembeddings. We evaluate ViOCE using the task of few-shot image classification,\nwhere it demonstrates superior performance on two standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayathilaka_M/0/1/0/all/0/1\">Mirantha Jayathilaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tingting Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_U/0/1/0/all/0/1\">Uli Sattler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Efficient Unpaired Real-world Super-Resolution using Image Statistics. (arXiv:2109.09071v1 [eess.IV])","link":"http://arxiv.org/abs/2109.09071","description":"<p>Learning super-resolution (SR) network without the paired low resolution (LR)\nand high resolution (HR) image is difficult because direct supervision through\nthe corresponding HR counterpart is unavailable. Recently, many real-world SR\nresearches take advantage of the unpaired image-to-image translation technique.\nThat is, they used two or more generative adversarial networks (GANs), each of\nwhich translates images from one domain to another domain, \\eg, translates\nimages from the HR domain to the LR domain. However, it is not easy to stably\nlearn such a translation with GANs using unpaired data. In this study, we\npresent a simple and efficient method of training of real-world SR network. To\nstably train the network, we use statistics of an image patch, such as means\nand variances. Our real-world SR framework consists of two GANs, one for\ntranslating HR images to LR images (degradation task) and the other for\ntranslating LR to HR (SR task). We argue that the unpaired image translation\nusing GANs can be learned efficiently with our proposed data sampling strategy,\nnamely, variance matching. We test our method on the NTIRE 2020 real-world SR\ndataset. Our method outperforms the current state-of-the-art method in terms of\nthe SSIM metric as well as produces comparable results on the LPIPS metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yoon_K/0/1/0/all/0/1\">Kwangjin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Urban-scale Point Clouds Segmentation with BEV Projection. (arXiv:2109.09074v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09074","description":"<p>Point clouds analysis has grasped researchers' eyes in recent years, while 3D\nsemantic segmentation remains a problem. Most deep point clouds models directly\nconduct learning on 3D point clouds, which will suffer from the severe sparsity\nand extreme data processing load in urban-scale data. To tackle the challenge,\nwe propose to transfer the 3D point clouds to dense bird's-eye-view projection.\nIn this case, the segmentation task is simplified because of class unbalance\nreduction and the feasibility of leveraging various 2D segmentation methods. We\nfurther design an attention-based fusion network that can conduct multi-modal\nlearning on the projected images. Finally, the 2D out are remapped to generate\n3D semantic segmentation results. To demonstrate the benefits of our method, we\nconduct various experiments on the SensatUrban dataset, in which our model\npresents competitive evaluation results (61.17% mIoU and 91.37%\nOverallAccuracy). We hope our work can inspire further exploration in point\ncloud analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhenhong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhe Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DECORAS: detection and characterization of radio-astronomical sources using deep learning. (arXiv:2109.09077v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2109.09077","description":"<p>We present DECORAS, a deep learning based approach to detect both point and\nextended sources from Very Long Baseline Interferometry (VLBI) observations.\nOur approach is based on an encoder-decoder neural network architecture that\nuses a low number of convolutional layers to provide a scalable solution for\nsource detection. In addition, DECORAS performs source characterization in\nterms of the position, effective radius and peak brightness of the detected\nsources. We have trained and tested the network with images that are based on\nrealistic Very Long Baseline Array (VLBA) observations at 20 cm. Also, these\nimages have not gone through any prior de-convolution step and are directly\nrelated to the visibility data via a Fourier transform. We find that the source\ncatalog generated by DECORAS has a better overall completeness and purity, when\ncompared to a traditional source detection algorithm. DECORAS is complete at\nthe 7.5$\\sigma$ level, and has an almost factor of two improvement in\nreliability at 5.5$\\sigma$. We find that DECORAS can recover the position of\nthe detected sources to within 0.61 $\\pm$ 0.69 mas, and the effective radius\nand peak surface brightness are recovered to within 20 per cent for 98 and 94\nper cent of the sources, respectively. Overall, we find that DECORAS provides a\nreliable source detection and characterization solution for future wide-field\nVLBI surveys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Rezaei_S/0/1/0/all/0/1\">S.Rezaei</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+McKean_J/0/1/0/all/0/1\">J.P.McKean</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Biehl_M/0/1/0/all/0/1\">M.Biehl</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Javadpour_A/0/1/0/all/0/1\">A.Javadpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards robustness under occlusion for face recognition. (arXiv:2109.09083v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09083","description":"<p>In this paper, we evaluate the effects of occlusions in the performance of a\nface recognition pipeline that uses a ResNet backbone. The classifier was\ntrained on a subset of the CelebA-HQ dataset containing 5,478 images from 307\nclasses, to achieve top-1 error rate of 17.91%. We designed 8 different\nocclusion masks which were applied to the input images. This caused a\nsignificant drop in the classifier performance: its error rate for each mask\nbecame at least two times worse than before. In order to increase robustness\nunder occlusions, we followed two approaches. The first is image inpainting\nusing the pre-trained pluralistic image completion network. The second is\nCutmix, a regularization strategy consisting of mixing training images and\ntheir labels using rectangular patches, making the classifier more robust\nagainst input corruptions. Both strategies revealed effective and interesting\nresults were observed. In particular, the Cutmix approach makes the network\nmore robust without requiring additional steps at the application time, though\nits training time is considerably longer. Our datasets containing the different\nocclusion masks as well as their inpainted counterparts are made publicly\navailable to promote research on the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borges_T/0/1/0/all/0/1\">Tomas M. Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_T/0/1/0/all/0/1\">Teofilo E. de Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Queiroz_R/0/1/0/all/0/1\">Ricardo de Queiroz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-resolution Human Pose Estimation. (arXiv:2109.09090v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09090","description":"<p>Human pose estimation has achieved significant progress on images with high\nimaging resolution. However, low-resolution imagery data bring nontrivial\nchallenges which are still under-studied. To fill this gap, we start with\ninvestigating existing methods and reveal that the most dominant heatmap-based\nmethods would suffer more severe model performance degradation from\nlow-resolution, and offset learning is an effective strategy. Established on\nthis observation, in this work we propose a novel Confidence-Aware Learning\n(CAL) method which further addresses two fundamental limitations of existing\noffset learning methods: inconsistent training and testing, decoupled heatmap\nand offset learning. Specifically, CAL selectively weighs the learning of\nheatmap and offset with respect to ground-truth and most confident prediction,\nwhilst capturing the statistical importance of model output in mini-batch\nlearning manner. Extensive experiments conducted on the COCO benchmark show\nthat our method outperforms significantly the state-of-the-art methods for\nlow-resolution human pose estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shuzhi Sam Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HPTQ: Hardware-Friendly Post Training Quantization. (arXiv:2109.09113v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09113","description":"<p>Neural network quantization enables the deployment of models on edge devices.\nAn essential requirement for their hardware efficiency is that the quantizers\nare hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To\nthe best of our knowledge, current post-training quantization methods do not\nsupport all of these constraints simultaneously. In this work, we introduce a\nhardware-friendly post training quantization (HPTQ) framework, which addresses\nthis problem by synergistically combining several known quantization methods.\nWe perform a large-scale study on four tasks: classification, object detection,\nsemantic segmentation and pose estimation over a wide variety of network\narchitectures. Our extensive experiments show that competitive results can be\nobtained under hardware-friendly constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habi_H/0/1/0/all/0/1\">Hai Victor Habi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peretz_R/0/1/0/all/0/1\">Reuven Peretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_E/0/1/0/all/0/1\">Elad Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikstein_L/0/1/0/all/0/1\">Lior Dikstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_O/0/1/0/all/0/1\">Oranit Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diamant_I/0/1/0/all/0/1\">Idit Diamant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jennings_R/0/1/0/all/0/1\">Roy H. Jennings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netzer_A/0/1/0/all/0/1\">Arnon Netzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ComicGAN: Text-to-Comic Generative Adversarial Network. (arXiv:2109.09120v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09120","description":"<p>Drawing and annotating comic illustrations is a complex and difficult\nprocess. No existing machine learning algorithms have been developed to create\ncomic illustrations based on descriptions of illustrations, or the dialogue in\ncomics. Moreover, it is not known if a generative adversarial network (GAN) can\ngenerate original comics that correspond to the dialogue and/or descriptions.\nGANs are successful in producing photo-realistic images, but this technology\ndoes not necessarily translate to generation of flawless comics. What is more,\ncomic evaluation is a prominent challenge as common metrics such as Inception\nScore will not perform comparably, as they are designed to work on photos. In\nthis paper: 1. We implement ComicGAN, a novel text-to-comic pipeline based on a\ntext-to-image GAN that synthesizes comics according to text descriptions. 2. We\ndescribe an in-depth empirical study of the technical difficulties of comic\ngeneration using GAN's. ComicGAN has two novel features: (i) text description\ncreation from labels via permutation and augmentation, and (ii) custom image\nencoding with Convolutional Neural Networks. We extensively evaluate the\nproposed ComicGAN in two scenarios, namely image generation from descriptions,\nand image generation from dialogue. Our results on 1000 Dilbert comic panels\nand 6000 descriptions show synthetic comic panels from text inputs resemble\noriginal Dilbert panels. Novel methods for text description creation and custom\nimage encoding brought improvements to Frechet Inception Distance, detail, and\noverall image quality over baseline algorithms. Generating illustrations from\ndescriptions provided clear comics including characters and colours that were\nspecified in the descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Proven_Bessel_B/0/1/0/all/0/1\">Ben Proven-Bessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zilong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning. (arXiv:2109.09129v1 [eess.IV])","link":"http://arxiv.org/abs/2109.09129","description":"<p>Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that\naffect patients' social abilities. In recent years, deep learning methods have\nbeen employed to detect ASD through functional MRI (fMRI). However, existing\napproaches solely concentrated on the abnormal brain functional connections but\nignored the importance of regional activities. Due to this biased prior\nknowledge, previous diagnosis models suffered from inter-site heterogeneity and\ninter-individual phenotypical differences. To address this issue, we propose a\nnovel feature extraction method for fMRI that can learn a personalized\nlowe-resolution representation of the entire brain networking regarding both\nthe functional connections and regional activities. First, we abstract the\nbrain imaging as a graph structure, where nodes represent brain areas and edges\ndenote functional connections, and downsample it to a sparse network by\nhierarchical graph pooling. Subsequently, by assigning each subject with the\nextracted features and building edges through inter-individual non-imaging\ncharacteristics, we build a population graph. The non-identically distributed\nnode features are further recalibrated to node embeddings learned by graph\nconvolutional networks. By these means, our framework can extract features\ndirectly and efficiently from the entire fMRI and be aware of implicit\ninter-individual differences. We have evaluated our framework on the ABIDE-I\ndataset with 10-fold cross-validation. The present model has achieved a mean\nclassification accuracy of 85.95\\% and a mean AUC of 0.92, which is better than\nthe state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1\">Li Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jundong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_M/0/1/0/all/0/1\">Mingqin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_C/0/1/0/all/0/1\">Chi Wah Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_K/0/1/0/all/0/1\">Kei Hang Katie Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RSI-Net: Two-Stream Deep Neural Network Integrating GCN and Atrous CNN for Semantic Segmentation of High-resolution Remote Sensing Images. (arXiv:2109.09148v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09148","description":"<p>For semantic segmentation of remote sensing images (RSI), trade-off between\nrepresentation power and location accuracy is quite important. How to get the\ntrade-off effectively is an open question, where current approaches of\nutilizing attention schemes or very deep models result in complex models with\nlarge memory consumption. Compared with the popularly-used convolutional neural\nnetwork (CNN) with fixed square kernels, graph convolutional network (GCN) can\nexplicitly utilize correlations between adjacent land covers and conduct\nflexible convolution on arbitrarily irregular image regions. However, the\nproblems of large variations of target scales and blurred boundary cannot be\neasily solved by GCN, while densely connected atrous convolution network\n(DenseAtrousCNet) with multi-scale atrous convolution can expand the receptive\nfields and obtain image global information. Inspired by the advantages of both\nGCN and Atrous CNN, a two-stream deep neural network for semantic segmentation\nof RSI (RSI-Net) is proposed in this paper to obtain improved performance\nthrough modeling and propagating spatial contextual structure effectively and a\nnovel decoding scheme with image-level and graph-level combination. Extensive\nexperiments are implemented on the Vaihingen, Potsdam and Gaofen RSI datasets,\nwhere the comparison results demonstrate the superior performance of RSI-Net in\nterms of overall accuracy, F1 score and kappa coefficient when compared with\nsix state-of-the-art RSI semantic segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xia Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jason Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haitong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kaiyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Haozhou Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chunqi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nizhuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LODE: Deep Local Deblurring and A New Benchmark. (arXiv:2109.09149v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09149","description":"<p>While recent deep deblurring algorithms have achieved remarkable progress,\nmost existing methods focus on the global deblurring problem, where the image\nblur mostly arises from severe camera shake. We argue that the local blur,\nwhich is mostly derived from moving objects with a relatively static\nbackground, is prevalent but remains under-explored. In this paper, we first\nlay the data foundation for local deblurring by constructing, for the first\ntime, a LOcal-DEblur (LODE) dataset consisting of 3,700 real-world captured\nlocally blurred images and their corresponding ground-truth. Then, we propose a\nnovel framework, termed BLur-Aware DEblurring network (BladeNet), which\ncontains three components: the Local Blur Synthesis module generates locally\nblurred training pairs, the Local Blur Perception module automatically captures\nthe locally blurred region and the Blur-guided Spatial Attention module guides\nthe deblurring network with spatial attention. This framework is flexible such\nthat it can be combined with many existing SotA algorithms. We carry out\nextensive experiments on REDS and LODE datasets showing that BladeNet improves\nPSNR by 2.5dB over SotAs for local deblurring while keeping comparable\nperformance for global deblurring. We will publish the dataset and codes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zerun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Liuyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jinzhao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haidong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of the Generalizability of Self-Supervised Representations. (arXiv:2109.09150v1 [cs.LG])","link":"http://arxiv.org/abs/2109.09150","description":"<p>Recent advancements in self-supervised learning (SSL) made it possible to\nlearn generalizable visual representations from unlabeled data. The performance\nof Deep Learning models fine-tuned on pretrained SSL representations is on par\nwith models fine-tuned on the state-of-the-art supervised learning (SL)\nrepresentations. Irrespective of the progress made in SSL, its generalizability\nhas not been studied extensively. In this article, we perform a deeper analysis\nof the generalizability of pretrained SSL and SL representations by conducting\na domain-based study for transfer learning classification tasks. The\nrepresentations are learned from the ImageNet source data, which are then\nfine-tuned using two types of target datasets: similar to the source dataset,\nand significantly different from the source dataset. We study generalizability\nof the SSL and SL-based models via their prediction accuracy as well as\nprediction confidence. In addition to this, we analyze the attribution of the\nfinal convolutional layer of these models to understand how they reason about\nthe semantic identity of the data. We show that the SSL representations are\nmore generalizable as compared to the SL representations. We explain the\ngeneralizability of the SSL representations by investigating its invariance\nproperty, which is shown to be better than that observed in the SL\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tendle_A/0/1/0/all/0/1\">Atharva Tendle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mohammad Rashedul Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation. (arXiv:2109.09163v1 [cs.RO])","link":"http://arxiv.org/abs/2109.09163","description":"<p>Task-relevant grasping is critical for industrial assembly, where downstream\nmanipulation tasks constrain the set of valid grasps. Learning how to perform\nthis task, however, is challenging, since task-relevant grasp labels are hard\nto define and annotate. There is also yet no consensus on proper\nrepresentations for modeling or off-the-shelf tools for performing\ntask-relevant grasps. This work proposes a framework to learn task-relevant\ngrasping for industrial objects without the need of time-consuming real-world\ndata collection or manual annotation. To achieve this, the entire framework is\ntrained solely in simulation, including supervised training with synthetic\nlabel generation and self-supervised, hand-object interaction. In the context\nof this framework, this paper proposes a novel, object-centric canonical\nrepresentation at the category level, which allows establishing dense\ncorrespondence across object instances and transferring task-relevant grasps to\nnovel instances. Extensive experiments on task-relevant grasping of\ndensely-cluttered industrial objects are conducted in both simulation and\nreal-world setups, demonstrating the effectiveness of the proposed framework.\nCode and data will be released upon acceptance at\nhttps://sites.google.com/view/catgrasp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_W/0/1/0/all/0/1\">Wenzhao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaal_S/0/1/0/all/0/1\">Stefan Schaal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Traffic-Net: 3D Traffic Monitoring Using a Single Camera. (arXiv:2109.09165v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09165","description":"<p>Computer Vision has played a major role in Intelligent Transportation Systems\n(ITS) and traffic surveillance. Along with the rapidly growing automated\nvehicles and crowded cities, the automated and advanced traffic management\nsystems (ATMS) using video surveillance infrastructures have been evolved by\nthe implementation of Deep Neural Networks. In this research, we provide a\npractical platform for real-time traffic monitoring, including 3D\nvehicle/pedestrian detection, speed detection, trajectory estimation,\ncongestion detection, as well as monitoring the interaction of vehicles and\npedestrians, all using a single CCTV traffic camera. We adapt a custom YOLOv5\ndeep neural network model for vehicle/pedestrian detection and an enhanced SORT\ntracking algorithm. For the first time, a hybrid satellite-ground based inverse\nperspective mapping (SG-IPM) method for camera auto-calibration is also\ndeveloped which leads to an accurate 3D object detection and visualisation. We\nalso develop a hierarchical traffic modelling solution based on short- and\nlong-term temporal video data stream to understand the traffic flow,\nbottlenecks, and risky spots for vulnerable road users. Several experiments on\nreal-world scenarios and comparisons with state-of-the-art are conducted using\nvarious traffic monitoring datasets, including MIO-TCD, UA-DETRAC and GRAM-RTM\ncollected from highways, intersections, and urban areas under different\nlighting and weather conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1\">Mahdi Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azarmi_M/0/1/0/all/0/1\">Mohsen Azarmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mir_F/0/1/0/all/0/1\">Farzam Mohammad Pour Mir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition. (arXiv:2109.09166v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09166","description":"<p>Dance experts often view dance as a hierarchy of information, spanning\nlow-level (raw images, image sequences), mid-levels (human poses and bodypart\nmovements), and high-level (dance genre). We propose a Hierarchical Dance Video\nRecognition framework (HDVR). HDVR estimates 2D pose sequences, tracks dancers,\nand then simultaneously estimates corresponding 3D poses and 3D-to-2D imaging\nparameters, without requiring ground truth for 3D poses. Unlike most methods\nthat work on a single person, our tracking works on multiple dancers, under\nocclusions. From the estimated 3D pose sequence, HDVR extracts body part\nmovements, and therefrom dance genre. The resulting hierarchical dance\nrepresentation is explainable to experts. To overcome noise and interframe\ncorrespondence ambiguities, we enforce spatial and temporal motion smoothness\nand photometric continuity over time. We use an LSTM network to extract 3D\nmovement subsequences from which we recognize the dance genre. For experiments,\nwe have identified 154 movement types, of 16 body parts, and assembled a new\nUniversity of Illinois Dance (UID) Dataset, containing 1143 video clips of 9\ngenres covering 30 hours, annotated with movement and genre labels. Our\nexperimental results demonstrate that our algorithms outperform the\nstate-of-the-art 3D pose estimation methods, which also enhances our dance\nrecognition performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaodan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_N/0/1/0/all/0/1\">Narendra Ahuja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPoint: A Deep Learning Model for 3D Reconstruction in Point Clouds via mmWave Radar. (arXiv:2109.09188v1 [eess.IV])","link":"http://arxiv.org/abs/2109.09188","description":"<p>Recent research has shown that mmWave radar sensing is effective for object\ndetection in low visibility environments, which makes it an ideal technique in\nautonomous navigation systems such as autonomous vehicles. However, due to the\ncharacteristics of radar signals such as sparsity, low resolution, specularity,\nand high noise, it is still quite challenging to reconstruct 3D object shapes\nvia mmWave radar sensing. Built on our recent proposed 3DRIMR (3D\nReconstruction and Imaging via mmWave Radar), we introduce in this paper\nDeepPoint, a deep learning model that generates 3D objects in point cloud\nformat that significantly outperforms the original 3DRIMR design. The model\nadopts a conditional Generative Adversarial Network (GAN) based deep neural\nnetwork architecture. It takes as input the 2D depth images of an object\ngenerated by 3DRIMR's Stage 1, and outputs smooth and dense 3D point clouds of\nthe object. The model consists of a novel generator network that utilizes a\nsequence of DeepPoint blocks or layers to extract essential features of the\nunion of multiple rough and sparse input point clouds of an object when\nobserved from various viewpoints, given that those input point clouds may\ncontain many incorrect points due to the imperfect generation process of\n3DRIMR's Stage 1. The design of DeepPoint adopts a deep structure to capture\nthe global features of input point clouds, and it relies on an optimally chosen\nnumber of DeepPoint blocks and skip connections to achieve performance\nimprovement over the original 3DRIMR design. Our experiments have demonstrated\nthat this model significantly outperforms the original 3DRIMR and other\nstandard techniques in reconstructing 3D objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yue Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhuoming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Benyuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capsule networks with non-iterative cluster routing. (arXiv:2109.09213v1 [cs.CV])","link":"http://arxiv.org/abs/2109.09213","description":"<p>Capsule networks use routing algorithms to flow information between\nconsecutive layers. In the existing routing procedures, capsules produce\npredictions (termed votes) for capsules of the next layer. In a nutshell, the\nnext-layer capsule's input is a weighted sum over all the votes it receives. In\nthis paper, we propose non-iterative cluster routing for capsule networks. In\nthe proposed cluster routing, capsules produce vote clusters instead of\nindividual votes for next-layer capsules, and each vote cluster sends its\ncentroid to a next-layer capsule. Generally speaking, the next-layer capsule's\ninput is a weighted sum over the centroid of each vote cluster it receives. The\ncentroid that comes from a cluster with a smaller variance is assigned a larger\nweight in the weighted sum process. Compared with the state-of-the-art capsule\nnetworks, the proposed capsule networks achieve the best accuracy on the\nFashion-MNIST and SVHN datasets with fewer parameters, and achieve the best\naccuracy on the smallNORB and CIFAR-10 datasets with a moderate number of\nparameters. The proposed capsule networks also produce capsules with\ndisentangled representation and generalize well to images captured at novel\nviewpoints. The proposed capsule networks also preserve 2D spatial information\nof an input image in the capsule channels: if the capsule channels are rotated,\nthe object reconstructed from these channels will be rotated by the same\ntransformation. Codes are available at\nhttps://github.com/ZHAOZHIHAO/ClusterRouting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhihao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Samuel Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Baby Robot: Improving the Motor Skills of Toddlers. (arXiv:2109.09223v1 [cs.RO])","link":"http://arxiv.org/abs/2109.09223","description":"<p>This article introduces \"Baby Robot\", a robot aiming to improve motor skills\nof babies and toddlers. Authors developed a car-like toy that moves\nautonomously using reinforcement learning and computer vision techniques. The\nrobot behaviour is to escape from a target baby that has been previously\nrecognized, or at least detected, while avoiding obstacles, so that the\nsecurity of the baby is not compromised. A myriad of commercial toys with a\nsimilar mobility improvement purpose are into the market; however, there is no\none that bets for an intelligent autonomous movement, as they perform simple\nyet repetitive trajectories in the best of the cases. Two crawling toys -- one\nin representation of \"Baby Robot\" -- were tested in a real environment with\nrespect to regular toys in order to check how they improved the toddlers\nmobility. These real-life experiments were conducted with our proposed robot in\na kindergarten, where a group of children interacted with the toys. Significant\nimprovement in the motion skills of participants were detected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Canas_E/0/1/0/all/0/1\">Eric Ca&#xf1;as</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1\">Alba M. G. Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrell_A/0/1/0/all/0/1\">Ana&#xed;s Garrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angulo_C/0/1/0/all/0/1\">Cecilio Angulo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Automated Framework for COVID-19 Disease Identification from a Multicenter Dataset of Chest CT Scans. (arXiv:2109.09241v1 [eess.IV])","link":"http://arxiv.org/abs/2109.09241","description":"<p>The objective of this study is to develop a robust deep learning-based\nframework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and\nNormal cases based on chest CT scans acquired in different imaging centers\nusing various protocols, and radiation doses. We showed that while our proposed\nmodel is trained on a relatively small dataset acquired from only one imaging\ncenter using a specific scanning protocol, the model performs well on\nheterogeneous test sets obtained by multiple scanners using different technical\nparameters. We also showed that the model can be updated via an unsupervised\napproach to cope with the data shift between the train and test sets and\nenhance the robustness of the model upon receiving a new external dataset from\na different center. We adopted an ensemble architecture to aggregate the\npredictions from multiple versions of the model. For initial training and\ndevelopment purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76\nNormal cases was used, which contained volumetric CT scans acquired from one\nimaging center using a constant standard radiation dose scanning protocol. To\nevaluate the model, we collected four different test sets retrospectively to\ninvestigate the effects of the shifts in the data characteristics on the\nmodel's performance. Among the test cases, there were CT scans with similar\ncharacteristics as the train set as well as noisy low-dose and ultra-low dose\nCT scans. In addition, some test CT scans were obtained from patients with a\nhistory of cardiovascular diseases or surgeries. The entire test dataset used\nin this study contained 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental\nresults indicate that our proposed framework performs well on all test sets\nachieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity\nof 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI:\n[76.50-99.19]).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Heidarian_S/0/1/0/all/0/1\">Shahin Heidarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afshar_P/0/1/0/all/0/1\">Parnian Afshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Enshaei_N/0/1/0/all/0/1\">Nastaran Enshaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naderkhani_F/0/1/0/all/0/1\">Farnoosh Naderkhani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rafiee_M/0/1/0/all/0/1\">Moezedin Javad Rafiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oikonomou_A/0/1/0/all/0/1\">Anastasia Oikonomou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shafiee_A/0/1/0/all/0/1\">Akbar Shafiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tyrrell_P/0/1/0/all/0/1\">Pascal N. Tyrrell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fard_F/0/1/0/all/0/1\">Faranak Babaki Fard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+plataniotis_K/0/1/0/all/0/1\">Konstantinos N. plataniotis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories. (arXiv:2002.06478v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.06478","description":"<p>We address the problem of discovering 3D parts for objects in unseen\ncategories. Being able to learn the geometry prior of parts and transfer this\nprior to unseen categories pose fundamental challenges on data-driven shape\nsegmentation approaches. Formulated as a contextual bandit problem, we propose\na learning-based agglomerative clustering framework which learns a grouping\npolicy to progressively group small part proposals into bigger ones in a\nbottom-up fashion. At the core of our approach is to restrict the local context\nfor extracting part-level features, which encourages the generalizability to\nunseen categories. On the large-scale fine-grained 3D part dataset, PartNet, we\ndemonstrate that our method can transfer knowledge of parts learned from 3\ntraining categories to 21 unseen testing categories without seeing any\nannotated samples. Quantitative comparisons against four shape segmentation\nbaselines shows that our approach achieve the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tiange Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Siyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Fusion of Deep Multitasking Representations for Robust Visual Tracking. (arXiv:2004.01382v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.01382","description":"<p>Visual object tracking remains an active research field in computer vision\ndue to persisting challenges with various problem-specific factors in\nreal-world scenes. Many existing tracking methods based on discriminative\ncorrelation filters (DCFs) employ feature extraction networks (FENs) to model\nthe target appearance during the learning process. However, using deep feature\nmaps extracted from FENs based on different residual neural networks (ResNets)\nhas not previously been investigated. This paper aims to evaluate the\nperformance of twelve state-of-the-art ResNet-based FENs in a DCF-based\nframework to determine the best for visual tracking purposes. First, it ranks\ntheir best feature maps and explores the generalized adoption of the best\nResNet-based FEN into another DCF-based method. Then, the proposed method\nextracts deep semantic information from a fully convolutional FEN and fuses it\nwith the best ResNet-based feature maps to strengthen the target representation\nin the learning process of continuous convolution filters. Finally, it\nintroduces a new and efficient semantic weighting method (using semantic\nsegmentation feature maps on each video frame) to reduce the drift problem.\nExtensive experimental results on the well-known OTB-2013, OTB-2015, TC-128 and\nVOT-2018 visual tracking datasets demonstrate that the proposed method\neffectively outperforms state-of-the-art methods in terms of precision and\nrobustness of visual tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanei_Yakhdan_H/0/1/0/all/0/1\">Hossein Ghanei-Yakhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1\">Kamal Nasrollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The shape and simplicity biases of adversarially robust ImageNet-trained CNNs. (arXiv:2006.09373v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.09373","description":"<p>Adversarial training has been the topic of dozens of studies and a leading\nmethod for defending against adversarial attacks. Yet, it remains largely\nunknown (a) how adversarially-robust ImageNet classifiers (R classifiers)\ngeneralize to out-of-distribution examples; and (b) how their generalization\ncapability relates to their hidden representations. In this paper, we perform a\nthorough, systematic study to answer these two questions across AlexNet,\nGoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet\nclassifiers have a strong texture bias, their R counterparts rely heavily on\nshapes. Remarkably, adversarial training induces three simplicity biases into\nhidden neurons in the process of 'robustifying' the network. That is, each\nconvolutional neuron in R networks often changes to detecting (1) pixel-wise\nsmoother patterns i.e. a mechanism that blocks high-frequency noise from\npassing through the network; (2) more lower-level features i.e. textures and\ncolors (instead of objects); and (3) fewer types of inputs. Our findings reveal\nthe interesting mechanisms that made networks more adversarially robust and\nalso explain some recent findings e.g. why R networks benefit from much larger\ncapacity (Xie and Yuille, 2020) and can act as a strong image prior in image\nsynthesis (Santurkar et al., 2019).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Loss re-scaling VQA: Revisiting the LanguagePrior Problem from a Class-imbalance View. (arXiv:2010.16010v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.16010","description":"<p>Recent studies have pointed out that many well-developed Visual Question\nAnswering (VQA) models are heavily affected by the language prior problem,\nwhich refers to making predictions based on the co-occurrence pattern between\ntextual questions and answers instead of reasoning visual contents. To tackle\nit, most existing methods focus on enhancing visual feature learning to reduce\nthis superficial textual shortcut influence on VQA model decisions. However,\nlimited effort has been devoted to providing an explicit interpretation for its\ninherent cause. It thus lacks a good guidance for the research community to\nmove forward in a purposeful way, resulting in model construction perplexity in\novercoming this non-trivial problem. In this paper, we propose to interpret the\nlanguage prior problem in VQA from a class-imbalance view. Concretely, we\ndesign a novel interpretation scheme whereby the loss of mis-predicted frequent\nand sparse answers of the same question type is distinctly exhibited during the\nlate training phase. It explicitly reveals why the VQA model tends to produce a\nfrequent yet obviously wrong answer, to a given question whose right answer is\nsparse in the training set. Based upon this observation, we further develop a\nnovel loss re-scaling approach to assign different weights to each answer based\non the training data statistics for computing the final loss. We apply our\napproach into three baselines and the experimental results on two VQA-CP\nbenchmark datasets evidently demonstrate its effectiveness. In addition, we\nalso justify the validity of the class imbalance interpretation scheme on other\ncomputer vision tasks, such as face recognition and image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUSE: Textual Attributes Guided Portrait Painting Generation. (arXiv:2011.04761v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.04761","description":"<p>We propose a novel approach, MUSE, to illustrate textual attributes visually\nvia portrait generation. MUSE takes a set of attributes written in text, in\naddition to facial features extracted from a photo of the subject as input. We\npropose 11 attribute types to represent inspirations from a subject's profile,\nemotion, story, and environment. We propose a novel stacked neural network\narchitecture by extending an image-to-image generative model to accept textual\nattributes. Experiments show that our approach significantly outperforms\nseveral state-of-the-art methods without using textual attributes, with\nInception Score score increased by 6% and Fr\\'echet Inception Distance (FID)\nscore decreased by 11%, respectively. We also propose a new attribute\nreconstruction metric to evaluate whether the generated portraits preserve the\nsubject's attributes. Experiments show that our approach can accurately\nillustrate 78% textual attributes, which also help MUSE capture the subject in\na more creative and expressive way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaodan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pengfei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_K/0/1/0/all/0/1\">Kevin Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Honghui Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing and Mitigating JPEG Compression Defects in Deep Learning. (arXiv:2011.08932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08932","description":"<p>With the proliferation of deep learning methods, many computer vision\nproblems which were considered academic are now viable in the consumer setting.\nOne drawback of consumer applications is lossy compression, which is necessary\nfrom an engineering standpoint to efficiently and cheaply store and transmit\nuser images. Despite this, there has been little study of the effect of\ncompression on deep neural networks and benchmark datasets are often losslessly\ncompressed or compressed at high quality. Here we present a unified study of\nthe effects of JPEG compression on a range of common tasks and datasets. We\nshow that there is a significant penalty on common performance metrics for high\ncompression. We test several methods for mitigating this penalty, including a\nnovel method based on artifact correction which requires no labels to train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ehrlich_M/0/1/0/all/0/1\">Max Ehrlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Residual Bi-Fusion Feature Pyramid Network for Accurate Single-Shot Object Detection. (arXiv:2012.01724v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01724","description":"<p>We propose the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN)\nfor fast and accurate single-shot object detection. Feature Pyramid (FP) is\nwidely used in recent visual detection, however the top-down pathway of FP\ncannot preserve accurate localization due to pooling shifting. The advantage of\nFP is weaken as deeper backbones with more layers are used. To address this\nissue, we propose a new parallel FP structure with bi-directional (top-down and\nbottom-up) fusion and associated improvements to retain high-quality features\nfor accurate localization. Our method is particularly suitable for detecting\nsmall objects. We provide the following design improvements: (1) A parallel\nbifusion FP structure with a Bottom-up Fusion Module (BFM) to detect both small\nand large objects at once with high accuracy. (2) A COncatenation and\nRE-organization (CORE) module provides a bottom-up pathway for feature fusion,\nwhich leads to the bi-directional fusion FP that can recover lost information\nfrom lower-layer feature maps. (3) The CORE feature is further purified to\nretain richer contextual information. Such purification is performed with CORE\nin a few iterations in both top-down and bottom-up pathways. (4) The adding of\na residual design to CORE leads to a new Re-CORE module that enables easy\ntraining and integration with a wide range of (deeper or lighter) backbones.\nThe proposed network achieves state-of-the-art performance on UAVDT17 and MS\nCOCO datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping-Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong-Sheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TFPnP: Tuning-free Plug-and-Play Proximal Algorithm with Applications to Inverse Imaging Problems. (arXiv:2012.05703v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05703","description":"<p>Plug-and-Play (PnP) is a non-convex optimization framework that combines\nproximal algorithms, for example, the alternating direction method of\nmultipliers (ADMM), with advanced denoising priors. Over the past few years,\ngreat empirical success has been obtained by PnP algorithms, especially for the\nones that integrate deep learning-based denoisers. However, a key challenge of\nPnP approaches is the need for manual parameter tweaking as it is essential to\nobtain high-quality results across the high discrepancy in imaging conditions\nand varying scene content. In this work, we present a class of tuning-free PnP\nproximal algorithms that can determine parameters such as denoising strength,\ntermination time, and other optimization-specific parameters automatically. A\ncore part of our approach is a policy network for automated parameter search\nwhich can be effectively learned via a mixture of model-free and model-based\ndeep reinforcement learning strategies. We demonstrate, through rigorous\nnumerical and visual experiments, that the learned policy can customize\nparameters to different settings, and is often more efficient and effective\nthan existing handcrafted criteria. Moreover, we discuss several practical\nconsiderations of PnP denoisers, which together with our learned policy yield\nstate-of-the-art results. This advanced performance is prevalent on both linear\nand nonlinear exemplar inverse imaging problems, and in particular shows\npromising results on compressed sensing MRI, sparse-view CT, single-photon\nimaging, and phase retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kaixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRDet: A Multi-Head Network for Accurate Oriented Object Detection in Aerial Images. (arXiv:2012.13135v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13135","description":"<p>Objects in aerial images usually have arbitrary orientations and are densely\nlocated over the ground, making them extremely challenge to be detected. Many\nrecently developed methods attempt to solve these issues by estimating an extra\norientation parameter and placing dense anchors, which will result in high\nmodel complexity and computational costs. In this paper, we propose an\narbitrary-oriented region proposal network (AO-RPN) to generate oriented\nproposals transformed from horizontal anchors. The AO-RPN is very efficient\nwith only a few amounts of parameters increase than the original RPN.\nFurthermore, to obtain accurate bounding boxes, we decouple the detection task\ninto multiple subtasks and propose a multi-head network to accomplish them.\nEach head is specially designed to learn the features optimal for the\ncorresponding task, which allows our network to detect objects accurately. We\nname it MRDet short for Multi-head Rotated object Detector for convenience. We\ntest the proposed MRDet on two challenging benchmarks, i.e., DOTA and HRSC2016,\nand compare it with several state-of-the-art methods. Our method achieves very\npromising results which clearly demonstrate its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ran Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangshuai Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classic versus deep learning approaches to address computer vision challenges. (arXiv:2101.09744v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.09744","description":"<p>Computer vision and image processing address many challenging applications.\nWhile the last decade has seen deep neural network architectures\nrevolutionizing those fields, early methods relied on 'classic', i.e.,\nnon-learned approaches. In this study, we explore the differences between\nclassic and deep learning (DL) algorithms to gain new insight regarding which\nis more suitable for a given application. The focus is on two challenging\nill-posed problems, namely faint edge detection and multispectral image\nregistration, studying recent state-of-the-art DL and classic solutions. While\nthose DL algorithms outperform classic methods in terms of accuracy and\ndevelopment time, they tend to have higher resource requirements and are unable\nto perform outside their training space. Moreover, classic algorithms are more\ntransparent, which facilitates their adoption for real-life applications. As\nboth classes of approaches have unique strengths and limitations, the choice of\na solution is clearly application dependent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ofir_N/0/1/0/all/0/1\">Nati Ofir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nebel_J/0/1/0/all/0/1\">Jean-Christophe Nebel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pose-only Solution to Visual Reconstruction and Navigation. (arXiv:2103.01530v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01530","description":"<p>Visual navigation and three-dimensional (3D) scene reconstruction are\nessential for robotics to interact with the surrounding environment.\nLarge-scale scenes and critical camera motions are great challenges facing the\nresearch community to achieve this goal. We raised a pose-only imaging geometry\nframework and algorithms that can help solve these challenges. The\nrepresentation is a linear function of camera global translations, which allows\nfor efficient and robust camera motion estimation. As a result, the spatial\nfeature coordinates can be analytically reconstructed and do not require\nnonlinear optimization. Experiments demonstrate that the computational\nefficiency of recovering the scene and associated camera poses is significantly\nimproved by 2-4 orders of magnitude. This solution might be promising to unlock\nreal-time 3D visual computing in many forefront applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lilian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenxian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dewen Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unmasking Face Embeddings by Self-restrained Triplet Loss for Accurate Masked Face Recognition. (arXiv:2103.01716v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01716","description":"<p>Using the face as a biometric identity trait is motivated by the contactless\nnature of the capture process and the high accuracy of the recognition\nalgorithms. After the current COVID-19 pandemic, wearing a face mask has been\nimposed in public places to keep the pandemic under control. However, face\nocclusion due to wearing a mask presents an emerging challenge for face\nrecognition systems. In this paper, we present a solution to improve the masked\nface recognition performance. Specifically, we propose the Embedding Unmasking\nModel (EUM) operated on top of existing face recognition models. We also\npropose a novel loss function, the Self-restrained Triplet (SRT), which enabled\nthe EUM to produce embeddings similar to these of unmasked faces of the same\nidentities. The achieved evaluation results on three face recognition models,\ntwo real masked datasets, and two synthetically generated masked face datasets\nproved that our proposed approach significantly improves the performance in\nmost experimental settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Pairwise Neuroimage Analysis using the Soft Jaccard Index and 3D Keypoint Sets. (arXiv:2103.06966v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.06966","description":"<p>We propose a novel pairwise distance measure between image keypoint sets, for\nthe purpose of large-scale medical image indexing. Our measure generalizes the\nJaccard index to account for soft set equivalence (SSE) between keypoint\nelements, via an adaptive kernel framework modeling uncertainty in keypoint\nappearance and geometry. A new kernel is proposed to quantify the variability\nof keypoint geometry in location and scale. Our distance measure may be\nestimated between $O(N^2)$ image pairs in $O(N~\\log~N)$ operations via keypoint\nindexing. Experiments report the first results for the task of predicting\nfamily relationships from medical images, using 1010 T1-weighted MRI brain\nvolumes of 434 families including monozygotic and dizygotic twins, siblings and\nhalf-siblings sharing 100%-25% of their polymorphic genes. Soft set equivalence\nand the keypoint geometry kernel improve upon standard hard set equivalence\n(HSE) and appearance kernels alone in predicting family relationships.\nMonozygotic twin identification is near 100%, and three subjects with uncertain\ngenotyping are automatically paired with their self-reported families, the\nfirst reported practical application of image-based family identification. Our\ndistance measure can also be used to predict group categories, sex is predicted\nwith an AUC=0.97. Software is provided for efficient fine-grained curation of\nlarge, generic image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chauvin_L/0/1/0/all/0/1\">Laurent Chauvin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_K/0/1/0/all/0/1\">Kuldeep Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wells_W/0/1/0/all/0/1\">William Wells III</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toews_M/0/1/0/all/0/1\">Matthew Toews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-DETR: Image-Level Few-Shot Object Detection with Inter-Class Correlation Exploitation. (arXiv:2103.11731v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11731","description":"<p>Few-shot object detection has been extensively investigated by incorporating\nmeta-learning into region-based detection frameworks. Despite its success, the\nsaid paradigm is constrained by several factors, such as (i) low-quality region\nproposals for novel classes and (ii) negligence of the inter-class correlation\namong different classes. Such limitations hinder the generalization of\nbase-class knowledge for the detection of novel-class objects. In this work, we\ndesign Meta-DETR, a novel few-shot detection framework that incorporates\ncorrelational aggregation for meta-learning into DETR detection frameworks.\nMeta-DETR works entirely at image level without any region proposals, which\ncircumvents the constraint of inaccurate proposals in prevalent few-shot\ndetection frameworks. Besides, Meta-DETR can simultaneously attend to multiple\nsupport classes within a single feed-forward. This unique design allows\ncapturing the inter-class correlation among different classes, which\nsignificantly reduces the misclassification of similar classes and enhances\nknowledge generalization to novel classes. Experiments over multiple few-shot\nobject detection benchmarks show that the proposed Meta-DETR outperforms\nstate-of-the-art methods by large margins. The implementation codes will be\nreleased at https://github.com/ZhangGongjie/Meta-DETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gongjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kaiwen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Virtual Examples for Long-tailed Recognition. (arXiv:2103.15042v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15042","description":"<p>We tackle the long-tailed visual recognition problem from the knowledge\ndistillation perspective by proposing a Distill the Virtual Examples (DiVE)\nmethod. Specifically, by treating the predictions of a teacher model as virtual\nexamples, we prove that distilling from these virtual examples is equivalent to\nlabel distribution learning under certain constraints. We show that when the\nvirtual example distribution becomes flatter than the original input\ndistribution, the under-represented tail classes will receive significant\nimprovements, which is crucial in long-tailed recognition. The proposed DiVE\nmethod can explicitly tune the virtual example distribution to become flat.\nExtensive experiments on three benchmark datasets, including the large-scale\niNaturalist ones, justify that the proposed DiVE method can significantly\noutperform state-of-the-art methods. Furthermore, additional analyses and\nexperiments verify the virtual example interpretation, and demonstrate the\neffectiveness of tailored designs in DiVE for long-tailed problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yin-Yin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiu-Shen Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing. (arXiv:2104.01375v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.01375","description":"<p>Although deep neural networks hold the state-of-the-art in several remote\nsensing tasks, their black-box operation hinders the understanding of their\ndecisions, concealing any bias and other shortcomings in datasets and model\nperformance. To this end, we have applied explainable artificial intelligence\n(XAI) methods in remote sensing multi-label classification tasks towards\nproducing human-interpretable explanations and improve transparency. In\nparticular, we utilized and trained deep learning models with state-of-the-art\nperformance in the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods\nwere employed towards understanding and interpreting models' predictions, along\nwith quantitative metrics to assess and compare their performance. Numerous\nexperiments were performed to assess the overall performance of XAI methods for\nstraightforward prediction cases, competing multiple labels, as well as\nmisclassification cases. According to our findings, Occlusion, Grad-CAM and\nLime were the most interpretable and reliable XAI methods. However, none\ndelivers high-resolution outputs, while apart from Grad-CAM, both Lime and\nOcclusion are computationally expensive. We also highlight different aspects of\nXAI performance and elaborate with insights on black-box decisions in order to\nimprove transparency, understand their behavior and reveal, as well, datasets'\nparticularities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kakogeorgiou_I/0/1/0/all/0/1\">Ioannis Kakogeorgiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karantzalos_K/0/1/0/all/0/1\">Konstantinos Karantzalos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action-Conditioned 3D Human Motion Synthesis with Transformer VAE. (arXiv:2104.05670v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05670","description":"<p>We tackle the problem of action-conditioned generation of realistic and\ndiverse human motion sequences. In contrast to methods that complete, or\nextend, motion sequences, this task does not require an initial pose or\nsequence. Here we learn an action-aware latent representation for human motions\nby training a generative variational autoencoder (VAE). By sampling from this\nlatent space and querying a certain duration through a series of positional\nencodings, we synthesize variable-length motion sequences conditioned on a\ncategorical action. Specifically, we design a Transformer-based architecture,\nACTOR, for encoding and decoding a sequence of parametric SMPL human body\nmodels estimated from action recognition datasets. We evaluate our approach on\nthe NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the\nstate of the art. Furthermore, we present two use cases: improving action\nrecognition through adding our synthesized data to training, and motion\ndenoising. Code and models are available on our project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrovich_M/0/1/0/all/0/1\">Mathis Petrovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Higher Order Recurrent Space-Time Transformer. (arXiv:2104.08665v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08665","description":"<p>Endowing visual agents with predictive capability is a key step towards video\nintelligence at scale. The predominant modeling paradigm for this is sequence\nlearning, mostly implemented through LSTMs. Feed-forward Transformer\narchitectures have replaced recurrent model designs in ML applications of\nlanguage processing and also partly in computer vision. In this paper we\ninvestigate on the competitiveness of Transformer-style architectures for video\npredictive tasks. To do so we propose HORST, a novel higher order recurrent\nlayer design whose core element is a spatial-temporal decomposition of\nself-attention for video. HORST achieves state of the art competitive\nperformance on Something-Something-V2 early action recognition and\nEPIC-Kitchens-55 action anticipation, without exploiting a task specific\ndesign. We believe this is promising evidence of causal predictive capability\nthat we attribute to our recurrent higher order design of self-attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tai_T/0/1/0/all/0/1\">Tsung-Ming Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Cheng-Kuang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanz_O/0/1/0/all/0/1\">Oswald Lanz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Carrying out CNN Channel Pruning in a White Box. (arXiv:2104.11883v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11883","description":"<p>Channel Pruning has been long studied to compress CNNs, which significantly\nreduces the overall computation. Prior works implement channel pruning in an\nunexplainable manner, which tends to reduce the final classification errors\nwhile failing to consider the internal influence of each channel. In this\npaper, we conduct channel pruning in a white box. Through deep visualization of\nfeature maps activated by different channels, we observe that different\nchannels have a varying contribution to different categories in image\nclassification. Inspired by this, we choose to preserve channels contributing\nto most categories. Specifically, to model the contribution of each channel to\ndifferentiating categories, we develop a class-wise mask for each channel,\nimplemented in a dynamic training manner w.r.t. the input image's category. On\nthe basis of the learned class-wise mask, we perform a global voting mechanism\nto remove channels with less category discrimination. Lastly, a fine-tuning\nprocess is conducted to recover the performance of the pruned model. To our\nbest knowledge, it is the first time that CNN interpretability theory is\nconsidered to guide channel pruning. Extensive experiments on representative\nimage classification tasks demonstrate the superiority of our White-Box over\nmany state-of-the-arts. For instance, on CIFAR-10, it reduces 65.23% FLOPs with\neven 0.62% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box\nachieves a 45.6% FLOPs reduction with only a small loss of 0.83% in the top-1\naccuracy for ResNet-50.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame. (arXiv:2105.06340v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06340","description":"<p>Facial expression spotting is the preliminary step for micro- and\nmacro-expression analysis. The task of reliably spotting such expressions in\nvideo sequences is currently unsolved. The current best systems depend upon\noptical flow methods to extract regional motion features, before categorisation\nof that motion into a specific class of facial movement. Optical flow is\nsusceptible to drift error, which introduces a serious problem for motions with\nlong-term dependencies, such as high frame-rate macro-expression. We propose a\npurely deep learning solution which, rather than tracking frame differential\nmotion, compares via a convolutional model, each frame with two temporally\nlocal reference frames. Reference frames are sampled according to calculated\nmicro- and macro-expression durations. We show that our solution achieves\nstate-of-the-art performance (F1-score of 0.105) in a dataset of high\nframe-rate (200 fps) long video sequences (SAMM-LV) and is competitive in a low\nframe-rate (30 fps) dataset (CAS(ME)2). In this paper, we document our deep\nlearning model and parameters, including how we use local contrast\nnormalisation, which we show is critical for optimal results. We surpass a\nlimitation in existing methods, and advance the state of deep learning in the\ndomain of facial expression spotting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yap_C/0/1/0/all/0/1\">Chuin Hong Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Adrian K. Davison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_R/0/1/0/all/0/1\">Ryan Cunningham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing bikeability with street view imagery and computer vision. (arXiv:2105.08499v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08499","description":"<p>Studies evaluating bikeability usually compute spatial indicators shaping\ncycling conditions and conflate them in a quantitative index. Much research\ninvolves site visits or conventional geospatial approaches, and few studies\nhave leveraged street view imagery (SVI) for conducting virtual audits. These\nhave assessed a limited range of aspects, and not all have been automated using\ncomputer vision (CV). Furthermore, studies have not yet zeroed in on gauging\nthe usability of these technologies thoroughly. We investigate, with\nexperiments at a fine spatial scale and across multiple geographies (Singapore\nand Tokyo), whether we can use SVI and CV to assess bikeability\ncomprehensively. Extending related work, we develop an exhaustive index of\nbikeability composed of 34 indicators. The results suggest that SVI and CV are\nadequate to evaluate bikeability in cities comprehensively. As they\noutperformed non-SVI counterparts by a wide margin, SVI indicators are also\nfound to be superior in assessing urban bikeability, and potentially can be\nused independently, replacing traditional techniques. However, the paper\nexposes some limitations, suggesting that the best way forward is combining\nboth SVI and non-SVI approaches. The new bikeability index presents a\ncontribution in transportation and urban analytics, and it is scalable to\nassess cycling appeal widely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ito_K/0/1/0/all/0/1\">Koichi Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling. (arXiv:2105.12441v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.12441","description":"<p>Since 2014 transfer learning has become the key driver for the improvement of\nspatial saliency prediction; however, with stagnant progress in the last 3-5\nyears. We conduct a large-scale transfer learning study which tests different\nImageNet backbones, always using the same read out architecture and learning\nprotocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze\nII with ResNet50 features we improve the performance on saliency prediction\nfrom 78% to 85%. However, as we continue to test better ImageNet models as\nbackbones (such as EfficientNetB5) we observe no additional improvement on\nsaliency prediction. By analyzing the backbones further, we find that\ngeneralization to other datasets differs substantially, with models being\nconsistently overconfident in their fixation predictions. We show that by\ncombining multiple backbones in a principled manner a good confidence\ncalibration on unseen datasets can be achieved. This new model, \"DeepGaze IIE\",\nyields a significant leap in benchmark performance in and out-of-domain with a\n15 percent point improvement over DeepGaze II to 93% on MIT1003, marking a new\nstate of the art on the MIT/Tuebingen Saliency Benchmark in all available\nmetrics (AUC: 88.3%, sAUC: 79.4%, CC: 82.4%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerer_M/0/1/0/all/0/1\">Matthias K&#xfc;mmerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ori Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVOS: Video Object Segmentation with Transformers. (arXiv:2106.00588v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00588","description":"<p>Recently, Space-Time Memory Network (STM) based methods have achieved\nstate-of-the-art performance in semi-supervised video object segmentation\n(VOS). A crucial problem in this task is how to model the dependency both among\ndifferent frames and inside every frame. However, most of these methods neglect\nthe spatial relationships (inside each frame) and do not make full use of the\ntemporal relationships (among different frames). In this paper, we propose a\nnew transformer-based framework, termed TransVOS, introducing a vision\ntransformer to fully exploit and model both the temporal and spatial\nrelationships. Moreover, most STM-based approaches employ two separate encoders\nto extract features of two significant inputs, i.e., reference sets (history\nframes with predicted masks) and query frame (current frame), respectively,\nincreasing the models' parameters and complexity. To slim the popular\ntwo-encoder pipeline while keeping the effectiveness, we design a single\ntwo-path feature extractor to encode the above two inputs in a unified way.\nExtensive experiments demonstrate the superiority of our TransVOS over\nstate-of-the-art methods on both DAVIS and YouTube-VOS datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jianbiao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yeneng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reveal of Vision Transformers Robustness against Adversarial Attacks. (arXiv:2106.03734v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03734","description":"<p>The major part of the vanilla vision transformer (ViT) is the attention block\nthat brings the power of mimicking the global context of the input image. For\nbetter performance, ViT needs large-scale training data. To overcome this data\nhunger limitation, many ViT-based networks, or hybrid-ViT, have been proposed\nto include local context during the training. The robustness of ViTs and its\nvariants against adversarial attacks has not been widely investigated in the\nliterature like CNNs. This work studies the robustness of ViT variants 1)\nagainst different Lp-based adversarial attacks in comparison with CNNs, 2)\nunder adversarial examples (AEs) after applying preprocessing defense methods\nand 3) under the adaptive attacks using expectation over transformation (EOT)\nframework. To that end, we run a set of experiments on 1000 images from\nImageNet-1k and then provide an analysis that reveals that vanilla ViT or\nhybrid-ViT are more robust than CNNs. For instance, we found that 1) Vanilla\nViTs or hybrid-ViTs are more robust than CNNs under Lp-based attacks and under\nadaptive attacks. 2) Unlike hybrid-ViTs, Vanilla ViTs are not responding to\npreprocessing defenses that mainly reduce the high frequency components.\nFurthermore, feature maps, attention maps, and Grad-CAM visualization jointly\nwith image quality measures, and perturbations' energy spectrum are provided\nfor an insight understanding of attention-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aldahdooh_A/0/1/0/all/0/1\">Ahmed Aldahdooh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XIRL: Cross-embodiment Inverse Reinforcement Learning. (arXiv:2106.03911v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2106.03911","description":"<p>We investigate the visual cross-embodiment imitation setting, in which agents\nlearn policies from videos of other agents (such as humans) demonstrating the\nsame task, but with stark differences in their embodiments -- shape, actions,\nend-effector dynamics, etc. In this work, we demonstrate that it is possible to\nautomatically discover and learn vision-based reward functions from\ncross-embodiment demonstration videos that are robust to these differences.\nSpecifically, we present a self-supervised method for Cross-embodiment Inverse\nReinforcement Learning (XIRL) that leverages temporal cycle-consistency\nconstraints to learn deep visual embeddings that capture task progression from\noffline videos of demonstrations across multiple expert agents, each performing\nthe same task differently due to embodiment differences. Prior to our work,\nproducing rewards from self-supervised embeddings typically required alignment\nwith a reference trajectory, which may be difficult to acquire under stark\nembodiment differences. We show empirically that if the embeddings are aware of\ntask progress, simply taking the negative distance between the current state\nand goal state in the learned embedding space is useful as a reward for\ntraining policies with reinforcement learning. We find our learned reward\nfunction not only works for embodiments seen during training, but also\ngeneralizes to entirely new embodiments. Additionally, when transferring\nreal-world human demonstrations to a simulated robot, we find that XIRL is more\nsample efficient than current best methods. Qualitative results, code, and\ndatasets are available at https://x-irl.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zakka_K/0/1/0/all/0/1\">Kevin Zakka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1\">Jonathan Tompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwibedi_D/0/1/0/all/0/1\">Debidatta Dwibedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surgical data science for safe cholecystectomy: a protocol for segmentation of hepatocystic anatomy and assessment of the critical view of safety. (arXiv:2106.10916v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.10916","description":"<p>Minimally invasive image-guided surgery heavily relies on vision. Deep\nlearning models for surgical video analysis could therefore support visual\ntasks such as assessing the critical view of safety (CVS) in laparoscopic\ncholecystectomy (LC), potentially contributing to surgical safety and\nefficiency. However, the performance, reliability and reproducibility of such\nmodels are deeply dependent on the quality of data and annotations used in\ntheir development. Here, we present a protocol, checklists, and visual examples\nto promote consistent annotation of hepatocystic anatomy and CVS criteria. We\nbelieve that sharing annotation guidelines can help build trustworthy\nmulticentric datasets for assessing generalizability of performance, thus\naccelerating the clinical translation of deep learning models for surgical\nvideo analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mascagni_P/0/1/0/all/0/1\">Pietro Mascagni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alapatt_D/0/1/0/all/0/1\">Deepak Alapatt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_A/0/1/0/all/0/1\">Alain Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Okamoto_N/0/1/0/all/0/1\">Nariaki Okamoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vardazaryan_A/0/1/0/all/0/1\">Armine Vardazaryan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Costamagna_G/0/1/0/all/0/1\">Guido Costamagna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dallemagne_B/0/1/0/all/0/1\">Bernard Dallemagne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocFormer: End-to-End Transformer for Document Understanding. (arXiv:2106.11539v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11539","description":"<p>We present DocFormer -- a multi-modal transformer based architecture for the\ntask of Visual Document Understanding (VDU). VDU is a challenging problem which\naims to understand documents in their varied formats (forms, receipts etc.) and\nlayouts. In addition, DocFormer is pre-trained in an unsupervised fashion using\ncarefully designed tasks which encourage multi-modal interaction. DocFormer\nuses text, vision and spatial features and combines them using a novel\nmulti-modal self-attention layer. DocFormer also shares learned spatial\nembeddings across modalities which makes it easy for the model to correlate\ntext to visual tokens and vice versa. DocFormer is evaluated on 4 different\ndatasets each with strong baselines. DocFormer achieves state-of-the-art\nresults on all of them, sometimes beating models 4x its size (in no. of\nparameters).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1\">Srikar Appalaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jasani_B/0/1/0/all/0/1\">Bhavan Jasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kota_B/0/1/0/all/0/1\">Bhargava Urala Kota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yusheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Counterfactual Visual Explanations With Overdetermination. (arXiv:2106.14556v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14556","description":"<p>A novel explainable AI method called CLEAR Image is introduced in this paper.\nCLEAR Image is based on the view that a satisfactory explanation should be\ncontrastive, counterfactual and measurable. CLEAR Image explains an image's\nclassification probability by contrasting the image with a corresponding image\ngenerated automatically via adversarial learning. This enables both salient\nsegmentation and perturbations that faithfully determine each segment's\nimportance. CLEAR Image was successfully applied to a medical imaging case\nstudy where it outperformed methods such as Grad-CAM and LIME by an average of\n27% using a novel pointing game metric. CLEAR Image excels in identifying cases\nof \"causal overdetermination\" where there are multiple patches in an image, any\none of which is sufficient by itself to cause the classification probability to\nbe close to one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Adam White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngan_K/0/1/0/all/0/1\">Kwun Ho Ngan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phelan_J/0/1/0/all/0/1\">James Phelan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afgeh_S/0/1/0/all/0/1\">Saman Sadeghi Afgeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_K/0/1/0/all/0/1\">Kevin Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_Aldasoro_C/0/1/0/all/0/1\">Constantino Carlos Reyes-Aldasoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcez_A/0/1/0/all/0/1\">Artur d&#x27;Avila Garcez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Cognitive Fatigue from fMRI Scans with Self-supervised Learning. (arXiv:2106.15009v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15009","description":"<p>Functional magnetic resonance imaging (fMRI) is a neuroimaging technique that\nrecords neural activations in the brain by capturing the blood oxygen level in\ndifferent regions based on the task performed by a subject. Given fMRI data,\nthe problem of predicting the state of cognitive fatigue in a person has not\nbeen investigated to its full extent. This paper proposes tackling this issue\nas a multi-class classification problem by dividing the state of cognitive\nfatigue into six different levels, ranging from no-fatigue to extreme fatigue\nconditions. We built a spatio-temporal model that uses convolutional neural\nnetworks (CNN) for spatial feature extraction and a long short-term memory\n(LSTM) network for temporal modeling of 4D fMRI scans. We also applied a\nself-supervised method called MoCo (Momentum Contrast) to pre-train our model\non a public dataset BOLD5000 and fine-tuned it on our labeled dataset to\npredict cognitive fatigue. Our novel dataset contains fMRI scans from Traumatic\nBrain Injury (TBI) patients and healthy controls (HCs) while performing a\nseries of N-back cognitive tasks. This method establishes a state-of-the-art\ntechnique to analyze cognitive fatigue from fMRI data and beats previous\napproaches to solve this problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ashish Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Ashwin Ramesh Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadeh_M/0/1/0/all/0/1\">Mohammad Zaki Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makedon_F/0/1/0/all/0/1\">Fillia Makedon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wylie_G/0/1/0/all/0/1\">Glenn Wylie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2106.16174","description":"<p>The cells and their spatial patterns in the tumor microenvironment (TME) play\na key role in tumor evolution, and yet the latter remains an understudied topic\nin computational pathology. This study, to the best of our knowledge, is among\nthe first to hybridize local and global graph methods to profile orchestration\nand interaction of cellular components. To address the challenge in\nhematolymphoid cancers, where the cell classes in TME may be unclear, we first\nimplemented cell-level unsupervised learning and identified two new cell\nsubtypes. Local cell graphs or supercells were built for each image by\nconsidering the individual cell's geospatial location and classes. Then, we\napplied supercell level clustering and identified two new cell communities. In\nthe end, we built global graphs to abstract spatial interaction patterns and\nextract features for disease diagnosis. We evaluate the proposed algorithm on\nH&amp;E slides of 60 hematolymphoid neoplasms and further compared it with three\ncell level graph-based algorithms, including the global cell graph, cluster\ncell graph, and FLocK. The proposed algorithm achieved a mean diagnosis\naccuracy of 0.703 with the repeated 5-fold cross-validation scheme. In\nconclusion, our algorithm shows superior performance over the existing methods\nand can be potentially applied to other cancer types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_P/0/1/0/all/0/1\">Pingjun Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Aminu_M/0/1/0/all/0/1\">Muhammad Aminu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hussein_S/0/1/0/all/0/1\">Siba El Hussein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khoury_J/0/1/0/all/0/1\">Joseph D. Khoury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YinYang-Net: Complementing Face and Body Information for Wild Gender Recognition. (arXiv:2107.06847v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06847","description":"<p>Soft biometrics inference in surveillance scenarios is a topic of interest\nfor various applications, particularly in security-related areas. However, soft\nbiometric analysis is not extensively reported in wild conditions. In\nparticular, previous works on gender recognition report their results in face\ndatasets, with relatively good image quality and frontal poses. Given the\nuncertainty of the availability of the facial region in wild conditions, we\nconsider that these methods are not adequate for surveillance settings. To\novercome these limitations, we: 1) present frontal and wild face versions of\nthree well-known surveillance datasets; and 2) propose YinYang-Net (YY-Net), a\nmodel that effectively and dynamically complements facial and body information,\nwhich makes it suitable for gender recognition in wild conditions. The frontal\nand wild face datasets derive from widely used Pedestrian Attribute Recognition\n(PAR) sets (PETA, PA-100K, and RAP), using a pose-based approach to filter the\nfrontal samples and facial regions. This approach retrieves the facial region\nof images with varying image/subject conditions, where the state-of-the-art\nface detectors often fail. YY-Net combines facial and body information through\na learnable fusion matrix and a channel-attention sub-network, focusing on the\nmost influential body parts according to the specific image/subject features.\nWe compare it with five PAR methods, consistently obtaining state-of-the-art\nresults on gender recognition, and reducing the prediction errors by up to 24%\nin frontal samples. The announced PAR datasets versions and YY-Net serve as the\nbasis for wild soft biometrics classification and are available in\nhttps://github.com/Tiago-Roxo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roxo_T/0/1/0/all/0/1\">Tiago Roxo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proenca_H/0/1/0/all/0/1\">Hugo Proen&#xe7;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wasserstein Distances, Geodesics and Barycenters of Merge Trees. (arXiv:2107.07789v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2107.07789","description":"<p>This paper presents a unified computational framework for the estimation of\ndistances, geodesics and barycenters of merge trees. We extend recent work on\nthe edit distance [106] and introduce a new metric, called the Wasserstein\ndistance between merge trees, which is purposely designed to enable efficient\ncomputations of geodesics and barycenters. Specifically, our new distance is\nstrictly equivalent to the L2-Wasserstein distance between extremum persistence\ndiagrams, but it is restricted to a smaller solution space, namely, the space\nof rooted partial isomorphisms between branch decomposition trees. This enables\na simple extension of existing optimization frameworks [112] for geodesics and\nbarycenters from persistence diagrams to merge trees. We introduce a task-based\nalgorithm which can be generically applied to distance, geodesic, barycenter or\ncluster computation. The task-based nature of our approach enables further\naccelerations with shared-memory parallelism. Extensive experiments on public\nensembles and SciVis contest benchmarks demonstrate the efficiency of our\napproach -- with barycenter computations in the orders of minutes for the\nlargest examples -- as well as its qualitative ability to generate\nrepresentative barycenter merge trees, visually summarizing the features of\ninterest found in the ensemble. We show the utility of our contributions with\ndedicated visualization applications: feature tracking, temporal reduction and\nensemble clustering. We provide a lightweight C++ implementation that can be\nused to reproduce our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pont_M/0/1/0/all/0/1\">Mathieu Pont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1\">Jules Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delon_J/0/1/0/all/0/1\">Julie Delon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1\">Julien Tierny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNE-RoadSeg+: Rethinking Depth-Normal Translation and Deep Supervision for Freespace Detection. (arXiv:2107.14599v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14599","description":"<p>Freespace detection is a fundamental component of autonomous driving\nperception. Recently, deep convolutional neural networks (DCNNs) have achieved\nimpressive performance for this task. In particular, SNE-RoadSeg, our\npreviously proposed method based on a surface normal estimator (SNE) and a\ndata-fusion DCNN (RoadSeg), has achieved impressive performance in freespace\ndetection. However, SNE-RoadSeg is computationally intensive, and it is\ndifficult to execute in real time. To address this problem, we introduce\nSNE-RoadSeg+, an upgraded version of SNE-RoadSeg. SNE-RoadSeg+ consists of 1)\nSNE+, a module for more accurate surface normal estimation, and 2) RoadSeg+, a\ndata-fusion DCNN that can greatly minimize the trade-off between accuracy and\nefficiency with the use of deep supervision. Extensive experimental results\nhave demonstrated the effectiveness of our SNE+ for surface normal estimation\nand the superior performance of our SNE-RoadSeg+ over all other freespace\ndetection approaches. Specifically, our SNE-RoadSeg+ runs in real time, and\nmeanwhile, achieves the state-of-the-art performance on the KITTI road\nbenchmark. Our project page is at\nhttps://www.sne-roadseg.site/sne-roadseg-plus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Peide Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02093","description":"<p>We unveil a long-standing problem in the prevailing co-saliency detection\nsystems: there is indeed inconsistency between training and testing.\nConstructing a high-quality co-saliency detection dataset involves\ntime-consuming and labor-intensive pixel-level labeling, which has forced most\nrecent works to rely instead on semantic segmentation or saliency detection\ndatasets for training. However, the lack of proper co-saliency and the absence\nof multiple foreground objects in these datasets can lead to spurious\nvariations and inherent biases learned by models. To tackle this, we introduce\nthe idea of counterfactual training through context adjustment and propose a\n\"cost-free\" group-cut-paste (GCP) procedure to leverage off-the-shelf images\nand synthesize new samples. Following GCP, we collect a novel dataset called\nContext Adjustment Training (CAT). CAT consists of 33,500 images, which is four\ntimes larger than the current co-saliency detection datasets. All samples are\nautomatically annotated with high-quality mask annotations, object categories,\nand edge maps. Extensive experiments on recent benchmarks are conducted, show\nthat CAT can improve various state-of-the-art models by a large margin (5% ~\n25%). We hope that the scale, diversity, and quality of our dataset can benefit\nresearchers in this area and beyond. Our dataset will be publicly accessible\nthrough our project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingdong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1\">Prakhar Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch Your Own GAN. (arXiv:2108.02774v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02774","description":"<p>Can a user create a deep generative model by sketching a single example?\nTraditionally, creating a GAN model has required the collection of a\nlarge-scale dataset of exemplars and specialized knowledge in deep learning. In\ncontrast, sketching is possibly the most universally accessible way to convey a\nvisual concept. In this work, we present a method, GAN Sketching, for rewriting\nGANs with one or more sketches, to make GANs training easier for novice users.\nIn particular, we change the weights of an original GAN model according to user\nsketches. We encourage the model's output to match the user sketches through a\ncross-domain adversarial loss. Furthermore, we explore different regularization\nmethods to preserve the original model's diversity and image quality.\nExperiments have shown that our method can mold GANs to match shapes and poses\nspecified by sketches while maintaining realism and diversity. Finally, we\ndemonstrate a few applications of the resulting GAN, including latent space\ninterpolation and image editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng-Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RECALL: Replay-based Continual Learning in Semantic Segmentation. (arXiv:2108.03673v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03673","description":"<p>Deep networks allow to obtain outstanding results in semantic segmentation,\nhowever they need to be trained in a single shot with a large amount of data.\nContinual learning settings where new classes are learned in incremental steps\nand previous training data is no longer available are challenging due to the\ncatastrophic forgetting phenomenon. Existing approaches typically fail when\nseveral incremental steps are performed or in presence of a distribution shift\nof the background class. We tackle these issues by recreating no longer\navailable data for the old classes and outlining a content inpainting scheme on\nthe background class. We propose two sources for replay data. The first resorts\nto a generative adversarial network to sample from the class space of past\nlearning steps. The second relies on web-crawled data to retrieve images\ncontaining examples of old classes from online databases. In both scenarios no\nsamples of past steps are stored, thus avoiding privacy concerns. Replay data\nare then blended with new samples during the incremental steps. Our approach,\nRECALL, outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maracani_A/0/1/0/all/0/1\">Andrea Maracani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening. (arXiv:2108.03815v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03815","description":"<p>Anomaly detection plays a pivotal role in numerous real-world scenarios, such\nas industrial automation and manufacturing intelligence. Recently, variational\ninference-based anomaly analysis has attracted researchers' and developers'\nattention. It aims to model the defect-free distribution so that anomalies can\nbe classified as out-of-distribution samples. Nevertheless, there are two\ndisturbing factors that need us to prioritize: (i) the simplistic prior latent\ndistribution inducing limited expressive capability; (ii) the strong\nprobability distance notion results in collapsed features. In this paper, we\npropose a novel Patch-wise Wasserstein AutoEncoder (P-WAE) architecture to\nalleviate those challenges. In particular, a patch-wise variational inference\nmodel coupled with solving the jigsaw puzzle is designed, which is a simple yet\neffective way to increase the expressiveness of the latent manifold. This makes\nusing the model on high-dimensional practical data possible. In addition, we\nleverage a weaker measure, sliced-Wasserstein distance, to achieve the\nequilibrium between the reconstruction fidelity and generalized\nrepresentations. Comprehensive experiments, conducted on the MVTec AD dataset,\ndemonstrate the superior performance of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03823","description":"<p>To mitigate the radiologist's workload, computer-aided diagnosis with the\ncapability to review and analyze medical images is gradually deployed. Deep\nlearning-based region of interest segmentation is among the most exciting use\ncases. However, this paradigm is restricted in real-world clinical applications\ndue to poor robustness and generalization. The issue is more sinister with a\nlack of training data. In this paper, we address the challenge from the\nrepresentation learning point of view. We investigate that the collapsed\nrepresentations, as one of the main reasons which caused poor robustness and\ngeneralization, could be avoided through transfer learning. Therefore, we\npropose a novel two-stage framework for robust generalized segmentation. In\nparticular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining\narchitecture is coined to learn meaningful representation for improving the\ngeneralization and robustness of the downstream tasks. Furthermore, the learned\nknowledge is transferred to the segmentation benchmark. Coupled with an image\nreconstruction network, the representation keeps to be decoded, encouraging the\nmodel to capture more semantic features. Experiments of lung segmentation on\nmulti chest X-ray datasets are conducted. Empirically, the related experimental\nresults demonstrate the superior generalization capability of the proposed\nframework on unseen domains in terms of high performance and robustness to\ncorruption, especially under the scenario of the limited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Cut by Watching Movies. (arXiv:2108.04294v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04294","description":"<p>Video content creation keeps growing at an incredible pace; yet, creating\nengaging stories remains challenging and requires non-trivial video editing\nexpertise. Many video editing components are astonishingly hard to automate\nprimarily due to the lack of raw video materials. This paper focuses on a new\ntask for computational video editing, namely the task of raking cut\nplausibility. Our key idea is to leverage content that has already been edited\nto learn fine-grained audiovisual patterns that trigger cuts. To do this, we\nfirst collected a data source of more than 10K videos, from which we extract\nmore than 255K cuts. We devise a model that learns to discriminate between real\nand artificial cuts via contrastive learning. We set up a new task and a set of\nbaselines to benchmark video cut generation. We observe that our proposed model\noutperforms the baselines by large margins. To demonstrate our model in\nreal-world applications, we conduct human studies in a collection of unedited\nvideos. The results show that our model does a better job at cutting than\nrandom and alternative baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cascaded Zoom-In Network for Patterned Fabric Defect Detection. (arXiv:2108.06760v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06760","description":"<p>Nowadays, Deep Convolutional Neural Networks (DCNNs) are widely used in\nfabric defect detection, which come with the cost of expensive training and\ncomplex model parameters. With the observation that most fabrics are defect\nfree in practice, a two-step Cascaded Zoom-In Network (CZI-Net) is proposed for\npatterned fabric defect detection. In the CZI-Net, the Aggregated HOG (A-HOG)\nand SIFT features are used to instead of simple convolution filters for feature\nextraction. Moreover, in order to extract more distinctive features, the\nfeature representation layer and full connection layer are included in the\nCZI-Net. In practice, Most defect-free fabrics only involve in the first step\nof our method and avoid a costive computation in the second step, which makes\nvery fast fabric detection. More importantly, we propose the\nLocality-constrained Reconstruction Error (LCRE) in the first step and\nRestrictive Locality-constrained Coding (RLC), Bag-of-Indexes (BoI) methods in\nthe second step. We also analyse the connections between different coding\nmethods and conclude that the index of visual words plays an essential role in\nthe coding methods. In conclusion, experiments based on real-world datasets are\nimplemented and demonstrate that our proposed method is not only\ncomputationally simple but also with high detection accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Image Coding for Machines: A Content-Adaptive Approach. (arXiv:2108.09992v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.09992","description":"<p>Today, according to the Cisco Annual Internet Report (2018-2023), the\nfastest-growing category of Internet traffic is machine-to-machine\ncommunication. In particular, machine-to-machine communication of images and\nvideos represents a new challenge and opens up new perspectives in the context\nof data compression. One possible solution approach consists of adapting\ncurrent human-targeted image and video coding standards to the use case of\nmachine consumption. Another approach consists of developing completely new\ncompression paradigms and architectures for machine-to-machine communications.\nIn this paper, we focus on image compression and present an inference-time\ncontent-adaptive finetuning scheme that optimizes the latent representation of\nan end-to-end learned image codec, aimed at improving the compression\nefficiency for machine-consumption. The conducted experiments show that our\nonline finetuning brings an average bitrate saving (BD-rate) of -3.66% with\nrespect to our pretrained image codec. In particular, at low bitrate points,\nour proposed method results in a significant bitrate saving of -9.85%. Overall,\nour pretrained-and-then-finetuned system achieves -30.54% BD-rate over the\nstate-of-the-art image/video codec Versatile Video Coding (VVC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1\">Nam Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Honglei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cricri_F/0/1/0/all/0/1\">Francesco Cricri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghaznavi_Youvalari_R/0/1/0/all/0/1\">Ramin Ghaznavi-Youvalari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tavakoli_H/0/1/0/all/0/1\">Hamed Rezazadegan Tavakoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sk-Unet Model with Fourier Domain for Mitosis Detection. (arXiv:2109.00957v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.00957","description":"<p>Mitotic count is the most important morphological feature of breast cancer\ngrading. Many deep learning-based methods have been proposed but suffer from\ndomain shift. In this work, we construct a Fourier-based segmentation model for\nmitosis detection to address the problem. Swapping the low-frequency spectrum\nof source and target images is shown effective to alleviate the discrepancy\nbetween different scanners. Our Fourier-based segmentation method can achieve\nF1 with 0.7456 on the preliminary test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiyue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation. (arXiv:2109.01827v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01827","description":"<p>In this paper, we propose GOHOME, a method leveraging graph representations\nof the High Definition Map and sparse projections to generate a heatmap output\nrepresenting the future position probability distribution for a given agent in\na traffic scene. This heatmap output yields an unconstrained 2D grid\nrepresentation of agent future possible locations, allowing inherent\nmultimodality and a measure of the uncertainty of the prediction. Our\ngraph-oriented model avoids the high computation burden of representing the\nsurrounding context as squared images and processing it with classical CNNs,\nbut focuses instead only on the most probable lanes where the agent could end\nup in the immediate future. GOHOME reaches 2$nd$ on Argoverse Motion\nForecasting Benchmark on the MissRate$_6$ metric while achieving significant\nspeed-up and memory burden diminution compared to Argoverse 1$^{st}$ place\nmethod HOME. We also highlight that heatmap output enables multimodal\nensembling and improve 1$^{st}$ place MissRate$_6$ by more than 15$\\%$ with our\nbest ensemble on Argoverse. Finally, we evaluate and reach state-of-the-art\nperformance on the other trajectory prediction datasets nuScenes and\nInteraction, demonstrating the generalizability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum-Classical Hybrid Machine Learning for Image Classification (ICCAD Special Session Paper). (arXiv:2109.02862v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02862","description":"<p>Image classification is a major application domain for conventional deep\nlearning (DL). Quantum machine learning (QML) has the potential to\nrevolutionize image classification. In any typical DL-based image\nclassification, we use convolutional neural network (CNN) to extract features\nfrom the image and multi-layer perceptron network (MLP) to create the actual\ndecision boundaries. On one hand, QML models can be useful in both of these\ntasks. Convolution with parameterized quantum circuits (Quanvolution) can\nextract rich features from the images. On the other hand, quantum neural\nnetwork (QNN) models can create complex decision boundaries. Therefore,\nQuanvolution and QNN can be used to create an end-to-end QML model for image\nclassification. Alternatively, we can extract image features separately using\nclassical dimension reduction techniques such as, Principal Components Analysis\n(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to\ntrain a QNN. We review two proposals on quantum-classical hybrid ML models for\nimage classification namely, Quanvolutional Neural Network and dimension\nreduction using a classical algorithm followed by QNN. Particularly, we make a\ncase for trainable filters in Quanvolution and CAE-based feature extraction for\nimage datasets (instead of dimension reduction using linear transformations\nsuch as, PCA). We discuss various design choices, potential opportunities, and\ndrawbacks of these models. We also release a Python-based framework to create\nand explore these hybrid models with a variety of design choices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mahabubul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Satwik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topaloglu_R/0/1/0/all/0/1\">Rasit Onur Topaloglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Swaroop Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths. (arXiv:2109.03310v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.03310","description":"<p>Malignant melanoma is a common skin cancer that is mostly curable before\nmetastasis -when growths spawn in organs away from the original site. Melanoma\nis the most dangerous type of skin cancer if left untreated due to the high\nrisk of metastasis. This paper presents Melatect, a machine learning (ML) model\nembedded in an iOS app that identifies potential malignant melanoma. Melatect\naccurately classifies lesions as malignant or benign over 96.6% of the time\nwith no apparent bias or overfitting. Using the Melatect app, users have the\nability to take pictures of skin lesions (moles) and subsequently receive a\nmole classification. The Melatect app provides a convenient way to get free\nadvice on lesions and track these lesions over time. A recursive computer image\nanalysis algorithm and modified MLOps pipeline was developed to create a model\nthat performs at a higher accuracy than existing models. Our training dataset\nincluded 18,400 images of benign and malignant lesions, including 18,000 from\nthe International Skin Imaging Collaboration (ISIC) archive, as well as 400\nimages gathered from local dermatologists; these images were augmented using\nDeepAugment, an AutoML tool, to 54,054 images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meel_V/0/1/0/all/0/1\">Vidushi Meel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bodepudi_A/0/1/0/all/0/1\">Asritha Bodepudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Tensor Network Representation for High-Order Tensor Completion. (arXiv:2109.04022v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04022","description":"<p>This work studies the problem of high-dimensional data (referred to tensors)\ncompletion from partially observed samplings. We consider that a tensor is a\nsuperposition of multiple low-rank components. In particular, each component\ncan be represented as multilinear connections over several latent factors and\nnaturally mapped to a specific tensor network (TN) topology. In this paper, we\npropose a fundamental tensor decomposition (TD) framework: Multi-Tensor Network\nRepresentation (MTNR), which can be regarded as a linear combination of a range\nof TD models, e.g., CANDECOMP/PARAFAC (CP) decomposition, Tensor Train (TT),\nand Tensor Ring (TR). Specifically, MTNR represents a high-order tensor as the\naddition of multiple TN models, and the topology of each TN is automatically\ngenerated instead of manually pre-designed. For the optimization phase, an\nadaptive topology learning (ATL) algorithm is presented to obtain latent\nfactors of each TN based on a rank incremental strategy and a projection error\nmeasurement strategy. In addition, we theoretically establish the fundamental\nmultilinear operations for the tensors with TN representation, and reveal the\nstructural transformation of MTNR to a single TN. Finally, MTNR is applied to a\ntypical task, tensor completion, and two effective algorithms are proposed for\nthe exact recovery of incomplete data based on the Alternating Least Squares\n(ALS) scheme and Alternating Direction Method of Multiplier (ADMM) framework.\nExtensive numerical experiments on synthetic data and real-world datasets\ndemonstrate the effectiveness of MTNR compared with the start-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1\">Chang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Transferable Adversarial Attacks on Vision Transformers. (arXiv:2109.04176v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04176","description":"<p>Vision transformers (ViTs) have demonstrated impressive performance on a\nseries of computer vision tasks, yet they still suffer from adversarial\nexamples. In this paper, we posit that adversarial attacks on transformers\nshould be specially tailored for their architecture, jointly considering both\npatches and self-attention, in order to achieve high transferability. More\nspecifically, we introduce a dual attack framework, which contains a Pay No\nAttention (PNA) attack and a PatchOut attack, to improve the transferability of\nadversarial samples across different ViTs. We show that skipping the gradients\nof attention during backpropagation can generate adversarial examples with high\ntransferability. In addition, adversarial perturbations generated by optimizing\nrandomly sampled subsets of patches at each iteration achieve higher attack\nsuccess rates than attacks using all patches. We evaluate the transferability\nof attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The\nresults of these experiments demonstrate that the proposed dual attack can\ngreatly boost transferability between ViTs and from ViTs to CNNs. In addition,\nthe proposed method can easily be combined with existing transfer methods to\nboost performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhipeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ErfAct and PSerf: Non-monotonic smooth trainable Activation Functions. (arXiv:2109.04386v3 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2109.04386","description":"<p>An activation function is a crucial component of a neural network that\nintroduces non-linearity in the network. The state-of-the-art performance of a\nneural network depends on the perfect choice of an activation function. We\npropose two novel non-monotonic smooth trainable activation functions, called\nErfAct and PSerf. Experiments suggest that the proposed functions improve the\nnetwork performance significantly compared to the widely used activations like\nReLU, Swish, and Mish. Replacing ReLU by ErfAct and PSerf, we have 5.21% and\n5.04% improvement for top-1 accuracy on PreactResNet-34 network in CIFAR100\ndataset, 2.58% and 2.76% improvement for top-1 accuracy on PreactResNet-34\nnetwork in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean average\nprecision (mAP) on SSD300 model in Pascal VOC dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Shilpak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvMLP: Hierarchical Convolutional MLPs for Vision. (arXiv:2109.04454v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04454","description":"<p>MLP-based architectures, which consist of a sequence of consecutive\nmulti-layer perceptron blocks, have recently been found to reach comparable\nresults to convolutional and transformer-based methods. However, most adopt\nspatial MLPs which take fixed dimension inputs, therefore making it difficult\nto apply them to downstream tasks, such as object detection and semantic\nsegmentation. Moreover, single-stage designs further limit performance in other\ncomputer vision tasks and fully connected layers bear heavy computation. To\ntackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for\nvisual recognition, which is a light-weight, stage-wise, co-design of\nconvolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1\naccuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of\nMLP-Mixer-B/16, respectively). Experiments on object detection and semantic\nsegmentation further show that visual representation learned by ConvMLP can be\nseamlessly transferred and achieve competitive results with fewer parameters.\nOur code and pre-trained models are publicly available at\nhttps://github.com/SHI-Labs/Convolutional-MLPs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time multimodal image registration with partial intraoperative point-set data. (arXiv:2109.05023v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.05023","description":"<p>We present Free Point Transformer (FPT) - a deep neural network architecture\nfor non-rigid point-set registration. Consisting of two modules, a global\nfeature extraction module and a point transformation module, FPT does not\nassume explicit constraints based on point vicinity, thereby overcoming a\ncommon requirement of previous learning-based point-set registration methods.\nFPT is designed to accept unordered and unstructured point-sets with a variable\nnumber of points and uses a \"model-free\" approach without heuristic\nconstraints. Training FPT is flexible and involves minimizing an intuitive\nunsupervised loss function, but supervised, semi-supervised, and partially- or\nweakly-supervised training are also supported. This flexibility makes FPT\namenable to multimodal image registration problems where the ground-truth\ndeformations are difficult or impossible to measure. In this paper, we\ndemonstrate the application of FPT to non-rigid registration of prostate\nmagnetic resonance (MR) imaging and sparsely-sampled transrectal ultrasound\n(TRUS) images. The registration errors were 4.71 mm and 4.81 mm for complete\nTRUS imaging and sparsely-sampled TRUS imaging, respectively. The results\nindicate superior accuracy to the alternative rigid and non-rigid registration\nalgorithms tested and substantially lower computation time. The rapid inference\npossible with FPT makes it particularly suitable for applications where\nreal-time registration is beneficial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M C Baum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C Barratt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team NeuroPoly: Description of the Pipelines for the MICCAI 2021 MS New Lesions Segmentation Challenge. (arXiv:2109.05409v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.05409","description":"<p>This paper gives a detailed description of the pipelines used for the 2nd\nedition of the MICCAI 2021 Challenge on Multiple Sclerosis Lesion Segmentation.\nAn overview of the data preprocessing steps applied is provided along with a\nbrief description of the pipelines used, in terms of the architecture and the\nhyperparameters. Our code for this work can be found at:\nhttps://github.com/ivadomed/ms-challenge-2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Macar_U/0/1/0/all/0/1\">Uzay Macar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karthik_E/0/1/0/all/0/1\">Enamundram Naga Karthik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gros_C/0/1/0/all/0/1\">Charley Gros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andr&#xe9;anne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MovieCuts: A New Dataset and Benchmark for Cut Type Recognition. (arXiv:2109.05569v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05569","description":"<p>Understanding movies and their structural patterns is a crucial task to\ndecode the craft of video editing. While previous works have developed tools\nfor general analysis such as detecting characters or recognizing cinematography\nproperties at the shot level, less effort has been devoted to understanding the\nmost basic video edit, the Cut. This paper introduces the cut type recognition\ntask, which requires modeling of multi-modal information. To ignite research in\nthe new task, we construct a large-scale dataset called MovieCuts, which\ncontains more than 170K videoclips labeled among ten cut types. We benchmark a\nseries of audio-visual approaches, including some that deal with the problem's\nmulti-modal and multi-label nature. Our best model achieves 45.7% mAP, which\nsuggests that the task is challenging and that attaining highly accurate cut\ntype recognition is an open research problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised domain adaptation for cross-modality liver segmentation via joint adversarial learning and self-learning. (arXiv:2109.05664v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05664","description":"<p>Liver segmentation on images acquired using computed tomography (CT) and\nmagnetic resonance imaging (MRI) plays an important role in clinical management\nof liver diseases. Compared to MRI, CT images of liver are more abundant and\nreadily available. However, MRI can provide richer quantitative information of\nthe liver compared to CT. Thus, it is desirable to achieve unsupervised domain\nadaptation for transferring the learned knowledge from the source domain\ncontaining labeled CT images to the target domain containing unlabeled MR\nimages. In this work, we report a novel unsupervised domain adaptation\nframework for cross-modality liver segmentation via joint adversarial learning\nand self-learning. We propose joint semantic-aware and shape-entropy-aware\nadversarial learning with post-situ identification manner to implicitly align\nthe distribution of task-related features extracted from the target domain with\nthose from the source domain. In proposed framework, a network is trained with\nthe above two adversarial losses in an unsupervised manner, and then a mean\ncompleter of pseudo-label generation is employed to produce pseudo-labels to\ntrain the next network (desired model). Additionally, semantic-aware\nadversarial learning and two self-learning methods, including pixel-adaptive\nmask refinement and student-to-partner learning, are proposed to train the\ndesired model. To improve the robustness of the desired model, a low-signal\naugmentation function is proposed to transform MRI images as the input of the\ndesired model to handle hard samples. Using the public data sets, our\nexperiments demonstrated the proposed unsupervised domain adaptation framework\noutperformed four supervised learning methods with a Dice score 0.912 plus or\nminus 0.037 (mean plus or minus standard deviation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Simon Chun-Ho Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weitian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UMPNet: Universal Manipulation Policy Network for Articulated Objects. (arXiv:2109.05668v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05668","description":"<p>We introduce the Universal Manipulation Policy Network (UMPNet) -- a single\nimage-based policy network that infers closed-loop action sequences for\nmanipulating arbitrary articulated objects. To infer a wide range of action\ntrajectories, the policy supports 6DoF action representation and varying\ntrajectory length. To handle a diverse set of objects, the policy learns from\nobjects with different articulation structures and generalizes to unseen\nobjects or categories. The policy is trained with self-guided exploration\nwithout any human demonstrations, scripted policy, or pre-defined goal\nconditions. To support effective multi-step interaction, we introduce a novel\nArrow-of-Time action attribute that indicates whether an action will change the\nobject state back to the past or forward into the future. With the\nArrow-of-Time inference at each interaction step, the learned policy is able to\nselect actions that consistently lead towards or away from a given state,\nthereby, enabling both effective state exploration and goal-conditioned\nmanipulation. Video is available at https://youtu.be/KqlvcL9RqKM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhanpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Separated Curve Rendering Network for Efficient and High-Resolution Image Harmonization. (arXiv:2109.05750v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05750","description":"<p>Image harmonization aims to modify the color of the composited region with\nrespect to the specific background. Previous works model this task as a\npixel-wise image-to-image translation using UNet family structures. However,\nthe model size and computational cost limit the performability of their models\non edge devices and higher-resolution images. To this end, we propose a novel\nspatial-separated curve rendering network (S$^2$CRNet) for efficient and\nhigh-resolution image harmonization for the first time. In S$^2$CRNet, we\nfirstly extract the spatial-separated embeddings from the thumbnails of the\nmasked foreground and background individually. Then, we design a curve\nrendering module (CRM), which learns and combines the spatial-specific\nknowledge using linear layers to generate the parameters of the pixel-wise\ncurve mapping in the foreground region. Finally, we directly render the\noriginal high-resolution images using the learned color curve. Besides, we also\nmake two extensions of the proposed framework via the Cascaded-CRM and\nSemantic-CRM for cascaded refinement and semantic guidance, respectively.\nExperiments show that the proposed method reduces more than 90% parameters\ncompared with previous methods but still achieves the state-of-the-art\nperformance on both synthesized iHarmony4 and real-world DIH test set.\nMoreover, our method can work smoothly on higher resolution images in real-time\nwhich is more than 10$\\times$ faster than the existing methods. The code and\npre-trained models will be made available and released at\nhttps://github.com/stefanLeong/S2CRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingtang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1\">Xiaodong Cun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2109.06274v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.06274","description":"<p>Automatic methods to segment the vestibular schwannoma (VS) tumors and the\ncochlea from magnetic resonance imaging (MRI) are critical to VS treatment\nplanning. Although supervised methods have achieved satisfactory performance in\nVS segmentation, they require full annotations by experts, which is laborious\nand time-consuming. In this work, we aim to tackle the VS and cochlea\nsegmentation problem in an unsupervised domain adaptation setting. Our proposed\nmethod leverages both the image-level domain alignment to minimize the domain\ndivergence and semi-supervised training to further boost the performance.\nFurthermore, we propose to fuse the labels predicted from multiple models via\nnoisy label correction. Our results on the challenge validation leaderboard\nshowed that our unsupervised method has achieved promising VS and cochlea\nsegmentation performance with mean dice score of 0.8261 $\\pm$ 0.0416; The mean\ndice value for the tumor is 0.8302 $\\pm$ 0.0772. This is comparable to the\nweakly-supervised based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dingjie Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McNeil_A/0/1/0/all/0/1\">Andrew McNeil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawant_B/0/1/0/all/0/1\">Benoit M.Dawant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Local-Global Transformer for Image Dehazing. (arXiv:2109.07100v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07100","description":"<p>Recently, the Vision Transformer (ViT) has shown impressive performance on\nhigh-level and low-level vision tasks. In this paper, we propose a new ViT\narchitecture, named Hybrid Local-Global Vision Transformer (HyLoG-ViT), for\nsingle image dehazing. The HyLoG-ViT block consists of two paths, the local ViT\npath and the global ViT path, which are used to capture local and global\ndependencies. The hybrid features are fused via convolution layers. As a\nresult, the HyLoG-ViT reduces the computational complexity and introduces\nlocality in the networks. Then, the HyLoG-ViT blocks are incorporated within\nour dehazing networks, which jointly learn the intrinsic image decomposition\nand image dehazing. Specifically, the network consists of one shared encoder\nand three decoders for reflectance prediction, shading prediction, and\nhaze-free image generation. The tasks of reflectance and shading prediction can\nproduce meaningful intermediate features that can serve as complementary\nfeatures for haze-free image generation. To effectively aggregate the\ncomplementary features, we propose a complementary features selection module\n(CFSM) to select the useful ones for image dehazing. Extensive experiments on\nhomogeneous, non-homogeneous, and nighttime dehazing tasks reveal that our\nproposed Transformer-based dehazing network can achieve comparable or even\nbetter performance than CNNs-based dehazing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Long Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Perspective on Progressive GANs Distillation for One-class Novelty Detection. (arXiv:2109.07295v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07295","description":"<p>One-class novelty detection is conducted to identify anomalous instances,\nwith different distributions from the expected normal instances. In this paper,\nthe Generative Adversarial Network based on the Encoder-Decoder-Encoder scheme\n(EDE-GAN) achieves state-of-the-art performance. The two factors bellow serve\nthe above purpose: 1) The EDE-GAN calculates the distance between two latent\nvectors as the anomaly score, which is unlike the previous methods by utilizing\nthe reconstruction error between images. 2) The model obtains best results when\nthe batch size is set to 1. To illustrate their superiority, we design a new\nGAN architecture, and compare performances according to different batch sizes.\nMoreover, with experimentation leads to discovery, our result implies there is\nalso evidence of just how beneficial constraint on the latent space are when\nengaging in model training. In an attempt to learn compact and fast models, we\npresent a new technology, Progressive Knowledge Distillation with GANs\n(P-KDGAN), which connects two standard GANs through the designed distillation\nloss. Two-step progressive learning continuously augments the performance of\nstudent GANs with improved results over single-step approach. Our experimental\nresults on CIFAR-10, MNIST, and FMNIST datasets illustrate that P-KDGAN\nimproves the performance of the student GAN by 2.44%, 1.77%, and 1.73% when\ncompressing the computationat ratios of 24.45:1, 311.11:1, and 700:1,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hanyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Assignment Distillation for Object Detection. (arXiv:2109.07843v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07843","description":"<p>Knowledge distillation methods are proved to be promising in improving the\nperformance of neural networks and no additional computational expenses are\nrequired during the inference time. For the sake of boosting the accuracy of\nobject detection, a great number of knowledge distillation methods have been\nproposed particularly designed for object detection. However, most of these\nmethods only focus on feature-level distillation and label-level distillation,\nleaving the label assignment step, a unique and paramount procedure for object\ndetection, by the wayside. In this work, we come up with a simple but effective\nknowledge distillation approach focusing on label assignment in object\ndetection, in which the positive and negative samples of student network are\nselected in accordance with the predictions of teacher network. Our method\nshows encouraging results on the MSCOCO2017 benchmark, and can not only be\napplied to both one-stage detectors and two-stage detectors but also be\nutilized orthogonally with other knowledge distillation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Minghao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hailun Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yige Yan</a> (2) ((1) Beijing Institute of Technology, (2) Hohai University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations. (arXiv:2109.07991v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.07991","description":"<p>Multisensory object-centric perception, reasoning, and interaction have been\na key research topic in recent years. However, the progress in these directions\nis limited by the small set of objects available -- synthetic objects are not\nrealistic enough and are mostly centered around geometry, while real object\ndatasets such as YCB are often practically challenging and unstable to acquire\ndue to international shipping, inventory, and financial cost. We present\nObjectFolder, a dataset of 100 virtualized objects that addresses both\nchallenges with two key innovations. First, ObjectFolder encodes the visual,\nauditory, and tactile sensory data for all objects, enabling a number of\nmultisensory object recognition tasks, beyond existing datasets that focus\npurely on object geometry. Second, ObjectFolder employs a uniform,\nobject-centric, and implicit representation for each object's visual textures,\nacoustic simulations, and tactile readings, making the dataset flexible to use\nand easy to share. We demonstrate the usefulness of our dataset as a testbed\nfor multisensory perception and control by evaluating it on a variety of\nbenchmark tasks, including instance recognition, cross-sensory retrieval, 3D\nreconstruction, and robotic grasping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruohan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-Yu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mall_S/0/1/0/all/0/1\">Shivani Mall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Hierarchical Dual Consistency for Semi-Supervised Left Atrium Segmentation on Cross-Domain Data. (arXiv:2109.08311v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.08311","description":"<p>Semi-supervised learning provides great significance in left atrium (LA)\nsegmentation model learning with insufficient labelled data. Generalising\nsemi-supervised learning to cross-domain data is of high importance to further\nimprove model robustness. However, the widely existing distribution difference\nand sample mismatch between different data domains hinder the generalisation of\nsemi-supervised learning. In this study, we alleviate these problems by\nproposing an Adaptive Hierarchical Dual Consistency (AHDC) for the\nsemi-supervised LA segmentation on cross-domain data. The AHDC mainly consists\nof a Bidirectional Adversarial Inference module (BAI) and a Hierarchical Dual\nConsistency learning module (HDC). The BAI overcomes the difference of\ndistributions and the sample mismatch between two different domains. It mainly\nlearns two mapping networks adversarially to obtain two matched domains through\nmutual adaptation. The HDC investigates a hierarchical dual learning paradigm\nfor cross-domain semi-supervised segmentation based on the obtained matched\ndomains. It mainly builds two dual-modelling networks for mining the\ncomplementary information in both intra-domain and inter-domain. For the\nintra-domain learning, a consistency constraint is applied to the\ndual-modelling targets to exploit the complementary modelling information. For\nthe inter-domain learning, a consistency constraint is applied to the LAs\nmodelled by two dual-modelling networks to exploit the complementary knowledge\namong different data domains. We demonstrated the performance of our proposed\nAHDC on four 3D late gadolinium enhancement cardiac MR (LGE-CMR) datasets from\ndifferent centres and a 3D CT dataset. Compared to other state-of-the-art\nmethods, our proposed AHDC achieved higher segmentation accuracy, which\nindicated its capability in the cross-domain semi-supervised LA segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Heye Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohiaddin_R/0/1/0/all/0/1\">Raad Mohiaddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_T/0/1/0/all/0/1\">Tom Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Firmin_D/0/1/0/all/0/1\">David Firmin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keegan_J/0/1/0/all/0/1\">Jennifer Keegan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Neural Architecture Search for Imbalanced Datasets. (arXiv:2109.08580v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08580","description":"<p>Neural Architecture Search (NAS) provides state-of-the-art results when\ntrained on well-curated datasets with annotated labels. However, annotating\ndata or even having balanced number of samples can be a luxury for\npractitioners from different scientific fields, e.g., in the medical domain. To\nthat end, we propose a NAS-based framework that bears the threefold\ncontributions: (a) we focus on the self-supervised scenario, i.e., where no\nlabels are required to determine the architecture, and (b) we assume the\ndatasets are imbalanced, (c) we design each component to be able to run on a\nresource constrained setup, i.e., on a single GPU (e.g. Google Colab). Our\ncomponents build on top of recent developments in self-supervised\nlearning~\\citep{zbontar2021barlow}, self-supervised NAS~\\citep{kaplan2020self}\nand extend them for the case of imbalanced datasets. We conduct experiments on\nan (artificially) imbalanced version of CIFAR-10 and we demonstrate our\nproposed method outperforms standard neural networks, while using $27\\times$\nless parameters. To validate our assumption on a naturally imbalanced dataset,\nwe also conduct experiments on ChestMNIST and COVID-19 X-ray. The results\ndemonstrate how the proposed method can be used in imbalanced datasets, while\nit can be fully run on a single GPU. Code is available\n\\href{https://github.com/TimofeevAlex/ssnas_imbalanced}{here}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Timofeev_A/0/1/0/all/0/1\">Aleksandr Timofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1\">Grigorios G. Chrysos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevher_V/0/1/0/all/0/1\">Volkan Cevher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hebbian Semi-Supervised Learning in a Sample Efficiency Setting. (arXiv:2103.09002v2 [cs.NE] CROSS LISTED)","link":"http://arxiv.org/abs/2103.09002","description":"<p>We propose to address the issue of sample efficiency, in Deep Convolutional\nNeural Networks (DCNN), with a semi-supervised training strategy that combines\nHebbian learning with gradient descent: all internal layers (both convolutional\nand fully connected) are pre-trained using an unsupervised approach based on\nHebbian learning, and the last fully connected layer (the classification layer)\nis trained using Stochastic Gradient Descent (SGD). In fact, as Hebbian\nlearning is an unsupervised learning method, its potential lies in the\npossibility of training the internal layers of a DCNN without labels. Only the\nfinal fully connected layer has to be trained with labeled examples.\n</p>\n<p>We performed experiments on various object recognition datasets, in different\nregimes of sample efficiency, comparing our semi-supervised (Hebbian for\ninternal layers + SGD for the final fully connected layer) approach with\nend-to-end supervised backprop training, and with semi-supervised learning\nbased on Variational Auto-Encoder (VAE). The results show that, in regimes\nwhere the number of available labeled samples is low, our semi-supervised\napproach outperforms the other approaches in almost all the cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lagani_G/0/1/0/all/0/1\">Gabriele Lagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1\">Claudio Gennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1\">Giuseppe Amato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}