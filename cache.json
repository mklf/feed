{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Attention-based Bidirectional LSTM for Deceptive Opinion Spam Classification. (arXiv:2112.14789v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14789","description":"<p>Online Reviews play a vital role in e commerce for decision making. Much of\nthe population makes the decision of which places, restaurant to visit, what to\nbuy and from where to buy based on the reviews posted on the respective\nplatforms. A fraudulent review or opinion spam is categorized as an untruthful\nor deceptive review. Positive reviews of a product or a restaurant helps\nattract customers and thereby lead to an increase in sales whereas negative\nreviews may hamper the progress of a restaurant or sales of a product and\nthereby lead to defamed reputation and loss. Fraudulent reviews are\ndeliberately posted on various online review platforms to trick customers to\nbuy, visit or distract against a product or a restaurant. They are also written\nto commend or discredit the product's repute. The work aims at detecting and\nclassifying the reviews as deceptive or truthful. It involves use of various\ndeep learning techniques for classifying the reviews and an overview of\nproposed approach involving Attention based Bidirectional LSTM to tackle issues\nrelated to semantic information in reviews and a comparative study over\nbaseline machine learning techniques for review classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salunkhe_A/0/1/0/all/0/1\">Ashish Salunkhe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Materialized Knowledge Bases from Commonsense Transformers. (arXiv:2112.14815v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14815","description":"<p>Starting from the COMET methodology by Bosselut et al. (2019), generating\ncommonsense knowledge directly from pre-trained language models has recently\nreceived significant attention. Surprisingly, up to now no materialized\nresource of commonsense knowledge generated this way is publicly available.\nThis paper fills this gap, and uses the materialized resources to perform a\ndetailed analysis of the potential of this approach in terms of precision and\nrecall. Furthermore, we identify common problem cases, and outline use cases\nenabled by materialized resources. We posit that the availability of these\nresources is important for the advancement of the field, as it enables an\noff-the-shelf-use of the resulting knowledge, as well as further analyses on\nits strengths and weaknesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Hierarchical Temporal Memory Theory for Document Categorization. (arXiv:2112.14820v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14820","description":"<p>The current work intends to study the performance of the Hierarchical\nTemporal Memory(HTM) theory for automated classification of text as well as\ndocuments. HTM is a biologically inspired theory based on the working\nprinciples of the human neocortex. The current study intends to provide an\nalternative framework for document categorization using the Spatial Pooler\nlearning algorithm in the HTM Theory. As HTM accepts only a stream of binary\ndata as input, Latent Semantic Indexing(LSI) technique is used for extracting\nthe top features from the input and converting them into binary format. The\nSpatial Pooler algorithm converts the binary input into sparse patterns with\nsimilar input text having overlapping spatial patterns making it easy for\nclassifying the patterns into categories. The results obtained prove that HTM\ntheory, although is in its nascent stages, performs at par with most of the\npopular machine learning based classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Deven Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghate_P/0/1/0/all/0/1\">Pinak Ghate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_M/0/1/0/all/0/1\">Manali Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amit Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QEMind: Alibaba's Submission to the WMT21 Quality Estimation Shared Task. (arXiv:2112.14890v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14890","description":"<p>Quality Estimation, as a crucial step of quality control for machine\ntranslation, has been explored for years. The goal is to investigate automatic\nmethods for estimating the quality of machine translation results without\nreference translations. In this year's WMT QE shared task, we utilize the\nlarge-scale XLM-Roberta pre-trained model and additionally propose several\nuseful features to evaluate the uncertainty of the translations to build our QE\nsystem, named \\textit{QEMind}. The system has been applied to the\nsentence-level scoring task of Direct Assessment and the binary score\nprediction task of Critical Error Detection. In this paper, we present our\nsubmissions to the WMT 2021 QE shared task and an extensive set of experimental\nresults have shown us that our multilingual systems outperform the best system\nin the Direct Assessment QE task of WMT 2020.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RheFrameDetect: A Text Classification System for Automatic Detection of Rhetorical Frames in AI from Open Sources. (arXiv:2112.14933v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14933","description":"<p>Rhetorical Frames in AI can be thought of as expressions that describe AI\ndevelopment as a competition between two or more actors, such as governments or\ncompanies. Examples of such Frames include robotic arms race, AI rivalry,\ntechnological supremacy, cyberwarfare dominance and 5G race. Detection of\nRhetorical Frames from open sources can help us track the attitudes of\ngovernments or companies towards AI, specifically whether attitudes are\nbecoming more cooperative or competitive over time. Given the rapidly\nincreasing volumes of open sources (online news media, twitter, blogs), it is\ndifficult for subject matter experts to identify Rhetorical Frames in (near)\nreal-time. Moreover, these sources are in general unstructured (noisy) and\ntherefore, detecting Frames from these sources will require state-of-the-art\ntext classification techniques. In this paper, we develop RheFrameDetect, a\ntext classification system for (near) real-time capture of Rhetorical Frames\nfrom open sources. Given an input document, RheFrameDetect employs text\nclassification techniques at multiple levels (document level and paragraph\nlevel) to identify all occurrences of Frames used in the discussion of AI. We\nperformed extensive evaluation of the text classification techniques used in\nRheFrameDetect against human annotated Frames from multiple news sources. To\nfurther demonstrate the effectiveness of RheFrameDetect, we show multiple case\nstudies depicting the Frames identified by RheFrameDetect compared against\nhuman annotated Frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saurav Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loustaunau_P/0/1/0/all/0/1\">Philippe Loustaunau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Mixed-Precision Quantization Search of BERT. (arXiv:2112.14938v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14938","description":"<p>Pre-trained language models such as BERT have shown remarkable effectiveness\nin various natural language processing tasks. However, these models usually\ncontain millions of parameters, which prevents them from practical deployment\non resource-constrained devices. Knowledge distillation, Weight pruning, and\nQuantization are known to be the main directions in model compression. However,\ncompact models obtained through knowledge distillation may suffer from\nsignificant accuracy drop even for a relatively small compression ratio. On the\nother hand, there are only a few quantization attempts that are specifically\ndesigned for natural language processing tasks. They suffer from a small\ncompression ratio or a large error rate since manual setting on\nhyper-parameters is required and fine-grained subgroup-wise quantization is not\nsupported. In this paper, we proposed an automatic mixed-precision quantization\nframework designed for BERT that can simultaneously conduct quantization and\npruning in a subgroup-wise level. Specifically, our proposed method leverages\nDifferentiable Neural Architecture Search to assign scale and precision for\nparameters in each sub-group automatically, and at the same time pruning out\nredundant groups of parameters. Extensive evaluations on BERT downstream tasks\nreveal that our proposed method outperforms baselines by providing the same\nperformance with much smaller model size. We also show the feasibility of\nobtaining the extremely light-weight model by combining our solution with\northogonal methods such as DistilBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Changsheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Ting Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Q/0/1/0/all/0/1\">Qian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Matters: Radiology Report Generation with General and Specific Knowledge. (arXiv:2112.15009v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15009","description":"<p>Automatic radiology report generation is critical in clinics which can\nrelieve experienced radiologists from the heavy workload and remind\ninexperienced radiologists of misdiagnosis or missed diagnose. Existing\napproaches mainly formulate radiology report generation as an image captioning\ntask and adopt the encoder-decoder framework. However, in the medical domain,\nsuch pure data-driven approaches suffer from the following problems: 1) visual\nand textual bias problem; 2) lack of expert knowledge. In this paper, we\npropose a knowledge-enhanced radiology report generation approach introduces\ntwo types of medical knowledge: 1) General knowledge, which is input\nindependent and provides the broad knowledge for report generation; 2) Specific\nknowledge, which is input dependent and provides the fine-grained knowledge for\nreport generation. To fully utilize both the general and specific knowledge, we\nalso propose a knowledge-enhanced multi-head attention mechanism. By merging\nthe visual features of the radiology image with general knowledge and specific\nknowledge, the proposed model can improve the quality of generated reports.\nExperimental results on two publicly available datasets IU-Xray and MIMIC-CXR\nshow that the proposed knowledge enhanced approach outperforms state-of-the-art\nimage captioning based methods. Ablation studies also demonstrate that both\ngeneral and specific knowledge can help to improve the performance of radiology\nreport generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shaohua Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment. (arXiv:2112.15011v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15011","description":"<p>In clinics, a radiology report is crucial for guiding a patient's treatment.\nUnfortunately, report writing imposes a heavy burden on radiologists. To\neffectively reduce such a burden, we hereby present an automatic, multi-modal\napproach for report generation from chest x-ray. Our approach, motivated by the\nobservation that the descriptions in radiology reports are highly correlated\nwith the x-ray images, features two distinct modules: (i) Learned knowledge\nbase. To absorb the knowledge embedded in the above-mentioned correlation, we\nautomatically build a knowledge base based on textual embedding. (ii)\nMulti-modal alignment. To promote the semantic alignment among reports, disease\nlabels and images, we explicitly utilize textual embedding to guide the\nlearning of the visual feature space. We evaluate the performance of the\nproposed model using metrics from both natural language generation and clinic\nefficacy on the public IU and MIMIC-CXR datasets. Our ablation study shows that\neach module contributes to improving the quality of generated reports.\nFurthermore, with the aid of both modules, our approach clearly outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xingwang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S.Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YACLC: A Chinese Learner Corpus with Multidimensional Annotation. (arXiv:2112.15043v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15043","description":"<p>Learner corpus collects language data produced by L2 learners, that is second\nor foreign-language learners. This resource is of great relevance for second\nlanguage acquisition research, foreign-language teaching, and automatic\ngrammatical error correction. However, there is little focus on learner corpus\nfor Chinese as Foreign Language (CFL) learners. Therefore, we propose to\nconstruct a large-scale, multidimensional annotated Chinese learner corpus. To\nconstruct the corpus, we first obtain a large number of topic-rich texts\ngenerated by CFL learners. Then we design an annotation scheme including a\nsentence acceptability score as well as grammatical error and fluency-based\ncorrections. We build a crowdsourcing platform to perform the annotation\neffectively (https://yaclc.wenmind.net). We name the corpus YACLC (Yet Another\nChinese Learner Corpus) and release it as part of the CUGE benchmark\n(<a href=\"http://cuge.baai.ac.cn\">this http URL</a>). By analyzing the original sentences and annotations\nin the corpus, we found that YACLC has a considerable size and very high\nannotation quality. We hope this corpus can further enhance the studies on\nChinese International Education and Chinese automatic grammatical error\ncorrection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Cunliang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liner Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaorong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Renfen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does QA-based intermediate training help fine-tuning language models for text classification?. (arXiv:2112.15051v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15051","description":"<p>Fine-tuning pre-trained language models for downstream tasks has become a\nnorm for NLP. Recently it is found that intermediate training based on\nhigh-level inference tasks such as Question Answering (QA) can improve the\nperformance of some language models for target tasks. However it is not clear\nif intermediate training generally benefits various language models. In this\npaper, using the SQuAD-2.0 QA task for intermediate training for target text\nclassification tasks, we experimented on eight tasks for single-sequence\nclassification and eight tasks for sequence-pair classification using two base\nand two compact language models. Our experiments show that QA-based\nintermediate training generates varying transfer performance across different\nlanguage models, except for similar QA tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiuzhen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextRGNN: Residual Graph Neural Networks for Text Classification. (arXiv:2112.15060v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15060","description":"<p>Recently, text classification model based on graph neural network (GNN) has\nattracted more and more attention. Most of these models adopt a similar network\nparadigm, that is, using pre-training node embedding initialization and\ntwo-layer graph convolution. In this work, we propose TextRGNN, an improved GNN\nstructure that introduces residual connection to deepen the convolution network\ndepth. Our structure can obtain a wider node receptive field and effectively\nsuppress the over-smoothing of node features. In addition, we integrate the\nprobabilistic language model into the initialization of graph node embedding,\nso that the non-graph semantic information of can be better extracted. The\nexperimental results show that our model is general and efficient. It can\nsignificantly improve the classification accuracy whether in corpus level or\ntext level, and achieve SOTA performance on a wide range of text classification\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiayuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KIND: an Italian Multi-Domain Dataset for Named Entity Recognition. (arXiv:2112.15099v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15099","description":"<p>In this paper we present KIND, an Italian dataset for Named-Entity\nRecognition. It contains more than one million tokens with the annotation\ncovering three classes: persons, locations, and organizations. Most of the\ndataset (around 600K tokens) contains manual gold annotations in three\ndifferent domains: news, literature, and political discourses. Texts and\nannotations are downloadable for free from the Github repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paccosi_T/0/1/0/all/0/1\">Teresa Paccosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aprosio_A/0/1/0/all/0/1\">Alessio Palmero Aprosio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing Wordnets for Cognate Detection among Indian Languages. (arXiv:2112.15124v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15124","description":"<p>Automatic Cognate Detection (ACD) is a challenging task which has been\nutilized to help NLP applications like Machine Translation, Information\nRetrieval and Computational Phylogenetics. Unidentified cognate pairs can pose\na challenge to these applications and result in a degradation of performance.\nIn this paper, we detect cognate word pairs among ten Indian languages with\nHindi and use deep learning methodologies to predict whether a word pair is\ncognate or not. We identify IndoWordnet as a potential resource to detect\ncognate word pairs based on orthographic similarity-based methods and train\nneural network models using the data obtained from it. We identify parallel\ncorpora as another potential resource and perform the same experiments for\nthem. We also validate the contribution of Wordnets through further\nexperimentation and report improved performance of up to 26%. We discuss the\nnuances of cognate detection among closely related Indian languages and release\nthe lists of detected cognates as a dataset. We also observe the behaviour of,\nto an extent, unrelated Indian language pairs and release the lists of detected\ncognates among them as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1\">Kevin Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_M/0/1/0/all/0/1\">Malhar Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First order linear logic and tensor type calculus for categorial grammars. (arXiv:2112.15253v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15253","description":"<p>We study relationship between first order multiplicative linear logic (MLL1),\nwhich has been known to provide representations to different categorial\ngrammars, and the recently introduced extended tensor type calculus (ETTC). We\nidentify a fragment of MLL1, which seems sufficient for many grammar\nrepresentations, and establish a correspondence between ETTC and this fragment.\nThe system ETTC, thus, can be seen as an alternative syntax and intrinsic\ndeductive system together with a geometric representation for the latter. We\nalso give a natural deduction formulation of ETTC, which might be convenient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slavnov_S/0/1/0/all/0/1\">Sergey Slavnov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNMT: Neural Machine Translation Tookit. (arXiv:2112.15272v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15272","description":"<p>We present an open-source toolkit for neural machine translation (NMT). The\nnew toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along\nwith many other improvements detailed below, in order to create a\nself-contained, simple to use, consistent and comprehensive framework for\nMachine Translation tasks of various domains. It is tooled to support both\nbilingual and multilingual translation tasks, starting from building the model\nfrom respective corpora, to inferring new predictions or packaging the model to\nserving-capable JIT format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quan_N/0/1/0/all/0/1\">Nguyen Hoang Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dat_N/0/1/0/all/0/1\">Nguyen Thanh Dat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_N/0/1/0/all/0/1\">Nguyen Hoang Minh Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Nguyen Van Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Ngo Thi Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_N/0/1/0/all/0/1\">Nguyen Phuong Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viet_T/0/1/0/all/0/1\">Tran Hong Viet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is Event Knowledge Graph: A Survey. (arXiv:2112.15280v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15280","description":"<p>Besides entity-centric knowledge, usually organized as Knowledge Graph (KG),\nevents are also an essential kind of knowledge in the world, which trigger the\nspring up of event-centric knowledge representation form like Event KG (EKG).\nIt plays an increasingly important role in many machine learning and artificial\nintelligence applications, such as intelligent search, question-answering,\nrecommendation, and text generation. This paper provides a comprehensive survey\nof EKG from history, ontology, instance, and application views. Specifically,\nto characterize EKG thoroughly, we focus on its history, definitions, schema\ninduction, acquisition, related representative graphs/systems, and\napplications. The development processes and trends are studied therein. We\nfurther summarize perspective directions to facilitate future research on EKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Saiping Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Long Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fujun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yutao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaolong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation. (arXiv:2112.15283v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15283","description":"<p>Conventional methods for the image-text generation tasks mainly tackle the\nnaturally bidirectional generation tasks separately, focusing on designing\ntask-specific frameworks to improve the quality and fidelity of the generated\nsamples. Recently, Vision-Language Pre-training models have greatly improved\nthe performance of the image-to-text generation tasks, but large-scale\npre-training models for text-to-image synthesis task are still under-developed.\nIn this paper, we propose ERNIE-ViLG, a unified generative pre-training\nframework for bidirectional image-text generation with transformer model. Based\non the image quantization models, we formulate both image generation and text\ngeneration as autoregressive generative tasks conditioned on the text/image\ninput. The bidirectional image-text generative modeling eases the semantic\nalignments across vision and language. For the text-to-image generation\nprocess, we further propose an end-to-end training method to jointly learn the\nvisual sequence generator and the image reconstructor. To explore the landscape\nof large-scale pre-training for bidirectional text-image generation, we train a\n10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million\n(Chinese) image-text pairs which achieves state-of-the-art performance for both\ntext-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for\ntext-to-image synthesis and best results on COCO-CN and AIC-ICC for image\ncaptioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Weichong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yewei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lanxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1\">Boqiang Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhihua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation with Category Attention Network for Deep Sentiment Analysis. (arXiv:2112.15290v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15290","description":"<p>Domain adaptation tasks such as cross-domain sentiment classification aim to\nutilize existing labeled data in the source domain and unlabeled or few labeled\ndata in the target domain to improve the performance in the target domain via\nreducing the shift between the data distributions. Existing cross-domain\nsentiment classification methods need to distinguish pivots, i.e., the\ndomain-shared sentiment words, and non-pivots, i.e., the domain-specific\nsentiment words, for excellent adaptation performance. In this paper, we first\ndesign a Category Attention Network (CAN), and then propose a model named\nCAN-CNN to integrate CAN and a Convolutional Neural Network (CNN). On the one\nhand, the model regards pivots and non-pivots as unified category attribute\nwords and can automatically capture them to improve the domain adaptation\nperformance; on the other hand, the model makes an attempt at interpretability\nto learn the transferred category attribute words. Specifically, the\noptimization objective of our model has three different components: 1) the\nsupervised classification loss; 2) the distributions loss of category feature\nweights; 3) the domain invariance loss. Finally, the proposed model is\nevaluated on three public sentiment analysis datasets and the results\ndemonstrate that CAN-CNN can outperform other various baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dongbo Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Ganbin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaohu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deconfounded Visual Grounding. (arXiv:2112.15324v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15324","description":"<p>We focus on the confounding bias between language and location in the visual\ngrounding pipeline, where we find that the bias is the major visual reasoning\nbottleneck. For example, the grounding process is usually a trivial\nlanguage-location association without visual reasoning, e.g., grounding any\nlanguage query containing sheep to the nearly central regions, due to that most\nqueries about sheep have ground-truth locations at the image center. First, we\nframe the visual grounding pipeline into a causal graph, which shows the\ncausalities among image, query, target location and underlying confounder.\nThrough the causal graph, we know how to break the grounding bottleneck:\ndeconfounded visual grounding. Second, to tackle the challenge that the\nconfounder is unobserved in general, we propose a confounder-agnostic approach\ncalled: Referring Expression Deconfounder (RED), to remove the confounding\nbias. Third, we implement RED as a simple language attention, which can be\napplied in any grounding method. On popular benchmarks, RED improves various\nstate-of-the-art grounding methods by a significant margin. Code will soon be\navailable at: https://github.com/JianqiangH/Deconfounded_VG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiaxin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Graph-Aware Reinforcement Learning to Identify Winning Strategies in Diplomacy Games (Student Abstract). (arXiv:2112.15331v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15331","description":"<p>This abstract proposes an approach towards goal-oriented modeling of the\ndetection and modeling complex social phenomena in multiparty discourse in an\nonline political strategy game. We developed a two-tier approach that first\nencodes sociolinguistic behavior as linguistic features then use reinforcement\nlearning to estimate the advantage afforded to any player. In the first tier,\nsociolinguistic behavior, such as Friendship and Reasoning, that speakers use\nto influence others are encoded as linguistic features to identify the\npersuasive strategies applied by each player in simultaneous two-party\ndialogues. In the second tier, a reinforcement learning approach is used to\nestimate a graph-aware reward function to quantify the advantage afforded to\neach player based on their standing in this multiparty setup. We apply this\ntechnique to the game Diplomacy, using a dataset comprising of over 15,000\nmessages exchanged between 78 users. Our graph-aware approach shows robust\nperformance compared to a context-agnostic setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_H/0/1/0/all/0/1\">Hansin Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Hui Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaidka_K/0/1/0/all/0/1\">Kokil Jaidka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering Vietnamese Conversations From Facebook Page To Build Training Dataset For Chatbot. (arXiv:2112.15338v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15338","description":"<p>The biggest challenge of building chatbots is training data. The required\ndata must be realistic and large enough to train chatbots. We create a tool to\nget actual training data from Facebook messenger of a Facebook page. After text\npreprocessing steps, the newly obtained dataset generates FVnC and Sample\ndataset. We use the Retraining of BERT for Vietnamese (PhoBERT) to extract\nfeatures of our text data. K-Means and DBSCAN clustering algorithms are used\nfor clustering tasks based on output embeddings from PhoBERT$_{base}$. We apply\nV-measure score and Silhouette score to evaluate the performance of clustering\nalgorithms. We also demonstrate the efficiency of PhoBERT compared to other\nmodels in feature extraction on Sample dataset. A GridSearch algorithm that\ncombines both clustering evaluations is also proposed to find optimal\nparameters. Thanks to clustering such a number of conversations, we save a lot\nof time and effort to build data and storylines for training chatbot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Trieu Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thi-Kim-Ngoan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Thi-Hong-Minh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Quynh-Chau Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenQA: Hybrid QA System Relying on Structured Knowledge Base as well as Non-structured Data. (arXiv:2112.15356v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15356","description":"<p>Search engines based on keyword retrieval can no longer adapt to the way of\ninformation acquisition in the era of intelligent Internet of Things due to the\nreturn of keyword related Internet pages. How to quickly, accurately and\neffectively obtain the information needed by users from massive Internet data\nhas become one of the key issues urgently needed to be solved. We propose an\nintelligent question-answering system based on structured KB and unstructured\ndata, called OpenQA, in which users can give query questions and the model can\nquickly give accurate answers back to users. We integrate KBQA structured\nquestion answering based on semantic parsing and deep representation learning,\nand two-stage unstructured question answering based on retrieval and neural\nmachine reading comprehension into OpenQA, and return the final answer with the\nhighest probability through the Transformer answer selection module in OpenQA.\nWe carry out preliminary experiments on our constructed dataset, and the\nexperimental results prove the effectiveness of the proposed intelligent\nquestion answering system. At the same time, the core technology of each module\nof OpenQA platform is still in the forefront of academic hot spots, and the\ntheoretical essence and enrichment of OpenQA will be further explored based on\nthese academic hot spots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gaochen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypers at ComMA@ICON: Modelling Aggressiveness, Gender Bias and Communal Bias Identification. (arXiv:2112.15417v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15417","description":"<p>Due to the exponentially increasing reach of social media, it is essential to\nfocus on its negative aspects as it can potentially divide society and incite\npeople into violence. In this paper, we present our system description of work\non the shared task ComMA@ICON, where we have to classify how aggressive the\nsentence is and if the sentence is gender-biased or communal biased. These\nthree could be the primary reasons to cause significant problems in society. As\nteam Hypers we have proposed an approach that utilizes different pretrained\nmodels with Attention and mean pooling methods. We were able to get Rank 3 with\n0.223 Instance F1 score on Bengali, Rank 2 with 0.322 Instance F1 score on\nMulti-lingual set, Rank 4 with 0.129 Instance F1 score on Meitei and Rank 5\nwith 0.336 Instance F1 score on Hindi. The source code and the pretrained\nmodels of this work can be found here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_R/0/1/0/all/0/1\">Roshan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navaneethakrishnan_S/0/1/0/all/0/1\">Subalalitha Chinnaudayar Navaneethakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi6_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi6</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Using Gaze Behaviour for Natural Language Processing. (arXiv:2112.15471v1 [cs.CL])","link":"http://arxiv.org/abs/2112.15471","description":"<p>Gaze behaviour has been used as a way to gather cognitive information for a\nnumber of years. In this paper, we discuss the use of gaze behaviour in solving\ndifferent tasks in natural language processing (NLP) without having to record\nit at test time. This is because the collection of gaze behaviour is a costly\ntask, both in terms of time and money. Hence, in this paper, we focus on\nresearch done to alleviate the need for recording gaze behaviour at run time.\nWe also mention different eye tracking corpora in multiple languages, which are\ncurrently available and can be used in natural language processing. We conclude\nour paper by discussing applications in a domain - education - and how learning\ngaze behaviour can help in solving the tasks of complex word identification and\nautomatic essay grading.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathias_S/0/1/0/all/0/1\">Sandeep Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Abhijit Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training and Generating Neural Networks in Compressed Weight Space. (arXiv:2112.15545v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15545","description":"<p>The inputs and/or outputs of some neural nets are weight matrices of other\nneural nets. Indirect encodings or end-to-end compression of weight matrices\ncould help to scale such approaches. Our goal is to open a discussion on this\ntopic, starting with recurrent neural networks for character-level language\nmodelling whose weight matrices are encoded by the discrete cosine transform.\nOur fast weight version thereof uses a recurrent neural network to parameterise\nthe compressed weights. We present experimental results on the enwik8 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1\">Kazuki Irie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTphone: Phonetically-Aware Encoder Representations for Utterance-Level Speaker and Language Recognition. (arXiv:1907.00457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1907.00457","description":"<p>We introduce BERTphone, a Transformer encoder trained on large speech corpora\nthat outputs phonetically-aware contextual representation vectors that can be\nused for both speaker and language recognition. This is accomplished by\ntraining on two objectives: the first, inspired by adapting BERT to the\ncontinuous domain, involves masking spans of input frames and reconstructing\nthe whole sequence for acoustic representation learning; the second, inspired\nby the success of bottleneck features from ASR, is a sequence-level CTC loss\napplied to phoneme labels for phonetic representation learning. We pretrain two\nBERTphone models (one on Fisher and one on TED-LIUM) and use them as feature\nextractors into x-vector-style DNNs for both tasks. We attain a\nstate-of-the-art $C_{\\text{avg}}$ of 6.16 on the challenging LRE07 3sec\nclosed-set language recognition task. On Fisher and VoxCeleb speaker\nrecognition tasks, we see an 18% relative reduction in speaker EER when\ntraining on BERTphone vectors instead of MFCCs. In general, BERTphone\noutperforms previous phonetic pretraining approaches on the same data. We\nrelease our code and models at\nhttps://github.com/awslabs/speech-representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1\">Shaoshi Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_J/0/1/0/all/0/1\">Julian Salazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuzong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refining Language Models with Compositional Explanations. (arXiv:2103.10415v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.10415","description":"<p>Pre-trained language models have been successful on text classification\ntasks, but are prone to learning spurious correlations from biased datasets,\nand are thus vulnerable when making inferences in a new domain. Prior work\nreveals such spurious patterns via post-hoc explanation algorithms which\ncompute the importance of input features. Further, the model is regularized to\nalign the importance scores with human knowledge, so that the unintended model\nbehaviors are eliminated. However, such a regularization technique lacks\nflexibility and coverage, since only importance scores towards a pre-defined\nlist of features are adjusted, while more complex human knowledge such as\nfeature interaction and pattern generalization can hardly be incorporated. In\nthis work, we propose to refine a learned language model for a target domain by\ncollecting human-provided compositional explanations regarding observed biases.\nBy parsing these explanations into executable logic rules, the human-specified\nrefinement advice from a small set of explanations can be generalized to more\ntraining examples. We additionally introduce a regularization term allowing\nadjustments for both importance and interaction of features to better rectify\nmodel behavior. We demonstrate the effectiveness of the proposed approach on\ntwo text classification tasks by showing improved performance in target domain\nas well as improved model fairness after refinement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huihan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Emotion and Reasoning its Flip in Multi-Party Conversations using Masked Memory Network and Transformer. (arXiv:2103.12360v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12360","description":"<p>Efficient discovery of a speaker's emotional states in a multi-party\nconversation is significant to design human-like conversational agents. During\na conversation, the cognitive state of a speaker often alters due to certain\npast utterances, which may lead to a flip in their emotional state. Therefore,\ndiscovering the reasons (triggers) behind the speaker's emotion-flip during a\nconversation is essential to explain the emotion labels of individual\nutterances. In this paper, along with addressing the task of emotion\nrecognition in conversations (ERC), we introduce a novel task - Emotion-Flip\nReasoning (EFR), that aims to identify past utterances which have triggered\none's emotional state to flip at a certain time. We propose a masked memory\nnetwork to address the former and a Transformer-based network for the latter\ntask. To this end, we consider MELD, a benchmark emotion recognition dataset in\nmulti-party conversations for the task of ERC, and augment it with new\nground-truth labels for EFR. An extensive comparison with five state-of-the-art\nmodels suggests improved performances of our models for both tasks. We further\npresent anecdotal evidence and both qualitative and quantitative error analyses\nto support the superiority of our models compared to the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shivani Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrimal_A/0/1/0/all/0/1\">Anubhav Shrimal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full-Sentence Models Perform Better in Simultaneous Translation Using the Information Enhanced Decoding Strategy. (arXiv:2105.01893v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01893","description":"<p>Simultaneous translation, which starts translating each sentence after\nreceiving only a few words in source sentence, has a vital role in many\nscenarios. Although the previous prefix-to-prefix framework is considered\nsuitable for simultaneous translation and achieves good performance, it still\nhas two inevitable drawbacks: the high computational resource costs caused by\nthe need to train a separate model for each latency $k$ and the insufficient\nability to encode information because each target token can only attend to a\nspecific source prefix. We propose a novel framework that adopts a simple but\neffective decoding strategy which is designed for full-sentence models. Within\nthis framework, training a single full-sentence model can achieve arbitrary\ngiven latency and save computational resources. Besides, with the competence of\nthe full-sentence model to encode the whole sentence, our decoding strategy can\nenhance the information maintained in the decoded states in real time.\nExperimental results show that our method achieves better translation quality\nthan baselines on 4 directions: Zh$\\rightarrow$En, En$\\rightarrow$Ro and\nEn$\\leftrightarrow$De.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengxin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing an Automatic Agent for Repeated Language based Persuasion Games. (arXiv:2105.04976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.04976","description":"<p>Persuasion games are fundamental in economics and AI research and serve as\nthe basis for important applications. However, work on this setup assumes\ncommunication with stylized messages that do not consist of rich human\nlanguage. In this paper we consider a repeated sender (expert) -- receiver\n(decision maker) game, where the sender is fully informed about the state of\nthe world and aims to persuade the receiver to accept a deal by sending one of\nseveral possible natural language reviews. We design an automatic expert that\nplays this repeated game, aiming to achieve the maximal payoff. Our expert is\nimplemented within the Monte Carlo Tree Search (MCTS) algorithm, with deep\nlearning models that exploit behavioral and linguistic signals in order to\npredict the next action of the decision maker, and the future payoff of the\nexpert given the state of the game and a candidate review. We demonstrate the\nsuperiority of our expert over strong baselines, its adaptability to different\ndecision makers, and that its selected reviews are nicely adapted to the\nproposed deal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raifer_M/0/1/0/all/0/1\">Maya Raifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rotman_G/0/1/0/all/0/1\">Guy Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apel_R/0/1/0/all/0/1\">Reut Apel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_M/0/1/0/all/0/1\">Moshe Tennenholtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition. (arXiv:2106.05642v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2106.05642","description":"<p>The unified streaming and non-streaming two-pass (U2) end-to-end model for\nspeech recognition has shown great performance in terms of streaming\ncapability, accuracy, real-time factor (RTF), and latency. In this paper, we\npresent U2++, an enhanced version of U2 to further improve the accuracy. The\ncore idea of U2++ is to use the forward and the backward information of the\nlabeling sequences at the same time at training to learn richer information,\nand combine the forward and backward prediction at decoding to give more\naccurate recognition results. We also proposed a new data augmentation method\ncalled SpecSub to help the U2++ model to be more accurate and robust. Our\nexperiments show that, compared with U2, U2++ shows faster convergence at\ntraining, better robustness to the decoding method, as well as consistent 5\\% -\n8\\% word error rate reduction gain over U2. On the experiment of AISHELL-1, we\nachieve a 4.63\\% character error rate (CER) with a non-streaming setup and\n5.05\\% with a streaming setup with 320ms latency by U2++. To the best of our\nknowledge, 5.05\\% is the best-published streaming result on the AISHELL-1 test\nset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wenjing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xin Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Training and Decoding For End-to-end Speech Recognition Using Lattice-free MMI. (arXiv:2112.02498v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2112.02498","description":"<p>Recently, End-to-End (E2E) frameworks have achieved remarkable results on\nvarious Automatic Speech Recognition (ASR) tasks. However, Lattice-Free Maximum\nMutual Information (LF-MMI), as one of the discriminative training criteria\nthat show superior performance in hybrid ASR systems, is rarely adopted in E2E\nASR frameworks. In this work, we propose a novel approach to integrate LF-MMI\ncriterion into E2E ASR frameworks in both training and decoding stages. The\nproposed approach shows its effectiveness on two of the most widely used E2E\nframeworks including Attention-Based Encoder-Decoders (AEDs) and Neural\nTransducers (NTs). Experiments suggest that the introduction of the LF-MMI\ncriterion consistently leads to significant performance improvements on various\ndatasets and different E2E ASR frameworks. The best of our models achieves\ncompetitive CER of 4.1\\% / 4.4\\% on Aishell-1 dev/test set; we also achieve\nsignificant error reduction on Aishell-2 and Librispeech datasets over strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jinchuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chao Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diformer: Directional Transformer for Neural Machine Translation. (arXiv:2112.11632v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11632","description":"<p>Autoregressive (AR) and Non-autoregressive (NAR) models have their own\nsuperiority on the performance and latency, combining them into one model may\ntake advantage of both. Current combination frameworks focus more on the\nintegration of multiple decoding paradigms with a unified generative model,\ne.g. Masked Language Model. However, the generalization can be harmful to the\nperformance due to the gap between training objective and inference. In this\npaper, we aim to close the gap by preserving the original objective of AR and\nNAR under a unified framework. Specifically, we propose the Directional\nTransformer (Diformer) by jointly modelling AR and NAR into three generation\ndirections (left-to-right, right-to-left and straight) with a newly introduced\ndirection variable, which works by controlling the prediction of each token to\nhave specific dependencies under that direction. The unification achieved by\ndirection successfully preserves the original dependency assumption used in AR\nand NAR, retaining both generalization and performance. Experiments on 4 WMT\nbenchmarks demonstrate that Diformer outperforms current united-modelling works\nwith more than 1.5 BLEU points for both AR and NAR decoding, and is also\ncompetitive to the state-of-the-art independent AR and NAR models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Daimeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1\">Hengchao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinglu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shimin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Video Reconstruction from a Single Motion Blurred Image using Learned Dynamic Phase Coding. (arXiv:2112.14768v1 [eess.IV])","link":"http://arxiv.org/abs/2112.14768","description":"<p>Video reconstruction from a single motion-blurred image is a challenging\nproblem, which can enhance existing cameras' capabilities. Recently, several\nworks addressed this task using conventional imaging and deep learning. Yet,\nsuch purely-digital methods are inherently limited, due to direction ambiguity\nand noise sensitivity. Some works proposed to address these limitations using\nnon-conventional image sensors, however, such sensors are extremely rare and\nexpensive. To circumvent these limitations with simpler means, we propose a\nhybrid optical-digital method for video reconstruction that requires only\nsimple modifications to existing optical systems. We use a learned dynamic\nphase-coding in the lens aperture during the image acquisition to encode the\nmotion trajectories, which serve as prior information for the video\nreconstruction process. The proposed computational camera generates a sharp\nframe burst of the scene at various frame rates from a single coded\nmotion-blurred image, using an image-to-video convolutional neural network. We\npresent advantages and improved performance compared to existing methods, using\nboth simulations and a real-world camera prototype.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yosef_E/0/1/0/all/0/1\">Erez Yosef</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elmalem_S/0/1/0/all/0/1\">Shay Elmalem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Graph Clustering via Dual Correlation Reduction. (arXiv:2112.14772v1 [cs.LG])","link":"http://arxiv.org/abs/2112.14772","description":"<p>Deep graph clustering, which aims to reveal the underlying graph structure\nand divide the nodes into different groups, has attracted intensive attention\nin recent years. However, we observe that, in the process of node encoding,\nexisting methods suffer from representation collapse which tends to map all\ndata into the same representation. Consequently, the discriminative capability\nof the node representation is limited, leading to unsatisfied clustering\nperformance. To address this issue, we propose a novel self-supervised deep\ngraph clustering method termed Dual Correlation Reduction Network (DCRN) by\nreducing information correlation in a dual manner. Specifically, in our method,\nwe first design a siamese network to encode samples. Then by forcing the\ncross-view sample correlation matrix and cross-view feature correlation matrix\nto approximate two identity matrices, respectively, we reduce the information\ncorrelation in the dual-level, thus improving the discriminative capability of\nthe resulting features. Moreover, in order to alleviate representation collapse\ncaused by over-smoothing in GCN, we introduce a propagation regularization term\nto enable the network to gain long-distance information with the shallow\nnetwork structure. Extensive experimental results on six benchmark datasets\ndemonstrate the effectiveness of the proposed DCRN against the existing\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linxuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xihong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">En Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning meets Liveness Detection: Recent Advancements and Challenges. (arXiv:2112.14796v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14796","description":"<p>Facial biometrics has been recently received tremendous attention as a\nconvenient replacement for traditional authentication systems. Consequently,\ndetecting malicious attempts has found great significance, leading to extensive\nstudies in face anti-spoofing~(FAS),i.e., face presentation attack detection.\nDeep feature learning and techniques, as opposed to hand-crafted features, have\npromised a dramatic increase in the FAS systems' accuracy, tackling the key\nchallenges of materializing the real-world application of such systems. Hence,\na new research area dealing with the development of more generalized as well as\naccurate models is increasingly attracting the attention of the research\ncommunity and industry. In this paper, we present a comprehensive survey on the\nliterature related to deep-feature-based FAS methods since 2017. To shed light\non this topic, a semantic taxonomy based on various features and learning\nmethodologies is represented. Further, we cover predominant public datasets for\nFAS in chronological order, their evolutional progress, and the evaluation\ncriteria (both intra-dataset and inter-dataset). Finally, we discuss the open\nresearch challenges and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabaghi_A/0/1/0/all/0/1\">Arian Sabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oghbaie_M/0/1/0/all/0/1\">Marzieh Oghbaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemifard_K/0/1/0/all/0/1\">Kooshan Hashemifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Inception Attention for Image Synthesis and Image Recognition. (arXiv:2112.14804v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14804","description":"<p>Image synthesis and image recognition have witnessed remarkable progress, but\noften at the expense of computationally expensive training and inference.\nLearning lightweight yet expressive deep model has emerged as an important and\ninteresting direction. Inspired by the well-known split-transform-aggregate\ndesign heuristic in the Inception building block, this paper proposes a\nSkip-Layer Inception Module (SLIM) that facilitates efficient learning of image\nsynthesis models, and a same-layer variant (dubbed as SLIM too) as a stronger\nalternative to the well-known ResNeXts for image recognition. In SLIM, the\ninput feature map is first split into a number of groups (e.g., 4).Each group\nis then transformed to a latent style vector(via channel-wise attention) and a\nlatent spatial mask (via spatial attention). The learned latent masks and\nlatent style vectors are aggregated to modulate the target feature map. For\ngenerative learning, SLIM is built on a recently proposed lightweight\nGenerative Adversarial Networks (i.e., FastGANs) which present a skip-layer\nexcitation(SLE) module. For few-shot image synthesis tasks, the proposed SLIM\nachieves better performance than the SLE work and other related methods. For\none-shot image synthesis tasks, it shows stronger capability of preserving\nimages structures than prior arts such as the SinGANs. For image classification\ntasks, the proposed SLIM is used as a drop-in replacement for convolution\nlayers in ResNets (resulting in ResNeXt-like models) and achieves better\naccuracy in theImageNet-1000 dataset, with significantly smaller model\ncomplexity\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianghao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Backdoor Defense Using Shapley Estimation. (arXiv:2112.14889v1 [cs.CR])","link":"http://arxiv.org/abs/2112.14889","description":"<p>Deep neural networks have achieved impressive performance in a variety of\ntasks over the last decade, such as autonomous driving, face recognition, and\nmedical diagnosis. However, prior works show that deep neural networks are\neasily manipulated into specific, attacker-decided behaviors in the inference\nstage by backdoor attacks which inject malicious small hidden triggers into\nmodel training, raising serious security threats. To determine the triggered\nneurons and protect against backdoor attacks, we exploit Shapley value and\ndevelop a new approach called Shapley Pruning (ShapPruning) that successfully\nmitigates backdoor attacks from models in a data-insufficient situation (1\nimage per class or even free of data). Considering the interaction between\nneurons, ShapPruning identifies the few infected neurons (under 1% of all\nneurons) and manages to protect the model's structure and accuracy after\npruning as many infected neurons as possible. To accelerate ShapPruning, we\nfurther propose discarding threshold and $\\epsilon$-greedy strategy to\naccelerate Shapley estimation, making it possible to repair poisoned models\nwith only several minutes. Experiments demonstrate the effectiveness and\nrobustness of our method against various attacks and tasks compared to existing\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jiyang Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuozhuo Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Generation and Hypothesis Verification for Reliable Face Anti-Spoofing. (arXiv:2112.14894v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14894","description":"<p>Although existing face anti-spoofing (FAS) methods achieve high accuracy in\nintra-domain experiments, their effects drop severely in cross-domain scenarios\nbecause of poor generalization. Recently, multifarious techniques have been\nexplored, such as domain generalization and representation disentanglement.\nHowever, the improvement is still limited by two issues: 1) It is difficult to\nperfectly map all faces to a shared feature space. If faces from unknown\ndomains are not mapped to the known region in the shared feature space,\naccidentally inaccurate predictions will be obtained. 2) It is hard to\ncompletely consider various spoof traces for disentanglement. In this paper, we\npropose a Feature Generation and Hypothesis Verification framework to alleviate\nthe two issues. Above all, feature generation networks which generate\nhypotheses of real faces and known attacks are introduced for the first time in\nthe FAS task. Subsequently, two hypothesis verification modules are applied to\njudge whether the input face comes from the real-face space and the real-face\ndistribution respectively. Furthermore, some analyses of the relationship\nbetween our framework and Bayesian uncertainty estimation are given, which\nprovides theoretical support for reliable defense in unknown domains.\nExperimental results show our framework achieves promising results and\noutperforms the state-of-the-art approaches on extensive public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shice Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shitao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieving Black-box Optimal Images from External Databases. (arXiv:2112.14921v1 [cs.IR])","link":"http://arxiv.org/abs/2112.14921","description":"<p>Suppose we have a black-box function (e.g., deep neural network) that takes\nan image as input and outputs a value that indicates preference. How can we\nretrieve optimal images with respect to this function from an external database\non the Internet? Standard retrieval problems in the literature (e.g., item\nrecommendations) assume that an algorithm has full access to the set of items.\nIn other words, such algorithms are designed for service providers. In this\npaper, we consider the retrieval problem under different assumptions.\nSpecifically, we consider how users with limited access to an image database\ncan retrieve images using their own black-box functions. This formulation\nenables a flexible and finer-grained image search defined by each user. We\nassume the user can access the database through a search query with tight API\nlimits. Therefore, a user needs to efficiently retrieve optimal images in terms\nof the number of queries. We propose an efficient retrieval algorithm Tiara for\nthis problem. In the experiments, we confirm that our proposed method performs\nbetter than several baselines under various settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Depth Estimation from Multiple 360-degree Images Using Virtual Depth. (arXiv:2112.14931v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14931","description":"<p>In this paper, we propose a dense depth estimation pipeline for multiview\n360\\degree\\: images. The proposed pipeline leverages a spherical camera model\nthat compensates for radial distortion in 360\\degree\\: images. The key\ncontribution of this paper is the extension of a spherical camera model to\nmultiview by introducing a translation scaling scheme. Moreover, we propose an\neffective dense depth estimation method by setting virtual depth and minimizing\nphotonic reprojection error. We validate the performance of the proposed\npipeline using the images of natural scenes as well as the synthesized dataset\nfor quantitive evaluation. The experimental results verify that the proposed\npipeline improves estimation accuracy compared to the current state-of-art\ndense depth estimation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongyeop Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeejin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SFU-HW-Tracks-v1: Object Tracking Dataset on Raw Video Sequences. (arXiv:2112.14934v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14934","description":"<p>We present a dataset that contains object annotations with unique object\nidentities (IDs) for the High Efficiency Video Coding (HEVC) v1 Common Test\nConditions (CTC) sequences. Ground-truth annotations for 13 sequences were\nprepared and released as the dataset called SFU-HW-Tracks-v1. For each video\nframe, ground truth annotations include object class ID, object ID, and\nbounding box location and its dimensions. The dataset can be used to evaluate\nobject tracking performance on uncompressed video sequences and study the\nrelationship between video compression and object tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Takehiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyomin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1\">Ivan V. Baji&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Generator with Auxiliary Branch for Improving GAN Performance. (arXiv:2112.14968v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14968","description":"<p>The generator in the generative adversarial network (GAN) learns image\ngeneration in a coarse-to-fine manner in which earlier layers learn an overall\nstructure of the image and the latter ones refine the details. To propagate the\ncoarse information well, recent works usually build their generators by\nstacking up multiple residual blocks. Although the residual block can produce\nthe high-quality image as well as be trained stably, it often impedes the\ninformation flow in the network. To alleviate this problem, this brief\nintroduces a novel generator architecture that produces the image by combining\nfeatures obtained through two different branches: the main and auxiliary\nbranches. The goal of the main branch is to produce the image by passing\nthrough the multiple residual blocks, whereas the auxiliary branch is to convey\nthe coarse information in the earlier layer to the later one. To combine the\nfeatures in the main and auxiliary branches successfully, we also propose a\ngated feature fusion module that controls the information flow in those\nbranches. To prove the superiority of the proposed method, this brief provides\nextensive experiments using various standard datasets including CIFAR-10,\nCIFAR-100, LSUN, CelebA-HQ, AFHQ, and tiny- ImageNet. Furthermore, we conducted\nvarious ablation studies to demonstrate the generalization ability of the\nproposed method. Quantitative evaluations prove that the proposed method\nexhibits impressive GAN performance in terms of Inception score (IS) and\nFrechet inception distance (FID). For instance, the proposed method boosts the\nFID and IS scores on the tiny-ImageNet dataset from 35.13 to 25.00 and 20.23 to\n25.57, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yong-Goo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Fine-grained Class Clustering via Generative Adversarial Networks. (arXiv:2112.14971v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14971","description":"<p>Unsupervised fine-grained class clustering is practical yet challenging task\ndue to the difficulty of feature representations learning of subtle object\ndetails. We introduce C3-GAN, a method that leverages the categorical inference\npower of InfoGAN by applying contrastive learning. We aim to learn feature\nrepresentations that encourage the data to form distinct cluster boundaries in\nthe embedding space, while also maximizing the mutual information between the\nlatent code and its observation. Our approach is to train the discriminator,\nwhich is used for inferring clusters, to optimize the contrastive loss, where\nthe image-latent pairs that maximize the mutual information are considered as\npositive pairs and the rest as negative pairs. Specifically, we map the input\nof the generator, which has sampled from the categorical distribution, to the\nembedding space of the discriminator and let them act as a cluster centroid. In\nthis way, C3-GAN achieved to learn a clustering-friendly embedding space where\neach cluster is distinctively separable. Experimental results show that C3-GAN\nachieved state-of-the-art clustering performance on four fine-grained benchmark\ndatasets, while also alleviating the mode collapse phenomenon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning of Semantic and Visual Representations for Text Tracking. (arXiv:2112.14976v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14976","description":"<p>Semantic representation is of great benefit to the video text tracking(VTT)\ntask that requires simultaneously classifying, detecting, and tracking texts in\nthe video. Most existing approaches tackle this task by appearance similarity\nin continuous frames, while ignoring the abundant semantic features. In this\npaper, we explore to robustly track video text with contrastive learning of\nsemantic and visual representations. Correspondingly, we present an end-to-end\nvideo text tracker with Semantic and Visual Representations(SVRep), which\ndetects and tracks texts by exploiting the visual and semantic relationships\nbetween different texts in a video sequence. Besides, with a light-weight\narchitecture, SVRep achieves state-of-the-art performance while maintaining\ncompetitive inference speed. Specifically, with a backbone of ResNet-18, SVRep\nachieves an ${\\rm ID_{F1}}$ of $\\textbf{65.9\\%}$, running at $\\textbf{16.7}$\nFPS, on the ICDAR2015(video) dataset with $\\textbf{8.6\\%}$ improvement than the\nprevious state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Size Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the pattern of Emotion in children with ASD as an early biomarker through Recurring-Convolution Neural Network (R-CNN). (arXiv:2112.14983v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14983","description":"<p>Autism Spectrum Disorder (ASD) is found to be a major concern among various\noccupational therapists. The foremost challenge of this neurodevelopmental\ndisorder lies in the fact of analyzing and exploring various symptoms of the\nchildren at their early stage of development. Such early identification could\nprop up the therapists and clinicians to provide proper assistive support to\nmake the children lead an independent life. Facial expressions and emotions\nperceived by the children could contribute to such early intervention of\nautism. In this regard, the paper implements in identifying basic facial\nexpression and exploring their emotions upon a time variant factor. The\nemotions are analyzed by incorporating the facial expression identified through\nCNN using 68 landmark points plotted on the frontal face with a prediction\nnetwork formed by RNN known as RCNN-FER system. The paper adopts R-CNN to take\nthe advantage of increased accuracy and performance with decreased time\ncomplexity in predicting emotion as a textual network analysis. The papers\nproves better accuracy in identifying the emotion in autistic children when\ncompared over simple machine learning models built for such identifications\ncontributing to autistic society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+P_A/0/1/0/all/0/1\">Abirami S P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_K/0/1/0/all/0/1\">Kousalya G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_K/0/1/0/all/0/1\">Karthick R</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THE Benchmark: Transferable Representation Learning for Monocular Height Estimation. (arXiv:2112.14985v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14985","description":"<p>Generating 3D city models rapidly is crucial for many applications. Monocular\nheight estimation is one of the most efficient and timely ways to obtain\nlarge-scale geometric information. However, existing works focus primarily on\ntraining and testing models using unbiased datasets, which don't align well\nwith real-world applications. Therefore, we propose a new benchmark dataset to\nstudy the transferability of height estimation models in a cross-dataset\nsetting. To this end, we first design and construct a large-scale benchmark\ndataset for cross-dataset transfer learning on the height estimation task. This\nbenchmark dataset includes a newly proposed large-scale synthetic dataset, a\nnewly collected real-world dataset, and four existing datasets from different\ncities. Next, two new experimental protocols, zero-shot and few-shot\ncross-dataset transfer, are designed. For few-shot cross-dataset transfer, we\nenhance the window-based Transformer with the proposed scale-deformable\nconvolution module to handle the severe scale-variation problem. To improve the\ngeneralizability of deep models in the zero-shot cross-dataset setting, a\nmax-normalization-based Transformer network is designed to decouple the\nrelative height map from the absolute heights. Experimental results have\ndemonstrated the effectiveness of the proposed methods in both the traditional\nand cross-dataset transfer settings. The datasets and codes are publicly\navailable at https://thebenchmarkh.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhitong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingtao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yilei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Matters: Radiology Report Generation with General and Specific Knowledge. (arXiv:2112.15009v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15009","description":"<p>Automatic radiology report generation is critical in clinics which can\nrelieve experienced radiologists from the heavy workload and remind\ninexperienced radiologists of misdiagnosis or missed diagnose. Existing\napproaches mainly formulate radiology report generation as an image captioning\ntask and adopt the encoder-decoder framework. However, in the medical domain,\nsuch pure data-driven approaches suffer from the following problems: 1) visual\nand textual bias problem; 2) lack of expert knowledge. In this paper, we\npropose a knowledge-enhanced radiology report generation approach introduces\ntwo types of medical knowledge: 1) General knowledge, which is input\nindependent and provides the broad knowledge for report generation; 2) Specific\nknowledge, which is input dependent and provides the fine-grained knowledge for\nreport generation. To fully utilize both the general and specific knowledge, we\nalso propose a knowledge-enhanced multi-head attention mechanism. By merging\nthe visual features of the radiology image with general knowledge and specific\nknowledge, the proposed model can improve the quality of generated reports.\nExperimental results on two publicly available datasets IU-Xray and MIMIC-CXR\nshow that the proposed knowledge enhanced approach outperforms state-of-the-art\nimage captioning based methods. Ablation studies also demonstrate that both\ngeneral and specific knowledge can help to improve the performance of radiology\nreport generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shaohua Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment. (arXiv:2112.15011v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15011","description":"<p>In clinics, a radiology report is crucial for guiding a patient's treatment.\nUnfortunately, report writing imposes a heavy burden on radiologists. To\neffectively reduce such a burden, we hereby present an automatic, multi-modal\napproach for report generation from chest x-ray. Our approach, motivated by the\nobservation that the descriptions in radiology reports are highly correlated\nwith the x-ray images, features two distinct modules: (i) Learned knowledge\nbase. To absorb the knowledge embedded in the above-mentioned correlation, we\nautomatically build a knowledge base based on textual embedding. (ii)\nMulti-modal alignment. To promote the semantic alignment among reports, disease\nlabels and images, we explicitly utilize textual embedding to guide the\nlearning of the visual feature space. We evaluate the performance of the\nproposed model using metrics from both natural language generation and clinic\nefficacy on the public IU and MIMIC-CXR datasets. Our ablation study shows that\neach module contributes to improving the quality of generated reports.\nFurthermore, with the aid of both modules, our approach clearly outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xingwang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S.Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Pose Representations and Motion Contexts Modeling for 3D Motion Prediction. (arXiv:2112.15012v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15012","description":"<p>Predicting human motion from historical pose sequence is crucial for a\nmachine to succeed in intelligent interactions with humans. One aspect that has\nbeen obviated so far, is the fact that how we represent the skeletal pose has a\ncritical impact on the prediction results. Yet there is no effort that\ninvestigates across different pose representation schemes. We conduct an\nindepth study on various pose representations with a focus on their effects on\nthe motion prediction task. Moreover, recent approaches build upon\noff-the-shelf RNN units for motion prediction. These approaches process input\npose sequence sequentially and inherently have difficulties in capturing\nlong-term dependencies. In this paper, we propose a novel RNN architecture\ntermed AHMR (Attentive Hierarchical Motion Recurrent network) for motion\nprediction which simultaneously models local motion contexts and a global\ncontext. We further explore a geodesic loss and a forward kinematics loss for\nthe motion prediction task, which have more geometric significance than the\nwidely employed L2 loss. Interestingly, we applied our method to a range of\narticulate objects including human, fish, and mouse. Empirical results show\nthat our approach outperforms the state-of-the-art methods in short-term\nprediction and achieves much enhanced long-term prediction proficiency, such as\nretaining natural human-like motions over 50 seconds predictions. Our codes are\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuyuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continually Learning Self-Supervised Representations with Projected Functional Regularization. (arXiv:2112.15022v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15022","description":"<p>Recent self-supervised learning methods are able to learn high-quality image\nrepresentations and are closing the gap with supervised methods. However, these\nmethods are unable to acquire new knowledge incrementally -- they are, in fact,\nmostly used only as a pre-training phase with IID data. In this work we\ninvestigate self-supervised methods in continual learning regimes without\nadditional memory or replay. To prevent forgetting of previous knowledge, we\npropose the usage of functional regularization. We will show that naive\nfunctional regularization, also known as feature distillation, leads to low\nplasticity and therefore seriously limits continual learning performance. To\naddress this problem, we propose Projected Functional Regularization where a\nseparate projection network ensures that the newly learned feature space\npreserves information of the previous feature space, while allowing for the\nlearning of new features. This allows us to prevent forgetting while\nmaintaining the plasticity of the learner. Evaluation against other incremental\nlearning approaches applied to self-supervision demonstrates that our method\nobtains competitive performance in different scenarios and on multiple\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Villa_A/0/1/0/all/0/1\">Alex Gomez-Villa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bartlomiej Twardowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagdanov_A/0/1/0/all/0/1\">Andrew D. Bagdanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of a face mask detection pipeline for mask-wearing monitoring in the era of the COVID-19 pandemic: A modular approach. (arXiv:2112.15031v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15031","description":"<p>During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to\nprevent spreading and contracting the virus. The ability to monitor the\nmask-wearing rate in the population would be useful for determining public\nhealth strategies against the virus. However, artificial intelligence\ntechnologies for detecting face masks have not been deployed at a large scale\nin real-life to measure the mask-wearing rate in public. In this paper, we\npresent a two-step face mask detection approach consisting of two separate\nmodules: 1) face detection and alignment and 2) face mask classification. This\napproach allowed us to experiment with different combinations of face detection\nand face mask classification modules. More specifically, we experimented with\nPyramidKey and RetinaFace as face detectors while maintaining a lightweight\nbackbone for the face mask classification module. Moreover, we also provide a\nrelabeled annotation of the test set of the AIZOO dataset, where we rectified\nthe incorrect labels for some face images. The evaluation results on the AIZOO\nand Moxa 3K datasets showed that the proposed face mask detection pipeline\nsurpassed the state-of-the-art methods. The proposed pipeline also yielded a\nhigher mAP on the relabeled test set of the AIZOO dataset than the original\ntest set. Since we trained the proposed model using in-the-wild face images, we\ncan successfully deploy our model to monitor the mask-wearing rate using public\nCCTV images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sommana_B/0/1/0/all/0/1\">Benjaphan Sommana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watchareeruetai_U/0/1/0/all/0/1\">Ukrit Watchareeruetai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Ankush Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earp_S/0/1/0/all/0/1\">Samuel W.F. Earp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitiyakara_T/0/1/0/all/0/1\">Taya Kitiyakara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boonmanunt_S/0/1/0/all/0/1\">Suparee Boonmanunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thammasudjarit_R/0/1/0/all/0/1\">Ratchainant Thammasudjarit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digital Rock Typing DRT Algorithm Formulation with Optimal Supervised Semantic Segmentation. (arXiv:2112.15068v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15068","description":"<p>Each grid block in a 3D geological model requires a rock type that represents\nall physical and chemical properties of that block. The properties that\nclassify rock types are lithology, permeability, and capillary pressure.\nScientists and engineers determined these properties using conventional\nlaboratory measurements, which embedded destructive methods to the sample or\naltered some of its properties (i.e., wettability, permeability, and porosity)\nbecause the measurements process includes sample crushing, fluid flow, or fluid\nsaturation. Lately, Digital Rock Physics (DRT) has emerged to quantify these\nproperties from micro-Computerized Tomography (uCT) and Magnetic Resonance\nImaging (MRI) images. However, the literature did not attempt rock typing in a\nwholly digital context. We propose performing Digital Rock Typing (DRT) by: (1)\nintegrating the latest DRP advances in a novel process that honors digital rock\nproperties determination, while; (2) digitalizing the latest rock typing\napproaches in carbonate, and (3) introducing a novel carbonate rock typing\nprocess that utilizes computer vision capabilities to provide more insight\nabout the heterogeneous carbonate rock texture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfarisi_O/0/1/0/all/0/1\">Omar Alfarisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouzzane_D/0/1/0/all/0/1\">Djamel Ouzzane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassi_M/0/1/0/all/0/1\">Mohamed Sassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiejun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Estimation of Specific Rigid Objects. (arXiv:2112.15075v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15075","description":"<p>In this thesis, we address the problem of estimating the 6D pose of rigid\nobjects from a single RGB or RGB-D input image, assuming that 3D models of the\nobjects are available. This problem is of great importance to many application\nfields such as robotic manipulation, augmented reality, and autonomous driving.\nFirst, we propose EPOS, a method for 6D object pose estimation from an RGB\nimage. The key idea is to represent an object by compact surface fragments and\npredict the probability distribution of corresponding fragments at each pixel\nof the input image by a neural network. Each pixel is linked with a\ndata-dependent number of fragments, which allows systematic handling of\nsymmetries, and the 6D poses are estimated from the links by a RANSAC-based\nfitting method. EPOS outperformed all RGB and most RGB-D and D methods on\nseveral standard datasets. Second, we present HashMatch, an RGB-D method that\nslides a window over the input image and searches for a match against\ntemplates, which are pre-generated by rendering 3D object models in different\norientations. The method applies a cascade of evaluation stages to each window\nlocation, which avoids exhaustive matching against all templates. Third, we\npropose ObjectSynth, an approach to synthesize photorealistic images of 3D\nobject models for training methods based on neural networks. The images yield\nsubstantial improvements compared to commonly used images of objects rendered\non top of random photographs. Fourth, we introduce T-LESS, the first dataset\nfor 6D object pose estimation that includes 3D models and RGB-D images of\nindustry-relevant objects. Fifth, we define BOP, a benchmark that captures the\nstatus quo in the field. BOP comprises eleven datasets in a unified format, an\nevaluation methodology, an online evaluation system, and public challenges held\nat international workshops organized at the ICCV and ECCV conferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hodan_T/0/1/0/all/0/1\">Tomas Hodan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Extraction and Prediction for Hand Hygiene Gestures with KNN Algorithm. (arXiv:2112.15085v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15085","description":"<p>This work focuses upon the analysis of hand gestures involved in the process\nof hand washing. There are six standard hand hygiene gestures for washing hands\nas provided by World Health Organisation hand hygiene guidelines. In this\npaper, hand features such as contours of hands, the centroid of the hands, and\nextreme hand points along the largest contour are extracted with the use of the\ncomputer vision library, OpenCV. These hand features are extracted for each\ndata frame in a hand hygiene video. A robust hand hygiene dataset of video\nrecordings was created in the project. A subset of this dataset is used in this\nwork. Extracted hand features are further grouped into classes based on the KNN\nalgorithm with a cross-fold validation technique for the classification and\nprediction of the unlabelled data. A mean accuracy score of &gt;95% is achieved\nand proves that the KNN algorithm with an appropriate input value of K=5 is\nefficient for classification. A complete dataset with six distinct hand hygiene\nclasses will be used with the KNN classifier for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging in-domain supervision for unsupervised image-to-image translation tasks via multi-stream generators. (arXiv:2112.15091v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15091","description":"<p>Supervision for image-to-image translation (I2I) tasks is hard to come by,\nbut bears significant effect on the resulting quality. In this paper, we\nobserve that for many Unsupervised I2I (UI2I) scenarios, one domain is more\nfamiliar than the other, and offers in-domain prior knowledge, such as semantic\nsegmentation. We argue that for complex scenes, figuring out the semantic\nstructure of the domain is hard, especially with no supervision, but is an\nimportant part of a successful I2I operation. We hence introduce two techniques\nto incorporate this invaluable in-domain prior knowledge for the benefit of\ntranslation quality: through a novel Multi-Stream generator architecture, and\nthrough a semantic segmentation-based regularization loss term. In essence, we\npropose splitting the input data according to semantic masks, explicitly\nguiding the network to different behavior for the different regions of the\nimage. In addition, we propose training a semantic segmentation network along\nwith the translation task, and to leverage this output as a loss term that\nimproves robustness. We validate our approach on urban data, demonstrating\nsuperior quality in the challenging UI2I tasks of converting day images to\nnight ones. In addition, we also demonstrate how reinforcing the target dataset\nwith our augmented images improves the training of downstream tasks such as the\nclassical detection one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yerushalmi_D/0/1/0/all/0/1\">Dvir Yerushalmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1\">Dov Danon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study. (arXiv:2112.15093v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15093","description":"<p>The flourishing blossom of deep learning has witnessed the rapid development\nof text recognition in recent years. However, the existing text recognition\nmethods are mainly for English texts, whereas ignoring the pivotal role of\nChinese texts. As another widely-spoken language, Chinese text recognition in\nall ways has extensive application markets. Based on our observations, we\nattribute the scarce attention on Chinese text recognition to the lack of\nreasonable dataset construction standards, unified evaluation methods, and\nresults of the existing baselines. To fill this gap, we manually collect\nChinese text datasets from publicly available competitions, projects, and\npapers, then divide them into four categories including scene, web, document,\nand handwriting datasets. Furthermore, we evaluate a series of representative\ntext recognition methods on these datasets with unified evaluation methods to\nprovide experimental results. By analyzing the experimental results, we\nsurprisingly observe that state-of-the-art baselines for recognizing English\ntexts cannot perform well on Chinese scenarios. We consider that there still\nremain numerous challenges under exploration due to the characteristics of\nChinese texts, which are quite different from English texts. The code and\ndatasets are made publicly available at\nhttps://github.com/FudanVI/benchmarking-chinese-text-recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1\">Mengnan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xixi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaocong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Shaobo Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A general technique for the estimation of farm animal body part weights from CT scans and its applications in a rabbit breeding program. (arXiv:2112.15095v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15095","description":"<p>Various applications of farm animal imaging are based on the estimation of\nweights of certain body parts and cuts from the CT images of animals. In many\ncases, the complexity of the problem is increased by the enormous variability\nof postures in CT images due to the scanning of non-sedated, living animals. In\nthis paper, we propose a general and robust approach for the estimation of the\nweights of cuts and body parts from the CT images of (possibly) living animals.\nWe adapt multi-atlas based segmentation driven by elastic registration and\njoint feature and model selection for the regression component to cape with the\nlarge number of features and low number of samples. The proposed technique is\nevaluated and illustrated through real applications in rabbit breeding\nprograms, showing r^2 scores 12% higher than previous techniques and methods\nthat used to drive the selection so far. The proposed technique is easily\nadaptable to similar problems, consequently, it is shared in an open source\nsoftware package for the benefit of the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Csoka_A/0/1/0/all/0/1\">&#xc1;d&#xe1;m Cs&#xf3;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovacs_G/0/1/0/all/0/1\">Gy&#xf6;rgy Kov&#xe1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acs_V/0/1/0/all/0/1\">Vir&#xe1;g &#xc1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matics_Z/0/1/0/all/0/1\">Zsolt Matics</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerencser_Z/0/1/0/all/0/1\">Zsolt Gerencs&#xe9;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szendro_Z/0/1/0/all/0/1\">Zsolt Szendr&#x151;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagy_I/0/1/0/all/0/1\">Istv&#xe1;n Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petnehazy_O/0/1/0/all/0/1\">&#xd6;rs Petneh&#xe1;zy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repa_I/0/1/0/all/0/1\">Imre Repa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moizs_M/0/1/0/all/0/1\">Mariann Moizs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donko_T/0/1/0/all/0/1\">Tam&#xe1;s Donk&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colour alignment for relative colour constancy via non-standard references. (arXiv:2112.15106v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15106","description":"<p>Relative colour constancy is an essential requirement for many scientific\nimaging applications. However, most digital cameras differ in their image\nformations and native sensor output is usually inaccessible, e.g., in\nsmartphone camera applications. This makes it hard to achieve consistent colour\nassessment across a range of devices, and that undermines the performance of\ncomputer vision algorithms. To resolve this issue, we propose a colour\nalignment model that considers the camera image formation as a black-box and\nformulates colour alignment as a three-step process: camera response\ncalibration, response linearisation, and colour matching. The proposed model\nworks with non-standard colour references, i.e., colour patches without knowing\nthe true colour values, by utilising a novel balance-of-linear-distances\nfeature. It is equivalent to determining the camera parameters through an\nunsupervised process. It also works with a minimum number of corresponding\ncolour patches across the images to be colour aligned to deliver the applicable\nprocessing. Two challenging image datasets collected by multiple cameras under\nvarious illumination and exposure conditions were used to evaluate the model.\nPerformance benchmarks demonstrated that our model achieved superior\nperformance compared to other popular and state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunfeng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferguson_S/0/1/0/all/0/1\">Stuart Ferguson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elliott_C/0/1/0/all/0/1\">Chris Elliott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rafferty_K/0/1/0/all/0/1\">Karen Rafferty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Layers in Vision Transformers. (arXiv:2112.15111v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15111","description":"<p>We introduce fully stochastic layers in vision transformers, without causing\nany severe drop in performance. The additional stochasticity boosts the\nrobustness of visual features and strengthens privacy. In this process, linear\nlayers with fully stochastic parameters are used, both during training and\ninference, to transform the feature activations of each multilayer perceptron.\nSuch stochastic linear operations preserve the topological structure, formed by\nthe set of tokens passing through the shared multilayer perceptron. This\noperation encourages the learning of the recognition task to rely on the\ntopological structures of the tokens, instead of their values, which in turn\noffers the desired robustness and privacy of the visual features. In this\npaper, we use our features for three different applications, namely,\nadversarial robustness, network calibration, and feature privacy. Our features\noffer exciting results on those tasks. Furthermore, we showcase an experimental\nsetup for federated and transfer learning, where the vision transformers with\nstochastic layers are again shown to be well behaved. Our source code will be\nmade publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nikola Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks. (arXiv:2112.15139v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15139","description":"<p>Quantized neural networks typically require smaller memory footprints and\nlower computation complexity, which is crucial for efficient deployment.\nHowever, quantization inevitably leads to a distribution divergence from the\noriginal network, which generally degrades the performance. To tackle this\nissue, massive efforts have been made, but most existing approaches lack\nstatistical considerations and depend on several manual configurations. In this\npaper, we present an adaptive-mapping quantization method to learn an optimal\nlatent sub-distribution that is inherent within models and smoothly\napproximated with a concrete Gaussian Mixture (GM). In particular, the network\nweights are projected in compliance with the GM-approximated sub-distribution.\nThis sub-distribution evolves along with the weight update in a co-tuning\nschema guided by the direct task-objective optimization. Sufficient experiments\non image classification and object detection over various modern architectures\ndemonstrate the effectiveness, generalization property, and transferability of\nthe proposed method. Besides, an efficient deployment flow for the mobile CPU\nis developed, achieving up to 7.46$\\times$ inference acceleration on an\nocta-core ARM CPU. Codes are publicly released at\nhttps://github.com/RunpeiDong/DGMS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Runpei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhanhong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengdi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Resolution Enhancement Plug-in for Deformable Registration of Medical Images. (arXiv:2112.15180v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15180","description":"<p>Image registration is a fundamental task for medical imaging. Resampling of\nthe intensity values is required during registration and better spatial\nresolution with finer and sharper structures can improve the resampling\nperformance and hence the registration accuracy. Super-resolution (SR) is an\nalgorithmic technique targeting at spatial resolution enhancement which can\nachieve an image resolution beyond the hardware limitation. In this work, we\nconsider SR as a preprocessing technique and present a CNN-based resolution\nenhancement module (REM) which can be easily plugged into the registration\nnetwork in a cascaded manner. Different residual schemes and network\nconfigurations of REM are investigated to obtain an effective architecture\ndesign of REM. In fact, REM is not confined to image registration, it can also\nbe straightforwardly integrated into other vision tasks for enhanced\nresolution. The proposed REM is thoroughly evaluated for deformable\nregistration on medical images quantitatively and qualitatively at different\nupscaling factors. Experiments on LPBA40 brain MRI dataset demonstrate that REM\nnot only improves the registration accuracy, especially when the input images\nsuffer from degraded spatial resolution, but also generates resolution enhanced\nimages which can be exploited for successive diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_K/0/1/0/all/0/1\">Kaicong Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simon_S/0/1/0/all/0/1\">Sven Simon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robustness of Neural Networks. (arXiv:2112.15188v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15188","description":"<p>We introduce several new datasets namely ImageNet-A/O and ImageNet-R as well\nas a synthetic environment and testing suite we called CAOS. ImageNet-A/O allow\nresearchers to focus in on the blind spots remaining in ImageNet. ImageNet-R\nwas specifically created with the intention of tracking robust representation\nas the representations are no longer simply natural but include artistic, and\nother renditions. The CAOS suite is built off of CARLA simulator which allows\nfor the inclusion of anomalous objects and can create reproducible synthetic\nenvironment and scenes for testing robustness. All of the datasets were created\nfor testing robustness and measuring progress in robustness. The datasets have\nbeen used in various other works to measure their own progress in robustness\nand allowing for tangential progress that does not focus exclusively on natural\naccuracy.\n</p>\n<p>Given these datasets, we created several novel methods that aim to advance\nrobustness research. We build off of simple baselines in the form of Maximum\nLogit, and Typicality Score as well as create a novel data augmentation method\nin the form of DeepAugment that improves on the aforementioned benchmarks.\nMaximum Logit considers the logit values instead of the values after the\nsoftmax operation, while a small change produces noticeable improvements. The\nTypicality Score compares the output distribution to a posterior distribution\nover classes. We show that this improves performance over the baseline in all\nbut the segmentation task. Speculating that perhaps at the pixel level the\nsemantic information of a pixel is less meaningful than that of class level\ninformation. Finally the new augmentation technique of DeepAugment utilizes\nneural networks to create augmentations on images that are radically different\nthan the traditional geometric and camera based transformations used\npreviously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual and Object Geo-localization: A Comprehensive Survey. (arXiv:2112.15202v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15202","description":"<p>The concept of geo-localization refers to the process of determining where on\nearth some `entity' is located, typically using Global Positioning System (GPS)\ncoordinates. The entity of interest may be an image, sequence of images, a\nvideo, satellite image, or even objects visible within the image. As massive\ndatasets of GPS tagged media have rapidly become available due to smartphones\nand the internet, and deep learning has risen to enhance the performance\ncapabilities of machine learning models, the fields of visual and object\ngeo-localization have emerged due to its significant impact on a wide range of\napplications such as augmented reality, robotics, self-driving vehicles, road\nmaintenance, and 3D reconstruction. This paper provides a comprehensive survey\nof geo-localization involving images, which involves either determining from\nwhere an image has been captured (Image geo-localization) or geo-locating\nobjects within an image (Object geo-localization). We will provide an in-depth\nstudy, including a summary of popular algorithms, a description of proposed\ndatasets, and an analysis of performance results to illustrate the current\nstate of each field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Daniel Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultani_W/0/1/0/all/0/1\">Waqas Sultani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wshah_S/0/1/0/all/0/1\">Safwan Wshah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Free Knowledge Transfer: A Survey. (arXiv:2112.15278v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15278","description":"<p>In the last decade, many deep learning models have been well trained and made\na great success in various fields of machine intelligence, especially for\ncomputer vision and natural language processing. To better leverage the\npotential of these well-trained models in intra-domain or cross-domain transfer\nlearning situations, knowledge distillation (KD) and domain adaptation (DA) are\nproposed and become research highlights. They both aim to transfer useful\ninformation from a well-trained model with original training data. However, the\noriginal data is not always available in many cases due to privacy, copyright\nor confidentiality. Recently, the data-free knowledge transfer paradigm has\nattracted appealing attention as it deals with distilling valuable knowledge\nfrom well-trained models without requiring to access to the training data. In\nparticular, it mainly consists of the data-free knowledge distillation (DFKD)\nand source data-free domain adaptation (SFDA). On the one hand, DFKD aims to\ntransfer the intra-domain knowledge of original data from a cumbersome teacher\nnetwork to a compact student network for model compression and efficient\ninference. On the other hand, the goal of SFDA is to reuse the cross-domain\nknowledge stored in a well-trained source model and adapt it to a target\ndomain. In this paper, we provide a comprehensive survey on data-free knowledge\ntransfer from the perspectives of knowledge distillation and unsupervised\ndomain adaptation, to help readers have a better understanding of the current\nresearch status and ideas. Applications and challenges of the two areas are\nbriefly reviewed, respectively. Furthermore, we provide some insights to the\nsubject of future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation. (arXiv:2112.15283v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15283","description":"<p>Conventional methods for the image-text generation tasks mainly tackle the\nnaturally bidirectional generation tasks separately, focusing on designing\ntask-specific frameworks to improve the quality and fidelity of the generated\nsamples. Recently, Vision-Language Pre-training models have greatly improved\nthe performance of the image-to-text generation tasks, but large-scale\npre-training models for text-to-image synthesis task are still under-developed.\nIn this paper, we propose ERNIE-ViLG, a unified generative pre-training\nframework for bidirectional image-text generation with transformer model. Based\non the image quantization models, we formulate both image generation and text\ngeneration as autoregressive generative tasks conditioned on the text/image\ninput. The bidirectional image-text generative modeling eases the semantic\nalignments across vision and language. For the text-to-image generation\nprocess, we further propose an end-to-end training method to jointly learn the\nvisual sequence generator and the image reconstructor. To explore the landscape\nof large-scale pre-training for bidirectional text-image generation, we train a\n10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million\n(Chinese) image-text pairs which achieves state-of-the-art performance for both\ntext-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for\ntext-to-image synthesis and best results on COCO-CN and AIC-ICC for image\ncaptioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Weichong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yewei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lanxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1\">Boqiang Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhihua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSformer: Bridging Convolution and Transformer for Compressive Sensing. (arXiv:2112.15299v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15299","description":"<p>Convolution neural networks (CNNs) have succeeded in compressive image\nsensing. However, due to the inductive bias of locality and weight sharing, the\nconvolution operations demonstrate the intrinsic limitations in modeling the\nlong-range dependency. Transformer, designed initially as a\nsequence-to-sequence model, excels at capturing global contexts due to the\nself-attention-based architectures even though it may be equipped with limited\nlocalization abilities. This paper proposes CSformer, a hybrid framework that\nintegrates the advantages of leveraging both detailed spatial information from\nCNN and the global context provided by transformer for enhanced representation\nlearning. The proposed approach is an end-to-end compressive image sensing\nmethod, composed of adaptive sampling and recovery. In the sampling module,\nimages are measured block-by-block by the learned sampling matrix. In the\nreconstruction stage, the measurement is projected into dual stems. One is the\nCNN stem for modeling the neighborhood relationships by convolution, and the\nother is the transformer stem for adopting global self-attention mechanism. The\ndual branches structure is concurrent, and the local features and global\nrepresentations are fused under different resolutions to maximize the\ncomplementary of features. Furthermore, we explore a progressive strategy and\nwindow-based transformer block to reduce the parameter and computational\ncomplexity. The experimental results demonstrate the effectiveness of the\ndedicated transformer-based architecture for compressive sensing, which\nachieves superior performance compared to state-of-the-art methods on different\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ye_D/0/1/0/all/0/1\">Dongjie Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_Z/0/1/0/all/0/1\">Zhangkai Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hanli Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SplitBrain: Hybrid Data and Model Parallel Deep Learning. (arXiv:2112.15317v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15317","description":"<p>The recent success of deep learning applications has coincided with those\nwidely available powerful computational resources for training sophisticated\nmachine learning models with huge datasets. Nonetheless, training large models\nsuch as convolutional neural networks using model parallelism (as opposed to\ndata parallelism) is challenging because the complex nature of communication\nbetween model shards makes it difficult to partition the computation\nefficiently across multiple machines with an acceptable trade-off. This paper\npresents SplitBrain, a high performance distributed deep learning framework\nsupporting hybrid data and model parallelism. Specifically, SplitBrain provides\nlayer-specific partitioning that co-locates compute intensive convolutional\nlayers while sharding memory demanding layers. A novel scalable group\ncommunication is proposed to further improve the training throughput with\nreduced communication overhead. The results show that SplitBrain can achieve\nnearly linear speedup while saving up to 67\\% of memory consumption for data\nand model parallel VGG over CIFAR-10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Farley Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruus_E/0/1/0/all/0/1\">Erik Kruus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InverseMV: Composing Piano Scores with a Convolutional Video-Music Transformer. (arXiv:2112.15320v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15320","description":"<p>Many social media users prefer consuming content in the form of videos rather\nthan text. However, in order for content creators to produce videos with a high\nclick-through rate, much editing is needed to match the footage to the music.\nThis posts additional challenges for more amateur video makers. Therefore, we\npropose a novel attention-based model VMT (Video-Music Transformer) that\nautomatically generates piano scores from video frames. Using music generated\nfrom models also prevent potential copyright infringements that often come with\nusing existing music. To the best of our knowledge, there is no work besides\nthe proposed VMT that aims to compose music for video. Additionally, there\nlacks a dataset with aligned video and symbolic music. We release a new dataset\ncomposed of over 7 hours of piano scores with fine alignment between pop music\nvideos and MIDI files. We conduct experiments with human evaluation on VMT,\nSeqSeq model (our baseline), and the original piano version soundtrack. VMT\nachieves consistent improvements over the baseline on music smoothness and\nvideo relevance. In particular, with the relevance scores and our case study,\nour model has shown the capability of multimodality on frame-level actors'\nmovement for music generation. Our VMT model, along with the new dataset,\npresents a promising research direction toward composing the matching\nsoundtrack for videos. We have released our code at\nhttps://github.com/linchintung/VMT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Tung Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deconfounded Visual Grounding. (arXiv:2112.15324v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15324","description":"<p>We focus on the confounding bias between language and location in the visual\ngrounding pipeline, where we find that the bias is the major visual reasoning\nbottleneck. For example, the grounding process is usually a trivial\nlanguage-location association without visual reasoning, e.g., grounding any\nlanguage query containing sheep to the nearly central regions, due to that most\nqueries about sheep have ground-truth locations at the image center. First, we\nframe the visual grounding pipeline into a causal graph, which shows the\ncausalities among image, query, target location and underlying confounder.\nThrough the causal graph, we know how to break the grounding bottleneck:\ndeconfounded visual grounding. Second, to tackle the challenge that the\nconfounder is unobserved in general, we propose a confounder-agnostic approach\ncalled: Referring Expression Deconfounder (RED), to remove the confounding\nbias. Third, we implement RED as a simple language attention, which can be\napplied in any grounding method. On popular benchmarks, RED improves various\nstate-of-the-art grounding methods by a significant margin. Code will soon be\navailable at: https://github.com/JianqiangH/Deconfounded_VG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiaxin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Distinctive Properties of Universal Perturbations. (arXiv:2112.15329v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15329","description":"<p>We identify properties of universal adversarial perturbations (UAPs) that\ndistinguish them from standard adversarial perturbations. Specifically, we show\nthat targeted UAPs generated by projected gradient descent exhibit two\nhuman-aligned properties: semantic locality and spatial invariance, which\nstandard targeted adversarial perturbations lack. We also demonstrate that UAPs\ncontain significantly less signal for generalization than standard adversarial\nperturbations -- that is, UAPs leverage non-robust features to a smaller extent\nthan standard adversarial perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sung Min Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kuo-An Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kai Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1\">Aleksander Madry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P2P-Loc: Point to Point Tiny Person Localization. (arXiv:2112.15344v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15344","description":"<p>Bounding-box annotation form has been the most frequently used method for\nvisual object localization tasks. However, bounding-box annotation relies on\nthe large amounts of precisely annotating bounding boxes, which is expensive,\nlaborious, thus impossible in practical scenarios, and even redundant for some\napplications caring not about size. Therefore, we propose a novel point-based\nframework for the person localization task by annotating each person as a\ncoarse point (CoarsePoint) which can be any point within the object extent,\ninstead of an accurate bounding box. And then predict the person's location as\na 2D coordinate in the image. That greatly simplifies the data annotation\npipeline. However, the CoarsePoint annotation inevitably causes the label\nreliability decrease (label uncertainty) and network confusion during training.\nAs a result, we propose a point self-refinement approach, which iteratively\nupdates point annotations in a self-paced way. The proposed refinement system\nalleviates the label uncertainty and progressively improves localization\nperformance. Experiments show that our approach achieves comparable object\nlocalization performance while saving annotation cost up to 80$\\%$. Code is\nenclosed in the supplementary materials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuehui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jianbin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhenjun Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Predict 3D Lane Shape and Camera Pose from a Single Image via Geometry Constraints. (arXiv:2112.15351v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15351","description":"<p>Detecting 3D lanes from the camera is a rising problem for autonomous\nvehicles. In this task, the correct camera pose is the key to generating\naccurate lanes, which can transform an image from perspective-view to the\ntop-view. With this transformation, we can get rid of the perspective effects\nso that 3D lanes would look similar and can accurately be fitted by low-order\npolynomials. However, mainstream 3D lane detectors rely on perfect camera poses\nprovided by other sensors, which is expensive and encounters multi-sensor\ncalibration issues. To overcome this problem, we propose to predict 3D lanes by\nestimating camera pose from a single image with a two-stage framework. The\nfirst stage aims at the camera pose task from perspective-view images. To\nimprove pose estimation, we introduce an auxiliary 3D lane task and geometry\nconstraints to benefit from multi-task learning, which enhances consistencies\nbetween 3D and 2D, as well as compatibility in the above two tasks. The second\nstage targets the 3D lane task. It uses previously estimated pose to generate\ntop-view images containing distance-invariant lane appearances for predicting\naccurate 3D lanes. Experiments demonstrate that, without ground truth camera\npose, our method outperforms the state-of-the-art perfect-camera-pose-based\nmethods and has the fewest parameters and computations. Codes are available at\nhttps://github.com/liuruijin17/CLGo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dapeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiliang Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zejian Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse LiDAR Assisted Self-supervised Stereo Disparity Estimation. (arXiv:2112.15355v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15355","description":"<p>Deep stereo matching has made significant progress in recent years. However,\nstate-of-the-art methods are based on expensive 4D cost volume, which limits\ntheir use in real-world applications. To address this issue, 3D correlation\nmaps and iterative disparity updates have been proposed. Regarding that in\nreal-world platforms, such as self-driving cars and robots, the Lidar is\nusually installed. Thus we further introduce the sparse Lidar point into the\niterative updates, which alleviates the burden of network updating the\ndisparity from zero states. Furthermore, we propose training the network in a\nself-supervised way so that it can be trained on any captured data for better\ngeneralization ability. Experiments and comparisons show that the presented\nmethod is effective and achieves comparable results with related methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peter C. Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Generative Data-Free Knowledge Distillation based on Attention Transfer. (arXiv:2112.15358v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15358","description":"<p>Knowledge distillation has made remarkable achievements in model compression.\nHowever, most existing methods demand original training data, while real data\nin practice are often unavailable due to privacy, security and transmission\nlimitation. To address this problem, we propose a conditional generative\ndata-free knowledge distillation (CGDD) framework to train efficient portable\nnetwork without any real data. In this framework, except using the knowledge\nextracted from teacher model, we introduce preset labels as additional\nauxiliary information to train the generator. Then, the trained generator can\nproduce meaningful training samples of specified category as required. In order\nto promote distillation process, except using conventional distillation loss,\nwe treat preset label as ground truth label so that student network is directly\nsupervised by the category of synthetic training sample. Moreover, we force\nstudent network to mimic the attention maps of teacher model and further\nimprove its performance. To verify the superiority of our method, we design a\nnew evaluation metric is called as relative accuracy to directly compare the\neffectiveness of different distillation methods. Trained portable network\nlearned with proposed data-free distillation method obtains 99.63%, 99.07% and\n99.84% relative accuracy on CIFAR10, CIFAR100 and Caltech101, respectively. The\nexperimental results demonstrate the superiority of proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+YU_X/0/1/0/all/0/1\">Xinyi YU</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Ling Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated Hyperspectral Image Reconstruction via Graph-based Self-Tuning Network. (arXiv:2112.15362v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15362","description":"<p>Recently, hyperspectral imaging (HSI) has attracted increasing research\nattention, especially for the ones based on a coded aperture snapshot spectral\nimaging (CASSI) system. Existing deep HSI reconstruction models are generally\ntrained on paired data to retrieve original signals upon 2D compressed\nmeasurements given by a particular optical hardware mask in CASSI, during which\nthe mask largely impacts the reconstruction performance and could work as a\n\"model hyperparameter\" governing on data augmentations. This mask-specific\ntraining style will lead to a hardware miscalibration issue, which sets up\nbarriers to deploying deep HSI models among different hardware and noisy\nenvironments. To address this challenge, we introduce mask uncertainty for HSI\nwith a complete variational Bayesian learning treatment and explicitly model it\nthrough a mask decomposition inspired by real hardware. Specifically, we\npropose a novel Graph-based Self-Tuning (GST) network to reason uncertainties\nadapting to varying spatial structures of masks among different hardware.\nMoreover, we develop a bilevel optimization framework to balance HSI\nreconstruction and uncertainty estimation, accounting for the hyperparameter\nproperty of masks. Extensive experimental results and model discussions\nvalidate the effectiveness (over 33/30 dB) of the proposed GST method under two\nmiscalibration scenarios and demonstrate a highly competitive performance\ncompared with the state-of-the-art well-calibrated methods. Our code and\npre-trained model are available at https://github.com/Jiamian\nWang/mask_uncertainty_spectral_SCI\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Ziyi Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Change Detection Using Guided Anisotropic Difusion. (arXiv:2112.15367v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15367","description":"<p>Large scale datasets created from crowdsourced labels or openly available\ndata have become crucial to provide training data for large scale learning\nalgorithms. While these datasets are easier to acquire, the data are frequently\nnoisy and unreliable, which is motivating research on weakly supervised\nlearning techniques. In this paper we propose original ideas that help us to\nleverage such datasets in the context of change detection. First, we propose\nthe guided anisotropic diffusion (GAD) algorithm, which improves semantic\nsegmentation results using the input images as guides to perform edge\npreserving filtering. We then show its potential in two weakly-supervised\nlearning strategies tailored for change detection. The first strategy is an\niterative learning method that combines model optimisation and data cleansing\nusing GAD to extract the useful information from a large scale change detection\ndataset generated from open vector data. The second one incorporates GAD within\na novel spatial attention layer that increases the accuracy of weakly\nsupervised networks trained to perform pixel-level predictions from image-level\nlabels. Improvements with respect to state-of-the-art are demonstrated on 4\ndifferent public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Daudt_R/0/1/0/all/0/1\">Rodrigo Caye Daudt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gousseau_Y/0/1/0/all/0/1\">Yann Gousseau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Single Image Super-Resolution Using Dual Path Connections with Multiple Scale Learning. (arXiv:2112.15386v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15386","description":"<p>Deep convolutional neural networks have been demonstrated to be effective for\nSISR in recent years. On the one hand, residual connections and dense\nconnections have been used widely to ease forward information and backward\ngradient flows to boost performance. However, current methods use residual\nconnections and dense connections separately in most network layers in a\nsub-optimal way. On the other hand, although various networks and methods have\nbeen designed to improve computation efficiency, save parameters, or utilize\ntraining data of multiple scale factors for each other to boost performance, it\neither do super-resolution in HR space to have a high computation cost or can\nnot share parameters between models of different scale factors to save\nparameters and inference time. To tackle these challenges, we propose an\nefficient single image super-resolution network using dual path connections\nwith multiple scale learning named as EMSRDPN. By introducing dual path\nconnections inspired by Dual Path Networks into EMSRDPN, it uses residual\nconnections and dense connections in an integrated way in most network layers.\nDual path connections have the benefits of both reusing common features of\nresidual connections and exploring new features of dense connections to learn a\ngood representation for SISR. To utilize the feature correlation of multiple\nscale factors, EMSRDPN shares all network units in LR space between different\nscale factors to learn shared features and only uses a separate reconstruction\nunit for each scale factor, which can utilize training data of multiple scale\nfactors to help each other to boost performance, meanwhile which can save\nparameters and support shared inference for multiple scale factors to improve\nefficiency. Experiments show EMSRDPN achieves better performance and comparable\nor even better parameter and inference efficiency over SOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1\">Bin-Cheng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering. (arXiv:2112.15399v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15399","description":"<p>We present an information-theoretic regularization technique for few-shot\nnovel view synthesis based on neural implicit representation. The proposed\napproach minimizes potential reconstruction inconsistency that happens due to\ninsufficient viewpoints by imposing the entropy constraint of the density in\neach ray. In addition, to alleviate the potential degenerate issue when all\ntraining images are acquired from almost redundant viewpoints, we further\nincorporate the spatially smoothness constraint into the estimated images by\nrestricting information gains from a pair of rays with slightly different\nviewpoints. The main idea of our algorithm is to make reconstructed scenes\ncompact along individual rays and consistent across rays in the neighborhood.\nThe proposed regularizers can be plugged into most of existing neural volume\nrendering techniques based on NeRF in a straightforward way. Despite its\nsimplicity, we achieve consistently improved performance compared to existing\nneural view synthesis methods by large margins on multiple standard benchmarks.\nOur project website is available at\n\\url{<a href=\"http://cvlab.snu.ac.kr/research/InfoNeRF\">this http URL</a>}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Mijeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seonguk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15402","description":"<p>Continual learning requires models to learn new tasks while maintaining\npreviously learned knowledge. Various algorithms have been proposed to address\nthis real challenge. Till now, rehearsal-based methods, such as experience\nreplay, have achieved state-of-the-art performance. These approaches save a\nsmall part of the data of the past tasks as a memory buffer to prevent models\nfrom forgetting previously learned knowledge. However, most of them treat every\nnew task equally, i.e., fixed the hyperparameters of the framework while\nlearning different new tasks. Such a setting lacks the consideration of the\nrelationship/similarity between past and new tasks. For example, the previous\nknowledge/features learned from dogs are more beneficial for the identification\nof cats (new task), compared to those learned from buses. In this regard, we\npropose a meta learning algorithm based on bi-level optimization to adaptively\ntune the relationship between the knowledge extracted from the past and new\ntasks. Therefore, the model can find an appropriate direction of gradient\nduring continual learning and avoid the serious overfitting problem on memory\nbuffer. Extensive experiments are conducted on three publicly available\ndatasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet). The experimental\nresults demonstrate that the proposed method can consistently improve the\nperformance of all baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disjoint Contrastive Regression Learning for Multi-Sourced Annotations. (arXiv:2112.15411v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15411","description":"<p>Large-scale datasets are important for the development of deep learning\nmodels. Such datasets usually require a heavy workload of annotations, which\nare extremely time-consuming and expensive. To accelerate the annotation\nprocedure, multiple annotators may be employed to label different subsets of\nthe data. However, the inconsistency and bias among different annotators are\nharmful to the model training, especially for qualitative and subjective\ntasks.To address this challenge, in this paper, we propose a novel contrastive\nregression framework to address the disjoint annotations problem, where each\nsample is labeled by only one annotator and multiple annotators work on\ndisjoint subsets of the data. To take account of both the intra-annotator\nconsistency and inter-annotator inconsistency, two strategies are\nemployed.Firstly, a contrastive-based loss is applied to learn the relative\nranking among different samples of the same annotator, with the assumption that\nthe ranking of samples from the same annotator is unanimous. Secondly, we apply\nthe gradient reversal layer to learn robust representations that are invariant\nto different annotators. Experiments on the facial expression prediction task,\nas well as the image quality assessment task, verify the effectiveness of our\nproposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1\">Xiaoqian Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Facial Synthesis: A New Challenge. (arXiv:2112.15439v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15439","description":"<p>The goal of this paper is to conduct a comprehensive study on the facial\nsketch synthesis (FSS) problem. However, due to the high costs in obtaining\nhand-drawn sketch datasets, there lacks a complete benchmark for assessing the\ndevelopment of FSS algorithms over the last decade. As such, we first introduce\na high-quality dataset for FSS, named FS2K, which consists of 2,104\nimage-sketch pairs spanning three types of sketch styles, image backgrounds,\nlighting conditions, skin colors, and facial attributes. FS2K differs from\nprevious FSS datasets in difficulty, diversity, and scalability, and should\nthus facilitate the progress of FSS research. Second, we present the\nlargest-scale FSS study by investigating 139 classical methods, including 24\nhandcrafted feature based facial sketch synthesis approaches, 37 general\nneural-style transfer methods, 43 deep image-to-image translation methods, and\n35 image-to-sketch approaches. Besides, we elaborate comprehensive experiments\nfor existing 19 cutting-edge models. Third, we present a simple baseline for\nFSS, named FSGAN. With only two straightforward components, i.e., facial-aware\nmasking and style-vector expansion, FSGAN surpasses the performance of all\nprevious state-of-the-art models on the proposed FS2K dataset, by a large\nmargin. Finally, we conclude with lessons learned over the past years, and\npoint out several unsolved challenges. Our open-source code is available at\nhttps://github.com/DengPingFan/FSGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PiFeNet: Pillar-Feature Network for Real-Time 3D Pedestrian Detection from Point Cloud. (arXiv:2112.15458v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15458","description":"<p>We present PiFeNet, an efficient and accurate real-time 3D detector for\npedestrian detection from point clouds. We address two challenges that 3D\nobject detection frameworks encounter when detecting pedestrians: low\nexpressiveness of pillar features and small occupation areas of pedestrians in\npoint clouds. Firstly, we introduce a stackable Pillar Aware Attention (PAA)\nmodule for enhanced pillar features extraction while suppressing noises in the\npoint clouds. By integrating multi-point-aware-pooling, point-wise,\nchannel-wise, and task-aware attention into a simple module, the representation\ncapabilities are boosted while requiring little additional computing resources.\nWe also present Mini-BiFPN, a small yet effective feature network that creates\nbidirectional information flow and multi-level cross-scale feature fusion to\nbetter integrate multi-resolution features. Our approach is ranked 1st in KITTI\npedestrian BEV and 3D leaderboards while running at 26 frames per second (FPS),\nand achieves state-of-the-art performance on Nuscenes detection benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duy-Tho Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hengcan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cloud Removal from Satellite Images. (arXiv:2112.15483v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15483","description":"<p>In this report, we have analyzed available cloud detection technique using\nsentinel hub. We have also implemented spatial attention generative adversarial\nnetwork and improved quality of generated image compared to previous solution\n[7].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_R/0/1/0/all/0/1\">Rutvik Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Antarpuneet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sujoy Saha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene-Adaptive Attention Network for Crowd Counting. (arXiv:2112.15509v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15509","description":"<p>In recent years, significant progress has been made on the research of crowd\ncounting. However, as the challenging scale variations and complex scenes\nexisted in crowds, neither traditional convolution networks nor recent\nTransformer architectures with fixed-size attention could handle the task well.\nTo address this problem, this paper proposes a scene-adaptive attention\nnetwork, termed SAANet. First of all, we design a deformable attention in-built\nTransformer backbone, which learns adaptive feature representations with\ndeformable sampling locations and dynamic attention weights. Then we propose\nthe multi-level feature fusion and count-attentive feature enhancement modules\nfurther, to strengthen feature representation under the global image context.\nThe learned representations could attend to the foreground and are adaptive to\ndifferent scales of crowds. We conduct extensive experiments on four\nchallenging crowd counting benchmarks, demonstrating that our method achieves\nstate-of-the-art performance. Especially, our method currently ranks No.1 on\nthe public leaderboard of the NWPU-Crowd benchmark. We hope our method could be\na strong baseline to support future research in crowd counting. The source code\nwill be released to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yuanrui Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jihao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yunfeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dahu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yihong Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer learning for cancer diagnosis in histopathological images. (arXiv:2112.15523v1 [eess.IV])","link":"http://arxiv.org/abs/2112.15523","description":"<p>Transfer learning allows us to exploit knowledge gained from one task to\nassist in solving another but relevant task. In modern computer vision\nresearch, the question is which architecture performs better for a given\ndataset. In this paper, we compare the performance of 14 pre-trained ImageNet\nmodels on the histopathologic cancer detection dataset, where each model has\nbeen configured as a naive model, feature extractor model, or fine-tuned model.\nDensenet161 has been shown to have high precision whilst Resnet101 has a high\nrecall. A high precision model is suitable to be used when follow-up\nexamination cost is high, whilst low precision but a high recall/sensitivity\nmodel can be used when the cost of follow-up examination is low. Results also\nshow that transfer learning helps to converge a model faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Aneja_S/0/1/0/all/0/1\">Sandhya Aneja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aneja_N/0/1/0/all/0/1\">Nagender Aneja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abas_P/0/1/0/all/0/1\">Pg Emeroylariffion Abas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naim_A/0/1/0/all/0/1\">Abdul Ghani Naim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"on the effectiveness of generative adversarial network on anomaly detection. (arXiv:2112.15541v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15541","description":"<p>Identifying anomalies refers to detecting samples that do not resemble the\ntraining data distribution. Many generative models have been used to find\nanomalies, and among them, generative adversarial network (GAN)-based\napproaches are currently very popular. GANs mainly rely on the rich contextual\ninformation of these models to identify the actual training distribution.\nFollowing this analogy, we suggested a new unsupervised model based on GANs --a\ncombination of an autoencoder and a GAN. Further, a new scoring function was\nintroduced to target anomalies where a linear combination of the internal\nrepresentation of the discriminator and the generator's visual representation,\nplus the encoded representation of the autoencoder, come together to define the\nproposed anomaly score. The model was further evaluated on benchmark datasets\nsuch as SVHN, CIFAR10, and MNIST, as well as a public medical dataset of\nleukemia images. In all the experiments, our model outperformed its existing\ncounterparts while slightly improving the inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sevyeri_L/0/1/0/all/0/1\">Laya Rafiee Sevyeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevens_T/0/1/0/all/0/1\">Thomas Fevens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Baselines in the Wild. (arXiv:2112.15550v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15550","description":"<p>We share our experience with the recently released WILDS benchmark, a\ncollection of ten datasets dedicated to developing models and training\nstrategies which are robust to domain shifts. Several experiments yield a\ncouple of critical observations which we believe are of general interest for\nany future work on WILDS. Our study focuses on two datasets: iWildCam and FMoW.\nWe show that (1) Conducting separate cross-validation for each evaluation\nmetric is crucial for both datasets, (2) A weak correlation between validation\nand test performance might make model development difficult for iWildCam, (3)\nMinor changes in the training of hyper-parameters improve the baseline by a\nrelatively large margin (mainly on FMoW), (4) There is a strong correlation\nbetween certain domains and certain target labels (mainly on iWildCam). To the\nbest of our knowledge, no prior work on these datasets has reported these\nobservations despite their obvious importance. Our code is public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1\">Kazuki Irie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1\">Imanol Schlag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1\">R&#xf3;bert Csord&#xe1;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Unsupervised Domain Adaptation Model based on Dual-module Adversarial Training. (arXiv:2112.15555v1 [cs.LG])","link":"http://arxiv.org/abs/2112.15555","description":"<p>In this paper, we propose a dual-module network architecture that employs a\ndomain discriminative feature module to encourage the domain invariant feature\nmodule to learn more domain invariant features. The proposed architecture can\nbe applied to any model that utilizes domain invariant features for\nunsupervised domain adaptation to improve its ability to extract domain\ninvariant features. We conduct experiments with the Domain-Adversarial Training\nof Neural Networks (DANN) model as a representative algorithm. In the training\nprocess, we supply the same input to the two modules and then extract their\nfeature distribution and prediction results respectively. We propose a\ndiscrepancy loss to find the discrepancy of the prediction results and the\nfeature distribution between the two modules. Through the adversarial training\nby maximizing the loss of their feature distribution and minimizing the\ndiscrepancy of their prediction results, the two modules are encouraged to\nlearn more domain discriminative and domain invariant features respectively.\nExtensive comparative evaluations are conducted and the proposed approach\noutperforms the state-of-the-art in most unsupervised domain adaptation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiju Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCACE: A Statistical Approach to Ranking Neurons for CNN Interpretability. (arXiv:2112.15571v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15571","description":"<p>In this paper we introduce a new problem within the growing literature of\ninterpretability for convolution neural networks (CNNs). While previous work\nhas focused on the question of how to visually interpret CNNs, we ask what it\nis that we care to interpret, that is, which layers and neurons are worth our\nattention? Due to the vast size of modern deep learning network architectures,\nautomated, quantitative methods are needed to rank the relative importance of\nneurons so as to provide an answer to this question. We present a new\nstatistical method for ranking the hidden neurons in any convolutional layer of\na network. We define importance as the maximal correlation between the\nactivation maps and the class score. We provide different ways in which this\nmethod can be used for visualization purposes with MNIST and ImageNet, and show\na real-world application of our method to air pollution prediction with\nstreet-level images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casacuberta_S/0/1/0/all/0/1\">S&#xed;lvia Casacuberta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suel_E/0/1/0/all/0/1\">Esra Suel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flaxman_S/0/1/0/all/0/1\">Seth Flaxman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3-D Material Style Transfer for Reconstructing Unknown Appearance in Complex Natural Materials. (arXiv:2112.15589v1 [cs.CV])","link":"http://arxiv.org/abs/2112.15589","description":"<p>We propose a 3-D material style transfer framework for reconstructing\ninvisible (or faded) appearance properties in complex natural materials. Our\nalgorithm addresses the technical challenge of transferring appearance\nproperties from one object to another of the same material when both objects\nhave intricate, noncorresponding color patterns. Eggshells, exoskeletons, and\nminerals, for example, have patterns composed of highly randomized layers of\norganic and inorganic compounds. These materials pose a challenge as the\ndistribution of compounds that determine surface color changes from object to\nobject and within local pattern regions. Our solution adapts appearance\nobservations from a material property distribution in an exemplar to the\nmaterial property distribution of a target object to reconstruct its unknown\nappearance. We use measured reflectance in 3-D bispectral textures to record\nchanging material property distributions. Our novel implementation of spherical\nharmonics uses principles from chemistry and biology to learn relationships\nbetween color (hue and saturation) and material composition and concentration\nin an exemplar. The encoded relationships are transformed to the property\ndistribution of a target for color recovery and material assignment.\nQuantitative and qualitative evaluation methods show that we replicate color\npatterns more accurately than methods that only rely on shape correspondences\nand coarse-level perceptual differences. We demonstrate applications of our\nwork for reconstructing color in extinct fossils, restoring faded artifacts and\ngenerating synthetic textures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_S/0/1/0/all/0/1\">Shashank Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toler_Franklin_C/0/1/0/all/0/1\">Corey Toler-Franklin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A theory of independent mechanisms for extrapolation in generative models. (arXiv:2004.00184v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2004.00184","description":"<p>Generative models can be trained to emulate complex empirical data, but are\nthey useful to make predictions in the context of previously unobserved\nenvironments? An intuitive idea to promote such extrapolation capabilities is\nto have the architecture of such model reflect a causal graph of the true data\ngenerating process, such that one can intervene on each node independently of\nthe others. However, the nodes of this graph are usually unobserved, leading to\noverparameterization and lack of identifiability of the causal structure. We\ndevelop a theoretical framework to address this challenging situation by\ndefining a weaker form of identifiability, based on the principle of\nindependence of mechanisms. We demonstrate on toy examples that classical\nstochastic gradient descent can hinder the model's extrapolation capabilities,\nsuggesting independence of mechanisms should be enforced explicitly during\ntraining. Experiments on deep generative models trained on real world data\nsupport these insights and illustrate how the extrapolation capabilities of\nsuch models can be leveraged.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">R&#xe9;my Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janzing_D/0/1/0/all/0/1\">Dominik Janzing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation. (arXiv:2007.01771v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.01771","description":"<p>Facial attributes (\\eg, age and attractiveness) estimation performance has\nbeen greatly improved by using convolutional neural networks. However, existing\nmethods have an inconsistency between the training objectives and the\nevaluation metric, so they may be suboptimal. In addition, these methods always\nadopt image classification or face recognition models with a large amount of\nparameters, which carry expensive computation cost and storage overhead. In\nthis paper, we firstly analyze the essential relationship between two\nstate-of-the-art methods (Ranking-CNN and DLDL) and show that the Ranking\nmethod is in fact learning label distribution implicitly. This result thus\nfirstly unifies two existing popular state-of-the-art methods into the DLDL\nframework. Second, in order to alleviate the inconsistency and reduce resource\nconsumption, we design a lightweight network architecture and propose a unified\nframework which can jointly learn facial attribute distribution and regress\nattribute value. The effectiveness of our approach has been demonstrated on\nboth facial age and attractiveness estimation tasks. Our method achieves new\nstate-of-the-art results using the single model with 36$\\times$ fewer\nparameters and 3$\\times$ faster inference speed on facial age/attractiveness\nestimation. Moreover, our method can achieve comparable results as the\nstate-of-the-art even though the number of parameters is further reduced to\n0.9M (3.8MB disk storage).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bin-Bin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin-Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xin Geng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taking Modality-free Human Identification as Zero-shot Learning. (arXiv:2010.00975v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.00975","description":"<p>Human identification is an important topic in event detection, person\ntracking, and public security. There have been numerous methods proposed for\nhuman identification, such as face identification, person re-identification,\nand gait identification. Typically, existing methods predominantly classify a\nqueried image to a specific identity in an image gallery set (I2I). This is\nseriously limited for the scenario where only a textual description of the\nquery or an attribute gallery set is available in a wide range of video\nsurveillance applications (A2I or I2A). However, very few efforts have been\ndevoted towards modality-free identification, i.e., identifying a query in a\ngallery set in a scalable way. In this work, we take an initial attempt, and\nformulate such a novel Modality-Free Human Identification (named MFHI) task as\na generic zero-shot learning model in a scalable way. Meanwhile, it is capable\nof bridging the visual and semantic modalities by learning a discriminative\nprototype of each identity. In addition, the semantics-guided spatial attention\nis enforced on visual modality to obtain representations with both high global\ncategory-level and local attribute-level discrimination. Finally, we design and\nconduct an extensive group of experiments on two common challenging\nidentification tasks, including face identification and person\nre-identification, demonstrating that our method outperforms a wide variety of\nstate-of-the-art methods on modality-free human identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing Hand-Object Interactions in the Wild. (arXiv:2012.09856v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.09856","description":"<p>In this work we explore reconstructing hand-object interactions in the wild.\nThe core challenge of this problem is the lack of appropriate 3D labeled data.\nTo overcome this issue, we propose an optimization-based procedure which does\nnot require direct 3D supervision. The general strategy we adopt is to exploit\nall available related data (2D bounding boxes, 2D hand keypoints, 2D instance\nmasks, 3D object models, 3D in-the-lab MoCap) to provide constraints for the 3D\nreconstruction. Rather than optimizing the hand and object individually, we\noptimize them jointly which allows us to impose additional constraints based on\nhand-object contact, collision, and occlusion. Our method produces compelling\nreconstructions on the challenging in-the-wild data from the EPIC Kitchens and\nthe 100 Days of Hands datasets, across a range of object categories.\nQuantitatively, we demonstrate that our approach compares favorably to existing\napproaches in the lab settings where ground truth 3D annotations are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhe Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radosavovic_I/0/1/0/all/0/1\">Ilija Radosavovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transitional Learning: Exploring the Transition States of Degradation for Blind Super-resolution. (arXiv:2103.15290v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15290","description":"<p>Being extremely dependent on iterative estimation of the degradation prior or\noptimization of the model from scratch, the existing blind super-resolution\n(SR) methods are generally time-consuming and less effective, as the estimation\nof degradation proceeds from a blind initialization and lacks interpretable\ndegradation priors. To address it, this paper proposes a transitional learning\nmethod for blind SR using an end-to-end network without any additional\niterations in inference, and explores an effective representation for unknown\ndegradation. To begin with, we analyze and demonstrate the transitionality of\ndegradations as interpretable prior information to indirectly infer the unknown\ndegradation model, including the widely used additive and convolutive\ndegradations. We then propose a novel Transitional Learning method for blind\nSuper-Resolution (TLSR), by adaptively inferring a transitional transformation\nfunction to solve the unknown degradations without any iterative operations in\ninference. Specifically, the end-to-end TLSR network consists of a degree of\ntransitionality (DoT) estimation network, a homogeneous feature extraction\nnetwork, and a transitional learning module. Quantitative and qualitative\nevaluations on blind SR tasks demonstrate that the proposed TLSR achieves\nsuperior performances and costs fewer complexities against the state-of-the-art\nblind SR methods. The code is available at github.com/YuanfeiHuang/TLSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuanfei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yanting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MutualNet: Adaptive ConvNet via Mutual Learning from Different Model Configurations. (arXiv:2105.07085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07085","description":"<p>Most existing deep neural networks are static, which means they can only do\ninference at a fixed complexity. But the resource budget can vary substantially\nacross different devices. Even on a single device, the affordable budget can\nchange with different scenarios, and repeatedly training networks for each\nrequired budget would be incredibly expensive. Therefore, in this work, we\npropose a general method called MutualNet to train a single network that can\nrun at a diverse set of resource constraints. Our method trains a cohort of\nmodel configurations with various network widths and input resolutions. This\nmutual learning scheme not only allows the model to run at different\nwidth-resolution configurations but also transfers the unique knowledge among\nthese configurations, helping the model to learn stronger representations\noverall. MutualNet is a general training methodology that can be applied to\nvarious network structures (e.g., 2D networks: MobileNets, ResNet, 3D networks:\nSlowFast, X3D) and various tasks (e.g., image classification, object detection,\nsegmentation, and action recognition), and is demonstrated to achieve\nconsistent improvements on a variety of datasets. Since we only train the model\nonce, it also greatly reduces the training cost compared to independently\ntraining several models. Surprisingly, MutualNet can also be used to\nsignificantly boost the performance of a single network, if dynamic resource\nconstraint is not a concern. In summary, MutualNet is a unified method for both\nstatic and adaptive, 2D and 3D networks. Codes and pre-trained models are\navailable at \\url{https://github.com/taoyang1122/MutualNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Taojiannan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1\">Matias Mendieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_R/0/1/0/all/0/1\">Ravikumar Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding. (arXiv:2105.12723v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.12723","description":"<p>Hierarchical structures are popular in recent vision transformers, however,\nthey require sophisticated designs and massive datasets to work well. In this\npaper, we explore the idea of nesting basic local transformers on\nnon-overlapping image blocks and aggregating them in a hierarchical way. We\nfind that the block aggregation function plays a critical role in enabling\ncross-block non-local information communication. This observation leads us to\ndesign a simplified architecture that requires minor code changes upon the\noriginal vision transformer. The benefits of the proposed judiciously-selected\ndesign are threefold: (1) NesT converges faster and requires much less training\ndata to achieve good generalization on both ImageNet and small datasets like\nCIFAR; (2) when extending our key ideas to image generation, NesT leads to a\nstrong decoder that is 8$\\times$ faster than previous transformer-based\ngenerators; and (3) we show that decoupling the feature learning and\nabstraction processes via this nested hierarchy in our design enables\nconstructing a novel method (named GradCAT) for visually interpreting the\nlearned model. Source code is available\nhttps://github.com/google-research/nested-transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1\">Sercan O. Arik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Pseudo Labels for Semi-Supervised Object Detection. (arXiv:2106.00168v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00168","description":"<p>Recent advances in semi-supervised object detection (SSOD) are largely driven\nby consistency-based pseudo-labeling methods for image classification tasks,\nproducing pseudo labels as supervisory signals. However, when using pseudo\nlabels, there is a lack of consideration in localization precision and\namplified class imbalance, both of which are critical for detection tasks. In\nthis paper, we introduce certainty-aware pseudo labels tailored for object\ndetection, which can effectively estimate the classification and localization\nquality of derived pseudo labels. This is achieved by converting conventional\nlocalization as a classification task followed by refinement. Conditioned on\nclassification and localization quality scores, we dynamically adjust the\nthresholds used to generate pseudo labels and reweight loss functions for each\ncategory to alleviate the class imbalance problem. Extensive experiments\ndemonstrate that our method improves state-of-the-art SSOD performance by 1-2%\nAP on COCO and PASCAL VOC while being orthogonal and complementary to most\nexisting methods. In the limited-annotation regime, our approach improves\nsupervised baselines by up to 10% AP using only 1-10% labeled data from COCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hengduo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry S. Davis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCNet: A Structure Similarity Enhancement Method for Multispectral and Multimodal Image Registration. (arXiv:2106.05124v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05124","description":"<p>Multispectral and multimodal image processing is important in the community\nof computer vision and computational photography. As the acquired multispectral\nand multimodal data are generally misaligned due to the alternation or movement\nof the image device, the image registration procedure is necessary. The\nregistration of multispectral or multimodal image is challenging due to the\nnon-linear intensity and gradient variation. To cope with this challenge, we\npropose the phase congruency network (PCNet), which is able to enhance the\nstructure similarity and alleviate the non-linear intensity and gradient\nvariation. The images can then be aligned using the similarity enhanced\nfeatures produced by the network. PCNet is constructed under the guidance of\nthe phase congruency prior. The network contains three trainable layers\naccompany with the modified learnable Gabor kernels according to the phase\ncongruency theory. Thanks to the prior knowledge, PCNet is extremely\nlight-weight. PCNet can be viewed to be fully convolutional and hence can take\ninput of arbitrary sizes. Once trained, PCNet is applicable on a variety of\nmultispectral and multimodal data such as RGB/NIR and flash/no-flash images\nwithout additional further tuning. Experimental results validate that PCNet\noutperforms current state-of-the-art registration algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Si-Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hui-Liang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shu-Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunguang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NCIS: Neural Contextual Iterative Smoothing for Purifying Adversarial Perturbations. (arXiv:2106.11644v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11644","description":"<p>We propose a novel and effective purification based adversarial defense\nmethod against pre-processor blind white- and black-box attacks. Our method is\ncomputationally efficient and trained only with self-supervised learning on\ngeneral images, without requiring any adversarial training or retraining of the\nclassification model. We first show an empirical analysis on the adversarial\nnoise, defined to be the residual between an original image and its adversarial\nexample, has almost zero mean, symmetric distribution. Based on this\nobservation, we propose a very simple iterative Gaussian Smoothing (GS) which\ncan effectively smooth out adversarial noise and achieve substantially high\nrobust accuracy. To further improve it, we propose Neural Contextual Iterative\nSmoothing (NCIS), which trains a blind-spot network (BSN) in a self-supervised\nmanner to reconstruct the discriminative features of the original image that is\nalso smoothed out by GS. From our extensive experiments on the large-scale\nImageNet using four classification models, we show that our method achieves\nboth competitive standard accuracy and state-of-the-art robust accuracy against\nmost strong purifier-blind white- and black-box attacks. Also, we propose a new\nbenchmark for evaluating a purification method based on commercial image\nclassification APIs, such as AWS, Azure, Clarifai and Google. We generate\nadversarial examples by ensemble transfer-based black-box attack, which can\ninduce complete misclassification of APIs, and demonstrate that our method can\nbe used to increase adversarial robustness of APIs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1\">Naeun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Measuring and Controlling the Spectral Bias of the Deep Image Prior. (arXiv:2107.01125v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.01125","description":"<p>The deep image prior showed that a randomly initialized network with a\nsuitable architecture can be trained to solve inverse imaging problems by\nsimply optimizing it's parameters to reconstruct a single degraded image.\nHowever, it suffers from two practical limitations. First, it remains unclear\nhow to control the prior beyond the choice of the network architecture. Second,\ntraining requires an oracle stopping criterion as during the optimization the\nperformance degrades after reaching an optimum value. To address these\nchallenges we introduce a frequency-band correspondence measure to characterize\nthe spectral bias of the deep image prior, where low-frequency image signals\nare learned faster and better than high-frequency counterparts. Based on our\nobservations, we propose techniques to prevent the eventual performance\ndegradation and accelerate convergence. We introduce a Lipschitz-controlled\nconvolution layer and a Gaussian-controlled upsampling layer as plug-in\nreplacements for layers used in the deep architectures. The experiments show\nthat with these changes the performance does not degrade during optimization,\nrelieving us from the need for an oracle stopping criterion. We further outline\na stopping criterion to avoid superfluous computation. Finally, we show that\nour approach obtains favorable results compared to current approaches across\nvarious denoising, deblocking, inpainting, super-resolution and detail\nenhancement tasks. Code is available at\n\\url{https://github.com/shizenglin/Measure-and-Control-Spectral-Bias}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zenglin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Learning Technique for Video Segmentation. (arXiv:2107.01153v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01153","description":"<p>Video segmentation, i.e., partitioning video frames into multiple segments or\nobjects, plays a critical role in a broad range of practical applications, from\nenhancing visual effects in movie, to understanding scenes in autonomous\ndriving, to virtual background creation in video conferencing, just to name a\nfew. Recently, due to the renaissance of connectionism in computer vision,\nthere has been an influx of deep learning based approaches for video\nsegmentation that have delivered compelling performance. In this survey, we\ncomprehensively review two basic lines of research - generic object\nsegmentation (of unknown categories) in videos and video semantic segmentation\n- by introducing their respective task settings, background concepts, perceived\nneed, development history, and main challenges. We also provide a detailed\noverview of representative literature on both methods and datasets.\nAdditionally, we present quantitative performance comparisons of the reviewed\nmethods on benchmark datasets. Finally, we point out a set of unsolved open\nissues in this field, and suggest possible opportunities for further research.\nA public website is provided to continuously track recent developments in this\nfast advancing field: https://github.com/tfzhou/VS-Survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Alignment Prediction for Few-Shot Video Classification. (arXiv:2107.11960v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11960","description":"<p>The goal of few-shot video classification is to learn a classification model\nwith good generalization ability when trained with only a few labeled videos.\nHowever, it is difficult to learn discriminative feature representations for\nvideos in such a setting. In this paper, we propose Temporal Alignment\nPrediction (TAP) based on sequence similarity learning for few-shot video\nclassification. In order to obtain the similarity of a pair of videos, we\npredict the alignment scores between all pairs of temporal positions in the two\nvideos with the temporal alignment prediction function. Besides, the inputs to\nthis function are also equipped with the context information in the temporal\ndomain. We evaluate TAP on two video classification benchmarks including\nKinetics and Something-Something V2. The experimental results verify the\neffectiveness of TAP and show its superiority over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunlei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisBuddy -- A Smart Wearable Assistant for the Visually Challenged. (arXiv:2108.07761v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07761","description":"<p>Vision plays a crucial role in comprehending the world around us. More than\n85\\% of the external information is obtained through the vision system. It\ninfluences our mobility, cognition, information access, and interaction with\nthe environment and other people. Blindness prevents a person from gaining\nknowledge of the surrounding environment and makes unassisted navigation,\nobject recognition, obstacle avoidance, and reading tasks significant\nchallenges. Many existing systems are often limited by cost and complexity. To\nhelp the visually challenged overcome these difficulties faced in everyday\nlife, we propose VisBuddy, a smart assistant to help the visually challenged\nwith their day-to-day activities. VisBuddy is a voice-based assistant where the\nuser can give voice commands to perform specific tasks. It uses the techniques\nof image captioning for describing the user's surroundings, optical character\nrecognition (OCR) for reading the text in the user's view, object detection to\nsearch and find the objects in a room and web scraping to give the user the\nlatest news. VisBuddy has been built by combining the concepts from Deep\nLearning and the Internet of Things. Thus, VisBuddy serves as a cost-efficient,\npowerful, all-in-one assistant for the visually challenged by helping them with\ntheir day-to-day activities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_I/0/1/0/all/0/1\">Ishwarya Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenakshisundaram_N/0/1/0/all/0/1\">Nishaali Meenakshisundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_I/0/1/0/all/0/1\">Ishwarya Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D_S/0/1/0/all/0/1\">Shiloah Elizabeth D</a>, <a href=\"http://arxiv.org/find/cs/1/au:+C_S/0/1/0/all/0/1\">Sunil Retmin Raj C</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"roadscene2vec: A Tool for Extracting and Embedding Road Scene-Graphs. (arXiv:2109.01183v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01183","description":"<p>Recently, road scene-graph representations used in conjunction with graph\nlearning techniques have been shown to outperform state-of-the-art deep\nlearning techniques in tasks including action classification, risk assessment,\nand collision prediction. To enable the exploration of applications of road\nscene-graph representations, we introduce roadscene2vec: an open-source tool\nfor extracting and embedding road scene-graphs. The goal of roadscene2vec is to\nenable research into the applications and capabilities of road scene-graphs by\nproviding tools for generating scene-graphs, graph learning models to generate\nspatio-temporal scene-graph embeddings, and tools for visualizing and analyzing\nscene-graph-based methodologies. The capabilities of roadscene2vec include (i)\ncustomized scene-graph generation from either video clips or data from the\nCARLA simulator, (ii) multiple configurable spatio-temporal graph embedding\nmodels and baseline CNN-based models, (iii) built-in functionality for using\ngraph and sequence embeddings for risk assessment and collision prediction\napplications, (iv) tools for evaluating transfer learning, and (v) utilities\nfor visualizing scene-graphs and analyzing the explainability of graph learning\nmodels. We demonstrate the utility of roadscene2vec for these use cases with\nexperimental results and qualitative evaluations for both graph learning models\nand CNN-based models. roadscene2vec is available at\nhttps://github.com/AICPS/roadscene2vec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malawade_A/0/1/0/all/0/1\">Arnav Vaibhav Malawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shih-Yuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_B/0/1/0/all/0/1\">Brandon Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaeley_H/0/1/0/all/0/1\">Harsimrat Kaeley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karra_A/0/1/0/all/0/1\">Anurag Karra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Regularization in DCE-MR Image Reconstruction for Functional Imaging of Kidneys. (arXiv:2109.07548v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.07548","description":"<p>Kidney DCE-MRI aims at both qualitative assessment of kidney anatomy and\nquantitative assessment of kidney function by estimating the tracer kinetic\n(TK) model parameters. Accurate estimation of TK model parameters requires an\naccurate measurement of the arterial input function (AIF) with high temporal\nresolution. Accelerated imaging is used to achieve high temporal resolution,\nwhich yields under-sampling artifacts in the reconstructed images. Compressed\nsensing (CS) methods offer a variety of reconstruction options. Most commonly,\nsparsity of temporal differences is encouraged for regularization to reduce\nartifacts. Increasing regularization in CS methods removes the ambient\nartifacts but also over-smooths the signal temporally which reduces the\nparameter estimation accuracy. In this work, we propose a single image trained\ndeep neural network to reduce MRI under-sampling artifacts without reducing the\naccuracy of functional imaging markers. Instead of regularizing with a penalty\nterm in optimization, we promote regularization by generating images from a\nlower dimensional representation. In this manuscript we motivate and explain\nthe lower dimensional input design. We compare our approach to CS\nreconstructions with multiple regularization weights. Proposed approach results\nin kidney biomarkers that are highly correlated with the ground truth markers\nestimated using the CS reconstruction which was optimized for functional\nanalysis. At the same time, the proposed approach reduces the artifacts in the\nreconstructed images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocanaogullari_A/0/1/0/all/0/1\">Aziz Ko&#xe7;anao&#x11f;ullar&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ariyurek_C/0/1/0/all/0/1\">Cemre Ariyurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afacan_O/0/1/0/all/0/1\">Onur Afacan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurugol_S/0/1/0/all/0/1\">Sila Kurugol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-supervised Learning. (arXiv:2109.10563v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10563","description":"<p>Due to difficulties in acquiring ground truth depth of equirectangular (360)\nimages, the quality and quantity of equirectangular depth data today is\ninsufficient to represent the various scenes in the world. Therefore, 360 depth\nestimation studies, which relied solely on supervised learning, are destined to\nproduce unsatisfactory results. Although self-supervised learning methods\nfocusing on equirectangular images (EIs) are introduced, they often have\nincorrect or non-unique solutions, causing unstable performance. In this paper,\nwe propose 360 monocular depth estimation methods which improve on the areas\nthat limited previous studies. First, we introduce a self-supervised 360 depth\nlearning method that only utilizes gravity-aligned videos, which has the\npotential to eliminate the needs for depth data during the training procedure.\nSecond, we propose a joint learning scheme realized by combining supervised and\nself-supervised learning. The weakness of each learning is compensated, thus\nleading to more accurate depth estimation. Third, we propose a non-local fusion\nblock, which can further retain the global information encoded by vision\ntransformer when reconstructing the depths. With the proposed methods, we\nsuccessfully apply the transformer to 360 depth estimations, to the best of our\nknowledge, which has not been tried before. On several benchmarks, our approach\nachieves significant improvements over previous works and establishes a state\nof the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_I/0/1/0/all/0/1\">Ilwi Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyuk-Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_C/0/1/0/all/0/1\">Chae Eun Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation. (arXiv:2109.12271v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.12271","description":"<p>Convolutional neural networks (CNNs) have achieved remarkable success in\nautomatically segmenting organs or lesions on 3D medical images. Recently,\nvision transformer networks have exhibited exceptional performance in 2D image\nclassification tasks. Compared with CNNs, transformer networks have an\nappealing advantage of extracting long-range features due to their\nself-attention algorithm. Therefore, we propose a CNN-Transformer combined\nmodel, called BiTr-Unet, with specific modifications for brain tumor\nsegmentation on multi-modal MRI scans. Our BiTr-Unet achieves good performance\non the BraTS2021 validation dataset with median Dice score 0.9335, 0.9304 and\n0.8899, and median Hausdorff distance 2.8284, 2.2361 and 1.4142 for the whole\ntumor, tumor core, and enhancing tumor, respectively. On the BraTS2021 testing\ndataset, the corresponding results are 0.9257, 0.9350 and 0.8874 for Dice\nscore, and 3, 2.2361 and 1.4142 for Hausdorff distance. The code is publicly\navailable at https://github.com/JustaTinyDot/BiTr-Unet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jia_Q/0/1/0/all/0/1\">Qiran Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shu_H/0/1/0/all/0/1\">Hai Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARKitScenes -- A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data. (arXiv:2111.08897v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08897","description":"<p>Scene understanding is an active research area. Commercial depth sensors,\nsuch as Kinect, have enabled the release of several RGB-D datasets over the\npast few years which spawned novel methods in 3D scene understanding. More\nrecently with the launch of the LiDAR sensor in Apple's iPads and iPhones, high\nquality RGB-D data is accessible to millions of people on a device they\ncommonly use. This opens a whole new era in scene understanding for the\nComputer Vision community as well as app developers. The fundamental research\nin scene understanding together with the advances in machine learning can now\nimpact people's everyday experiences. However, transforming these scene\nunderstanding methods to real-world experiences requires additional innovation\nand development. In this paper we introduce ARKitScenes. It is not only the\nfirst RGB-D dataset that is captured with a now widely available depth sensor,\nbut to our best knowledge, it also is the largest indoor scene understanding\ndata released. In addition to the raw and processed data from the mobile\ndevice, ARKitScenes includes high resolution depth maps captured using a\nstationary laser scanner, as well as manually labeled 3D oriented bounding\nboxes for a large taxonomy of furniture. We further analyze the usefulness of\nthe data for two downstream tasks: 3D object detection and color-guided depth\nupsampling. We demonstrate that our dataset can help push the boundaries of\nexisting state-of-the-art methods and it introduces new challenges that better\nrepresent real-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baruch_G/0/1/0/all/0/1\">Gilad Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghan_A/0/1/0/all/0/1\">Afshin Dehghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimry_T/0/1/0/all/0/1\">Tal Dimry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feigin_Y/0/1/0/all/0/1\">Yuri Feigin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1\">Peter Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebauer_T/0/1/0/all/0/1\">Thomas Gebauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joffe_B/0/1/0/all/0/1\">Brandon Joffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_A/0/1/0/all/0/1\">Arik Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shulman_E/0/1/0/all/0/1\">Elad Shulman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ML-Decoder: Scalable and Versatile Classification Head. (arXiv:2111.12933v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12933","description":"<p>In this paper, we introduce ML-Decoder, a new attention-based classification\nhead. ML-Decoder predicts the existence of class labels via queries, and\nenables better utilization of spatial data compared to global average pooling.\nBy redesigning the decoder architecture, and using a novel group-decoding\nscheme, ML-Decoder is highly efficient, and can scale well to thousands of\nclasses. Compared to using a larger backbone, ML-Decoder consistently provides\na better speed-accuracy trade-off. ML-Decoder is also versatile - it can be\nused as a drop-in replacement for various classification heads, and generalize\nto unseen classes when operated with word queries. Novel query augmentations\nfurther improve its generalization ability. Using ML-Decoder, we achieve\nstate-of-the-art results on several classification tasks: on MS-COCO\nmulti-label, we reach 91.4% mAP; on NUS-WIDE zero-shot, we reach 31.1% ZSL mAP;\nand on ImageNet single-label, we reach with vanilla ResNet50 backbone a new top\nscore of 80.7%, without extra data or distillation. Public code is available\nat: https://github.com/Alibaba-MIIL/ML_Decoder\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharir_G/0/1/0/all/0/1\">Gilad Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Cohen_A/0/1/0/all/0/1\">Avi Ben-Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MutualFormer: Multi-Modality Representation Learning via Mutual Transformer. (arXiv:2112.01177v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01177","description":"<p>Aggregating multi-modality data to obtain accurate and reliable data\nrepresentation attracts more and more attention. The pristine researchers\ngenerally adopt the CNN to extract features of independent modality and\naggregate them with a fusion module. However, the overall performance is\nbecoming saturated due to limited local convolutional features. Recent studies\ndemonstrate that Transformer models usually work comparable or even better than\nCNN for multi-modality task, but they simply adopt concatenation or\ncross-attention for feature fusion which may just obtain sub-optimal results.\nIn this work, we re-thinking the self-attention based Transformer and propose a\nnovel MutualFormer for multi-modality data fusion and representation. The core\nof MutualFormer is the design of both token mixer and modality mixer to conduct\nthe communication among both tokens and modalities. Specifically, it contains\nthree main modules, i.e., i) Self-attention (SA) for intra-modality token\nmixer, ii) Cross-diffusion attention (CDA) for inter-modality mixer and iii)\nAggregation module. The main advantage of the proposed CDA is that it is\ndefined based on individual domain similarities in the metric space which thus\ncan naturally avoid the issue of domain/modality gap in cross-modality\nsimilarities computation. We successfully apply the MutualFormer to the\nsaliency detection problem and propose a novel approach to obtain the\nreinforced features of RGB and Depth images. Extensive experiments on six\npopular datasets demonstrate that our model achieves comparable results with 16\nSOTA models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xixi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep learning based Document Image Enhancement. (arXiv:2112.02719v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02719","description":"<p>Digitized documents such as scientific articles, tax forms, invoices,\ncontract papers, historic texts are widely used nowadays. These document images\ncould be degraded or damaged due to various reasons including poor lighting\nconditions, shadow, distortions like noise and blur, aging, ink stain,\nbleed-through, watermark, stamp, etc. Document image enhancement plays a\ncrucial role as a pre-processing step in many automated document analysis and\nrecognition tasks such as character recognition. With recent advances in deep\nlearning, many methods are proposed to enhance the quality of these document\nimages. In this paper, we review deep learning-based methods, datasets, and\nmetrics for six main document image enhancement tasks, including binarization,\ndebluring, denoising, defading, watermark removal, and shadow removal. We\nsummarize the recent works for each task and discuss their features,\nchallenges, and limitations. We introduce multiple document image enhancement\ntasks that have received little to no attention, including over and under\nexposure correction, super resolution, and bleed-through removal. We identify\nseveral promising research directions and opportunities for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anvari_Z/0/1/0/all/0/1\">Zahra Anvari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAC-GAN: Structure-Aware Image-to-Image Composition for Self-Driving. (arXiv:2112.06596v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06596","description":"<p>We present a compositional approach to image augmentation for self-driving\napplications. It is an end-to-end neural network that is trained to seamlessly\ncompose an object (e.g., a vehicle or pedestrian) represented as a cropped\npatch from an object image, into a background scene image. As our approach\nemphasizes more on semantic and structural coherence of the composed images,\nrather than their pixel-level RGB accuracies, we tailor the input and output of\nour network with structure-aware features and design our network losses\naccordingly. Specifically, our network takes the semantic layout features from\nthe input scene image, features encoded from the edges and silhouette in the\ninput object patch, as well as a latent code as inputs, and generates a 2D\nspatial affine transform defining the translation and scaling of the object\npatch. The learned parameters are further fed into a differentiable spatial\ntransformer network to transform the object patch into the target image, where\nour model is trained adversarially using an affine transform discriminator and\na layout discriminator. We evaluate our network, coined SAC-GAN for\nstructure-aware composition, on prominent self-driving datasets in terms of\nquality, composability, and generalizability of the composite images.\nComparisons are made to state-of-the-art alternatives, confirming superiority\nof our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-FasterSeg: An Efficient Semantic Segmentation Network Based on Neural Architecture Search. (arXiv:2112.07918v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07918","description":"<p>Image semantic segmentation technology is one of the key technologies for\nintelligent systems to understand natural scenes. As one of the important\nresearch directions in the field of visual intelligence, this technology has\nbroad application scenarios in the fields of mobile robots, drones, smart\ndriving, and smart security. However, in the actual application of mobile\nrobots, problems such as inaccurate segmentation semantic label prediction and\nloss of edge information of segmented objects and background may occur. This\npaper proposes an improved structure of a semantic segmentation network based\non a deep learning network that combines self-attention neural network and\nneural network architecture search methods. First, a neural network search\nmethod NAS (Neural Architecture Search) is used to find a semantic segmentation\nnetwork with multiple resolution branches. In the search process, combine the\nself-attention network structure module to adjust the searched neural network\nstructure, and then combine the semantic segmentation network searched by\ndifferent branches to form a fast semantic segmentation network structure, and\ninput the picture into the network structure to get the final forecast result.\nThe experimental results on the Cityscapes dataset show that the accuracy of\nthe algorithm is 69.8%, and the segmentation speed is 48/s. It achieves a good\nbalance between real-time and accuracy, can optimize edge segmentation, and has\na better performance in complex scenes. Good robustness is suitable for\npractical application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1\">Huiyu Kuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEVR3D: Compositional Language and Elementary Visual Reasoning for Question Answering in 3D Real-World Scenes. (arXiv:2112.11691v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11691","description":"<p>3D scene understanding is a relatively emerging research field. In this\npaper, we introduce the Visual Question Answering task in 3D real-world scenes\n(VQA-3D), which aims to answer all possible questions given a 3D scene. To\ntackle this problem, the first VQA-3D dataset, namely CLEVR3D, is proposed,\nwhich contains 60K questions in 1,129 real-world scenes. Specifically, we\ndevelop a question engine leveraging 3D scene graph structures to generate\ndiverse reasoning questions, covering the questions of objects' attributes\n(i.e., size, color, and material) and their spatial relationships. Built upon\nthis dataset, we further design the first VQA-3D baseline model, TransVQA3D.\nThe TransVQA3D model adopts well-designed Transformer architectures to achieve\nsuperior VQA-3D performance, compared with the pure language baseline and\nprevious 3D reasoning methods directly applied to 3D scenarios. Experimental\nresults verify that taking VQA-3D as an auxiliary task can boost the\nperformance of 3D scene understanding, including scene graph analysis for the\nnode-wise classification and whole-graph recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuhao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense anomaly detection by robust learning on synthetic negative data. (arXiv:2112.12833v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12833","description":"<p>Standard machine learning is unable to accommodate inputs which do not belong\nto the training distribution. The resulting models often give rise to confident\nincorrect predictions which may lead to devastating consequences. This problem\nis especially demanding in the context of dense prediction since input images\nmay be partially anomalous. Previous work has addressed dense anomaly detection\nby discriminative training on mixed-content images. We extend this approach\nwith synthetic negative patches which simultaneously achieve high inlier\nlikelihood and uniform discriminative prediction. We generate synthetic\nnegatives with normalizing flows due to their outstanding distribution coverage\nand capability to generate samples at different resolutions. We also propose to\ndetect anomalies according to a principled information-theoretic criterion\nwhich can be consistently applied through training and inference. The resulting\nmodels set the new state of the art on standard benchmarks and datasets in\nspite of minimal computational overhead and refraining from auxiliary negative\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grcic_M/0/1/0/all/0/1\">Matej Grci&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bevandic_P/0/1/0/all/0/1\">Petra Bevandi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalafatic_Z/0/1/0/all/0/1\">Zoran Kalafati&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12970","description":"<p>Scene Graph Generation (SGG) remains a challenging visual understanding task\ndue to its complex compositional property. Most previous works adopt a\nbottom-up two-stage or a point-based one-stage approach, which often suffers\nfrom overhead time complexity or sub-optimal design assumption. In this work,\nwe propose a novel SGG method to address the aforementioned issues, which\nformulates the task as a bipartite graph construction problem. To solve the\nproblem, we develop a transformer-based end-to-end framework that first\ngenerates the entity and predicate proposal set, followed by inferring directed\nedges to form the relation triplets. In particular, we develop a new\nentity-aware predicate representation based on a structural predicate generator\nto leverage the compositional property of relationships. Moreover, we design a\ngraph assembling module to infer the connectivity of the bipartite scene graph\nbased on our entity-aware structure, enabling us to generate the scene graph in\nan end-to-end manner. Extensive experimental results show that our design is\nable to achieve the state-of-the-art or comparable performance on two\nchallenging benchmarks, surpassing most of the existing approaches and enjoying\nhigher efficiency in inference. We hope our model can serve as a strong\nbaseline for the Transformer-based scene graph generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human View Synthesis using a Single Sparse RGB-D Input. (arXiv:2112.13889v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13889","description":"<p>Novel view synthesis for humans in motion is a challenging computer vision\nproblem that enables applications such as free-viewpoint video. Existing\nmethods typically use complex setups with multiple input views, 3D supervision,\nor pre-trained models that do not generalize well to new identities. Aiming to\naddress these limitations, we present a novel view synthesis framework to\ngenerate realistic renders from unseen views of any human captured from a\nsingle-view sensor with sparse RGB-D, similar to a low-cost depth camera, and\nwithout actor-specific models. We propose an architecture to learn dense\nfeatures in novel views obtained by sphere-based neural rendering, and create\ncomplete renders using a global context inpainting model. Additionally, an\nenhancer network leverages the overall fidelity, even in occluded areas from\nthe original view, producing crisp renders with fine details. We show our\nmethod generates high-quality novel views of synthetic and real human actors\ngiven a single sparse RGB-D input. It generalizes to unseen identities, new\nposes and faithfully reconstructs facial expressions. Our approach outperforms\nprior human view synthesis methods and is robust to different levels of input\nsparsity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1\">Nikolaos Sarafianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaGraspNet: A Large-Scale Benchmark Dataset for Vision-driven Robotic Grasping via Physics-based Metaverse Synthesis. (arXiv:2112.14663v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14663","description":"<p>There has been increasing interest in smart factories powered by robotics\nsystems to tackle repetitive, laborious tasks. One impactful yet challenging\ntask in robotics-powered smart factory applications is robotic grasping: using\nrobotic arms to grasp objects autonomously in different settings. Robotic\ngrasping requires a variety of computer vision tasks such as object detection,\nsegmentation, grasp prediction, pick planning, etc. While significant progress\nhas been made in leveraging of machine learning for robotic grasping,\nparticularly with deep learning, a big challenge remains in the need for\nlarge-scale, high-quality RGBD datasets that cover a wide diversity of\nscenarios and permutations. To tackle this big, diverse data problem, we are\ninspired by the recent rise in the concept of metaverse, which has greatly\nclosed the gap between virtual worlds and the physical world. Metaverses allow\nus to create digital twins of real-world manufacturing scenarios and to\nvirtually create different scenarios from which large volumes of data can be\ngenerated for training models. In this paper, we present MetaGraspNet: a\nlarge-scale benchmark dataset for vision-driven robotic grasping via\nphysics-based metaverse synthesis. The proposed dataset contains 100,000 images\nand 25 different object types and is split into 5 difficulties to evaluate\nobject detection and segmentation model performance in different grasping\nscenarios. We also propose a new layout-weighted performance metric alongside\nthe dataset for evaluating object detection and segmentation performance in a\nmanner that is more appropriate for robotic grasp applications compared to\nexisting general-purpose performance metrics. Our benchmark dataset is\navailable open-source on Kaggle, with the first phase consisting of detailed\nobject detection, segmentation, layout annotations, and a layout-weighted\nperformance metric script.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_E/0/1/0/all/0/1\">E. Zhixuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilles_M/0/1/0/all/0/1\">Maximilian Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Music-to-Dance Generation with Optimal Transport. (arXiv:2112.01806v1 [cs.SD] CROSS LISTED)","link":"http://arxiv.org/abs/2112.01806","description":"<p>Dance choreography for a piece of music is a challenging task, having to be\ncreative in presenting distinctive stylistic dance elements while taking into\naccount the musical theme and rhythm. It has been tackled by different\napproaches such as similarity retrieval, sequence-to-sequence modeling and\ngenerative adversarial networks, but their generated dance sequences are often\nshort of motion realism, diversity and music consistency. In this paper, we\npropose a Music-to-Dance with Optimal Transport Network (MDOT-Net) for learning\nto generate 3D dance choreographs from music. We introduce an optimal transport\ndistance for evaluating the authenticity of the generated dance distribution\nand a Gromov-Wasserstein distance to measure the correspondence between the\ndance distribution and the input music. This gives a well defined and\nnon-divergent training objective that mitigates the limitation of standard GAN\ntraining which is frequently plagued with instability and divergent generator\nloss issues. Extensive experiments demonstrate that our MDOT-Net can synthesize\nrealistic and diverse dances which achieve an organic unity with the input\nmusic, reflecting the shared intentionality and matching the rhythmic\narticulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPViT: Enabling Faster Vision Transformers via Soft Token Pruning. (arXiv:2112.13890v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2112.13890","description":"<p>Recently, Vision Transformer (ViT) has continuously established new\nmilestones in the computer vision field, while the high computation and memory\ncost makes its propagation in industrial production difficult. Pruning, a\ntraditional model compression paradigm for hardware efficiency, has been widely\napplied in various DNN structures. Nevertheless, it stays ambiguous on how to\nperform exclusive pruning on the ViT structure. Considering three key points:\nthe structural characteristics, the internal data pattern of ViTs, and the\nrelated edge device deployment, we leverage the input token sparsity and\npropose a computation-aware soft pruning framework, which can be set up on\nvanilla Transformers of both flatten and CNN-type structures, such as\nPooling-based ViT (PiT). More concretely, we design a dynamic attention-based\nmulti-head token selector, which is a lightweight module for adaptive\ninstance-wise token selection. We further introduce a soft pruning technique,\nwhich integrates the less informative tokens generated by the selector module\ninto a package token that will participate in subsequent calculations rather\nthan being completely discarded. Our framework is bound to the trade-off\nbetween accuracy and computation constraints of specific edge devices through\nour proposed computation-aware training strategy. Experimental results show\nthat our framework significantly reduces the computation cost of ViTs while\nmaintaining comparable performance on image classification. Moreover, our\nframework can guarantee the identified model to meet resource specifications of\nmobile devices and FPGA, and even achieve the real-time execution of DeiT-T on\nmobile platforms. For example, our method reduces the latency of DeiT-T to 26\nms (26%$\\sim $41% superior to existing works) on the mobile device with\n0.25%$\\sim $4% higher top-1 accuracy on ImageNet. Our code will be released\nsoon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenglun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peiyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mengshu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}