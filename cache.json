{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?. (arXiv:2203.08850v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08850","description":"<p>What can pre-trained multilingual sequence-to-sequence models like mBART\ncontribute to translating low-resource languages? We conduct a thorough\nempirical experiment in 10 languages to ascertain this, considering five\nfactors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning\ndata, (3) the amount of pre-training data in the model, (4) the impact of\ndomain mismatch, and (5) language typology. In addition to yielding several\nheuristics, the experiments form a framework for evaluating the data\nsensitivities of machine translation systems. While mBART is robust to domain\ndifferences, its translations for unseen and typologically distant languages\nremain below 3.0 BLEU. In answer to our title's question, mBART is not a\nlow-resource panacea; we therefore encourage shifting the emphasis from new\nmodels to new data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">En-Shiun Annie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thillainathan_S/0/1/0/all/0/1\">Sarubi Thillainathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Shravan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruisi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_A/0/1/0/all/0/1\">Arya D. McCarthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Learning on Graphs for Disease Relation Extraction. (arXiv:2203.08893v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08893","description":"<p>Objective: Disease knowledge graphs are a way to connect, organize, and\naccess disparate information about diseases with numerous benefits for\nartificial intelligence (AI). To create knowledge graphs, it is necessary to\nextract knowledge from multimodal datasets in the form of relationships between\ndisease concepts and normalize both concepts and relationship types. Methods:\nWe introduce REMAP, a multimodal approach for disease relation extraction and\nclassification. The REMAP machine learning approach jointly embeds a partial,\nincomplete knowledge graph and a medical language dataset into a compact latent\nvector space, followed by aligning the multimodal embeddings for optimal\ndisease relation extraction. Results: We apply REMAP approach to a disease\nknowledge graph with 96,913 relations and a text dataset of 1.24 million\nsentences. On a dataset annotated by human experts, REMAP improves text-based\ndisease relation extraction by 10.0% (accuracy) and 17.2% (F1-score) by fusing\ndisease knowledge graphs with text information. Further, REMAP leverages text\ninformation to recommend new relationships in the knowledge graph,\noutperforming graph-based methods by 8.4% (accuracy) and 10.4% (F1-score).\nDiscussion: Systematized knowledge is becoming the backbone of AI, creating\nopportunities to inject semantics into AI and fully integrate it into machine\nlearning algorithms. While prior semantic knowledge can assist in extracting\ndisease relationships from text, existing methods can not fully leverage\nmultimodal datasets. Conclusion: REMAP is a multimodal approach for extracting\nand classifying disease relationships by fusing structured knowledge and text\ninformation. REMAP provides a flexible neural architecture to easily find,\naccess, and validate AI-driven relationships between disease concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yucong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianxi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1\">Marinka Zitnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphological Processing of Low-Resource Languages: Where We Are and What's Next. (arXiv:2203.08909v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08909","description":"<p>Automatic morphological processing can aid downstream natural language\nprocessing applications, especially for low-resource languages, and assist\nlanguage documentation efforts for endangered languages. Having long been\nmultilingual, the field of computational morphology is increasingly moving\ntowards approaches suitable for languages with minimal or no annotated\nresources. First, we survey recent developments in computational morphology\nwith a focus on low-resource languages. Second, we argue that the field is\nready to tackle the logical next challenge: understanding a language's\nmorphology from raw text alone. We perform an empirical study on a truly\nunsupervised version of the paradigm completion task and show that, while\nexisting state-of-the-art models bridged by two newly proposed models we devise\nperform reasonably, there is still much room for improvement. The stakes are\nhigh: solving this task will increase the language coverage of morphological\nresources by a number of magnitudes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiemerslage_A/0/1/0/all/0/1\">Adam Wiemerslage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silfverberg_M/0/1/0/all/0/1\">Miikka Silfverberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Changbing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_A/0/1/0/all/0/1\">Arya D. McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolai_G/0/1/0/all/0/1\">Garrett Nicolai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colunga_E/0/1/0/all/0/1\">Eliana Colunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memorizing Transformers. (arXiv:2203.08913v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08913","description":"<p>Language models typically need to be trained or finetuned in order to acquire\nnew knowledge, which involves updating their weights. We instead envision\nlanguage models that can simply read and memorize new data at inference time,\nthus acquiring new knowledge immediately. In this work, we extend language\nmodels with the ability to memorize the internal representations of past\ninputs. We demonstrate that an approximate kNN lookup into a non-differentiable\nmemory of recent (key, value) pairs improves language modeling across various\nbenchmarks and tasks, including generic webtext (C4), math papers (arXiv),\nbooks (PG-19), code (Github), as well as formal theorems (Isabelle). We show\nthat the performance steadily improves when we increase the size of memory up\nto 262K tokens. On benchmarks including code and mathematics, we find that the\nmodel is capable of making use of newly defined functions and theorems during\ntest time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabe_M/0/1/0/all/0/1\">Markus N. Rabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchins_D/0/1/0/all/0/1\">DeLesley Hutchins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szegedy_C/0/1/0/all/0/1\">Christian Szegedy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Question Value Estimation for Domain Adaptation of Question Answering. (arXiv:2203.08926v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08926","description":"<p>Synthesizing QA pairs with a question generator (QG) on the target domain has\nbecome a popular approach for domain adaptation of question answering (QA)\nmodels. Since synthetic questions are often noisy in practice, existing work\nadapts scores from a pretrained QA (or QG) model as criteria to select\nhigh-quality questions. However, these scores do not directly serve the\nultimate goal of improving QA performance on the target domain. In this paper,\nwe introduce a novel idea of training a question value estimator (QVE) that\ndirectly estimates the usefulness of synthetic questions for improving the\ntarget-domain QA performance. By conducting comprehensive experiments, we show\nthat the synthetic questions selected by QVE can help achieve better\ntarget-domain QA performance, in comparison with existing techniques. We\nadditionally show that by using such questions and only around 15% of the human\nannotations on the target domain, we can achieve comparable performance to the\nfully-supervised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References. (arXiv:2203.08928v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08928","description":"<p>We consider the problem of pretraining a two-stage open-domain question\nanswering (QA) system (retriever + reader) with strong transfer capabilities.\nThe key challenge is how to construct a large amount of high-quality\nquestion-answer-context triplets without task-specific annotations.\nSpecifically, the triplets should align well with downstream tasks by: (i)\ncovering a wide range of domains (for open-domain applications), (ii) linking a\nquestion to its semantically relevant context with supporting evidence (for\ntraining the retriever), and (iii) identifying the correct answer in the\ncontext (for training the reader). Previous pretraining approaches generally\nfall short of one or more of these requirements. In this work, we automatically\nconstruct a large-scale corpus that meets all three criteria by consulting\nmillions of references cited within Wikipedia. The well-aligned pretraining\nsignals benefit both the retriever and the reader significantly. Our pretrained\nretriever leads to 2%-10% absolute gains in top-20 accuracy. And with our\npretrained reader, the entire system improves by up to 4% in exact match.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Negation in Natural Language Understanding Corpora. (arXiv:2203.08929v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08929","description":"<p>This paper analyzes negation in eight popular corpora spanning six natural\nlanguage understanding tasks. We show that these corpora have few negations\ncompared to general-purpose English, and that the few negations in them are\noften unimportant. Indeed, one can often ignore negations and still make the\nright predictions. Additionally, experimental results show that\nstate-of-the-art transformers trained with these corpora obtain substantially\nworse results with instances that contain negation, especially if the negations\nare important. We conclude that new corpora accounting for negation are needed\nto solve natural language understanding tasks when negation is present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Mosharaf Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnappa_D/0/1/0/all/0/1\">Dhivya Chinnappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_E/0/1/0/all/0/1\">Eduardo Blanco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating Multimedia Summaries Using Tweets and Videos. (arXiv:2203.08931v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08931","description":"<p>While popular televised events such as presidential debates or TV shows are\nairing, people provide commentary on them in real-time. In this paper, we\npropose a simple yet effective approach to combine social media commentary and\nvideos to create a multimedia summary of televised events. Our approach\nidentifies scenes from these events based on spikes of mentions of people\ninvolved in the event and automatically selects tweets and frames from the\nvideos that occur during the time period of the spike that talk about and show\nthe people being discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andy_A/0/1/0/all/0/1\">Anietie Andy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kriz_R/0/1/0/all/0/1\">Reno Kriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BPE vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages. (arXiv:2203.08954v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08954","description":"<p>Morphologically-rich polysynthetic languages present a challenge for NLP\nsystems due to data sparsity, and a common strategy to handle this issue is to\napply subword segmentation. We investigate a wide variety of supervised and\nunsupervised morphological segmentation methods for four polysynthetic\nlanguages: Nahuatl, Raramuri, Shipibo-Konibo, and Wixarika. Then, we compare\nthe morphologically inspired segmentation methods against Byte-Pair Encodings\n(BPEs) as inputs for machine translation (MT) when translating to and from\nSpanish. We show that for all language pairs except for Nahuatl, an\nunsupervised morphological segmentation algorithm outperforms BPEs consistently\nand that, although supervised methods achieve better segmentation scores, they\nunder-perform in MT challenges. Finally, we contribute two new morphological\nsegmentation datasets for Raramuri and Shipibo-Konibo, and a parallel corpus\nfor Raramuri--Spanish.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mager_M/0/1/0/all/0/1\">Manuel Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oncevay_A/0/1/0/all/0/1\">Arturo Oncevay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mager_E/0/1/0/all/0/1\">Elisabeth Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker Information Can Guide Models to Better Inductive Biases: A Case Study On Predicting Code-Switching. (arXiv:2203.08979v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08979","description":"<p>Natural language processing (NLP) models trained on people-generated data can\nbe unreliable because, without any constraints, they can learn from spurious\ncorrelations that are not relevant to the task. We hypothesize that enriching\nmodels with speaker information in a controlled, educated way can guide them to\npick up on relevant inductive biases. For the speaker-driven task of predicting\ncode-switching points in English--Spanish bilingual dialogues, we show that\nadding sociolinguistically-grounded speaker features as prepended prompts\nsignificantly improves accuracy. We find that by adding influential phrases to\nthe input, speaker-informed models learn useful and explainable linguistic\ninformation. To our knowledge, we are the first to incorporate speaker\ncharacteristics in a neural model for code-switching, and more generally, take\na step towards developing transparent, personalized models that use speaker\ninformation in a controlled way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ostapenko_A/0/1/0/all/0/1\">Alissa Ostapenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1\">Shuly Wintner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fricke_M/0/1/0/all/0/1\">Melinda Fricke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Semantics for Few Shot Named Entity Recognition. (arXiv:2203.08985v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08985","description":"<p>We study the problem of few shot learning for named entity recognition.\nSpecifically, we leverage the semantic information in the names of the labels\nas a way of giving the model additional signal and enriched priors. We propose\na neural architecture that consists of two BERT encoders, one to encode the\ndocument and its tokens and another one to encode each of the labels in natural\nlanguage format. Our model learns to match the representations of named\nentities computed by the first encoder with label representations computed by\nthe second encoder. The label semantics signal is shown to support improved\nstate-of-the-art results in multiple few shot NER benchmarks and on-par\nperformance in standard benchmarks. Our model is especially effective in low\nresource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doss_S/0/1/0/all/0/1\">Srikanth Doss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anubhai_R/0/1/0/all/0/1\">Rishita Anubhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallya_S/0/1/0/all/0/1\">Sunil Mallya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Onaizan_Y/0/1/0/all/0/1\">Yaser Al-Onaizan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapLeR: Speeding up Inference by Adaptive Length Reduction. (arXiv:2203.08991v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08991","description":"<p>Pre-trained language models have shown stellar performance in various\ndownstream tasks. But, this usually comes at the cost of high latency and\ncomputation, hindering their usage in resource-limited settings. In this work,\nwe propose a novel approach for reducing the computational cost of BERT with\nminimal loss in downstream performance. Our method dynamically eliminates less\ncontributing tokens through layers, resulting in shorter lengths and\nconsequently lower computational cost. To determine the importance of each\ntoken representation, we train a Contribution Predictor for each layer using a\ngradient-based saliency method. Our experiments on several diverse\nclassification tasks show speedups up to 22x during inference time without much\nsacrifice in performance. We also validate the quality of the selected tokens\nin our method using human annotations in the ERASER benchmark. In comparison to\nother widely used strategies for selecting important tokens, such as saliency\nand attention, our proposed method has a significantly lower false positive\nrate in generating rationales. Our code is freely available at\nhttps://github.com/amodaresi/AdapLeR .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modarressi_A/0/1/0/all/0/1\">Ali Modarressi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohebbi_H/0/1/0/all/0/1\">Hosein Mohebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension. (arXiv:2203.08992v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08992","description":"<p>Recent machine reading comprehension datasets such as ReClor and LogiQA\nrequire performing logical reasoning over text. Conventional neural models are\ninsufficient for logical reasoning, while symbolic reasoners cannot directly\napply to text. To meet the challenge, we present a neural-symbolic approach\nwhich, to predict an answer, passes messages over a graph representing logical\nrelations between text units. It incorporates an adaptive logic graph network\n(AdaLoGN) which adaptively infers logical relations to extend the graph and,\nessentially, realizes mutual and iterative reinforcement between neural and\nsymbolic reasoning. We also implement a novel subgraph-to-node message passing\nmechanism to enhance context-option interaction for answering multiple-choice\nquestions. Our approach shows promising results on ReClor and LogiQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yawei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Autonomy: Self-Initiation, Adaptation and Continual Learning. (arXiv:2203.08994v1 [cs.AI])","link":"http://arxiv.org/abs/2203.08994","description":"<p>As more and more AI agents are used in practice, it is time to think about\nhow to make these agents fully autonomous so that they can (1) learn by\nthemselves continually in a self-motivated and self-initiated manner rather\nthan being retrained offline periodically on the initiation of human engineers\nand (2) accommodate or adapt to unexpected or novel circumstances. As the\nreal-world is an open environment that is full of unknowns or novelties,\ndetecting novelties, characterizing them, accommodating or adapting to them,\nand gathering ground-truth training data and incrementally learning the\nunknowns/novelties are critical to making the AI agent more and more\nknowledgeable and powerful over time. The key challenge is how to automate the\nprocess so that it is carried out continually on the agent's own initiative and\nthrough its own interactions with humans, other agents and the environment just\nlike human on-the-job learning. This paper proposes a framework (called SOLA)\nfor this learning paradigm to promote the research of building autonomous and\ncontinual learning enabled AI agents. To show feasibility, an implemented agent\nis also described.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1\">Sahisnu Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1\">Eric Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigsby_S/0/1/0/all/0/1\">Scott Grigsby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Communication with a Teachable Agent. (arXiv:2203.09016v1 [cs.HC])","link":"http://arxiv.org/abs/2203.09016","description":"<p>Conversational teachable agents offer a promising platform to support\nlearning, both in the classroom and in remote settings. In this context, the\nagent takes the role of the novice, while the student takes on the role of\nteacher. This framing is significant for its ability to elicit the Prot\\'eg\\'e\neffect in the student-teacher, a pedagogical phenomenon known to increase\nengagement in the teaching task, and also improve cognitive outcomes. In prior\nwork, teachable agents often take a passive role in the learning interaction,\nand there are few studies in which the agent and student engage in natural\nlanguage dialogue during the teaching task. This work investigates the effect\nof teaching modality when interacting with a virtual agent, via the web-based\nteaching platform, the Curiosity Notebook. A method of teaching the agent by\nselecting sentences from source material is compared to a method paraphrasing\nthe source material and typing text input to teach. A user study has been\nconducted to measure the effect teaching modality on the learning outcomes and\nengagement of the participants. The results indicate that teaching via\nparaphrasing and text input has a positive effect on learning outcomes for the\nmaterial covered, and also on aspects of affective engagement. Furthermore,\nincreased paraphrasing effort, as measured by the similarity between the source\nmaterial and the material the teacher conveyed to the robot, improves learning\noutcomes for participants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Love_R/0/1/0/all/0/1\">Rachel Love</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Law_E/0/1/0/all/0/1\">Edith Law</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_P/0/1/0/all/0/1\">Philip R. Cohen</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1\">Dana Kuli&#x107;</a> (1) ((1) Monash University, (2) University of Waterloo, (3) Openstream Inc)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triangular Transfer: Freezing the Pivot for Triangular Machine Translation. (arXiv:2203.09027v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09027","description":"<p>Triangular machine translation is a special case of low-resource machine\ntranslation where the language pair of interest has limited parallel data, but\nboth languages have abundant parallel data with a pivot language. Naturally,\nthe key to triangular machine translation is the successful exploitation of\nsuch auxiliary data. In this work, we propose a transfer-learning-based\napproach that utilizes all types of auxiliary data. As we train auxiliary\nsource-pivot and pivot-target translation models, we initialize some parameters\nof the pivot side with a pre-trained language model and freeze them to\nencourage both translation models to work in the same pivot language space, so\nthat they can be smoothly transferred to the source-target translation model.\nExperiments show that our approach can outperform previous ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training. (arXiv:2203.09052v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09052","description":"<p>Due to the limitations of the model structure and pre-training objectives,\nexisting vision-and-language generation models cannot utilize pair-wise images\nand text through bi-directional generation. In this paper, we propose DU-VLG, a\nframework which unifies vision-and-language generation as sequence generation\nproblems. DU-VLG is trained with novel dual pre-training tasks: multi-modal\ndenoising autoencoder tasks and modality translation tasks. To bridge the gap\nbetween image understanding and generation, we further design a novel\ncommitment loss. We compare pre-training objectives on image captioning and\ntext-to-image generation datasets. Results show that DU-VLG yields better\nperformance than variants trained with uni-directional generation objectives or\nthe variant without the commitment loss. We also obtain higher scores compared\nto previous state-of-the-art systems on three vision-and-language generation\ntasks. In addition, human judges further confirm that our model generates real\nand relevant images as well as faithful and informative captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Luyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guocheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework. (arXiv:2203.09053v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09053","description":"<p>Simultaneous machine translation (SiMT) starts translating while receiving\nthe streaming source inputs, and hence the source sentence is always incomplete\nduring translating. Different from the full-sentence MT using the conventional\nseq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,\nwhich forces each target word to only align with a partial source prefix to\nadapt to the incomplete source in streaming inputs. However, the source words\nin the front positions are always illusoryly considered more important since\nthey appear in more prefixes, resulting in position bias, which makes the model\npay more attention on the front source positions in testing. In this paper, we\nfirst analyze the phenomenon of position bias in SiMT, and develop a\nLength-Aware Framework to reduce the position bias by bridging the structural\ngap between SiMT and full-sentence MT. Specifically, given the streaming\ninputs, we first predict the full-sentence length and then fill the future\nsource position with positional encoding, thereby turning the streaming inputs\ninto a pseudo full-sentence. The proposed framework can be integrated into most\nexisting SiMT methods to further improve performance. Experiments on two\nrepresentative SiMT methods, including the state-of-the-art adaptive policy,\nshow that our method successfully reduces the position bias and achieves better\nSiMT performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT. (arXiv:2203.09055v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09055","description":"<p>Transformer-based pre-trained models, such as BERT, have shown extraordinary\nsuccess in achieving state-of-the-art results in many natural language\nprocessing applications. However, deploying these models can be prohibitively\ncostly, as the standard self-attention mechanism of the Transformer suffers\nfrom quadratic computational cost in the input sequence length. To confront\nthis, we propose FCA, a fine- and coarse-granularity hybrid self-attention that\nreduces the computation cost through progressively shortening the computational\nsequence length in self-attention. Specifically, FCA conducts an\nattention-based scoring strategy to determine the informativeness of tokens at\neach layer. Then, the informative tokens serve as the fine-granularity\ncomputing units in self-attention and the uninformative tokens are replaced\nwith one or several clusters as the coarse-granularity computing units in\nself-attention. Experiments on GLUE and RACE datasets show that BERT with FCA\nachieves 2x reduction in FLOPs over original BERT with &lt;1% loss in accuracy. We\nshow that FCA offers a significantly better trade-off between accuracy and\nFLOPs compared to prior methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIMO-2: End-to-End Unified Vision-Language Grounded Learning. (arXiv:2203.09067v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09067","description":"<p>Vision-Language Pre-training (VLP) has achieved impressive performance on\nvarious cross-modal downstream tasks. However, most existing methods can only\nlearn from aligned image-caption data and rely heavily on expensive regional\nfeatures, which greatly limits their scalability and performance. In this\npaper, we propose an end-to-end unified-modal pre-training framework, namely\nUNIMO-2, for joint learning on both aligned image-caption data and unaligned\nimage-only and text-only corpus. We build a unified Transformer model to\njointly learn visual representations, textual representations and semantic\nalignment between images and texts. In particular, we propose to conduct\ngrounded learning on both images and texts via a sharing grounded space, which\nhelps bridge unaligned images and texts, and align the visual and textual\nsemantic spaces on different types of corpora. The experiments show that our\ngrounded learning method can improve textual and visual semantic alignment for\nimproving performance on various cross-modal tasks. Moreover, benefiting from\neffective joint modeling of different types of corpora, our model also achieves\nimpressive performance on single-modal visual and textual tasks. Our code and\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Can Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guocheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaussian Multi-head Attention for Simultaneous Machine Translation. (arXiv:2203.09072v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09072","description":"<p>Simultaneous machine translation (SiMT) outputs translation while receiving\nthe streaming source inputs, and hence needs a policy to determine where to\nstart translating. The alignment between target and source words often implies\nthe most informative source word for each target word, and hence provides the\nunified control over translation quality and latency, but unfortunately the\nexisting SiMT methods do not explicitly model the alignment to perform the\ncontrol. In this paper, we propose Gaussian Multi-head Attention (GMA) to\ndevelop a new SiMT policy by modeling alignment and translation in a unified\nmanner. For SiMT policy, GMA models the aligned source position of each target\nword, and accordingly waits until its aligned position to start translating. To\nintegrate the learning of alignment into the translation model, a Gaussian\ndistribution centered on predicted aligned position is introduced as an\nalignment-related prior, which cooperates with translation-related soft\nattention to determine the final attention. Experiments on En-Vi and De-En\ntasks show that our method outperforms strong baselines on the trade-off\nbetween translation and latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ask to Understand: Question Generation for Multi-hop Question Answering. (arXiv:2203.09073v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09073","description":"<p>Multi-hop Question Answering (QA) requires the machine to answer complex\nquestions by finding scattering clues and reasoning from multiple documents.\nGraph Network (GN) and Question Decomposition (QD) are two common approaches at\npresent. The former uses the \"black-box\" reasoning process to capture the\npotential relationship between entities and sentences, thus achieving good\nperformance. At the same time, the latter provides a clear reasoning logical\nroute by decomposing multi-hop questions into simple single-hop sub-questions.\nIn this paper, we propose a novel method to complete multi-hop QA from the\nperspective of Question Generation (QG). Specifically, we carefully design an\nend-to-end QG module on the basis of a classical QA module, which could help\nthe model understand the context by asking inherently logical sub-questions,\nthus inheriting interpretability from the QD-based method and showing superior\nperformance. Experiments on the HotpotQA dataset demonstrate that the\neffectiveness of our proposed QG module, human evaluation further clarifies its\ninterpretability quantitatively, and thorough analysis shows that the QG module\ncould generate better sub-questions than QD methods in terms of fluency,\nconsistency, and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yizhe Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLANET: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation. (arXiv:2203.09100v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09100","description":"<p>Despite recent progress of pre-trained language models on generating fluent\ntext, existing methods still suffer from incoherence problems in long-form text\ngeneration tasks that require proper content control and planning to form a\ncoherent high-level logical flow. In this work, we propose PLANET, a novel\ngeneration framework leveraging autoregressive self-attention mechanism to\nconduct content planning and surface realization dynamically. To guide the\ngeneration of output sentences, our framework enriches the Transformer decoder\nwith latent representations to maintain sentence-level semantic plans grounded\nby bag-of-words. Moreover, we introduce a new coherence-based contrastive\nlearning objective to further improve the coherence of output. Extensive\nexperiments are conducted on two challenging long-form text generation tasks\nincluding counterargument generation and opinion article generation. Both\nautomatic and human evaluations show that our method significantly outperforms\nstrong baselines and generates more coherent texts with richer contents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction. (arXiv:2203.09101v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09101","description":"<p>Despite the importance of relation extraction in building and representing\nknowledge, less research is focused on generalizing to unseen relations types.\nWe introduce the task setting of Zero-Shot Relation Triplet Extraction\n(ZeroRTE) to encourage further research in low-resource relation extraction\nmethods. Given an input sentence, each extracted triplet consists of the head\nentity, relation label, and tail entity where the relation label is not seen at\nthe training stage. To solve ZeroRTE, we propose to synthesize relation\nexamples by prompting language models to generate structured texts. Concretely,\nwe unify language model prompts and structured text approaches to design a\nstructured prompt template for generating synthetic relation samples when\nconditioning on relation label prompts (RelationPrompt). To overcome the\nlimitation for extracting multiple relation triplets in a sentence, we design a\nnovel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL\ndatasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot\nrelation classification. Our code and data are available at\ngithub.com/declare-lab/RelationPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph-Enabled Text-Based Automatic Personality Prediction. (arXiv:2203.09103v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09103","description":"<p>How people think, feel, and behave, primarily is a representation of their\npersonality characteristics. By being conscious of personality characteristics\nof individuals whom we are dealing with or decided to deal with, one can\ncompetently ameliorate the relationship, regardless of its type. With the rise\nof Internet-based communication infrastructures (social networks, forums,\netc.), a considerable amount of human communications take place there. The most\nprominent tool in such communications, is the language in written and spoken\nform that adroitly encodes all those essential personality characteristics of\nindividuals. Text-based Automatic Personality Prediction (APP) is the automated\nforecasting of the personality of individuals based on the generated/exchanged\ntext contents. This paper presents a novel knowledge graph-enabled approach to\ntext-based APP that relies on the Big Five personality traits. To this end,\ngiven a text a knowledge graph which is a set of interlinked descriptions of\nconcepts, was built through matching the input text's concepts with DBpedia\nknowledge base entries. Then, due to achieving more powerful representation the\ngraph was enriched with the DBpedia ontology, NRC Emotion Intensity Lexicon,\nand MRC psycholinguistic database information. Afterwards, the knowledge graph\nwhich is now a knowledgeable alternative for the input text was embedded to\nyield an embedding matrix. Finally, to perform personality predictions the\nresulting embedding matrix was fed to four suggested deep learning models\nindependently, which are based on convolutional neural network (CNN), simple\nrecurrent neural network (RNN), long short term memory (LSTM) and bidirectional\nlong short term memory (BiLSTM). The results indicated a considerable\nimprovements in prediction accuracies in all of the suggested classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohammad-Ali Balafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Community-Driven Comprehensive Scientific Paper Summarization: Insight from cvpaper.challenge. (arXiv:2203.09109v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09109","description":"<p>The present paper introduces a group activity involving writing summaries of\nconference proceedings by volunteer participants. The rapid increase in\nscientific papers is a heavy burden for researchers, especially non-native\nspeakers, who need to survey scientific literature. To alleviate this problem,\nwe organized a group of non-native English speakers to write summaries of\npapers presented at a computer vision conference to share the knowledge of the\npapers read by the group. We summarized a total of 2,000 papers presented at\nthe Conference on Computer Vision and Pattern Recognition, a top-tier\nconference on computer vision, in 2019 and 2020. We quantitatively analyzed\nparticipants' selection regarding which papers they read among the many\navailable papers. The experimental results suggest that we can summarize a wide\nrange of papers without asking participants to read papers unrelated to their\ninterests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_S/0/1/0/all/0/1\">Shintaro Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kataoka_H/0/1/0/all/0/1\">Hirokatsu Kataoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_R/0/1/0/all/0/1\">Ryota Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinagawa_S/0/1/0/all/0/1\">Seitaro Shinagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishima_S/0/1/0/all/0/1\">Shigeo Morishima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time and the Value of Data. (arXiv:2203.09118v1 [cs.LG])","link":"http://arxiv.org/abs/2203.09118","description":"<p>Managers often believe that collecting more data will continually improve the\naccuracy of their machine learning models. However, we argue in this paper that\nwhen data lose relevance over time, it may be optimal to collect a limited\namount of recent data instead of keeping around an infinite supply of older\n(less relevant) data. In addition, we argue that increasing the stock of data\nby including older datasets may, in fact, damage the model's accuracy.\nExpectedly, the model's accuracy improves by increasing the flow of data\n(defined as data collection rate); however, it requires other tradeoffs in\nterms of refreshing or retraining machine learning models more frequently.\n</p>\n<p>Using these results, we investigate how the business value created by machine\nlearning models scales with data and when the stock of data establishes a\nsustainable competitive advantage. We argue that data's time-dependency weakens\nthe barrier to entry that the stock of data creates. As a result, a competing\nfirm equipped with a limited (yet sufficient) amount of recent data can develop\nmore accurate models. This result, coupled with the fact that older datasets\nmay deteriorate models' accuracy, suggests that created business value doesn't\nscale with the stock of available data unless the firm offloads less relevant\ndata from its data repository. Consequently, a firm's growth policy should\nincorporate a balance between the stock of historical data and the flow of new\ndata.\n</p>\n<p>We complement our theoretical results with an experiment. In the experiment,\nwe empirically measure the loss in the accuracy of a next word prediction model\ntrained on datasets from various time periods. Our empirical measurements\nconfirm the economic significance of the value decline over time. For example,\n100MB of text data, after seven years, becomes as valuable as 50MB of current\ndata for the next word prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valavi_E/0/1/0/all/0/1\">Ehsan Valavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1\">Joel Hestness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardalani_N/0/1/0/all/0/1\">Newsha Ardalani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iansiti_M/0/1/0/all/0/1\">Marco Iansiti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POLARIS: A Geographic Pre-trained Model and its Applications in Baidu Maps. (arXiv:2203.09127v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09127","description":"<p>Pre-trained models (PTMs) have become a fundamental backbone for downstream\ntasks in natural language processing and computer vision. Despite initial gains\nthat were obtained by applying generic PTMs to geo-related tasks at Baidu Maps,\na clear performance plateau over time was observed. One of the main reasons for\nthis plateau is the lack of readily available geographic knowledge in generic\nPTMs. To address this problem, in this paper, we present POLARIS, which is a\ngeographic pre-trained model designed and developed for improving the\ngeo-related tasks at Baidu Maps. POLARIS is elaborately designed to learn a\nuniversal representation of geography-language by pre-training on large-scale\ndata generated from a heterogeneous graph that contains abundant geographic\nknowledge. Extensive quantitative and qualitative experiments conducted on\nlarge-scale real-world datasets demonstrate the superiority and effectiveness\nof POLARIS. POLARIS has already been deployed in production at Baidu Maps since\nApril 2021, which significantly benefits the performance of a wide range of\ndownstream tasks. This demonstrates that POLARIS can serve as a fundamental\nbackbone for geo-related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jizhou_H/0/1/0/all/0/1\">Huang Jizhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haifeng_W/0/1/0/all/0/1\">Wang Haifeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yibo_S/0/1/0/all/0/1\">Sun Yibo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yunsheng_S/0/1/0/all/0/1\">Shi Yunsheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhengjie_H/0/1/0/all/0/1\">Huang Zhengjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhuo An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shikun_F/0/1/0/all/0/1\">Feng Shikun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Dependency, Data Flow, and Competitive Advantage. (arXiv:2203.09128v1 [cs.LG])","link":"http://arxiv.org/abs/2203.09128","description":"<p>Data is fundamental to machine learning-based products and services and is\nconsidered strategic due to its externalities for businesses, governments,\nnon-profits, and more generally for society. It is renowned that the value of\norganizations (businesses, government agencies and programs, and even\nindustries) scales with the volume of available data. What is often less\nappreciated is that the data value in making useful organizational predictions\nwill range widely and is prominently a function of data characteristics and\nunderlying algorithms.\n</p>\n<p>In this research, our goal is to study how the value of data changes over\ntime and how this change varies across contexts and business areas (e.g. next\nword prediction in the context of history, sports, politics). We focus on data\nfrom Reddit.com and compare the value's time-dependency across various Reddit\ntopics (Subreddits). We make this comparison by measuring the rate at which\nuser-generated text data loses its relevance to the algorithmic prediction of\nconversations. We show that different subreddits have different rates of\nrelevance decline over time.\n</p>\n<p>Relating the text topics to various business areas of interest, we argue that\ncompeting in a business area in which data value decays rapidly alters\nstrategies to acquire competitive advantage. When data value decays rapidly,\naccess to a continuous flow of data will be more valuable than access to a\nfixed stock of data. In this kind of setting, improving user engagement and\nincreasing user-base help creating and maintaining a competitive advantage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valavi_E/0/1/0/all/0/1\">Ehsan Valavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1\">Joel Hestness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iansiti_M/0/1/0/all/0/1\">Marco Iansiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardalani_N/0/1/0/all/0/1\">Newsha Ardalani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhani_K/0/1/0/all/0/1\">Karim R. Lakhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Type-Driven Multi-Turn Corrections for Grammatical Error Correction. (arXiv:2203.09136v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09136","description":"<p>Grammatical Error Correction (GEC) aims to automatically detect and correct\ngrammatical errors. In this aspect, dominant models are trained by\none-iteration learning while performing multiple iterations of corrections\nduring inference. Previous studies mainly focus on the data augmentation\napproach to combat the exposure bias, which suffers from two drawbacks. First,\nthey simply mix additionally-constructed training instances and original ones\nto train models, which fails to help models be explicitly aware of the\nprocedure of gradual corrections. Second, they ignore the interdependence\nbetween different types of corrections. In this paper, we propose a Type-Driven\nMulti-Turn Corrections approach for GEC. Using this approach, from each\ntraining instance, we additionally construct multiple training instances, each\nof which involves the correction of a specific type of errors. Then, we use\nthese additionally-constructed training instances and the original one to train\nthe model in turn. Experimental results and in-depth analysis show that our\napproach significantly benefits the model training. Particularly, our enhanced\nmodel achieves state-of-the-art single-model performance on English GEC\nbenchmarks. We release our code at Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shaopeng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiali Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of speech intelligibility with DNN-based performance measures. (arXiv:2203.09148v1 [cs.SD])","link":"http://arxiv.org/abs/2203.09148","description":"<p>This paper presents a speech intelligibility model based on automatic speech\nrecognition (ASR), combining phoneme probabilities from deep neural networks\n(DNN) and a performance measure that estimates the word error rate from these\nprobabilities. This model does not require the clean speech reference nor the\nword labels during testing as the ASR decoding step, which finds the most\nlikely sequence of words given phoneme posterior probabilities, is omitted. The\nmodel is evaluated via the root-mean-squared error between the predicted and\nobserved speech reception thresholds from eight normal-hearing listeners. The\nrecognition task consists of identifying noisy words from a German matrix\nsentence test. The speech material was mixed with eight noise maskers covering\ndifferent modulation types, from speech-shaped stationary noise to a\nsingle-talker masker. The prediction performance is compared to five\nestablished models and an ASR-model using word labels. Two combinations of\nfeatures and networks were tested. Both include temporal information either at\nthe feature level (amplitude modulation filterbanks and a feed-forward network)\nor captured by the architecture (mel-spectrograms and a time-delay deep neural\nnetwork, TDNN). The TDNN model is on par with the DNN while reducing the number\nof parameters by a factor of 37; this optimization allows parallel streams on\ndedicated hearing aid hardware as a forward-pass can be computed within the\n10ms of each frame. The proposed model performs almost as well as the\nlabel-based model and produces more accurate predictions than the baseline\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Angel Mario Castro Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spille_C/0/1/0/all/0/1\">Constantin Spille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossbach_J/0/1/0/all/0/1\">Jana Ro&#xdf;bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollmeier_B/0/1/0/all/0/1\">Birger Kollmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_B/0/1/0/all/0/1\">Bernd T. Meyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09161","description":"<p>Recently introduced instruction-paradigm empowers non-expert users to\nleverage NLP resources by defining a new task in natural language.\nInstruction-tuned models have significantly outperformed multitask learning\nmodels (without instruction); however they are far from state of the art task\nspecific models. Conventional approaches to improve model performance via\ncreating large datasets with lots of task instances or architectural/training\nchanges in model may not be feasible for non-expert users. However, they can\nwrite alternate instructions to represent an instruction task. Is\nInstruction-augumentation helpful? We augment a subset of tasks in NATURAL\nINSTRUCTIONS with additional instructions and find that these significantly\nimprove model performance (upto 35%) specially in low-data regime. Our results\nindicate that an additional instruction can be equivalent to ~40 instances on\naverage across our evaluation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ravsehaj Singh Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Dual Read/Write Paths for Simultaneous Machine Translation. (arXiv:2203.09163v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09163","description":"<p>Simultaneous machine translation (SiMT) outputs the translation while reading\nthe source sentence and hence requires a policy to determine whether to wait\nfor the next source word (READ) or generate a target word (WRITE), the actions\nof which form a read/write path. Although the read/write path is essential to\nSiMT performance, there is no direct supervision given to the path in the\nexisting methods. In this paper, we propose a method of Dual Path SiMT which\nintroduces duality constraints to guide the read/write path. According to\nduality constraints, the read/write paths in source-to-target and\ntarget-to-source SiMT models can be mapped to each other. Therefore, the SiMT\nmodels in two directions are jointly optimized by forcing their read/write\npaths to satisfy the mapping relation. Experiments on En-Vi and De-En SiMT\ntasks show that our method can outperform strong baselines under all latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Vision Features in Multimodal Machine Translation. (arXiv:2203.09173v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09173","description":"<p>Previous work on multimodal machine translation (MMT) has focused on the way\nof incorporating vision features into translation but little attention is on\nthe quality of vision models. In this work, we investigate the impact of vision\nmodels on MMT. Given the fact that Transformer is becoming popular in computer\nvision, we experiment with various strong models (such as Vision Transformer)\nand enhanced features (such as object-detection and image captioning). We\ndevelop a selective attention model to study the patch-level contribution of an\nimage in MMT. On detailed probing tasks, we find that stronger vision models\nare helpful for learning translation from the visual modality. Our results also\nsuggest the need of carefully examining MMT models, especially when current\nbenchmarks are small-scale and biased. Our code could be found at\n\\url{https://github.com/libeineu/fairseq_mmt}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chuanhao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zefan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Anxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">JingBo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation. (arXiv:2203.09176v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09176","description":"<p>Residual networks are an Euler discretization of solutions to Ordinary\nDifferential Equations (ODE). This paper explores a deeper relationship between\nTransformer and numerical ODE methods. We first show that a residual block of\nlayers in Transformer can be described as a higher-order solution to ODE.\nInspired by this, we design a new architecture, {\\it ODE Transformer}, which is\nanalogous to the Runge-Kutta method that is well motivated in ODE. As a natural\nextension to Transformer, ODE Transformer is easy to implement and efficient to\nuse. Experimental results on the large-scale machine translation, abstractive\nsummarization, and grammar error correction tasks demonstrate the high\ngenericity of ODE Transformer. It can gain large improvements in model\nperformance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the\nWMT'14 English-German and English-French benchmarks) at a slight cost in\ninference efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Quan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1\">Yi Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuhan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xin Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">JingBo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Detection of Personal Employment Status on Twitter. (arXiv:2203.09178v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09178","description":"<p>Detecting disclosures of individuals' employment status on social media can\nprovide valuable information to match job seekers with suitable vacancies,\noffer social protection, or measure labor market flows. However, identifying\nsuch personal disclosures is a challenging task due to their rarity in a sea of\nsocial media content and the variety of linguistic forms used to describe them.\nHere, we examine three Active Learning (AL) strategies in real-world settings\nof extreme class imbalance, and identify five types of disclosures about\nindividuals' employment status (e.g. job loss) in three languages using\nBERT-based classification models. Our findings show that, even under extreme\nimbalance settings, a small number of AL iterations is sufficient to obtain\nlarge and significant gains in precision, recall, and diversity of results\ncompared to a supervised baseline with the same number of labels. We also find\nthat no AL strategy consistently outperforms the rest. Qualitative analysis\nsuggests that AL helps focus the attention mechanism of BERT on core terms and\nadjust the boundaries of semantic expansion, highlighting the importance of\ninterpretable models to provide greater control and visibility into this\ndynamic learning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tonneau_M/0/1/0/all/0/1\">Manuel Tonneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adjodah_D/0/1/0/all/0/1\">Dhaval Adjodah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palotti_J/0/1/0/all/0/1\">Jo&#xe3;o Palotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grinberg_N/0/1/0/all/0/1\">Nir Grinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraiberger_S/0/1/0/all/0/1\">Samuel Fraiberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoMe: A Robust Metric for Evaluating Natural Language Generation. (arXiv:2203.09183v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09183","description":"<p>Evaluating Natural Language Generation (NLG) systems is a challenging task.\nFirstly, the metric should ensure that the generated hypothesis reflects the\nreference's semantics. Secondly, it should consider the grammatical quality of\nthe generated sentence. Thirdly, it should be robust enough to handle various\nsurface forms of the generated sentence. Thus, an effective evaluation metric\nhas to be multifaceted. In this paper, we propose an automatic evaluation\nmetric incorporating several core aspects of natural language understanding\n(language competence, syntactic and semantic variation). Our proposed metric,\nRoMe, is trained on language features such as semantic similarity combined with\ntree edit distance and grammatical acceptability, using a self-supervised\nneural network to assess the overall quality of the generated sentence.\nMoreover, we perform an extensive robustness analysis of the state-of-the-art\nmethods and RoMe. Empirical results suggest that RoMe has a stronger\ncorrelation to human judgment over state-of-the-art metrics in evaluating\nsystem-generated sentences across several NLG tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rony_M/0/1/0/all/0/1\">Md Rashad Al Hasan Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovriguina_L/0/1/0/all/0/1\">Liubov Kovriguina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_D/0/1/0/all/0/1\">Debanjan Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstract Interpretation on E-Graphs. (arXiv:2203.09191v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09191","description":"<p>Recent e-graph applications have typically considered concrete semantics of\nexpressions, where the notion of equivalence stems from concrete interpretation\nof expressions. However, equivalences that hold over one interpretation may not\nhold in an alternative interpretation. Such an observation can be exploited. We\nconsider the application of abstract interpretation to e-graphs, and show that\nwithin an e-graph, the lattice meet operation associated with the abstract\ndomain has a natural interpretation for an e-class, leading to improved\nprecision in over-approximation. In this extended abstract, we use Interval\nArithmetic (IA) to illustrate this point.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coward_S/0/1/0/all/0/1\">Samuel Coward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantinides_G/0/1/0/all/0/1\">George A. Constantinides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drane_T/0/1/0/all/0/1\">Theo Drane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists. (arXiv:2203.09192v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09192","description":"<p>Natural Language Processing (NLP) models risk overfitting to specific terms\nin the training data, thereby reducing their performance, fairness, and\ngeneralizability. E.g., neural hate speech detection models are strongly\ninfluenced by identity terms like gay, or women, resulting in false positives,\nsevere unintended bias, and lower performance. Most mitigation techniques use\nlists of identity terms or samples from the target domain during training.\nHowever, this approach requires a-priori knowledge and introduces further bias\nif important terms are neglected. Instead, we propose a knowledge-free\nEntropy-based Attention Regularization (EAR) to discourage overfitting to\ntraining-specific terms. An additional objective function penalizes tokens with\nlow self-attention entropy. We fine-tune BERT via EAR: the resulting model\nmatches or exceeds state-of-the-art performance for hate speech classification\nand bias metrics on three benchmark corpora in English and Italian. EAR also\nreveals overfitting terms, i.e., terms most likely to induce bias, to help\nidentify their effect on the model, task, and predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baralis_E/0/1/0/all/0/1\">Elena Baralis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Conditional Masked Language Pre-training for Neural Machine Translation. (arXiv:2203.09210v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09210","description":"<p>Pre-trained sequence-to-sequence models have significantly improved Neural\nMachine Translation (NMT). Different from prior works where pre-trained models\nusually adopt an unidirectional decoder, this paper demonstrates that\npre-training a sequence-to-sequence model but with a bidirectional decoder can\nproduce notable performance gains for both Autoregressive and\nNon-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked\nlanguage model pre-trained on large-scale bilingual and monolingual corpora in\nmany languages. We also introduce two simple but effective methods to enhance\nthe CeMAT, aligned code-switching &amp; masking and dynamic dual-masking. We\nconduct extensive experiments and show that our CeMAT can achieve significant\nperformance improvement for all scenarios from low to extremely high resource,\ni.e., up to 14.4 BLEU on low resource and 7.9 BLEU improvements on average for\nAutoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also\nproduce consistent performance gains, i.e., up to 5.3 BLEU. As far as we know,\nthis is the first work to pre-train a unified model for fine-tuning on both NMT\ntasks. Code, data, and pre-trained models are available at\nhttps://github.com/huawei-noah/Pretrained-Language-Model/CeMAT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Calibration for Intent Detection via Hyperspherical Space and Rebalanced Accuracy-Uncertainty Loss. (arXiv:2203.09278v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09278","description":"<p>Data-driven methods have achieved notable performance on intent detection,\nwhich is a task to comprehend user queries. Nonetheless, they are controversial\nfor over-confident predictions. In some scenarios, users do not only care about\nthe accuracy but also the confidence of model. Unfortunately, mainstream neural\nnetworks are poorly calibrated, with a large gap between accuracy and\nconfidence. To handle this problem defined as confidence calibration, we\npropose a model using the hyperspherical space and rebalanced\naccuracy-uncertainty loss. Specifically, we project the label vector onto\nhyperspherical space uniformly to generate a dense label representation matrix,\nwhich mitigates over-confident predictions due to overfitting sparce one-hot\nlabel matrix. Besides, we rebalance samples of different accuracy and\nuncertainty to better guide model training. Experiments on the open datasets\nverify that our model outperforms the existing calibration methods and achieves\na significant improvement on the calibration metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yantao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xunliang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiansong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Structural Knowledge in Multimodal-BERT. (arXiv:2203.09306v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09306","description":"<p>In this work, we investigate the knowledge learned in the embeddings of\nmultimodal-BERT models. More specifically, we probe their capabilities of\nstoring the grammatical structure of linguistic data and the structure learned\nover objects in visual data. To reach that goal, we first make the inherent\nstructure of language and visuals explicit by a dependency parse of the\nsentences that describe the image and by the dependencies between the object\nregions in the image, respectively. We call this explicit visual structure the\n\\textit{scene tree}, that is based on the dependency tree of the language\ndescription. Extensive probing experiments show that the multimodal-BERT models\ndo not encode these scene trees.Code available at\n\\url{https://github.com/VSJMilewski/multimodal-probes}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milewski_V/0/1/0/all/0/1\">Victor Milewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1\">Miryam de Lhoneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training. (arXiv:2203.09313v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09313","description":"<p>Large-scale pre-training has shown remarkable performance in building\nopen-domain dialogue systems. However, previous works mainly focus on showing\nand evaluating the conversational performance of the released dialogue model,\nignoring the discussion of some key factors towards a powerful human-like\nchatbot, especially in Chinese scenarios. In this paper, we conduct extensive\nexperiments to investigate these under-explored factors, including data quality\ncontrol, model architecture designs, training approaches, and decoding\nstrategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese\ndialogue model with 2.8 billion parameters, and make our models and code\npublicly available. To our knowledge, EVA2.0 is the largest open-source Chinese\ndialogue model. Automatic and human evaluations show that our model\nsignificantly outperforms other open-source counterparts. We also discuss the\nlimitations of this work by presenting some failure cases and pose some future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jianzhu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Static and Contextualised Multilingual Embeddings. (arXiv:2203.09326v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09326","description":"<p>Static and contextual multilingual embeddings have complementary strengths.\nStatic embeddings, while less expressive than contextual language models, can\nbe more straightforwardly aligned across multiple languages. We combine the\nstrengths of static and contextual models to improve multilingual\nrepresentations. We extract static embeddings for 40 languages from XLM-R,\nvalidate those embeddings with cross-lingual word retrieval, and then align\nthem using VecMap. This results in high-quality, highly multilingual static\nembeddings. Then we apply a novel continued pre-training approach to XLM-R,\nleveraging the high quality alignment of our static embeddings to better align\nthe representation space of XLM-R. We show positive results for multiple\ncomplex semantic tasks. We release the static embeddings and the continued\npre-training code. Unlike most previous work, our continued pre-training\napproach does not require parallel text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammerl_K/0/1/0/all/0/1\">Katharina H&#xe4;mmerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation. (arXiv:2203.09391v1 [cs.LG])","link":"http://arxiv.org/abs/2203.09391","description":"<p>Data Augmentation (DA) is known to improve the generalizability of deep\nneural networks. Most existing DA techniques naively add a certain number of\naugmented samples without considering the quality and the added computational\ncost of these samples. To tackle this problem, a common strategy, adopted by\nseveral state-of-the-art DA methods, is to adaptively generate or re-weight\naugmented samples with respect to the task objective during training. However,\nthese adaptive DA methods: (1) are computationally expensive and not\nsample-efficient, and (2) are designed merely for a specific setting. In this\nwork, we present a universal DA technique, called Glitter, to overcome both\nissues. Glitter can be plugged into any DA method, making training\nsample-efficient without sacrificing performance. From a pre-generated pool of\naugmented samples, Glitter adaptively selects a subset of worst-case samples\nwith maximal loss, analogous to adversarial DA. Without altering the training\nstrategy, the task objective can be optimized on the selected subset. Our\nthorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three\nwidely used training setups including consistency training, self-distillation\nand knowledge distillation reveal that Glitter is substantially faster to train\nand achieves a competitive performance, compared to strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models. (arXiv:2203.09397v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09397","description":"<p>Relations between words are governed by hierarchical structure rather than\nlinear ordering. Sequence-to-sequence (seq2seq) models, despite their success\nin downstream NLP applications, often fail to generalize in a\nhierarchy-sensitive manner when performing syntactic transformations - for\nexample, transforming declarative sentences into questions. However, syntactic\nevaluations of seq2seq models have only observed models that were not\npre-trained on natural language data before being trained to perform syntactic\ntransformations, in spite of the fact that pre-training has been found to\ninduce hierarchical linguistic generalizations in language models; in other\nwords, the syntactic capabilities of seq2seq models may have been greatly\nunderstated. We address this gap using the pre-trained seq2seq models T5 and\nBART, as well as their multilingual variants mT5 and mBART. We evaluate whether\nthey generalize hierarchically on two transformations in two languages:\nquestion formation and passivization in English and German. We find that\npre-trained seq2seq models generalize hierarchically when performing syntactic\ntransformations, whereas models trained from scratch on syntactic\ntransformations do not. This result presents evidence for the learnability of\nhierarchical syntactic information from non-annotated natural language text\nwhile also demonstrating that seq2seq models are capable of syntactic\ngeneralization, though only after exposure to much more language data than\nhuman learners receive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1\">Robert Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Luheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_S/0/1/0/all/0/1\">Sebastian Schuster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"elBERto: Self-supervised Commonsense Learning for Question Answering. (arXiv:2203.09424v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09424","description":"<p>Commonsense question answering requires reasoning about everyday situations\nand causes and effects implicit in context. Typically, existing approaches\nfirst retrieve external evidence and then perform commonsense reasoning using\nthese evidence. In this paper, we propose a Self-supervised Bidirectional\nEncoder Representation Learning of Commonsense (elBERto) framework, which is\ncompatible with off-the-shelf QA model architectures. The framework comprises\nfive self-supervised tasks to force the model to fully exploit the additional\ntraining signals from contexts containing rich commonsense. The tasks include a\nnovel Contrastive Relation Learning task to encourage the model to distinguish\nbetween logically contrastive contexts, a new Jigsaw Puzzle task that requires\nthe model to infer logical chains in long contexts, and three classic SSL tasks\nto maintain pre-trained models language encoding ability. On the representative\nWIQA, CosmosQA, and ReClor datasets, elBERto outperforms all other methods,\nincluding those utilizing explicit graph reasoning and external knowledge\nretrieval. Moreover, elBERto achieves substantial improvements on\nout-of-paragraph and no-effect questions where simple lexical similarity\ncomparison does not help, indicating that it successfully learns commonsense\nand is able to leverage it when given dynamic context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation. (arXiv:2203.09435v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09435","description":"<p>The performance of multilingual pretrained models is highly dependent on the\navailability of monolingual or parallel text present in a target language.\nThus, the majority of the world's languages cannot benefit from recent progress\nin NLP as they have no or limited textual data. To expand possibilities of\nusing NLP technology in these under-represented languages, we systematically\nstudy strategies that relax the reliance on conventional language resources\nthrough the use of bilingual lexicons, an alternative resource with much better\nlanguage coverage. We analyze different strategies to synthesize textual or\nlabeled data using lexicons, and how this data can be combined with monolingual\nor parallel text when available. For 19 under-represented languages across 3\ntasks, our methods lead to consistent improvements of up to 5 and 15 points\nwith and without extra monolingual text respectively. Overall, our study\nhighlights how NLP methods can be adapted to thousands more languages that are\nunder-served by current technology\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models. (arXiv:2203.09486v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09486","description":"<p>We propose a framework for training non-autoregressive sequence-to-sequence\nmodels for editing tasks, where the original input sequence is iteratively\nedited to produce the output. We show that the imitation learning algorithms\ndesigned to train such models for machine translation introduces mismatches\nbetween training and inference that lead to undertraining and poor\ngeneralization in editing scenarios. We address this issue with two\ncomplementary strategies: 1) a roll-in policy that exposes the model to\nintermediate training sequences that it is more likely to encounter during\ninference, 2) a curriculum that presents easy-to-learn edit operations first,\ngradually increasing the difficulty of training samples as the model becomes\ncompetent. We show the efficacy of these strategies on two challenging English\nediting tasks: controllable text simplification and abstractive summarization.\nOur approach significantly improves output quality on both tasks and controls\noutput complexity better on the simplification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sweta Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents. (arXiv:2203.09498v1 [cs.AI])","link":"http://arxiv.org/abs/2203.09498","description":"<p>Learned communication between agents is a powerful tool when approaching\ndecision-making problems that are hard to overcome by any single agent in\nisolation. However, continual coordination and communication learning between\nmachine agents or human-machine partnerships remains a challenging open\nproblem. As a stepping stone toward solving the continual communication\nlearning problem, in this paper we contribute a multi-faceted study into what\nwe term Pavlovian signalling -- a process by which learned, temporally extended\npredictions made by one agent inform decision-making by another agent with\ndifferent perceptual access to their shared environment. We seek to establish\nhow different temporal processes and representational choices impact Pavlovian\nsignalling between learning agents. To do so, we introduce a partially\nobservable decision-making domain we call the Frost Hollow. In this domain a\nprediction learning agent and a reinforcement learning agent are coupled into a\ntwo-part decision-making system that seeks to acquire sparse reward while\navoiding time-conditional hazards. We evaluate two domain variations: 1)\nmachine prediction and control learning in a linear walk, and 2) a prediction\nlearning machine interacting with a human participant in a virtual reality\nenvironment. Our results showcase the speed of learning for Pavlovian\nsignalling, the impact that different temporal representations do (and do not)\nhave on agent-agent coordination, and how temporal aliasing impacts agent-agent\nand human-agent interactions differently. As a main contribution, we establish\nPavlovian signalling as a natural bridge between fixed signalling paradigms and\nfully adaptive communication learning. Our results therefore point to an\nactionable, constructivist path towards continual communication learning\nbetween reinforcement learning agents, with potential impact in a range of\nreal-world settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pilarski_P/0/1/0/all/0/1\">Patrick M. Pilarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butcher_A/0/1/0/all/0/1\">Andrew Butcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoodi_E/0/1/0/all/0/1\">Elnaz Davoodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johanson_M/0/1/0/all/0/1\">Michael Bradley Johanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenneis_D/0/1/0/all/0/1\">Dylan J. A. Brenneis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_A/0/1/0/all/0/1\">Adam S. R. Parker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acker_L/0/1/0/all/0/1\">Leslie Acker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matthew M. Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modayil_J/0/1/0/all/0/1\">Joseph Modayil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Adam White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. (arXiv:2203.09509v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09509","description":"<p>Toxic language detection systems often falsely flag text that contains\nminority group mentions as toxic, as those groups are often the targets of\nonline hate. Such over-reliance on spurious correlations also causes systems to\nstruggle with detecting implicitly toxic language. To help mitigate these\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\n274k toxic and benign statements about 13 minority groups. We develop a\ndemonstration-based prompting framework and an adversarial\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\nwith a massive pretrained language model. Controlling machine generation in\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\nabout more demographic groups, than previous resources of human-written text.\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\nannotators struggle to distinguish machine-generated text from human-written\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\nby human annotators. Using three publicly-available datasets, we show that\nfinetuning a toxicity classifier on our data improves its performance on\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\nto fight machine-generated toxicity as finetuning improves the classifier\nsignificantly on our evaluation subset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartvigsen_T/0/1/0/all/0/1\">Thomas Hartvigsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_D/0/1/0/all/0/1\">Dipankar Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KART: Parameterization of Privacy Leakage Scenarios from Pre-trained Language Models. (arXiv:2101.00036v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00036","description":"<p>For the safe sharing pre-trained language models, no guidelines exist at\npresent owing to the difficulty in estimating the upper bound of the risk of\nprivacy leakage. One problem is that previous studies have assessed the risk\nfor different real-world privacy leakage scenarios and attack methods, which\nreduces the portability of the findings. To tackle this problem, we represent\ncomplex real-world privacy leakage scenarios under a universal\nparameterization, \\textit{Knowledge, Anonymization, Resource, and Target}\n(KART). KART parameterization has two merits: (i) it clarifies the definition\nof privacy leakage in each experiment and (ii) it improves the comparability of\nthe findings of risk assessments. We show that previous studies can be simply\nreviewed by parameterizing the scenarios with KART. We also demonstrate privacy\nrisk assessments in different scenarios under the same attack method, which\nsuggests that KART helps approximate the upper bound of risk under a specific\nattack or scenario. We believe that KART helps integrate past and future\nfindings on privacy risk and will contribute to a standard for sharing language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_Y/0/1/0/all/0/1\">Yuta Nakamura</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hanaoka_S/0/1/0/all/0/1\">Shouhei Hanaoka</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Nomura_Y/0/1/0/all/0/1\">Yukihiro Nomura</a> (3 and 4), <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_N/0/1/0/all/0/1\">Naoto Hayashi</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Abe_O/0/1/0/all/0/1\">Osamu Abe</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Yada_S/0/1/0/all/0/1\">Shuntaro Yada</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Wakamiya_S/0/1/0/all/0/1\">Shoko Wakamiya</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Aramaki_E/0/1/0/all/0/1\">Eiji Aramaki</a> (5) ((1) The University of Tokyo, (2) The Department of Radiology, The University of Tokyo Hospital, (3) The Department of Computational Diagnostic Radiology and Preventive Medicine, The University of Tokyo Hospital, (4) Chiba University, (5) Nara Institute of Science and Technology)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling. (arXiv:2103.10360v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.10360","description":"<p>There have been various types of pretraining architectures including\nautoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and\nencoder-decoder models (e.g., T5). However, none of the pretraining frameworks\nperforms the best for all tasks of three main categories including natural\nlanguage understanding (NLU), unconditional generation, and conditional\ngeneration. We propose a General Language Model (GLM) based on autoregressive\nblank infilling to address this challenge. GLM improves blank filling\npretraining by adding 2D positional encodings and allowing an arbitrary order\nto predict spans, which results in performance gains over BERT and T5 on NLU\ntasks. Meanwhile, GLM can be pretrained for different types of tasks by varying\nthe number and lengths of blanks. On a wide range of tasks across NLU,\nconditional and unconditional generation, GLM outperforms BERT, T5, and GPT\ngiven the same model sizes and data, and achieves the best performance from a\nsingle pretrained model with 1.25x parameters of BERT Large , demonstrating its\ngeneralizability to different downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhengxiao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yujie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First the worst: Finding better gender translations during beam search. (arXiv:2104.07429v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07429","description":"<p>Neural machine translation inference procedures like beam search generate the\nmost likely output under the model. This can exacerbate any demographic biases\nexhibited by the model. We focus on gender bias resulting from systematic\nerrors in grammatical gender translation, which can lead to human referents\nbeing misrepresented or misgendered.\n</p>\n<p>Most approaches to this problem adjust the training data or the model. By\ncontrast, we experiment with simply adjusting the inference procedure. We\nexperiment with reranking nbest lists using gender features obtained\nautomatically from the source sentence, and applying gender constraints while\ndecoding to improve nbest list gender diversity. We find that a combination of\nthese techniques allows large gains in WinoMT accuracy without requiring\nadditional bilingual data or an additional NMT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_D/0/1/0/all/0/1\">Danielle Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sallis_R/0/1/0/all/0/1\">Rosie Sallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Idioms: Conventionality and Contingency. (arXiv:2104.08664v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08664","description":"<p>Idioms are unlike other phrases in two important ways. First, the words in an\nidiom have unconventional meanings. Second, the unconventional meaning of words\nin an idiom are contingent on the presence of the other words in the idiom.\nLinguistic theories disagree about whether these two properties depend on one\nanother, as well as whether special theoretical machinery is needed to\naccommodate idioms. We define two measures that correspond to these two\nproperties, and we show that idioms fall at the expected intersection of the\ntwo dimensions, but that the dimensions themselves are not correlated. Our\nresults suggest that idioms are no more anomalous than other types of phrases,\nand that introducing special machinery to handle idioms may not be warranted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Socolof_M/0/1/0/all/0/1\">Michaela Socolof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1\">Michael Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages. (arXiv:2104.08726v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08726","description":"<p>Pretrained multilingual models are able to perform cross-lingual transfer in\na zero-shot setting, even for languages unseen during pretraining. However,\nprior work evaluating performance on unseen languages has largely been limited\nto low-level, syntactic tasks, and it remains unclear if zero-shot learning of\nhigh-level, semantic tasks is possible for unseen languages. To explore this\nquestion, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018)\nto 10 indigenous languages of the Americas. We conduct experiments with XLM-R,\ntesting multiple zero-shot and translation-based approaches. Additionally, we\nexplore model adaptation via continued pretraining and provide an analysis of\nthe dataset by considering hypothesis-only models. We find that XLM-R's\nzero-shot performance is poor for all 10 languages, with an average performance\nof 38.62%. Continued pretraining offers improvements, with an average accuracy\nof 44.05%. Surprisingly, training on poorly translated data by far outperforms\nall other methods with an accuracy of 48.72%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_A/0/1/0/all/0/1\">Abteen Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mager_M/0/1/0/all/0/1\">Manuel Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oncevay_A/0/1/0/all/0/1\">Arturo Oncevay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiruzzo_L/0/1/0/all/0/1\">Luis Chiruzzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_R/0/1/0/all/0/1\">Ricardo Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1\">Annette Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meza_Ruiz_I/0/1/0/all/0/1\">Ivan Meza-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimenez_Lugo_G/0/1/0/all/0/1\">Gustavo A. Gim&#xe9;nez-Lugo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mager_E/0/1/0/all/0/1\">Elisabeth Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_A/0/1/0/all/0/1\">Alexis Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coto_Solano_R/0/1/0/all/0/1\">Rolando Coto-Solano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Multi-Branch Layers for On-Device Neural Machine Translation. (arXiv:2105.06679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06679","description":"<p>With the rapid development of artificial intelligence (AI), there is a trend\nin moving AI applications, such as neural machine translation (NMT), from cloud\nto mobile devices. Constrained by limited hardware resources and battery, the\nperformance of on-device NMT systems is far from satisfactory. Inspired by\nconditional computation, we propose to improve the performance of on-device NMT\nsystems with dynamic multi-branch layers. Specifically, we design a layer-wise\ndynamic multi-branch network with only one branch activated during training and\ninference. As not all branches are activated during training, we propose\nshared-private reparameterization to ensure sufficient training for each\nbranch. At almost the same computational cost, our method achieves improvements\nof up to 1.7 BLEU points on the WMT14 English-German translation task and 1.8\nBLEU points on the WMT20 Chinese-English translation task over the Transformer\nmodel, respectively. Compared with a strong baseline that also uses multiple\nbranches, the proposed method is up to 1.5 times faster with the same number of\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zeyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues. (arXiv:2105.07122v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07122","description":"<p>It is a common practice for recent works in vision language cross-modal\nreasoning to adopt a binary or multi-choice classification formulation taking\nas input a set of source image(s) and textual query. In this work, we take a\nsober look at such an unconditional formulation in the sense that no prior\nknowledge is specified with respect to the source image(s). Inspired by the\ndesigns of both visual commonsense reasoning and natural language inference\ntasks, we propose a new task termed Premise-based Multi-modal Reasoning(PMR)\nwhere a textual premise is the background presumption on each source image. The\nPMR dataset contains 15,360 manually annotated samples which are created by a\nmulti-phase crowd-sourcing process. With selected high-quality movie\nscreenshots and human-curated premise templates from 6 pre-defined categories,\nwe ask crowd-source workers to write one true hypothesis and three distractors\n(4 choices) given the premise and image through a cross-check procedure.\nBesides, we generate adversarial samples to alleviate the annotation artifacts\nand double the size of PMR. We benchmark various state-of-the-art (pretrained)\nmulti-modal inference models on PMR and conduct comprehensive experimental\nanalyses to showcase the utility of our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Ziwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Heming Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shoujie Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Haoran Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Weidong Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zuifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.10199","description":"<p>We introduce BitFit, a sparse-finetuning method where only the bias-terms of\nthe model (or a subset of them) are being modified. We show that with\nsmall-to-medium training data, applying BitFit on pre-trained BERT models is\ncompetitive with (and sometimes better than) fine-tuning the entire model. For\nlarger data, the method is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1\">Elad Ben Zaken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03158","description":"<p>Data augmentation, the artificial creation of training data for machine\nlearning by transformations, is a widely studied research field across machine\nlearning disciplines. While it is useful for increasing a model's\ngeneralization capabilities, it can also address many other challenges and\nproblems, from overcoming a limited amount of training data, to regularizing\nthe objective, to limiting the amount data used to protect privacy. Based on a\nprecise description of the goals and applications of data augmentation and a\ntaxonomy for existing works, this survey is concerned with data augmentation\nmethods for textual classification and aims to provide a concise and\ncomprehensive overview for researchers and practitioners. Derived from the\ntaxonomy, we divide more than 100 methods into 12 different groupings and give\nstate-of-the-art references expounding which methods are highly promising by\nrelating them to each other. Finally, research perspectives that may constitute\na building block for future work are provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Kaufhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10314","description":"<p>We present small-text, a simple and modular active learning library, which\noffers pool-based active learning for single- and multi-label text\nclassification in Python. It comes with various pre-implemented\nstate-of-the-art query strategies, including some that can leverage the GPU.\nClearly defined interfaces allow the combination of a multitude of classifiers,\nquery strategies, and stopping criteria, thereby facilitating a quick mix and\nmatch, and enabling a rapid development of both active learning experiments and\napplications. To make various classifiers accessible in a consistent way, it\nintegrates several well-known existing machine learning libraries, namely,\nscikit-learn, PyTorch, and huggingface transformers, where the latter\nintegrations are available as optionally installable extensions, making the\navailability of a GPU competely optional. The library is available under the\nMIT License at https://github.com/webis-de/small-text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10904","description":"<p>With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chronic Pain and Language: A Topic Modelling Approach to Personal Pain Descriptions. (arXiv:2109.00402v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00402","description":"<p>Chronic pain is recognized as a major health problem, with impacts not only\nat the economic, but also at the social, and individual levels. Being a private\nand subjective experience, it is impossible to externally and impartially\nexperience, describe, and interpret chronic pain as a purely noxious stimulus\nthat would directly point to a causal agent and facilitate its mitigation,\ncontrary to acute pain, the assessment of which is usually straightforward.\nVerbal communication is, thus, key to convey relevant information to health\nprofessionals that would otherwise not be accessible to external entities,\nnamely, intrinsic qualities about the painful experience and the patient. We\npropose and discuss a topic modelling approach to recognize patterns in verbal\ndescriptions of chronic pain, and use these patterns to quantify and qualify\nexperiences of pain. Our approaches allow for the extraction of novel insights\non chronic pain experiences from the obtained topic models and latent spaces.\nWe argue that our results are clinically relevant for the assessment and\nmanagement of chronic pain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nunes_D/0/1/0/all/0/1\">Diogo A. P. Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_J/0/1/0/all/0/1\">Joana Ferreira Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_F/0/1/0/all/0/1\">Fani Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_D/0/1/0/all/0/1\">David Martins de Matos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xGQA: Cross-Lingual Visual Question Answering. (arXiv:2109.06082v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06082","description":"<p>Recent advances in multimodal vision and language modeling have predominantly\nfocused on the English language, mostly due to the lack of multilingual\nmultimodal datasets to steer modeling efforts. In this work, we address this\ngap and provide xGQA, a new multilingual evaluation benchmark for the visual\nquestion answering task. We extend the established English GQA dataset to 7\ntypologically diverse languages, enabling us to detect and explore crucial\nchallenges in cross-lingual visual question answering. We further propose new\nadapter-based approaches to adapt multimodal transformer-based models to become\nmultilingual, and -- vice versa -- multilingual models to become multimodal.\nOur proposed methods outperform current state-of-the-art multilingual\nmultimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the\naccuracy remains low across the board; a performance drop of around 38 accuracy\npoints in target languages showcases the difficulty of zero-shot cross-lingual\ntransfer for this task. Our results suggest that simple cross-lingual transfer\nof multimodal models yields latent multilingual multimodal misalignment,\ncalling for more sophisticated methods for vision and multilingual language\nmodeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Aishwarya Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steitz_J/0/1/0/all/0/1\">Jan-Martin O. Steitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_S/0/1/0/all/0/1\">Stefan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Isotropy Analysis in the Multilingual BERT Embedding Space. (arXiv:2110.04504v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04504","description":"<p>Several studies have explored various advantages of multilingual pre-trained\nmodels (such as multilingual BERT) in capturing shared linguistic knowledge.\nHowever, less attention has been paid to their limitations. In this paper, we\ninvestigate the multilingual BERT for two known issues of the monolingual\nmodels: anisotropic embedding space and outlier dimensions. We show that,\nunlike its monolingual counterpart, the multilingual BERT model exhibits no\noutlier dimension in its representations while it has a highly anisotropic\nspace. There are a few dimensions in the monolingual BERT with high\ncontributions to the anisotropic distribution. However, we observe no such\ndimensions in the multilingual BERT. Furthermore, our experimental results\ndemonstrate that increasing the isotropy of multilingual space can\nsignificantly improve its representation power and performance, similarly to\nwhat had been observed for monolingual CWRs on semantic similarity tasks. Our\nanalysis indicates that, despite having different degenerated directions, the\nembedding spaces in various languages tend to be partially similar with respect\nto their structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained style control in Transformer-based Text-to-speech Synthesis. (arXiv:2110.06306v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.06306","description":"<p>In this paper, we present a novel architecture to realize fine-grained style\ncontrol on the transformer-based text-to-speech synthesis (TransformerTTS).\nSpecifically, we model the speaking style by extracting a time sequence of\nlocal style tokens (LST) from the reference speech. The existing content\nencoder in TransformerTTS is then replaced by our designed cross-attention\nblocks for fusion and alignment between content and style. As the fusion is\nperformed along with the skip connection, our cross-attention block provides a\ngood inductive bias to gradually infuse the phoneme representation with a given\nstyle. Additionally, we prevent the style embedding from encoding linguistic\ncontent by randomly truncating LST during training and using wav2vec 2.0\nfeatures. Experiments show that with fine-grained style control, our system\nperforms better in terms of naturalness, intelligibility, and style\ntransferability. Our code and samples are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators. (arXiv:2110.06609v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06609","description":"<p>Prompting has recently been shown as a promising approach for applying\npre-trained language models to perform downstream tasks. We present Multi-Stage\nPrompting (MSP), a simple and automatic approach for leveraging pre-trained\nlanguage models to translation tasks. To better mitigate the discrepancy\nbetween pre-training and translation, MSP divides the translation process via\npre-trained language models into multiple separate stages: the encoding stage,\nthe re-encoding stage, and the decoding stage. During each stage, we\nindependently apply different continuous prompts for allowing pre-trained\nlanguage models better shift to translation tasks. We conduct extensive\nexperiments on three translation tasks. Experiments show that our method can\nsignificantly improve the translation performance of pre-trained language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILM: Following Instructions in Language with Modular Methods. (arXiv:2110.07342v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07342","description":"<p>Recent methods for embodied instruction following are typically trained\nend-to-end using imitation learning. This often requires the use of expert\ntrajectories and low-level language instructions. Such approaches assume that\nneural states will integrate multimodal semantics to perform state tracking,\nbuilding spatial memory, exploration, and long-term planning. In contrast, we\npropose a modular method with structured representations that (1) builds a\nsemantic map of the scene and (2) performs exploration with a semantic search\npolicy, to achieve the natural language goal. Our modular method achieves SOTA\nperformance (24.46 %) with a substantial (8.17 % absolute) gap from previous\nwork while using less data by eschewing both expert trajectories and low-level\ninstructions. Leveraging low-level language, however, can further increase our\nperformance (26.49 %). Our findings suggest that an explicit spatial memory and\na semantic search policy can provide a stronger and more general representation\nfor state-tracking and guidance, even in the absence of expert trajectories or\nlow-level instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">So Yeon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer. (arXiv:2110.07904v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07904","description":"<p>There has been growing interest in parameter-efficient methods to apply\npre-trained language models to downstream tasks. Building on the Prompt Tuning\napproach of Lester et al. (2021), which learns task-specific soft prompts to\ncondition a frozen pre-trained model to perform different tasks, we propose a\nnovel prompt-based transfer learning approach called SPoT: Soft Prompt\nTransfer. SPoT first learns a prompt on one or more source tasks and then uses\nit to initialize the prompt for a target task. We show that SPoT significantly\nboosts the performance of Prompt Tuning across many tasks. More remarkably,\nacross all model sizes, SPoT matches or outperforms standard Model Tuning\n(which fine-tunes all model parameters) on the SuperGLUE benchmark, while using\nup to 27,000x fewer task-specific parameters. To understand where SPoT is most\neffective, we conduct a large-scale study on task transferability with 26 NLP\ntasks in 160 combinations, and demonstrate that many tasks can benefit each\nother via prompt transfer. Finally, we propose an efficient retrieval approach\nthat interprets task prompts as task embeddings to identify similar tasks and\npredict the most transferable source tasks for a novel target task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization. (arXiv:2110.08207v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08207","description":"<p>Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks (Brown et al., 2020). It has been\nhypothesized that this is a consequence of implicit multitask learning in\nlanguage models' pretraining (Radford et al., 2019). Can zero-shot\ngeneralization instead be directly induced by explicit multitask learning? To\ntest this question at scale, we develop a system for easily mapping any natural\nlanguage tasks into a human-readable prompted form. We convert a large set of\nsupervised datasets, each with multiple prompts with diverse wording. These\nprompted datasets allow for benchmarking the ability of a model to perform\ncompletely held-out tasks. We fine-tune a pretrained encoder-decoder model\n(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a\nwide variety of tasks. The model attains strong zero-shot performance on\nseveral standard datasets, often outperforming models up to 16x its size.\nFurther, our approach attains strong performance on a subset of tasks from the\nBIG-bench benchmark, outperforming models up to 6x its size. All trained models\nare available at https://github.com/bigscience-workshop/t-zero and all prompts\nare available at https://github.com/bigscience-workshop/promptsource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1\">Antoine Chaffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiegler_A/0/1/0/all/0/1\">Arnaud Stiegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_A/0/1/0/all/0/1\">Arun Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shanya Sharma Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szczechla_E/0/1/0/all/0/1\">Eliza Szczechla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_D/0/1/0/all/0/1\">Debajyoti Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jonathan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mike Tian-Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manica_M/0/1/0/all/0/1\">Matteo Manica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeraj_T/0/1/0/all/0/1\">Trishala Neeraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozen_J/0/1/0/all/0/1\">Jos Rozen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1\">Thibault Fevry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1\">Jason Alan Fries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teehan_R/0/1/0/all/0/1\">Ryan Teehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bers_T/0/1/0/all/0/1\">Tali Bers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1\">Thomas Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generated Knowledge Prompting for Commonsense Reasoning. (arXiv:2110.08387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08387","description":"<p>It remains an open question whether incorporating external knowledge benefits\ncommonsense reasoning while maintaining the flexibility of pretrained sequence\nmodels. To investigate this question, we develop generated knowledge prompting,\nwhich consists of generating knowledge from a language model, then providing\nthe knowledge as additional input when answering a question. Our method does\nnot require task-specific supervision for knowledge integration, or access to a\nstructured knowledge base, yet it improves performance of large-scale,\nstate-of-the-art models on four commonsense reasoning tasks, achieving\nstate-of-the-art results on numerical commonsense (NumerSense), general\ncommonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks.\nGenerated knowledge prompting highlights large-scale language models as\nflexible sources of external knowledge for improving commonsense reasoning. Our\ncode is available at \\url{github.com/liujch1998/GKP}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08486","description":"<p>The ability to sequence unordered events is an essential skill to comprehend\nand reason about real world task procedures, which often requires thorough\nunderstanding of temporal common sense and multimodal information, as these\nprocedures are often communicated through a combination of texts and images.\nSuch capability is essential for applications such as sequential task planning\nand multi-source instruction summarization. While humans are capable of\nreasoning about and sequencing unordered multimodal procedural instructions,\nwhether current machine learning models have such essential capability is still\nan open question. In this work, we benchmark models' capability of reasoning\nover and sequencing unordered multimodal instructions by curating datasets from\npopular online instructional manuals and collecting comprehensive human\nannotations. We find models not only perform significantly worse than humans\nbut also seem incapable of efficiently utilizing the multimodal information. To\nimprove machines' performance on multimodal event sequencing, we propose\nsequentiality-aware pretraining techniques that exploit the sequential\nalignment properties of both texts and images, resulting in &gt; 5% significant\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alex Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1\">Pegah Alipoormolabashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1\">Marjorie Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph Weischedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization. (arXiv:2110.08499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08499","description":"<p>We introduce PRIMERA, a pre-trained model for multi-document representation\nwith a focus on summarization that reduces the need for dataset-specific\narchitectures and large amounts of fine-tuning labeled data. PRIMERA uses our\nnewly proposed pre-training objective designed to teach the model to connect\nand aggregate information across documents. It also uses efficient\nencoder-decoder transformers to simplify the processing of concatenated input\ndocuments. With extensive experiments on 6 multi-document summarization\ndatasets from 3 different domains on zero-shot, few-shot and full-supervised\nsettings, PRIMERA outperforms current state-of-the-art dataset-specific and\npre-trained models on most of these settings with large margins. The code and\npre-trained models can be found at \\url{https://github.com/allenai/PRIMER}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking. (arXiv:2112.01206v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2112.01206","description":"<p>The goal of local citation recommendation is to recommend a missing reference\nfrom the local citation context and optionally also from the global context. To\nbalance the tradeoff between speed and accuracy of citation recommendation in\nthe context of a large-scale paper database, a viable approach is to first\nprefetch a limited number of relevant documents using efficient ranking methods\nand then to perform a fine-grained reranking using more sophisticated models.\nIn that vein, BM25 has been found to be a tough-to-beat approach to\nprefetching, which is why recent work has focused mainly on the reranking step.\nEven so, we explore prefetching with nearest neighbor search among text\nembeddings constructed by a hierarchical attention network. When coupled with a\nSciBERT reranker fine-tuned on local citation recommendation tasks, our\nhierarchical Attention encoder (HAtten) achieves high prefetch recall for a\ngiven number of candidates to be reranked. Consequently, our reranker requires\nfewer prefetch candidates to rerank, yet still achieves state-of-the-art\nperformance on various local citation recommendation datasets such as ACL-200,\nFullTextPeerRead, RefSeer, and arXiv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingqiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1\">Richard H.R. Hahnloser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TiltedBERT: Resource Adjustable Version of BERT. (arXiv:2201.03327v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03327","description":"<p>In this paper, a novel adjustable fine-tuning method is proposed that\nimproves the training and inference time of the BERT model on downstream tasks.\nIn the proposed method, first, the more important word vectors are detected in\neach layer by the proposed Attention Context Contribution (ACC) metric. Second,\nthe less important ones are eliminated with the proposed strategy. In the\nTiltedBERT method, the word vector elimination rate in each layer is controlled\nby the Tilt-Rate hyper-parameter, and the model learns to work with a\nconsiderably lower number of Floating Point Operations (FLOPs) than the\noriginal BERTbase model. The proposed method does not need any extra training\nsteps, and also it can be generalized to other transformer-based models. The\nextensive experiments show that the word vectors in higher layers have less\ncontribution that can be eliminated and improve the training and inference\ntime. Experimental results on extensive sentiment analysis, classification and\nregression datasets, and benchmarks like IMDB and GLUE showed that the\nTiltedBERT is effective in various datasets. TiltedBERT improves the inference\ntime of BERTbase up to 5.3 times with less than 0.85% accuracy drop on average.\nAfter the fine-tuning by the offline-tuning property, the inference time of the\nmodel can be adjusted for a wide range of Tilt-Rate selection. Also, A\nmathematical speedup analysis is proposed to estimate the TiltedBERT methods\nspeedup accurately. With the help of this analysis, the proper Tilt-Rate value\ncan be selected before finetuning and during offline-tuning phases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kachuee_S/0/1/0/all/0/1\">Sajjad Kachuee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifkhani_M/0/1/0/all/0/1\">Mohammad Sharifkhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02113","description":"<p>Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKGC/tree/main/GenKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks. (arXiv:2202.12499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12499","description":"<p>This paper focuses on the Data Augmentation for low-resource Natural Language\nUnderstanding (NLU) tasks. We propose Prompt-based D}ata Augmentation model\n(PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable\nvectors) in the frozen Pre-trained Language Models (PLMs). This avoids human\neffort in collecting unlabeled in-domain data and maintains the quality of\ngenerated synthetic data. In addition, PromDA generates synthetic data via two\ndifferent views and filters out the low-quality data using NLU models.\nExperiments on four benchmarks show that synthetic data produced by PromDA\nsuccessfully boost up the performance of NLU models which consistently\noutperform several competitive baseline models, including a state-of-the-art\nsemi-supervised model using unlabeled in-domain data. The synthetic data from\nPromDA are also complementary with unlabeled in-domain data. The NLU models can\nbe further improved when they are combined for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation. (arXiv:2202.13663v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13663","description":"<p>Most dominant neural machine translation (NMT) models are restricted to make\npredictions only according to the local context of preceding words in a\nleft-to-right manner. Although many previous studies try to incorporate global\ninformation into NMT models, there still exist limitations on how to\neffectively exploit bidirectional global context. In this paper, we propose a\nConfidence Based Bidirectional Global Context Aware (CBBGCA) training framework\nfor NMT, where the NMT model is jointly trained with an auxiliary conditional\nmasked language model (CMLM). The training consists of two stages: (1)\nmulti-task joint training; (2) confidence based knowledge distillation. At the\nfirst stage, by sharing encoder parameters, the NMT model is additionally\nsupervised by the signal from the CMLM decoder that contains bidirectional\nglobal contexts. Moreover, at the second stage, using the CMLM as teacher, we\nfurther pertinently incorporate bidirectional global context to the NMT model\non its unconfidently-predicted target words via knowledge distillation.\nExperimental results show that our proposed CBBGCA training framework\nsignificantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on\nthree large-scale translation datasets, namely WMT'14 English-to-German, WMT'19\nChinese-to-English and WMT'14 English-to-French, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chulun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongji Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OneRel:Joint Entity and Relation Extraction with One Module in One Step. (arXiv:2203.05412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05412","description":"<p>Joint entity and relation extraction is an essential task in natural language\nprocessing and knowledge graph construction. Existing approaches usually\ndecompose the joint extraction task into several basic modules or processing\nsteps to make it easy to conduct. However, such a paradigm ignores the fact\nthat the three elements of a triple are interdependent and indivisible.\nTherefore, previous joint methods suffer from the problems of cascading errors\nand redundant information. To address these issues, in this paper, we propose a\nnovel joint entity and relation extraction model, named OneRel, which casts\njoint extraction as a fine-grained triple classification problem. Specifically,\nour model consists of a scoring-based classifier and a relation-specific horns\ntagging strategy. The former evaluates whether a token pair and a relation\nbelong to a factual triple. The latter ensures a simple but effective decoding\nprocess. Extensive experimental results on two widely used datasets demonstrate\nthat the proposed method performs better than the state-of-the-art baselines,\nand delivers consistent performance gain on complex scenarios of various\noverlapping patterns and multiple triples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yu-Ming Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Language Modeling with Sparse all-MLP. (arXiv:2203.06850v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06850","description":"<p>All-MLP architectures have attracted increasing interest as an alternative to\nattention-based models. In NLP, recent work like gMLP shows that all-MLPs can\nmatch Transformers in language modeling, but still lag behind in downstream\ntasks. In this work, we analyze the limitations of MLPs in expressiveness, and\npropose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature\nand input (token) dimensions. Such sparse all-MLPs significantly increase model\ncapacity and expressiveness while keeping the compute constant. We address\ncritical challenges in incorporating conditional computation with two routing\nstrategies. The proposed sparse all-MLP improves language modeling perplexity\nand obtains up to 2$\\times$ improvement in training efficiency compared to both\nTransformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH\nLayers) as well as dense Transformers and all-MLPs. Finally, we evaluate its\nzero-shot in-context learning performance on six downstream tasks, and find\nthat it surpasses Transformer-based MoEs and dense Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shleifer_S/0/1/0/all/0/1\">Sam Shleifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting associations and meanings of objects depicted in artworks through bi-modal deep networks. (arXiv:2203.07026v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07026","description":"<p>We present a novel bi-modal system based on deep networks to address the\nproblem of learning associations and simple meanings of objects depicted in\n\"authored\" images, such as fine art paintings and drawings. Our overall system\nprocesses both the images and associated texts in order to learn associations\nbetween images of individual objects, their identities and the abstract\nmeanings they signify. Unlike past deep nets that describe depicted objects and\ninfer predicates, our system identifies meaning-bearing objects (\"signifiers\")\nand their associations (\"signifieds\") as well as basic overall meanings for\ntarget artworks. Our system had precision of 48% and recall of 78% with an F1\nmetric of 0.6 on a curated set of Dutch vanitas paintings, a genre celebrated\nfor its concentration on conveying a meaning of great import at the time of\ntheir execution. We developed and tested our system on fine art paintings but\nour general methods can be applied to other authored images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kell_G/0/1/0/all/0/1\">Gregory Kell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1\">Ryan-Rhys Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourached_A/0/1/0/all/0/1\">Anthony Bourached</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stork_D/0/1/0/all/0/1\">David G. Stork</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data. (arXiv:2203.07264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07264","description":"<p>Procedures are inherently hierarchical. To \"make videos\", one may need to\n\"purchase a camera\", which in turn may require one to \"set a budget\". While\nsuch hierarchical knowledge is critical for reasoning about complex procedures,\nmost existing work has treated procedures as shallow structures without\nmodeling the parent-child relation. In this work, we attempt to construct an\nopen-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a\nwebsite containing more than 110k instructional articles, each documenting the\nsteps to carry out a complex procedure. To this end, we develop a simple and\nefficient method that links steps (e.g., \"purchase a camera\") in an article to\nother articles with similar goals (e.g., \"how to choose a camera\"), recursively\nconstructing the KB. Our method significantly outperforms several strong\nbaselines according to automatic evaluation, human judgment, and application to\ndownstream tasks such as instructional video retrieval.\n</p>\n<p>A demo with partial data can be found at https://wikihow-hierarchy.github.io.\nThe code and the data are at https://github.com/shuyanzhou/wikihow_hierarchy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer. (arXiv:2203.07519v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07519","description":"<p>Pre-trained language models are still far from human performance in tasks\nthat need understanding of properties (e.g. appearance, measurable quantity)\nand affordances of everyday objects in the real world since the text lacks such\ninformation due to reporting bias. In this work, we study whether integrating\nvisual knowledge into a language model can fill the gap. We investigate two\ntypes of knowledge transfer: (1) text knowledge transfer using image captions\nthat may contain enriched visual knowledge and (2) cross-modal knowledge\ntransfer using both images and captions with vision-language training\nobjectives. On 5 downstream tasks that may need visual knowledge to solve the\nproblem, we perform extensive empirical comparisons over the presented\nobjectives. Our experiments show that visual knowledge transfer can improve\nperformance in both low-resource and fully supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation. (arXiv:2203.08394v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08394","description":"<p>Back-translation is a critical component of Unsupervised Neural Machine\nTranslation (UNMT), which generates pseudo parallel data from target\nmonolingual data. A UNMT model is trained on the pseudo parallel data with\ntranslated source, and translates natural source sentences in inference. The\nsource discrepancy between training and inference hinders the translation\nperformance of UNMT models. By carefully designing experiments, we identify two\nrepresentative characteristics of the data gap in source: (1) style gap (i.e.,\ntranslated vs. natural text style) that leads to poor generalization\ncapability; (2) content gap that induces the model to produce hallucination\ncontent biased towards the target language. To narrow the data gap, we propose\nan online self-training approach, which simultaneously uses the pseudo parallel\ndata {natural source, translated target} to mimic the inference scenario.\nExperimental results on several widely-used language pairs show that our\napproach outperforms two strong baselines (XLM and MASS) by remedying the style\nand content gaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model. (arXiv:2203.08459v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08459","description":"<p>Pre-trained language models such as BERT have been successful at tackling\nmany natural language processing tasks. However, the unsupervised sub-word\ntokenization methods commonly used in these models (e.g., byte-pair encoding -\nBPE) are sub-optimal at handling morphologically rich languages. Even given a\nmorphological analyzer, naive sequencing of morphemes into a standard BERT\narchitecture is inefficient at capturing morphological compositionality and\nexpressing word-relative syntactic regularities. We address these challenges by\nproposing a simple yet effective two-tier BERT architecture that leverages a\nmorphological analyzer and explicitly represents morphological\ncompositionality. Despite the success of BERT, most of its evaluations have\nbeen conducted on high-resource languages, obscuring its applicability on\nlow-resource languages. We evaluate our proposed method on the low-resource\nmorphologically rich Kinyarwanda language, naming the proposed model\narchitecture KinyaBERT. A robust set of experimental results reveal that\nKinyaBERT outperforms solid baselines by 2% in F1 score on a named entity\nrecognition task and by 4.3% in average score of a machine-translated GLUE\nbenchmark. KinyaBERT fine-tuning has better convergence and achieves more\nrobust results on multiple tasks even in the presence of translation noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nzeyimana_A/0/1/0/all/0/1\">Antoine Nzeyimana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubungo_A/0/1/0/all/0/1\">Andre Niyongabo Rubungo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Disparities in Dermatology AI Performance on a Diverse, Curated Clinical Image Set. (arXiv:2203.08807v1 [eess.IV])","link":"http://arxiv.org/abs/2203.08807","description":"<p>Access to dermatological care is a major issue, with an estimated 3 billion\npeople lacking access to care globally. Artificial intelligence (AI) may aid in\ntriaging skin diseases. However, most AI models have not been rigorously\nassessed on images of diverse skin tones or uncommon diseases. To ascertain\npotential biases in algorithm performance in this context, we curated the\nDiverse Dermatology Images (DDI) dataset-the first publicly available, expertly\ncurated, and pathologically confirmed image dataset with diverse skin tones.\nUsing this dataset of 656 images, we show that state-of-the-art dermatology AI\nmodels perform substantially worse on DDI, with receiver operator curve area\nunder the curve (ROC-AUC) dropping by 27-36 percent compared to the models'\noriginal test results. All the models performed worse on dark skin tones and\nuncommon diseases, which are represented in the DDI dataset. Additionally, we\nfind that dermatologists, who typically provide visual labels for AI training\nand test datasets, also perform worse on images of dark skin tones and uncommon\ndiseases compared to ground truth biopsy annotations. Finally, fine-tuning AI\nmodels on the well-characterized and diverse DDI images closed the performance\ngap between light and dark skin tones. Moreover, algorithms fine-tuned on\ndiverse skin tones outperformed dermatologists on identifying malignancy on\nimages of dark skin tones. Our findings identify important weaknesses and\nbiases in dermatology AI that need to be addressed to ensure reliable\napplication to diverse patients and diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Daneshjou_R/0/1/0/all/0/1\">Roxana Daneshjou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vodrahalli_K/0/1/0/all/0/1\">Kailas Vodrahalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Novoa_R/0/1/0/all/0/1\">Roberto A Novoa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jenkins_M/0/1/0/all/0/1\">Melissa Jenkins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rotemberg_V/0/1/0/all/0/1\">Veronica Rotemberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ko_J/0/1/0/all/0/1\">Justin Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Swetter_S/0/1/0/all/0/1\">Susan M Swetter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bailey_E/0/1/0/all/0/1\">Elizabeth E Bailey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gevaert_O/0/1/0/all/0/1\">Olivier Gevaert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_P/0/1/0/all/0/1\">Pritam Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phung_M/0/1/0/all/0/1\">Michelle Phung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yekrang_K/0/1/0/all/0/1\">Kiana Yekrang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fong_B/0/1/0/all/0/1\">Bradley Fong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahasrabudhe_R/0/1/0/all/0/1\">Rachna Sahasrabudhe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Allerup_J/0/1/0/all/0/1\">Johan A. C. Allerup</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Okata_Karigane_U/0/1/0/all/0/1\">Utako Okata-Karigane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiou_A/0/1/0/all/0/1\">Albert Chiou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Deep Learning to Enhance Breast Cancer Detection on Screening Mammography. (arXiv:2203.08812v1 [eess.IV])","link":"http://arxiv.org/abs/2203.08812","description":"<p>A major limitation in applying deep learning to artificial intelligence (AI)\nsystems is the scarcity of high-quality curated datasets. We investigate strong\naugmentation based self-supervised learning (SSL) techniques to address this\nproblem. Using breast cancer detection as an example, we first identify a\nmammogram-specific transformation paradigm and then systematically compare four\nrecent SSL methods representing a diversity of approaches. We develop a method\nto convert a pretrained model from making predictions on uniformly tiled\npatches to whole images, and an attention-based pooling method that improves\nthe classification performance. We found that the best SSL model substantially\noutperformed the baseline supervised model. The best SSL model also improved\nthe data efficiency of sample labeling by nearly 4-fold and was highly\ntransferrable from one dataset to another. SSL represents a major breakthrough\nin computer vision and may help the AI for medical imaging field to shift away\nfrom supervised learning and dependency on scarce labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Miller_J/0/1/0/all/0/1\">John D. Miller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arasu_V/0/1/0/all/0/1\">Vignesh A. Arasu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pu_A/0/1/0/all/0/1\">Albert X. Pu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Margolies_L/0/1/0/all/0/1\">Laurie R. Margolies</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sieh_W/0/1/0/all/0/1\">Weiva Sieh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Example Perplexity. (arXiv:2203.08813v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08813","description":"<p>Some examples are easier for humans to classify than others. The same should\nbe true for deep neural networks (DNNs). We use the term example perplexity to\nrefer to the level of difficulty of classifying an example. In this paper, we\npropose a method to measure the perplexity of an example and investigate what\nfactors contribute to high example perplexity. The related codes and resources\nare available at https://github.com/vaynexie/Example-Perplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weiyan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanfang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao-Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caleb Chen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunpeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DePS: An improved deep learning model for de novo peptide sequencing. (arXiv:2203.08820v1 [q-bio.QM])","link":"http://arxiv.org/abs/2203.08820","description":"<p>De novo peptide sequencing from mass spectrometry data is an important method\nfor protein identification. Recently, various deep learning approaches were\napplied for de novo peptide sequencing and DeepNovoV2 is one of the\nrepresetative models. In this study, we proposed an enhanced model, DePS, which\ncan improve the accuracy of de novo peptide sequencing even with missing signal\npeaks or large number of noisy peaks in tandem mass spectrometry data. It is\nshowed that, for the same test set of DeepNovoV2, the DePS model achieved\nexcellent results of 74.22%, 74.21% and 41.68% for amino acid recall, amino\nacid precision and peptide recall respectively. Furthermore, the results\nsuggested that DePS outperforms DeepNovoV2 on the cross species dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ge_C/0/1/0/all/0/1\">Cheng Ge</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lu_Y/0/1/0/all/0/1\">Yi Lu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Qu_J/0/1/0/all/0/1\">Jia Qu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xie_L/0/1/0/all/0/1\">Liangxu Xie</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kong_R/0/1/0/all/0/1\">Ren Kong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chang_S/0/1/0/all/0/1\">Shan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding robustness and generalization of artificial neural networks through Fourier masks. (arXiv:2203.08822v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08822","description":"<p>Despite the enormous success of artificial neural networks (ANNs) in many\ndisciplines, the characterization of their computations and the origin of key\nproperties such as generalization and robustness remain open questions. Recent\nliterature suggests that robust networks with good generalization properties\ntend to be biased towards processing low frequencies in images. To explore the\nfrequency bias hypothesis further, we develop an algorithm that allows us to\nlearn modulatory masks highlighting the essential input frequencies needed for\npreserving a trained network's performance. We achieve this by imposing\ninvariance in the loss with respect to such modulations in the input\nfrequencies. We first use our method to test the low-frequency preference\nhypothesis of adversarially trained or data-augmented networks. Our results\nsuggest that adversarially robust networks indeed exhibit a low-frequency bias\nbut we find this bias is also dependent on directions in frequency space.\nHowever, this is not necessarily true for other types of data augmentation. Our\nresults also indicate that the essential frequencies in question are\neffectively the ones used to achieve generalization in the first place.\nSurprisingly, images seen through these modulatory masks are not recognizable\nand resemble texture-like patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karantzas_N/0/1/0/all/0/1\">Nikos Karantzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besier_E/0/1/0/all/0/1\">Emma Besier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caro_J/0/1/0/all/0/1\">Josue Ortega Caro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1\">Xaq Pitkow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1\">Andreas S. Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ankit B. Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anselmi_F/0/1/0/all/0/1\">Fabio Anselmi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real-Time Region Tracking Algorithm Tailored to Endoscopic Video with Open-Source Implementation. (arXiv:2203.08858v1 [eess.IV])","link":"http://arxiv.org/abs/2203.08858","description":"<p>With a video data source, such as multispectral video acquired during\nadministration of fluorescent tracers, extraction of time-resolved data\ntypically requires the compensation of motion. While this can be done manually,\nwhich is arduous, or using off-the-shelf object tracking software, which often\nyields unsatisfactory performance, we present an algorithm which is simple and\nperformant. Most importantly, we provide an open-source implementation, with an\neasy-to-use interface for researchers not inclined to write their own code, as\nwell as Python modules that can be used programmatically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Epperlein_J/0/1/0/all/0/1\">Jonathan P. Epperlein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuk_S/0/1/0/all/0/1\">Sergiy Zhuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SC2: Supervised Compression for Split Computing. (arXiv:2203.08875v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08875","description":"<p>Split computing distributes the execution of a neural network (e.g., for a\nclassification task) between a mobile device and a more powerful edge server. A\nsimple alternative to splitting the network is to carry out the supervised task\npurely on the edge server while compressing and transmitting the full data, and\nmost approaches have barely outperformed this baseline. This paper proposes a\nnew approach for discretizing and entropy-coding intermediate feature\nactivations to efficiently transmit them from the mobile device to the edge\nserver. We show that a efficient splittable network architecture results from a\nthree-way tradeoff between (a) minimizing the computation on the mobile device,\n(b) minimizing the size of the data to be transmitted, and (c) maximizing the\nmodel's prediction performance. We propose an architecture based on this\ntradeoff and train the splittable network and entropy model in a knowledge\ndistillation framework. In an extensive set of experiments involving three\nvision tasks, three datasets, nine baselines, and more than 180 trained models,\nwe show that our approach improves supervised rate-distortion tradeoffs while\nmaintaining a considerably smaller encoder size. We also release sc2bench, an\ninstallable Python package, to encourage and facilitate future studies on\nsupervised compression for split computing (SC2).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer Ensembles: A Single-Pass Uncertainty Estimation in Deep Learning for Segmentation. (arXiv:2203.08878v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08878","description":"<p>Uncertainty estimation in deep learning has become a leading research field\nin medical image analysis due to the need for safe utilisation of AI algorithms\nin clinical practice. Most approaches for uncertainty estimation require\nsampling the network weights multiple times during testing or training multiple\nnetworks. This leads to higher training and testing costs in terms of time and\ncomputational resources. In this paper, we propose Layer Ensembles, a novel\nuncertainty estimation method that uses a single network and requires only a\nsingle pass to estimate predictive uncertainty of a network. Moreover, we\nintroduce an image-level uncertainty metric, which is more beneficial for\nsegmentation tasks compared to the commonly used pixel-wise metrics such as\nentropy and variance. We evaluate our approach on 2D and 3D, binary and\nmulti-class medical image segmentation tasks. Our method shows competitive\nresults with state-of-the-art Deep Ensembles, requiring only a single network\nand a single pass.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campello_V/0/1/0/all/0/1\">V&#xed;ctor Manuel Campello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moras_L/0/1/0/all/0/1\">Lidia Garrucho Moras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radeva_P/0/1/0/all/0/1\">Petia Radeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperbolic Uncertainty Aware Semantic Segmentation. (arXiv:2203.08881v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08881","description":"<p>Semantic segmentation (SS) aims to classify each pixel into one of the\npre-defined classes. This task plays an important role in self-driving cars and\nautonomous drones. In SS, many works have shown that most misclassified pixels\nare commonly near object boundaries with high uncertainties. However, existing\nSS loss functions are not tailored to handle these uncertain pixels during\ntraining, as these pixels are usually treated equally as confidently classified\npixels and cannot be embedded with arbitrary low distortion in Euclidean space,\nthereby degenerating the performance of SS. To overcome this problem, this\npaper designs a \"Hyperbolic Uncertainty Loss\" (HyperUL), which dynamically\nhighlights the misclassified and high-uncertainty pixels in Hyperbolic space\nduring training via the hyperbolic distances. The proposed HyperUL is model\nagnostic and can be easily applied to various neural architectures. After\nemploying HyperUL to three recent SS models, the experimental results on\nCityscapes and UAVid datasets reveal that the segmentation performance of\nexisting SS models can be consistently improved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bike Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaofeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roning_J/0/1/0/all/0/1\">Juha R&#xf6;ning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient Objects and Shadow Modeling Using RPC Cameras. (arXiv:2203.08896v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08896","description":"<p>We introduce the Satellite Neural Radiance Field (Sat-NeRF), a new end-to-end\nmodel for learning multi-view satellite photogrammetry in the wild. Sat-NeRF\ncombines some of the latest trends in neural rendering with native satellite\ncamera models, represented by rational polynomial coefficient (RPC) functions.\nThe proposed method renders new views and infers surface models of similar\nquality to those obtained with traditional state-of-the-art stereo pipelines.\nMulti-date images exhibit significant changes in appearance, mainly due to\nvarying shadows and transient objects (cars, vegetation). Robustness to these\nchallenges is achieved by a shadow-aware irradiance model and uncertainty\nweighting to deal with transient phenomena that cannot be explained by the\nposition of the sun. We evaluate Sat-NeRF using WorldView-3 images from\ndifferent locations and stress the advantages of applying a bundle adjustment\nto the satellite camera models prior to training. This boosts the network\nperformance and can optionally be used to extract additional cues for depth\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mari_R/0/1/0/all/0/1\">Roger Mar&#xed;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1\">Gabriele Facciolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehret_T/0/1/0/all/0/1\">Thibaud Ehret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gate-Shift-Fuse for Video Action Recognition. (arXiv:2203.08897v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08897","description":"<p>Convolutional Neural Networks are the de facto models for image recognition.\nHowever 3D CNNs, the straight forward extension of 2D CNNs for video\nrecognition, have not achieved the same success on standard action recognition\nbenchmarks. One of the main reasons for this reduced performance of 3D CNNs is\nthe increased computational complexity requiring large scale annotated datasets\nto train them in scale. 3D kernel factorization approaches have been proposed\nto reduce the complexity of 3D CNNs. Existing kernel factorization approaches\nfollow hand-designed and hard-wired techniques. In this paper we propose\nGate-Shift-Fuse (GSF), a novel spatio-temporal feature extraction module which\ncontrols interactions in spatio-temporal decomposition and learns to adaptively\nroute features through time and combine them in a data dependent manner. GSF\nleverages grouped spatial gating to decompose input tensor and channel\nweighting to fuse the decomposed tensors. GSF can be inserted into existing 2D\nCNNs to convert them into an efficient and high performing spatio-temporal\nfeature extractor, with negligible parameter and compute overhead. We perform\nan extensive analysis of GSF using two popular 2D CNN families and achieve\nstate-of-the-art or competitive performance on five standard action recognition\nbenchmarks. Code and models will be made publicly available at\nhttps://github.com/swathikirans/GSF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1\">Swathikiran Sudhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanz_O/0/1/0/all/0/1\">Oswald Lanz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Grading of Radiographic Knee Osteoarthritis Severity Combined with Joint Space Narrowing. (arXiv:2203.08914v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08914","description":"<p>The assessment of knee osteoarthritis (KOA) severity on knee X-rays is a\ncentral criteria for the use of total knee arthroplasty. However, this\nassessment suffers from imprecise standards and a remarkably high inter-reader\nvariability. An algorithmic, automated assessment of KOA severity could improve\noverall outcomes of knee replacement procedures by increasing the\nappropriateness of its use. We propose a novel deep learning-based five-step\nalgorithm to automatically grade KOA from posterior-anterior (PA) views of\nradiographs: (1) image preprocessing (2) localization of knees joints in the\nimage using the YOLO v3-Tiny model, (3) initial assessment of the severity of\nosteoarthritis using a convolutional neural network-based classifier, (4)\nsegmentation of the joints and calculation of the joint space narrowing (JSN),\nand (5), a combination of the JSN and the initial assessment to determine a\nfinal Kellgren-Lawrence (KL) score. Furthermore, by displaying the segmentation\nmasks used to make the assessment, our algorithm demonstrates a higher degree\nof transparency compared to typical \"black box\" deep learning classifiers. We\nperform a comprehensive evaluation using two public datasets and one dataset\nfrom our institution, and show that our algorithm reaches state-of-the art\nperformance. Moreover, we also collected ratings from multiple radiologists at\nour institution and showed that our algorithm performs at the radiologist\nlevel.\n</p>\n<p>The software has been made publicly available at\nhttps://github.com/MaciejMazurowski/osteoarthritis-classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1\">Hanxue Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Keyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colglazier_R/0/1/0/all/0/1\">Roy J. Colglazier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jichen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebhar_M/0/1/0/all/0/1\">Michael Lebhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_J/0/1/0/all/0/1\">Jonathan O&#x27;Donnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiranek_W/0/1/0/all/0/1\">William A. Jiranek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mather_R/0/1/0/all/0/1\">Richard C. Mather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+French_R/0/1/0/all/0/1\">Rob J. French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Said_N/0/1/0/all/0/1\">Nicholas Said</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jikai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Christine Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Pixel-Unshuffled Network for Lightweight Image Super-Resolution. (arXiv:2203.08921v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08921","description":"<p>Convolutional neural network (CNN) has achieved great success on image\nsuper-resolution (SR). However, most deep CNN-based SR models take massive\ncomputations to obtain high performance. Downsampling features for\nmulti-resolution fusion is an efficient and effective way to improve the\nperformance of visual recognition. Still, it is counter-intuitive in the SR\ntask, which needs to project a low-resolution input to high-resolution. In this\npaper, we propose a novel Hybrid Pixel-Unshuffled Network (HPUN) by introducing\nan efficient and effective downsampling module into the SR task. The network\ncontains pixel-unshuffled downsampling and Self-Residual Depthwise Separable\nConvolutions. Specifically, we utilize pixel-unshuffle operation to downsample\nthe input features and use grouped convolution to reduce the channels. Besides,\nwe enhance the depthwise convolution's performance by adding the input feature\nto its output. Experiments on benchmark datasets show that our HPUN achieves\nand surpasses the state-of-the-art reconstruction performance with fewer\nparameters and computation costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Songyao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards True Detail Restoration for Super-Resolution: A Benchmark and a Quality Metric. (arXiv:2203.08923v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08923","description":"<p>Super-resolution (SR) has become a widely researched topic in recent years.\nSR methods can improve overall image and video quality and create new\npossibilities for further content analysis. But the SR mainstream focuses\nprimarily on increasing the naturalness of the resulting image despite\npotentially losing context accuracy. Such methods may produce an incorrect\ndigit, character, face, or other structural object even though they otherwise\nyield good visual quality. Incorrect detail restoration can cause errors when\ndetecting and identifying objects both manually and automatically. To analyze\nthe detail-restoration capabilities of image and video SR models, we developed\na benchmark based on our own video dataset, which contains complex patterns\nthat SR models generally fail to correctly restore. We assessed 32 recent SR\nmodels using our benchmark and compared their ability to preserve scene\ncontext. We also conducted a crowd-sourced comparison of restored details and\ndeveloped an objective assessment metric that outperforms other quality metrics\nby correlation with subjective scores for this task. In conclusion, we provide\na deep analysis of benchmark results that yields insights for future SR-based\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyapustin_E/0/1/0/all/0/1\">Eugene Lyapustin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillova_A/0/1/0/all/0/1\">Anastasia Kirillova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meshchaninov_V/0/1/0/all/0/1\">Viacheslav Meshchaninov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimin_E/0/1/0/all/0/1\">Evgeney Zimin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karetin_N/0/1/0/all/0/1\">Nikolai Karetin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating Multimedia Summaries Using Tweets and Videos. (arXiv:2203.08931v1 [cs.CL])","link":"http://arxiv.org/abs/2203.08931","description":"<p>While popular televised events such as presidential debates or TV shows are\nairing, people provide commentary on them in real-time. In this paper, we\npropose a simple yet effective approach to combine social media commentary and\nvideos to create a multimedia summary of televised events. Our approach\nidentifies scenes from these events based on spikes of mentions of people\ninvolved in the event and automatically selects tweets and frames from the\nvideos that occur during the time period of the spike that talk about and show\nthe people being discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andy_A/0/1/0/all/0/1\">Anietie Andy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kriz_R/0/1/0/all/0/1\">Reno Kriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABN: Agent-Aware Boundary Networks for Temporal Action Proposal Generation. (arXiv:2203.08942v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08942","description":"<p>Temporal action proposal generation (TAPG) aims to estimate temporal\nintervals of actions in untrimmed videos, which is a challenging yet plays an\nimportant role in many tasks of video analysis and understanding. Despite the\ngreat achievement in TAPG, most existing works ignore the human perception of\ninteraction between agents and the surrounding environment by applying a deep\nlearning model as a black-box to the untrimmed videos to extract video visual\nrepresentation. Therefore, it is beneficial and potentially improve the\nperformance of TAPG if we can capture these interactions between agents and the\nenvironment. In this paper, we propose a novel framework named Agent-Aware\nBoundary Network (ABN), which consists of two sub-networks (i) an Agent-Aware\nRepresentation Network to obtain both agent-agent and agents-environment\nrelationships in the video representation, and (ii) a Boundary Generation\nNetwork to estimate the confidence score of temporal intervals. In the\nAgent-Aware Representation Network, the interactions between agents are\nexpressed through local pathway, which operates at a local level to focus on\nthe motions of agents whereas the overall perception of the surroundings are\nexpressed through global pathway, which operates at a global level to perceive\nthe effects of agents-environment. Comprehensive evaluations on 20-action\nTHUMOS-14 and 200-action ActivityNet-1.3 datasets with different backbone\nnetworks (i.e C3D, SlowFast and Two-Stream) show that our proposed ABN robustly\noutperforms state-of-the-art methods regardless of the employed backbone\nnetwork on TAPG. We further examine the proposal quality by leveraging\nproposals generated by our method onto temporal action detection (TAD)\nframeworks and evaluate their detection performances. The source code can be\nfound in this URL https://github.com/vhvkhoa/TAPG-AgentEnvNetwork.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1\">Khoa Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Sang Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CapsNet for Medical Image Segmentation. (arXiv:2203.08948v1 [eess.IV])","link":"http://arxiv.org/abs/2203.08948","description":"<p>Convolutional Neural Networks (CNNs) have been successful in solving tasks in\ncomputer vision including medical image segmentation due to their ability to\nautomatically extract features from unstructured data. However, CNNs are\nsensitive to rotation and affine transformation and their success relies on\nhuge-scale labeled datasets capturing various input variations. This network\nparadigm has posed challenges at scale because acquiring annotated data for\nmedical segmentation is expensive, and strict privacy regulations. Furthermore,\nvisual representation learning with CNNs has its own flaws, e.g., it is\narguable that the pooling layer in traditional CNNs tends to discard positional\ninformation and CNNs tend to fail on input images that differ in orientations\nand sizes. Capsule network (CapsNet) is a recent new architecture that has\nachieved better robustness in representation learning by replacing pooling\nlayers with dynamic routing and convolutional strides, which has shown\npotential results on popular tasks such as classification, recognition,\nsegmentation, and natural language processing. Different from CNNs, which\nresult in scalar outputs, CapsNet returns vector outputs, which aim to preserve\nthe part-whole relationships. In this work, we first introduce the limitations\nof CNNs and fundamentals of CapsNet. We then provide recent developments of\nCapsNet for the task of medical image segmentation. We finally discuss various\neffective network architectures to implement a CapsNet for both 2D images and\n3D volumetric medical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quinn_K/0/1/0/all/0/1\">Kyle Quinn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Hien Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning of NAS for Few-shot Learning in Medical Image Applications. (arXiv:2203.08951v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08951","description":"<p>Deep learning methods have been successful in solving tasks in machine\nlearning and have made breakthroughs in many sectors owing to their ability to\nautomatically extract features from unstructured data. However, their\nperformance relies on manual trial-and-error processes for selecting an\nappropriate network architecture, hyperparameters for training, and\npre-/post-procedures. Even though it has been shown that network architecture\nplays a critical role in learning feature representation feature from data and\nthe final performance, searching for the best network architecture is\ncomputationally intensive and heavily relies on researchers' experience.\nAutomated machine learning (AutoML) and its advanced techniques i.e. Neural\nArchitecture Search (NAS) have been promoted to address those limitations. Not\nonly in general computer vision tasks, but NAS has also motivated various\napplications in multiple areas including medical imaging. In medical imaging,\nNAS has significant progress in improving the accuracy of image classification,\nsegmentation, reconstruction, and more. However, NAS requires the availability\nof large annotated data, considerable computation resources, and pre-defined\ntasks. To address such limitations, meta-learning has been adopted in the\nscenarios of few-shot learning and multiple tasks. In this book chapter, we\nfirst present a brief review of NAS by discussing well-known approaches in\nsearch space, search strategy, and evaluation strategy. We then introduce\nvarious NAS approaches in medical imaging with different applications such as\nclassification, segmentation, detection, reconstruction, etc. Meta-learning in\nNAS for few-shot learning and multiple tasks is then explained. Finally, we\ndescribe several open problems in NAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_H/0/1/0/all/0/1\">Hieu Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training. (arXiv:2203.08959v1 [cs.LG])","link":"http://arxiv.org/abs/2203.08959","description":"<p>In this paper, we introduce a novel neural network training framework that\nincreases model's adversarial robustness to adversarial attacks while\nmaintaining high clean accuracy by combining contrastive learning (CL) with\nadversarial training (AT). We propose to improve model robustness to\nadversarial attacks by learning feature representations that are consistent\nunder both data augmentations and adversarial perturbations. We leverage\ncontrastive learning to improve adversarial robustness by considering an\nadversarial example as another positive example, and aim to maximize the\nsimilarity between random augmentations of data samples and their adversarial\nexample, while constantly updating the classification head in order to avoid a\ncognitive dissociation between the classification head and the embedding space.\nThis dissociation is caused by the fact that CL updates the network up to the\nembedding space, while freezing the classification head which is used to\ngenerate new positive adversarial examples. We validate our method, Contrastive\nLearning with Adversarial Features(CLAF), on the CIFAR-10 dataset on which it\noutperforms both robust accuracy and clean accuracy over alternative supervised\nand self-supervised adversarial learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahamim_A/0/1/0/all/0/1\">Adir Rahamim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeh_I/0/1/0/all/0/1\">Itay Naeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-Unet: A Context-aware Point-based Neural Network for Volumetric Segmentation. (arXiv:2203.08964v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08964","description":"<p>Medical image analysis using deep learning has recently been prevalent,\nshowing great performance for various downstream tasks including medical image\nsegmentation and its sibling, volumetric image segmentation. Particularly, a\ntypical volumetric segmentation network strongly relies on a voxel grid\nrepresentation which treats volumetric data as a stack of individual voxel\n`slices', which allows learning to segment a voxel grid to be as\nstraightforward as extending existing image-based segmentation networks to the\n3D domain. However, using a voxel grid representation requires a large memory\nfootprint, expensive test-time and limiting the scalability of the solutions.\nIn this paper, we propose Point-Unet, a novel method that incorporates the\nefficiency of deep learning with 3D point clouds into volumetric segmentation.\nOur key idea is to first predict the regions of interest in the volume by\nlearning an attentional probability map, which is then used for sampling the\nvolume into a sparse point cloud that is subsequently segmented using a\npoint-based neural network. We have conducted the experiments on the medical\nvolumetric segmentation task with both a small-scale dataset Pancreas and\nlarge-scale datasets BraTS18, BraTS19, and BraTS20 challenges. A comprehensive\nbenchmark on different metrics has shown that our context-aware Point-Unet\nrobustly outperforms the SOTA voxel-based networks at both accuracies, memory\nusage during training, and time consumption during testing. Our code is\navailable at https://github.com/VinAIResearch/Point-Unet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Ngoc-Vuong Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diep_G/0/1/0/all/0/1\">Gia-Han Diep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-UCaps: 3D Capsules Unet for Volumetric Image Segmentation. (arXiv:2203.08965v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08965","description":"<p>Medical image segmentation has been so far achieving promising results with\nConvolutional Neural Networks (CNNs). However, it is arguable that in\ntraditional CNNs, its pooling layer tends to discard important information such\nas positions. Moreover, CNNs are sensitive to rotation and affine\ntransformation. Capsule network is a data-efficient network design proposed to\novercome such limitations by replacing pooling layers with dynamic routing and\nconvolutional strides, which aims to preserve the part-whole relationships.\nCapsule network has shown a great performance in image recognition and natural\nlanguage processing, but applications for medical image segmentation,\nparticularly volumetric image segmentation, has been limited. In this work, we\npropose 3D-UCaps, a 3D voxel-based Capsule network for medical volumetric image\nsegmentation. We build the concept of capsules into a CNN by designing a\nnetwork with two pathways: the first pathway is encoded by 3D Capsule blocks,\nwhereas the second pathway is decoded by 3D CNNs blocks. 3D-UCaps, therefore\ninherits the merits from both Capsule network to preserve the spatial\nrelationship and CNNs to learn visual representation. We conducted experiments\non various datasets to demonstrate the robustness of 3D-UCaps including\niSeg-2017, LUNA16, Hippocampus, and Cardiac, where our method outperforms\nprevious Capsule networks and 3D-Unets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extensive Threat Analysis of Vein Attack Databases and Attack Detection by Fusion of Comparison Scores. (arXiv:2203.08972v1 [cs.CV])","link":"http://arxiv.org/abs/2203.08972","description":"<p>The last decade has brought forward many great contributions regarding\npresentation attack detection for the domain of finger and hand vein\nbiometrics. Among those contributions, one is able to find a variety of\ndifferent attack databases that are either private or made publicly available\nto the research community. However, it is not always shown whether the used\nattack samples hold the capability to actually deceive a realistic vein\nrecognition system. Inspired by previous works, this study provides a\nsystematic threat evaluation including three publicly available finger vein\nattack databases and one private dorsal hand vein database. To do so, 14\ndistinct vein recognition schemes are confronted with attack samples and the\npercentage of wrongly accepted attack samples is then reported as the Impostor\nAttack Presentation Match Rate. As a second step, comparison scores from\ndifferent recognition schemes are combined using score level fusion with the\ngoal of performing presentation attack detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuiki_J/0/1/0/all/0/1\">Johannes Schuiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linortner_M/0/1/0/all/0/1\">Michael Linortner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_G/0/1/0/all/0/1\">Georg Wimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhl_A/0/1/0/all/0/1\">Andreas Uhl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-diversity transfer network for generalized zero-shot learning via inner disagreement based OOD detector. (arXiv:2203.09017v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09017","description":"<p>Zero-shot learning (ZSL) aims to recognize objects from unseen classes, where\nthe kernel problem is to transfer knowledge from seen classes to unseen classes\nby establishing appropriate mappings between visual and semantic features. The\nknowledge transfer in many existing works is limited mainly due to the facts\nthat 1) the widely used visual features are global ones but not totally\nconsistent with semantic attributes; 2) only one mapping is learned in existing\nworks, which is not able to effectively model diverse visual-semantic\nrelations; 3) the bias problem in the generalized ZSL (GZSL) could not be\neffectively handled. In this paper, we propose two techniques to alleviate\nthese limitations. Firstly, we propose a Semantic-diversity transfer Network\n(SetNet) addressing the first two limitations, where 1) a multiple-attention\narchitecture and a diversity regularizer are proposed to learn multiple local\nvisual features that are more consistent with semantic attributes and 2) a\nprojector ensemble that geometrically takes diverse local features as inputs is\nproposed to model visual-semantic relations from diverse local perspectives.\nSecondly, we propose an inner disagreement based domain detection module (ID3M)\nfor GZSL to alleviate the third limitation, which picks out unseen-class data\nbefore class-level classification. Due to the absence of unseen-class data in\ntraining stage, ID3M employs a novel self-contained training scheme and detects\nout unseen-class data based on a designed inner disagreement criterion.\nExperimental results on three public datasets demonstrate that the proposed\nSetNet with the explored ID3M achieves a significant improvement against $30$\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanyi Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HybridNets: End-to-End Perception Network. (arXiv:2203.09035v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09035","description":"<p>End-to-end Network has become increasingly important in multi-tasking. One\nprominent example of this is the growing significance of a driving perception\nsystem in autonomous driving. This paper systematically studies an end-to-end\nperception network for multi-tasking and proposes several key optimizations to\nimprove accuracy. First, the paper proposes efficient segmentation head and\nbox/class prediction networks based on weighted bidirectional feature network.\nSecond, the paper proposes automatically customized anchor for each level in\nthe weighted bidirectional feature network. Third, the paper proposes an\nefficient training loss function and training strategy to balance and optimize\nnetwork. Based on these optimizations, we have developed an end-to-end\nperception network to perform multi-tasking, including traffic object\ndetection, drivable area segmentation and lane detection simultaneously, called\nHybridNets, which achieves better accuracy than prior art. In particular,\nHybridNets achieves 77.3 mean Average Precision on Berkeley DeepDrive Dataset,\noutperforms lane detection with 31.6 mean Intersection Over Union with 12.83\nmillion parameters and 15.6 billion floating-point operations. In addition, it\ncan perform visual perception tasks in real-time and thus is a practical and\naccurate solution to the multi-tasking problem. Code is available at\nhttps://github.com/datvuthanh/HybridNets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1\">Dat Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_B/0/1/0/all/0/1\">Bao Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hung Phan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Active Contour Model with Local Variance Force Term and Its Efficient Minimization Solver for Multi-phase Image Segmentation. (arXiv:2203.09036v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09036","description":"<p>In this paper, we propose an active contour model with a local variance force\n(LVF) term that can be applied to multi-phase image segmentation problems. With\nthe LVF, the proposed model is very effective in the segmentation of images\nwith noise. To solve this model efficiently, we represent the regularization\nterm by characteristic functions and then design a minimization algorithm based\non a modification of the iterative convolution-thresholding method (ICTM),\nnamely ICTM-LVF. This minimization algorithm enjoys the energy-decaying\nproperty under some conditions and has highly efficient performance in the\nsegmentation. To overcome the initialization issue of active contour models, we\ngeneralize the inhomogeneous graph Laplacian initialization method (IGLIM) to\nthe multi-phase case and then apply it to give the initial contour of the\nICTM-LVF solver. Numerical experiments are conducted on synthetic images and\nreal images to demonstrate the capability of our initialization method, and the\neffectiveness of the local variance force for noise robustness in the\nmulti-phase image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhonghua Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DATA: Domain-Aware and Task-Aware Pre-training. (arXiv:2203.09041v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09041","description":"<p>The paradigm of training models on massive data without label through\nself-supervised learning (SSL) and finetuning on many downstream tasks has\nbecome a trend recently. However, due to the high training costs and the\nunconsciousness of downstream usages, most self-supervised learning methods\nlack the capability to correspond to the diversities of downstream scenarios,\nas there are various data domains, different vision tasks and latency\nconstraints on models. Neural architecture search (NAS) is one universally\nacknowledged fashion to conquer the issues above, but applying NAS on SSL seems\nimpossible as there is no label or metric provided for judging model selection.\nIn this paper, we present DATA, a simple yet effective NAS approach specialized\nfor SSL that provides Domain-Aware and Task-Aware pre-training. Specifically,\nwe (i) train a supernet which could be deemed as a set of millions of networks\ncovering a wide range of model scales without any label, (ii) propose a\nflexible searching mechanism compatible with SSL that enables finding networks\nof different computation costs, for various downstream vision tasks and data\ndomains without explicit metric provided. Instantiated With MoCo v2, our method\nachieves promising results across a wide range of computation costs on\ndownstream tasks, including image classification, object detection and semantic\nsegmentation. DATA is orthogonal to most existing SSL methods and endows them\nthe ability of customization on downstream needs. Extensive experiments on\nother SSL methods demonstrate the generalizability of the proposed method. Code\nis released at https://github.com/GAIA-vision/GAIA-ssl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qing Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Junran Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxie Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiajun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Haoran Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Image Animator: Learning to Animate Images via Latent Space Navigation. (arXiv:2203.09043v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09043","description":"<p>Due to the remarkable progress of deep generative models, animating images\nhas become increasingly efficient, whereas associated results have become\nincreasingly realistic. Current animation-approaches commonly exploit structure\nrepresentation extracted from driving videos. Such structure representation is\ninstrumental in transferring motion from driving videos to still images.\nHowever, such approaches fail in case the source image and driving video\nencompass large appearance variation. Moreover, the extraction of structure\ninformation requires additional modules that endow the animation-model with\nincreased complexity. Deviating from such models, we here introduce the Latent\nImage Animator (LIA), a self-supervised autoencoder that evades need for\nstructure representation. LIA is streamlined to animate images by linear\nnavigation in the latent space. Specifically, motion in generated video is\nconstructed by linear displacement of codes in the latent space. Towards this,\nwe learn a set of orthogonal motion directions simultaneously, and use their\nlinear combination, in order to represent any displacement in the latent space.\nExtensive quantitative and qualitative analysis suggests that our model\nsystematically and significantly outperforms state-of-art methods on VoxCeleb,\nTaichi and TED-talk datasets w.r.t. generated quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Di Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training. (arXiv:2203.09052v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09052","description":"<p>Due to the limitations of the model structure and pre-training objectives,\nexisting vision-and-language generation models cannot utilize pair-wise images\nand text through bi-directional generation. In this paper, we propose DU-VLG, a\nframework which unifies vision-and-language generation as sequence generation\nproblems. DU-VLG is trained with novel dual pre-training tasks: multi-modal\ndenoising autoencoder tasks and modality translation tasks. To bridge the gap\nbetween image understanding and generation, we further design a novel\ncommitment loss. We compare pre-training objectives on image captioning and\ntext-to-image generation datasets. Results show that DU-VLG yields better\nperformance than variants trained with uni-directional generation objectives or\nthe variant without the commitment loss. We also obtain higher scores compared\nto previous state-of-the-art systems on three vision-and-language generation\ntasks. In addition, human judges further confirm that our model generates real\nand relevant images as well as faithful and informative captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Luyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guocheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Table Detection and Structure Recognition from Heterogeneous Document Images. (arXiv:2203.09056v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09056","description":"<p>We introduce a new table detection and structure recognition approach named\nRobusTabNet to detect the boundaries of tables and reconstruct the cellular\nstructure of the table from heterogeneous document images. For table detection,\nwe propose to use CornerNet as a new region proposal network to generate higher\nquality table proposals for Faster R-CNN, which has significantly improved the\nlocalization accuracy of Faster R-CNN for table detection. Consequently, our\ntable detection approach achieves state-of-the-art performance on three public\ntable detection benchmarks, namely cTDaR TrackA, PubLayNet and IIIT-AR-13K, by\nonly using a lightweight ResNet-18 backbone network. Furthermore, we propose a\nnew split-and-merge based table structure recognition approach, in which a\nnovel spatial CNN based separation line prediction module is proposed to split\neach detected table into a grid of cells, and a Grid CNN based cell merging\nmodule is applied to recover the spanning cells. As the spatial CNN module can\neffectively propagate contextual information across the whole table image, our\ntable structure recognizer can robustly recognize tables with large blank\nspaces and geometrically distorted (even curved) tables. Thanks to these two\ntechniques, our table structure recognition approach achieves state-of-the-art\nperformance on three public benchmarks, including SciTSR, PubTabNet and cTDaR\nTrackB. Moreover, we have further demonstrated the advantages of our approach\nin recognizing tables with complex structures, large blank spaces, empty or\nspanning cells as well as geometrically distorted or even curved tables on a\nmore challenging in-house dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chixiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weihong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1\">Qiang Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-shot Learning. (arXiv:2203.09064v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09064","description":"<p>This paper presents new hierarchically cascaded transformers that can improve\ndata efficiency through attribute surrogates learning and spectral tokens\npooling. Vision transformers have recently been thought of as a promising\nalternative to convolutional neural networks for visual recognition. But when\nthere is no sufficient data, it gets stuck in overfitting and shows inferior\nperformance. To improve data efficiency, we propose hierarchically cascaded\ntransformers that exploit intrinsic image structures through spectral tokens\npooling and optimize the learnable parameters through latent attribute\nsurrogates. The intrinsic image structure is utilized to reduce the ambiguity\nbetween foreground content and background noise by spectral tokens pooling. And\nthe attribute surrogate learning scheme is designed to benefit from the rich\nvisual information in image-label pairs instead of simple visual concepts\nassigned by their labels. Our Hierarchically Cascaded Transformers, called\nHCTransformers, is built upon a self-supervised learning framework DINO and is\ntested on several popular few-shot learning benchmarks.\n</p>\n<p>In the inductive setting, HCTransformers surpass the DINO baseline by a large\nmargin of 9.7% 5-way 1-shot accuracy and 9.17% 5-way 5-shot accuracy on\nminiImageNet, which demonstrates HCTransformers are efficient to extract\ndiscriminative features. Also, HCTransformers show clear advantages over SOTA\nfew-shot classification methods in both 5-way 1-shot and 5-way 5-shot settings\non four popular benchmark datasets, including miniImageNet, tieredImageNet,\nFC100, and CIFAR-FS. The trained weights and codes are available at\nhttps://github.com/StomachCold/HCTransformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yangji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weihan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STPLS3D: A Large-Scale Synthetic and Real Aerial Photogrammetry 3D Point Cloud Dataset. (arXiv:2203.09065v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09065","description":"<p>Although various 3D datasets with different functions and scales have been\nproposed recently, it remains challenging for individuals to complete the whole\npipeline of large-scale data collection, sanitization, and annotation.\nMoreover, the created datasets usually suffer from extremely imbalanced class\ndistribution or partial low-quality data samples. Motivated by this, we explore\nthe procedurally synthetic 3D data generation paradigm to equip individuals\nwith the full capability of creating large-scale annotated photogrammetry point\nclouds. Specifically, we introduce a synthetic aerial photogrammetry point\nclouds generation pipeline that takes full advantage of open geospatial data\nsources and off-the-shelf commercial packages. Unlike generating synthetic data\nin virtual games, where the simulated data usually have limited gaming\nenvironments created by artists, the proposed pipeline simulates the\nreconstruction process of the real environment by following the same UAV flight\npattern on different synthetic terrain shapes and building densities, which\nensure similar quality, noise pattern, and diversity with real data. In\naddition, the precise semantic and instance annotations can be generated fully\nautomatically, avoiding the expensive and time-consuming manual annotation.\nBased on the proposed pipeline, we present a richly-annotated synthetic 3D\naerial photogrammetry point cloud dataset, termed STPLS3D, with more than 16\n$km^2$ of landscapes and up to 18 fine-grained semantic categories. For\nverification purposes, we also provide a parallel dataset collected from four\nareas in the real environment. Extensive experiments conducted on our datasets\ndemonstrate the effectiveness and quality of the proposed synthetic dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meida Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hugues_T/0/1/0/all/0/1\">Thomas Hugues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Andrew Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCullough_K/0/1/0/all/0/1\">Kyle McCullough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soibelman_L/0/1/0/all/0/1\">Lucio Soibelman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIMO-2: End-to-End Unified Vision-Language Grounded Learning. (arXiv:2203.09067v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09067","description":"<p>Vision-Language Pre-training (VLP) has achieved impressive performance on\nvarious cross-modal downstream tasks. However, most existing methods can only\nlearn from aligned image-caption data and rely heavily on expensive regional\nfeatures, which greatly limits their scalability and performance. In this\npaper, we propose an end-to-end unified-modal pre-training framework, namely\nUNIMO-2, for joint learning on both aligned image-caption data and unaligned\nimage-only and text-only corpus. We build a unified Transformer model to\njointly learn visual representations, textual representations and semantic\nalignment between images and texts. In particular, we propose to conduct\ngrounded learning on both images and texts via a sharing grounded space, which\nhelps bridge unaligned images and texts, and align the visual and textual\nsemantic spaces on different types of corpora. The experiments show that our\ngrounded learning method can improve textual and visual semantic alignment for\nimproving performance on various cross-modal tasks. Moreover, benefiting from\neffective joint modeling of different types of corpora, our model also achieves\nimpressive performance on single-modal visual and textual tasks. Our code and\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Can Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guocheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do We Really Need a Learnable Classifier at the End of Deep Neural Network?. (arXiv:2203.09081v1 [cs.LG])","link":"http://arxiv.org/abs/2203.09081","description":"<p>Modern deep neural networks for classification usually jointly learn a\nbackbone for representation and a linear classifier to output the logit of each\nclass. A recent study has shown a phenomenon called neural collapse that the\nwithin-class means of features and the classifier vectors converge to the\nvertices of a simplex equiangular tight frame (ETF) at the terminal phase of\ntraining on a balanced dataset. Since the ETF geometric structure maximally\nseparates the pair-wise angles of all classes in the classifier, it is natural\nto raise the question, why do we spend an effort to learn a classifier when we\nknow its optimal geometric structure? In this paper, we study the potential of\nlearning a neural network for classification with the classifier randomly\ninitialized as an ETF and fixed during training. Our analytical work based on\nthe layer-peeled model indicates that the feature learning with a fixed ETF\nclassifier naturally leads to the neural collapse state even when the dataset\nis imbalanced among classes. We further show that in this case the cross\nentropy (CE) loss is not necessary and can be replaced by a simple squared loss\nthat shares the same global optimality but enjoys a more accurate gradient and\nbetter convergence property. Our experimental results show that our method is\nable to achieve similar performances on image classification for balanced\ndatasets, and bring significant improvements in the long-tailed and\nfine-grained classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Point Cloud Simplification for High-quality Surface Reconstruction. (arXiv:2203.09088v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09088","description":"<p>The growing size of point clouds enlarges consumptions of storage,\ntransmission, and computation of 3D scenes. Raw data is redundant, noisy, and\nnon-uniform. Therefore, simplifying point clouds for achieving compact, clean,\nand uniform points is becoming increasingly important for 3D vision and\ngraphics tasks. Previous learning based methods aim to generate fewer points\nfor scene understanding, regardless of the quality of surface reconstruction,\nleading to results with low reconstruction accuracy and bad point distribution.\nIn this paper, we propose a novel point cloud simplification network (PCS-Net)\ndedicated to high-quality surface mesh reconstruction while maintaining\ngeometric fidelity. We first learn a sampling matrix in a feature-aware\nsimplification module to reduce the number of points. Then we propose a novel\ndouble-scale resampling module to refine the positions of the sampled points,\nto achieve a uniform distribution. To further retain important shape features,\nan adaptive sampling strategy with a novel saliency loss is designed. With our\nPCS-Net, the input non-uniform and noisy point cloud can be simplified in a\nfeature-aware manner, i.e., points near salient features are consolidated but\nstill with uniform distribution locally. Experiments demonstrate the\neffectiveness of our method and show that we outperform previous simplification\nor reconstruction-oriented upsampling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"deepNIR: Datasets for generating synthetic NIR images and improved fruit detection system using deep learning techniques. (arXiv:2203.09091v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09091","description":"<p>This paper presents datasets utilised for synthetic near-infrared (NIR) image\ngeneration and bounding-box level fruit detection systems. It is undeniable\nthat high-calibre machine learning frameworks such as Tensorflow or Pytorch,\nand large-scale ImageNet or COCO datasets with the aid of accelerated GPU\nhardware have pushed the limit of machine learning techniques for more than\ndecades. Among these breakthroughs, a high-quality dataset is one of the\nessential building blocks that can lead to success in model generalisation and\nthe deployment of data-driven deep neural networks. In particular, synthetic\ndata generation tasks often require more training samples than other supervised\napproaches. Therefore, in this paper, we share the NIR+RGB datasets that are\nre-processed from two public datasets (i.e., nirscene and SEN12MS) and our\nnovel NIR+RGB sweet pepper(capsicum) dataset. We quantitatively and\nqualitatively demonstrate that these NIR+RGB datasets are sufficient to be used\nfor synthetic NIR image generation. We achieved Frechet Inception Distance\n(FID) of 11.36, 26.53, and 40.15 for nirscene1, SEN12MS, and sweet pepper\ndatasets respectively. In addition, we release manual annotations of 11 fruit\nbounding boxes that can be exported as various formats using cloud service.\nFour newly added fruits [blueberry, cherry, kiwi, and wheat] compound 11 novel\nbounding box datasets on top of our previous work presented in the deepFruits\nproject [apple, avocado, capsicum, mango, orange, rockmelon, strawberry]. The\ntotal number of bounding box instances of the dataset is 162k and it is ready\nto use from cloud service. For the evaluation of the dataset, Yolov5 single\nstage detector is exploited and reported impressive\nmean-average-precision,mAP[0.5:0.95] results of[min:0.49, max:0.812]. We hope\nthese datasets are useful and serve as a baseline for the future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sa_I/0/1/0/all/0/1\">Inkyu Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">JongYoon Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1\">Ho Seok Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonald_B/0/1/0/all/0/1\">Bruce MacDonald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-aligned Fusion Transformer for One-shot Object Detection. (arXiv:2203.09093v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09093","description":"<p>One-shot object detection aims at detecting novel objects according to merely\none given instance. With extreme data scarcity, current approaches explore\nvarious feature fusions to obtain directly transferable meta-knowledge. Yet,\ntheir performances are often unsatisfactory. In this paper, we attribute this\nto inappropriate correlation methods that misalign query-support semantics by\noverlooking spatial structures and scale variances. Upon analysis, we leverage\nthe attention mechanism and propose a simple but effective architecture named\nSemantic-aligned Fusion Transformer (SaFT) to resolve these issues.\nSpecifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale\nsemantic enhancement and a horizontal fusion module (HFM) for cross-sample\nfeature fusion. Together, they broaden the vision for each feature point from\nthe support to a whole augmented feature pyramid from the query, facilitating\nsemantic-aligned associations. Extensive experiments on multiple benchmarks\ndemonstrate the superiority of our framework. Without fine-tuning on novel\nclasses, it brings significant performance gains to one-stage baselines,\nlifting state-of-the-art results to a higher level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Community-Driven Comprehensive Scientific Paper Summarization: Insight from cvpaper.challenge. (arXiv:2203.09109v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09109","description":"<p>The present paper introduces a group activity involving writing summaries of\nconference proceedings by volunteer participants. The rapid increase in\nscientific papers is a heavy burden for researchers, especially non-native\nspeakers, who need to survey scientific literature. To alleviate this problem,\nwe organized a group of non-native English speakers to write summaries of\npapers presented at a computer vision conference to share the knowledge of the\npapers read by the group. We summarized a total of 2,000 papers presented at\nthe Conference on Computer Vision and Pattern Recognition, a top-tier\nconference on computer vision, in 2019 and 2020. We quantitatively analyzed\nparticipants' selection regarding which papers they read among the many\navailable papers. The experimental results suggest that we can summarize a wide\nrange of papers without asking participants to read papers unrelated to their\ninterests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_S/0/1/0/all/0/1\">Shintaro Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kataoka_H/0/1/0/all/0/1\">Hirokatsu Kataoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_R/0/1/0/all/0/1\">Ryota Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinagawa_S/0/1/0/all/0/1\">Seitaro Shinagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishima_S/0/1/0/all/0/1\">Shigeo Morishima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionAug: Augmentation with Physical Correction for Human Motion Prediction. (arXiv:2203.09116v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09116","description":"<p>This paper presents a motion data augmentation scheme incorporating motion\nsynthesis encouraging diversity and motion correction imposing physical\nplausibility. This motion synthesis consists of our modified Variational\nAutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed\nsampling-near-samples method generates various valid motions even with\ninsufficient training motion data. Our IK-based motion synthesis method allows\nus to generate a variety of motions semi-automatically. Since these two schemes\ngenerate unrealistic artifacts in the synthesized motions, our motion\ncorrection rectifies them. This motion correction scheme consists of imitation\nlearning with physics simulation and subsequent motion debiasing. For this\nimitation learning, we propose the PD-residual force that significantly\naccelerates the training process. Furthermore, our motion debiasing\nsuccessfully offsets the motion bias induced by imitation learning to maximize\nthe effect of augmentation. As a result, our method outperforms previous\nnoise-based motion augmentation methods by a large margin on both Recurrent\nNeural Network-based and Graph Convolutional Network-based human motion\nprediction models. The code is available at {\\rm\n\\url{https://github.com/meaten/MotionAug}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maeda_T/0/1/0/all/0/1\">Takahiro Maeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ukita_N/0/1/0/all/0/1\">Norimichi Ukita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRAG: Dynamic Region-Aware GCN for Privacy-Leaking Image Detection. (arXiv:2203.09121v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09121","description":"<p>The daily practice of sharing images on social media raises a severe issue\nabout privacy leakage. To address the issue, privacy-leaking image detection is\nstudied recently, with the goal to automatically identify images that may leak\nprivacy. Recent advance on this task benefits from focusing on crucial objects\nvia pretrained object detectors and modeling their correlation. However, these\nmethods have two limitations: 1) they neglect other important elements like\nscenes, textures, and objects beyond the capacity of pretrained object\ndetectors; 2) the correlation among objects is fixed, but a fixed correlation\nis not appropriate for all the images. To overcome the limitations, we propose\nthe Dynamic Region-Aware Graph Convolutional Network (DRAG) that dynamically\nfinds out crucial regions including objects and other important elements, and\nmodels their correlation adaptively for each input image. To find out crucial\nregions, we cluster spatially-correlated feature channels into several\nregion-aware feature maps. Further, we dynamically model the correlation with\nthe self-attention mechanism and explore the interaction among the regions with\na graph convolutional network. The DRAG achieved an accuracy of 87% on the\nlargest dataset for privacy-leaking image detection, which is 10 percentage\npoints higher than the state of the art. The further case study demonstrates\nthat it found out crucial regions containing not only objects but other\nimportant elements like textures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input. (arXiv:2203.09123v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09123","description":"<p>The transferability of adversarial examples allows the deception on black-box\nmodels, and transfer-based targeted attacks have attracted a lot of interest\ndue to their practical applicability. To maximize the transfer success rate,\nadversarial examples should avoid overfitting to the source model, and image\naugmentation is one of the primary approaches for this. However, prior works\nutilize simple image transformations such as resizing, which limits input\ndiversity. To tackle this limitation, we propose the object-based diverse input\n(ODI) method that draws an adversarial image on a 3D object and induces the\nrendered image to be classified as the target class. Our motivation comes from\nthe humans' superior perception of an image printed on a 3D object. If the\nimage is clear enough, humans can recognize the image content in a variety of\nviewing conditions. Likewise, if an adversarial example looks like the target\nclass to the model, the model should also classify the rendered image of the 3D\nobject as the target class. The ODI method effectively diversifies the input by\nleveraging an ensemble of multiple source objects and randomizing viewing\nconditions. In our experimental results on the ImageNet-Compatible dataset,\nthis method boosts the average targeted attack success rate from 28.3% to 47.0%\ncompared to the state-of-the-art methods. We also demonstrate the applicability\nof the ODI method to adversarial examples on the face verification task and its\nsuperior performance improvement. Our code is available at\nhttps://github.com/dreamflake/ODI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Byun_J/0/1/0/all/0/1\">Junyoung Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seungju Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1\">Myung-Joon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hee-Seon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Vision Transformers Robust to Spurious Correlations?. (arXiv:2203.09125v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09125","description":"<p>Deep neural networks may be susceptible to learning spurious correlations\nthat hold on average but not in atypical test samples. As with the recent\nemergence of vision transformer (ViT) models, it remains underexplored how\nspurious correlations are manifested in such architectures. In this paper, we\nsystematically investigate the robustness of vision transformers to spurious\ncorrelations on three challenging benchmark datasets and compare their\nperformance with popular CNNs. Our study reveals that when pre-trained on a\nsufficiently large dataset, ViT models are more robust to spurious correlations\nthan CNNs. Key to their success is the ability to generalize better from the\nexamples where spurious correlations do not hold. Further, we perform extensive\nablations and experiments to understand the role of the self-attention\nmechanism in providing robustness under spuriously correlated environments. We\nhope that our work will inspire future research on further understanding the\nrobustness of ViT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_S/0/1/0/all/0/1\">Soumya Suvra Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yifei Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Generative Transformer Learning for Cross-view Geo-localization. (arXiv:2203.09135v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09135","description":"<p>Cross-view geo-localization (CVGL), which aims to estimate the geographical\nlocation of the ground-level camera by matching against enormous geo-tagged\naerial (e.g., satellite) images, remains extremely challenging due to the\ndrastic appearance differences across views. Existing methods mainly employ\nSiamese-like CNNs to extract global descriptors without examining the mutual\nbenefits between the two modes. In this paper, we present a novel approach\nusing cross-modal knowledge generative tactics in combination with transformer,\nnamely mutual generative transformer learning (MGTL), for CVGL. Specifically,\nMGTL develops two separate generative modules--one for aerial-like knowledge\ngeneration from ground-level semantic information and vice versa--and fully\nexploits their mutual benefits through the attention mechanism. Experiments on\nchallenging public benchmarks, CVACT and CVUSA, demonstrate the effectiveness\nof the proposed method compared to the existing state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Q/0/1/0/all/0/1\">Qiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hong Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning. (arXiv:2203.09137v1 [cs.LG])","link":"http://arxiv.org/abs/2203.09137","description":"<p>Model-agnostic meta-learning (MAML) and its variants have become popular\napproaches for few-shot learning. However, due to the non-convexity of deep\nneural nets (DNNs) and the bi-level formulation of MAML, the theoretical\nproperties of MAML with DNNs remain largely unknown. In this paper, we first\nprove that MAML with over-parameterized DNNs is guaranteed to converge to\nglobal optima at a linear rate. Our convergence analysis indicates that MAML\nwith over-parameterized DNNs is equivalent to kernel regression with a novel\nclass of kernels, which we name as Meta Neural Tangent Kernels (MetaNTK). Then,\nwe propose MetaNTK-NAS, a new training-free neural architecture search (NAS)\nmethod for few-shot learning that uses MetaNTK to rank and select\narchitectures. Empirically, we compare our MetaNTK-NAS with previous NAS\nmethods on two popular few-shot learning benchmarks, miniImageNet, and\ntieredImageNet. We show that the performance of MetaNTK-NAS is comparable or\nbetter than the state-of-the-art NAS method designed for few-shot learning\nwhile enjoying more than 100x speedup. We believe the efficiency of MetaNTK-NAS\nmakes itself more practical for many real-world tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yite Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering. (arXiv:2203.09138v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09138","description":"<p>Knowledge-based visual question answering requires the ability of associating\nexternal knowledge for open-ended cross-modal scene understanding. One\nlimitation of existing solutions is that they capture relevant knowledge from\ntext-only knowledge bases, which merely contain facts expressed by first-order\npredicates or language descriptions while lacking complex but indispensable\nmultimodal knowledge for visual understanding. How to construct vision-relevant\nand explainable multimodal knowledge for the VQA scenario has been less\nstudied. In this paper, we propose MuKEA to represent multimodal knowledge by\nan explicit triplet to correlate visual objects and fact answers with implicit\nrelations. To bridge the heterogeneous gap, we propose three objective losses\nto learn the triplet representations from complementary views: embedding\nstructure, topological relation and semantic space. By adopting a pre-training\nand fine-tuning learning strategy, both basic and domain-specific multimodal\nknowledge are progressively accumulated for answer prediction. We outperform\nthe state-of-the-art by 3.35% and 6.08% respectively on two challenging\nknowledge-required datasets: OK-VQA and KRVQA. Experimental results prove the\ncomplementary benefits of the multimodal knowledge with existing knowledge\nbases and the advantages of our end-to-end framework over the existing pipeline\nmethods. The code is available at https://github.com/AndersonStra/MuKEA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Mingxin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Visuo-Haptic Object Shape Completion. (arXiv:2203.09149v1 [cs.RO])","link":"http://arxiv.org/abs/2203.09149","description":"<p>Recent advancements in object shape completion have enabled impressive object\nreconstructions using only visual input. However, due to self-occlusion, the\nreconstructions have high uncertainty in the occluded object parts, which\nnegatively impacts the performance of downstream robotic tasks such as\ngrasping. In this work, we propose an active visuo-haptic shape completion\nmethod called Act-VH that actively computes where to touch the objects based on\nthe reconstruction uncertainty. Act-VH reconstructs objects from point clouds\nand calculates the reconstruction uncertainty using IGR, a recent\nstate-of-the-art implicit surface deep neural network. We experimentally\nevaluate the reconstruction accuracy of Act-VH against five baselines in\nsimulation and in the real world. We also propose a new simulation environment\nfor this purpose. The results show that Act-VH outperforms all baselines and\nthat an uncertainty-driven haptic exploration policy leads to higher\nreconstruction accuracy than a random policy and a policy driven by Gaussian\nProcess Implicit Surfaces. As a final experiment, we evaluate Act-VH and the\nbest reconstruction baseline on grasping 10 novel objects. The results show\nthat Act-VH reaches a significantly higher grasp success rate than the baseline\non all objects. Together, this work opens up the door for using active\nvisuo-haptic shape completion in more complex cluttered scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rustler_L/0/1/0/all/0/1\">Lukas Rustler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundell_J/0/1/0/all/0/1\">Jens Lundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrens_J/0/1/0/all/0/1\">Jan Kristof Behrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1\">Matej Hoffmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Rejection Function Meets Character Recognition Tasks. (arXiv:2203.09151v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09151","description":"<p>In this paper, we propose an optimal rejection method for rejecting ambiguous\nsamples by a rejection function. This rejection function is trained together\nwith a classification function under the framework of Learning-with-Rejection\n(LwR). The highlights of LwR are: (1) the rejection strategy is not heuristic\nbut has a strong background from a machine learning theory, and (2) the\nrejection function can be trained on an arbitrary feature space which is\ndifferent from the feature space for classification. The latter suggests we can\nchoose a feature space that is more suitable for rejection. Although the past\nresearch on LwR focused only on its theoretical aspect, we propose to utilize\nLwR for practical pattern classification tasks. Moreover, we propose to use\nfeatures from different CNN layers for classification and rejection. Our\nextensive experiments of notMNIST classification and character/non-character\nclassification demonstrate that the proposed method achieves better performance\nthan traditional rejection strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiaotong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuchen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suehiro_D/0/1/0/all/0/1\">Daiki Suehiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biasing Like Human: A Cognitive Bias Framework for Scene Graph Generation. (arXiv:2203.09160v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09160","description":"<p>Scene graph generation is a sophisticated task because there is no specific\nrecognition pattern (e.g., \"looking at\" and \"near\" have no conspicuous\ndifference concerning vision, whereas \"near\" could occur between entities with\ndifferent morphology). Thus some scene graph generation methods are trapped\ninto most frequent relation predictions caused by capricious visual features\nand trivial dataset annotations. Therefore, recent works emphasized the\n\"unbiased\" approaches to balance predictions for a more informative scene\ngraph. However, human's quick and accurate judgments over relations between\nnumerous objects should be attributed to \"bias\" (i.e., experience and\nlinguistic knowledge) rather than pure vision. To enhance the model capability,\ninspired by the \"cognitive bias\" mechanism, we propose a novel 3-paradigms\nframework that simulates how humans incorporate the label linguistic features\nas guidance of vision-based representations to better mine hidden relation\npatterns and alleviate noisy visual propagation. Our framework is\nmodel-agnostic to any scene graph model. Comprehensive experiments prove our\nframework outperforms baseline modules in several metrics with minimum\nparameters increment and achieves new SOTA performance on Visual Genome\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaoguang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changyin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Wenzhe Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09161","description":"<p>Recently introduced instruction-paradigm empowers non-expert users to\nleverage NLP resources by defining a new task in natural language.\nInstruction-tuned models have significantly outperformed multitask learning\nmodels (without instruction); however they are far from state of the art task\nspecific models. Conventional approaches to improve model performance via\ncreating large datasets with lots of task instances or architectural/training\nchanges in model may not be feasible for non-expert users. However, they can\nwrite alternate instructions to represent an instruction task. Is\nInstruction-augumentation helpful? We augment a subset of tasks in NATURAL\nINSTRUCTIONS with additional instructions and find that these significantly\nimprove model performance (upto 35%) specially in low-data regime. Our results\nindicate that an additional instruction can be equivalent to ~40 instances on\naverage across our evaluation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ravsehaj Singh Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UWED: Unsigned Distance Field for Accurate 3D Scene Representation and Completion. (arXiv:2203.09167v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09167","description":"<p>Scene Completion is the task of completing missing geometry from a partial\nscan of a scene. The majority of previous methods compute an implicit\nrepresentation from range data using a Truncated Signed Distance Function\n(TSDF) on a 3D grid as input to neural networks. The truncation limits but does\nnot remove the ambiguous cases introduced by the sign for non-closed surfaces.\nAs an alternative, we present an Unsigned Distance Function (UDF) called\nUnsigned Weighted Euclidean Distance (UWED) as input to the scene completion\nneural networks. UWED is simple and efficient as a surface representation, and\ncan be computed on any noisy point cloud without normals. To obtain the\nexplicit geometry, we present a method for extracting a point cloud from\ndiscretized UDF values on a regular grid. We compare different SDFs and UDFs\nfor the scene completion task on indoor and outdoor point clouds collected from\nRGB-D and LiDAR sensors and show improved completion using the proposed UWED\nfunction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richa_J/0/1/0/all/0/1\">Jean Pierre Richa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1\">Jean-Emmanuel Deschaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1\">Fran&#xe7;ois Goulette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmasso_N/0/1/0/all/0/1\">Nicolas Dalmasso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Classification of Satellite Image Time Series with Thermal Positional Encoding. (arXiv:2203.09175v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09175","description":"<p>Large-scale crop type classification is a task at the core of remote sensing\nefforts with applications of both economic and ecological importance. Current\nstate-of-the-art deep learning methods are based on self-attention and use\nsatellite image time series (SITS) to discriminate crop types based on their\nunique growth patterns. However, existing methods generalize poorly to regions\nnot seen during training mainly due to not being robust to temporal shifts of\nthe growing season caused by variations in climate. To this end, we propose\nThermal Positional Encoding (TPE) for attention-based crop classifiers. Unlike\nprevious positional encoding based on calendar time (e.g. day-of-year), TPE is\nbased on thermal time, which is obtained by accumulating daily average\ntemperatures over the growing season. Since crop growth is directly related to\nthermal time, but not calendar time, TPE addresses the temporal shifts between\ndifferent regions to improve generalization. We propose multiple TPE\nstrategies, including learnable methods, to further improve results compared to\nthe common fixed positional encodings. We demonstrate our approach on a crop\nclassification task across four different European regions, where we obtain\nstate-of-the-art generalization results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nyborg_J/0/1/0/all/0/1\">Joachim Nyborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelletier_C/0/1/0/all/0/1\">Charlotte Pelletier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assent_I/0/1/0/all/0/1\">Ira Assent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel End-To-End Network for Reconstruction of Non-Regularly Sampled Image Data Using Locally Fully Connected Layers. (arXiv:2203.09180v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09180","description":"<p>Quarter sampling and three-quarter sampling are novel sensor concepts that\nenable the acquisition of higher resolution images without increasing the\nnumber of pixels. This is achieved by non-regularly covering parts of each\npixel of a low-resolution sensor such that only one quadrant or three quadrants\nof the sensor area of each pixel is sensitive to light. Combining a properly\ndesigned mask and a high-quality reconstruction algorithm, a higher image\nquality can be achieved than using a low-resolution sensor and subsequent\nupsampling. For the latter case, the image quality can be further enhanced\nusing super resolution algorithms such as the very deep super resolution\nnetwork (VDSR). In this paper, we propose a novel end-to-end neural network to\nreconstruct high resolution images from non-regularly sampled sensor data. The\nnetwork is a concatenation of a locally fully connected reconstruction network\n(LFCR) and a standard VDSR network. Altogether, using a three-quarter sampling\nsensor with our novel neural network layout, the image quality in terms of PSNR\nfor the Urban100 dataset can be increased by 2.96 dB compared to the\nstate-of-the-art approach. Compared to a low-resolution sensor with VDSR, a\ngain of 1.11 dB is achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Grosche_S/0/1/0/all/0/1\">Simon Grosche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brand_F/0/1/0/all/0/1\">Fabian Brand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Interactive Explanatory AI System for Industrial Quality Control. (arXiv:2203.09181v1 [cs.LG])","link":"http://arxiv.org/abs/2203.09181","description":"<p>Machine learning based image classification algorithms, such as deep neural\nnetwork approaches, will be increasingly employed in critical settings such as\nquality control in industry, where transparency and comprehensibility of\ndecisions are crucial. Therefore, we aim to extend the defect detection task\ntowards an interactive human-in-the-loop approach that allows us to integrate\nrich background knowledge and the inference of complex relationships going\nbeyond traditional purely data-driven approaches. We propose an approach for an\ninteractive support system for classifications in an industrial quality control\nsetting that combines the advantages of both (explainable) knowledge-driven and\ndata-driven machine learning methods, in particular inductive logic programming\nand convolutional neural networks, with human expertise and control. The\nresulting system can assist domain experts with decisions, provide transparent\nexplanations for results, and integrate feedback from users; thus reducing\nworkload for humans while both respecting their expertise and without removing\ntheir agency or accountability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_D/0/1/0/all/0/1\">Dennis M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marz_M/0/1/0/all/0/1\">Michael M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheele_S/0/1/0/all/0/1\">Stephan Scheele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Details or Artifacts: A Locally Discriminative Learning Approach to Realistic Image Super-Resolution. (arXiv:2203.09195v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09195","description":"<p>Single image super-resolution (SISR) with generative adversarial networks\n(GAN) has recently attracted increasing attention due to its potentials to\ngenerate rich details. However, the training of GAN is unstable, and it often\nintroduces many perceptually unpleasant artifacts along with the generated\ndetails. In this paper, we demonstrate that it is possible to train a GAN-based\nSISR model which can stably generate perceptually realistic details while\ninhibiting visual artifacts. Based on the observation that the local statistics\n(e.g., residual variance) of artifact areas are often different from the areas\nof perceptually friendly details, we develop a framework to discriminate\nbetween GAN-generated artifacts and realistic details, and consequently\ngenerate an artifact map to regularize and stabilize the model training\nprocess. Our proposed locally discriminative learning (LDL) method is simple\nyet effective, which can be easily plugged in off-the-shelf SISR methods and\nboost their performance. Experiments demonstrate that LDL outperforms the\nstate-of-the-art GAN based SISR methods, achieving not only higher\nreconstruction accuracy but also superior perceptual quality on both synthetic\nand real-world datasets. Codes and models are available at\nhttps://github.com/csjliang/LDL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel Consistency Check For Fast Recursive Reconstruction Of Non-Regularly Sampled Video Data. (arXiv:2203.09200v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09200","description":"<p>Quarter sampling is a novel sensor design that allows for an acquisition of\nhigher resolution images without increasing the number of pixels. When being\nused for video data, one out of four pixels is measured in each frame.\nEffectively, this leads to a non-regular spatio-temporal sub-sampling. Compared\nto purely spatial or temporal sub-sampling, this allows for an increased\nreconstruction quality, as aliasing artifacts can be reduced. For the fast\nreconstruction of such sensor data with a fixed mask, recursive variant of\nfrequency selective reconstruction (FSR) was proposed. Here, pixels measured in\nprevious frames are projected into the current frame to support its\nreconstruction. In doing so, the motion between the frames is computed using\ntemplate matching. Since some of the motion vectors may be erroneous, it is\nimportant to perform a proper consistency checking. In this paper, we propose\nfaster consistency checking methods as well as a novel recursive FSR that uses\nthe projected pixels different than in literature and can handle dynamic masks.\nAltogether, we are able to significantly increase the reconstruction quality by\n+ 1.01 dB compared to the state-of-the-art recursive reconstruction method\nusing a fixed mask. Compared to a single frame reconstruction, an average gain\nof about + 1.52 dB is achieved for dynamic masks. At the same time, the\ncomputational complexity of the consistency checks is reduced by a factor of 13\ncompared to the literature algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Grosche_S/0/1/0/all/0/1\">Simon Grosche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seiler_J/0/1/0/all/0/1\">J&#xfc;rgen Seiler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulation-Driven Training of Vision Transformers Enabling Metal Segmentation in X-Ray Images. (arXiv:2203.09207v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09207","description":"<p>In several image acquisition and processing steps of X-ray radiography,\nknowledge of the existence of metal implants and their exact position is highly\nbeneficial (e.g. dose regulation, image contrast adjustment). Another\napplication which would benefit from an accurate metal segmentation is cone\nbeam computed tomography (CBCT) which is based on 2D X-ray projections. Due to\nthe high attenuation of metals, severe artifacts occur in the 3D X-ray\nacquisitions. The metal segmentation in CBCT projections usually serves as a\nprerequisite for metal artifact avoidance and reduction algorithms. Since the\ngeneration of high quality clinical training is a constant challenge, this\nstudy proposes to generate simulated X-ray images based on CT data sets\ncombined with self-designed computer aided design (CAD) implants and make use\nof convolutional neural network (CNN) and vision transformer (ViT) for metal\nsegmentation. Model test is performed on accurately labeled X-ray test datasets\nobtained from specimen scans. The CNN encoder-based network like U-Net has\nlimited performance on cadaver test data with an average dice score below 0.30,\nwhile the metal segmentation transformer with dual decoder (MST-DD) shows high\nrobustness and generalization on the segmentation task, with an average dice\nscore of 0.90. Our study indicates that the CAD model-based data generation has\nhigh flexibility and could be a way to overcome the problem of shortage in\nclinical data sampling and labelling. Furthermore, the MST-DD approach\ngenerates a more reliable neural network in case of training on simulated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_F/0/1/0/all/0/1\">Fuxin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ritschl_L/0/1/0/all/0/1\">Ludwig Ritschl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beister_M/0/1/0/all/0/1\">Marcel Beister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biniazan_R/0/1/0/all/0/1\">Ramyar Biniazan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreher_B/0/1/0/all/0/1\">Bj&#xf6;rn Kreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gottschalk_T/0/1/0/all/0/1\">Tristan M. Gottschalk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kappler_S/0/1/0/all/0/1\">Steffen Kappler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Compression-Based Feature Learning for Video Restoration. (arXiv:2203.09208v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09208","description":"<p>How to efficiently utilize the temporal features is crucial, yet challenging,\nfor video restoration. The temporal features usually contain various noisy and\nuncorrelated information, and they may interfere with the restoration of the\ncurrent frame. This paper proposes learning noise-robust feature\nrepresentations to help video restoration. We are inspired by that the neural\ncodec is a natural denoiser. In neural codec, the noisy and uncorrelated\ncontents which are hard to predict but cost lots of bits are more inclined to\nbe discarded for bitrate saving. Therefore, we design a neural compression\nmodule to filter the noise and keep the most useful information in features for\nvideo restoration. To achieve robustness to noise, our compression module\nadopts a spatial-channel-wise quantization mechanism to adaptively determine\nthe quantization step size for each position in the latent. Experiments show\nthat our method can significantly boost the performance on video denoising,\nwhere we obtain 0.13 dB improvement over BasicVSR++ with only 0.23x FLOPs.\nMeanwhile, our method also obtains SOTA results on video deraining and\ndehazing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09215","description":"<p>We propose Human-centered 4D Scene Capture (HSC4D) to accurately and\nefficiently create a dynamic digital world, containing large-scale\nindoor-outdoor scenes, diverse human motions, and rich interactions between\nhumans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is\nspace-free without any external devices' constraints and map-free without\npre-built maps. Considering that IMUs can capture human poses but always drift\nfor long-period use, while LiDAR is stable for global localization but rough\nfor local positions and orientations, HSC4D makes both sensors complement each\nother by a joint optimization and achieves promising results for long-term\ncapture. Relationships between humans and environments are also explored to\nmake their interaction more realistic. To facilitate many down-stream tasks,\nlike AR, VR, robots, autonomous driving, etc., we propose a dataset containing\nthree large scenes (1k-5k $m^2$) with accurate dynamic human motions and\nlocations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)\nand challenging human activities (exercising, walking up/down stairs, climbing,\netc.) demonstrate the effectiveness and the generalization ability of HSC4D.\nThe dataset and code is available at https://github.com/climbingdaily/HSC4D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yudi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yitai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chenglu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surgical Workflow Recognition: from Analysis of Challenges to Architectural Study. (arXiv:2203.09230v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09230","description":"<p>Algorithmic surgical workflow recognition is an ongoing research field and\ncan be divided into laparoscopic (Internal) and operating room (External)\nanalysis. So far many different works for the internal analysis have been\nproposed with the combination of a frame-level and an additional temporal model\nto address the temporal ambiguities between different workflow phases. For the\nExternal recognition task, Clip-level methods are in the focus of researchers\ntargeting the local ambiguities present in the OR scene. In this work we\nevaluate combinations of different model architectures for the task of surgical\nworkflow recognition to provide a fair comparison of the methods for both\nInternal and External analysis. We show that methods designed for the Internal\nanalysis can be transferred to the external task with comparable performance\ngains for different architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Czempiel_T/0/1/0/all/0/1\">Tobias Czempiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharghi_A/0/1/0/all/0/1\">Aidean Sharghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paschali_M/0/1/0/all/0/1\">Magdalini Paschali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohareri_O/0/1/0/all/0/1\">Omid Mohareri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-aware Neural Style Transfer using Instance Normalization. (arXiv:2203.09242v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09242","description":"<p>Neural Style Transfer (NST) is concerned with the artistic stylization of\nvisual media. It can be described as the process of transferring the style of\nan artistic image onto an ordinary photograph. Recently, a number of studies\nhave considered the enhancement of the depth-preserving capabilities of the NST\nalgorithms to address the undesired effects that occur when the input content\nimages include numerous objects at various depths. Our approach uses a deep\nresidual convolutional network with instance normalization layers that utilizes\nan advanced depth prediction network to integrate depth preservation as an\nadditional loss function to content and style. We demonstrate results that are\neffective in retaining the depth and global structure of content images. Three\ndifferent evaluation processes show that our system is capable of preserving\nthe structure of the stylized results while exhibiting style-capture\ncapabilities and aesthetic qualities comparable or superior to state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ioannou_E/0/1/0/all/0/1\">Eleftherios Ioannou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddock_S/0/1/0/all/0/1\">Steve Maddock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Properties of Adversarially-Trained CNNs. (arXiv:2203.09243v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09243","description":"<p>Adversarial Training has proved to be an effective training paradigm to\nenforce robustness against adversarial examples in modern neural network\narchitectures. Despite many efforts, explanations of the foundational\nprinciples underpinning the effectiveness of Adversarial Training are limited\nand far from being widely accepted by the Deep Learning community. In this\npaper, we describe surprising properties of adversarially-trained models,\nshedding light on mechanisms through which robustness against adversarial\nattacks is implemented. Moreover, we highlight limitations and failure modes\naffecting these models that were not discussed by prior works. We conduct\nextensive analyses on a wide range of architectures and datasets, performing a\ndeep comparison between robust and natural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carletti_M/0/1/0/all/0/1\">Mattia Carletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terzi_M/0/1/0/all/0/1\">Matteo Terzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susto_G/0/1/0/all/0/1\">Gian Antonio Susto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. (arXiv:2203.09249v1 [cs.LG])","link":"http://arxiv.org/abs/2203.09249","description":"<p>Federated Learning (FL) is an emerging distributed learning paradigm under\nprivacy constraint. Data heterogeneity is one of the main challenges in FL,\nwhich results in slow convergence and degraded performance. Most existing\napproaches only tackle the heterogeneity challenge by restricting the local\nmodel update in client, ignoring the performance drop caused by direct global\nmodel aggregation. Instead, we propose a data-free knowledge distillation\nmethod to fine-tune the global model in the server (FedFTG), which relieves the\nissue of direct model aggregation. Concretely, FedFTG explores the input space\nof local models through a generator, and uses it to transfer the knowledge from\nlocal models to the global model. Besides, we propose a hard sample mining\nscheme to achieve effective knowledge distillation throughout the training. In\naddition, we develop customized label sampling and class-level ensemble to\nderive maximum utilization of knowledge, which implicitly mitigates the\ndistribution discrepancy across clients. Extensive experiments show that our\nFedFTG significantly outperforms the state-of-the-art (SOTA) FL algorithms and\ncan serve as a strong plugin for enhancing FedAvg, FedProx, FedDyn, and\nSCAFFOLD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Cross-Domain Open World Recognition. (arXiv:2203.09257v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09257","description":"<p>The ability to evolve is fundamental for any valuable autonomous agent whose\nknowledge cannot remain limited to that injected by the manufacturer. Consider\nfor example a home assistant robot: it should be able to incrementally learn\nnew object categories when requested, but also to recognize the same objects in\ndifferent environments (rooms) and poses (hand-held/on the floor/above\nfurniture), while rejecting unknown ones. Despite its importance, this scenario\nhas started to raise interest in the robotic community only recently and the\nrelated research is still in its infancy, with existing experimental testbeds\nbut no tailored methods. With this work, we propose the first learning approach\nthat deals with all the previously mentioned challenges at once by exploiting a\nsingle contrastive objective. We show how it learns a feature space perfectly\nsuitable to incrementally include new classes and is able to capture knowledge\nwhich generalizes across a variety of visual domains. Our method is endowed\nwith a tailored effective stopping criterion for each learning episode and\nexploits a novel self-paced thresholding strategy that provides the classifier\nwith a reliable rejection option. Both these contributions are based on the\nobservation of the data statistics and do not need manual tuning. An extensive\nexperimental analysis confirms the effectiveness of the proposed approach\nestablishing the new state-of-the-art. The code is available at\nhttps://github.com/FrancescoCappio/Contrastive_Open_World.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1\">Francesco Cappio Borlino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1\">Silvia Bucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1\">Tatiana Tommasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09268","description":"<p>We present PROSUB: PROgressive SUBsampling, a deep learning based, automated\nmethodology that subsamples an oversampled data set (e.g. multi-channeled 3D\nimages) with minimal loss of information. We build upon a recent dual-network\napproach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI\nmeasurement sampling-reconstruction challenge, but suffers from deep learning\ntraining instability, by subsampling with a hard decision boundary. PROSUB uses\nthe paradigm of recursive feature elimination (RFE) and progressively\nsubsamples measurements during deep learning training, improving optimization\nstability. PROSUB also integrates a neural architecture search (NAS) paradigm,\nallowing the network architecture hyperparameters to respond to the subsampling\nprocess. We show PROSUB outperforms the winner of the MUDI MICCAI challenge,\nproducing large improvements &gt;18% MSE on the MUDI challenge sub-tasks and\nqualitative improvements on downstream processes useful for clinical\napplications. We also show the benefits of incorporating NAS and analyze the\neffect of PROSUB's components. As our method generalizes to other problems\nbeyond MRI measurement selection-reconstruction, our code is\nhttps://github.com/sbb-gh/PROSUB\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Blumberg_S/0/1/0/all/0/1\">Stefano B. Blumberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Hongxiang Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grussu_F/0/1/0/all/0/1\">Francesco Grussu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yukun Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Figini_M/0/1/0/all/0/1\">Matteo Figini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ART-SS: An Adaptive Rejection Technique for Semi-Supervised restoration for adverse weather-affected images. (arXiv:2203.09275v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09275","description":"<p>In recent years, convolutional neural network-based single image adverse\nweather removal methods have achieved significant performance improvements on\nmany benchmark datasets. However, these methods require large amounts of\nclean-weather degraded image pairs for training, which is often difficult to\nobtain in practice. Although various weather degradation synthesis methods\nexist in the literature, the use of synthetically generated weather degraded\nimages often results in sub-optimal performance on the real weather degraded\nimages due to the domain gap between synthetic and real-world images. To deal\nwith this problem, various semi-supervised restoration (SSR) methods have been\nproposed for deraining or dehazing which learn to restore the clean image using\nsynthetically generated datasets while generalizing better using unlabeled\nreal-world images. The performance of a semi-supervised method is essentially\nbased on the quality of the unlabeled data. In particular, if the unlabeled\ndata characteristics are very different from that of the labeled data, then the\nperformance of a semi-supervised method degrades significantly. We\ntheoretically study the effect of unlabeled data on the performance of an SSR\nmethod and develop a technique that rejects the unlabeled images that degrade\nthe performance. Extensive experiments and ablation study show that the\nproposed sample rejection method increases the performance of existing SSR\nderaining and dehazing methods significantly. Code is available at\n:https://github.com/rajeevyasarla/ART-SS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasarla_R/0/1/0/all/0/1\">Rajeev Yasarla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanoFormer: Panorama Transformer for Indoor 360{\\deg} Depth Estimation. (arXiv:2203.09283v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09283","description":"<p>Existing panoramic depth estimation methods based on convolutional neural\nnetworks (CNNs) focus on removing panoramic distortions, failing to perceive\npanoramic structures efficiently due to the fixed receptive field in CNNs. This\npaper proposes the panorama transformer (named PanoFormer) to estimate the\ndepth in panorama images, with tangent patches from spherical domain, learnable\ntoken flows, and panorama specific metrics. In particular, we divide patches on\nthe spherical tangent domain into tokens to reduce the negative effect of\npanoramic distortions. Since the geometric structures are essential for depth\nestimation, a self-attention module is redesigned with an additional learnable\ntoken flow. In addition, considering the characteristic of the spherical\ndomain, we present two panorama-specific metrics to comprehensively evaluate\nthe panoramic depth estimation models' performance. Extensive experiments\ndemonstrate that our approach significantly outperforms the state-of-the-art\n(SOTA) methods. Furthermore, the proposed method can be effectively extended to\nsolve semantic panorama segmentation, a similar pixel2pixel task. Code will be\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhijie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zishuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HybridCap: Inertia-aid Monocular Capture of Challenging Human Motions. (arXiv:2203.09287v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09287","description":"<p>Monocular 3D motion capture (mocap) is beneficial to many applications. The\nuse of a single camera, however, often fails to handle occlusions of different\nbody parts and hence it is limited to capture relatively simple movements. We\npresent a light-weight, hybrid mocap technique called HybridCap that augments\nthe camera with only 4 Inertial Measurement Units (IMUs) in a\nlearning-and-optimization framework. We first employ a weakly-supervised and\nhierarchical motion inference module based on cooperative Gated Recurrent Unit\n(GRU) blocks that serve as limb, body and root trackers as well as an inverse\nkinematics solver. Our network effectively narrows the search space of\nplausible motions via coarse-to-fine pose estimation and manages to tackle\nchallenging movements with high efficiency. We further develop a hybrid\noptimization scheme that combines inertial feedback and visual cues to improve\ntracking accuracy. Extensive experiments on various datasets demonstrate\nHybridCap can robustly handle challenging movements ranging from fitness\nactions to Latin dance. It also achieves real-time performance up to 60 fps\nwith state-of-the-art accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Han Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yannan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengfeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mutian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PreTR: Spatio-Temporal Non-Autoregressive Trajectory Prediction Transformer. (arXiv:2203.09293v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09293","description":"<p>Nowadays, our mobility systems are evolving into the era of intelligent\nvehicles that aim to improve road safety. Due to their vulnerability,\npedestrians are the users who will benefit the most from these developments.\nHowever, predicting their trajectory is one of the most challenging concerns.\nIndeed, accurate prediction requires a good understanding of multi-agent\ninteractions that can be complex. Learning the underlying spatial and temporal\npatterns caused by these interactions is even more of a competitive and open\nproblem that many researchers are tackling. In this paper, we introduce a model\ncalled PRediction Transformer (PReTR) that extracts features from the\nmulti-agent scenes by employing a factorized spatio-temporal attention module.\nIt shows less computational needs than previously studied models with\nempirically better results. Besides, previous works in motion prediction suffer\nfrom the exposure bias problem caused by generating future sequences\nconditioned on model prediction samples rather than ground-truth samples. In\norder to go beyond the proposed solutions, we leverage encoder-decoder\nTransformer networks for parallel decoding a set of learned object queries.\nThis non-autoregressive solution avoids the need for iterative conditioning and\narguably decreases training and testing computational time. We evaluate our\nmodel on the ETH/UCY datasets, a publicly available benchmark for pedestrian\ntrajectory prediction. Finally, we justify our usage of the parallel decoding\ntechnique by showing that the trajectory prediction task can be better solved\nas a non-autoregressive task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1\">Lina Achaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barry_T/0/1/0/all/0/1\">Thierno Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1\">Thibault Fouqueray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1\">Francois Aioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1\">Francois Charpillet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Differentiable Two-stage Alignment Scheme for Burst Image Reconstruction with Large Shift. (arXiv:2203.09294v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09294","description":"<p>Denoising and demosaicking are two essential steps to reconstruct a clean\nfull-color image from the raw data. Recently, joint denoising and demosaicking\n(JDD) for burst images, namely JDD-B, has attracted much attention by using\nmultiple raw images captured in a short time to reconstruct a single\nhigh-quality image. One key challenge of JDD-B lies in the robust alignment of\nimage frames. State-of-the-art alignment methods in feature domain cannot\neffectively utilize the temporal information of burst images, where large\nshifts commonly exist due to camera and object motion. In addition, the higher\nresolution (e.g., 4K) of modern imaging devices results in larger displacement\nbetween frames. To address these challenges, we design a differentiable\ntwo-stage alignment scheme sequentially in patch and pixel level for effective\nJDD-B. The input burst images are firstly aligned in the patch level by using a\ndifferentiable progressive block matching method, which can estimate the offset\nbetween distant frames with small computational cost. Then we perform implicit\npixel-wise alignment in full-resolution feature domain to refine the alignment\nresults. The two stages are jointly trained in an end-to-end manner. Extensive\nexperiments demonstrate the significant improvement of our method over existing\nJDD-B methods. Codes are available at https://github.com/GuoShi28/2StageAlign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Gaofeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Shot Adaptation of GAN in Just One CLIP. (arXiv:2203.09301v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09301","description":"<p>There are many recent research efforts to fine-tune a pre-trained generator\nwith a few target images to generate images of a novel domain. Unfortunately,\nthese methods often suffer from overfitting or under-fitting when fine-tuned\nwith a single target image. To address this, here we present a novel\nsingle-shot GAN adaptation method through unified CLIP space manipulations.\nSpecifically, our model employs a two-step training strategy: reference image\nsearch in the source generator using a CLIP-guided latent optimization,\nfollowed by generator fine-tuning with a novel loss function that imposes CLIP\nspace consistency between the source and adapted generators. To further improve\nthe adapted model to produce spatially consistent samples with respect to the\nsource generator, we also propose contrastive regularization for patchwise\nrelationships in the CLIP space. Experimental results show that our model\ngenerates diverse outputs with the target texture and outperforms the baseline\nmodels both qualitatively and quantitatively. Furthermore, we show that our\nCLIP space manipulation strategy allows more effective attribute editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gihyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Prediction at Multiple Scales with Hierarchical Recurrent Networks. (arXiv:2203.09303v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09303","description":"<p>Autonomous systems not only need to understand their current environment, but\nshould also be able to predict future actions conditioned on past states, for\ninstance based on captured camera frames. For certain tasks, detailed\npredictions such as future video frames are required in the near future,\nwhereas for others it is beneficial to also predict more abstract\nrepresentations for longer time horizons. However, existing video prediction\nmodels mainly focus on forecasting detailed possible outcomes for short\ntime-horizons, hence being of limited use for robot perception and spatial\nreasoning. We propose Multi-Scale Hierarchical Prediction (MSPred), a novel\nvideo prediction model able to forecast future possible outcomes of different\nlevels of granularity at different time-scales simultaneously. By combining\nspatial and temporal downsampling, MSPred is able to efficiently predict\nabstract representations such as human poses or object locations over long time\nhorizons, while still maintaining a competitive performance for video frame\nprediction. In our experiments, we demonstrate that our proposed model\naccurately predicts future video frames as well as other representations (e.g.\nkeypoints or positions) on various scenarios, including bin-picking scenes or\naction recognition datasets, consistently outperforming popular approaches for\nvideo frame prediction. Furthermore, we conduct an ablation study to\ninvestigate the importance of the different modules and design choices in\nMSPred. In the spirit of reproducible research, we open-source VP-Suite, a\ngeneral framework for deep-learning-based video prediction, as well as\npretrained models to reproduce our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karapetyan_A/0/1/0/all/0/1\">Ani Karapetyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villar_Corrales_A/0/1/0/all/0/1\">Angel Villar-Corrales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boltres_A/0/1/0/all/0/1\">Andreas Boltres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Structural Knowledge in Multimodal-BERT. (arXiv:2203.09306v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09306","description":"<p>In this work, we investigate the knowledge learned in the embeddings of\nmultimodal-BERT models. More specifically, we probe their capabilities of\nstoring the grammatical structure of linguistic data and the structure learned\nover objects in visual data. To reach that goal, we first make the inherent\nstructure of language and visuals explicit by a dependency parse of the\nsentences that describe the image and by the dependencies between the object\nregions in the image, respectively. We call this explicit visual structure the\n\\textit{scene tree}, that is based on the dependency tree of the language\ndescription. Extensive probing experiments show that the multimodal-BERT models\ndo not encode these scene trees.Code available at\n\\url{https://github.com/VSJMilewski/multimodal-probes}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milewski_V/0/1/0/all/0/1\">Victor Milewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1\">Miryam de Lhoneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localizing Visual Sounds the Easy Way. (arXiv:2203.09324v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09324","description":"<p>Unsupervised audio-visual source localization aims at localizing visible\nsound sources in a video without relying on ground-truth localization for\ntraining. Previous works often seek high audio-visual similarities for likely\npositive (sounding) regions and low similarities for likely negative regions.\nHowever, accurately distinguishing between sounding and non-sounding regions is\nchallenging without manual annotations. In this work, we propose a simple yet\neffective approach for Easy Visual Sound Localization, namely EZ-VSL, without\nrelying on the construction of positive and/or negative regions during\ntraining. Instead, we align audio and visual spaces by seeking audio-visual\nrepresentations that are aligned in, at least, one location of the associated\nimage, while not matching other images, at any location. We also introduce a\nnovel object guided localization scheme at inference time for improved\nprecision. Our simple and effective framework achieves state-of-the-art\nperformance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In\nparticular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to\n83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is\navailable at https://github.com/stoneMo/EZ-VSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgado_P/0/1/0/all/0/1\">Pedro Morgado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modulated Contrast for Versatile Image Synthesis. (arXiv:2203.09333v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09333","description":"<p>Perceiving the similarity between images has been a long-standing and\nfundamental problem underlying various visual generation tasks. Predominant\napproaches measure the inter-image distance by computing pointwise absolute\ndeviations, which tends to estimate the median of instance distributions and\nleads to blurs and artifacts in the generated images. This paper presents\nMoNCE, a versatile metric that introduces image contrast to learn a calibrated\nmetric for the perception of multifaceted inter-image distances. Unlike vanilla\ncontrast which indiscriminately pushes negative samples from the anchor\nregardless of their similarity, we propose to re-weight the pushing force of\nnegative samples adaptively according to their similarity to the anchor, which\nfacilitates the contrastive learning from informative negative samples. Since\nmultiple patch-level contrastive objectives are involved in image distance\nmeasurement, we introduce optimal transport in MoNCE to modulate the pushing\nforce of negative samples collaboratively across multiple contrastive\nobjectives. Extensive experiments over multiple image translation tasks show\nthat the proposed MoNCE outperforms various prevailing metrics substantially.\nThe code is available at https://github.com/fnzhan/MoNCE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Localization under Single Coarse Point Supervision. (arXiv:2203.09338v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09338","description":"<p>Point-based object localization (POL), which pursues high-performance object\nsensing under low-cost data annotation, has attracted increased attention.\nHowever, the point annotation mode inevitably introduces semantic variance for\nthe inconsistency of annotated points. Existing POL methods heavily reply on\naccurate key-point annotations which are difficult to define. In this study, we\npropose a POL method using coarse point annotations, relaxing the supervision\nsignals from accurate key points to freely spotted points. To this end, we\npropose a coarse point refinement (CPR) approach, which to our best knowledge\nis the first attempt to alleviate semantic variance from the perspective of\nalgorithm. CPR constructs point bags, selects semantic-correlated points, and\nproduces semantic center points through multiple instance learning (MIL). In\nthis way, CPR defines a weakly supervised evolution procedure, which ensures\ntraining high-performance object localizer under coarse point supervision.\nExperimental results on COCO, DOTA and our proposed SeaPerson dataset validate\nthe effectiveness of the CPR approach. The dataset and code will be available\nat https://github.com/ucas-vg/PointTinyBenchmark/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuehui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_N/0/1/0/all/0/1\">Najmul Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guorong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhenjun Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CYBORGS: Contrastively Bootstrapping Object Representations by Grounding in Segmentation. (arXiv:2203.09343v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09343","description":"<p>Many recent approaches in contrastive learning have worked to close the gap\nbetween pretraining on iconic images like ImageNet and pretraining on complex\nscenes like COCO. This gap exists largely because commonly used random crop\naugmentations obtain semantically inconsistent content in crowded scene images\nof diverse objects. Previous works use preprocessing pipelines to localize\nsalient objects for improved cropping, but an end-to-end solution is still\nelusive. In this work, we propose a framework which accomplishes this goal via\njoint learning of representations and segmentation. We leverage segmentation\nmasks to train a model with a mask-dependent contrastive loss, and use the\npartially trained model to bootstrap better masks. By iterating between these\ntwo components, we ground the contrastive updates in segmentation information,\nand simultaneously improve segmentation throughout pretraining. Experiments\nshow our representations transfer robustly to downstream tasks in\nclassification, detection and segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POSTER: Diagnosis of COVID-19 through Transfer Learning Techniques on CT Scans: A Comparison of Deep Learning Models. (arXiv:2203.09348v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09348","description":"<p>The novel coronavirus disease (COVID-19) constitutes a public health\nemergency globally. It is a deadly disease which has infected more than 230\nmillion people worldwide. Therefore, early and unswerving detection of COVID-19\nis necessary. Evidence of this virus is most commonly being tested by RT-PCR\ntest. This test is not 100% reliable as it is known to give false positives and\nfalse negatives. Other methods like X-Ray images or CT scans show the detailed\nimaging of lungs and have been proven more reliable. This paper compares\ndifferent deep learning models used to detect COVID-19 through transfer\nlearning technique on CT scan dataset. VGG-16 outperforms all the other models\nachieving an accuracy of 85.33% on the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ashraf_A/0/1/0/all/0/1\">Aeyan Ashraf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_A/0/1/0/all/0/1\">Asad Malik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_Z/0/1/0/all/0/1\">Zahid Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine Detailed Texture Learning for 3D Meshes with Generative Models. (arXiv:2203.09362v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09362","description":"<p>This paper presents a method to reconstruct high-quality textured 3D models\nfrom both multi-view and single-view images. The reconstruction is posed as an\nadaptation problem and is done progressively where in the first stage, we focus\non learning accurate geometry, whereas in the second stage, we focus on\nlearning the texture with a generative adversarial network. In the generative\nlearning pipeline, we propose two improvements. First, since the learned\ntextures should be spatially aligned, we propose an attention mechanism that\nrelies on the learnable positions of pixels. Secondly, since discriminator\nreceives aligned texture maps, we augment its input with a learnable embedding\nwhich improves the feedback to the generator. We achieve significant\nimprovements on multi-view sequences from Tripod dataset as well as on\nsingle-view image datasets, Pascal 3D+ and CUB. We demonstrate that our method\nachieves superior 3D textured models compared to the previous works. Please\nvisit our web-page for 3D visuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1\">Aysegul Dundar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1\">Andrew Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interacting Attention Graph for Single Image Two-Hand Reconstruction. (arXiv:2203.09364v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09364","description":"<p>Graph convolutional network (GCN) has achieved great success in single hand\nreconstruction task, while interacting two-hand reconstruction by GCN remains\nunexplored. In this paper, we present Interacting Attention Graph Hand\n(IntagHand), the first graph convolution based network that reconstructs two\ninteracting hands from a single RGB image. To solve occlusion and interaction\nchallenges of two-hand reconstruction, we introduce two novel attention based\nmodules in each upsampling step of the original GCN. The first module is the\npyramid image feature attention (PIFA) module, which utilizes multiresolution\nfeatures to implicitly obtain vertex-to-image alignment. The second module is\nthe cross hand attention (CHA) module that encodes the coherence of interacting\nhands by building dense cross-attention between two hand vertices. As a result,\nour model outperforms all existing two-hand reconstruction methods by a large\nmargin on InterHand2.6M benchmark. Moreover, ablation studies verify the\neffectiveness of both PIFA and CHA modules for improving the reconstruction\naccuracy. Results on in-the-wild images further demonstrate the generalization\nability of our network. Our code is available at\nhttps://github.com/Dw1010/IntagHand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Liang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lianpeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming Gait: Video-Based Spatiotemporal Gait Analysis. (arXiv:2203.09371v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09371","description":"<p>Human pose estimation from monocular video is a rapidly advancing field that\noffers great promise to human movement science and rehabilitation. This\npotential is tempered by the smaller body of work ensuring the outputs are\nclinically meaningful and properly calibrated. Gait analysis, typically\nperformed in a dedicated lab, produces precise measurements including\nkinematics and step timing. Using over 7000 monocular video from an\ninstrumented gait analysis lab, we trained a neural network to map 3D joint\ntrajectories and the height of individuals onto interpretable biomechanical\noutputs including gait cycle timing and sagittal plane joint kinematics and\nspatiotemporal trajectories. This task specific layer produces accurate\nestimates of the timing of foot contact and foot off events. After parsing the\nkinematic outputs into individual gait cycles, it also enables accurate\ncycle-by-cycle estimates of cadence, step time, double and single support time,\nwalking speed and step length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cotton_R/0/1/0/all/0/1\">R. James Cotton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClerklin_E/0/1/0/all/0/1\">Emoonah McClerklin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimorelli_A/0/1/0/all/0/1\">Anthony Cimorelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ankit Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakostas_T/0/1/0/all/0/1\">Tasos Karakostas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using the Order of Tomographic Slices as a Prior for Neural Networks Pre-Training. (arXiv:2203.09372v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09372","description":"<p>The technical advances in Computed Tomography (CT) allow to obtain immense\namounts of 3D data. For such datasets it is very costly and time-consuming to\nobtain the accurate 3D segmentation markup to train neural networks. The\nannotation is typically done for a limited number of 2D slices, followed by an\ninterpolation. In this work, we propose a pre-training method SortingLoss. It\nperforms pre-training on slices instead of volumes, so that a model could be\nfine-tuned on a sparse set of slices, without the interpolation step. Unlike\ngeneral methods (e.g. SimCLR or Barlow Twins), the task specific methods (e.g.\nTransferable Visual Words) trade broad applicability for quality benefits by\nimposing stronger assumptions on the input data. We propose a relatively mild\nassumption -- if we take several slices along some axis of a volume, structure\nof the sample presented on those slices, should give a strong clue to\nreconstruct the correct order of those slices along the axis. Many biomedical\ndatasets fulfill this requirement due to the specific anatomy of a sample and\npre-defined alignment of the imaging setup. We examine the proposed method on\ntwo datasets: medical CT of lungs affected by COVID-19 disease, and\nhigh-resolution synchrotron-based full-body CT of model organisms (Medaka\nfish). We show that the proposed method performs on par with SimCLR, while\nworking 2x faster and requiring 1.5x less memory. In addition, we present the\nbenefits in terms of practical scenarios, especially the applicability to the\npre-training of large models and the ability to localize samples within volumes\nin an unsupervised setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zharov_Y/0/1/0/all/0/1\">Yaroslav Zharov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ershov_A/0/1/0/all/0/1\">Alexey Ershov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumbach_T/0/1/0/all/0/1\">Tilo Baumbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heuveline_V/0/1/0/all/0/1\">Vincent Heuveline</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Part Priors: Learning to Optimize Part-Based Object Completion in RGB-D Scans. (arXiv:2203.09375v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09375","description":"<p>3D object recognition has seen significant advances in recent years, showing\nimpressive performance on real-world 3D scan benchmarks, but lacking in object\npart reasoning, which is fundamental to higher-level scene understanding such\nas inter-object similarities or object functionality. Thus, we propose to\nleverage large-scale synthetic datasets of 3D shapes annotated with part\ninformation to learn Neural Part Priors (NPPs), optimizable spaces\ncharacterizing geometric part priors. Crucially, we can optimize over the\nlearned part priors in order to fit to real-world scanned 3D scenes at test\ntime, enabling robust part decomposition of the real objects in these scenes\nthat also estimates the complete geometry of the object while fitting\naccurately to the observed real geometry. Moreover, this enables global\noptimization over geometrically similar detected objects in a scene, which\noften share strong geometric commonalities, enabling scene-consistent part\ndecompositions. Experiments on the ScanNet dataset demonstrate that NPPs\nsignificantly outperforms state of the art in part decomposition and object\ncompletion in real-world scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokhovkin_A/0/1/0/all/0/1\">Alexey Bokhovkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Stage Deep Edge Detection Based on Dense-Scale Feature Fusion and Pixel-Level Imbalance Learning. (arXiv:2203.09387v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09387","description":"<p>Edge detection, a basic task in the field of computer vision, is an important\npreprocessing operation for the recognition and understanding of a visual\nscene. In conventional models, the edge image generated is ambiguous, and the\nedge lines are also very thick, which typically necessitates the use of\nnon-maximum suppression (NMS) and morphological thinning operations to generate\nclear and thin edge images. In this paper, we aim to propose a one-stage neural\nnetwork model that can generate high-quality edge images without\npostprocessing. The proposed model adopts a classic encoder-decoder framework\nin which a pre-trained neural model is used as the encoder and a\nmulti-feature-fusion mechanism that merges the features of each level with each\nother functions as a learnable decoder. Further, we propose a new loss function\nthat addresses the pixel-level imbalance in the edge image by suppressing the\nfalse positive (FP) edge information near the true positive (TP) edge and the\nfalse negative (FN) non-edge. The results of experiments conducted on several\nbenchmark datasets indicate that the proposed method achieves state-of-the-art\nresults without using NMS and morphological thinning operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dawei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shuyin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-resolution. (arXiv:2203.09388v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09388","description":"<p>Scene text image super-resolution aims to increase the resolution and\nreadability of the text in low-resolution images. Though significant\nimprovement has been achieved by deep convolutional neural networks (CNNs), it\nremains difficult to reconstruct high-resolution images for spatially deformed\ntexts, especially rotated and curve-shaped ones. This is because the current\nCNN-based methods adopt locality-based operations, which are not effective to\ndeal with the variation caused by deformations. In this paper, we propose a CNN\nbased Text ATTention network (TATT) to address this problem. The semantics of\nthe text are firstly extracted by a text recognition module as text prior\ninformation. Then we design a novel transformer-based module, which leverages\nglobal attention mechanism, to exert the semantic guidance of text prior to the\ntext reconstruction process. In addition, we propose a text structure\nconsistency loss to refine the visual appearance by imposing structural\nconsistency on the reconstructions of regular and deformed texts. Experiments\non the benchmark TextZoom dataset show that the proposed TATT not only achieves\nstate-of-the-art performance in terms of PSNR/SSIM metrics, but also\nsignificantly improves the recognition accuracy in the downstream text\nrecognition task, particularly for text instances with multi-orientation and\ncurved shapes. Code is available at https://github.com/mjq11302010044/TATT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhetong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medium Transmission Map Matters for Learning to Restore Real-World Underwater Images. (arXiv:2203.09414v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09414","description":"<p>Underwater visual perception is essentially important for underwater\nexploration, archeology, ecosystem and so on. The low illumination, light\nreflections, scattering, absorption and suspended particles inevitably lead to\nthe critically degraded underwater image quality, which causes great challenges\non recognizing the objects from the underwater images. The existing underwater\nenhancement methods that aim to promote the underwater visibility, heavily\nsuffer from the poor image restoration performance and generalization ability.\nTo reduce the difficulty of underwater image enhancement, we introduce the\nmedia transmission map as guidance to assist in image enhancement. We formulate\nthe interaction between the underwater visual images and the transmission map\nto obtain better enhancement results. Even with simple and lightweight network\nconfiguration, the proposed method can achieve advanced results of 22.6 dB on\nthe challenging Test-R90 with an impressive 30 times faster than the existing\nmodels. Comprehensive experimental results have demonstrated the superiority\nand potential on underwater perception. Paper's code is privoded on:\nhttps://github.com/GroupG-yk/MTUR-Net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kai_Y/0/1/0/all/0/1\">Yan Kai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanyue_L/0/1/0/all/0/1\">Liang Lanyue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziqiang_Z/0/1/0/all/0/1\">Zheng Ziqiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guoqing_W/0/1/0/all/0/1\">Wang Guoqing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-directional Object-context Prioritization Learning for Saliency Ranking. (arXiv:2203.09416v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09416","description":"<p>The saliency ranking task is recently proposed to study the visual behavior\nthat humans would typically shift their attention over different objects of a\nscene based on their degrees of saliency. Existing approaches focus on learning\neither object-object or object-scene relations. Such a strategy follows the\nidea of object-based attention in Psychology, but it tends to favor those\nobjects with strong semantics (e.g., humans), resulting in unrealistic saliency\nranking. We observe that spatial attention works concurrently with object-based\nattention in the human visual recognition system. During the recognition\nprocess, the human spatial attention mechanism would move, engage, and\ndisengage from region to region (i.e., context to context). This inspires us to\nmodel the region-level interactions, in addition to the object-level reasoning,\nfor saliency ranking. To this end, we propose a novel bi-directional method to\nunify spatial attention and object-based attention for saliency ranking. Our\nmodel includes two novel modules: (1) a selective object saliency (SOS) module\nthat models objectbased attention via inferring the semantic representation of\nthe salient object, and (2) an object-context-object relation (OCOR) module\nthat allocates saliency ranks to objects by jointly modeling the object-context\nand context-object interactions of the salient objects. Extensive experiments\nshow that our approach outperforms existing state-of-theart methods. Our code\nand pretrained model are available at https://github.com/GrassBro/OCOR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation. (arXiv:2203.09418v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09418","description":"<p>Establishing correspondences from image to 3D has been a key task of 6DoF\nobject pose estimation for a long time. To predict pose more accurately, deeply\nlearned dense maps replaced sparse templates. Dense methods also improved pose\nestimation in the presence of occlusion. More recently researchers have shown\nimprovements by learning object fragments as segmentation. In this work, we\npresent a discrete descriptor, which can represent the object surface densely.\nBy incorporating a hierarchical binary grouping, we can encode the object\nsurface very efficiently. Moreover, we propose a coarse to fine training\nstrategy, which enables fine-grained correspondence prediction. Finally, by\nmatching predicted codes with object surface and using a PnP solver, we\nestimate the 6DoF pose. Results on the public LM-O and YCB-V datasets show\nmajor improvement over the state of the art w.r.t. ADD(-S) metric, even\nsurpassing RGB-D based methods in some cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yongzhi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1\">Mahdi Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetzer_T/0/1/0/all/0/1\">Torben Fetzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Unsupervised Hashing with Latent Semantic Components. (arXiv:2203.09420v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09420","description":"<p>Deep unsupervised hashing has been appreciated in the regime of image\nretrieval. However, most prior arts failed to detect the semantic components\nand their relationships behind the images, which makes them lack discriminative\npower. To make up the defect, we propose a novel Deep Semantic Components\nHashing (DSCH), which involves a common sense that an image normally contains a\nbunch of semantic components with homology and co-occurrence relationships.\nBased on this prior, DSCH regards the semantic components as latent variables\nunder the Expectation-Maximization framework and designs a two-step iterative\nalgorithm with the objective of maximum likelihood of training data. Firstly,\nDSCH constructs a semantic component structure by uncovering the fine-grained\nsemantics components of images with a Gaussian Mixture Modal~(GMM), where an\nimage is represented as a mixture of multiple components, and the semantics\nco-occurrence are exploited. Besides, coarse-grained semantics components, are\ndiscovered by considering the homology relationships between fine-grained\ncomponents, and the hierarchy organization is then constructed. Secondly, DSCH\nmakes the images close to their semantic component centers at both fine-grained\nand coarse-grained levels, and also makes the images share similar semantic\ncomponents close to each other. Extensive experiments on three benchmark\ndatasets demonstrate that the proposed hierarchical semantic components indeed\nfacilitate the hashing model to achieve superior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinghong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaotian Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Learning for Domain Adaptation: Self-distillation Image Dehazing Network with Sample-cycle. (arXiv:2203.09430v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09430","description":"<p>Deep learning-based methods have made significant achievements for image\ndehazing. However, most of existing dehazing networks are concentrated on\ntraining models using simulated hazy images, resulting in generalization\nperformance degradation when applied on real-world hazy images because of\ndomain shift. In this paper, we propose a mutual learning dehazing framework\nfor domain adaption. Specifically, we first devise two siamese networks: a\nteacher network in the synthetic domain and a student network in the real\ndomain, and then optimize them in a mutual learning manner by leveraging EMA\nand joint loss. Moreover, we design a sample-cycle strategy based on density\naugmentation (HDA) module to introduce pseudo real-world image pairs provided\nby the student network into training for further improving the generalization\nperformance. Extensive experiments on both synthetic and real-world dataset\ndemonstrate that the propose mutual learning framework outperforms\nstate-of-the-art dehazing techniques in terms of subjective and objective\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Erkang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes. (arXiv:2203.09440v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09440","description":"<p>Many basic indoor activities such as eating or writing are always conducted\nupon different tabletops (e.g., coffee tables, writing desks). It is\nindispensable to understanding tabletop scenes in 3D indoor scene parsing\napplications. Unfortunately, it is hard to meet this demand by directly\ndeploying data-driven algorithms, since 3D tabletop scenes are rarely available\nin current datasets. To remedy this defect, we introduce TO-Scene, a\nlarge-scale dataset focusing on tabletop scenes, which contains 20,740 scenes\nwith three variants. To acquire the data, we design an efficient and scalable\nframework, where a crowdsourcing UI is developed to transfer CAD objects onto\ntables from ScanNet. Then the output tabletop scenes are simulated into real\nscans and annotated automatically.\n</p>\n<p>Further, we propose a tabletop-aware learning strategy for better perceiving\nthe small-sized tabletop instances. Notably, we also provide a real scanned\ntest set TO-Real to verify the practical value of TO-Scene. Experiments show\nthat the algorithms trained on TO-Scene indeed work on the realistic test data,\nand our proposed tabletop-aware learning strategy greatly improves the\nstate-of-the-art results on both 3D semantic segmentation and object detection\ntasks. TO-Scene and TO-Real, plus Web UI, will all be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mutian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haolin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Super-Resolution With Deep Variational Autoencoders. (arXiv:2203.09445v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09445","description":"<p>Image super-resolution (SR) techniques are used to generate a high-resolution\nimage from a low-resolution image. Until now, deep generative models such as\nautoregressive models and Generative Adversarial Networks (GANs) have proven to\nbe effective at modelling high-resolution images. Models based on Variational\nAutoencoders (VAEs) have often been criticized for their feeble generative\nperformance, but with new advancements such as VDVAE (very deep VAE), there is\nnow strong evidence that deep VAEs have the potential to outperform current\nstate-of-the-art models for high-resolution image generation. In this paper, we\nintroduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE\nmethodologies to improve upon image super-resolution using transfer learning on\npretrained VDVAEs. Through qualitative and quantitative evaluations, we show\nthat the proposed model is competitive with other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chira_D/0/1/0/all/0/1\">Darius Chira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haralampiev_I/0/1/0/all/0/1\">Ilian Haralampiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1\">Valentin Li&#xe9;vin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vox2Cortex: Fast Explicit Reconstruction of Cortical Surfaces from 3D MRI Scans with Geometric Deep Neural Networks. (arXiv:2203.09446v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09446","description":"<p>The reconstruction of cortical surfaces from brain magnetic resonance imaging\n(MRI) scans is essential for quantitative analyses of cortical thickness and\nsulcal morphology. Although traditional and deep learning-based algorithmic\npipelines exist for this purpose, they have two major drawbacks: lengthy\nruntimes of multiple hours (traditional) or intricate post-processing, such as\nmesh extraction and topology correction (deep learning-based). In this work, we\naddress both of these issues and propose Vox2Cortex, a deep learning-based\nalgorithm that directly yields topologically correct, three-dimensional meshes\nof the boundaries of the cortex. Vox2Cortex leverages convolutional and graph\nconvolutional neural networks to deform an initial template to the densely\nfolded geometry of the cortex represented by an input MRI scan. We show in\nextensive experiments on three brain MRI datasets that our meshes are as\naccurate as the ones reconstructed by state-of-the-art methods in the field,\nwithout the need for time- and resource-intensive post-processing. To\naccurately reconstruct the tightly folded cortex, we work with meshes\ncontaining about 168,000 vertices at test time, scaling deep explicit\nreconstruction methods to a new level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bongratz_F/0/1/0/all/0/1\">Fabian Bongratz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rickmann_A/0/1/0/all/0/1\">Anne-Marie Rickmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning Based on OOD Detection and Task Masking. (arXiv:2203.09450v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09450","description":"<p>Existing continual learning techniques focus on either task incremental\nlearning (TIL) or class incremental learning (CIL) problem, but not both. CIL\nand TIL differ mainly in that the task-id is provided for each test sample\nduring testing for TIL, but not provided for CIL. Continual learning methods\nintended for one problem have limitations on the other problem. This paper\nproposes a novel unified approach based on out-of-distribution (OOD) detection\nand task masking, called CLOM, to solve both problems. The key novelty is that\neach task is trained as an OOD detection model rather than a traditional\nsupervised learning model, and a task mask is trained to protect each task to\nprevent forgetting. Our evaluation shows that CLOM outperforms existing\nstate-of-the-art baselines by large margins. The average TIL/CIL accuracy of\nCLOM over six experiments is 87.6/67.9% while that of the best baselines is\nonly 82.4/55.0%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuhak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilpour_S/0/1/0/all/0/1\">Sepideh Esmaeilpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Changnan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic-to-Real Domain Adaptation using Contrastive Unpaired Translation. (arXiv:2203.09454v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09454","description":"<p>The usefulness of deep learning models in robotics is largely dependent on\nthe availability of training data. Manual annotation of training data is often\ninfeasible. Synthetic data is a viable alternative, but suffers from domain\ngap. We propose a multi-step method to obtain training data without manual\nannotation effort: From 3D object meshes, we generate images using a modern\nsynthesis pipeline. We utilize a state-of-the-art image-to-image translation\nmethod to adapt the synthetic images to the real domain, minimizing the domain\ngap in a learned manner. The translation network is trained from unpaired\nimages, i.e. just requires an un-annotated collection of real images. The\ngenerated and refined images can then be used to train deep learning models for\na particular task. We also propose and evaluate extensions to the translation\nmethod that further increase performance, such as patch-based training, which\nshortens training time and increases global consistency. We evaluate our method\nand demonstrate its effectiveness on two robotic datasets. We finally give\ninsight into the learned refinement operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imbusch_B/0/1/0/all/0/1\">Benedikt T. Imbusch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1\">Max Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image. (arXiv:2203.09457v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09457","description":"<p>Novel view synthesis from a single image has recently attracted a lot of\nattention, and it has been primarily advanced by 3D deep learning and rendering\ntechniques. However, most work is still limited by synthesizing new views\nwithin relatively small camera motions. In this paper, we propose a novel\napproach to synthesize a consistent long-term video given a single scene image\nand a trajectory of large camera motions. Our approach utilizes an\nautoregressive Transformer to perform sequential modeling of multiple frames,\nwhich reasons the relations between multiple frames and the corresponding\ncameras to predict the next frame. To facilitate learning and ensure\nconsistency among generated frames, we introduce a locality constraint based on\nthe input cameras to guide self-attention among a large number of patches\nacross space and time. Our method outperforms state-of-the-art view synthesis\napproaches by a large margin, especially when synthesizing long-term future in\nindoor 3D scenes. Project page at https://xrenaa.github.io/look-outside-room/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos. (arXiv:2203.09463v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09463","description":"<p>Current benchmarks for facial expression recognition (FER) mainly focus on\nstatic images, while there are limited datasets for FER in videos. It is still\nambiguous to evaluate whether performances of existing methods remain\nsatisfactory in real-world application-oriented scenes. For example, the\n\"Happy\" expression with high intensity in Talk-Show is more discriminating than\nthe same expression with low intensity in Official-Event. To fill this gap, we\nbuild a large-scale multi-scene dataset, coined as FERV39k. We analyze the\nimportant ingredients of constructing such a novel dataset in three aspects:\n(1) multi-scene hierarchy and expression class, (2) generation of candidate\nvideo clips, (3) trusted manual labelling process. Based on these guidelines,\nwe select 4 scenarios subdivided into 22 scenes, annotate 86k samples\nautomatically obtained from 4k videos based on the well-designed workflow, and\nfinally build 38,935 video clips labeled with 7 classic expressions. Experiment\nbenchmarks on four kinds of baseline frameworks were also provided and further\nanalysis on their performance across different scenes and some challenges for\nfuture research were given. Besides, we systematically investigate key\ncomponents of DFER by ablation studies. The baseline framework and our project\nare available on url.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yixuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Normalized Density Map (SNDM) for Counting Microbiological Objects. (arXiv:2203.09474v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09474","description":"<p>The statistical properties of the density map (DM) approach to counting\nmicrobiological objects on images are studied in detail. The DM is given by\nU$^2$-Net. Two statistical methods for deep neural networks are utilized: the\nbootstrap and the Monte Carlo (MC) dropout. The detailed analysis of the\nuncertainties for the DM predictions leads to a deeper understanding of the DM\nmodel's deficiencies. Based on our investigation, we propose a\nself-normalization module in the network. The improved network model, called\nSelf-Normalized Density Map (SNDM), can correct its output density map by\nitself to accurately predict the total number of objects in the image. The SNDM\narchitecture outperforms the original model. Moreover, both statistical\nframeworks -- bootstrap and MC dropout -- have consistent statistical results\nfor SNDM, which were not observed in the original model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graczyk_K/0/1/0/all/0/1\">Krzysztof M. Graczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawlowski_J/0/1/0/all/0/1\">Jaros&#x142;aw Paw&#x142;owski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1\">Sylwia Majchrowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golan_T/0/1/0/all/0/1\">Tomasz Golan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data. (arXiv:2203.09475v1 [cs.RO])","link":"http://arxiv.org/abs/2203.09475","description":"<p>Vision-based segmentation of the robotic tool during robot-assisted surgery\nenables downstream applications, such as augmented reality feedback, while\nallowing for inaccuracies in robot kinematics. With the introduction of deep\nlearning, many methods were presented to solve instrument segmentation directly\nand solely from images. While these approaches made remarkable progress on\nbenchmark datasets, fundamental challenges pertaining to their robustness\nremain. We present CaRTS, a causality-driven robot tool segmentation algorithm,\nthat is designed based on a complementary causal model of the robot tool\nsegmentation task. Rather than directly inferring segmentation masks from\nobserved images, CaRTS iteratively aligns tool models with image observations\nby updating the initially incorrect robot kinematic parameters through forward\nkinematics and differentiable rendering to optimize image feature similarity\nend-to-end. We benchmark CaRTS with competing techniques on both synthetic as\nwell as real data from the dVRK, generated in precisely controlled scenarios to\nallow for counterfactual synthesis. On training-domain test data, CaRTS\nachieves a Dice score of 93.4 that is preserved well (Dice score of 91.8) when\ntested on counterfactual altered test data, exhibiting low brightness, smoke,\nblood, and altered background patterns. This compares favorably to Dice scores\nof 95.0 and 62.8, respectively, of a purely image-based method trained and\ntested on the same data. Future work will involve accelerating CaRTS to achieve\nvideo framerate and estimating the impact occlusion has in practice. Despite\nthese limitations, our results are promising: In addition to achieving high\nsegmentation accuracy, CaRTS provides estimates of the true robot kinematics,\nwhich may benefit applications such as force estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jintan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazanzides_P/0/1/0/all/0/1\">Peter Kazanzides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jieying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision Algorithm for Predicting the Welding Efficiency of Friction Stir Welded Copper Joints from its Microstructures. (arXiv:2203.09479v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09479","description":"<p>Friction Stir Welding is a robust joining process, and numerous AI-based\nalgorithms are being developed in this field to enhance mechanical and\nmicrostructure properties. Convolutional Neural Networks (CNNs) are Artificial\nNeural Networks that use image data as input. Identical to Artificial Neural\nNetworks, they are composed of weights that are determined throughout learning,\nneurons (activated functions), and a goal (loss function). CNN is utilized in a\nvariety of applications, including image recognition, semantic segmentation,\nimage recognition, and localization. Utilizing training on 3000 microstructure\npictures and new tests on 300 microstructure photographs, the current work\ninvestigates the predictions of Friction Stir Welded joint effectiveness using\nmicrostructure images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Akshansh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suman_A/0/1/0/all/0/1\">Asmita Suman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_D/0/1/0/all/0/1\">Devarrishi Dixit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09481","description":"<p>Denoising diffusion probabilistic models are a promising new class of\ngenerative models that are competitive with GANs on perceptual metrics. In this\npaper, we explore their potential for sequentially generating video. Inspired\nby recent advances in neural video compression, we use denoising diffusion\nmodels to stochastically generate a residual to a deterministic next-frame\nprediction. We compare this approach to two sequential VAE and two GAN\nbaselines on four datasets, where we test the generated frames for perceptual\nquality and forecasting accuracy against ground truth frames. We find\nsignificant improvements in terms of perceptual quality on all data and\nimprovements in terms of frame forecasting for complex high-resolution videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prakhar Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1\">Stephan Mandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transframer: Arbitrary Frame Prediction with Generative Models. (arXiv:2203.09494v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09494","description":"<p>We present a general-purpose framework for image modelling and vision tasks\nbased on probabilistic frame prediction. Our approach unifies a broad range of\ntasks, from image segmentation, to novel view synthesis and video\ninterpolation. We pair this framework with an architecture we term Transframer,\nwhich uses U-Net and Transformer components to condition on annotated context\nframes, and outputs sequences of sparse, compressed image features. Transframer\nis the state-of-the-art on a variety of video generation benchmarks, is\ncompetitive with the strongest models on few-shot view synthesis, and can\ngenerate coherent 30 second videos from a single image without any explicit\ngeometric information. A single generalist Transframer simultaneously produces\npromising results on 8 tasks, including semantic segmentation, image\nclassification and optical flow prediction with no task-specific architectural\ncomponents, demonstrating that multi-task computer vision can be tackled using\nprobabilistic image models. Our approach can in principle be applied to a wide\nrange of applications that require learning the conditional structure of\nannotated image-formatted data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nash_C/0/1/0/all/0/1\">Charlie Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">Jacob Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barr_I/0/1/0/all/0/1\">Iain Barr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter Battaglia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing Global Explanations of Point Cloud DNNs. (arXiv:2203.09505v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09505","description":"<p>In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose a point cloud-applicable explainability approach\nbased on a local surrogate model-based method to show which components\ncontribute to the classification. Moreover, we propose quantitative fidelity\nvalidations for generated explanations that enhance the persuasive power of\nexplainability and compare the plausibility of different existing point\ncloud-applicable explainability methods. Our new explainability approach\nprovides a fairly accurate, more semantically coherent and widely applicable\nexplanation for point cloud classification tasks. Our code is available at\nhttps://github.com/Explain3D/LIME-3D\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Data-Efficient Detection Transformers. (arXiv:2203.09507v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09507","description":"<p>Detection Transformers have achieved competitive performance on the\nsample-rich COCO dataset. However, we show most of them suffer from significant\nperformance drops on small-size datasets, like Cityscapes. In other words, the\ndetection transformers are generally data-hungry. To tackle this problem, we\nempirically analyze the factors that affect data efficiency, through a\nstep-by-step transition from a data-efficient RCNN variant to the\nrepresentative DETR. The empirical results suggest that sparse feature sampling\nfrom local image areas holds the key. Based on this observation, we alleviate\nthe data-hungry issue of existing detection transformers by simply alternating\nhow key and value sequences are constructed in the cross-attention layer, with\nminimum modifications to the original models. Besides, we introduce a simple\nyet effective label augmentation method to provide richer supervision and\nimprove data efficiency. Experiments show that our method can be readily\napplied to different detection transformers and improve their performance on\nboth small-size and sample-rich datasets. Code will be made publicly available\nat \\url{https://github.com/encounter1997/DE-DETRs}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DetMatch: Two Teachers are Better Than One for Joint 2D and 3D Semi-Supervised Object Detection. (arXiv:2203.09510v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09510","description":"<p>While numerous 3D detection works leverage the complementary relationship\nbetween RGB images and point clouds, developments in the broader framework of\nsemi-supervised object recognition remain uninfluenced by multi-modal fusion.\nCurrent methods develop independent pipelines for 2D and 3D semi-supervised\nlearning despite the availability of paired image and point cloud frames.\nObserving that the distinct characteristics of each sensor cause them to be\nbiased towards detecting different objects, we propose DetMatch, a flexible\nframework for joint semi-supervised learning on 2D and 3D modalities. By\nidentifying objects detected in both sensors, our pipeline generates a cleaner,\nmore robust set of pseudo-labels that both demonstrates stronger performance\nand stymies single-modality error propagation. Further, we leverage the richer\nsemantics of RGB images to rectify incorrect 3D class predictions and improve\nlocalization of 3D boxes. Evaluating on the challenging KITTI and Waymo\ndatasets, we improve upon strong semi-supervised learning methods and observe\nhigher quality pseudo-labels. Code will be released at\nhttps://github.com/Divadi/DetMatch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinhyung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Multi-Domain Long-Tailed Recognition, Generalization and Beyond. (arXiv:2203.09513v1 [cs.LG])","link":"http://arxiv.org/abs/2203.09513","description":"<p>Real-world data often exhibit imbalanced label distributions. Existing\nstudies on data imbalance focus on single-domain settings, i.e., samples are\nfrom the same data distribution. However, natural data can originate from\ndistinct domains, where a minority class in one domain could have abundant\ninstances from other domains. We formalize the task of Multi-Domain Long-Tailed\nRecognition (MDLT), which learns from multi-domain imbalanced data, addresses\nlabel imbalance, domain shift, and divergent label distributions across\ndomains, and generalizes to all domain-class pairs. We first develop the\ndomain-class transferability graph, and show that such transferability governs\nthe success of learning in MDLT. We then propose BoDA, a theoretically grounded\nlearning strategy that tracks the upper bound of transferability statistics,\nand ensures balanced alignment and calibration across imbalanced domain-class\ndistributions. We curate five MDLT benchmarks based on widely-used multi-domain\ndatasets, and compare BoDA to twenty algorithms that span different learning\nstrategies. Extensive and rigorous experiments verify the superior performance\nof BoDA. Further, as a byproduct, BoDA establishes new state-of-the-art on\nDomain Generalization benchmarks, improving generalization to unseen domains.\nCode and data are available at\nhttps://github.com/YyzHarry/multi-domain-imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuzhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1\">Dina Katabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation. (arXiv:2203.09516v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09516","description":"<p>Powerful priors allow us to perform inference with insufficient information.\nIn this paper, we propose an autoregressive prior for 3D shapes to solve\nmultimodal 3D tasks such as shape completion, reconstruction, and generation.\nWe model the distribution over 3D shapes as a non-sequential autoregressive\ndistribution over a discretized, low-dimensional, symbolic grid-like latent\nrepresentation of 3D shapes. This enables us to represent distributions over 3D\nshapes conditioned on information from an arbitrary set of spatially anchored\nquery locations and thus perform shape completion in such arbitrary settings\n(e.g., generating a complete chair given only a view of the back leg). We also\nshow that the learned autoregressive prior can be leveraged for conditional\ntasks such as single-view reconstruction and language-based generation. This is\nachieved by learning task-specific naive conditionals which can be approximated\nby light-weight models trained on minimal paired data. We validate the\neffectiveness of the proposed method using both quantitative and qualitative\nevaluation and show that the proposed method outperforms the specialized\nstate-of-the-art methods trained for individual tasks. The project page with\ncode and video visualizations can be found at\nhttps://yccyenchicheng.github.io/AutoSDF/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Paritosh Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yen-Chi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1\">Shubham Tulsiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TensoRF: Tensorial Radiance Fields. (arXiv:2203.09517v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09517","description":"<p>We present TensoRF, a novel approach to model and reconstruct radiance\nfields. Unlike NeRF that purely uses MLPs, we model the radiance field of a\nscene as a 4D tensor, which represents a 3D voxel grid with per-voxel\nmulti-channel features. Our central idea is to factorize the 4D scene tensor\ninto multiple compact low-rank tensor components. We demonstrate that applying\ntraditional CP decomposition -- that factorizes tensors into rank-one\ncomponents with compact vectors -- in our framework leads to improvements over\nvanilla NeRF. To further boost performance, we introduce a novel vector-matrix\n(VM) decomposition that relaxes the low-rank constraints for two modes of a\ntensor and factorizes tensors into compact vector and matrix factors. Beyond\nsuperior rendering quality, our models with CP and VM decompositions lead to a\nsignificantly lower memory footprint in comparison to previous and concurrent\nworks that directly optimize per-voxel features. Experimentally, we demonstrate\nthat TensoRF with CP decomposition achieves fast reconstruction (&lt;30 min) with\nbetter rendering quality and even a smaller model size (&lt;4 MB) compared to\nNeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality\nand outperforms previous state-of-the-art methods, while reducing the\nreconstruction time (&lt;10 min) and retaining a compact model size (&lt;75 MB).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anpei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Descent Synchronization in $\\mathrm{SO}(D)$. (arXiv:2002.05299v3 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2002.05299","description":"<p>We give robust recovery results for synchronization on the rotation group,\n$\\mathrm{SO}(D)$. In particular, we consider an adversarial corruption setting,\nwhere a limited percentage of the observations are arbitrarily corrupted. We\ngive a novel algorithm that exploits Tukey depth in the tangent space, which\nexactly recovers the underlying rotations up to an outlier percentage of\n$1/(D(D-1)+2)$. This corresponds to an outlier fraction of $1/4$ for\n$\\mathrm{SO}(2)$ and $1/8$ for $\\mathrm{SO}(3)$. In the case of $D=2$, we\ndemonstrate that a variant of this algorithm converges linearly to the ground\ntruth rotations. We finish by discussing this result in relation to a simpler\nnonconvex energy minimization framework based on least absolute deviations,\nwhich exhibits spurious fixed points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Maunu_T/0/1/0/all/0/1\">Tyler Maunu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lerman_G/0/1/0/all/0/1\">Gilad Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intracranial Hemorrhage Detection Using Neural Network Based Methods With Federated Learning. (arXiv:2005.08644v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.08644","description":"<p>Intracranial hemorrhage, bleeding that occurs inside the cranium, is a\nserious health problem requiring rapid and often intensive medical treatment.\nSuch a condition is traditionally diagnosed by highly-trained specialists\nanalyzing computed tomography (CT) scan of the patient and identifying the\nlocation and type of hemorrhage if one exists. We propose a neural network\napproach to find and classify the condition based upon the CT scan. The model\narchitecture implements a time distributed convolutional network. We observed\naccuracy above 92% from such an architecture, provided enough data. We propose\nfurther extensions to our approach involving the deployment of federated\nlearning. This would be helpful in pooling learned parameters without violating\nthe inherent privacy of the data involved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_U/0/1/0/all/0/1\">Utkarsh Chandra Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anshuman Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dr. K. Sree Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers. (arXiv:2012.05858v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05858","description":"<p>Light-based adversarial attacks use spatial augmented reality (SAR)\ntechniques to fool image classifiers by altering the physical light condition\nwith a controllable light source, e.g., a projector. Compared with physical\nattacks that place hand-crafted adversarial objects, projector-based ones\nobviate modifying the physical entities, and can be performed transiently and\ndynamically by altering the projection pattern. However, subtle light\nperturbations are insufficient to fool image classifiers, due to the complex\nenvironment and project-and-capture process. Thus, existing approaches focus on\nprojecting clearly perceptible adversarial patterns, while the more interesting\nyet challenging goal, stealthy projector-based attack, remains open. In this\npaper, for the first time, we formulate this problem as an end-to-end\ndifferentiable process and propose a Stealthy Projector-based Adversarial\nAttack (SPAA) solution. In SPAA, we approximate the real Project-and-Capture\nprocess using a deep neural network named PCNet, then we include PCNet in the\noptimization of projector-based attacks such that the generated adversarial\nprojection is physically plausible. Finally, to generate both robust and\nstealthy adversarial projections, we propose an algorithm that uses minimum\nperturbation and adversarial confidence thresholds to alternate between the\nadversarial loss and stealthiness loss optimization. Our experimental\nevaluations show that SPAA clearly outperforms other methods by achieving\nhigher attack success rates and meanwhile being stealthier, for both targeted\nand untargeted attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bingyao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. (arXiv:2103.08573v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08573","description":"<p>The use of local detectors and descriptors in typical computer vision\npipelines work well until variations in viewpoint and appearance change become\nextreme. Past research in this area has typically focused on one of two\napproaches to this challenge: the use of projections into spaces more suitable\nfor feature matching under extreme viewpoint changes, and attempting to learn\nfeatures that are inherently more robust to viewpoint change. In this paper, we\npresent a novel framework that combines learning of invariant descriptors\nthrough data augmentation and orthographic viewpoint projection. We propose\nrotation-robust local descriptors, learnt through training data augmentation\nbased on rotation homographies, and a correspondence ensemble technique that\ncombines vanilla feature correspondences with those obtained through\nrotation-robust features. Using a range of benchmark datasets as well as\ncontributing a new bespoke dataset for this research domain, we evaluate the\neffectiveness of the proposed approach on key tasks including pose estimation\nand visual place recognition. Our system outperforms a range of baseline and\nstate-of-the-art techniques, including enabling higher levels of place\nrecognition precision across opposing place viewpoints and achieves\npractically-useful performance levels even under extreme viewpoint changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parihar_U/0/1/0/all/0/1\">Udit Singh Parihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujarathi_A/0/1/0/all/0/1\">Aniket Gujarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1\">Kinal Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1\">Satyajit Tourani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">K. Madhava Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Loss Weight Adjustment in Object Detection. (arXiv:2103.09488v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09488","description":"<p>Object detection is a typical multi-task learning application, which\noptimizes classification and regression simultaneously. However, classification\nloss always dominates the multi-task loss in anchor-based methods, hampering\nthe consistent and balanced optimization of the tasks. In this paper, we find\nthat shifting the bounding boxes can change the division of positive and\nnegative samples in classification, meaning classification depends on\nregression. Moreover, we summarize three important conclusions about\nfine-tuning loss weights, considering different datasets, optimizers and\nregression loss functions. Based on the above conclusions, we propose Adaptive\nLoss Weight Adjustment(ALWA) to solve the imbalance in optimizing anchor-based\nmethods according to statistical characteristics of losses. By incorporating\nALWA into previous state-of-the-art detectors, we achieve a significant\nperformance gain on PASCAL VOC and MS COCO, even with L1, SmoothL1 and CIoU\nloss. The code is available at https://github.com/ywx-hub/ALWA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenxin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xueling Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiajie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dong Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation. (arXiv:2103.09716v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09716","description":"<p>Identifying the status of individual network units is critical for\nunderstanding the mechanism of convolutional neural networks (CNNs). However,\nit is still challenging to reliably give a general indication of unit status,\nespecially for units in different network models. To this end, we propose a\nnovel method for quantitatively clarifying the status of single unit in CNN\nusing algebraic topological tools. Unit status is indicated via the calculation\nof a defined topological-based entropy, called feature entropy, which measures\nthe degree of chaos of the global spatial pattern hidden in the unit for a\ncategory. In this way, feature entropy could provide an accurate indication of\nstatus for units in different networks with diverse situations like\nweight-rescaling operation. Further, we show that feature entropy decreases as\nthe layer goes deeper and shares almost simultaneous trend with loss during\ntraining. We show that by investigating the feature entropy of units on only\ntraining data, it could give discrimination between networks with different\ngeneralization ability from the view of the effectiveness of feature\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13389","description":"<p>Given a large dataset for training, GANs can achieve remarkable performance\nfor the image synthesis task. However, training GANs in extremely low data\nregimes remains a challenge, as overfitting often occurs, leading to\nmemorization or training divergence. In this work, we introduce SIV-GAN, an\nunconditional generative model that can generate new scene compositions from a\nsingle training image or a single video clip. We propose a two-branch\ndiscriminator architecture, with content and layout branches designed to judge\ninternal content and scene layout realism separately from each other. This\ndiscriminator design enables synthesis of visually plausible, novel\ncompositions of a scene, with varying content and layout, while preserving the\ncontext of the original sample. Compared to previous single-image GANs, our\nmodel generates more diverse, higher quality images, while not being restricted\nto a single image setting. We show that SIV-GAN successfully deals with a new\nchallenging task of learning from a single video, for which prior GAN models\nfail to achieve synthesis of both high quality and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sushko_V/0/1/0/all/0/1\">Vadim Sushko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1\">Anna Khoreva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Anomaly Detection via Multi-task Self-Supervision. (arXiv:2104.09993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09993","description":"<p>Detecting anomalies using deep learning has become a major challenge over the\nlast years, and is becoming increasingly promising in several fields. The\nintroduction of self-supervised learning has greatly helped many methods\nincluding anomaly detection where simple geometric transformation recognition\ntasks are used. However these methods do not perform well on fine-grained\nproblems since they lack finer features. By combining in a multi-task framework\nhigh-scale shape features oriented task with low-scale fine features oriented\ntask, our method greatly improves fine-grained anomaly detection. It\noutperforms state-of-the-art with up to 31% relative error reduction measured\nwith AUROC on various anomaly detection problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jezequel_L/0/1/0/all/0/1\">Loic Jezequel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc-Son Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaudet_J/0/1/0/all/0/1\">Jean Beaudet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Histace_A/0/1/0/all/0/1\">Aymeric Histace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLSD: The Global Large-Scale Ship Database and Baseline Evaluations. (arXiv:2106.02773v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02773","description":"<p>In this paper, we introduce a challenging global large-scale ship database\n(called GLSD), designed specifically for ship detection tasks. The designed\nGLSD database includes a total of 212,357 annotated instances from 152,576\nimages. Based on the collected images, we propose 13 ship categories that\nwidely exist in international routes. These categories include Sailing boat,\nFishing boat, Passenger ship, Warship, General cargo ship, Container ship, Bulk\ncargo carrier, Barge, Ore carrier, Speed boat, Canoe, Oil carrier, and Tug. The\nmotivations of developing GLSD include the following: 1) providing a refine and\nextensive ship detection database that benefits the object detection community,\n2) establishing a database with exhaustive labels (bounding boxes and ship\nclass categories) in a uniform classification scheme, and 3) providing a\nlarge-scale ship database with geographic information (covering more than 3000\nports and 33 routes) that benefits multi-modal analysis. In addition, we\ndiscuss the evaluation protocols corresponding to image characteristics in GLSD\nand analyze the performance of selected state-of-the-art object detection\nalgorithms on GSLD, aiming to establish baselines for future studies. More\ninformation regarding the designed GLSD can be found at\nhttps://github.com/jiaming-wang/GLSD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lianbing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xianwei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_C/0/1/0/all/0/1\">Chaoya Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1\">Qing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category Contrast for Unsupervised Domain Adaptation in Visual Tasks. (arXiv:2106.02885v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02885","description":"<p>Instance contrast for unsupervised representation learning has achieved great\nsuccess in recent years. In this work, we explore the idea of instance\ncontrastive learning in unsupervised domain adaptation (UDA) and propose a\nnovel Category Contrast technique (CaCo) that introduces semantic priors on top\nof instance discrimination for visual UDA tasks. By considering instance\ncontrastive learning as a dictionary look-up operation, we construct a\nsemantics-aware dictionary with samples from both source and target domains\nwhere each target sample is assigned a (pseudo) category label based on the\ncategory priors of source samples. This allows category contrastive learning\n(between target queries and the category-level dictionary) for\ncategory-discriminative yet domain-invariant feature representations: samples\nof the same category (from either source or target domain) are pulled closer\nwhile those of different categories are pushed apart simultaneously. Extensive\nUDA experiments in multiple visual tasks (e.g., segmentation, classification\nand detection) show that CaCo achieves superior performance as compared with\nstate-of-the-art methods. The experiments also demonstrate that CaCo is\ncomplementary to existing UDA methods and generalizable to other learning\nsetups such as unsupervised model adaptation, open-/partial-set adaptation etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet-Packets for Deepfake Image Analysis and Detection. (arXiv:2106.09369v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09369","description":"<p>As neural networks become able to generate realistic artificial images, they\nhave the potential to improve movies, music, video games and make the internet\nan even more creative and inspiring place. Yet, the latest technology\npotentially enables new digital ways to lie. In response, the need for a\ndiverse and reliable method toolbox arises to identify artificial images and\nother content. Previous work primarily relies on pixel-space CNNs or the\nFourier transform. To the best of our knowledge, synthesized fake image\nanalysis and detection methods based on a multi-scale wavelet representation,\nlocalized in both space and frequency, have been absent thus far. The wavelet\ntransform conserves spatial information to a degree, which allows us to present\na new analysis. Comparing the wavelet coefficients of real and fake images\nallows interpretation. Significant differences are identified. Additionally,\nthis paper proposes to learn a model for the detection of synthetic images\nbased on the wavelet-packet representation of natural and GAN-generated images.\nOur lightweight forensic classifiers exhibit competitive or improved\nperformance at comparatively small network sizes, as we demonstrate on the\nFFHQ, CelebA and LSUN source identification problems. Furthermore, we study the\nbinary FaceForensics++ fake-detection problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolter_M/0/1/0/all/0/1\">Moritz Wolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanke_F/0/1/0/all/0/1\">Felix Blanke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heese_R/0/1/0/all/0/1\">Raoul Heese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcke_J/0/1/0/all/0/1\">Jochen Garcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReGO: Reference-Guided Outpainting for Scenery Image. (arXiv:2106.10601v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10601","description":"<p>We aim to tackle the challenging yet practical scenery image outpainting task\nin this work. Recently, generative adversarial learning has significantly\nadvanced the image outpainting by producing semantic consistent content for the\ngiven image. However, the existing methods always suffer from the blurry\ntexture and the artifacts of the generative part, making the overall\noutpainting results lack authenticity. To overcome the weakness, this work\ninvestigates a principle way to synthesize texture-rich results by borrowing\npixels from its neighbors (i.e., reference images), named\n\\textbf{Re}ference-\\textbf{G}uided \\textbf{O}utpainting (ReGO). Particularly,\nthe ReGO designs an Adaptive Content Selection (ACS) module to transfer the\npixel of reference images for texture compensating of the target one. To\nprevent the style of the generated part from being affected by the reference\nimages, a style ranking loss is further proposed to augment the ReGO to\nsynthesize style-consistent results. Extensive experiments on two popular\nbenchmarks, NS6K \\cite{yangzx} and NS8K \\cite{wang}, well demonstrate the\neffectiveness of our ReGO. Our code will be made public available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag of Instances Aggregation Boosts Self-supervised Distillation. (arXiv:2107.01691v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01691","description":"<p>Recent advances in self-supervised learning have experienced remarkable\nprogress, especially for contrastive learning based methods, which regard each\nimage as well as its augmentations as an individual class and try to\ndistinguish them from all other images. However, due to the large quantity of\nexemplars, this kind of pretext task intrinsically suffers from slow\nconvergence and is hard for optimization. This is especially true for\nsmall-scale models, in which we find the performance drops dramatically\ncomparing with its supervised counterpart. In this paper, we propose a simple\nbut effective distillation strategy for unsupervised learning. The highlight is\nthat the relationship among similar samples counts and can be seamlessly\ntransferred to the student to boost the performance. Our method, termed as\nBINGO, which is short for Bag of InstaNces aGgregatiOn, targets at transferring\nthe relationship learned by the teacher to the student. Here bag of instances\nindicates a set of similar samples constructed by the teacher and are grouped\nwithin a bag, and the goal of distillation is to aggregate compact\nrepresentations over the student with respect to instances in a bag. Notably,\nBINGO achieves new state-of-the-art performance on small-scale models, i.e.,\n65.5% and 68.9% top-1 accuracies with linear evaluation on ImageNet, using\nResNet-18 and ResNet-34 as the backbones respectively, surpassing baselines\n(52.5% and 57.4% top-1 accuracies) by a significant margin. The code is\navailable at https://github.com/haohang96/bingo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haohang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiemin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenrui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration. (arXiv:2107.05446v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05446","description":"<p>Source-free domain adaptation (SFDA) aims to adapt a model trained on\nlabelled data in a source domain to unlabelled data in a target domain without\naccess to the source-domain data during adaptation. Existing methods for SFDA\nleverage entropy-minimization techniques which: (i) apply only to\nclassification; (ii) destroy model calibration; and (iii) rely on the source\nmodel achieving a good level of feature-space class-separation in the target\ndomain. We address these issues for a particularly pervasive type of domain\nshift called measurement shift which can be resolved by restoring the source\nfeatures rather than extracting new ones. In particular, we propose Feature\nRestoration (FR) wherein we: (i) store a lightweight and flexible approximation\nof the feature distribution under the source data; and (ii) adapt the\nfeature-extractor such that the approximate feature distribution under the\ntarget data realigns with that saved on the source. We additionally propose a\nbottom-up training scheme which boosts performance, which we call Bottom-Up\nFeature Restoration (BUFR). On real and synthetic data, we demonstrate that\nBUFR outperforms existing SFDA methods in terms of accuracy, calibration, and\ndata efficiency, while being less reliant on the performance of the source\nmodel in the target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1\">Cian Eastwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mason_I/0/1/0/all/0/1\">Ian Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1\">Christopher K. I. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08323","description":"<p>Temporal action proposal generation is an essential and challenging task that\naims at localizing temporal intervals containing human actions in untrimmed\nvideos. Most of existing approaches are unable to follow the human cognitive\nprocess of understanding the video context due to lack of attention mechanism\nto express the concept of an action or an agent who performs the action or the\ninteraction between the agent and the environment. Based on the action\ndefinition that a human, known as an agent, interacts with the environment and\nperforms an action that affects the environment, we propose a contextual\nAgent-Environment Network. Our proposed contextual AEN involves (i) agent\npathway, operating at a local level to tell about which humans/agents are\nacting and (ii) environment pathway operating at a global level to tell about\nhow the agents interact with the environment. Comprehensive evaluations on\n20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different\nbackbone networks, i.e C3D and SlowFast, show that our method robustly exhibits\noutperformance against state-of-the-art methods regardless of the employed\nbackbone network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AS-MLP: An Axial Shifted MLP Architecture for Vision. (arXiv:2107.08391v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08391","description":"<p>An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper.\nDifferent from MLP-Mixer, where the global spatial feature is encoded for\ninformation flow through matrix transposition and one token-mixing MLP, we pay\nmore attention to the local features interaction. By axially shifting channels\nof the feature map, AS-MLP is able to obtain the information flow from\ndifferent axial directions, which captures the local dependencies. Such an\noperation enables us to utilize a pure MLP architecture to achieve the same\nlocal receptive field as CNN-like architecture. We can also design the\nreceptive field size and dilation of blocks of AS-MLP, etc, in the same spirit\nof convolutional neural networks. With the proposed AS-MLP architecture, our\nmodel obtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the\nImageNet-1K dataset. Such a simple yet effective architecture outperforms all\nMLP-based architectures and achieves competitive performance compared to the\ntransformer-based architectures (e.g., Swin Transformer) even with slightly\nlower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be\napplied to the downstream tasks (e.g., object detection and semantic\nsegmentation). The experimental results are also impressive. Our proposed\nAS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the\nADE20K dataset, which is competitive compared to the transformer-based\narchitectures. Our AS-MLP establishes a strong baseline of MLP-based\narchitecture. Code is available at https://github.com/svip-lab/AS-MLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Dongze Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10904","description":"<p>With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Fidelity GAN Inversion for Image Attribute Editing. (arXiv:2109.06590v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06590","description":"<p>We present a novel high-fidelity generative adversarial network (GAN)\ninversion framework that enables attribute editing with image-specific details\nwell-preserved (e.g., background, appearance, and illumination). We first\nanalyze the challenges of high-fidelity GAN inversion from the perspective of\nlossy data compression. With a low bit-rate latent code, previous works have\ndifficulties in preserving high-fidelity details in reconstructed and edited\nimages. Increasing the size of a latent code can improve the accuracy of GAN\ninversion but at the cost of inferior editability. To improve image fidelity\nwithout compromising editability, we propose a distortion consultation approach\nthat employs a distortion map as a reference for high-fidelity reconstruction.\nIn the distortion consultation inversion (DCI), the distortion map is first\nprojected to a high-rate latent map, which then complements the basic low-rate\nlatent code with more details via consultation fusion. To achieve high-fidelity\nediting, we propose an adaptive distortion alignment (ADA) module with a\nself-supervised training scheme, which bridges the gap between the edited and\ninversion images. Extensive experiments in the face and car domains show a\nclear improvement in both inversion and editing quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAda! Temporally-Adaptive Convolutions for Video Understanding. (arXiv:2110.06178v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06178","description":"<p>Spatial convolutions are widely used in numerous deep video models. It\nfundamentally assumes spatio-temporal invariance, i.e., using shared weights\nfor every location in different frames. This work presents Temporally-Adaptive\nConvolutions (TAdaConv) for video understanding, which shows that adaptive\nweight calibration along the temporal dimension is an efficient way to\nfacilitate modelling complex temporal dynamics in videos. Specifically,\nTAdaConv empowers the spatial convolutions with temporal modelling abilities by\ncalibrating the convolution weights for each frame according to its local and\nglobal temporal context. Compared to previous temporal modelling operations,\nTAdaConv is more efficient as it operates over the convolution kernels instead\nof the features, whose dimension is an order of magnitude smaller than the\nspatial resolutions. Further, the kernel calibration brings an increased model\ncapacity. We construct TAda2D and TAdaConvNeXt networks by replacing the 2D\nconvolutions in ResNet and ConvNeXt with TAdaConv, which leads to at least on\npar or better performance compared to state-of-the-art approaches on multiple\nvideo action recognition and localization benchmarks. We also demonstrate that\nas a readily plug-in operation with negligible computation overhead, TAdaConv\ncan effectively improve many existing video models with a convincing margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H. Ang Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes. (arXiv:2110.08059v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08059","description":"<p>When designing Convolutional Neural Networks (CNNs), one must select the\nsize\\break of the convolutional kernels before training. Recent works show CNNs\nbenefit from different kernel sizes at different layers, but exploring all\npossible combinations is unfeasible in practice. A more efficient approach is\nto learn the kernel size during training. However, existing works that learn\nthe kernel size have a limited bandwidth. These approaches scale kernels by\ndilation, and thus the detail they can describe is limited. In this work, we\npropose FlexConv, a novel convolutional operation with which high bandwidth\nconvolutional kernels of learnable kernel size can be learned at a fixed\nparameter cost. FlexNets model long-term dependencies without the use of\npooling, achieve state-of-the-art performance on several sequential datasets,\noutperform recent works with learned kernel sizes, and are competitive with\nmuch deeper ResNets on image benchmark datasets. Additionally, FlexNets can be\ndeployed at higher resolutions than those seen during training. To avoid\naliasing, we propose a novel kernel parameterization with which the frequency\nof the kernels can be analytically controlled. Our novel kernel\nparameterization shows higher descriptive power and faster convergence speed\nthan existing parameterizations. This leads to important improvements in\nclassification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1\">David W. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruintjes_R/0/1/0/all/0/1\">Robert-Jan Bruintjes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik J. Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1\">Mark Hoogendoorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08486","description":"<p>The ability to sequence unordered events is an essential skill to comprehend\nand reason about real world task procedures, which often requires thorough\nunderstanding of temporal common sense and multimodal information, as these\nprocedures are often communicated through a combination of texts and images.\nSuch capability is essential for applications such as sequential task planning\nand multi-source instruction summarization. While humans are capable of\nreasoning about and sequencing unordered multimodal procedural instructions,\nwhether current machine learning models have such essential capability is still\nan open question. In this work, we benchmark models' capability of reasoning\nover and sequencing unordered multimodal instructions by curating datasets from\npopular online instructional manuals and collecting comprehensive human\nannotations. We find models not only perform significantly worse than humans\nbut also seem incapable of efficiently utilizing the multimodal information. To\nimprove machines' performance on multimodal event sequencing, we propose\nsequentiality-aware pretraining techniques that exploit the sequential\nalignment properties of both texts and images, resulting in &gt; 5% significant\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alex Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1\">Pegah Alipoormolabashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1\">Marjorie Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph Weischedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporally stable video segmentation without video annotations. (arXiv:2110.08893v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08893","description":"<p>Temporally consistent dense video annotations are scarce and hard to collect.\nIn contrast, image segmentation datasets (and pre-trained models) are\nubiquitous, and easier to label for any novel task. In this paper, we introduce\na method to adapt still image segmentation models to video in an unsupervised\nmanner, by using an optical flow-based consistency measure. To ensure that the\ninferred segmented videos appear more stable in practice, we verify that the\nconsistency measure is well correlated with human judgement via a user study.\nTraining a new multi-input multi-output decoder using this measure as a loss,\ntogether with a technique for refining current image segmentation datasets and\na temporal weighted-guided filter, we observe stability improvements in the\ngenerated segmented videos with minimal loss of accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azulay_A/0/1/0/all/0/1\">Aharon Azulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halperin_T/0/1/0/all/0/1\">Tavi Halperin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vantzos_O/0/1/0/all/0/1\">Orestis Vantzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borenstein_N/0/1/0/all/0/1\">Nadav Borenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_O/0/1/0/all/0/1\">Ofir Bibi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConAM: Confidence Attention Module for Convolutional Neural Networks. (arXiv:2110.14369v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14369","description":"<p>The so-called \"attention\" is an efficient mechanism to improve the\nperformance of convolutional neural networks. It uses contextual information to\nrecalibrate the input to strengthen the propagation of informative features.\nHowever, the majority of the attention mechanisms only consider either local or\nglobal contextual information, which is singular to extract features. Moreover,\nmany existing mechanisms directly use the contextual information to recalibrate\nthe input, which unilaterally enhances the propagation of the informative\nfeatures, but does not suppress the useless ones. This paper proposes a new\nattention mechanism module based on the correlation between local and global\ncontextual information and we name this correlation as confidence. The novel\nattention mechanism extracts the local and global contextual information\nsimultaneously, and calculates the confidence between them, then uses this\nconfidence to recalibrate the input pixels. The extraction of local and global\ncontextual information increases the diversity of features. The recalibration\nwith confidence suppresses useless information while enhancing the informative\none with fewer parameters. We use CIFAR-10 and CIFAR-100 in our experiments and\nexplore the performance of our method's components by sufficient ablation\nstudies. Finally, we compare our method with a various state-of-the-art\nconvolutional neural networks and the results show that our method completely\nsurpasses these models. We implement ConAM with the Python library, Pytorch,\nand the code and models will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Ziming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neri_F/0/1/0/all/0/1\">Ferrante Neri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer for Classification of Breast Ultrasound Images. (arXiv:2110.14731v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14731","description":"<p>Medical ultrasound (US) imaging has become a prominent modality for breast\ncancer imaging due to its ease-of-use, low-cost and safety. In the past decade,\nconvolutional neural networks (CNNs) have emerged as the method of choice in\nvision applications and have shown excellent potential in automatic\nclassification of US images. Despite their success, their restricted local\nreceptive field limits their ability to learn global context information.\nRecently, Vision Transformer (ViT) designs that are based on self-attention\nbetween image patches have shown great potential to be an alternative to CNNs.\nIn this study, for the first time, we utilize ViT to classify breast US images\nusing different augmentation strategies. The results are provided as\nclassification accuracy and Area Under the Curve (AUC) metrics, and the\nperformance is compared with the state-of-the-art CNNs. The results indicate\nthat the ViT models have comparable efficiency with or even better than the\nCNNs in classification of US breast images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gheflati_B/0/1/0/all/0/1\">Behnaz Gheflati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDGNet: Class Distribution Guided Network for Human Parsing. (arXiv:2111.14173v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14173","description":"<p>The objective of human parsing is to partition a human in an image into\nconstituent parts. This task involves labeling each pixel of the human image\naccording to the classes. Since the human body comprises hierarchically\nstructured parts, each body part of an image can have its sole position\ndistribution characteristic. Probably, a human head is less likely to be under\nthe feet, and arms are more likely to be near the torso. Inspired by this\nobservation, we make instance class distributions by accumulating the original\nhuman parsing label in the horizontal and vertical directions, which can be\nutilized as supervision signals. Using these horizontal and vertical class\ndistribution labels, the network is guided to exploit the intrinsic position\ndistribution of each class. We combine two guided features to form a spatial\nguidance map, which is then superimposed onto the baseline network by\nmultiplication and concatenation to distinguish the human parts precisely. We\nconducted extensive experiments to demonstrate the effectiveness and\nsuperiority of our method on three well-known benchmarks: LIP, ATR, and CIHP\ndatabases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_O/0/1/0/all/0/1\">Ouk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleMesh: Style Transfer for Indoor 3D Scene Reconstructions. (arXiv:2112.01530v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01530","description":"<p>We apply style transfer on mesh reconstructions of indoor scenes. This\nenables VR applications like experiencing 3D environments painted in the style\nof a favorite artist. Style transfer typically operates on 2D images, making\nstylization of a mesh challenging. When optimized over a variety of poses,\nstylization patterns become stretched out and inconsistent in size. On the\nother hand, model-based 3D style transfer methods exist that allow stylization\nfrom a sparse set of images, but they require a network at inference time. To\nthis end, we optimize an explicit texture for the reconstructed mesh of a scene\nand stylize it jointly from all available input images. Our depth- and\nangle-aware optimization leverages surface normal and depth data of the\nunderlying mesh to create a uniform and consistent stylization for the whole\nscene. Our experiments show that our method creates sharp and detailed results\nfor the complete scene without view-dependent artifacts. Through extensive\nablation studies, we show that the proposed 3D awareness enables style transfer\nto be applied to the 3D domain of a mesh. Our method can be used to render a\nstylized mesh in real-time with traditional rendering pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hollein_L/0/1/0/all/0/1\">Lukas H&#xf6;llein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceFormer: Speech-Driven 3D Facial Animation with Transformers. (arXiv:2112.05329v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05329","description":"<p>Speech-driven 3D facial animation is challenging due to the complex geometry\nof human faces and the limited availability of 3D audio-visual data. Prior\nworks typically focus on learning phoneme-level features of short audio windows\nwith limited context, occasionally resulting in inaccurate lip movements. To\ntackle this limitation, we propose a Transformer-based autoregressive model,\nFaceFormer, which encodes the long-term audio context and autoregressively\npredicts a sequence of animated 3D face meshes. To cope with the data scarcity\nissue, we integrate the self-supervised pre-trained speech representations.\nAlso, we devise two biased attention mechanisms well suited to this specific\ntask, including the biased cross-modal multi-head (MH) attention and the biased\ncausal MH self-attention with a periodic positional encoding strategy. The\nformer effectively aligns the audio-motion modalities, whereas the latter\noffers abilities to generalize to longer audio sequences. Extensive experiments\nand a perceptual user study show that our approach outperforms the existing\nstate-of-the-arts. The code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yingruo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09081","description":"<p>We present a visual localization system that learns to estimate camera poses\nin the real world with the help of synthetic data. Despite significant progress\nin recent years, most learning-based approaches to visual localization target\nat a single domain and require a dense database of geo-tagged images to\nfunction well. To mitigate the data scarcity issue and improve the scalability\nof the neural localization models, we introduce TOPO-DataGen, a versatile\nsynthetic data generation tool that traverses smoothly between the real and\nvirtual world, hinged on the geographic camera viewpoint. New large-scale\nsim-to-real benchmark datasets are proposed to showcase and evaluate the\nutility of the said synthetic data. Our experiments reveal that synthetic data\ngenerically enhances the neural network performance on real data. Furthermore,\nwe introduce CrossLoc, a cross-modal visual representation learning approach to\npose estimation that makes full use of the scene coordinate ground truth via\nself-supervision. Without any extra data, CrossLoc significantly outperforms\nthe state-of-the-art methods and achieves substantially higher real-data sample\nefficiency. Our code and datasets are all available at\nhttps://github.com/TOPO-EPFL/CrossLoc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reding_S/0/1/0/all/0/1\">Simon Reding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doytchinov_I/0/1/0/all/0/1\">Iordan Doytchinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Visual Tracking with Exemplar Transformers. (arXiv:2112.09686v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09686","description":"<p>The design of more complex and powerful neural network models has\nsignificantly advanced the state-of-the-art in visual object tracking. These\nadvances can be attributed to deeper networks, or to the introduction of new\nbuilding blocks, such as transformers. However, in the pursuit of increased\ntracking performance, efficient tracking architectures have received\nsurprisingly little attention. In this paper, we introduce the Exemplar\nTransformer, an efficient transformer for real-time visual object tracking.\nE.T.Track, our visual tracker that incorporates Exemplar Transformer layers,\nruns at 47 fps on a CPU. This is up to 8 times faster than other\ntransformer-based models, making it the only real-time transformer-based\ntracker. When compared to lightweight trackers that can operate in real-time on\nstandard CPUs, E.T.Track consistently outperforms all other methods on the\nLaSOT, OTB-100, NFS, TrackingNet and VOT-ST2020 datasets. The code will soon be\nreleased on https://github.com/visionml/pytracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blatter_P/0/1/0/all/0/1\">Philippe Blatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization. (arXiv:2112.11177v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11177","description":"<p>For medical image segmentation, imagine if a model was only trained using MR\nimages in source domain, how about its performance to directly segment CT\nimages in target domain? This setting, namely generalizable cross-modality\nsegmentation, owning its clinical potential, is much more challenging than\nother related settings, e.g., domain adaptation. To achieve this goal, we in\nthis paper propose a novel dual-normalization model by leveraging the augmented\nsource-similar and source-dissimilar images during our generalizable\nsegmentation. To be specific, given a single source domain, aiming to simulate\nthe possible appearance change in unseen target domains, we first utilize a\nnonlinear transformation to augment source-similar and source-dissimilar\nimages. Then, to sufficiently exploit these two types of augmentations, our\nproposed dual-normalization based model employs a shared backbone yet\nindependent batch normalization layer for separate normalization. Afterward, we\nput forward a style-based selection scheme to automatically choose the\nappropriate path in the test stage. Extensive experiments on three publicly\navailable datasets, i.e., BraTS, Cross-Modality Cardiac, and Abdominal\nMulti-Organ datasets, have demonstrated that our method outperforms other\nstate-of-the-art domain generalization methods. Code is available at\nhttps://github.com/zzzqzhou/Dual-Normalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FourierMask: Instance Segmentation using Fourier Mapping in Implicit Neural Networks. (arXiv:2112.12535v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12535","description":"<p>We present FourierMask, which employs Fourier series combined with implicit\nneural representations to generate instance segmentation masks. We apply a\nFourier mapping (FM) to the coordinate locations and utilize the mapped\nfeatures as inputs to an implicit representation (coordinate-based multi-layer\nperceptron (MLP)). FourierMask learns to predict the coefficients of the FM for\na particular instance, and therefore adapts the FM to a specific object. This\nallows FourierMask to be generalized to predict instance segmentation masks\nfrom natural images. Since implicit functions are continuous in the domain of\ninput coordinates, we illustrate that by sub-sampling the input pixel\ncoordinates, we can generate higher resolution masks during inference.\nFurthermore, we train a renderer MLP (FourierRend) on the uncertain predictions\nof FourierMask and illustrate that it significantly improves the quality of the\nmasks. FourierMask shows competitive results on the MS COCO dataset compared to\nthe baseline Mask R-CNN at the same output resolution and surpasses it on\nhigher resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riaz_H/0/1/0/all/0/1\">Hamd ul Moqeet Riaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1\">Nuri Benbarka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefer_T/0/1/0/all/0/1\">Timon Hoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeMask: Semantically Masked Transformers for Semantic Segmentation. (arXiv:2112.12782v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12782","description":"<p>Finetuning a pretrained backbone in the encoder part of an image transformer\nnetwork has been the traditional approach for the semantic segmentation task.\nHowever, such an approach leaves out the semantic context that an image\nprovides during the encoding stage. This paper argues that incorporating\nsemantic information of the image into pretrained hierarchical\ntransformer-based backbones while finetuning improves the performance\nconsiderably. To achieve this, we propose SeMask, a simple and effective\nframework that incorporates semantic information into the encoder with the help\nof a semantic attention operation. In addition, we use a lightweight semantic\ndecoder during training to provide supervision to the intermediate semantic\nprior maps at every stage. Our experiments demonstrate that incorporating\nsemantic priors enhances the performance of the established hierarchical\nencoders with a slight increase in the number of FLOPs. We provide empirical\nproof by integrating SeMask into Swin Transformer and Mix Transformer backbones\nas our encoder paired with different decoders. Our framework achieves a new\nstate-of-the-art of 58.22% mIoU on the ADE20K dataset and improvements of over\n3% in the mIoU metric on the Cityscapes dataset. The code and checkpoints are\npublicly available at\nhttps://github.com/Picsart-AI-Research/SeMask-Segmentation .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_J/0/1/0/all/0/1\">Jitesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anukriti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_N/0/1/0/all/0/1\">Nikita Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zilong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.04584","description":"<p>Automatic segmentation of lung lesions associated with COVID-19 in CT images\nrequires large amount of annotated volumes. Annotations mandate expert\nknowledge and are time-intensive to obtain through fully manual segmentation\nmethods. Additionally, lung lesions have large inter-patient variations, with\nsome pathologies having similar visual appearance as healthy lung tissues. This\nposes a challenge when applying existing semi-automatic interactive\nsegmentation techniques for data labelling. To address these challenges, we\npropose an efficient convolutional neural networks (CNNs) that can be learned\nonline while the annotator provides scribble-based interaction. To accelerate\nlearning from only the samples labelled through user-interactions, a\npatch-based approach is used for training the network. Moreover, we use\nweighted cross-entropy loss to address the class imbalance that may result from\nuser-interactions. During online inference, the learned network is applied to\nthe whole input volume using a fully convolutional approach. We compare our\nproposed method with state-of-the-art using synthetic scribbles and show that\nit outperforms existing methods on the task of annotating lung lesions\nassociated with COVID-19, achieving 16% higher Dice score while reducing\nexecution time by 3$\\times$ and requiring 9000 lesser scribbles-based labelled\nvoxels. Due to the online learning aspect, our approach adapts quickly to user\ninput, resulting in high quality segmentation labels. Source code for ECONet is\navailable at: https://github.com/masadcv/ECONet-MONAILabel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asad_M/0/1/0/all/0/1\">Muhammad Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Video-text Retrieval with Multiple Choice Questions. (arXiv:2201.04850v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04850","description":"<p>Pre-training a model to learn transferable video-text representation for\nretrieval has attracted a lot of attention in recent years. Previous dominant\nworks mainly adopt two separate encoders for efficient retrieval, but ignore\nlocal associations between videos and texts. Another line of research uses a\njoint encoder to interact video with texts, but results in low efficiency since\neach text-video pair needs to be fed into the model. In this work, we enable\nfine-grained video-text interactions while maintaining high efficiency for\nretrieval via a novel pretext task, dubbed as Multiple Choice Questions (MCQ),\nwhere a parametric module BridgeFormer is trained to answer the \"questions\"\nconstructed by the text features via resorting to the video features.\nSpecifically, we exploit the rich semantics of text (i.e., nouns and verbs) to\nbuild questions, with which the video encoder can be trained to capture more\nregional content and temporal dynamics. In the form of questions and answers,\nthe semantic associations between local video-text features can be properly\nestablished. BridgeFormer is able to be removed for downstream retrieval,\nrendering an efficient and flexible model with only two encoders. Our method\noutperforms state-of-the-art methods on the popular text-to-video retrieval\ntask in five datasets with different experimental setups (i.e., zero-shot and\nfine-tune), including HowTo100M (one million videos). We further conduct\nzero-shot action recognition, which can be cast as video-to-text retrieval, and\nour approach also significantly surpasses its counterparts. As an additional\nbenefit, our method achieves competitive results with much shorter pre-training\nvideos on single-modality downstream tasks, e.g., action recognition with\nlinear evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yuying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xihui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Pseudo Label Quality for Semi-Supervised Domain-Generalized Medical Image Segmentation. (arXiv:2201.08657v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08657","description":"<p>Generalizing the medical image segmentation algorithms to unseen domains is\nan important research topic for computer-aided diagnosis and surgery. Most\nexisting methods require a fully labeled dataset in each source domain.\nAlthough some researchers developed a semi-supervised domain generalized\nmethod, it still requires the domain labels. This paper presents a novel\nconfidence-aware cross pseudo supervision algorithm for semi-supervised domain\ngeneralized medical image segmentation. The main goal is to enhance the pseudo\nlabel quality for unlabeled images from unknown distributions. To achieve it,\nwe perform the Fourier transformation to learn low-level statistic information\nacross domains and augment the images to incorporate cross-domain information.\nWith these augmentations as perturbations, we feed the input to a\nconfidence-aware cross pseudo supervision network to measure the variance of\npseudo labels and regularize the network to learn with more confident pseudo\nlabels. Our method sets new records on public datasets, i.e., M&amp;Ms and SCGM.\nNotably, without using domain labels, our method surpasses the prior art that\neven uses domain labels by 11.67% on Dice on M&amp;Ms dataset with 2% labeled data.\nCode is available at https://github.com/XMed-Lab/EPL_SemiDG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huifeng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds. (arXiv:2201.12769v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12769","description":"<p>Semantic segmentation of 3D point cloud is an essential task for autonomous\ndriving environment perception. The pipeline of most pointwise point cloud\nsemantic segmentation methods includes points sampling, neighbor searching,\nfeature aggregation, and classification. Neighbor searching method like\nK-nearest neighbors algorithm, KNN, has been widely applied. However, the\ncomplexity of KNN is always a bottleneck of efficiency. In this paper, we\npropose an end-to-end neural architecture, Multiple View Pointwise Net,\nMVP-Net, to efficiently and directly infer large-scale outdoor point cloud\nwithout KNN or any complex pre/postprocessing. Instead, assumption-based space\nfilling curves and multi-rotation of point cloud methods are introduced to\npoint feature aggregation and receptive field expanding. Numerical experiments\nshow that the proposed MVP-Net is 11 times faster than the most efficient\npointwise semantic segmentation method RandLA-Net and achieves the same\naccuracy on the large-scale benchmark SemanticKITTI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chuanyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Nuo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Han Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shengguang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation. (arXiv:2202.07191v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07191","description":"<p>With rising male infertility, sperm head morphology classification becomes\ncritical for accurate and timely clinical diagnosis. Recent deep learning (DL)\nmorphology analysis methods achieve promising benchmark results, but leave\nperformance and robustness on the table by relying on limited and possibly\nnoisy class labels. To address this, we introduce a new DL training framework\nthat leverages anatomical and image priors from human sperm microscopy crops to\nextract useful features without additional labeling cost. Our core idea is to\ndistill sperm head information with reliably-generated pseudo-masks and\nunsupervised spatial prediction tasks. The predicted foreground masks from this\ndistillation step are then leveraged to regularize and reduce image and label\nnoise in the tuning stage. We evaluate our new approach on two public sperm\ndatasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy\nand 96.5% HuSHeM accuracy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yejia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingjing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_X/0/1/0/all/0/1\">Xiaomin Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiru Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunxia Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danny Z. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDAM: Heuristic Difference Attention Module for Convolutional Neural Networks. (arXiv:2202.09556v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09556","description":"<p>The attention mechanism is one of the most important priori knowledge to\nenhance convolutional neural networks. Most attention mechanisms are bound to\nthe convolutional layer and use local or global contextual information to\nrecalibrate the input. This is a popular attention strategy design method.\nGlobal contextual information helps the network to consider the overall\ndistribution, while local contextual information is more general. The\ncontextual information makes the network pay attention to the mean or maximum\nvalue of a particular receptive field. Different from the most attention\nmechanism, this article proposes a novel attention mechanism with the heuristic\ndifference attention module, HDAM. HDAM's input recalibration is based on the\ndifference between the local and global contextual information instead of the\nmean and maximum values. At the same time, to make different layers have a more\nsuitable local receptive field size and increase the exibility of the local\nreceptive field design, we use genetic algorithm to heuristically produce local\nreceptive fields. First, HDAM extracts the mean value of the global and local\nreceptive fields as the corresponding contextual information. Then the\ndifference between the global and local contextual information is calculated.\nFinally HDAM uses this difference to recalibrate the input. In addition, we use\nthe heuristic ability of genetic algorithm to search for the local receptive\nfield size of each layer. Our experiments on CIFAR-10 and CIFAR-100 show that\nHDAM can use fewer parameters than other attention mechanisms to achieve higher\naccuracy. We implement HDAM with the Python library, Pytorch, and the code and\nmodels will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Ziming Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computing Multiple Image Reconstructions with a Single Hypernetwork. (arXiv:2202.11009v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11009","description":"<p>Deep learning based techniques achieve state-of-the-art results in a wide\nrange of image reconstruction tasks like compressed sensing. These methods\nalmost always have hyperparameters, such as the weight coefficients that\nbalance the different terms in the optimized loss function. The typical\napproach is to train the model for a hyperparameter setting determined with\nsome empirical or theoretical justification. Thus, at inference time, the model\ncan only compute reconstructions corresponding to the pre-determined\nhyperparameter values. In this work, we present a hypernetwork-based approach,\ncalled HyperRecon, to train reconstruction models that are agnostic to\nhyperparameter settings. At inference time, HyperRecon can efficiently produce\ndiverse reconstructions, which would each correspond to different\nhyperparameter values. In this framework, the user is empowered to select the\nmost useful output(s) based on their own judgement. We demonstrate our method\nin compressed sensing, super-resolution and denoising tasks, using two\nlarge-scale and publicly-available MRI datasets. Our code is available at\nhttps://github.com/alanqrwang/hyperrecon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alan Q. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Adaptive SCEne Tracing. (arXiv:2202.13664v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13664","description":"<p>Neural rendering with implicit neural networks has recently emerged as an\nattractive proposition for scene reconstruction, achieving excellent quality\nalbeit at high computational cost. While the most recent generation of such\nmethods has made progress on the rendering (inference) times, very little\nprogress has been made on improving the reconstruction (training) times. In\nthis work, we present Neural Adaptive Scene Tracing (NAScenT), the first neural\nrendering method based on directly training a hybrid explicit-implicit neural\nrepresentation. NAScenT uses a hierarchical octree representation with one\nneural network per leaf node and combines this representation with a two-stage\nsampling process that concentrates ray samples where they matter most near\nobject surfaces. As a result, NAScenT is capable of reconstructing challenging\nscenes including both large, sparsely populated volumes like UAV captured\noutdoor environments, as well as small scenes with high geometric complexity.\nNAScenT outperforms existing neural rendering approaches in terms of both\nquality and training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckert_D/0/1/0/all/0/1\">Darius R&#xfc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idoughi_R/0/1/0/all/0/1\">Ramzi Idoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Local Feature Learning for 3D Point Cloud Processing using Unary-Pairwise Attention. (arXiv:2203.00172v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00172","description":"<p>We present a simple but effective attention named the unary-pairwise\nattention (UPA) for modeling the relationship between 3D point clouds. Our idea\nis motivated by the analysis that the standard self-attention (SA) that\noperates globally tends to produce almost the same attention maps for different\nquery positions, revealing difficulties for learning query-independent and\nquery-dependent information jointly. Therefore, we reformulate the SA and\npropose query-independent (Unary) and query-dependent (Pairwise) components to\nfacilitate the learning of both terms. In contrast to the SA, the UPA ensures\nquery dependence via operating locally. Extensive experiments show that the UPA\noutperforms the SA consistently on various point cloud understanding tasks\nincluding shape classification, part segmentation, and scene segmentation.\nMoreover, simply equipping the popular PointNet++ method with the UPA even\noutperforms or is on par with the state-of-the-art attention-based approaches.\nIn addition, the UPA systematically boosts the performance of both standard and\nmodern networks when it is integrated into them as a compositional module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiu_H/0/1/0/all/0/1\">Haoyi Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyoung-Sook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinohara_T/0/1/0/all/0/1\">Takayuki Shinohara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qiong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuoka_M/0/1/0/all/0/1\">Masashi Matsuoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement. (arXiv:2203.03622v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03622","description":"<p>A stroke occurs when an artery in the brain ruptures and bleeds or when the\nblood supply to the brain is cut off. Blood and oxygen cannot reach the brain's\ntissues due to the rupture or obstruction resulting in tissue death. The Middle\ncerebral artery (MCA) is the largest cerebral artery and the most commonly\ndamaged vessel in stroke. The quick onset of a focused neurological deficit\ncaused by interruption of blood flow in the territory supplied by the MCA is\nknown as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is\nused to estimate the extent of early ischemic changes in patients with MCA\nstroke. This study proposes a deep learning-based method to score the CT scan\nfor ASPECTS. Our work has three highlights. First, we propose a novel method\nfor medical image segmentation for stroke detection. Second, we show the\neffectiveness of AI solution for fully-automated ASPECT scoring with reduced\ndiagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a\ndice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72\nfor the infarcts segmentation. Lastly, we show that our model's performance is\ninline with inter-reader variability between radiologists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Ujjwal Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ranjan_M/0/1/0/all/0/1\">Mukul Ranjan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golla_S/0/1/0/all/0/1\">Satish Golla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanamala_S/0/1/0/all/0/1\">Swetha Tanamala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sreenivas_P/0/1/0/all/0/1\">Preetham Sreenivas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chilamkurthy_S/0/1/0/all/0/1\">Sasank Chilamkurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pandian_J/0/1/0/all/0/1\">Jeyaraj Pandian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tarpley_J/0/1/0/all/0/1\">Jason Tarpley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN. (arXiv:2203.04036v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04036","description":"<p>One-shot talking face generation aims at synthesizing a high-quality talking\nface video from an arbitrary portrait image, driven by a video or an audio\nsegment. One challenging quality factor is the resolution of the output video:\nhigher resolution conveys more details. In this work, we investigate the latent\nfeature space of a pre-trained StyleGAN and discover some excellent spatial\ntransformation properties. Upon the observation, we explore the possibility of\nusing a pre-trained StyleGAN to break through the resolution limit of training\ndatasets. We propose a novel unified framework based on a pre-trained StyleGAN\nthat enables a set of powerful functionalities, i.e., high-resolution video\ngeneration, disentangled control by driving video or audio, and flexible face\nediting. Our framework elevates the resolution of the synthesized talking face\nto 1024*1024 for the first time, even though the training dataset has a lower\nresolution. We design a video-based motion generation module and an audio-based\none, which can be plugged into the framework either individually or jointly to\ndrive the video generation. The predicted motion is used to transform the\nlatent features of StyleGAN for visual animation. To compensate for the\ntransformation distortion, we propose a calibration network as well as a domain\nloss to refine the features. Moreover, our framework allows two types of facial\nediting, i.e., global editing via GAN inversion and intuitive editing based on\n3D morphable models. Comprehensive experiments show superior video quality,\nflexible controllability, and editability over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1\">Xiaodong Cun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Qingyan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P2M: A Processing-in-Pixel-in-Memory Paradigm for Resource-Constrained TinyML Applications. (arXiv:2203.04737v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.04737","description":"<p>The demand to process vast amounts of data generated from state-of-the-art\nhigh resolution cameras has motivated novel energy-efficient on-device AI\nsolutions. Visual data in such cameras are usually captured in the form of\nanalog voltages by a sensor pixel array, and then converted to the digital\ndomain for subsequent AI processing using analog-to-digital converters (ADC).\nRecent research has tried to take advantage of massively parallel low-power\nanalog/digital computing in the form of near- and in-sensor processing, in\nwhich the AI computation is performed partly in the periphery of the pixel\narray and partly in a separate on-board CPU/accelerator. Unfortunately,\nhigh-resolution input images still need to be streamed between the camera and\nthe AI processing unit, frame by frame, causing energy, bandwidth, and security\nbottlenecks. To mitigate this problem, we propose a novel\nProcessing-in-Pixel-in-memory (P2M) paradigm, that customizes the pixel array\nby adding support for analog multi-channel, multi-bit convolution, batch\nnormalization, and ReLU (Rectified Linear Units). Our solution includes a\nholistic algorithm-circuit co-design approach and the resulting P2M paradigm\ncan be used as a drop-in replacement for embedding memory-intensive first few\nlayers of convolutional neural network (CNN) models within\nfoundry-manufacturable CMOS image sensor platforms. Our experimental results\nindicate that P2M reduces data transfer bandwidth from sensors and analog to\ndigital conversions by ~21x, and the energy-delay product (EDP) incurred in\nprocessing a MobileNetV2 model on a TinyML use case for visual wake words\ndataset (VWW) by up to ~11x compared to standard near-processing or in-sensor\nimplementations, without any significant drop in test accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1\">Gourav Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zihan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkireddy_R/0/1/0/all/0/1\">Ravi Teja Lakkireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathai_J/0/1/0/all/0/1\">Joe Mathai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_A/0/1/0/all/0/1\">Ajey Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Akhilesh R. Jaiswal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Transformer for Hyperspectral Image Classification. (arXiv:2203.04771v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04771","description":"<p>Hyperspectral images (HSI) not only have a broad macroscopic field of view\nbut also contain rich spectral information, and the types of surface objects\ncan be identified through spectral information, which is one of the main\napplications in hyperspectral image related research.In recent years, more and\nmore deep learning methods have been proposed, among which convolutional neural\nnetworks (CNN) are the most influential. However, CNN-based methods are\ndifficult to capture long-range dependencies, and also require a large amount\nof labeled data for model training.Besides, most of the self-supervised\ntraining methods in the field of HSI classification are based on the\nreconstruction of input samples, and it is difficult to achieve effective use\nof unlabeled samples. To address the shortcomings of CNN networks, we propose a\nnoval multi-scale convolutional embedding module for HSI to realize effective\nextraction of spatial-spectral information, which can be better combined with\nTransformer network.In order to make more efficient use of unlabeled data, we\npropose a new self-supervised pretask. Similar to Mask autoencoder, but our\npre-training method only masks the corresponding token of the central pixel in\nthe encoder, and inputs the remaining token into the decoder to reconstruct the\nspectral information of the central pixel.Such a pretask can better model the\nrelationship between the central feature and the domain feature, and obtain\nmore stable training results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalisation for Object Detection. (arXiv:2203.05294v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05294","description":"<p>Domain generalisation aims to promote the learning of domain-invariant\nfeatures while suppressing domain specific features, so that a model can\ngeneralise well on previously unseen target domains. This paper studies domain\ngeneralisation in the object detection setting. We propose new terms for\nhandling both the bounding box detector and domain belonging, and incorporate\nthem with consistency regularisation. This allows us to learn a domain agnostic\nfeature representation for object detection, applicable to the problem of\ndomain generalisation. The proposed approach is evaluated using four standard\nobject detection datasets with available domain metadata, namely GWHD,\nCityscapes, BDD100K, Sim10K and exhibits consistently superior generalisation\nperformance over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seemakurthy_K/0/1/0/all/0/1\">Karthik Seemakurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_C/0/1/0/all/0/1\">Charles Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aptoula_E/0/1/0/all/0/1\">Erchan Aptoula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosilj_P/0/1/0/all/0/1\">Petra Bosilj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Bi-directional Skip Connections in Encoder-Decoder Architectures and Beyond. (arXiv:2203.05709v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05709","description":"<p>U-Net, as an encoder-decoder architecture with forward skip connections, has\nachieved promising results in various medical image analysis tasks. Many recent\napproaches have also extended U-Net with more complex building blocks, which\ntypically increase the number of network parameters considerably. Such\ncomplexity makes the inference stage highly inefficient for clinical\napplications. Towards an effective yet economic segmentation network design, in\nthis work, we propose backward skip connections that bring decoded features\nback to the encoder. Our design can be jointly adopted with forward skip\nconnections in any encoder-decoder architecture forming a recurrence structure\nwithout introducing extra parameters. With the backward skip connections, we\npropose a U-Net based network family, namely Bi-directional O-shape networks,\nwhich set new benchmarks on multiple public medical imaging segmentation\ndatasets. On the other hand, with the most plain architecture (BiO-Net),\nnetwork computations inevitably increase along with the pre-set recurrence\ntime. We have thus studied the deficiency bottleneck of such recurrent design\nand propose a novel two-phase Neural Architecture Search (NAS) algorithm,\nnamely BiX-NAS, to search for the best multi-scale bi-directional skip\nconnections. The ineffective skip connections are then discarded to reduce\ncomputational costs and speed up network inference. The finally searched\nBiX-Net yields the least network complexity and outperforms other\nstate-of-the-art counterparts by large margins. We evaluate our methods on both\n2D and 3D segmentation tasks in a total of six datasets. Extensive ablation\nstudies have also been conducted to provide a comprehensive analysis for our\nproposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Protocol Matters: Towards Accurate Scene Text Recognition via Training Protocol Searching. (arXiv:2203.06696v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06696","description":"<p>The development of scene text recognition (STR) in the era of deep learning\nhas been mainly focused on novel architectures of STR models. However, training\nprotocol (i.e., settings of the hyper-parameters involved in the training of\nSTR models), which plays an equally important role in successfully training a\ngood STR model, is under-explored for scene text recognition. In this work, we\nattempt to improve the accuracy of existing STR models by searching for optimal\ntraining protocol. Specifically, we develop a training protocol search\nalgorithm, based on a newly designed search space and an efficient search\nalgorithm using evolutionary optimization and proxy tasks. Experimental results\nshow that our searched training protocol can improve the recognition accuracy\nof mainstream STR models by 2.7%~3.9%. In particular, with the searched\ntraining protocol, TRBA-Net achieves 2.1% higher accuracy than the\nstate-of-the-art STR model (i.e., EFIFSTR), while the inference speed is 2.3x\nand 3.7x faster on CPU and GPU respectively. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed method and the\ngeneralization ability of the training protocol found by our search method.\nCode is available at https://github.com/VDIGPKU/STR_TPSearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06717","description":"<p>We revisit large kernel design in modern convolutional neural networks\n(CNNs). Inspired by recent advances of vision transformers (ViTs), in this\npaper, we demonstrate that using a few large convolutional kernels instead of a\nstack of small kernels could be a more powerful paradigm. We suggested five\nguidelines, e.g., applying re-parameterized large depth-wise convolutions, to\ndesign efficient high-performance large-kernel CNNs. Following the guidelines,\nwe propose RepLKNet, a pure CNN architecture whose kernel size is as large as\n31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the\nperformance gap between CNNs and ViTs, e.g., achieving comparable or superior\nresults than Swin Transformer on ImageNet and a few typical downstream tasks,\nwith lower latency. RepLKNet also shows nice scalability to big data and large\nmodels, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K,\nwhich is very competitive among the state-of-the-arts with similar model sizes.\nOur study further reveals that, in contrast to small-kernel CNNs, large-kernel\nCNNs have much larger effective receptive fields, and higher shape bias rather\nthan texture bias. Code &amp; models at\nhttps://github.com/megvii-research/RepLKNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yizhuang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimMatch: Semi-supervised Learning with Similarity Matching. (arXiv:2203.06915v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06915","description":"<p>Learning with few labeled data has been a longstanding problem in the\ncomputer vision and machine learning research community. In this paper, we\nintroduced a new semi-supervised learning framework, SimMatch, which\nsimultaneously considers semantic similarity and instance similarity. In\nSimMatch, the consistency regularization will be applied on both semantic-level\nand instance-level. The different augmented views of the same instance are\nencouraged to have the same class prediction and similar similarity\nrelationship respected to other instances. Next, we instantiated a labeled\nmemory buffer to fully leverage the ground truth labels on instance-level and\nbridge the gaps between the semantic and instance similarities. Finally, we\nproposed the \\textit{unfolding} and \\textit{aggregation} operation which allows\nthese two similarities be isomorphically transformed with each other. In this\nway, the semantic and instance pseudo-labels can be mutually propagated to\ngenerate more high-quality and reliable matching targets. Extensive\nexperimental results demonstrate that SimMatch improves the performance of\nsemi-supervised learning tasks across different benchmark datasets and\ndifferent settings. Notably, with 400 epochs of training, SimMatch achieves\n67.2\\%, and 74.4\\% Top-1 Accuracy with 1\\% and 10\\% labeled examples on\nImageNet, which significantly outperforms the baseline methods and is better\nthan previous semi-supervised learning frameworks. Code and pre-trained models\nare available at https://github.com/KyleZheng1997/simmatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting associations and meanings of objects depicted in artworks through bi-modal deep networks. (arXiv:2203.07026v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07026","description":"<p>We present a novel bi-modal system based on deep networks to address the\nproblem of learning associations and simple meanings of objects depicted in\n\"authored\" images, such as fine art paintings and drawings. Our overall system\nprocesses both the images and associated texts in order to learn associations\nbetween images of individual objects, their identities and the abstract\nmeanings they signify. Unlike past deep nets that describe depicted objects and\ninfer predicates, our system identifies meaning-bearing objects (\"signifiers\")\nand their associations (\"signifieds\") as well as basic overall meanings for\ntarget artworks. Our system had precision of 48% and recall of 78% with an F1\nmetric of 0.6 on a curated set of Dutch vanitas paintings, a genre celebrated\nfor its concentration on conveying a meaning of great import at the time of\ntheir execution. We developed and tested our system on fine art paintings but\nour general methods can be applied to other authored images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kell_G/0/1/0/all/0/1\">Gregory Kell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1\">Ryan-Rhys Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourached_A/0/1/0/all/0/1\">Anthony Bourached</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stork_D/0/1/0/all/0/1\">David G. Stork</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Non-Rigid 3D Registration. (arXiv:2203.07858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07858","description":"<p>Non-rigid registration computes an alignment between a source surface with a\ntarget surface in a non-rigid manner. In the past decade, with the advances in\n3D sensing technologies that can measure time-varying surfaces, non-rigid\nregistration has been applied for the acquisition of deformable shapes and has\na wide range of applications. This survey presents a comprehensive review of\nnon-rigid registration methods for 3D shapes, focusing on techniques related to\ndynamic shape acquisition and reconstruction. In particular, we review\ndifferent approaches for representing the deformation field, and the methods\nfor computing the desired deformation. Both optimization-based and\nlearning-based methods are covered. We also review benchmarks and datasets for\nevaluating non-rigid registration methods, and discuss potential future\nresearch directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuxin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyke_R/0/1/0/all/0/1\">Roberto M. Dyke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting. (arXiv:2203.07918v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07918","description":"<p>While 6D object pose estimation has recently made a huge leap forward, most\nmethods can still only handle a single or a handful of different objects, which\nlimits their applications. To circumvent this problem, category-level object\npose estimation has recently been revamped, which aims at predicting the 6D\npose as well as the 3D metric size for previously unseen instances from a given\nset of object classes. This is, however, a much more challenging task due to\nsevere intra-class shape variations. To address this issue, we propose\nGPV-Pose, a novel framework for robust category-level pose estimation,\nharnessing geometric insights to enhance the learning of category-level\npose-sensitive features. First, we introduce a decoupled confidence-driven\nrotation representation, which allows geometry-aware recovery of the associated\nrotation matrix. Second, we propose a novel geometry-guided point-wise voting\nparadigm for robust retrieval of the 3D object bounding box. Finally,\nleveraging these different output streams, we can enforce several geometric\nconsistency terms, further increasing performance, especially for non-symmetric\ncategories. GPV-Pose produces superior results to state-of-the-art competitors\non common public benchmarks, whilst almost achieving real-time inference speed\nat 20 FPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1\">Yan Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruida Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Z/0/1/0/all/0/1\">Zhiqiang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1\">Fabian Manhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsic Neural Fields: Learning Functions on Manifolds. (arXiv:2203.07967v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07967","description":"<p>Neural fields have gained significant attention in the computer vision\ncommunity due to their excellent performance in novel view synthesis, geometry\nreconstruction, and generative modeling. Some of their advantages are a sound\ntheoretic foundation and an easy implementation in current deep learning\nframeworks. While neural fields have been applied to signals on manifolds,\ne.g., for texture reconstruction, their representation has been limited to\nextrinsically embedding the shape into Euclidean space. The extrinsic embedding\nignores known intrinsic manifold properties and is inflexible wrt. transfer of\nthe learned function. To overcome these limitations, this work introduces\nintrinsic neural fields, a novel and versatile representation for neural fields\non manifolds. Intrinsic neural fields combine the advantages of neural fields\nwith the spectral properties of the Laplace-Beltrami operator. We show\ntheoretically that intrinsic neural fields inherit many desirable properties of\nthe extrinsic neural field framework but exhibit additional intrinsic\nqualities, like isometry invariance. In experiments, we show intrinsic neural\nfields can reconstruct high-fidelity textures from images with state-of-the-art\nquality and are robust to the discretization of the underlying manifold. We\ndemonstrate the versatility of intrinsic neural fields by tackling various\napplications: texture transfer between deformed shapes &amp; different shapes,\ntexture reconstruction from real-world images with view dependence, and\ndiscretization-agnostic learning on meshes and point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koestler_L/0/1/0/all/0/1\">Lukas Koestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grittner_D/0/1/0/all/0/1\">Daniel Grittner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1\">Zorah L&#xe4;hner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motif Mining: Finding and Summarizing Remixed Image Content. (arXiv:2203.08327v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08327","description":"<p>On the internet, images are no longer static; they have become dynamic\ncontent. Thanks to the availability of smartphones with cameras and easy-to-use\nediting software, images can be remixed (i.e., redacted, edited, and recombined\nwith other content) on-the-fly and with a world-wide audience that can repeat\nthe process. From digital art to memes, the evolution of images through time is\nnow an important topic of study for digital humanists, social scientists, and\nmedia forensics specialists. However, because typical data sets in computer\nvision are composed of static content, the development of automated algorithms\nto analyze remixed content has been limited. In this paper, we introduce the\nidea of Motif Mining - the process of finding and summarizing remixed image\ncontent in large collections of unlabeled and unsorted data. In this paper,\nthis idea is formalized and a reference implementation is introduced.\nExperiments are conducted on three meme-style data sets, including a newly\ncollected set associated with the information war in the Russo-Ukrainian\nconflict. The proposed motif mining approach is able to identify related\nremixed content that, when compared to similar approaches, more closely aligns\nwith the preferences and expectations of human observers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theisen_W/0/1/0/all/0/1\">William Theisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cedre_D/0/1/0/all/0/1\">Daniel Gonzalez Cedre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1\">Zachariah Carmichael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_D/0/1/0/all/0/1\">Daniel Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weninger_T/0/1/0/all/0/1\">Tim Weninger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter Scheirer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Efficient 3D Learner via Knowledge Transferred from 2D Model. (arXiv:2203.08479v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08479","description":"<p>Collecting and labeling the registered 3D point cloud is costly. As a result,\n3D resources for training are typically limited in quantity compared to the 2D\nimages counterpart. In this work, we deal with the data scarcity challenge of\n3D tasks by transferring knowledge from strong 2D models via RGB-D images.\nSpecifically, we utilize a strong and well-trained semantic segmentation model\nfor 2D images to augment RGB-D images with pseudo-label. The augmented dataset\ncan then be used to pre-train 3D models. Finally, by simply fine-tuning on a\nfew labeled 3D instances, our method already outperforms existing\nstate-of-the-art that is tailored for 3D label efficiency. We also show that\nthe results of mean-teacher and entropy minimization can be improved by our\npre-training, suggesting that the transferred knowledge is helpful in\nsemi-supervised setting. We verify the effectiveness of our approach on two\npopular 3D models and three different tasks. On ScanNet official evaluation, we\nestablish new state-of-the-art semantic segmentation results on the\ndata-efficient track.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping-Chung Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complexity Reduction of Learned In-Loop Filtering in Video Coding. (arXiv:2203.08650v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.08650","description":"<p>In video coding, in-loop filters are applied on reconstructed video frames to\nenhance their perceptual quality, before storing the frames for output.\nConventional in-loop filters are obtained by hand-crafted methods. Recently,\nlearned filters based on convolutional neural networks that utilize attention\nmechanisms have been shown to improve upon traditional techniques. However,\nthese solutions are typically significantly more computationally expensive,\nlimiting their potential for practical applications. The proposed method uses a\nnovel combination of sparsity and structured pruning for complexity reduction\nof learned in-loop filters. This is done through a three-step training process\nof magnitude-guidedweight pruning, insignificant neuron identification and\nremoval, and fine-tuning. Through initial tests we find that network parameters\ncan be significantly reduced with a minimal impact on network performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayliss_W/0/1/0/all/0/1\">Woody Bayliss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1\">Luka Murn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Izquierdo_E/0/1/0/all/0/1\">Ebroul Izquierdo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Flow: Cross-layer Graph Flow Distillation for Dual-Efficient Medical Image Segmentation. (arXiv:2203.08667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08667","description":"<p>With the development of deep convolutional neural networks, medical image\nsegmentation has achieved a series of breakthroughs in recent years. However,\nthe higher-performance convolutional neural networks always mean numerous\nparameters and expensive computation costs, which will hinder the applications\nin clinical scenarios. Meanwhile, the scarceness of large-scale annotated\nmedical image datasets further impedes the application of high-performance\nnetworks. To tackle these problems, we propose Graph Flow, a novel\ncomprehensive knowledge distillation method, to exploit the cross-layer graph\nflow knowledge for both network-efficient and annotation-efficient medical\nimage segmentation. Specifically, our Graph Flow Distillation constructs a\nvariation graph which is employed to measure the flow of channel-wise salience\nfeatures between different layers. Next, the knowledge included in the\nvariation graph is transferred from a well-trained cumbersome teacher network\nto a non-trained compact student network. In addition, an unsupervised\nParaphraser Module is designed to refine the knowledge of the teacher network,\nwhich is also beneficial for the stabilization of training procedure.\nFurthermore, we build a unified distillation framework by integrating the\nadversarial distillation and the vanilla logits distillation, which can further\npromote the final performance respectively. As a result, extensive experiments\nconducted on Gastric Cancer Segmentation Dataset and Synapse Multi-organ\nSegmentation Dataset demonstrate the prominent ability of our method which\nachieves state-of-the-art performance on these different-modality and\nmulti-category medical image datasets. Moreover, we demonstrate the\neffectiveness of our Graph Flow through a new semi-supervised paradigm for\ndual-efficient medical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenxuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Exploration for Neural Global Illumination of Variable Scenes. (arXiv:2203.08272v1 [cs.GR] CROSS LISTED)","link":"http://arxiv.org/abs/2203.08272","description":"<p>Neural rendering algorithms introduce a fundamentally new approach for\nphotorealistic rendering, typically by learning a neural representation of\nillumination on large numbers of ground truth images. When training for a given\nvariable scene, i.e., changing objects, materials, lights and viewpoint, the\nspace D of possible training data instances quickly becomes unmanageable as the\ndimensions of variable parameters increase. We introduce a novel Active\nExploration method using Markov Chain Monte Carlo, which explores D, generating\nsamples (i.e., ground truth renderings) that best help training and interleaves\ntraining and on-the-fly sample data generation. We introduce a self-tuning\nsample reuse strategy to minimize the expensive step of rendering training\nsamples. We apply our approach on a neural generator that learns to render\nnovel scene instances given an explicit parameterization of the scene\nconfiguration. Our results show that Active Exploration trains our network much\nmore efficiently than uniformly sampling, and together with our resolution\nenhancement approach, achieves better quality than uniform sampling at\nconvergence. Our method allows interactive rendering of hard light transport\npaths (e.g., complex caustics) -- that require very high samples counts to be\ncaptured -- and provides dynamic scene navigation and manipulation, after\ntraining for 5-18 hours depending on required quality and variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diolatzis_S/0/1/0/all/0/1\">Stavros Diolatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drettakis_G/0/1/0/all/0/1\">George Drettakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}