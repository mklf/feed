{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness. (arXiv:2205.00001v1 [cs.AI])","link":"http://arxiv.org/abs/2205.00001","description":"<p>Having a rich multimodal inner language is an important component of human\nintelligence that enables several necessary core cognitive functions such as\nmultimodal prediction, translation, and generation. Building upon the Conscious\nTuring Machine (CTM), a machine model for consciousness as proposed by Blum and\nBlum (2021), we describe the desiderata of a multimodal language called\nBrainish, comprising words, images, audio, and sensations combined in\nrepresentations that the CTM's processors use to communicate with each other.\nWe define the syntax and semantics of Brainish before operationalizing this\nlanguage through the lens of multimodal artificial intelligence, a vibrant\nresearch area studying the computational tools necessary for processing and\nrelating information from heterogeneous signals. Our general framework for\nlearning Brainish involves designing (1) unimodal encoders to segment and\nrepresent unimodal data, (2) a coordinated representation space that relates\nand composes unimodal features to derive holistic meaning across multimodal\ninputs, and (3) decoders to map multimodal representations into predictions\n(for fusion) or raw data (for translation or generation). Through discussing\nhow Brainish is crucial for communication and coordination in order to achieve\nconsciousness in the CTM, and by implementing a simple version of Brainish and\nevaluating its capability of demonstrating intelligence on multimodal\nprediction and retrieval tasks on several real-world image, text, and audio\ndatasets, we argue that such an inner language will be important for advances\nin machine models of intelligence and consciousness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Aware Feedback-Based Self-Learning in Large-Scale Conversational AI. (arXiv:2205.00029v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00029","description":"<p>Self-learning paradigms in large-scale conversational AI agents tend to\nleverage user feedback in bridging between what they say and what they mean.\nHowever, such learning, particularly in Markov-based query rewriting systems\nhave far from addressed the impact of these models on future training where\nsuccessive feedback is inevitably contingent on the rewrite itself, especially\nin a continually updating environment. In this paper, we explore the\nconsequences of this inherent lack of self-awareness towards impairing the\nmodel performance, ultimately resulting in both Type I and II errors over time.\nTo that end, we propose augmenting the Markov Graph construction with a\nsuperposition-based adjacency matrix. Here, our method leverages an induced\nstochasticity to reactively learn a locally-adaptive decision boundary based on\nthe performance of the individual rewrites in a bi-variate beta setting. We\nalso surface a data augmentation strategy that leverages template-based\ngeneration in abridging complex conversation hierarchies of dialogs so as to\nsimplify the learning process. All in all, we demonstrate that our self-aware\nmodel improves the overall PR-AUC by 27.45%, achieves a relative defect\nreduction of up to 31.22%, and is able to adapt quicker to changes in global\npreferences across a large number of customers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ponnusamy_P/0/1/0/all/0/1\">Pragaash Ponnusamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathialagan_C/0/1/0/all/0/1\">Clint Solomon Mathialagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_G/0/1/0/all/0/1\">Gustavo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenlei Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do we Really Know about State of the Art NER?. (arXiv:2205.00034v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00034","description":"<p>Named Entity Recognition (NER) is a well researched NLP task and is widely\nused in real world NLP scenarios. NER research typically focuses on the\ncreation of new ways of training NER, with relatively less emphasis on\nresources and evaluation. Further, state of the art (SOTA) NER models, trained\non standard datasets, typically report only a single performance measure\n(F-score) and we don't really know how well they do for different entity types\nand genres of text, or how robust are they to new, unseen entities. In this\npaper, we perform a broad evaluation of NER using a popular dataset, that takes\ninto consideration various text genres and sources constituting the dataset at\nhand. Additionally, we generate six new adversarial test sets through small\nperturbations in the original test set, replacing select entities while\nretaining the context. We also train and test our models on randomly generated\ntrain/dev/test splits followed by an experiment where the models are trained on\na select set of genres but tested genres not seen in training. These\ncomprehensive evaluation strategies were performed using three SOTA NER models.\nBased on our results, we recommend some useful reporting practices for NER\nresearchers, that could help in providing a better understanding of a SOTA\nmodel's performance in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vajjala_S/0/1/0/all/0/1\">Sowmya Vajjala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_R/0/1/0/all/0/1\">Ramya Balasubramaniam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answer Consolidation: Formulation and Benchmarking. (arXiv:2205.00042v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00042","description":"<p>Current question answering (QA) systems primarily consider the single-answer\nscenario, where each question is assumed to be paired with one correct answer.\nHowever, in many real-world QA applications, multiple answer scenarios arise\nwhere consolidating answers into a comprehensive and non-redundant set of\nanswers is a more efficient user interface. In this paper, we formulate the\nproblem of answer consolidation, where answers are partitioned into multiple\ngroups, each representing different aspects of the answer set. Then, given this\npartitioning, a comprehensive and non-redundant set of answers can be\nconstructed by picking one answer from each group. To initiate research on\nanswer consolidation, we construct a dataset consisting of 4,699 questions and\n24,006 sentences and evaluate multiple models. Despite a promising performance\nachieved by the best-performing supervised models, we still believe this task\nhas room for further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1\">Qiang Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elfardy_H/0/1/0/all/0/1\">Heba Elfardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logically Consistent Adversarial Attacks for Soft Theorem Provers. (arXiv:2205.00047v1 [cs.LG])","link":"http://arxiv.org/abs/2205.00047","description":"<p>Recent efforts within the AI community have yielded impressive results\ntowards \"soft theorem proving\" over natural language sentences using language\nmodels. We propose a novel, generative adversarial framework for probing and\nimproving these models' reasoning capabilities. Adversarial attacks in this\ndomain suffer from the logical inconsistency problem, whereby perturbations to\nthe input may alter the label. Our Logically consistent AdVersarial Attacker,\nLAVA, addresses this by combining a structured generative process with a\nsymbolic solver, guaranteeing logical consistency. Our framework successfully\ngenerates adversarial attacks and identifies global weaknesses common across\nmultiple target models. Our analyses reveal naive heuristics and\nvulnerabilities in these models' reasoning capabilities, exposing an incomplete\ngrasp of logical deduction under logic programs. Finally, in addition to\neffective probing of these models, we show that training on the generated\nsamples improves the target model's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaskell_A/0/1/0/all/0/1\">Alexander Gaskell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Consistency for Zero-Shot Task Generalization. (arXiv:2205.00049v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00049","description":"<p>One of the most impressive results of recent NLP history is the ability of\npre-trained language models to solve new tasks in a zero-shot setting. To\nachieve this, NLP tasks are framed as natural language prompts, generating a\nresponse indicating the predicted output. Nonetheless, the performance in such\nsettings often lags far behind its supervised counterpart, suggesting a large\nspace for potential improvement. In this paper, we explore methods to utilize\nunlabeled data to improve zero-shot performance. Specifically, we take\nadvantage of the fact that multiple prompts can be used to specify a single\ntask, and propose to regularize prompt consistency, encouraging consistent\npredictions over this diverse set of prompts. Our method makes it possible to\nfine-tune the model either with extra unlabeled training data, or directly on\ntest input at inference time in an unsupervised manner. In experiments, our\napproach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,\n2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points\nin terms of accuracy. The gains are often attained with a small number of\nunlabeled examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExSum: From Local Explanations to Model Understanding. (arXiv:2205.00130v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00130","description":"<p>Interpretability methods are developed to understand the working mechanisms\nof black-box models, which is crucial to their responsible deployment.\nFulfilling this goal requires both that the explanations generated by these\nmethods are correct and that people can easily and reliably understand them.\nWhile the former has been addressed in prior work, the latter is often\noverlooked, resulting in informal model understanding derived from a handful of\nlocal explanations. In this paper, we introduce explanation summary (ExSum), a\nmathematical framework for quantifying model understanding, and propose metrics\nfor its quality assessment. On two domains, ExSum highlights various\nlimitations in the current practice, helps develop accurate model\nunderstanding, and reveals easily overlooked properties of the model. We also\nconnect understandability to other properties of explanations such as human\nalignment, robustness, and counterfactual minimality and plausibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yilun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Representation Learning With Text and Images. (arXiv:2205.00142v1 [cs.LG])","link":"http://arxiv.org/abs/2205.00142","description":"<p>In recent years, multimodal AI has seen an upward trend as researchers are\nintegrating data of different types such as text, images, speech into modelling\nto get the best results. This project leverages multimodal AI and matrix\nfactorization techniques for representation learning, on text and image data\nsimultaneously, thereby employing the widely used techniques of Natural\nLanguage Processing (NLP) and Computer Vision. The learnt representations are\nevaluated using downstream classification and regression tasks. The methodology\nadopted can be extended beyond the scope of this project as it uses\nAuto-Encoders for unsupervised representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayagopal_A/0/1/0/all/0/1\">Aishwarya Jayagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aiswarya_A/0/1/0/all/0/1\">Ankireddy Monica Aiswarya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ankita Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_S/0/1/0/all/0/1\">Srinivasan Kolumam Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Know by the Company Words Keep and What Else Lies in the Vicinity. (arXiv:2205.00148v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00148","description":"<p>The development of state-of-the-art (SOTA) Natural Language Processing (NLP)\nsystems has steadily been establishing new techniques to absorb the statistics\nof linguistic data. These techniques often trace well-known constructs from\ntraditional theories, and we study these connections to close gaps around key\nNLP methods as a means to orient future work. For this, we introduce an\nanalytic model of the statistics learned by seminal algorithms (including GloVe\nand Word2Vec), and derive insights for systems that use these algorithms and\nthe statistics of co-occurrence, in general. In this work, we derive -- to the\nbest of our knowledge -- the first known solution to Word2Vec's\nsoftmax-optimized, skip-gram algorithm. This result presents exciting potential\nfor future development as a direct solution to a deep learning (DL) language\nmodel's (LM's) matrix factorization. However, we use the solution to\ndemonstrate a seemingly-universal existence of a property that word vectors\nexhibit and which allows for the prophylactic discernment of biases in data --\nprior to their absorption by DL models. To qualify our work, we conduct an\nanalysis of independence, i.e., on the density of statistical dependencies in\nco-occurrence models, which in turn renders insights on the distributional\nhypothesis' partial fulfillment by co-occurrence statistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jake Ryland Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidenreich_H/0/1/0/all/0/1\">Hunter Scott Heidenreich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models. (arXiv:2205.00176v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00176","description":"<p>Recent open-domain dialogue models have brought numerous breakthroughs.\nHowever, building a chat system is not scalable since it often requires a\nconsiderable volume of human-human dialogue data, especially when enforcing\nfeatures such as persona, style, or safety. In this work, we study the\nchallenge of imposing roles on open-domain dialogue systems, with the goal of\nmaking the systems maintain consistent roles while conversing naturally with\nhumans. To accomplish this, the system must satisfy a role specification that\nincludes certain conditions on the stated features as well as a system policy\non whether or not certain types of utterances are allowed. For this, we propose\nan efficient data collection framework leveraging in-context few-shot learning\nof large-scale language models for building role-satisfying dialogue dataset\nfrom scratch. We then compare various architectures for open-domain dialogue\nsystems in terms of meeting role specifications while maintaining\nconversational abilities. Automatic and human evaluations show that our models\nreturn few out-of-bounds utterances, keeping competitive performance on general\nmetrics. We release a Korean dialogue dataset we built for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sanghwan Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_D/0/1/0/all/0/1\">Donghyun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_D/0/1/0/all/0/1\">Donghoon Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Soyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Woomyoung Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers. (arXiv:2205.00177v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00177","description":"<p>Existing Math Word Problem (MWP) solvers have achieved high accuracy on\nbenchmark datasets. However, prior works have shown that such solvers do not\ngeneralize well and rely on superficial cues to achieve high performance. In\nthis paper, we first conduct experiments to showcase that this behaviour is\nmainly associated with the limited size and diversity present in existing MWP\ndatasets. Next, we propose several data augmentation techniques broadly\ncategorized into Substitution and Paraphrasing based methods. By deploying\nthese methods we increase the size of existing datasets by five folds.\nExtensive experiments on two benchmark datasets across three state-of-the-art\nMWP solvers show that proposed methods increase the generalization and\nrobustness of existing solvers. On average, proposed methods significantly\nincrease the state-of-the-art results by over five percentage points on\nbenchmark datasets. Further, the solvers trained on the augmented dataset\nperform comparatively better on the challenge test set. We also show the\neffectiveness of proposed techniques through ablation studies and verify the\nquality of augmented samples through human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwary_R/0/1/0/all/0/1\">Rishabh Maheshwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pudi_V/0/1/0/all/0/1\">Vikram Pudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Evaluation Method: Evaluation Data and Metrics for Chinese Grammar Error Correction. (arXiv:2205.00217v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00217","description":"<p>As a fundamental task in natural language processing, Chinese Grammatical\nError Correction (CGEC) has gradually received widespread attention and become\na research hotspot. However, one obvious deficiency for the existing CGEC\nevaluation system is that the evaluation values are significantly influenced by\nthe Chinese word segmentation results or different language models. The\nevaluation values of the same error correction model can vary considerably\nunder different word segmentation systems or different language models.\nHowever, it is expected that these metrics should be independent of the word\nsegmentation results and language models, as they may lead to a lack of\nuniqueness and comparability in the evaluation of different methods. To this\nend, we propose three novel evaluation metrics for CGEC in two dimensions:\nreference-based and reference-less. In terms of the reference-based metric, we\nintroduce sentence-level accuracy and char-level BLEU to evaluate the corrected\nsentences. Besides, in terms of the reference-less metric, we adopt char-level\nmeaning preservation to measure the semantic preservation degree of the\ncorrected sentences. We deeply evaluate and analyze the reasonableness and\nvalidity of the three proposed metrics, and we expect them to become a new\nstandard for CGEC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaotian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction. (arXiv:2205.00241v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00241","description":"<p>Most previous studies aim at extracting events from a single sentence, while\ndocument-level event extraction still remains under-explored. In this paper, we\nfocus on extracting event arguments from an entire document, which mainly faces\ntwo critical problems: a) the long-distance dependency between trigger and\narguments over sentences; b) the distracting context towards an event in the\ndocument. To address these issues, we propose a Two-Stream Abstract meaning\nRepresentation enhanced extraction model (TSAR). TSAR encodes the document from\ndifferent perspectives by a two-stream encoding module, to utilize local and\nglobal information and lower the impact of distracting context. Besides, TSAR\nintroduces an AMR-guided interaction module to capture both intra-sentential\nand inter-sentential features, based on the locally and globally constructed\nAMR semantic graphs. An auxiliary boundary loss is introduced to enhance the\nboundary information for text spans explicitly. Extensive experiments\nillustrate that TSAR outperforms previous state-of-the-art by a large margin,\nwith 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents\ndatasets respectively, showing the superiority in the cross-sentence arguments\nextraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shuang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing. (arXiv:2205.00258v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00258","description":"<p>The success of Pre-Trained Models (PTMs) has reshaped the development of\nNatural Language Processing (NLP). Yet, it is not easy to obtain\nhigh-performing models and deploy them online for industrial practitioners. To\nbridge this gap, EasyNLP is designed to make it easy to build NLP applications,\nwhich supports a comprehensive suite of NLP algorithms. It further features\nknowledge-enhanced pre-training, knowledge distillation and few-shot learning\nfunctionalities for large-scale PTMs, and provides a unified framework of model\ntraining, inference and deployment for real-world applications. Currently,\nEasyNLP has powered over ten business units within Alibaba Group and is\nseamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.\nThe source code of our EasyNLP toolkit is released at GitHub\n(https://github.com/alibaba/EasyNLP).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Ming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposing Cross-Lingual Lexical Knowledge from Multilingual Sentence Encoders. (arXiv:2205.00267v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00267","description":"<p>Pretrained multilingual language models (LMs) can be successfully transformed\ninto multilingual sentence encoders (SEs; e.g., LaBSE, xMPNET) via additional\nfine-tuning or model distillation on parallel data. However, it remains\nuncertain how to best leverage their knowledge to represent sub-sentence\nlexical items (i.e., words and phrases) in cross-lingual lexical tasks. In this\nwork, we probe these SEs for the amount of cross-lingual lexical knowledge\nstored in their parameters, and compare them against the original multilingual\nLMs. We also devise a novel method to expose this knowledge by additionally\nfine-tuning multilingual models through inexpensive contrastive learning\nprocedure, requiring only a small amount of word translation pairs. We evaluate\nour method on bilingual lexical induction (BLI), cross-lingual lexical semantic\nsimilarity, and cross-lingual entity linking, and report substantial gains on\nstandard benchmarks (e.g., +10 Precision@1 points in BLI), validating that the\nSEs such as LaBSE can be 'rewired' into effective cross-lingual lexical\nencoders. Moreover, we show that resulting representations can be successfully\ninterpolated with static embeddings from cross-lingual word embedding spaces to\nfurther boost the performance in lexical tasks. In sum, our approach provides\nan effective tool for exposing and harnessing multilingual lexical knowledge\n'hidden' in multilingual sentence encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clues Before Answers: Generation-Enhanced Multiple-Choice QA. (arXiv:2205.00274v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00274","description":"<p>A trending paradigm for multiple-choice question answering (MCQA) is using a\ntext-to-text framework. By unifying data in different tasks into a single\ntext-to-text format, it trains a generative encoder-decoder model which is both\npowerful and universal. However, a side effect of twisting a generation target\nto fit the classification nature of MCQA is the under-utilization of the\ndecoder and the knowledge that can be decoded. To exploit the generation\ncapability and underlying knowledge of a pre-trained encoder-decoder model, in\nthis paper, we propose a generation-enhanced MCQA model named GenMC. It\ngenerates a clue from the question and then leverages the clue to enhance a\nreader for MCQA. It outperforms text-to-text models on multiple MCQA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiaying Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Emotion-specific Features to Improve Transformer Performance for Emotion Classification. (arXiv:2205.00283v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00283","description":"<p>This paper describes the approach to the Emotion Classification shared task\nheld at WASSA 2022 by team PVGs AI Club. This Track 2 sub-task focuses on\nbuilding models which can predict a multi-class emotion label based on essays\nfrom news articles where a person, group or another entity is affected.\nBaseline transformer models have been demonstrating good results on sequence\nclassification tasks, and we aim to improve this performance with the help of\nensembling techniques, and by leveraging two variations of emotion-specific\nrepresentations. We observe better results than our baseline models and achieve\nan accuracy of 0.619 and a macro F1 score of 0.520 on the emotion\nclassification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaily Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kshirsagar_A/0/1/0/all/0/1\">Atharva Kshirsagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidnerlikar_A/0/1/0/all/0/1\">Aditi Sidnerlikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodake_N/0/1/0/all/0/1\">Nikhil Khodake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marathe_M/0/1/0/all/0/1\">Manisha Marathe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks. (arXiv:2205.00305v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00305","description":"<p>Transformer-based pre-trained models with millions of parameters require\nlarge storage. Recent approaches tackle this shortcoming by training adapters,\nbut these approaches still require a relatively large number of parameters. In\nthis study, AdapterBias, a surprisingly simple yet effective adapter\narchitecture, is proposed. AdapterBias adds a token-dependent shift to the\nhidden output of transformer layers to adapt to downstream tasks with only a\nvector and a linear layer. Extensive experiments are conducted to demonstrate\nthe effectiveness of AdapterBias. The experiments show that our proposed method\ncan dramatically reduce the trainable parameters compared to the previous works\nwith a minimal decrease in task performances compared with fine-tuned\npre-trained models. We further find that AdapterBias automatically learns to\nassign more significant representation shifts to the tokens related to the task\nin consideration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chin-Lun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zih-Ching Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yun-Ru Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detoxifying Language Models with a Toxic Corpus. (arXiv:2205.00320v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00320","description":"<p>Existing studies have investigated the tendency of autoregressive language\nmodels to generate contexts that exhibit undesired biases and toxicity. Various\ndebiasing approaches have been proposed, which are primarily categorized into\ndata-based and decoding-based. In our study, we investigate the ensemble of the\ntwo debiasing paradigms, proposing to use toxic corpus as an additional\nresource to reduce the toxicity. Our result shows that toxic corpus can indeed\nhelp to reduce the toxicity of the language generation process substantially,\ncomplementing the existing debiasing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Yoon A Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HateCheckHIn: Evaluating Hindi Hate Speech Detection Models. (arXiv:2205.00328v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00328","description":"<p>Due to the sheer volume of online hate, the AI and NLP communities have\nstarted building models to detect such hateful content. Recently, multilingual\nhate is a major emerging challenge for automated detection where code-mixing or\nmore than one language have been used for conversation in social media.\nTypically, hate speech detection models are evaluated by measuring their\nperformance on the held-out test data using metrics such as accuracy and\nF1-score. While these metrics are useful, it becomes difficult to identify\nusing them where the model is failing, and how to resolve it. To enable more\ntargeted diagnostic insights of such multilingual hate speech models, we\nintroduce a set of functionalities for the purpose of evaluation. We have been\ninspired to design this kind of functionalities based on real-world\nconversation on social media. Considering Hindi as a base language, we craft\ntest cases for each functionality. We name our evaluation dataset HateCheckHIn.\nTo illustrate the utility of these functionalities , we test state-of-the-art\ntransformer based m-BERT model and the Perspective API.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mithun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opponent Modeling in Negotiation Dialogues by Related Data Adaptation. (arXiv:2205.00344v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00344","description":"<p>Opponent modeling is the task of inferring another party's mental state\nwithin the context of social interactions. In a multi-issue negotiation, it\ninvolves inferring the relative importance that the opponent assigns to each\nissue under discussion, which is crucial for finding high-value deals. A\npractical model for this task needs to infer these priorities of the opponent\non the fly based on partial dialogues as input, without needing additional\nannotations for training. In this work, we propose a ranker for identifying\nthese priorities from negotiation dialogues. The model takes in a partial\ndialogue as input and predicts the priority order of the opponent. We further\ndevise ways to adapt related data sources for this task to provide more\nexplicit supervision for incorporating the opponent's preferences and offers,\nas a proxy to relying on granular utterance-level annotations. We show the\nutility of our proposed approach through extensive experiments based on two\ndialogue datasets. We find that the proposed data adaptations lead to strong\nperformance in zero-shot and few-shot scenarios. Moreover, they allow the model\nto perform better than baselines while accessing fewer utterances from the\nopponent. We release our code to support future work in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chawla_K/0/1/0/all/0/1\">Kushal Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_G/0/1/0/all/0/1\">Gale M. Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Process-Oriented, Modular, and Versatile Question Generation that Meets Educational Needs. (arXiv:2205.00355v1 [cs.HC])","link":"http://arxiv.org/abs/2205.00355","description":"<p>NLP-powered automatic question generation (QG) techniques carry great\npedagogical potential of saving educators' time and benefiting student\nlearning. Yet, QG systems have not been widely adopted in classrooms to date.\nIn this work, we aim to pinpoint key impediments and investigate how to improve\nthe usability of automatic QG techniques for educational purposes by\nunderstanding how instructors construct questions and identifying touch points\nto enhance the underlying NLP models. We perform an in-depth need finding study\nwith 11 instructors across 7 different universities, and summarize their\nthought processes and needs when creating questions. While instructors show\ngreat interests in using NLP systems to support question design, none of them\nhas used such tools in practice. They resort to multiple sources of\ninformation, ranging from domain knowledge to students' misconceptions, all of\nwhich missing from today's QG systems. We argue that building effective\nhuman-NLP collaborative QG systems that emphasize instructor control and\nexplainability is imperative for real-world adoption. We call for QG systems to\nprovide process-oriented support, use modular design, and handle diverse\nsources of input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Simin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houghton_J/0/1/0/all/0/1\">Jessica Houghton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Spatial Reasoning. (arXiv:2205.00363v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00363","description":"<p>Spatial relations are fundamental to human cognition and are the most basic\nknowledge for us to understand and communicate about our physical surroundings.\nIn this paper, we ask the critical question: Are current vision-and-language\nmodels (VLMs) able to correctly understand spatial relations? To answer this\nquestion, we propose Visual Spatial Reasoning (VSR), a novel benchmark task\nwith human labelled dataset for investigating VLMs' capabilities in recognising\n65 types of spatial relationships (e.g., under, in front of, facing etc.) in\nnatural text-image pairs. Specifically, given a caption and an image, the model\nneeds to perform binary classification and decide if the caption accurately\ndescribes the spatial relationships of two objects presented in the image.\nWhile being seemingly simple and straightforward, the task shows a large gap\nbetween human and model performance (human ceiling on the VSR task is above 95%\nand models only achieve around 70%). With fine-grained categorisation and\ncontrol on both concepts and relations, our VSR benchmark enables us to perform\ninteresting probing analysis to pinpoint VLMs' failure cases and the reasons\nbehind. We observe that VLMs' by-relation performances have little correlation\nwith the number of training examples and the tested models are in general\nincapable of recognising relations that concern orientations of objects. Also,\nVLMs have poor zero-shot generalisation toward unseen concepts. The dataset and\ncode are released at github.com/cambridgeltl/visual-spatial-reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_G/0/1/0/all/0/1\">Guy Emerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting COVID-19 Conspiracy Theories with Transformers and TF-IDF. (arXiv:2205.00377v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00377","description":"<p>The sharing of fake news and conspiracy theories on social media has\nwide-spread negative effects. By designing and applying different machine\nlearning models, researchers have made progress in detecting fake news from\ntext. However, existing research places a heavy emphasis on general,\ncommon-sense fake news, while in reality fake news often involves rapidly\nchanging topics and domain-specific vocabulary. In this paper, we present our\nmethods and results for three fake news detection tasks at MediaEval benchmark\n2021 that specifically involve COVID-19 related topics. We experiment with a\ngroup of text-based models including Support Vector Machines, Random Forest,\nBERT, and RoBERTa. We find that a pre-trained transformer yields the best\nvalidation results, but a randomly initialized transformer with smart design\ncan also be trained to reach accuracies close to that of the pre-trained\ntransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haoming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Mingyue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedland_G/0/1/0/all/0/1\">Gerald Friedland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Cross-lingual Conversation Summarization Challenge. (arXiv:2205.00379v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00379","description":"<p>We propose the shared task of cross-lingual conversation summarization,\n\\emph{ConvSumX Challenge}, opening new avenues for researchers to investigate\nsolutions that integrate conversation summarization and machine translation.\nThis task can be particularly useful due to the emergence of online meetings\nand conferences. We construct a new benchmark, covering 2 real-world scenarios\nand 3 language directions, including a low-resource language. We hope that\n\\emph{ConvSumX} can motivate researches to go beyond English and break the\nbarrier for non-English speakers to benefit from recent advances of\nconversation summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xianchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crude Oil-related Events Extraction and Processing: A Transfer Learning Approach. (arXiv:2205.00387v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00387","description":"<p>One of the challenges in event extraction via traditional supervised learning\nparadigm is the need for a sizeable annotated dataset to achieve satisfactory\nmodel performance. It is even more challenging when it comes to event\nextraction in the finance and economics domain, a domain with considerably\nfewer resources. This paper presents a complete framework for extracting and\nprocessing crude oil-related events found in CrudeOilNews corpus, addressing\nthe issue of annotation scarcity and class imbalance by leveraging on the\neffectiveness of transfer learning. Apart from event extraction, we place\nspecial emphasis on event properties (Polarity, Modality, and Intensity)\nclassification to determine the factual certainty of each event. We build\nbaseline models first by supervised learning and then exploit Transfer Learning\nmethods to boost event extraction model performance despite the limited amount\nof annotated data and severe class imbalance. This is done via methods within\nthe transfer learning framework such as Domain Adaptive Pre-training,\nMulti-task Learning and Sequential Transfer Learning. Based on experiment\nresults, we are able to improve all event extraction sub-task models both in F1\nand MCC1-score as compared to baseline models trained via the standard\nsupervised learning. Accurate and holistic event extraction from crude oil news\nis very useful for downstream tasks such as understanding event chains and\nlearning event-event relations, which can be used for other downstream tasks\nsuch as commodity price prediction, summarisation, etc. to support a wide range\nof business decision making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Meisin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1\">Lay-Ki Soon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siew_E/0/1/0/all/0/1\">Eu-Gene Siew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELQA: A Corpus of Questions and Answers about the English Language. (arXiv:2205.00395v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00395","description":"<p>We introduce a community-sourced dataset for English Language Question\nAnswering (ELQA), which consists of more than 180k questions and answers on\nnumerous topics about English language such as grammar, meaning, fluency, and\netymology. The ELQA corpus will enable new NLP applications for language\nlearners. We introduce three tasks based on the ELQA corpus: 1) answer quality\nclassification, 2) semantic search for finding similar questions, and 3) answer\ngeneration. We present baselines for each task along with analysis, showing the\nstrengths and weaknesses of current transformer-based models. The ELQA corpus\nand scripts are publicly available for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behzad_S/0/1/0/all/0/1\">Shabnam Behzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions. (arXiv:2205.00415v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00415","description":"<p>In recent years, progress in NLU has been driven by benchmarks. These\nbenchmarks are typically collected by crowdsourcing, where annotators write\nexamples based on annotation instructions crafted by dataset creators. In this\nwork, we hypothesize that annotators pick up on patterns in the crowdsourcing\ninstructions, which bias them to write similar examples that are then\nover-represented in the collected data. We study this form of bias, termed\ninstruction bias, in 14 recent NLU benchmarks, showing that instruction\nexamples often exhibit concrete patterns, which are propagated by crowdworkers\nto the collected data. This extends previous work (Geva et al., 2019) and\nraises a new concern of whether we are modeling the dataset creator's\ninstructions, rather than the task. Through a series of experiments, we show\nthat, indeed, instruction bias can lead to overestimation of model performance,\nand that models struggle to generalize beyond biases originating in the\ncrowdsourcing instructions. We further analyze the influence of instruction\nbias in terms of pattern frequency and model size, and derive concrete\nrecommendations for creating future NLU benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ETMS@IITKGP at SemEval-2022 Task 10: Structured Sentiment Analysis Using A Generative Approach. (arXiv:2205.00440v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00440","description":"<p>Structured Sentiment Analysis (SSA) deals with extracting opinion tuples in a\ntext, where each tuple (h, e, t, p) consists of h, the holder, who expresses a\nsentiment polarity p towards a target t through a sentiment expression e. While\nprior works explore graph-based or sequence labeling-based approaches for the\ntask, we in this paper present a novel unified generative method to solve SSA,\na SemEval2022 shared task. We leverage a BART-based encoder-decoder\narchitecture and suitably modify it to generate, given a sentence, a sequence\nof opinion tuples. Each generated tuple consists of seven integers respectively\nrepresenting the indices corresponding to the start and end positions of the\nholder, target, and expression spans, followed by the sentiment polarity class\nassociated between the target and the sentiment expression. We perform rigorous\nexperiments for both Monolingual and Cross-lingual subtasks, and achieve\ncompetitive Sentiment F1 scores on the leaderboard in both settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R_R/0/1/0/all/0/1\">Raghav R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemali_A/0/1/0/all/0/1\">Adarsh Vemali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1\">Rajdeep Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. (arXiv:2205.00445v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00445","description":"<p>Huge language models (LMs) have ushered in a new era for AI, serving as a\ngateway to natural-language-based knowledge tasks. Although an essential\nelement of modern AI, LMs are also inherently limited in a number of ways. We\ndiscuss these limitations and how they can be avoided by adopting a systems\napproach. Conceptualizing the challenge as one that involves knowledge and\nreasoning in addition to linguistic processing, we define a flexible\narchitecture with multiple neural models, complemented by discrete knowledge\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\nModular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system,\nsome of the technical challenges in implementing it, and Jurassic-X, AI21 Labs'\nMRKL system implementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpas_E/0/1/0/all/0/1\">Ehud Karpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_B/0/1/0/all/0/1\">Barak Lenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieber_O/0/1/0/all/0/1\">Opher Lieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_N/0/1/0/all/0/1\">Nir Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1\">Yoav Shoham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bata_H/0/1/0/all/0/1\">Hofit Bata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhlgay_D/0/1/0/all/0/1\">Dor Muhlgay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozen_N/0/1/0/all/0/1\">Noam Rozen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1\">Erez Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shachaf_G/0/1/0/all/0/1\">Gal Shachaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1\">Shai Shalev-Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenholtz_M/0/1/0/all/0/1\">Moshe Tenenholtz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The use of Data Augmentation as a technique for improving neural network accuracy in detecting fake news about COVID-19. (arXiv:2205.00452v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00452","description":"<p>This paper aims to present how the application of Natural Language Processing\n(NLP) and data augmentation techniques can improve the performance of a neural\nnetwork for better detection of fake news in the Portuguese language. Fake news\nis one of the main controversies during the growth of the internet in the last\ndecade. Verifying what is fact and what is false has proven to be a difficult\ntask, while the dissemination of false news is much faster, which leads to the\nneed for the creation of tools that, automated, assist in the process of\nverification of what is fact and what is false. In order to bring a solution,\nan experiment was developed with neural network using news, real and fake,\nwhich were never seen by artificial intelligence (AI). There was a significant\nperformance in the news classification after the application of the mentioned\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Junior_W/0/1/0/all/0/1\">Wilton O. J&#xfa;nior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_M/0/1/0/all/0/1\">Mauricio S. da Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyzykowski_A/0/1/0/all/0/1\">Andre Brasil Vieira Wyzykowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jesus_A/0/1/0/all/0/1\">Arnaldo Bispo de Jesus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptualizing Treatment Leakage in Text-based Causal Inference. (arXiv:2205.00465v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00465","description":"<p>Causal inference methods that control for text-based confounders are becoming\nincreasingly important in the social sciences and other disciplines where text\nis readily available. However, these methods rely on a critical assumption that\nthere is no treatment leakage: that is, the text only contains information\nabout the confounder and no information about treatment assignment. When this\nassumption does not hold, methods that control for text to adjust for\nconfounders face the problem of post-treatment (collider) bias. However, the\nassumption that there is no treatment leakage may be unrealistic in real-world\nsituations involving text, as human language is rich and flexible. Language\nappearing in a public policy document or health records may refer to the future\nand the past simultaneously, and thereby reveal information about the treatment\nassignment.\n</p>\n<p>In this article, we define the treatment-leakage problem, and discuss the\nidentification as well as the estimation challenges it raises. Second, we\ndelineate the conditions under which leakage can be addressed by removing the\ntreatment-related signal from the text in a pre-processing step we define as\ntext distillation. Lastly, using simulation, we show how treatment leakage\nintroduces a bias in estimates of the average treatment effect (ATE) and how\ntext distillation can mitigate this bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daoud_A/0/1/0/all/0/1\">Adel Daoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jerzak_C/0/1/0/all/0/1\">Connor T. Jerzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1\">Richard Johansson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"None Class Ranking Loss for Document-Level Relation Extraction. (arXiv:2205.00476v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00476","description":"<p>Document-level relation extraction (RE) aims at extracting relations among\nentities expressed across multiple sentences, which can be viewed as a\nmulti-label classification problem. In a typical document, most entity pairs do\nnot express any pre-defined relation and are labeled as \"none\" or \"no\nrelation\". For good document-level RE performance, it is crucial to distinguish\nsuch \\textit{none} class instances (entity pairs) from those of pre-defined\nclasses (relations). However, most existing methods only estimate the\nprobability of pre-defined relations independently without considering the\nprobability of \"no relation\". This ignores the context of entity pairs and the\nlabel correlations between the none class and pre-defined classes, leading to\nsub-optimal predictions. To address this problem, we propose a new multi-label\nloss that encourages large \\textit{margins} of label confidence scores between\neach pre-defined class and the none class, which enables captured label\ncorrelations and context-dependent thresholding for label prediction. To gain\nfurther robustness against positive-negative imbalance and mislabeled data that\ncould appear in real-world RE datasets, we propose a margin regularization and\na margin shifting technique. Experimental results demonstrate that our method\nsignificantly outperforms existing multi-label losses for document-level RE and\nworks well in other multi-label tasks such as emotion classification when none\nclass instances are available for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wee Sun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nearest Neighbor Knowledge Distillation for Neural Machine Translation. (arXiv:2205.00479v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00479","description":"<p>k-nearest-neighbor machine translation (NN-MT), proposed by Khandelwal et al.\n(2021), has achieved many state-of-the-art results in machine translation\ntasks. Although effective, NN-MT requires conducting NN searches through the\nlarge datastore for each decoding step during inference, prohibitively\nincreasing the decoding cost and thus leading to the difficulty for the\ndeployment in real-world applications. In this paper, we propose to move the\ntime-consuming NN search forward to the preprocessing phase, and then introduce\nNearest Neighbor Knowledge Distillation (NN-KD) that trains the base NMT model\nto directly learn the knowledge of NN. Distilling knowledge retrieved by NN can\nencourage the NMT model to take more reasonable target tokens into\nconsideration, thus addressing the overcorrection problem. Extensive\nexperimental results show that, the proposed method achieves consistent\nimprovement over the state-of-the-art baselines including NN-MT, while\nmaintaining the same training and decoding speed as the standard NMT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhixian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Renliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank HMMs and PCFGs. (arXiv:2205.00484v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00484","description":"<p>Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs)\nare widely used structured models, both of which can be represented as factor\ngraph grammars (FGGs), a powerful formalism capable of describing a wide range\nof models. Recent research found it beneficial to use large state spaces for\nHMMs and PCFGs. However, inference with large state spaces is computationally\ndemanding, especially for PCFGs. To tackle this challenge, we leverage tensor\nrank decomposition (aka.\\ CPD) to decrease inference computational complexities\nfor a subset of FGGs subsuming HMMs and PCFGs. We apply CPD on the factors of\nan FGG and then construct a new FGG defined in the rank space. Inference with\nthe new FGG produces the same result but has a lower time complexity when the\nrank size is smaller than the state size. We conduct experiments on HMM\nlanguage modeling and unsupervised PCFG parsing, showing better performance\nthan previous work. Our code is publicly available at\n\\url{https://github.com/VPeterV/RankSpace-Models}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilingual End-to-End ASR with Byte-Level Subwords. (arXiv:2205.00485v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00485","description":"<p>In this paper, we investigate how the output representation of an end-to-end\nneural network affects multilingual automatic speech recognition (ASR). We\nstudy different representations including character-level, byte-level, byte\npair encoding (BPE), and byte-level byte pair encoding (BBPE) representations,\nand analyze their strengths and weaknesses. We focus on developing a single\nend-to-end model to support utterance-based bilingual ASR, where speakers do\nnot alternate between two languages in a single utterance but may change\nlanguages across utterances. We conduct our experiments on English and Mandarin\ndictation tasks, and we find that BBPE with penalty schemes can improve\nutterance-based bilingual ASR performance by 2% to 5% relative even with\nsmaller number of outputs and fewer parameters. We conclude with analysis that\nindicates directions for further improving multilingual ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liuhui Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_R/0/1/0/all/0/1\">Roger Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghoshal_A/0/1/0/all/0/1\">Arnab Ghoshal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction. (arXiv:2205.00498v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00498","description":"<p>Implicit event argument extraction (EAE) aims to identify arguments that\ncould scatter over the document. Most previous work focuses on learning the\ndirect relations between arguments and the given trigger, while the implicit\nrelations with long-range dependency are not well studied. Moreover, recent\nneural network based approaches rely on a large amount of labeled data for\ntraining, which is unavailable due to the high labelling cost. In this paper,\nwe propose a Curriculum learning based Prompt tuning (CUP) approach, which\nresolves implicit EAE by four learning stages. The stages are defined according\nto the relations with the trigger node in a semantic graph, which well captures\nthe long-range dependency between arguments and the trigger. In addition, we\nintegrate a prompt-based encoder-decoder model to elicit related knowledge from\npre-trained language models (PLMs) in each stage, where the prompt templates\nare adapted with the learning progress to enhance the reasoning for arguments.\nExperimental results on two well-known benchmark datasets show the great\nadvantages of our proposed approach. In particular, we outperform the\nstate-of-the-art models in both fully-supervised and low-data scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaju Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation. (arXiv:2205.00501v1 [cs.HC])","link":"http://arxiv.org/abs/2205.00501","description":"<p>Machine learning models are commonly used to detect toxicity in online\nconversations. These models are trained on datasets annotated by human raters.\nWe explore how raters' self-described identities impact how they annotate\ntoxicity in online comments. We first define the concept of specialized rater\npools: rater pools formed based on raters' self-described identities, rather\nthan at random. We formed three such rater pools for this study--specialized\nrater pools of raters from the U.S. who identify as African American, LGBTQ,\nand those who identify as neither. Each of these rater pools annotated the same\nset of comments, which contains many references to these identity groups. We\nfound that rater identity is a statistically significant factor in how raters\nwill annotate toxicity for identity-related annotations. Using preliminary\ncontent analysis, we examined the comments with the most disagreement between\nrater pools and found nuanced differences in the toxicity annotations. Next, we\ntrained models on the annotations from each of the different rater pools, and\ncompared the scores of these models on comments from several test sets.\nFinally, we discuss how using raters that self-identify with the subjects of\ncomments can create more inclusive machine learning models, and provide more\nnuanced ratings than those by random raters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Nitesh Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kivlichan_I/0/1/0/all/0/1\">Ian Kivlichan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_R/0/1/0/all/0/1\">Rachel Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasserman_L/0/1/0/all/0/1\">Lucy Vasserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Stylistic Variation: Choices, Genres and Individuals. (arXiv:2205.00510v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00510","description":"<p>This chapter argues for more informed target metrics for the statistical\nprocessing of stylistic variation in text collections. Much as operationalised\nrelevance proved a useful goal to strive for in information retrieval, research\nin textual stylistics, whether application oriented or philologically inclined,\nneeds goals formulated in terms of pertinence, relevance, and utility - notions\nthat agree with reader experience of text. Differences readers are aware of are\nmostly based on utility - not on textual characteristics per se. Mostly,\nreaders report stylistic differences in terms of genres. Genres, while vague\nand undefined, are well-established and talked about: very early on, readers\nlearn to distinguish genres. This chapter discusses variation given by genre,\nand contrasts it to variation occasioned by individual choice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karlgren_J/0/1/0/all/0/1\">Jussi Karlgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conventions and Mutual Expectations -- understanding sources for web genres. (arXiv:2205.00512v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00512","description":"<p>Genres can be understood in many different ways. They are often perceived as\na primarily sociological construction, or, alternatively, as a\nstylostatistically observable objective characteristic of texts. The latter\nview is more common in the research field of information and language\ntechnology. These two views can be quite compatible and can inform each other;\nthis present investigation discusses knowledge sources for studying genre\nvariation and change by observing reader and author behaviour rather than\nperforming analyses on the information objects themselves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karlgren_J/0/1/0/all/0/1\">Jussi Karlgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enumeration Classes Defined by Circuits. (arXiv:2205.00539v1 [cs.CC])","link":"http://arxiv.org/abs/2205.00539","description":"<p>We refine the complexity landscape for enumeration problems by introducing\nvery low classes defined by using Boolean circuits as enumerators. We locate\nwell-known enumeration problems, e.g., from graph theory, Gray code\nenumeration, and propositional satisfiability in our classes. In this way we\nobtain a framework to distinguish between the complexity of different problems\nknown to be in $\\mathbf{DelayP}$, for which a formal way of comparison was not\npossible to this day.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Creignou_N/0/1/0/all/0/1\">Nadia Creignou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_A/0/1/0/all/0/1\">Arnaud Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollmer_H/0/1/0/all/0/1\">Heribert Vollmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Multi-Document Summarization with Information Extraction and Compression. (arXiv:2205.00548v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00548","description":"<p>We develop an abstractive summarization framework independent of labeled data\nfor multiple heterogeneous documents. Unlike existing multi-document\nsummarization methods, our framework processes documents telling different\nstories instead of documents on the same topic. We also enhance an existing\nsentence fusion method with a uni-directional language model to prioritize\nfused sentences with higher sentence probability with the goal of increasing\nreadability. Lastly, we construct a total of twelve dataset variations based on\nCNN/Daily Mail and the NewsRoom datasets, where each document group contains a\nlarge and diverse collection of documents to evaluate the performance of our\nmodel in comparison with other baseline systems. Our experiments demonstrate\nthat our framework outperforms current state-of-the-art methods in this more\ngeneric setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1\">Diego Klabjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Bias in Masked Language Models for Multiple Languages. (arXiv:2205.00551v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00551","description":"<p>Masked Language Models (MLMs) pre-trained by predicting masked tokens on\nlarge corpora have been used successfully in natural language processing tasks\nfor a variety of languages. Unfortunately, it was reported that MLMs also learn\ndiscriminative biases regarding attributes such as gender and race. Because\nmost studies have focused on MLMs in English, the bias of MLMs in other\nlanguages has rarely been investigated. Manual annotation of evaluation data\nfor languages other than English has been challenging due to the cost and\ndifficulty in recruiting annotators. Moreover, the existing bias evaluation\nmethods require the stereotypical sentence pairs consisting of the same context\nwith attribute words (e.g. He/She is a nurse). We propose Multilingual Bias\nEvaluation (MBE) score, to evaluate bias in various languages using only\nEnglish attribute word lists and parallel corpora between the target language\nand English without requiring manually annotated data. We evaluated MLMs in\neight languages using the MBE and confirmed that gender-related biases are\nencoded in MLMs for all those languages. We manually created datasets for\ngender bias in Japanese and Russian to evaluate the validity of the MBE. The\nresults show that the bias scores reported by the MBE significantly correlates\nwith that computed from the above manually created datasets and the existing\nEnglish datasets for gender bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imankulova_A/0/1/0/all/0/1\">Aizhan Imankulova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Informed Slang Interpretation. (arXiv:2205.00616v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00616","description":"<p>Slang is a predominant form of informal language making flexible and extended\nuse of words that is notoriously hard for natural language processing systems\nto interpret. Existing approaches to slang interpretation tend to rely on\ncontext but ignore semantic extensions common in slang word usage. We propose a\nsemantically informed slang interpretation (SSI) framework that considers\njointly the contextual and semantic appropriateness of a candidate\ninterpretation for a query slang. We perform rigorous evaluation on two\nlarge-scale online slang dictionaries and show that our approach not only\nachieves state-of-the-art accuracy for slang interpretation in English, but\nalso does so in zero-shot and few-shot scenarios where training data is sparse.\nFurthermore, we show how the same framework can be applied to enhancing machine\ntranslation of slang from English to other languages. Our work creates\nopportunities for the automated interpretation and translation of informal\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhewei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection. (arXiv:2205.00619v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00619","description":"<p>Ideology is at the core of political science research. Yet, there still does\nnot exist general-purpose tools to characterize and predict ideology across\ndifferent genres of text. To this end, we study Pretrained Language Models\nusing novel ideology-driven pretraining objectives that rely on the comparison\nof articles on the same story written by media of different ideologies. We\nfurther collect a large-scale dataset, consisting of more than 3.6M political\nnews articles, for pretraining. Our model POLITICS outperforms strong baselines\nand the previous state-of-the-art models on ideology prediction and stance\ndetection tasks. Further analyses show that POLITICS is especially good at\nunderstanding long or formally written texts, and is also robust in few-shot\nlearning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yujian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinliang Frederick Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegsman_D/0/1/0/all/0/1\">David Wegsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beauchamp_N/0/1/0/all/0/1\">Nick Beauchamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection. (arXiv:2205.00620v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00620","description":"<p>In modern interactive speech-based systems, speech is consumed and\ntranscribed incrementally prior to having disfluencies removed. This\npost-processing step is crucial for producing clean transcripts and high\nperformance on downstream tasks (e.g. machine translation). However, most\ncurrent state-of-the-art NLP models such as the Transformer operate\nnon-incrementally, potentially causing unacceptable delays. We propose a\nstreaming BERT-based sequence tagging model that, combined with a novel\ntraining objective, is capable of detecting disfluencies in real-time while\nbalancing accuracy and latency. This is accomplished by training the model to\ndecide whether to immediately output a prediction for the current input or to\nwait for further context. Essentially, the model learns to dynamically size its\nlookahead window. Our results demonstrate that our model produces comparably\naccurate predictions and does so sooner than our baselines, with lower flicker.\nFurthermore, the model attains state-of-the-art latency and stability scores\nwhen compared with recent work on incremental disfluency detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zayats_V/0/1/0/all/0/1\">Vicky Zayats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_D/0/1/0/all/0/1\">Daniel D. Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padfield_D/0/1/0/all/0/1\">Dirk Padfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Fine-tuning via Perturbation and Interpolation from In-batch Instances. (arXiv:2205.00633v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00633","description":"<p>Fine-tuning pretrained language models (PLMs) on downstream tasks has become\ncommon practice in natural language processing. However, most of the PLMs are\nvulnerable, e.g., they are brittle under adversarial attacks or imbalanced\ndata, which hinders the application of the PLMs on some downstream tasks,\nespecially in safe-critical scenarios. In this paper, we propose a simple yet\neffective fine-tuning method called Match-Tuning to force the PLMs to be more\nrobust. For each instance in a batch, we involve other instances in the same\nbatch to interact with it. To be specific, regarding the instances with other\nlabels as a perturbation, Match-Tuning makes the model more robust to noise at\nthe beginning of training. While nearing the end, Match-Tuning focuses more on\nperforming an interpolation among the instances with the same label for better\ngeneralization. Extensive experiments on various tasks in GLUE benchmark show\nthat Match-Tuning consistently outperforms the vanilla fine-tuning by $1.64$\nscores. Moreover, Match-Tuning exhibits remarkable robustness to adversarial\nattacks and data imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shoujie Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+song_Y/0/1/0/all/0/1\">Yifan song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Two Parameters Equation for Word Rank-Frequency Relation. (arXiv:2205.00638v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00638","description":"<p>Let $f (\\cdot)$ be the absolute frequency of words and $r$ be the rank of\nwords in decreasing order of frequency, then the following function can fit the\nrank-frequency relation \\[ f (r;s,t) = \\left(\\frac{r_{\\tt max}}{r}\\right)^{1-s}\n\\left(\\frac{r_{\\tt max}+t \\cdot r_{\\tt exp}}{r+t \\cdot r_{\\tt\nexp}}\\right)^{1+(1+t)s} \\] where $r_{\\tt max}$ and $r_{\\tt exp}$ are the\nmaximum and the expectation of the rank, respectively; $s&gt;0$ and $t&gt;0$ are\nparameters estimated from data. On well-behaved data, there should be $s&lt;1$ and\n$s \\cdot t &lt; 1$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chenchen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Contrastive Learning of Unsupervised Sentence Representations. (arXiv:2205.00656v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00656","description":"<p>Recently, contrastive learning has been shown to be effective in improving\npre-trained language models (PLM) to derive high-quality sentence\nrepresentations. It aims to pull close positive examples to enhance the\nalignment while push apart irrelevant negatives for the uniformity of the whole\nrepresentation space. However, previous works mostly adopt in-batch negatives\nor sample from training data at random. Such a way may cause the sampling bias\nthat improper negatives (e.g. false negatives and anisotropy representations)\nare used to learn sentence representations, which will hurt the uniformity of\nthe representation space. To address it, we present a new framework\n\\textbf{DCLR} (\\underline{D}ebiased \\underline{C}ontrastive\n\\underline{L}earning of unsupervised sentence \\underline{R}epresentations) to\nalleviate the influence of these improper negatives. In DCLR, we design an\ninstance weighting method to punish false negatives and generate noise-based\nnegatives to guarantee the uniformity of the representation space. Experiments\non seven semantic textual similarity tasks show that our approach is more\neffective than competitive baselines. Our code and data are publicly available\nat the link: \\textcolor{blue}{\\url{https://github.com/RUCAIBox/DCLR}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Implicit Length Bias of Label Smoothing on Beam Search Decoding. (arXiv:2205.00659v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00659","description":"<p>Label smoothing is ubiquitously applied in Neural Machine Translation (NMT)\ntraining. While label smoothing offers a desired regularization effect during\nmodel training, in this paper we demonstrate that it nevertheless introduces\nlength biases in the beam search decoding procedure. Our analysis shows that\nlabel smoothing implicitly applies a length penalty term to output sequence,\ncausing a bias towards shorter translations. We also show that for a model\nfully optimized with label smoothing, translation length is implicitly upper\nbounded by a fixed constant independent of input. We verify our theory by\napplying a simple rectification function at inference time to restore the\nunbiased distributions from the label-smoothed model predictions. This\nrectification method led to consistent quality improvements on WMT\nEnglish-German, English-French, English-Czech and English-Chinese tasks, up to\n+0.3 BLEU at beam size 4 and +2.8 BLEU at beam size 200.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Bowen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Task Effects in Human Reading with Neural Attention. (arXiv:1808.00054v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1808.00054","description":"<p>Humans read by making a sequence of fixations and saccades. They often skip\nwords, without apparent detriment to understanding. We offer a novel\nexplanation for skipping: readers optimize a tradeoff between performing a\nlanguage-related task and fixating as few words as possible. We propose a\nneural architecture that combines an attention module (deciding whether to skip\nwords) and a task module (memorizing the input). We show that our model\npredicts human skipping behavior, while also modeling reading times well, even\nthough it skips 40% of the input. A key prediction of our model is that\ndifferent reading tasks should result in different skipping behaviors. We\nconfirm this prediction in an eye-tracking experiment in which participants\nanswers questions about a text. We are able to capture these experimental\nresults using the our model, replacing the memorization module with a task\nmodule that performs neural question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">Michael Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Entity Disambiguation with BERT. (arXiv:1909.00426v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1909.00426","description":"<p>We propose a global entity disambiguation (ED) model based on BERT. To\ncapture global contextual information for ED, our model treats not only words\nbut also entities as input tokens, and solves the task by sequentially\nresolving mentions to their referent entities and using resolved entities as\ninputs at each step. We train the model using a large entity-annotated corpus\nobtained from Wikipedia. We achieve new state-of-the-art results on five\nstandard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The\nsource code and model checkpoint are available at\nhttps://github.com/studio-ousia/luke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washio_K/0/1/0/all/0/1\">Koki Washio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shindo_H/0/1/0/all/0/1\">Hiroyuki Shindo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1\">Yuji Matsumoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational reasoning and generalization using non-symbolic neural networks. (arXiv:2006.07968v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.07968","description":"<p>The notion of equality (identity) is simple and ubiquitous, making it a key\ncase study for broader questions about the representations supporting abstract\nrelational reasoning. Previous work suggested that neural networks were not\nsuitable models of human relational reasoning because they could not represent\nmathematically identity, the most basic form of equality. We revisit this\nquestion. In our experiments, we assess out-of-sample generalization of\nequality using both arbitrary representations and representations that have\nbeen pretrained on separate tasks to imbue them with structure. We find neural\nnetworks are able to learn (1) basic equality (mathematical identity), (2)\nsequential equality problems (learning ABA-patterned sequences) with only\npositive training instances, and (3) a complex, hierarchical equality problem\nwith only basic equality training instances (\"zero-shot'\" generalization). In\nthe two latter cases, our models perform tasks proposed in previous work to\ndemarcate human-unique symbolic abilities. These results suggest that essential\naspects of symbolic reasoning can emerge from data-driven, non-symbolic\nlearning processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Atticus Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carstensen_A/0/1/0/all/0/1\">Alexandra Carstensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_M/0/1/0/all/0/1\">Michael C. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring COVID-19 Biological Pathways from Clinical Phenotypes via Topological Analysis. (arXiv:2101.07417v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.07417","description":"<p>COVID-19 has caused thousands of deaths around the world and also resulted in\na large international economic disruption. Identifying the pathways associated\nwith this illness can help medical researchers to better understand the\nproperties of the condition. This process can be carried out by analyzing the\nmedical records. It is crucial to develop tools and models that can aid\nresearchers with this process in a timely manner. However, medical records are\noften unstructured clinical notes, and this poses significant challenges to\ndeveloping the automated systems. In this article, we propose a pipeline to aid\npractitioners in analyzing clinical notes and revealing the pathways associated\nwith this disease. Our pipeline relies on topological properties and consists\nof three steps: 1) pre-processing the clinical notes to extract the salient\nconcepts, 2) constructing a feature space of the patients to characterize the\nextracted concepts, and finally, 3) leveraging the topological properties to\ndistill the available knowledge and visualize the result. Our experiments on a\npublicly available dataset of COVID-19 clinical notes testify that our pipeline\ncan indeed extract meaningful pathways.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_N/0/1/0/all/0/1\">Negin Karisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platt_D/0/1/0/all/0/1\">Daniel E. Platt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Saugata Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parida_L/0/1/0/all/0/1\">Laxmi Parida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Graph Convolutional Networks for Text Classification. (arXiv:2102.09604v3 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2102.09604","description":"<p>Graph convolutional networks (GCNs) are a powerful architecture for\nrepresentation learning on documents that naturally occur as graphs, e.g.,\ncitation or social networks. However, sensitive personal information, such as\ndocuments with people's profiles or relationships as edges, are prone to\nprivacy leaks, as the trained model might reveal the original input. Although\ndifferential privacy (DP) offers a well-founded privacy-preserving framework,\nGCNs pose theoretical and practical challenges due to their training specifics.\nWe address these challenges by adapting differentially-private gradient-based\ntraining to GCNs and conduct experiments using two optimizers on five NLP\ndatasets in two languages. We propose a simple yet efficient method based on\nrandom graph splits that not only improves the baseline privacy bounds by a\nfactor of 2.7 while retaining competitive F1 scores, but also provides strong\nprivacy guarantees of epsilon = 1.0. We show that, under certain modeling\nchoices, privacy-preserving GCNs perform up to 90% of their non-private\nvariants, while formally guaranteeing strong privacy measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Igamberdiev_T/0/1/0/all/0/1\">Timour Igamberdiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Cues and Error Correction for Translation Robustness. (arXiv:2103.07352v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07352","description":"<p>Neural Machine Translation models are sensitive to noise in the input texts,\nsuch as misspelled words and ungrammatical constructions. Existing robustness\ntechniques generally fail when faced with unseen types of noise and their\nperformance degrades on clean texts. In this paper, we focus on three types of\nrealistic noise that are commonly generated by humans and introduce the idea of\nvisual context to improve translation robustness for noisy texts. In addition,\nwe describe a novel error correction training regime that can be used as an\nauxiliary task to further improve translation robustness. Experiments on\nEnglish-French and English-German translation show that both multimodal and\nerror correction components improve model robustness to noisy texts, while\nstill retaining translation quality on clean texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervising Model Attention with Human Explanations for Robust Natural Language Inference. (arXiv:2104.08142v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08142","description":"<p>Natural Language Inference (NLI) models are known to learn from biases and\nartefacts within their training data, impacting how well they generalise to\nother unseen datasets. Existing de-biasing approaches focus on preventing the\nmodels from learning these biases, which can result in restrictive models and\nlower performance. We instead investigate teaching the model how a human would\napproach the NLI task, in order to learn features that will generalise better\nto previously unseen examples. Using natural language explanations, we\nsupervise the model's attention weights to encourage more attention to be paid\nto the words present in the explanations, significantly improving model\nperformance. Our experiments show that the in-distribution improvements of this\nmethod are also accompanied by out-of-distribution improvements, with the\nsupervised models learning from features that generalise better to other NLI\ndatasets. Analysis of the model indicates that human explanations encourage\nincreased attention on the important words, with more attention paid to words\nin the premise and less attention paid to punctuation and stop-words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stacey_J/0/1/0/all/0/1\">Joe Stacey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08043","description":"<p>The vast majority of existing methods and systems for causal inference assume\nthat all variables under consideration are categorical or numerical (e.g.,\ngender, price, blood pressure, enrollment). In this paper, we present\nCausalNLP, a toolkit for inferring causality from observational data that\nincludes text in addition to traditional numerical and categorical variables.\nCausalNLP employs the use of meta-learners for treatment effect estimation and\nsupports using raw text and its linguistic properties as both a treatment and a\n\"controlled-for\" variable (e.g., confounder). The library is open-source and\navailable at: https://github.com/amaiya/causalnlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Do You Get When You Cross Beam Search with Nucleus Sampling?. (arXiv:2107.09729v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.09729","description":"<p>We combine beam search with the probabilistic pruning technique of nucleus\nsampling to create two deterministic nucleus search algorithms for natural\nlanguage generation. The first algorithm, p-exact search, locally prunes the\nnext-token distribution and performs an exact search over the remaining space.\nThe second algorithm, dynamic beam search, shrinks and expands the beam size\naccording to the entropy of the candidate's probability distribution. Despite\nthe probabilistic intuition behind nucleus search, experiments on machine\ntranslation and summarization benchmarks show that both algorithms reach the\nsame performance levels as standard beam search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMChat: Multi-Modal Chat Dataset on Social Media. (arXiv:2108.07154v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07154","description":"<p>Incorporating multi-modal contexts in conversation is important for\ndeveloping more engaging dialogue systems. In this work, we explore this\ndirection by introducing MMChat: a large-scale Chinese multi-modal dialogue\ncorpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous\ncorpora that are crowd-sourced or collected from fictitious movies, MMChat\ncontains image-grounded dialogues collected from real conversations on social\nmedia, in which the sparsity issue is observed. Specifically, image-initiated\ndialogues in common communications may deviate to some non-image-grounded\ntopics as the conversation proceeds. To better investigate this issue, we\nmanually annotate 100K dialogues from MMChat and further filter the corpus\naccordingly, which yields MMChat-hf. We develop a benchmark model to address\nthe sparsity issue in dialogue generation tasks by adapting the attention\nrouting mechanism on image features. Experiments demonstrate the usefulness of\nincorporating image features and the effectiveness of handling the sparsity of\nimage features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08614","description":"<p>Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but such systems barely\nutilize semantic data and knowledge. This paper presents the first QA system\nthat can seamlessly operate over RDF datasets and text corpora, or both\ntogether, in a unified framework. Our method, called UNIQORN, builds a context\ngraph on-the-fly, by retrieving question-relevant evidences from the RDF data\nand/or a text corpus, using fine-tuned BERT models. The resulting graph is\ntypically rich but highly noisy. UNIQORN copes with this input by a graph\nalgorithm for Group Steiner Trees, that identifies the best answer candidates\nin the context graph. Experimental results on several benchmarks of complex\nquestions with multiple entities and relations, show that UNIQORN significantly\noutperforms state-of-the-art methods for QA over heterogeneous sources. The\ngraph-based methodology provides user-interpretable evidence for the complete\nanswering process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented Dialogue Systems. (arXiv:2109.04919v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04919","description":"<p>The ability to recognise emotions lends a conversational artificial\nintelligence a human touch. While emotions in chit-chat dialogues have received\nsubstantial attention, emotions in task-oriented dialogues remain largely\nunaddressed. This is despite emotions and dialogue success having equally\nimportant roles in a natural system. Existing emotion-annotated task-oriented\ncorpora are limited in size, label richness, and public availability, creating\na bottleneck for downstream tasks. To lay a foundation for studies on emotions\nin task-oriented dialogues, we introduce EmoWOZ, a large-scale manually\nemotion-annotated corpus of task-oriented dialogues. EmoWOZ is based on\nMultiWOZ, a multi-domain task-oriented dialogue dataset. It contains more than\n11K dialogues with more than 83K emotion annotations of user utterances. In\naddition to Wizard-of-Oz dialogues from MultiWOZ, we collect human-machine\ndialogues within the same set of domains to sufficiently cover the space of\nvarious emotions that can happen during the lifetime of a data-driven dialogue\nsystem. To the best of our knowledge, this is the first large-scale open-source\ncorpus of its kind. We propose a novel emotion labelling scheme, which is\ntailored to task-oriented dialogues. We report a set of experimental results to\nshow the usability of this corpus for emotion recognition and state tracking in\ntask-oriented dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction. (arXiv:2109.12093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12093","description":"<p>Stepping from sentence-level to document-level, the research on relation\nextraction (RE) confronts increasing text length and more complicated entity\ninteractions. Consequently, it is more challenging to encode the key\ninformation sources--relevant contexts and entity types. However, existing\nmethods only implicitly learn to model these critical information sources while\nbeing trained for RE. As a result, they suffer the problems of ineffective\nsupervision and uninterpretable model predictions. In contrast, we propose to\nexplicitly teach the model to capture relevant contexts and entity types by\nsupervising and augmenting intermediate steps (SAIS) for RE. Based on a broad\nspectrum of carefully designed tasks, our proposed SAIS method not only\nextracts relations of better quality due to more effective supervision, but\nalso retrieves the corresponding supporting evidence more accurately so as to\nenhance interpretability. By assessing model uncertainty, SAIS further boosts\nthe performance via evidence-based data augmentation and ensemble inference\nwhile reducing the computational cost. Eventually, SAIS delivers\nstate-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and\noutperforms the runner-up by 5.04% relatively in F1 score in evidence retrieval\non DocRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yuxin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zecheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skill Induction and Planning with Latent Language. (arXiv:2110.01517v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.01517","description":"<p>We present a framework for learning hierarchical policies from\ndemonstrations, using sparse natural language annotations to guide the\ndiscovery of reusable skills for autonomous decision-making. We formulate a\ngenerative model of action sequences in which goals generate sequences of\nhigh-level subtask descriptions, and these descriptions generate sequences of\nlow-level actions. We describe how to train this model using primarily\nunannotated demonstrations by parsing demonstrations into sequences of named\nhigh-level subtasks, using only a small number of seed annotations to ground\nlanguage in action. In trained models, natural language commands index a\ncombinatorial library of skills; agents can use these skills to plan by\ngenerating high-level instruction sequences tailored to novel goals. We\nevaluate this approach in the ALFRED household simulation environment,\nproviding natural language annotations for only 10% of demonstrations. It\nachieves task completion rates comparable to state-of-the-art models\n(outperforming several recent methods with access to ground-truth plans during\ntraining and evaluation) while providing structured and human-readable\nhigh-level plans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pratyusha Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Language Learning for Entity Matching. (arXiv:2110.03338v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03338","description":"<p>Transformer-based entity matching methods have significantly moved the state\nof the art for less-structured matching tasks such as matching product offers\nin e-commerce. In order to excel at these tasks, Transformer-based matching\nmethods require a decent amount of training pairs. Providing enough training\ndata can be challenging, especially if a matcher for non-English product\ndescriptions should be learned. This poster explores along the use case of\nmatching product offers from different e-shops to which extent it is possible\nto improve the performance of Transformer-based matchers by complementing a\nsmall set of training pairs in the target language, German in our case, with a\nlarger set of English-language training pairs. Our experiments using different\nTransformers show that extending the German set with English pairs improves the\nmatching performance in all cases. The impact of adding the English pairs is\nespecially high in low-resource settings in which only a rather small number of\nnon-English pairs is available. As it is often possible to automatically gather\nEnglish training pairs from the Web by exploiting schema.org annotations, our\nresults are relevant for many product matching scenarios targeting low-resource\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peeters_R/0/1/0/all/0/1\">Ralph Peeters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizer_C/0/1/0/all/0/1\">Christian Bizer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training. (arXiv:2110.07731v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07731","description":"<p>With the rise of large-scale pre-trained language models, open-domain\nquestion-answering (ODQA) has become an important research topic in NLP. Based\non the popular pre-training fine-tuning approach, we posit that an additional\nin-domain pre-training stage using a large-scale, natural, and diverse\nquestion-answering (QA) dataset can be beneficial for ODQA. Consequently, we\npropose a novel QA dataset based on the Common Crawl project in this paper.\nUsing the readily available schema.org annotation, we extract around 130\nmillion multilingual question-answer pairs, including about 60 million English\ndata-points. With this previously unseen number of natural QA pairs, we\npre-train popular language models to show the potential of large-scale\nin-domain pre-training for the task of question-answering. In our experiments,\nwe find that pre-training question-answering models on our Common Crawl\nQuestion Answering dataset (CCQA) achieves promising results in zero-shot, low\nresource and fine-tuned settings across multiple tasks, models and benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1\">Patrick Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Pattern based Black-box Model Watermarking for Automatic Speech Recognition. (arXiv:2110.09814v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.09814","description":"<p>As an effective method for intellectual property (IP) protection, model\nwatermarking technology has been applied on a wide variety of deep neural\nnetworks (DNN), including speech classification models. However, how to design\na black-box watermarking scheme for automatic speech recognition (ASR) models\nis still an unsolved problem, which is a significant demand for protecting\nremote ASR Application Programming Interface (API) deployed in cloud servers.\nDue to conditional independence assumption and label-detection-based evasion\nattack risk of ASR models, the black-box model watermarking scheme for speech\nclassification models cannot apply to ASR models. In this paper, we propose the\nfirst black-box model watermarking framework for protecting the IP of ASR\nmodels. Specifically, we synthesize trigger audios by spreading the speech\nclips of model owners over the entire input audios and labeling the trigger\naudios with the stego texts, which hides the authorship information with\nlinguistic steganography. Experiments on the state-of-the-art open-source ASR\nsystem DeepSpeech demonstrate the feasibility of the proposed watermarking\nscheme, which is robust against five kinds of attacks and has little impact on\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haozhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kejiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep convolutional forest: a dynamic deep ensemble approach for spam detection in text. (arXiv:2110.15718v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15718","description":"<p>The increase in people's use of mobile messaging services has led to the\nspread of social engineering attacks like phishing, considering that spam text\nis one of the main factors in the dissemination of phishing attacks to steal\nsensitive data such as credit cards and passwords. In addition, rumors and\nincorrect medical information regarding the COVID-19 pandemic are widely shared\non social media leading to people's fear and confusion. Thus, filtering spam\ncontent is vital to reduce risks and threats. Previous studies relied on\nmachine learning and deep learning approaches for spam classification, but\nthese approaches have two limitations. Machine learning models require manual\nfeature engineering, whereas deep neural networks require a high computational\ncost. This paper introduces a dynamic deep ensemble model for spam detection\nthat adjusts its complexity and extracts features automatically. The proposed\nmodel utilizes convolutional and pooling layers for feature extraction along\nwith base classifiers such as random forests and extremely randomized trees for\nclassifying texts into spam or legitimate ones. Moreover, the model employs\nensemble learning procedures like boosting and bagging. As a result, the model\nachieved high precision, recall, f1-score and accuracy of 98.38\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaaban_M/0/1/0/all/0/1\">Mai A. Shaaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_Y/0/1/0/all/0/1\">Yasser F. Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guirguis_S/0/1/0/all/0/1\">Shawkat K. Guirguis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calculating Question Similarity is Enough: A New Method for KBQA Tasks. (arXiv:2111.07658v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07658","description":"<p>Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with the help of an external knowledge base. The core idea is to find\nthe link between the internal knowledge behind questions and known triples of\nthe knowledge base. Traditional KBQA task pipelines contain several steps,\nincluding entity recognition, entity linking, answering selection, etc. In this\nkind of pipeline methods, errors in any procedure will inevitably propagate to\nthe final prediction. To address this challenge, this paper proposes a Corpus\nGeneration - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for\nthe KBQA task. The major novelty lies in the design of the new method, wherein\nour approach, the knowledge enhanced T5 (kT5) model aims to generate natural\nlanguage QA pairs based on Knowledge Graph triples and directly solve the QA by\nretrieving the synthetic dataset. The new method can extract more information\nabout the entities from PLM to improve accuracy and simplify the processes. We\ntest our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that\nour method improves the performance of KBQA and the out straight-forward method\nis competitive with the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Ledell Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combined Scaling for Open-Vocabulary Image Classification. (arXiv:2111.10050v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.10050","description":"<p>We present a combined scaling method - named BASIC - that achieves 85.7%\ntop-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from\nany labeled ImageNet example. This accuracy surpasses best published similar\nmodels - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant\nimprovements in robustness benchmarks. For instance, on 5 test sets with\nnatural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our\nmodel achieves 84.3% top-1 average accuracy, only a small drop from its\noriginal ImageNet accuracy.\n</p>\n<p>To achieve these results, we scale up the contrastive learning framework of\nCLIP and ALIGN in three dimensions: data size, model size, and batch size. Our\ndataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x\nlarger than CLIP. Our largest model has 3B weights, which is 3.75x larger in\nparameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size\nis 65536 which is 2x more than CLIP and 4x more than ALIGN.\n</p>\n<p>We encountered two main challenges with the scaling rules of BASIC. First,\nthe main challenge with implementing the combined scaling rules of BASIC is the\nlimited memory of accelerators, such as GPUs and TPUs. To overcome the memory\nlimit, we propose two simple methods which make use of gradient checkpointing\nand model parallelism. Second, while increasing the dataset size and the model\nsize has been the defacto method to improve the performance of deep learning\nmodels like BASIC, the effect of a large contrastive batch size on such\ncontrastive-trained image-text models is not well-understood. To shed light on\nthe benefits of large contrastive batch sizes, we develop a theoretical\nframework which shows that larger contrastive batch sizes lead to smaller\ngeneralization gaps for image-text models such as BASIC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering. (arXiv:2112.02732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.02732","description":"<p>Existing KG-augmented models for commonsense question answering primarily\nfocus on designing elaborate Graph Neural Networks (GNNs) to model knowledge\ngraphs (KGs). However, they ignore (i) the effectively fusing and reasoning\nover question context representations and the KG representations, and (ii)\nautomatically selecting relevant nodes from the noisy KGs during reasoning. In\nthis paper, we propose a novel model, JointLK, which solves the above\nlimitations through the joint reasoning of LM and GNN and the dynamic KGs\npruning mechanism. Specifically, JointLK performs joint reasoning between LM\nand GNN through a novel dense bidirectional attention module, in which each\nquestion token attends on KG nodes and each KG node attends on question tokens,\nand the two modal representations fuse and update mutually by multi-step\ninteractions. Then, the dynamic pruning module uses the attention weights\ngenerated by joint reasoning to prune irrelevant KG nodes recursively. We\nevaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate\nits improvements to the existing LM and LM+KG models, as well as its capability\nto perform interpretable reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yueqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Le Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Context-Word Biases in Lexical Semantic Datasets. (arXiv:2112.06733v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06733","description":"<p>State-of-the-art contextualized models eg. BERT use tasks such as WiC and WSD\nto evaluate their word-in-context representations. This inherently assumes that\nperformance in these tasks reflect how well a model represents the coupled word\nand context semantics. We question this assumption by presenting the first\nquantitative analysis on the context-word interaction required and being tested\nin major contextual lexical semantic tasks, taking into account that tasks can\nbe inherently biased and models can learn spurious correlations from datasets.\nTo achieve this, we run probing baselines on masked input, based on which we\nthen propose measures to calculate the degree of context or word biases in a\ndataset, and plot existing datasets on a continuum. The analysis were performed\non both models and humans to decouple biases inherent to the tasks and biases\nlearned from the datasets. We found that, (1) to models, most existing datasets\nfall into the extreme ends of the continuum: the retrieval-based tasks and\nespecially the ones in the medical domain (eg. COMETA) exhibit strong target\nword bias while WiC-style tasks and WSD show strong context bias; (2) AM2iCo\nand Sense Retrieval show less extreme model biases and challenge a model more\nto represent both the context and target words. (3) A similar trend of biases\nexists in humans but humans are much less biased compared with models as humans\nfound semantic judgments more difficult with the masked input, indicating\nmodels are learning spurious correlations. This study demonstrates that with\nheavy context or target word biases, models are usually not being tested for\nword-in-context representations as such in these tasks and results are\ntherefore open to misinterpretation. We recommend our framework as a sanity\ncheck for context and target word biases in future task design and model\ninterpretation in lexical semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_D/0/1/0/all/0/1\">Diana McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework. (arXiv:2112.07522v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07522","description":"<p>Vast efforts have been devoted to creating high-performance few-shot\nlearners, i.e., large-scale pretrained language models (PLMs) that perform well\nwith little downstream task training data. Training PLMs has incurred\nsignificant cost, but utilizing the few-shot learners is still challenging due\nto their enormous size. This work focuses on a crucial question: How to make\neffective use of these few-shot learners? We propose LMTurk, a novel approach\nthat treats few-shot learners as crowdsourcing workers. The rationale is that\ncrowdsourcing workers are in fact few-shot learners: They are shown a few\nillustrative examples to learn about a task and then start annotating. LMTurk\nemploys few-shot learners built upon PLMs as workers. We show that the\nresulting annotations can be utilized to train models that solve the task well\nand are small enough to be deployable in practical scenarios. Active learning\nis integrated into LMTurk to reduce the amount of queries made to PLMs,\nminimizing the computational cost of running PLM inference passes. Altogether,\nLMTurk is an important step towards making effective use of current PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengjie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minglei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning. (arXiv:2112.08558v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08558","description":"<p>Compared to standard retrieval tasks, passage retrieval for conversational\nquestion answering (CQA) poses new challenges in understanding the current user\nquestion, as each question needs to be interpreted within the dialogue context.\nMoreover, it can be expensive to re-train well-established retrievers such as\nsearch engines that are originally developed for non-conversational queries. To\nfacilitate their use, we develop a query rewriting model CONQRR that rewrites a\nconversational question in the context into a standalone question. It is\ntrained with a novel reward function to directly optimize towards retrieval\nusing reinforcement learning and can be adapted to any off-the-shelf retriever.\nWe show that CONQRR achieves state-of-the-art results on a recent open-domain\nCQA dataset containing conversations from three different sources, and is\neffective for two different off-the-shelf retrievers. Our extensive analysis\nalso shows the robustness of CONQRR to out-of-domain dialogues as well as to\nzero query rewriting supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1\">Hannah Rashkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1\">Gaurav Singh Tomar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Korean-Specific Dataset for Table Question Answering. (arXiv:2201.06223v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06223","description":"<p>Existing question answering systems mainly focus on dealing with text data.\nHowever, much of the data produced daily is stored in the form of tables that\ncan be found in documents and relational databases, or on the web. To solve the\ntask of question answering over tables, there exist many datasets for table\nquestion answering written in English, but few Korean datasets. In this paper,\nwe demonstrate how we construct Korean-specific datasets for table question\nanswering: Korean tabular dataset is a collection of 1.4M tables with\ncorresponding descriptions for unsupervised pre-training language models.\nKorean table question answering corpus consists of 70k pairs of questions and\nanswers created by crowd-sourced workers. Subsequently, we then build a\npre-trained language model based on Transformer and fine-tune the model for\ntable question answering with these datasets. We then report the evaluation\nresults of our model. We make our datasets publicly available via our GitHub\nrepository and hope that those datasets will help further studies for question\nanswering over tables, and for the transformation of table formats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jun_C/0/1/0/all/0/1\">Changwook Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_M/0/1/0/all/0/1\">Myoseop Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hansol Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1\">Kyungkoo Min</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable N-gram Objective on Abstractive Summarization. (arXiv:2202.04003v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.04003","description":"<p>ROUGE is a standard automatic evaluation metric based on n-grams for\nsequence-to-sequence tasks, while cross-entropy loss is an essential objective\nof neural network language model that optimizes at a unigram level. We present\ndifferentiable n-gram objectives, attempting to alleviate the discrepancy\nbetween training criterion and evaluating criterion. The objective maximizes\nthe probabilistic weight of matched sub-sequences, and the novelty of our work\nis the objective weights the matched sub-sequences equally and does not ceil\nthe number of matched sub-sequences by the ground truth count of n-grams in\nreference sequence. We jointly optimize cross-entropy loss and the proposed\nobjective, providing decent ROUGE score enhancement over abstractive\nsummarization dataset CNN/DM and XSum, outperforming alternative n-gram\nobjectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST-MoE: Designing Stable and Transferable Sparse Expert Models. (arXiv:2202.08906v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08906","description":"<p>Scale has opened new frontiers in natural language processing -- but at a\nhigh cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have\nbeen proposed as an energy efficient path to even larger and more capable\nlanguage models. But advancing the state-of-the-art across a broad set of\nnatural language tasks has been hindered by training instabilities and\nuncertain quality during fine-tuning. Our work focuses on these issues and acts\nas a design guide. We conclude by scaling a sparse model to 269B parameters,\nwith a computational cost comparable to a 32B dense encoder-decoder Transformer\n(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,\na sparse model achieves state-of-the-art performance in transfer learning,\nacross a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC\nChallenge), summarization (XSum, CNN-DM), closed book question answering\n(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,\nANLI R3).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_I/0/1/0/all/0/1\">Irwan Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sameer Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.12163","description":"<p>In this paper, we introduce a novel language identification system based on\nconformer layers. We propose an attentive temporal pooling mechanism to allow\nthe model to carry information in long-form audio via a recurrent form, such\nthat the inference can be performed in a streaming fashion. Additionally, we\ninvestigate two domain adaptation approaches to allow adapting an existing\nlanguage identification model without retraining the model parameters for a new\ndomain. We perform a comparative study of different model topologies under\ndifferent constraints of model size, and find that conformer-based models\nsignificantly outperform LSTM and transformer based models. Our experiments\nalso show that attentive temporal pooling and domain adaptation improve model\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yiling Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition. (arXiv:2203.00545v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00545","description":"<p>The MultiCoNER shared task aims at detecting semantically ambiguous and\ncomplex named entities in short and low-context settings for multiple\nlanguages. The lack of contexts makes the recognition of ambiguous named\nentities challenging. To alleviate this issue, our team DAMO-NLP proposes a\nknowledge-based system, where we build a multilingual knowledge base based on\nWikipedia to provide related context information to the named entity\nrecognition (NER) model. Given an input sentence, our system effectively\nretrieves related contexts from the knowledge base. The original input\nsentences are then augmented with such context information, allowing\nsignificantly better contextualized token representations to be captured. Our\nsystem wins 10 out of 13 tracks in the MultiCoNER shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization. (arXiv:2203.08257v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08257","description":"<p>The IMPRESSIONS section of a radiology report about an imaging study is a\nsummary of the radiologist's reasoning and conclusions, and it also aids the\nreferring physician in confirming or excluding certain diagnoses. A cascade of\ntasks are required to automatically generate an abstractive summary of the\ntypical information-rich radiology report. These tasks include acquisition of\nsalient content from the report and generation of a concise, easily consumable\nIMPRESSIONS section. Prior research on radiology report summarization has\nfocused on single-step end-to-end models -- which subsume the task of salient\ncontent acquisition. To fully explore the cascade structure and explainability\nof radiology report summarization, we introduce two innovations. First, we\ndesign a two-step approach: extractive summarization followed by abstractive\nsummarization. Second, we additionally break down the extractive part into two\nindependent tasks: extraction of salient (1) sentences and (2) keywords.\nExperiments on English radiology reports from two clinical sites show our novel\napproach leads to a more precise summary compared to single-step and to\ntwo-step-with-single-extractive-process baselines with an overall improvement\nin F1 score Of 3-4%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karn_S/0/1/0/all/0/1\">Sanjeev Kumar Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuetze_H/0/1/0/all/0/1\">Hinrich Schuetze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farri_O/0/1/0/all/0/1\">Oladimeji Farri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?. (arXiv:2203.08850v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08850","description":"<p>What can pre-trained multilingual sequence-to-sequence models like mBART\ncontribute to translating low-resource languages? We conduct a thorough\nempirical experiment in 10 languages to ascertain this, considering five\nfactors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning\ndata, (3) the amount of pre-training data in the model, (4) the impact of\ndomain mismatch, and (5) language typology. In addition to yielding several\nheuristics, the experiments form a framework for evaluating the data\nsensitivities of machine translation systems. While mBART is robust to domain\ndifferences, its translations for unseen and typologically distant languages\nremain below 3.0 BLEU. In answer to our title's question, mBART is not a\nlow-resource panacea; we therefore encourage shifting the emphasis from new\nmodels to new data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">En-Shiun Annie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thillainathan_S/0/1/0/all/0/1\">Sarubi Thillainathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Shravan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruisi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_A/0/1/0/all/0/1\">Arya D. McCarthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRUE: Re-evaluating Factual Consistency Evaluation. (arXiv:2204.04991v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04991","description":"<p>Grounded text generation systems often generate text that contains factual\ninconsistencies, hindering their real-world applicability. Automatic factual\nconsistency evaluation may help alleviate this limitation by accelerating\nevaluation cycles, filtering inconsistent outputs and augmenting training data.\nWhile attracting increasing attention, such evaluation metrics are usually\ndeveloped and evaluated in silo for a single task or dataset, slowing their\nadoption. Moreover, previous meta-evaluation protocols focused on system-level\ncorrelations with human annotations, which leave the example-level accuracy of\nsuch metrics unclear. In this work, we introduce TRUE: a comprehensive survey\nand assessment of factual consistency metrics on a standardized collection of\nexisting texts from diverse tasks, manually annotated for factual consistency.\nOur standardization enables an example-level meta-evaluation protocol that is\nmore actionable and interpretable than previously reported correlations,\nyielding clearer quality measures. Across diverse state-of-the-art metrics and\n11 datasets we find that large-scale NLI and question\ngeneration-and-answering-based approaches achieve strong and complementary\nresults. We recommend those methods as a starting point for model and metric\ndevelopers, and hope TRUE will foster progress towards even better evaluation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honovich_O/0/1/0/all/0/1\">Or Honovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taitelbaum_H/0/1/0/all/0/1\">Hagai Taitelbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukliansy_D/0/1/0/all/0/1\">Doron Kukliansy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_V/0/1/0/all/0/1\">Vered Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1\">Avinatan Hassidim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation. (arXiv:2204.05953v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05953","description":"<p>Sign language recognition and translation first uses a recognition module to\ngenerate glosses from sign language videos and then employs a translation\nmodule to translate glosses into spoken sentences. Most existing works focus on\nthe recognition step, while paying less attention to sign language translation.\nIn this work, we propose a task-aware instruction network, namely TIN-SLT, for\nsign language translation, by introducing the instruction module and the\nlearning-based feature fuse strategy into a Transformer network. In this way,\nthe pre-trained model's language ability can be well explored and utilized to\nfurther boost the translation performance. Moreover, by exploring the\nrepresentation space of sign language glosses and target spoken language, we\npropose a multi-level data augmentation scheme to adjust the data distribution\nof the training set. We conduct extensive experiments on two challenging\nbenchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method\noutperforms former best solutions by 1.65 and 1.42 in terms of BLEU-4. Our code\nis published at https://github.com/yongcaoplus/TIN-SLT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Long Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengdao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kai_H/0/1/0/all/0/1\">Hwang Kai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Adapt Domain Shifts of Moral Values via Instance Weighting. (arXiv:2204.07603v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07603","description":"<p>Classifying moral values in user-generated text from social media is critical\nin understanding community cultures and interpreting user behaviors of social\nmovements. Moral values and language usage can change across the social\nmovements; however, text classifiers are usually trained in source domains of\nexisting social movements and tested in target domains of new social issues\nwithout considering the variations. In this study, we examine domain shifts of\nmoral values and language usage, quantify the effects of domain shifts on the\nmorality classification task, and propose a neural adaptation framework via\ninstance weighting to improve cross-domain classification tasks. The\nquantification analysis suggests a strong correlation between morality shifts,\nlanguage usage, and classification performance. We evaluate the neural\nadaptation framework on a public Twitter data across 7 social movements and\ngain classification improvements up to 12.1\\%. Finally, we release a new data\nof the COVID-19 vaccine labeled with moral values and evaluate our approach on\nthe new target domain. For the case study of the COVID-19 vaccine, our\nadaptation framework achieves up to 5.26\\% improvements over neural baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wormley_A/0/1/0/all/0/1\">Alexandra Wormley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Adam Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks. (arXiv:2204.07705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07705","description":"<p>How can we measure the generalization of models to a variety of unseen tasks\nwhen provided with their language instructions? To facilitate progress in this\ngoal, we introduce Natural-Instructions v2, a benchmark of 1,600+ diverse\nlanguage tasks and their expert-written instructions. It covers 70+ distinct\ntask types, such as tagging, in-filling, and rewriting. These tasks are\ncollected with contributions of NLP practitioners in the community and through\nan iterative peer review process to ensure their quality. With this large and\ndiverse collection of tasks, we are able to rigorously benchmark cross-task\ngeneralization of models -- training on a subset of tasks and evaluating on the\nremaining unseen ones. For instance, we quantify generalization as a function\nof various scaling parameters, such as the number of observed tasks, the number\nof instances, and model sizes. Based on these insights, we introduce\nTk-Instruct, an encoder-decoder Transformer that is trained to follow a variety\nof in-context instructions (plain language task definitions or k-shot examples)\nwhich outperforms existing larger models on our benchmark. We hope this\nbenchmark facilitates future progress toward more general-purpose language\nunderstanding models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1\">Pegah Alipoormolabashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordi_Y/0/1/0/all/0/1\">Yeganeh Kordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1\">Amirreza Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">Anjana Arunkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1\">Arjun Ashok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhanasekaran_A/0/1/0/all/0/1\">Arut Selvan Dhanasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Atharva Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stap_D/0/1/0/all/0/1\">David Stap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_E/0/1/0/all/0/1\">Eshaan Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamanolakis_G/0/1/0/all/0/1\">Giannis Karamanolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Haizhi Gary Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_I/0/1/0/all/0/1\">Ishan Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1\">Ishani Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_J/0/1/0/all/0/1\">Jacob Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznia_K/0/1/0/all/0/1\">Kirby Kuznia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_K/0/1/0/all/0/1\">Krima Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Maitreya Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradshahi_M/0/1/0/all/0/1\">Mehrad Moradshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_M/0/1/0/all/0/1\">Mirali Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaza_P/0/1/0/all/0/1\">Phani Rohitha Kaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Pulkit Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ravsehaj Singh Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karia_R/0/1/0/all/0/1\">Rushang Karia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampat_S/0/1/0/all/0/1\">Shailaja Keyur Sampat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_S/0/1/0/all/0/1\">Savan Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Sujan Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patro_S/0/1/0/all/0/1\">Sumanta Patro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_T/0/1/0/all/0/1\">Tanay Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xudong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models. (arXiv:2204.08110v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08110","description":"<p>English pretrained language models, which make up the backbone of many modern\nNLP systems, require huge amounts of unlabeled training data. These models are\ngenerally presented as being trained only on English text but have been found\nto transfer surprisingly well to other languages. We investigate this\nphenomenon and find that common English pretraining corpora actually contain\nsignificant amounts of non-English text: even when less than 1% of data is not\nEnglish (well within the error rate of strong language classifiers), this leads\nto hundreds of millions of foreign language tokens in large-scale datasets. We\nthen demonstrate that even these small percentages of non-English data\nfacilitate cross-lingual transfer for models trained on them, with target\nlanguage performance strongly correlated to the amount of in-language data seen\nduring pretraining. In light of these findings, we argue that no model is truly\nmonolingual when pretrained at scale, which should be considered when\nevaluating cross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting. (arXiv:2204.11406v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11406","description":"<p>Self-augmentation has received increasing research interest recently to\nimprove named entity recognition (NER) performance in low-resource scenarios.\nToken substitution and mixup are two feasible heterogeneous self-augmentation\ntechniques for NER that can achieve effective performance with certain\nspecialized efforts. Noticeably, self-augmentation may introduce potentially\nnoisy augmented data. Prior research has mainly resorted to heuristic\nrule-based constraints to reduce the noise for specific self-augmentation\nmethods individually. In this paper, we revisit these two typical\nself-augmentation methods for NER, and propose a unified meta-reweighting\nstrategy for them to achieve a natural integration. Our method is easily\nextensible, imposing little effort on a specific self-augmentation method.\nExperiments on different Chinese and English NER benchmarks show that our token\nsubstitution and mixup method, as well as their integration, can achieve\neffective performance improvement. Based on the meta-reweighting mechanism, we\ncan enhance the advantages of the self-augmentation techniques without much\nextra effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Linzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chunping Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeaNF: Weak Supervision with Normalizing Flows. (arXiv:2204.13409v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13409","description":"<p>A popular approach to decrease the need for costly manual annotation of large\ndata sets is weak supervision, which introduces problems of noisy labels,\ncoverage and bias. Methods for overcoming these problems have either relied on\ndiscriminative models, trained with cost functions specific to weak\nsupervision, and more recently, generative models, trying to model the output\nof the automatic annotation process. In this work, we explore a novel direction\nof generative modeling for weak supervision: Instead of modeling the output of\nthe annotation process (the labeling function matches), we generatively model\nthe input-side data distributions (the feature space) covered by labeling\nfunctions. Specifically, we estimate a density for each weak labeling source,\nor labeling function, by using normalizing flows. An integral part of our\nmethod is the flow-based modeling of multiple simultaneously matching labeling\nfunctions, and therefore phenomena such as labeling function overlap and\ncorrelations are captured. We analyze the effectiveness and modeling\ncapabilities on various commonly used weak supervision data sets, and show that\nweakly supervised normalizing flows compare favorably to standard weak\nsupervision baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stephan_A/0/1/0/all/0/1\">Andreas Stephan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"My nose is running.\"\"Are you also coughing?\": Building A Medical Diagnosis Agent with Interpretable Inquiry Logics. (arXiv:2204.13953v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13953","description":"<p>With the rise of telemedicine, the task of developing Dialogue Systems for\nMedical Diagnosis (DSMD) has received much attention in recent years. Different\nfrom early researches that needed to rely on extra human resources and\nexpertise to help construct the system, recent researches focused on how to\nbuild DSMD in a purely data-driven manner. However, the previous data-driven\nDSMD methods largely overlooked the system interpretability, which is critical\nfor a medical application, and they also suffered from the data sparsity issue\nat the same time. In this paper, we explore how to bring interpretability to\ndata-driven DSMD. Specifically, we propose a more interpretable decision\nprocess to implement the dialogue manager of DSMD by reasonably mimicking real\ndoctors' inquiry logics, and we devise a model with highly transparent\ncomponents to conduct the inference. Moreover, we collect a new DSMD dataset,\nwhich has a much larger scale, more diverse patterns and is of higher quality\nthan the existing ones. The experiments show that our method obtains 7.7%,\n10.0%, 3.0% absolute improvement in diagnosis accuracy respectively on three\ndatasets, demonstrating the effectiveness of its rational decision process and\nmodel design. Our codes and the GMD-12 dataset are available at\nhttps://github.com/lwgkzl/BR-Agent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jianheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yafei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Language Models with Natural Language Feedback. (arXiv:2204.14146v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14146","description":"<p>Pretrained language models often do not perform tasks in ways that are in\nline with our preferences, e.g., generating offensive text or factually\nincorrect summaries. Recent work approaches the above issue by learning from a\nsimple form of human evaluation: comparisons between pairs of model-generated\ntask outputs. Comparison feedback conveys limited information about human\npreferences per human evaluation. Here, we propose to learn from natural\nlanguage feedback, which conveys more information per human evaluation. We\nlearn from language feedback on model outputs using a three-step learning\nalgorithm. First, we condition the language model on the initial output and\nfeedback to generate many refinements. Second, we choose the refinement with\nthe highest similarity to the feedback. Third, we finetune a language model to\nmaximize the likelihood of the chosen refinement given the input. In synthetic\nexperiments, we first evaluate whether language models accurately incorporate\nfeedback to produce refinements, finding that only large language models (175B\nparameters) do so. Using only 100 samples of human-written feedback, our\nlearning algorithm finetunes a GPT-3 model to roughly human-level\nsummarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_J/0/1/0/all/0/1\">Jon Ander Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jun Shern Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Birds' Eye View: Measuring Behavior and Posture of Chickens as a Metric for Their Well-Being. (arXiv:2205.00069v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00069","description":"<p>Chicken well-being is important for ensuring food security and better\nnutrition for a growing global human population. In this research, we represent\nbehavior and posture as a metric to measure chicken well-being. With the\nobjective of detecting chicken posture and behavior in a pen, we employ two\nalgorithms: Mask R-CNN for instance segmentation and YOLOv4 in combination with\nResNet50 for classification. Our results indicate a weighted F1 score of 88.46%\nfor posture and behavior detection using Mask R-CNN and an average of 91%\naccuracy in behavior detection and 86.5% average accuracy in posture detection\nusing YOLOv4. These experiments are conducted under uncontrolled scenarios for\nboth posture and behavior measurements. These metrics establish a strong\nfoundation to obtain a decent indication of individual and group behaviors and\npostures. Such outcomes would help improve the overall well-being of the\nchickens. The dataset used in this research is collected in-house and will be\nmade public after the publication as it would serve as a very useful resource\nfor future research. To the best of our knowledge no other research work has\nbeen conducted in this specific setup used for this work involving multiple\nbehaviors and postures simultaneously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joo_K/0/1/0/all/0/1\">Kevin Hyekang Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Shiyuan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weimer_S/0/1/0/all/0/1\">Shawna L. Weimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teli_M/0/1/0/all/0/1\">Mohammad Nayeem Teli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Negative Sampling for Audio-Visual Contrastive Learning from Movies. (arXiv:2205.00073v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00073","description":"<p>The abundance and ease of utilizing sound, along with the fact that auditory\nclues reveal a plethora of information about what happens in a scene, make the\naudio-visual space an intuitive choice for representation learning. In this\npaper, we explore the efficacy of audio-visual self-supervised learning from\nuncurated long-form content i.e movies. Studying its differences with\nconventional short-form content, we identify a non-i.i.d distribution of data,\ndriven by the nature of movies. Specifically, we find long-form content to\nnaturally contain a diverse set of semantic concepts (semantic diversity),\nwhere a large portion of them, such as main characters and environments often\nreappear frequently throughout the movie (reoccurring semantic concepts). In\naddition, movies often contain content-exclusive artistic artifacts, such as\ncolor palettes or thematic music, which are strong signals for uniquely\ndistinguishing a movie (non-semantic consistency). Capitalizing on these\nobservations, we comprehensively study the effect of emphasizing within-movie\nnegative sampling in a contrastive learning setup. Our view is different from\nthose of prior works who consider within-video positive sampling, inspired by\nthe notion of semantic persistency over time, and operate in a short-video\nregime. Our empirical findings suggest that, with certain modifications,\ntraining on uncurated long-form videos yields representations which transfer\ncompetitively with the state-of-the-art to a variety of action recognition and\naudio classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalayeh_M/0/1/0/all/0/1\">Mahdi M. Kalayeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardeshir_S/0/1/0/all/0/1\">Shervin Ardeshir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_N/0/1/0/all/0/1\">Nagendra Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrashekar_A/0/1/0/all/0/1\">Ashok Chandrashekar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Method to Boost Human Pose Estimation Accuracy by Correcting the Joint Regressor for the Human3.6m Dataset. (arXiv:2205.00076v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00076","description":"<p>Many human pose estimation methods estimate Skinned Multi-Person Linear\n(SMPL) models and regress the human joints from these SMPL estimates. In this\nwork, we show that the most widely used SMPL-to-joint linear layer (joint\nregressor) is inaccurate, which may mislead pose evaluation results. To achieve\na more accurate joint regressor, we propose a method to create\npseudo-ground-truth SMPL poses, which can then be used to train an improved\nregressor. Specifically, we optimize SMPL estimates coming from a\nstate-of-the-art method so that its projection matches the silhouettes of\nhumans in the scene, as well as the ground-truth 2D joint locations. While the\nquality of this pseudo-ground-truth is challenging to assess due to the lack of\nactual ground-truth SMPL, with the Human 3.6m dataset, we qualitatively show\nthat our joint locations are more accurate and that our regressor leads to\nimproved pose estimations results on the test set without any need for\nretraining. We release our code and joint regressor at\nhttps://github.com/ubc-vision/joint-regressor-refinement\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedlin_E/0/1/0/all/0/1\">Eric Hedlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Contrastive Learning based Transformer for Lung Nodule Detection. (arXiv:2205.00122v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00122","description":"<p>Early detection of lung nodules with computed tomography (CT) is critical for\nthe longer survival of lung cancer patients and better quality of life.\nComputer-aided detection/diagnosis (CAD) is proven valuable as a second or\nconcurrent reader in this context. However, accurate detection of lung nodules\nremains a challenge for such CAD systems and even radiologists due to not only\nthe variability in size, location, and appearance of lung nodules but also the\ncomplexity of lung structures. This leads to a high false-positive rate with\nCAD, compromising its clinical efficacy. Motivated by recent computer vision\ntechniques, here we present a self-supervised region-based 3D transformer model\nto identify lung nodules among a set of candidate regions. Specifically, a 3D\nvision transformer (ViT) is developed that divides a CT image volume into a\nsequence of non-overlap cubes, extracts embedding features from each cube with\nan embedding layer, and analyzes all embedding features with a self-attention\nmechanism for the prediction. To effectively train the transformer model on a\nrelatively small dataset, the region-based contrastive learning method is used\nto boost the performance by pre-training the 3D transformer with public CT\nimages. Our experiments show that the proposed method can significantly improve\nthe performance of lung nodule screening in comparison with the commonly used\n3D convolutional neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaze-enhanced Crossmodal Embeddings for Emotion Recognition. (arXiv:2205.00129v1 [cs.LG])","link":"http://arxiv.org/abs/2205.00129","description":"<p>Emotional expressions are inherently multimodal -- integrating facial\nbehavior, speech, and gaze -- but their automatic recognition is often limited\nto a single modality, e.g. speech during a phone call. While previous work\nproposed crossmodal emotion embeddings to improve monomodal recognition\nperformance, despite its importance, an explicit representation of gaze was not\nincluded. We propose a new approach to emotion recognition that incorporates an\nexplicit representation of gaze in a crossmodal emotion embedding framework. We\nshow that our method outperforms the previous state of the art for both\naudio-only and video-only emotion classification on the popular One-Minute\nGradual Emotion Recognition dataset. Furthermore, we report extensive ablation\nexperiments and provide detailed insights into the performance of different\nstate-of-the-art gaze representations and integration strategies. Our results\nnot only underline the importance of gaze for emotion recognition but also\ndemonstrate a practical and highly effective approach to leveraging gaze\ninformation for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdou_A/0/1/0/all/0/1\">Ahmed Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_E/0/1/0/all/0/1\">Ekta Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1\">Philipp M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulling_A/0/1/0/all/0/1\">Andreas Bulling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Understand Negation in Video Retrieval. (arXiv:2205.00132v1 [cs.MM])","link":"http://arxiv.org/abs/2205.00132","description":"<p>Negation is a common linguistic skill that allows human to express what we do\nNOT want. Naturally, one might expect video retrieval to support\nnatural-language queries with negation, e.g., finding shots of kids sitting on\nthe floor and not playing with the dog. However, the state-of-the-art deep\nlearning based video retrieval models lack such ability, as they are typically\ntrained on video description datasets such as MSR-VTT and VATEX that lack\nnegated descriptions. Their retrieved results basically ignore the negator in\nthe sample query, incorrectly returning videos showing kids playing with the\ndog. In this paper, we present the first study on learning to understand\nnegation in video retrieval and make contributions as follows. First, by\nre-purposing two existing datasets, i.e. MSR-VTT and VATEX, we propose a new\nevaluation protocol for testing video retrieval with negation. Second, we\npropose a learning based method for training a negation-aware video retrieval\nmodel. The key idea is to first construct a soft negative caption for a\nspecific training video by partially negating its original caption, and then\ncompute a bidirectionally constrained loss on the triplet. This auxiliary loss\nis then weightedly added to a standard retrieval loss. Experiments on the\nre-purposed benchmarks show that re-training the CLIP (Contrastive\nLanguage-Image Pre-Training) model by the proposed method clearly improves its\nability to handle queries with negation. In addition, its performance on the\noriginal benchmarks is also improved. Data and source code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aozhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Representation Learning With Text and Images. (arXiv:2205.00142v1 [cs.LG])","link":"http://arxiv.org/abs/2205.00142","description":"<p>In recent years, multimodal AI has seen an upward trend as researchers are\nintegrating data of different types such as text, images, speech into modelling\nto get the best results. This project leverages multimodal AI and matrix\nfactorization techniques for representation learning, on text and image data\nsimultaneously, thereby employing the widely used techniques of Natural\nLanguage Processing (NLP) and Computer Vision. The learnt representations are\nevaluated using downstream classification and regression tasks. The methodology\nadopted can be extended beyond the scope of this project as it uses\nAuto-Encoders for unsupervised representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayagopal_A/0/1/0/all/0/1\">Aishwarya Jayagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aiswarya_A/0/1/0/all/0/1\">Ankireddy Monica Aiswarya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ankita Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_S/0/1/0/all/0/1\">Srinivasan Kolumam Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Closer to Supervise Better: One-Shot Font Generation via Component-Based Discriminator. (arXiv:2205.00146v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00146","description":"<p>Automatic font generation remains a challenging research issue due to the\nlarge amounts of characters with complicated structures. Typically, only a few\nsamples can serve as the style/content reference (termed few-shot learning),\nwhich further increases the difficulty to preserve local style patterns or\ndetailed glyph structures. We investigate the drawbacks of previous studies and\nfind that a coarse-grained discriminator is insufficient for supervising a font\ngenerator. To this end, we propose a novel Component-Aware Module (CAM), which\nsupervises the generator to decouple content and style at a more fine-grained\nlevel, \\textit{i.e.}, the component level. Different from previous studies\nstruggling to increase the complexity of generators, we aim to perform more\neffective supervision for a relatively simple generator to achieve its full\npotential, which is a brand new perspective for font generation. The whole\nframework achieves remarkable results by coupling component-level supervision\nwith adversarial learning, hence we call it Component-Guided GAN, shortly\nCG-GAN. Extensive experiments show that our approach outperforms\nstate-of-the-art one-shot font generation methods. Furthermore, it can be\napplied to handwritten word synthesis and scene text image editing, suggesting\nthe generalization of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yuxin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weihong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnimalTrack: A Large-scale Benchmark for Multi-Animal Tracking in the Wild. (arXiv:2205.00158v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00158","description":"<p>Multi-animal tracking (MAT), a multi-object tracking (MOT) problem, is\ncrucial for animal motion and behavior analysis and has many crucial\napplications such as biology, ecology, animal conservation and so forth.\nDespite its importance, MAT is largely under-explored compared to other MOT\nproblems such as multi-human tracking due to the scarcity of large-scale\nbenchmark. To address this problem, we introduce AnimalTrack, a large-scale\nbenchmark for multi-animal tracking in the wild. Specifically, AnimalTrack\nconsists of 58 sequences from a diverse selection of 10 common animal\ncategories. On average, each sequence comprises of 33 target objects for\ntracking. In order to ensure high quality, every frame in AnimalTrack is\nmanually labeled with careful inspection and refinement. To our best knowledge,\nAnimalTrack is the first benchmark dedicated to multi-animal tracking. In\naddition, to understand how existing MOT algorithms perform on AnimalTrack and\nprovide baselines for future comparison, we extensively evaluate 14\nstate-of-the-art representative trackers. The evaluation results demonstrate\nthat, not surprisingly, most of these trackers become degenerated due to the\ndifferences between pedestrians and animals in various aspects (e.g., pose,\nmotion, appearance, etc), and more efforts are desired to improve multi-animal\ntracking. We hope that AnimalTrack together with evaluation and analysis will\nfoster further progress on multi-animal tracking. The dataset and evaluation as\nwell as our analysis will be made available upon the acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVTR: Scene Text Recognition with a Single Visual Model. (arXiv:2205.00159v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00159","description":"<p>Dominant scene text recognition models commonly contain two building blocks,\na visual model for feature extraction and a sequence model for text\ntranscription. This hybrid architecture, although accurate, is complex and less\nefficient. In this study, we propose a Single Visual model for Scene Text\nrecognition within the patch-wise image tokenization framework, which dispenses\nwith the sequential modeling entirely. The method, termed SVTR, firstly\ndecomposes an image text into small patches named character components.\nAfterward, hierarchical stages are recurrently carried out by component-level\nmixing, merging and/or combining. Global and local mixing blocks are devised to\nperceive the inter-character and intra-character patterns, leading to a\nmulti-grained character component perception. Thus, characters are recognized\nby a simple linear prediction. Experimental results on both English and Chinese\nscene text recognition tasks demonstrate the effectiveness of SVTR. SVTR-L\n(Large) achieves highly competitive accuracy in English and outperforms\nexisting methods by a large margin in Chinese, while running faster. In\naddition, SVTR-T (Tiny) is an effective and much smaller model, which shows\nappealing speed at inference. The code is publicly available at\nhttps://github.com/PaddlePaddle/PaddleOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yongkun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhineng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Caiyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiaoting Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianlun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elucidating Meta-Structures of Noisy Labels in Semantic Segmentation by Deep Neural Networks. (arXiv:2205.00160v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00160","description":"<p>The supervised training of deep neural networks (DNNs) by noisy labels has\nbeen studied extensively in image classification but much less in image\nsegmentation. So far, our understanding of the learning behavior of DNNs\ntrained by noisy segmentation labels remains limited. In this study, we address\nthis deficiency in both binary segmentation of biological microscopy images and\nmulti-class segmentation of natural images. We classify segmentation labels\naccording to their noise transition matrices (NTM) and compare performance of\nDNNs trained by different types of labels. When we randomly sample a small\nfraction (e.g., 10%) or flipping a large fraction (e.g., 90%) of the\nground-truth labels to train DNNs, their segmentation performance remains\nlargely the same. This indicates that DNNs learn structures hidden in labels\nrather than pixel-level labels per se in their supervised training for semantic\nsegmentation. We call these hidden structures \"meta-structures\". When we use\nlabels with different perturbations to the meta-structures to train DNNs, their\nperformance in feature extraction and segmentation degrades consistently. In\ncontrast, addition of meta-structure information substantially improves\nperformance of an unsupervised model in binary semantic segmentation. We\nformulate meta-structures mathematically as spatial density distributions and\nquantify semantic information of different types of labels, which we find to\ncorrelate strongly with ranks of their NTM. We show theoretically and\nexperimentally how this formulation explains key observed learning behavior of\nDNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yaoru Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guole Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanhao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClusterQ: Semantic Feature Distribution Alignment for Data-Free Quantization. (arXiv:2205.00179v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00179","description":"<p>Network quantization has emerged as a promising method for model compression\nand inference acceleration. However, tradtional quantization methods (such as\nquantization aware training and post training quantization) require original\ndata for the fine-tuning or calibration of quantized model, which makes them\ninapplicable to the cases that original data are not accessed due to privacy or\nsecurity. This gives birth to the data-free quantization with synthetic data\ngeneration. While current DFQ methods still suffer from severe performance\ndegradation when quantizing a model into lower bit, caused by the low\ninter-class separability of semantic features. To this end, we propose a new\nand effective data-free quantization method termed ClusterQ, which utilizes the\nsemantic feature distribution alignment for synthetic data generation. To\nobtain high inter-class separability of semantic features, we cluster and align\nthe feature distribution statistics to imitate the distribution of real data,\nso that the performance degradation is alleviated. Moreover, we incorporate the\nintra-class variance to solve class-wise mode collapse. We also employ the\nexponential moving average to update the centroid of each cluster for further\nfeature distribution improvement. Extensive experiments across various deep\nmodels (e.g., ResNet-18 and MobileNet-V2) over the ImageNet dataset demonstrate\nthat our ClusterQ obtains state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yangcheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jicong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Label Correction is a Good Booster When Learning with Extremely Noisy Labels. (arXiv:2205.00186v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00186","description":"<p>Learning with noisy labels has aroused much research interest since data\nannotations, especially for large-scale datasets, may be inevitably imperfect.\nRecent approaches resort to a semi-supervised learning problem by dividing\ntraining samples into clean and noisy sets. This paradigm, however, is prone to\nsignificant degeneration under heavy label noise, as the number of clean\nsamples is too small for conventional methods to behave well. In this paper, we\nintroduce a novel framework, termed as LC-Booster, to explicitly tackle\nlearning under extreme noise. The core idea of LC-Booster is to incorporate\nlabel correction into the sample selection, so that more purified samples,\nthrough the reliable label correction, can be utilized for training, thereby\nalleviating the confirmation bias. Experiments show that LC-Booster advances\nstate-of-the-art results on several noisy-label benchmarks, including CIFAR-10,\nCIFAR-100, Clothing1M and WebVision. Remarkably, under the extreme 90\\% noise\nratio, LC-Booster achieves 93.5\\% and 48.4\\% accuracy on CIFAR-10 and\nCIFAR-100, surpassing the state-of-the-art by 1.6\\% and 7.2\\% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"And Then There Were None\": Cracking White-box DNN Watermarks via Invariant Neuron Transforms. (arXiv:2205.00199v1 [cs.CR])","link":"http://arxiv.org/abs/2205.00199","description":"<p>Recently, how to protect the Intellectual Property (IP) of deep neural\nnetworks (DNN) becomes a major concern for the AI industry. To combat potential\nmodel piracy, recent works explore various watermarking strategies to embed\nsecret identity messages into the prediction behaviors or the internals (e.g.,\nweights and neuron activation) of the target model. Sacrificing less\nfunctionality and involving more knowledge about the target model, the latter\nbranch of watermarking schemes (i.e., white-box model watermarking) is claimed\nto be accurate, credible and secure against most known watermark removal\nattacks, with emerging research efforts and applications in the industry.\n</p>\n<p>In this paper, we present the first effective removal attack which cracks\nalmost all the existing white-box watermarking schemes with provably no\nperformance overhead and no required prior knowledge. By analyzing these IP\nprotection mechanisms at the granularity of neurons, we for the first time\ndiscover their common dependence on a set of fragile features of a local neuron\ngroup, all of which can be arbitrarily tampered by our proposed chain of\ninvariant neuron transforms. On $9$ state-of-the-art white-box watermarking\nschemes and a broad set of industry-level DNN architectures, our attack for the\nfirst time reduces the embedded identity message in the protected models to be\nalmost random. Meanwhile, unlike known removal attacks, our attack requires no\nprior knowledge on the training data distribution or the adopted watermark\nalgorithms, and leaves model functionality intact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yifan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xudong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DefakeHop++: An Enhanced Lightweight Deepfake Detector. (arXiv:2205.00211v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00211","description":"<p>On the basis of DefakeHop, an enhanced lightweight Deepfake detector called\nDefakeHop++ is proposed in this work. The improvements lie in two areas. First,\nDefakeHop examines three facial regions (i.e., two eyes and mouth) while\nDefakeHop++ includes eight more landmarks for broader coverage. Second, for\ndiscriminant features selection, DefakeHop uses an unsupervised approach while\nDefakeHop++ adopts a more effective approach with supervision, called the\nDiscriminant Feature Test (DFT). In DefakeHop++, rich spatial and spectral\nfeatures are first derived from facial regions and landmarks automatically.\nThen, DFT is used to select a subset of discriminant features for classifier\ntraining. As compared with MobileNet v3 (a lightweight CNN model of 1.5M\nparameters targeting at mobile applications), DefakeHop++ has a model of 238K\nparameters, which is 16% of MobileNet v3. Furthermore, DefakeHop++ outperforms\nMobileNet v3 in Deepfake image detection performance in a weakly-supervised\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong-Shuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer. (arXiv:2205.00214v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00214","description":"<p>Video denoising aims to recover high-quality frames from the noisy video.\nWhile most existing approaches adopt convolutional neural networks(CNNs) to\nseparate the noise from the original visual content, however, CNNs focus on\nlocal information and ignore the interactions between long-range regions.\nFurthermore, most related works directly take the output after spatio-temporal\ndenoising as the final result, neglecting the fine-grained denoising process.\nIn this paper, we propose a Dual-stage Spatial-Channel Transformer (DSCT) for\ncoarse-to-fine video denoising, which inherits the advantages of both\nTransformer and CNNs. Specifically, DSCT is proposed based on a progressive\ndual-stage architecture, namely a coarse-level and a fine-level to extract\ndynamic feature and static feature, respectively. At both stages, a\nSpatial-Channel Encoding Module(SCEM) is designed to model the long-range\ncontextual dependencies at spatial and channel levels. Meanwhile, we design a\nMulti-scale Residual Structure to preserve multiple aspects of information at\ndifferent stages, which contains a Temporal Features Aggregation Module(TFAM)\nto summarize the dynamic representation. Extensive experiments on four publicly\navailable datasets demonstrate our proposed DSCT achieves significant\nimprovements compared to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1\">Wulian Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1\">Mengshi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuanming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huiyuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Huadong Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognising Known Configurations of Garments For Dual-Arm Robotic Flattening. (arXiv:2205.00225v1 [cs.RO])","link":"http://arxiv.org/abs/2205.00225","description":"<p>Robotic deformable-object manipulation is a challenge in the robotic industry\nbecause deformable objects have complicated and various object states.\nPredicting those object states and updating manipulation planning are\ntime-consuming and computationally expensive. In this paper, we propose an\neffective robotic manipulation approach for recognising 'known configurations'\nof garments with a 'Known Configuration neural Network' (KCNet) and choosing\npre-designed manipulation plans based on the recognised known configurations.\nOur robotic manipulation plan features a four-action strategy: finding two\ncritical grasping points, stretching the garments, and lifting down the\ngarments. We demonstrate that our approach only needs 98 seconds on average to\nflatten garments of five categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Li Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argon_Camarasa_G/0/1/0/all/0/1\">Gerardo Argon-Camarasa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Visible-light Images Guided Cross-Spectrum Depth Estimation from Dual-Modality Cameras. (arXiv:2205.00257v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00257","description":"<p>Cross-spectrum depth estimation aims to provide a depth map in all\nillumination conditions with a pair of dual-spectrum images. It is valuable for\nautonomous vehicle applications when the vehicle is equipped with two cameras\nof different modalities. However, images captured by different-modality cameras\ncan be photometrically quite different. Therefore, cross-spectrum depth\nestimation is a very challenging problem. Moreover, the shortage of large-scale\nopen-source datasets also retards further research in this field. In this\npaper, we propose an unsupervised visible-light image guided cross-spectrum\n(i.e., thermal and visible-light, TIR-VIS in short) depth estimation framework\ngiven a pair of RGB and thermal images captured from a visible-light camera and\na thermal one. We first adopt a base depth estimation network using RGB-image\npairs. Then we propose a multi-scale feature transfer network to transfer\nfeatures from the TIR-VIS domain to the VIS domain at the feature level to fit\nthe trained depth estimation network. At last, we propose a cross-spectrum\ndepth cycle consistency to improve the depth result of dual-spectrum image\npairs. Meanwhile, we release a large dual-spectrum depth estimation dataset\nwith visible-light and far-infrared stereo images captured in different scenes\nto the society. The experiment result shows that our method achieves better\nperformance than the compared existing methods. Our datasets is available at\nhttps://github.com/whitecrow1027/VIS-TIR-Datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yubin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xinlei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1\">Hui Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning. (arXiv:2205.00272v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00272","description":"<p>Visual grounding is a task to locate the target indicated by a natural\nlanguage expression. Existing methods extend the generic object detection\nframework to this problem. They base the visual grounding on the features from\npre-generated proposals or anchors, and fuse these features with the text\nembeddings to locate the target mentioned by the text. However, modeling the\nvisual features from these predefined locations may fail to fully exploit the\nvisual context and attribute information in the text query, which limits their\nperformance. In this paper, we propose a transformer-based framework for\naccurate visual grounding by establishing text-conditioned discriminative\nfeatures and performing multi-stage cross-modal reasoning. Specifically, we\ndevelop a visual-linguistic verification module to focus the visual features on\nregions relevant to the textual descriptions while suppressing the unrelated\nareas. A language-guided feature encoder is also devised to aggregate the\nvisual contexts of the target object to improve the object's distinctiveness.\nTo retrieve the target from the encoded visual features, we further propose a\nmulti-stage cross-modal decoder to iteratively speculate on the correlations\nbetween the image and text for accurate target localization. Extensive\nexperiments on five widely used datasets validate the efficacy of our proposed\ncomponents and demonstrate state-of-the-art performance. Our code is public at\nhttps://github.com/yangli18/VLTVG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunfeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Curriculum Learning for Great Ape Detection in the Wild. (arXiv:2205.00275v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00275","description":"<p>We propose a novel end-to-end curriculum learning approach that leverages\nlarge volumes of unlabelled great ape camera trap footage to improve supervised\nspecies detector construction in challenging real-world jungle environments. In\ncontrast to previous semi-supervised methods, our approach gradually improves\ndetection quality by steering training towards virtuous self-reinforcement. To\nachieve this, we propose integrating pseudo-labelling with dynamic curriculum\nlearning policies. We show that such dynamics and controls can avoid learning\ncollapse and gradually tie detector adjustments to higher model quality. We\nprovide theoretical arguments and ablations, and confirm significant\nperformance improvements against various state-of-the-art systems when\nevaluating on the Extended PanAfrican Dataset holding several thousand camera\ntrap videos of great apes. We note that system performance is strongest for\nsmaller labelled ratios, which are common in ecological applications. Our\napproach, although designed with wildlife data in mind, also shows competitive\nbenchmarks for generic object detection in the MS-COCO dataset, indicating\nwider applicability of introduced concepts. The code is available at\nhttps://github.com/youshyee/DCL-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1\">Majid Mirmehdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ONCE-3DLanes: Building Monocular 3D Lane Detection. (arXiv:2205.00301v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00301","description":"<p>We present ONCE-3DLanes, a real-world autonomous driving dataset with lane\nlayout annotation in 3D space. Conventional 2D lane detection from a monocular\nimage yields poor performance of following planning and control tasks in\nautonomous driving due to the case of uneven road. Predicting the 3D lane\nlayout is thus necessary and enables effective and safe driving. However,\nexisting 3D lane detection datasets are either unpublished or synthesized from\na simulated environment, severely hampering the development of this field. In\nthis paper, we take steps towards addressing these issues. By exploiting the\nexplicit relationship between point clouds and image pixels, a dataset\nannotation pipeline is designed to automatically generate high-quality 3D lane\nlocations from 2D lane annotations in 211K road scenes. In addition, we present\nan extrinsic-free, anchor-free method, called SALAD, regressing the 3D\ncoordinates of lanes in image view without converting the feature map into the\nbird's-eye view (BEV). To facilitate future research on 3D lane detection, we\nbenchmark the dataset and provide a novel evaluation metric, performing\nextensive experiments of both existing approaches and our proposed method. The\naim of our work is to revive the interest of 3D lane detection in a real-world\nscenario. We believe our work can lead to the expected and unexpected\ninnovations in both academia and industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1\">Fan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1\">Ming Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xinyue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chaoqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1\">Michael Bi Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs. (arXiv:2205.00303v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00303","description":"<p>In this paper, we study the graphic layout generation problem of producing\nhigh-quality visual-textual presentation designs for given images. We note that\nimage compositions, which contain not only global semantics but also spatial\ninformation, would largely affect layout results. Hence, we propose a deep\ngenerative model, dubbed as composition-aware graphic layout GAN (CGL-GAN), to\nsynthesize layouts based on the global and spatial visual contents of input\nimages. To obtain training images from images that already contain manually\ndesigned graphic layout data, previous work suggests masking design elements\n(e.g., texts and embellishments) as model inputs, which inevitably leaves hint\nof the ground truth. We study the misalignment between the training inputs\n(with hint masks) and test inputs (without masks), and design a novel domain\nalignment module (DAM) to narrow this gap. For training, we built a large-scale\nlayout dataset which consists of 60,548 advertising posters with annotated\nlayout information. To evaluate the generated layouts, we propose three novel\nmetrics according to aesthetic intuitions. Through both quantitative and\nqualitative evaluations, we demonstrate that the proposed model can synthesize\nhigh-quality graphic layouts according to image compositions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Min Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Ye Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tiezheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source Domain Subset Sampling for Semi-Supervised Domain Adaptation in Semantic Segmentation. (arXiv:2205.00312v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00312","description":"<p>In this paper, we introduce source domain subset sampling (SDSS) as a new\nperspective of semi-supervised domain adaptation. We propose domain adaptation\nby sampling and exploiting only a meaningful subset from source data for\ntraining. Our key assumption is that the entire source domain data may contain\nsamples that are unhelpful for the adaptation. Therefore, the domain adaptation\ncan benefit from a subset of source data composed solely of helpful and\nrelevant samples. The proposed method effectively subsamples full source data\nto generate a small-scale meaningful subset. Therefore, training time is\nreduced, and performance is improved with our subsampled source data. To\nfurther verify the scalability of our method, we construct a new dataset called\nOcean Ship, which comprises 500 real and 200K synthetic sample images with\nground-truth labels. The SDSS achieved a state-of-the-art performance when\napplied on GTA5 to Cityscapes and SYNTHIA to Cityscapes public benchmark\ndatasets and a 9.13 mIoU improvement on our Ocean Ship dataset over a baseline\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daehan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinsun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Geol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutBERT: Masked Language Layout Model for Object Insertion. (arXiv:2205.00347v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00347","description":"<p>Image compositing is one of the most fundamental steps in creative workflows.\nIt involves taking objects/parts of several images to create a new image,\ncalled a composite. Currently, this process is done manually by creating\naccurate masks of objects to be inserted and carefully blending them with the\ntarget scene or images, usually with the help of tools such as Photoshop or\nGIMP. While there have been several works on automatic selection of objects for\ncreating masks, the problem of object placement within an image with the\ncorrect position, scale, and harmony remains a difficult problem with limited\nexploration. Automatic object insertion in images or designs is a difficult\nproblem as it requires understanding of the scene geometry and the color\nharmony between objects. We propose LayoutBERT for the object insertion task.\nIt uses a novel self-supervised masked language model objective and\nbidirectional multi-head self-attention. It outperforms previous layout-based\nlikelihood models and shows favorable properties in terms of model capacity. We\ndemonstrate the effectiveness of our approach for object insertion in the image\ncompositing setting and other settings like documents and design templates. We\nfurther demonstrate the usefulness of the learned representations for\nlayout-based retrieval tasks. We provide both qualitative and quantitative\nevaluations on datasets from diverse domains like COCO, PublayNet, and two new\ndatasets which we call Image Layouts and Template Layouts. Image Layouts which\nconsists of 5.8 million images with layout annotations is the largest image\nlayout dataset to our knowledge. We also share ablation study results on the\neffect of dataset size, model size and class sample size for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sanat Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1\">Jayant Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Spatial Reasoning. (arXiv:2205.00363v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00363","description":"<p>Spatial relations are fundamental to human cognition and are the most basic\nknowledge for us to understand and communicate about our physical surroundings.\nIn this paper, we ask the critical question: Are current vision-and-language\nmodels (VLMs) able to correctly understand spatial relations? To answer this\nquestion, we propose Visual Spatial Reasoning (VSR), a novel benchmark task\nwith human labelled dataset for investigating VLMs' capabilities in recognising\n65 types of spatial relationships (e.g., under, in front of, facing etc.) in\nnatural text-image pairs. Specifically, given a caption and an image, the model\nneeds to perform binary classification and decide if the caption accurately\ndescribes the spatial relationships of two objects presented in the image.\nWhile being seemingly simple and straightforward, the task shows a large gap\nbetween human and model performance (human ceiling on the VSR task is above 95%\nand models only achieve around 70%). With fine-grained categorisation and\ncontrol on both concepts and relations, our VSR benchmark enables us to perform\ninteresting probing analysis to pinpoint VLMs' failure cases and the reasons\nbehind. We observe that VLMs' by-relation performances have little correlation\nwith the number of training examples and the tested models are in general\nincapable of recognising relations that concern orientations of objects. Also,\nVLMs have poor zero-shot generalisation toward unseen concepts. The dataset and\ncode are released at github.com/cambridgeltl/visual-spatial-reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_G/0/1/0/all/0/1\">Guy Emerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RADNet: A Deep Neural Network Model for Robust Perception in Moving Autonomous Systems. (arXiv:2205.00364v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00364","description":"<p>Interactive autonomous applications require robustness of the perception\nengine to artifacts in unconstrained videos. In this paper, we examine the\neffect of camera motion on the task of action detection. We develop a novel\nranking method to rank videos based on the degree of global camera motion. For\nthe high ranking camera videos we show that the accuracy of action detection is\ndecreased. We propose an action detection pipeline that is robust to the camera\nmotion effect and verify it empirically. Specifically, we do actor feature\nalignment across frames and couple global scene features with local\nactor-specific features. We do feature alignment using a novel formulation of\nthe Spatio-temporal Sampling Network (STSN) but with multi-scale offset\nprediction and refinement using a pyramid structure. We also propose a novel\ninput dependent weighted averaging strategy for fusing local and global\nfeatures. We show the applicability of our network on our dataset of moving\ncamera videos with high camera motion (MOVE dataset) with a 4.1% increase in\nframe mAP and 17% increase in video mAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mudassar_B/0/1/0/all/0/1\">Burhan A. Mudassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sho Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Maojingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Priyabrata Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1\">Saibal Mukhopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fractional Vegetation Cover Estimation using Hough Lines and Linear Iterative Clustering. (arXiv:2205.00366v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00366","description":"<p>A common requirement of plant breeding programs across the country is\ncompanion planting -- growing different species of plants in close proximity so\nthey can mutually benefit each other. However, the determination of companion\nplants requires meticulous monitoring of plant growth. The technique of ocular\nmonitoring is often laborious and error prone. The availability of image\nprocessing techniques can be used to address the challenge of plant growth\nmonitoring and provide robust solutions that assist plant scientists to\nidentify companion plants. This paper presents a new image processing algorithm\nto determine the amount of vegetation cover present in a given area, called\nfractional vegetation cover. The proposed technique draws inspiration from the\ntrusted Daubenmire method for vegetation cover estimation and expands upon it.\nBriefly, the idea is to estimate vegetation cover from images containing\nmultiple rows of plant species growing in close proximity separated by a\nmulti-segment PVC frame of known size. The proposed algorithm applies a Hough\nTransform and Simple Linear Iterative Clustering (SLIC) to estimate the amount\nof vegetation cover within each segment of the PVC frame. The analysis when\nrepeated over images captured at regular intervals of time provides crucial\ninsights into plant growth. As a means of comparison, the proposed algorithm is\ncompared with SamplePoint and Canopeo, two trusted applications used for\nvegetation cover estimation. The comparison shows a 99% similarity with both\nSamplePoint and Canopeo demonstrating the accuracy and feasibility of the\nalgorithm for fractional vegetation cover estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Margapuri_V/0/1/0/all/0/1\">Venkat Margapuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rife_T/0/1/0/all/0/1\">Trevor Rife</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courtney_C/0/1/0/all/0/1\">Chaney Courtney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlautman_B/0/1/0/all/0/1\">Brandon Schlautman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neilsen_M/0/1/0/all/0/1\">Mitchell Neilsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Traffic Context Aware Data Augmentation for Rare Object Detection in Autonomous Driving. (arXiv:2205.00376v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00376","description":"<p>Detection of rare objects (e.g., traffic cones, traffic barrels and traffic\nwarning triangles) is an important perception task to improve the safety of\nautonomous driving. Training of such models typically requires a large number\nof annotated data which is expensive and time consuming to obtain. To address\nthe above problem, an emerging approach is to apply data augmentation to\nautomatically generate cost-free training samples. In this work, we propose a\nsystematic study on simple Copy-Paste data augmentation for rare object\ndetection in autonomous driving. Specifically, local adaptive instance-level\nimage transformation is introduced to generate realistic rare object masks from\nsource domain to the target domain. Moreover, traffic scene context is utilized\nto guide the placement of masks of rare objects. To this end, our data\naugmentation generates training data with high quality and realistic\ncharacteristics by leveraging both local and global consistency. In addition,\nwe build a new dataset named NM10k consisting 10k training images, 4k\nvalidation images and the corresponding labels with a diverse range of\nscenarios in autonomous driving. Experiments on NM10k show that our method\nachieves promising results on rare object detection. We also present a thorough\nstudy to illustrate the effectiveness of our local-adaptive and global\nconstraints based Copy-Paste data augmentation for rare object detection. The\ndata, development kit and more information of NM10k dataset are available\nonline at: \\url{https://nullmax-vision.github.io}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Naifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Fan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Pengpeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1\">Erkang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition. (arXiv:2205.00380v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00380","description":"<p>Micro-expression recognition (MER) is valuable because the involuntary nature\nof micro-expressions (MEs) can reveal genuine emotions. Most works recognize\nMEs by taking RGB videos or images as input. In fact, the activated facial\nregions in ME images are very small and the subtle motion can be easily\nsubmerged in the unrelated information. Facial landmarks are a low-dimensional\nand compact modality, which leads to much lower computational cost and can\npotentially concentrate more on ME-related features. However, the\ndiscriminability of landmarks for MER is not clear. Thus, this paper explores\nthe contribution of facial landmarks and constructs a new framework to\nefficiently recognize MEs with sole facial landmark information. Specially, we\ndesign a separate structure module to separately aggregate the spatial and\ntemporal information in the geometric movement graph based on facial landmarks,\nand a Geometric Two-Stream Graph Network is constructed to aggregate the\nlow-order geometric information and high-order semantic information of facial\nlandmarks. Furthermore, two core components are proposed to enhance features.\nSpecifically, a semantic adjacency matrix can automatically model the\nrelationship between nodes even long-distance nodes in a self-learning fashion;\nand an Adaptive Action Unit loss is introduced to guide the learning process\nsuch that the learned features are forced to have a synchronized pattern with\nfacial action units. Notably, this work tackles MER only utilizing geometric\nfeatures, processed based on a graph model, which provides a new idea with much\nhigher efficiency to promote MER. The experimental results demonstrate that the\nproposed method can achieve competitive or even superior performance with a\nsignificantly reduced computational cost, and facial landmarks can\nsignificantly contribute to MER and are worth further study for efficient ME\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jinsheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guanming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yante Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jingjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convex Combination Consistency between Neighbors for Weakly-supervised Action Localization. (arXiv:2205.00400v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00400","description":"<p>In weakly-supervised temporal action localization (WS-TAL), the methods\ncommonly follow the \"localization by classification\" procedure, which uses the\nsnippet predictions to form video class scores and then optimizes a video\nclassification loss. In this procedure, the snippet predictions (or snippet\nattention weights) are used to separate foreground and background. However, the\nsnippet predictions are usually inaccurate due to absence of frame-wise labels,\nand then the overall performance is hindered. In this paper, we propose a novel\nC$^3$BN to achieve robust snippet predictions. C$^3$BN includes two key designs\nby exploring the inherent characteristics of video data. First, because of the\nnatural continuity of adjacent snippets, we propose a micro data augmentation\nstrategy to increase the diversity of snippets with convex combination of\nadjacent snippets. Second, we propose a macro-micro consistency regularization\nstrategy to force the model to be invariant (or equivariant) to the\ntransformations of snippets with respect to video semantics, snippet\npredictions and snippet features. Experimental results demonstrate the\neffectiveness of our proposed method on top of baselines for the WS-TAL tasks\nwith video-level and point-level supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qinying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhilin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions. (arXiv:2205.00415v1 [cs.CL])","link":"http://arxiv.org/abs/2205.00415","description":"<p>In recent years, progress in NLU has been driven by benchmarks. These\nbenchmarks are typically collected by crowdsourcing, where annotators write\nexamples based on annotation instructions crafted by dataset creators. In this\nwork, we hypothesize that annotators pick up on patterns in the crowdsourcing\ninstructions, which bias them to write similar examples that are then\nover-represented in the collected data. We study this form of bias, termed\ninstruction bias, in 14 recent NLU benchmarks, showing that instruction\nexamples often exhibit concrete patterns, which are propagated by crowdworkers\nto the collected data. This extends previous work (Geva et al., 2019) and\nraises a new concern of whether we are modeling the dataset creator's\ninstructions, rather than the task. Through a series of experiments, we show\nthat, indeed, instruction bias can lead to overestimation of model performance,\nand that models struggle to generalize beyond biases originating in the\ncrowdsourcing instructions. We further analyze the influence of instruction\nbias in terms of pattern frequency and model size, and derive concrete\nrecommendations for creating future NLU benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog. (arXiv:2205.00423v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00423","description":"<p>Visual Dialog aims to answer multi-round, interactive questions based on the\ndialog history and image content. Existing methods either consider answer\nranking and generating individually or only weakly capture the relation across\nthe two tasks implicitly by two separate models. The research on a universal\nframework that jointly learns to rank and generate answers in a single model is\nseldom explored. In this paper, we propose a contrastive learning-based\nframework UTC to unify and facilitate both discriminative and generative tasks\nin visual dialog with a single model. Specifically, considering the inherent\nlimitation of the previous learning paradigm, we devise two inter-task\ncontrastive losses i.e., context contrastive loss and answer contrastive loss\nto make the discriminative and generative tasks mutually reinforce each other.\nThese two complementary contrastive losses exploit dialog context and target\nanswer as anchor points to provide representation learning signals from\ndifferent perspectives. We evaluate our proposed UTC on the VisDial v1.0\ndataset, where our method outperforms the state-of-the-art on both\ndiscriminative and generative tasks and surpasses previous state-of-the-art\ngenerative methods by more than 2 absolute points on Recall@1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yudong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhenshan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qingrong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaodong Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Diffractive Neural Networks for Seeing Through Random Diffusers. (arXiv:2205.00428v1 [physics.optics])","link":"http://arxiv.org/abs/2205.00428","description":"<p>Imaging through diffusive media is a challenging problem, where the existing\nsolutions heavily rely on digital computers to reconstruct distorted images. We\nprovide a detailed analysis of a computer-free, all-optical imaging method for\nseeing through random, unknown phase diffusers using diffractive neural\nnetworks, covering different deep learning-based training strategies. By\nanalyzing various diffractive networks designed to image through random\ndiffusers with different correlation lengths, a trade-off between the image\nreconstruction fidelity and distortion reduction capability of the diffractive\nnetwork was observed. During its training, random diffusers with a range of\ncorrelation lengths were used to improve the diffractive network's\ngeneralization performance. Increasing the number of random diffusers used in\neach epoch reduced the overfitting of the diffractive network's imaging\nperformance to known diffusers. We also demonstrated that the use of additional\ndiffractive layers improved the generalization capability to see through new,\nrandom diffusers. Finally, we introduced deliberate misalignments in training\nto 'vaccinate' the network against random layer-to-layer shifts that might\narise due to the imperfect assembly of the diffractive networks. These analyses\nprovide a comprehensive guide in designing diffractive networks to see through\nrandom diffusers, which might profoundly impact many fields, such as biomedical\nimaging, atmospheric physics, and autonomous driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1\">Yi Luo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bai_B/0/1/0/all/0/1\">Bijie Bai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Swin-Convs Transformer for Underwater Image Enhancement. (arXiv:2205.00434v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00434","description":"<p>Underwater Image Enhancement (UIE) technology aims to tackle the challenge of\nrestoring the degraded underwater images due to light absorption and\nscattering. To address problems, a novel U-Net based Reinforced Swin-Convs\nTransformer for the Underwater Image Enhancement method (URSCT-UIE) is\nproposed. Specifically, with the deficiency of U-Net based on pure\nconvolutions, we embedded the Swin Transformer into U-Net for improving the\nability to capture the global dependency. Then, given the inadequacy of the\nSwin Transformer capturing the local attention, the reintroduction of\nconvolutions may capture more local attention. Thus, we provide an ingenious\nmanner for the fusion of convolutions and the core attention mechanism to build\na Reinforced Swin-Convs Transformer Block (RSCTB) for capturing more local\nattention, which is reinforced in the channel and the spatial attention of the\nSwin Transformer. Finally, the experimental results on available datasets\ndemonstrate that the proposed URSCT-UIE achieves state-of-the-art performance\ncompared with other methods in terms of both subjective and objective\nevaluations. The code will be released on GitHub after acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tingdi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Gangyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Ting Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v1 [eess.IV])","link":"http://arxiv.org/abs/2205.00463","description":"<p>Low-dose CT (LDCT) imaging attracted a considerable interest for the\nreduction of the object's exposure to X-ray radiation. In recent years,\nsupervised deep learning has been extensively studied for LDCT image\nreconstruction, which trains a network over a dataset containing many pairs of\nnormal-dose and low-dose images. However, the challenge on collecting many such\npairs in the clinical setup limits the application of such\nsupervised-learning-based methods for LDCT image reconstruction in practice.\nAiming at addressing the challenges raised by the collection of training\ndataset, this paper proposed a unsupervised deep learning method for LDCT image\nreconstruction, which does not require any external training data. The proposed\nmethod is built on a re-parametrization technique for Bayesian inference via\ndeep network with random weights, combined with additional total variational\n(TV) regularization. The experiments show that the proposed method noticeably\noutperforms existing dataset-free image reconstruction methods on the test\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_Q/0/1/0/all/0/1\">Qiaoqiao Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1\">Hui Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quan_Y/0/1/0/all/0/1\">Yuhui Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preserve Pre-trained Knowledge: Transfer Learning With Self-Distillation For Action Recognition. (arXiv:2205.00506v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00506","description":"<p>Video-based action recognition is one of the most popular topics in computer\nvision. With recent advances of selfsupervised video representation learning\napproaches, action recognition usually follows a two-stage training framework,\ni.e., self-supervised pre-training on large-scale unlabeled sets and transfer\nlearning on a downstream labeled set. However, catastrophic forgetting of the\npre-trained knowledge becomes the main issue in the downstream transfer\nlearning of action recognition, resulting in a sub-optimal solution. In this\npaper, to alleviate the above issue, we propose a novel transfer learning\napproach that combines self-distillation in fine-tuning to preserve knowledge\nfrom the pre-trained model learned from the large-scale dataset. Specifically,\nwe fix the encoder from the last epoch as the teacher model to guide the\ntraining of the encoder from the current epoch in the transfer learning. With\nsuch a simple yet effective learning strategy, we outperform state-of-the-art\nmethods on widely used UCF101 and HMDB51 datasets in action recognition task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhanhao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Best of Both Worlds: Combining Model-based and Nonparametric Approaches for 3D Human Body Estimation. (arXiv:2205.00508v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00508","description":"<p>Nonparametric based methods have recently shown promising results in\nreconstructing human bodies from monocular images while model-based methods can\nhelp correct these estimates and improve prediction. However, estimating model\nparameters from global image features may lead to noticeable misalignment\nbetween the estimated meshes and image evidence. To address this issue and\nleverage the best of both worlds, we propose a framework of three consecutive\nmodules. A dense map prediction module explicitly establishes the dense UV\ncorrespondence between the image evidence and each part of the body model. The\ninverse kinematics module refines the key point prediction and generates a\nposed template mesh. Finally, a UV inpainting module relies on the\ncorresponding feature, prediction and the posed template, and completes the\npredictions of occluded body shape. Our framework leverages the best of\nnon-parametric and model-based methods and is also robust to partial occlusion.\nExperiments demonstrate that our framework outperforms existing 3D human\nestimation methods on multiple public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep vs. Shallow Learning: A Benchmark Study in Low Magnitude Earthquake Detection. (arXiv:2205.00525v1 [cs.LG])","link":"http://arxiv.org/abs/2205.00525","description":"<p>While deep learning models have seen recent high uptake in the geosciences,\nand are appealing in their ability to learn from minimally processed input\ndata, as black box models they do not provide an easy means to understand how a\ndecision is reached, which in safety-critical tasks especially can be\nproblematical. An alternative route is to use simpler, more transparent white\nbox models, in which task-specific feature construction replaces the more\nopaque feature discovery process performed automatically within deep learning\nmodels. Using data from the Groningen Gas Field in the Netherlands, we build on\nan existing logistic regression model by the addition of four further features\ndiscovered using elastic net driven data mining within the catch22 time series\nanalysis package. We then evaluate the performance of the augmented logistic\nregression model relative to a deep (CNN) model, pre-trained on the Groningen\ndata, on progressively increasing noise-to-signal ratios. We discover that, for\neach ratio, our logistic regression model correctly detects every earthquake,\nwhile the deep model fails to detect nearly 20 % of seismic events, thus\njustifying at least a degree of caution in the application of deep models,\nespecially to data with higher noise-to-signal ratios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Akshat Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorse_D/0/1/0/all/0/1\">Denise Gorse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COUCH: Towards Controllable Human-Chair Interactions. (arXiv:2205.00541v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00541","description":"<p>Humans interact with an object in many different ways by making contact at\ndifferent locations, creating a highly complex motion space that can be\ndifficult to learn, particularly when synthesizing such human interactions in a\ncontrollable manner. Existing works on synthesizing human scene interaction\nfocus on the high-level control of action but do not consider the fine-grained\ncontrol of motion. In this work, we study the problem of synthesizing scene\ninteractions conditioned on different contact positions on the object. As a\ntestbed to investigate this new problem, we focus on human-chair interaction as\none of the most common actions which exhibit large variability in terms of\ncontacts. We propose a novel synthesis framework COUCH that plans ahead the\nmotion by predicting contact-aware control signals of the hands, which are then\nused to synthesize contact-conditioned interactions. Furthermore, we contribute\na large human-chair interaction dataset with clean annotations, the COUCH\nDataset. Our method shows significant quantitative and qualitative improvements\nover existing methods for human-object interactions. More importantly, our\nmethod enables control of the motion through user-specified or automatically\npredicted contacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1\">Bharat Lal Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzov_V/0/1/0/all/0/1\">Vladimir Guzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Starke_S/0/1/0/all/0/1\">Sebastian Starke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using a novel fractional-order gradient method for CNN back-propagation. (arXiv:2205.00581v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00581","description":"<p>Computer-aided diagnosis tools have experienced rapid growth and development\nin recent years. Among all, deep learning is the most sophisticated and popular\ntool. In this paper, researchers propose a novel deep learning model and apply\nit to COVID-19 diagnosis. Our model uses the tool of fractional calculus, which\nhas the potential to improve the performance of gradient methods. To this end,\nthe researcher proposes a fractional-order gradient method for the\nback-propagation of convolutional neural networks based on the Caputo\ndefinition. However, if only the first term of the infinite series of the\nCaputo definition is used to approximate the fractional-order derivative, the\nlength of the memory is truncated. Therefore, the fractional-order gradient\n(FGD) method with a fixed memory step and an adjustable number of terms is used\nto update the weights of the layers. Experiments were performed on the COVIDx\ndataset to demonstrate fast convergence, good accuracy, and the ability to\nbypass the local optimal point. We also compared the performance of the\ndeveloped fractional-order neural networks and Integer-order neural networks.\nThe results confirmed the effectiveness of our proposed model in the diagnosis\nof COVID-19.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taresh_M/0/1/0/all/0/1\">Mundher Mohammed Taresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_N/0/1/0/all/0/1\">Ningbo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_T/0/1/0/all/0/1\">Talal Ahmed Ali Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alghaili_M/0/1/0/all/0/1\">Mohammed Alghaili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weihua Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries. (arXiv:2205.00613v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00613","description":"<p>Accurate and consistent 3D tracking from multiple cameras is a key component\nin a vision-based autonomous driving system. It involves modeling 3D dynamic\nobjects in complex scenes across multiple cameras. This problem is inherently\nchallenging due to depth estimation, visual occlusions, appearance ambiguity,\netc. Moreover, objects are not consistently associated across time and cameras.\nTo address that, we propose an end-to-end \\textbf{MU}lti-camera\n\\textbf{TR}acking framework called MUTR3D. In contrast to prior works, MUTR3D\ndoes not explicitly rely on the spatial and appearance similarity of objects.\nInstead, our method introduces \\textit{3D track query} to model spatial and\nappearance coherent track for each object that appears in multiple cameras and\nmultiple frames. We use camera transformations to link 3D trackers with their\nobservations in 2D images. Each tracker is further refined according to the\nfeatures that are obtained from camera images. MUTR3D uses a set-to-set loss to\nmeasure the difference between the predicted tracking results and the ground\ntruths. Therefore, it does not require any post-processing such as non-maximum\nsuppression and/or bounding box association. MUTR3D outperforms\nstate-of-the-art methods by 5.3 AMOTA on the nuScenes dataset. Code is\navailable at: \\url{https://github.com/a1600012888/MUTR3D}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFC: Anatomically Informed Fiber Clustering with Self-supervised Deep Learning for Fast and Effective Tractography Parcellation. (arXiv:2205.00627v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00627","description":"<p>White matter fiber clustering (WMFC) parcellates tractography data into\nanatomically meaningful fiber bundles, usually in an unsupervised manner\nwithout the need of labeled ground truth data. While widely used WMFC\napproaches have shown good performance using classical machine learning\ntechniques, recent advances in deep learning reveal a promising direction\ntowards fast and effective WMFC. In this work, we propose a novel deep learning\nframework for WMFC, Deep Fiber Clustering (DFC), which solves the unsupervised\nclustering problem as a self-supervised learning task with a domain-specific\npretext task to predict pairwise fiber distances. This accelerates the fiber\nrepresentation learning to handle a known challenge in WMFC, i.e., the\nsensitivity of clustering results to the point ordering along fibers. We design\na novel network architecture that represents input fibers as point clouds and\nallows the incorporation of additional sources of input information from gray\nmatter parcellation. Thus DFC makes use of the combined white matter fiber\ngeometry and gray matter anatomical parcellation to improve anatomical\ncoherence of fiber clusters. In addition, DFC conducts outlier removal in a\nnatural way by rejecting fibers with low cluster assignment probabilities. We\nevaluate DFC on three independently acquired cohorts (including data from 220\nsubjects) and compare it to several state-of-the-art WMFC algorithms.\nExperimental results demonstrate superior performance of DFC in terms of\ncluster compactness, generalization ability, anatomical coherence, and\ncomputational efficiency. In addition, DFC parcellates whole brain tractography\nwith 50k fibers in about 1.5 minutes, providing a fast and efficient tool for\nlarge data analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tengfei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1\">Nikos Makris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design equivariant neural networks for 3D point cloud. (arXiv:2205.00630v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00630","description":"<p>This work seeks to improve the generalization and robustness of existing\nneural networks for 3D point clouds by inducing group equivariance under\ngeneral group transformations. The main challenge when designing equivariant\nmodels for point clouds is how to trade-off the performance of the model and\nthe complexity. Existing equivariant models are either too complicate to\nimplement or very high complexity. The main aim of this study is to build a\ngeneral procedure to introduce group equivariant property to SOTA models for 3D\npoint clouds. The group equivariant models built form our procedure are simple\nto implement, less complexity in comparison with the existing ones, and they\npreserve the strengths of the original SOTA backbone. From the results of the\nexperiments on object classification, it is shown that our methods are superior\nto other group equivariant models in performance and complexity. Moreover, our\nmethod also helps to improve the mIoU of semantic segmentation models. Overall,\nby using a combination of only-finite-rotation equivariance and augmentation,\nour models can outperform existing full $SO(3)$-equivariance models with much\ncheaper complexity and GPU memory. The proposed procedure is general and forms\na fundamental approach to group equivariant neural networks. We believe that it\ncan be easily adapted to other SOTA models in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trang_T/0/1/0/all/0/1\">Thuan N.A. Trang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1\">Thieu N. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khuong D. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Adversarial Training with Feature Separability. (arXiv:2205.00637v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00637","description":"<p>Deep Neural Network (DNN) are vulnerable to adversarial attacks. As a\ncountermeasure, adversarial training aims to achieve robustness based on the\nmin-max optimization problem and it has shown to be one of the most effective\ndefense strategies. However, in this work, we found that compared with natural\ntraining, adversarial training fails to learn better feature representations\nfor either clean or adversarial samples, which can be one reason why\nadversarial training tends to have severe overfitting issues and less satisfied\ngeneralize performance. Specifically, we observe two major shortcomings of the\nfeatures learned by existing adversarial training methods:(1) low intra-class\nfeature similarity; and (2) conservative inter-classes feature variance. To\novercome these shortcomings, we introduce a new concept of adversarial training\ngraph (ATG) with which the proposed adversarial training with feature\nseparability (ATFS) enables to coherently boost the intra-class feature\nsimilarity and increase inter-class feature variance. Through comprehensive\nexperiments, we demonstrate that the proposed ATFS framework significantly\nimproves both clean and robust performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Han Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Application to Generate Style Guided Compatible Outfit. (arXiv:2205.00663v1 [cs.IR])","link":"http://arxiv.org/abs/2205.00663","description":"<p>Fashion recommendation has witnessed a phenomenal growth of research,\nparticularly in the domains of shop-the-look, contextaware outfit creation,\npersonalizing outfit creation etc. Majority of the work in this area focuses on\nbetter understanding of the notion of complimentary relationship between\nlifestyle items. Quite recently, some works have realised that style plays a\nvital role in fashion, especially in the understanding of compatibility\nlearning and outfit creation. In this paper, we would like to present the\nend-to-end design of a methodology in which we aim to generate outfits guided\nby styles or themes using a novel style encoder network. We present an\nextensive analysis of different aspects of our method through various\nexperiments. We also provide a demonstration api to showcase the ability of our\nwork in generating outfits based on an anchor item and styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debopriyo Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_H/0/1/0/all/0/1\">Harsh Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakad1_L/0/1/0/all/0/1\">Lucky Dhakad1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya1_A/0/1/0/all/0/1\">Arnab Bhattacharya1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelliah_M/0/1/0/all/0/1\">Muthusamy Chelliah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal1_S/0/1/0/all/0/1\">Suyash Agarwal1</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Classical Multiclass Linear Discriminant Analysis with a Novel Prototype-based Interpretable Solution. (arXiv:2205.00668v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00668","description":"<p>Linear discriminant analysis (LDA) is a fundamental method for feature\nextraction and dimensionality reduction. Despite having many variants,\nclassical LDA has its importance, as it is a keystone in human knowledge about\npattern recognition. For a dataset containing $C$ clusters, the classical\nsolution to LDA extracts at most $C-1$ features. In this paper, we introduce a\nnovel solution to classical LDA, called LDA++, that yields $C$ features, each\none interpretable as measuring similarity to one cluster. This novel solution\nbridges between dimensionality reduction and multiclass classification.\nSpecifically, we prove that, under some mild conditions, the optimal weights of\na linear multiclass classifier for homoscedastic Gaussian data also make an\noptimal solution to LDA. In addition, this novel interpretable solution reveals\nsome new facts about LDA and its relation with PCA. We provide a complete\nnumerical solution for our novel method, covering the cases 1) when the scatter\nmatrices can be constructed explicitly, 2) when constructing the scatter\nmatrices is infeasible, and 3) the kernel extension. The code is available at\nhttps://github.com/k-ghiasi/LDA-plus-plus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_Shirazi_S/0/1/0/all/0/1\">Sayed Kamaledin Ghiasi-Shirazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Video Harmonization with Color Mapping Consistency. (arXiv:2205.00687v1 [cs.CV])","link":"http://arxiv.org/abs/2205.00687","description":"<p>Video harmonization aims to adjust the foreground of a composite video to\nmake it compatible with the background. So far, video harmonization has only\nreceived limited attention and there is no public dataset for video\nharmonization. In this work, we construct a new video harmonization dataset\nHYouTube by adjusting the foreground of real videos to create synthetic\ncomposite videos. Moreover, we consider the temporal consistency in video\nharmonization task. Unlike previous works which establish the spatial\ncorrespondence, we design a novel framework based on the assumption of color\nmapping consistency, which leverages the color mapping of neighboring frames to\nrefine the current frame. Extensive experiments on our HYouTube dataset prove\nthe effectiveness of our proposed framework. Our dataset and code are available\nat https://github.com/bcmi/Video-Harmonization-Dataset-HYouTube.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xinyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1\">Wenyan Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Graph Message Passing Networks. (arXiv:1908.06955v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1908.06955","description":"<p>Modelling long-range dependencies is critical for scene understanding tasks\nin computer vision. Although convolution neural networks (CNNs) have excelled\nin many vision tasks, they are still limited in capturing long-range structured\nrelationships as they typically consist of layers of local kernels. A\nfully-connected graph, such as the self-attention operation in Transformers, is\nbeneficial for such modelling, however, its computational overhead is\nprohibitive. In this paper, we propose a dynamic graph message passing network,\nthat significantly reduces the computational complexity compared to related\nworks modelling a fully-connected graph. This is achieved by adaptively\nsampling nodes in the graph, conditioned on the input, for message passing.\nBased on the sampled nodes, we dynamically predict node-dependent filter\nweights and the affinity matrix for propagating information between them. This\nformulation allows us to design a self-attention module, and more importantly a\nnew Transformer-based backbone network, that we use for both image\nclassification pretraining, and for addressing various downstream tasks (e.g.\nobject detection, instance and semantic segmentation). Using this model, we\nshow significant improvements with respect to strong, state-of-the-art\nbaselines on four different tasks. Our approach also outperforms\nfully-connected graphs while using substantially fewer floating-point\noperations and parameters. Code and models will be made publicly available at\nhttps://github.com/fudan-zvg/DGMN2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perception and Navigation in Autonomous Systems in the Era of Learning: A Survey. (arXiv:2001.02319v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.02319","description":"<p>Autonomous systems possess the features of inferring their own state,\nunderstanding their surroundings, and performing autonomous navigation. With\nthe applications of learning systems, like deep learning and reinforcement\nlearning, the visual-based self-state estimation, environment perception and\nnavigation capabilities of autonomous systems have been efficiently addressed,\nand many new learning-based algorithms have surfaced with respect to autonomous\nvisual perception and navigation. In this review, we focus on the applications\nof learning-based monocular approaches in ego-motion perception, environment\nperception and navigation in autonomous systems, which is different from\nprevious reviews that discussed traditional methods. First, we delineate the\nshortcomings of existing classical visual simultaneous localization and mapping\n(vSLAM) solutions, which demonstrate the necessity to integrate deep learning\ntechniques. Second, we review the visual-based environmental perception and\nunderstanding methods based on deep learning, including deep learning-based\nmonocular depth estimation, monocular ego-motion prediction, image enhancement,\nobject detection, semantic segmentation, and their combinations with\ntraditional vSLAM frameworks. Then, we focus on the visual navigation based on\nlearning systems, mainly including reinforcement learning and deep\nreinforcement learning. Finally, we examine several challenges and promising\ndirections discussed and concluded in related research of learning systems in\nthe era of computer science and robotics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weixing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenli Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_F/0/1/0/all/0/1\">Feng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurths_J/0/1/0/all/0/1\">Juergen Kurths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Frame based Deep View Synchronization for Unsynchronized Multi-Camera Surveillance. (arXiv:2007.03891v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.03891","description":"<p>Multi-camera surveillance has been an active research topic for understanding\nand modeling scenes. Compared to a single camera, multi-cameras provide larger\nfield-of-view and more object cues, and the related applications are multi-view\ncounting, multi-view tracking, 3D pose estimation or 3D reconstruction, etc. It\nis usually assumed that the cameras are all temporally synchronized when\ndesigning models for these multi-camera based tasks. However, this assumption\nis not always valid,especially for multi-camera systems with network\ntransmission delay and low frame-rates due to limited network bandwidth,\nresulting in desynchronization of the captured frames across cameras. To handle\nthe issue of unsynchronized multi-cameras, in this paper, we propose a\nsynchronization model that works in conjunction with existing DNN-based\nmulti-view models, thus avoiding the redesign of the whole model. Under the\nlow-fps regime, we assume that only a single relevant frame is available from\neach view, and synchronization is achieved by matching together image contents\nguided by epipolar geometry. We consider two variants of the model, based on\nwhere in the pipeline the synchronization occurs, scene-level synchronization\nand camera-level synchronization. The view synchronization step and the\ntask-specific view fusion and prediction step are unified in the same framework\nand trained in an end-to-end fashion. Our view synchronization models are\napplied to different DNNs-based multi-camera vision tasks under the\nunsynchronized setting, including multi-view counting and 3D pose estimation,\nand achieve good performance compared to baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Scale Recovery for Monocular Depth and Egomotion Estimation. (arXiv:2009.03787v5 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2009.03787","description":"<p>The self-supervised loss formulation for jointly training depth and egomotion\nneural networks with monocular images is well studied and has demonstrated\nstate-of-the-art accuracy. One of the main limitations of this approach,\nhowever, is that the depth and egomotion estimates are only determined up to an\nunknown scale. In this paper, we present a novel scale recovery loss that\nenforces consistency between a known camera height and the estimated camera\nheight, generating metric (scaled) depth and egomotion predictions. We show\nthat our proposed method is competitive with other scale recovery techniques\nthat require more information. Further, we demonstrate that our method\nfacilitates network retraining within new environments, whereas other\nscale-resolving approaches are incapable of doing so. Notably, our egomotion\nnetwork is able to produce more accurate estimates than a similar method which\nrecovers scale at test time only.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1\">Brandon Wagstaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-tailed Recognition by Routing Diverse Distribution-Aware Experts. (arXiv:2010.01809v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.01809","description":"<p>Natural data are often long-tail distributed over semantic classes. Existing\nrecognition methods tackle this imbalanced classification by placing more\nemphasis on the tail data, through class re-balancing/re-weighting or\nensembling over different data groups, resulting in increased tail accuracies\nbut reduced head accuracies.\n</p>\n<p>We take a dynamic view of the training data and provide a principled model\nbias and variance analysis as the training data fluctuates: Existing long-tail\nclassifiers invariably increase the model variance and the head-tail model bias\ngap remains large, due to more and larger confusion with hard negatives for the\ntail.\n</p>\n<p>We propose a new long-tailed classifier called RoutIng Diverse Experts\n(RIDE). It reduces the model variance with multiple experts, reduces the model\nbias with a distribution-aware diversity loss, reduces the computational cost\nwith a dynamic expert routing module. RIDE outperforms the state-of-the-art by\n5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is\nalso a universal framework that is applicable to various backbone networks,\nlong-tailed algorithms, and training mechanisms for consistent performance\ngains. Our code is available at:\nhttps://github.com/frank-xwang/RIDE-LongTailRecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1\">Long Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhongqi Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shedding Light on Blind Spots: Developing a Reference Architecture to Leverage Video Data for Process Mining. (arXiv:2010.11289v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.11289","description":"<p>Process mining is one of the most active research streams in business process\nmanagement. In recent years, numerous methods have been proposed for analyzing\nstructured process data. Yet, in many cases, it is only the digitized parts of\nprocesses that are directly captured from process-aware information systems,\nand manual activities often result in blind spots. While the use of video\ncameras to observe these activities could help to fill this gap, a standardized\napproach to extracting event logs from unstructured video data remains lacking.\nHere, we propose a reference architecture to bridge the gap between computer\nvision and process mining. Various evaluation activities (i.e., competing\nartifact analysis, prototyping, and real-world application) ensured that the\nproposed reference architecture allows flexible, use-case-driven, and\ncontext-specific instantiations. Our results also show that an exemplary\nsoftware prototype instantiation of the proposed reference architecture is\ncapable of automatically extracting most of the process-relevant events from\nunstructured video data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kratsch_W/0/1/0/all/0/1\">Wolfgang Kratsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenig_F/0/1/0/all/0/1\">Fabian Koenig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roeglinger_M/0/1/0/all/0/1\">Maximilian Roeglinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wide-Area Crowd Counting: Multi-View Fusion Networks for Counting in Large Scenes. (arXiv:2012.00946v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.00946","description":"<p>Crowd counting in single-view images has achieved outstanding performance on\nexisting counting datasets. However, single-view counting is not applicable to\nlarge and wide scenes (e.g., public parks, long subway platforms, or event\nspaces) because a single camera cannot capture the whole scene in adequate\ndetail for counting, e.g., when the scene is too large to fit into the\nfield-of-view of the camera, too long so that the resolution is too low on\nfaraway crowds, or when there are too many large objects that occlude large\nportions of the crowd. Therefore, to solve the wide-area counting task requires\nmultiple cameras with overlapping fields-of-view. In this paper, we propose a\ndeep neural network framework for multi-view crowd counting, which fuses\ninformation from multiple camera views to predict a scene-level density map on\nthe ground-plane of the 3D world. We consider three versions of the fusion\nframework: the late fusion model fuses camera-view density map; the naive early\nfusion model fuses camera-view feature maps; and the multi-view multi-scale\nearly fusion model ensures that features aligned to the same ground-plane point\nhave consistent scales. A rotation selection module further ensures consistent\nrotation alignment of the features. We test our 3 fusion models on 3 multi-view\ncounting datasets, PETS2009, DukeMTMC, and a newly collected multi-view\ncounting dataset containing a crowded street intersection. Our methods achieve\nstate-of-the-art results compared to other multi-view counting baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CosSGD: Communication-Efficient Federated Learning with a Simple Cosine-Based Quantization. (arXiv:2012.08241v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.08241","description":"<p>Federated learning is a promising framework to mitigate data privacy and\ncomputation concerns. However, the communication cost between the server and\nclients has become the major bottleneck for successful deployment. Despite\nnotable progress in gradient compression, the existing quantization methods\nrequire further improvement when low-bits compression is applied, especially\nthe overall systems often degenerate a lot when quantization are applied in\ndouble directions to compress model weights and gradients. In this work, we\npropose a simple cosine-based nonlinear quantization and achieve impressive\nresults in compressing round-trip communication costs. We are not only able to\ncompress model weights and gradients at higher ratios than previous methods,\nbut also achieve competing model performance at the same time. Further, our\napproach is highly suitable for federated learning problems since it has low\ncomputational complexity and requires only a little additional data to recover\nthe compressed information. Extensive experiments have been conducted on image\nclassification and brain tumor semantic segmentation using the CIFAR-10, and\nBraTS datasets where we show state-of-the-art effectiveness and impressive\ncommunication efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui-Po Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenk_M/0/1/0/all/0/1\">Maximilian Zenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slimmable Compressive Autoencoders for Practical Neural Image Compression. (arXiv:2103.15726v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.15726","description":"<p>Neural image compression leverages deep neural networks to outperform\ntraditional image codecs in rate-distortion performance. However, the resulting\nmodels are also heavy, computationally demanding and generally optimized for a\nsingle rate, limiting their practical use. Focusing on practical image\ncompression, we propose slimmable compressive autoencoders (SlimCAEs), where\nrate (R) and distortion (D) are jointly optimized for different capacities.\nOnce trained, encoders and decoders can be executed at different capacities,\nleading to different rates and complexities. We show that a successful\nimplementation of SlimCAEs requires suitable capacity-specific RD tradeoffs.\nOur experiments show that SlimCAEs are highly flexible models that provide\nexcellent rate-distortion performance, variable rate, and dynamic adjustment of\nmemory, computational cost and latency, thus addressing the main requirements\nof practical image compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1\">Yongmei Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mozerov_M/0/1/0/all/0/1\">Mikhail G. Mozerov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Generalized Face Presentation Attack Detection. (arXiv:2104.06595v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06595","description":"<p>Face presentation attack detection plays a critical role in the modern face\nrecognition pipeline. A face presentation attack detection model with good\ngeneralization can be obtained when it is trained with face images from\ndifferent input distributions and different types of spoof attacks. In reality,\ntraining data (both real face images and spoof images) are not directly shared\nbetween data owners due to legal and privacy issues. In this paper, with the\nmotivation of circumventing this challenge, we propose a Federated Face\nPresentation Attack Detection (FedPAD) framework that simultaneously takes\nadvantage of rich fPAD information available at different data owners while\npreserving data privacy. In the proposed framework, each data center locally\ntrains its own fPAD model. A server learns a global fPAD model by iteratively\naggregating model updates from all data centers without accessing private data\nin each of them. To equip the aggregated fPAD model in the server with better\ngeneralization ability to unseen attacks from users, following the basic idea\nof FedPAD, we further propose a Federated Generalized Face Presentation Attack\nDetection (FedGPAD) framework. A federated domain disentanglement strategy is\nintroduced in FedGPAD, which treats each data center as one domain and\ndecomposes the fPAD model into domain-invariant and domain-specific parts in\neach data center. Two parts disentangle the domain-invariant and\ndomain-specific features from images in each local data center, respectively. A\nserver learns a global fPAD model by only aggregating domain-invariant parts of\nthe fPAD models from data centers and thus a more generalized fPAD model can be\naggregated in server. We introduce the experimental setting to evaluate the\nproposed FedPAD and FedGPAD frameworks and carry out extensive experiments to\nprovide various insights about federated learning for fPAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Rui Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1\">Pramuditha Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuen_P/0/1/0/all/0/1\">Pong C. Yuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HINet: Half Instance Normalization Network for Image Restoration. (arXiv:2105.06086v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.06086","description":"<p>In this paper, we explore the role of Instance Normalization in low-level\nvision tasks. Specifically, we present a novel block: Half Instance\nNormalization Block (HIN Block), to boost the performance of image restoration\nnetworks. Based on HIN Block, we design a simple and powerful multi-stage\nnetwork named HINet, which consists of two subnetworks. With the help of HIN\nBlock, HINet surpasses the state-of-the-art (SOTA) on various image restoration\ntasks. For image denoising, we exceed it 0.11dB and 0.28 dB in PSNR on SIDD\ndataset, with only 7.5% and 30% of its multiplier-accumulator operations\n(MACs), 6.8 times and 2.9 times speedup respectively. For image deblurring, we\nget comparable performance with 22.5% of its MACs and 3.3 times speedup on REDS\nand GoPro datasets. For image deraining, we exceed it by 0.3 dB in PSNR on the\naverage result of multiple datasets with 1.4 times speedup. With HINet, we won\n1st place on the NTIRE 2021 Image Deblurring Challenge - Track2. JPEG\nArtifacts, with a PSNR of 29.70. The code is available at\nhttps://github.com/megvii-model/HINet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chengpeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing the Effect of Selection Bias on Generalization: A Thought Experiment. (arXiv:2105.09934v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09934","description":"<p>Learned systems in the domain of visual recognition and cognition impress in\npart because even though they are trained with datasets many orders of\nmagnitude smaller than the full population of possible images, they exhibit\nsufficient generalization to be applicable to new and previously unseen data.\nSince training data sets typically represent small sampling of a domain, the\npossibility of bias in their composition is very real. But what are the limits\nof generalization given such bias, and up to what point might it be sufficient\nfor a real problem task? Although many have examined issues regarding\ngeneralization, this question may require examining the data itself. Here, we\nfocus on the characteristics of the training data that may play a role. Other\ndisciplines have grappled with these problems, most interestingly epidemiology,\nwhere experimental bias is a critical concern. The range and nature of data\nbiases seen clinically are really quite relatable to learned vision systems.\nOne obvious way to deal with bias is to ensure a large enough training set, but\nthis might be infeasible for many domains. Another approach might be to perform\na statistical analysis of the actual training set, to determine if all aspects\nof the domain are fairly captured. This too is difficult, in part because the\nfull set of variables might not be known, or perhaps not even knowable. Here,\nwe try a different approach in the tradition of the Thought Experiment, whose\nmost famous instance may be Schr\\\"odinger's Cat. There are many types of bias\nas will be seen, but we focus only on one, selection bias. The point of the\nthought experiment is not to demonstrate problems with all learned systems.\nRather, this might be a simple theoretical tool to probe into bias during data\ncollection to highlight deficiencies that might then deserve extra attention\neither in data collection or system development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDOF-Tracker: Fast and Accurate Multiple Human Tracking by Skipped-Detection and Optical-Flow. (arXiv:2106.14259v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14259","description":"<p>Multiple human tracking is a fundamental problem for scene understanding.\nAlthough both accuracy and speed are required in real-world applications,\nrecent tracking methods based on deep learning have focused on accuracy and\nrequire substantial running time. This study aims to improve running speed by\nperforming human detection at a certain frame interval because it accounts for\nmost of the running time. The question is how to maintain accuracy while\nskipping human detection. In this paper, we propose a method that complements\nthe detection results with optical flow, based on the fact that someone's\nappearance does not change much between adjacent frames. To maintain the\ntracking accuracy, we introduce robust interest point selection within human\nregions and a tracking termination metric calculated by the distribution of the\ninterest points. On the MOT20 dataset in the MOTChallenge, the proposed\nSDOF-Tracker achieved the best performance in terms of the total running speed\nwhile maintaining the MOTA metric. Our code is available at\nhttps://github.com/hitottiez/sdof-tracker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_H/0/1/0/all/0/1\">Hitoshi Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komorita_S/0/1/0/all/0/1\">Satoshi Komorita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawanishi_Y/0/1/0/all/0/1\">Yasutomo Kawanishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murase_H/0/1/0/all/0/1\">Hiroshi Murase</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boggart: Towards General-Purpose Acceleration of Retrospective Video Analytics. (arXiv:2106.15315v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15315","description":"<p>Commercial retrospective video analytics platforms have increasingly adopted\ngeneral interfaces to support the custom queries and convolutional neural\nnetworks (CNNs) that different applications require. However, existing\noptimizations were designed for settings where CNNs were platform- (not user-)\ndetermined, and fail to meet at least one of the following key platform goals\nwhen that condition is violated: reliable accuracy, low latency, and minimal\nwasted work.\n</p>\n<p>We present Boggart, a system that simultaneously meets all three goals while\nsupporting the generality that today's platforms seek. Prior to queries being\nissued, Boggart carefully employs traditional computer vision algorithms to\ngenerate indices that are imprecise, but are fundamentally comprehensive across\ndifferent CNNs/queries. For each issued query, Boggart employs new techniques\nto quickly characterize the imprecision of its index, and sparingly run CNNs\n(and propagate the results to other frames) in a way that bounds accuracy\ndrops. Our results highlight that Boggart's improved generality comes at low\ncost, with speedups that match (and most often, exceed) prior, model-specific\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1\">Neil Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1\">Ravi Netravali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models. (arXiv:2107.02041v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02041","description":"<p>To improve the viewer's Quality of Experience (QoE) and optimize computer\ngraphics applications, 3D model quality assessment (3D-QA) has become an\nimportant task in the multimedia area. Point cloud and mesh are the two most\nwidely used digital representation formats of 3D models, the visual quality of\nwhich is quite sensitive to lossy operations like simplification and\ncompression. Therefore, many related studies such as point cloud quality\nassessment (PCQA) and mesh quality assessment (MQA) have been carried out to\nmeasure the visual quality degradations of 3D models. However, a large part of\nprevious studies utilize full-reference (FR) metrics, which indicates they can\nnot predict the quality level with the absence of the reference 3D model.\nFurthermore, few 3D-QA metrics consider color information, which significantly\nrestricts their effectiveness and scope of application. In this paper, we\npropose a no-reference (NR) quality assessment metric for colored 3D models\nrepresented by both point cloud and mesh. First, we project the 3D models from\n3D space into quality-related geometry and color feature domains. Then, the 3D\nnatural scene statistics (3D-NSS) and entropy are utilized to extract\nquality-aware features. Finally, machine learning is employed to regress the\nquality-aware features into visual quality scores. Our method is validated on\nthe colored point cloud quality assessment database (SJTU-PCQA), the Waterloo\npoint cloud assessment database (WPC), and the colored mesh quality assessment\ndatabase (CMDM). The experimental results show that the proposed method\noutperforms most compared NR 3D-QA metrics with competitive computational\nresources and greatly reduces the performance gap with the state-of-the-art FR\n3D-QA metrics. The code of the proposed model is publicly available now to\nfacilitate further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-supervised NeRF: Fewer Views and Faster Training for Free. (arXiv:2107.02791v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02791","description":"<p>A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting\nincorrect geometries when given an insufficient number of input views. One\npotential reason is that standard volumetric rendering does not enforce the\nconstraint that most of a scene's geometry consist of empty space and opaque\nsurfaces. We formalize the above assumption through DS-NeRF (Depth-supervised\nNeural Radiance Fields), a loss for learning radiance fields that takes\nadvantage of readily-available depth supervision. We leverage the fact that\ncurrent NeRF pipelines require images with known camera poses that are\ntypically estimated by running structure-from-motion (SFM). Crucially, SFM also\nproduces sparse 3D points that can be used as \"free\" depth supervision during\ntraining: we add a loss to encourage the distribution of a ray's terminating\ndepth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can\nrender better images given fewer training views while training 2-3x faster.\nFurther, we show that our loss is compatible with other recently proposed NeRF\nmethods, demonstrating that depth is a cheap and easily digestible supervisory\nsignal. And finally, we find that DS-NeRF can support other types of depth\nsupervision such as scanned depth sensors and RGB-D reconstruction outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kangle Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andrew Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RewriteNet: Reliable Scene Text Editing with Implicit Decomposition of Text Contents and Styles. (arXiv:2107.11041v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11041","description":"<p>Scene text editing (STE), which converts a text in a scene image into the\ndesired text while preserving an original style, is a challenging task due to a\ncomplex intervention between text and style. In this paper, we propose a novel\nSTE model, referred to as RewriteNet, that decomposes text images into content\nand style features and re-writes a text in the original image. Specifically,\nRewriteNet implicitly distinguishes the content from the style by introducing\nscene text recognition. Additionally, independent of the exact supervisions\nwith synthetic examples, we propose a self-supervised training scheme for\nunlabeled real-world images, which bridges the domain gap between synthetic and\nreal data. Our experiments present that RewriteNet achieves better generation\nperformances than other comparisons. Further analysis proves the feature\ndecomposition of RewriteNet and demonstrates the reliability and robustness\nthrough diverse experiments. Our implementation is publicly available at\n\\url{https://github.com/clovaai/rewritenet}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_M/0/1/0/all/0/1\">Moonbin Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gayoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMChat: Multi-Modal Chat Dataset on Social Media. (arXiv:2108.07154v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07154","description":"<p>Incorporating multi-modal contexts in conversation is important for\ndeveloping more engaging dialogue systems. In this work, we explore this\ndirection by introducing MMChat: a large-scale Chinese multi-modal dialogue\ncorpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous\ncorpora that are crowd-sourced or collected from fictitious movies, MMChat\ncontains image-grounded dialogues collected from real conversations on social\nmedia, in which the sparsity issue is observed. Specifically, image-initiated\ndialogues in common communications may deviate to some non-image-grounded\ntopics as the conversation proceeds. To better investigate this issue, we\nmanually annotate 100K dialogues from MMChat and further filter the corpus\naccordingly, which yields MMChat-hf. We develop a benchmark model to address\nthe sparsity issue in dialogue generation tasks by adapting the attention\nrouting mechanism on image features. Experiments demonstrate the usefulness of\nincorporating image features and the effectiveness of handling the sparsity of\nimage features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Misalignment Problem in Dense Object Detection. (arXiv:2108.12176v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12176","description":"<p>Object detection aims to localize and classify the objects in a given image,\nand these two tasks are sensitive to different object regions. Therefore, some\nlocations predict high-quality bounding boxes but low classification scores,\nand some locations are quite the opposite. A misalignment exists between the\ntwo tasks, and their features are spatially entangled. In order to solve the\nmisalignment problem, we propose a plug-in Spatial-disentangled and\nTask-aligned operator (SALT). By predicting two task-aware point sets that are\nlocated in each task's sensitive regions, SALT can reassign features from those\nregions and align them to the corresponding anchor point. Therefore, features\nfor the two tasks are spatially aligned and disentangled. To minimize the\ndifference between the two regression stages, we propose a Self-distillation\nregression (SDR) loss that can transfer knowledge from the refined regression\nresults to the coarse regression results. On the basis of SALT and SDR loss, we\npropose SALT-Net, which explicitly exploits task-aligned point-set features for\naccurate detection results. Extensive experiments on the MS-COCO dataset show\nthat our proposed methods can consistently boost different state-of-the-art\ndense detectors by $\\sim$2 AP. Notably, SALT-Net with Res2Net-101-DCN backbone\nachieves 53.8 AP on the MS-COCO test-dev.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Wind Power Curve Modeling Via Machine Vision: A Self-learning Deep Convolutional Network Based Method. (arXiv:2109.00894v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00894","description":"<p>This paper develops a novel self-training U-net (STU-net) based method for\nthe automated WPC model generation without requiring data pre-processing. The\nself-training (ST) process of STU-net has two steps. First, different from\ntraditional studies regarding the WPC modeling as a curve fitting problem, in\nthis paper, we renovate the WPC modeling formulation from a machine vision\naspect. To develop sufficiently diversified training samples, we synthesize\nsupervisory control and data acquisition (SCADA) data based on a set of S-shape\nfunctions depicting WPCs. These synthesized SCADA data and WPC functions are\nvisualized as images and paired as training samples(I_x, I_wpc). A U-net is\nthen developed to approximate the model recovering I_wpc from I_x. The\ndeveloped U-net is applied into observed SCADA data and can successfully\ngenerate the I_wpc. Moreover, we develop a pixel mapping and correction process\nto derive a mathematical form f_wpc representing I_wpcgenerated previously. The\nproposed STU-net only needs to train once and does not require any data\npreprocessing in applications. Numerical experiments based on 76 WTs are\nconducted to validate the superiority of the proposed method by benchmarking\nagainst classical WPC modeling methods. To demonstrate the repeatability of the\npresented research, we release our code at https://github.com/IkeYang/STU-net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Luoxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Long Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Learned Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.03082","description":"<p>This paper proposes a Perceptual Learned Video Compression (PLVC) approach\nwith recurrent conditional GAN. We employ the recurrent auto-encoder-based\ncompression network as the generator, and most importantly, we propose a\nrecurrent conditional discriminator, which judges on raw vs. compressed video\nconditioned on both spatial and temporal features, including the latent\nrepresentation, temporal motion and hidden states in recurrent cells. This way,\nthe adversarial training pushes the generated video to be not only spatially\nphoto-realistic but also temporally consistent with the groundtruth and\ncoherent among video frames. The experimental results show that the learned\nPLVC model compresses video with good perceptual quality at low bit-rate, and\nthat it outperforms the official HEVC test model (HM 16.20) and the existing\nlearned video compression approaches for several perceptual quality metrics and\nuser studies. The codes will be released at the project page:\nhttps://github.com/RenYang-home/PLVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ren Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Collaborative Representation Learning for Dynamic Saliency Prediction. (arXiv:2109.08371v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08371","description":"<p>The Dynamic Saliency Prediction (DSP) task simulates the human selective\nattention mechanism to perceive the dynamic scene, which is significant and\nimperative in many vision tasks. Most of existing methods only consider visual\ncues, while neglect the accompanied audio information, which can provide\ncomplementary information for the scene understanding. In fact, there exists a\nstrong relation between auditory and visual cues, and humans generally perceive\nthe surrounding scene by collaboratively sensing these cues. Motivated by this,\nan audio-visual collaborative representation learning method is proposed for\nthe DSP task, which explores the audio modality to better predict the dynamic\nsaliency map by assisting vision modality. The proposed method consists of\nthree parts: 1) audio-visual encoding, 2) audio-visual location, and 3)\ncollaborative integration parts. Firstly, a refined SoundNet architecture is\nadopted to encode audio modality for obtaining corresponding features, and a\nmodified 3D ResNet-50 architecture is employed to learn visual features,\ncontaining both spatial location and temporal motion information. Secondly, an\naudio-visual location part is devised to locate the sound source in the visual\nscene by learning the correspondence between audio-visual information. Thirdly,\na collaborative integration part is devised to adaptively aggregate\naudio-visual information and center-bias prior to generate the final saliency\nmap. Extensive experiments are conducted on six challenging audiovisual\neye-tracking datasets, including DIEM, AVAD, Coutrot1, Coutrot2, SumMe, and\nETMD, which shows significant superiority over state-of-the-art DSP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Hailong Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanxuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_E/0/1/0/all/0/1\">Ercheng Pei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12925","description":"<p>Due to its role in many computer vision tasks, image matching has been\nsubjected to an active investigation by researchers, which has lead to better\nand more discriminant feature descriptors and to more robust matching\nstrategies, also thanks to the advent of the deep learning and the increased\ncomputational power of the modern hardware. Despite of these achievements, the\nkeypoint extraction process at the base of the image matching pipeline has not\nseen equivalent progresses. This paper presents HarrisZ$^+$, an upgrade to the\nHarrisZ corner detector, optimized to synergically take advance of the recent\nimprovements of the other steps of the image matching pipeline. HarrisZ$^+$\ndoes not only consists of a tuning of the setup parameters, but introduces\nfurther refinements to the selection criteria delineated by HarrisZ, so\nproviding more, yet discriminative, keypoints, which are better distributed on\nthe image and with higher localization accuracy. The image matching pipeline\nincluding HarrisZ$^+$, together with the other modern components, obtained in\ndifferent recent matching benchmarks state-of-the-art results among the classic\nimage matching pipelines. These results are quite close to those obtained by\nthe more recent fully deep end-to-end trainable approaches and show that there\nis still a proper margin of improvement that can be granted by the research in\nclassic image matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1\">Fabio Bellavia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_D/0/1/0/all/0/1\">Dmytro Mishkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skill Induction and Planning with Latent Language. (arXiv:2110.01517v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.01517","description":"<p>We present a framework for learning hierarchical policies from\ndemonstrations, using sparse natural language annotations to guide the\ndiscovery of reusable skills for autonomous decision-making. We formulate a\ngenerative model of action sequences in which goals generate sequences of\nhigh-level subtask descriptions, and these descriptions generate sequences of\nlow-level actions. We describe how to train this model using primarily\nunannotated demonstrations by parsing demonstrations into sequences of named\nhigh-level subtasks, using only a small number of seed annotations to ground\nlanguage in action. In trained models, natural language commands index a\ncombinatorial library of skills; agents can use these skills to plan by\ngenerating high-level instruction sequences tailored to novel goals. We\nevaluate this approach in the ALFRED household simulation environment,\nproviding natural language annotations for only 10% of demonstrations. It\nachieves task completion rates comparable to state-of-the-art models\n(outperforming several recent methods with access to ground-truth plans during\ntraining and evaluation) while providing structured and human-readable\nhigh-level plans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pratyusha Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v5 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2110.06986","description":"<p>In this paper, we propose a new deep unfolding neural network based on the\nADMM algorithm for analysis Compressed Sensing. The proposed network jointly\nlearns a redundant analysis operator for sparsification and reconstructs the\nsignal of interest. We compare our proposed network with a state-of-the-art\nunfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we\nconsider not only image, but also speech datasets as test examples.\nComputational experiments demonstrate that our proposed network outperforms the\nstate-of-the-art deep unfolding network, consistently for both real-world image\nand speech datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kouni_V/0/1/0/all/0/1\">Vasiliki Kouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandropoulos_G/0/1/0/all/0/1\">George C. Alexandropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CeyMo: See More on Roads -- A Novel Benchmark Dataset for Road Marking Detection. (arXiv:2110.11867v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11867","description":"<p>In this paper, we introduce a novel road marking benchmark dataset for road\nmarking detection, addressing the limitations in the existing publicly\navailable datasets such as lack of challenging scenarios, prominence given to\nlane markings, unavailability of an evaluation script, lack of annotation\nformats and lower resolutions. Our dataset consists of 2887 total images with\n4706 road marking instances belonging to 11 classes. The images have a high\nresolution of 1920 x 1080 and capture a wide range of traffic, lighting and\nweather conditions. We provide road marking annotations in polygons, bounding\nboxes and pixel-level segmentation masks to facilitate a diverse range of road\nmarking detection algorithms. The evaluation metrics and the evaluation script\nwe provide, will further promote direct comparison of novel approaches for road\nmarking detection with existing methods. Furthermore, we evaluate the\neffectiveness of using both instance segmentation and object detection based\napproaches for the road marking detection task. Speed and accuracy scores for\ntwo instance segmentation models and two object detector models are provided as\na performance baseline for our benchmark dataset. The dataset and the\nevaluation script is publicly available at https://github.com/oshadajay/CeyMo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayasinghe_O/0/1/0/all/0/1\">Oshada Jayasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemachandra_S/0/1/0/all/0/1\">Sahan Hemachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anhettigama_D/0/1/0/all/0/1\">Damith Anhettigama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariyawasam_S/0/1/0/all/0/1\">Shenali Kariyawasam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasekara_P/0/1/0/all/0/1\">Peshala Jayasekara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOFT: Softmax-free Transformer with Linear Complexity. (arXiv:2110.11945v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11945","description":"<p>Vision transformers (ViTs) have pushed the state-of-the-art for various\nvisual recognition tasks by patch-wise image tokenization followed by\nself-attention. However, the employment of self-attention modules results in a\nquadratic complexity in both computation and memory usage. Various attempts on\napproximating the self-attention computation with linear complexity have been\nmade in Natural Language Processing. However, an in-depth analysis in this work\nshows that they are either theoretically flawed or empirically ineffective for\nvisual recognition. We further identify that their limitations are rooted in\nkeeping the softmax self-attention during approximations. Specifically,\nconventional self-attention is computed by normalizing the scaled dot-product\nbetween token feature vectors. Keeping this softmax operation challenges any\nsubsequent linearization efforts. Based on this insight, for the first time, a\nsoftmax-free transformer or SOFT is proposed. To remove softmax in\nself-attention, Gaussian kernel function is used to replace the dot-product\nsimilarity without further normalization. This enables a full self-attention\nmatrix to be approximated via a low-rank matrix decomposition. The robustness\nof the approximation is achieved by calculating its Moore-Penrose inverse using\na Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT\nsignificantly improves the computational efficiency of existing ViT variants.\nCrucially, with a linear complexity, much longer token sequences are permitted\nin SOFT, resulting in superior trade-off between accuracy and complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiachen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jinghan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weiguo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A vectorized sea horizon edge filter for maritime video processing tasks. (arXiv:2110.13694v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13694","description":"<p>The horizon line is a fundamental semantic feature in several maritime video\nprocessing tasks, such as digital video stabilization, camera calibration,\ntarget tracking, and target distance estimation. Visible range Electro-Optical\n(EO) sensors capture richer information in the daytime, which often comes with\nchallenging clutter. The best methods rely on tailored filters to keep,\nideally, only horizon edge pixels. These methods work well but often fail in\nthe case of edge-degraded horizons. Our first aim is to solve this problem\nwhile taking the real-time constraint into account; we propose a tailored edge\nfilter that relies on growing line segments with a low edge threshold and\nfilters them based on their slope, length, and relative position. Next, we\nbuild the filtered edge map by computing Cartesian coordinates of pixels across\nline segments that survived the filter. We infer the horizon from the filtered\nedge map using line fitting techniques and simple temporal information. We\nconsider the real-time constraint by vectorizing the computations and proposing\na better way to leverage image downsizing. Extensive experiments on 26,125\nvisible range frames show that the proposed method achieves significant\nrobustness while satisfying the real-time constraint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zardoua_Y/0/1/0/all/0/1\">Yassir Zardoua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_B/0/1/0/all/0/1\">Boulaala Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Astito Abdelali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Visual Transformers. (arXiv:2111.06091v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06091","description":"<p>Transformer, an attention-based encoder-decoder model, has already\nrevolutionized the field of natural language processing (NLP). Inspired by such\nsignificant achievements, some pioneering works have recently been done on\nemploying Transformer-liked architectures in the computer vision (CV) field,\nwhich have demonstrated their effectiveness on three fundamental CV tasks\n(classification, detection, and segmentation) as well as multiple sensory data\nstream (images, point clouds, and vision-language data). Because of their\ncompetitive modeling capabilities, the visual Transformers have achieved\nimpressive performance improvements over multiple benchmarks as compared with\nmodern Convolution Neural Networks (CNNs). In this survey, we have reviewed\nover one hundred of different visual Transformers comprehensively according to\nthree fundamental CV tasks and different data stream types, where a taxonomy is\nproposed to organize the representative methods according to their motivations,\nstructures, and application scenarios. Because of their differences on training\nsettings and dedicated vision tasks, we have also evaluated and compared all\nthese existing visual Transformers under different configurations. Furthermore,\nwe have revealed a series of essential but unexploited aspects that may empower\nsuch visual Transformers to stand out from numerous architectures, e.g., slack\nhigh-level semantic embeddings to bridge the gap between the visual\nTransformers and the sequential ones. Finally, three promising research\ndirections are suggested for future investment. We will continue to update the\nlatest articles and their released source codes at\nhttps://github.com/liuyang-ict/awesome-visual-transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongchao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIMEcraft: Handcrafted superpixel selection and inspection for Visual eXplanations. (arXiv:2111.08094v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08094","description":"<p>The increased interest in deep learning applications, and their\nhard-to-detect biases result in the need to validate and explain complex\nmodels. However, current explanation methods are limited as far as both the\nexplanation of the reasoning process and prediction results are concerned. They\nusually only show the location in the image that was important for model\nprediction. The lack of possibility to interact with explanations makes it\ndifficult to verify and understand exactly how the model works. This creates a\nsignificant risk when using the model. The risk is compounded by the fact that\nexplanations do not take into account the semantic meaning of the explained\nobjects. To escape from the trap of static and meaningless explanations, we\npropose a tool and a process called LIMEcraft. LIMEcraft enhances the process\nof explanation by allowing a user to interactively select semantically\nconsistent areas and thoroughly examine the prediction for the image instance\nin case of many image features. Experiments on several models show that our\ntool improves model safety by inspecting model fairness for image pieces that\nmay indicate model bias. The code is available at:\n<a href=\"http://github.com/MI2DataLab/LIMEcraft\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hryniewska_W/0/1/0/all/0/1\">Weronika Hryniewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grudzien_A/0/1/0/all/0/1\">Adrianna Grudzie&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1\">Przemys&#x142;aw Biecek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combined Scaling for Open-Vocabulary Image Classification. (arXiv:2111.10050v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.10050","description":"<p>We present a combined scaling method - named BASIC - that achieves 85.7%\ntop-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from\nany labeled ImageNet example. This accuracy surpasses best published similar\nmodels - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant\nimprovements in robustness benchmarks. For instance, on 5 test sets with\nnatural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our\nmodel achieves 84.3% top-1 average accuracy, only a small drop from its\noriginal ImageNet accuracy.\n</p>\n<p>To achieve these results, we scale up the contrastive learning framework of\nCLIP and ALIGN in three dimensions: data size, model size, and batch size. Our\ndataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x\nlarger than CLIP. Our largest model has 3B weights, which is 3.75x larger in\nparameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size\nis 65536 which is 2x more than CLIP and 4x more than ALIGN.\n</p>\n<p>We encountered two main challenges with the scaling rules of BASIC. First,\nthe main challenge with implementing the combined scaling rules of BASIC is the\nlimited memory of accelerators, such as GPUs and TPUs. To overcome the memory\nlimit, we propose two simple methods which make use of gradient checkpointing\nand model parallelism. Second, while increasing the dataset size and the model\nsize has been the defacto method to improve the performance of deep learning\nmodels like BASIC, the effect of a large contrastive batch size on such\ncontrastive-trained image-text models is not well-understood. To shed light on\nthe benefits of large contrastive batch sizes, we develop a theoretical\nframework which shows that larger contrastive batch sizes lead to smaller\ngeneralization gaps for image-text models such as BASIC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking. (arXiv:2111.11892v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11892","description":"<p>Multi-Camera Multi-Object Tracking is currently drawing attention in the\ncomputer vision field due to its superior performance in real-world\napplications such as video surveillance in crowded scenes or in wide spaces. In\nthis work, we propose a mathematically elegant multi-camera multiple object\ntracking approach based on a spatial-temporal lifted multicut formulation. Our\nmodel utilizes state-of-the-art tracklets produced by single-camera trackers as\nproposals. As these tracklets may contain ID-Switch errors, we refine them\nthrough a novel pre-clustering obtained from 3D geometry projections. As a\nresult, we derive a better tracking graph without ID switches and more precise\naffinity costs for the data association phase. Tracklets are then matched to\nmulti-camera trajectories by solving a global lifted multicut formulation that\nincorporates short and long-range temporal interactions on tracklets located in\nthe same camera as well as inter-camera ones. Experimental results on the\nWildTrack dataset yield near-perfect performance, outperforming\nstate-of-the-art trackers on Campus while being on par on the PETS-09 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy M. H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henschel_R/0/1/0/all/0/1\">Roberto Henschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPCL: A New Framework for Domain Adaptive Semantic Segmentation via Semantic Prototype-based Contrastive Learning. (arXiv:2111.12358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12358","description":"<p>Although there is significant progress in supervised semantic segmentation,\nit remains challenging to deploy the segmentation models to unseen domains due\nto domain biases. Domain adaptation can help in this regard by transferring\nknowledge from a labeled source domain to an unlabeled target domain. Previous\nmethods typically attempt to perform the adaptation on global features,\nhowever, the local semantic affiliations accounting for each pixel in the\nfeature space are often ignored, resulting in less discriminability. To solve\nthis issue, we propose a novel semantic prototype-based contrastive learning\nframework for fine-grained class alignment. Specifically, the semantic\nprototypes provide supervisory signals for per-pixel discriminative\nrepresentation learning and each pixel of source and target domains in the\nfeature space is required to reflect the content of the corresponding semantic\nprototype. In this way, our framework is able to explicitly make intra-class\npixel representations closer and inter-class pixel representations further\napart to improve the robustness of the segmentation model as well as alleviate\nthe domain shift problem. Our method is easy to implement and attains superior\nresults compared to state-of-the-art approaches, as is demonstrated with a\nnumber of experiments. The code is publicly available at\nhttps://github.com/BinhuiXie/SPCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Binhui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingjia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Targeted Supervised Contrastive Learning for Long-Tailed Recognition. (arXiv:2111.13998v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13998","description":"<p>Real-world data often exhibits long tail distributions with heavy class\nimbalance, where the majority classes can dominate the training process and\nalter the decision boundaries of the minority classes. Recently, researchers\nhave investigated the potential of supervised contrastive learning for\nlong-tailed recognition, and demonstrated that it provides a strong performance\ngain. In this paper, we show that while supervised contrastive learning can\nhelp improve performance, past baselines suffer from poor uniformity brought in\nby imbalanced data distribution. This poor uniformity manifests in samples from\nthe minority class having poor separability in the feature space. To address\nthis problem, we propose targeted supervised contrastive learning (TSC), which\nimproves the uniformity of the feature distribution on the hypersphere. TSC\nfirst generates a set of targets uniformly distributed on a hypersphere. It\nthen makes the features of different classes converge to these distinct and\nuniformly distributed targets during training. This forces all classes,\nincluding minority classes, to maintain a uniform distribution in the feature\nspace, improves class boundaries, and provides better generalization even in\nthe presence of long-tail data. Experiments on multiple datasets show that TSC\nachieves state-of-the-art performance on long-tailed recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Peng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lijie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuzhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1\">Piotr Indyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1\">Dina Katabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-Based Video Coding with Joint Deep Compression and Enhancement. (arXiv:2111.14474v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.14474","description":"<p>The end-to-end learning-based video compression has attracted substantial\nattentions by paving another way to compress video signals as stacked visual\nfeatures. This paper proposes an efficient end-to-end deep video codec with\njointly optimized compression and enhancement modules (JCEVC). First, we\npropose a dual-path generative adversarial network (DPEG) to reconstruct video\ndetails after compression. An $\\alpha$-path facilitates the structure\ninformation reconstruction with a large receptive field and multi-frame\nreferences, while a $\\beta$-path facilitates the reconstruction of local\ntextures. Both paths are fused and co-trained within a generative-adversarial\nprocess. Second, we reuse the DPEG network in both motion compensation and\nquality enhancement modules, which are further combined with other necessary\nmodules to formulate our JCEVC framework. Third, we employ a joint training of\ndeep video compression and enhancement that further improves the\nrate-distortion (RD) performance of compression. Compared with x265 LDP very\nfast mode, our JCEVC reduces the average bit-per-pixel (bpp) by 39.39\\%/54.92\\%\nat the same PSNR/MS-SSIM, which outperforms the state-of-the-art deep video\ncodecs by a considerable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tiesong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1\">Weize Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_H/0/1/0/all/0/1\">Hongji Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_Y/0/1/0/all/0/1\">Yuzhen Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSG0: Continual Urban Scene Generation with Zero Forgetting. (arXiv:2112.03252v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03252","description":"<p>With the rapid advances in generative adversarial networks (GANs), the visual\nquality of synthesised scenes keeps improving, including for complex urban\nscenes with applications to automated driving. We address in this work a\ncontinual scene generation setup in which GANs are trained on a stream of\ndistinct domains; ideally, the learned models should eventually be able to\ngenerate new scenes in all seen domains. This setup reflects the real-life\nscenario where data are continuously acquired in different places at different\ntimes. In such a continual setup, we aim for learning with zero forgetting,\n\\IE, with no degradation in synthesis quality over earlier domains due to\ncatastrophic forgetting. To this end, we introduce a novel framework that not\nonly (i) enables seamless knowledge transfer in continual training but also\n(ii) guarantees zero forgetting with a small overhead cost. While being more\nmemory efficient, thanks to continual learning, our model obtains better\nsynthesis quality as compared against the brute-force solution that trains one\nfull model for each domain. Especially, under extreme low-data regimes, our\napproach outperforms the brute-force one by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1\">Himalaya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tuan-Hung Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection. (arXiv:2112.04532v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04532","description":"<p>Object detection plays a key role in many security-critical systems.\nAdversarial patch attacks, which are easy to implement in the physical world,\npose a serious threat to state-of-the-art object detectors. Developing reliable\ndefenses for object detectors against patch attacks is critical but severely\nunderstudied. In this paper, we propose Segment and Complete defense (SAC), a\ngeneral framework for defending object detectors against patch attacks through\ndetection and removal of adversarial patches. We first train a patch segmenter\nthat outputs patch masks which provide pixel-level localization of adversarial\npatches. We then propose a self adversarial training algorithm to robustify the\npatch segmenter. In addition, we design a robust shape completion algorithm,\nwhich is guaranteed to remove the entire patch from the images if the outputs\nof the patch segmenter are within a certain Hamming distance of the\nground-truth patch masks. Our experiments on COCO and xView datasets\ndemonstrate that SAC achieves superior robustness even under strong adaptive\nattacks with no reduction in performance on clean images, and generalizes well\nto unseen patch shapes, attack budgets, and unseen attack methods. Furthermore,\nwe present the APRICOT-Mask dataset, which augments the APRICOT dataset with\npixel-level annotations of adversarial patches. We show SAC can significantly\nreduce the targeted attack success rate of physical patch attacks. Our code is\navailable at https://github.com/joellliu/SegmentAndComplete.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1\">Alexander Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1\">Chun Pong Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeadNeRF: A Real-time NeRF-based Parametric Head Model. (arXiv:2112.05637v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05637","description":"<p>In this paper, we propose HeadNeRF, a novel NeRF-based parametric head model\nthat integrates the neural radiance field to the parametric representation of\nthe human head. It can render high fidelity head images in real-time on modern\nGPUs, and supports directly controlling the generated images' rendering pose\nand various semantic attributes. Different from existing related parametric\nmodels, we use the neural radiance fields as a novel 3D proxy instead of the\ntraditional 3D textured mesh, which makes that HeadNeRF is able to generate\nhigh fidelity images. However, the computationally expensive rendering process\nof the original NeRF hinders the construction of the parametric NeRF model. To\naddress this issue, we adopt the strategy of integrating 2D neural rendering to\nthe rendering process of NeRF and design novel loss terms. As a result, the\nrendering speed of HeadNeRF can be significantly accelerated, and the rendering\ntime of one frame is reduced from 5s to 25ms. The well designed loss terms also\nimprove the rendering accuracy, and the fine-level details of the human head,\nsuch as the gaps between teeth, wrinkles, and beards, can be represented and\nsynthesized by HeadNeRF. Extensive experimental results and several\napplications demonstrate its effectiveness. The trained parametric model is\navailable at https://github.com/CrisHY1995/headnerf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Haiyao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ligang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Planner-Actor-Critic for Unsupervised Deformable Image Registration. (arXiv:2112.07415v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.07415","description":"<p>Large deformations of organs, caused by diverse shapes and nonlinear shape\nchanges, pose a significant challenge for medical image registration.\nTraditional registration methods need to iteratively optimize an objective\nfunction via a specific deformation model along with meticulous parameter\ntuning, but which have limited capabilities in registering images with large\ndeformations. While deep learning-based methods can learn the complex mapping\nfrom input images to their respective deformation field, it is regression-based\nand is prone to be stuck at local minima, particularly when large deformations\nare involved. To this end, we present Stochastic Planner-Actor-Critic (SPAC), a\nnovel reinforcement learning-based framework that performs step-wise\nregistration. The key notion is warping a moving image successively by each\ntime step to finally align to a fixed image. Considering that it is challenging\nto handle high dimensional continuous action and state spaces in the\nconventional reinforcement learning (RL) framework, we introduce a new concept\n`Plan' to the standard Actor-Critic model, which is of low dimension and can\nfacilitate the actor to generate a tractable high dimensional action. The\nentire framework is based on unsupervised training and operates in an\nend-to-end manner. We evaluate our method on several 2D and 3D medical image\ndatasets, some of which contain large deformations. Our empirical results\nhighlight that our work achieves consistent, significant gains and outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_Z/0/1/0/all/0/1\">Ziwei Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1\">Jing Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_B/0/1/0/all/0/1\">Bin Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_Y/0/1/0/all/0/1\">Youbing Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_Q/0/1/0/all/0/1\">Qi Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continually Learning Self-Supervised Representations with Projected Functional Regularization. (arXiv:2112.15022v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15022","description":"<p>Recent self-supervised learning methods are able to learn high-quality image\nrepresentations and are closing the gap with supervised approaches. However,\nthese methods are unable to acquire new knowledge incrementally -- they are, in\nfact, mostly used only as a pre-training phase over IID data. In this work we\ninvestigate self-supervised methods in continual learning regimes without any\nreplay mechanism. We show that naive functional regularization, also known as\nfeature distillation, leads to lower plasticity and limits continual learning\nperformance. Instead, we propose Projected Functional Regularization in which a\nseparate temporal projection network ensures that the newly learned feature\nspace preserves information of the previous one, while at the same time\nallowing for the learning of new features. This prevents forgetting while\nmaintaining the plasticity of the learner. Comparison with other incremental\nlearning approaches applied to self-supervision demonstrates that our method\nobtains competitive performance in different scenarios and on multiple\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Villa_A/0/1/0/all/0/1\">Alex Gomez-Villa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bartlomiej Twardowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagdanov_A/0/1/0/all/0/1\">Andrew D. Bagdanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Dual Contouring. (arXiv:2202.01999v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.01999","description":"<p>We introduce neural dual contouring (NDC), a new data-driven approach to mesh\nreconstruction based on dual contouring (DC). Like traditional DC, it produces\nexactly one vertex per grid cell and one quad for each grid edge intersection,\na natural and efficient structure for reproducing sharp features. However,\nrather than computing vertex locations and edge crossings with hand-crafted\nfunctions that depend directly on difficult-to-obtain surface gradients, NDC\nuses a neural network to predict them. As a result, NDC can be trained to\nproduce meshes from signed or unsigned distance fields, binary voxel grids, or\npoint clouds (with or without normals); and it can produce open surfaces in\ncases where the input represents a sheet or partial surface. During experiments\nwith five prominent datasets, we find that NDC, when trained on one of the\ndatasets, generalizes well to the others. Furthermore, NDC provides better\nsurface reconstruction accuracy, feature preservation, output complexity,\ntriangle quality, and inference time in comparison to previous learned (e.g.,\nneural marching cubes, convolutional occupancy networks) and traditional (e.g.,\nPoisson) methods. Code and data are available at\nhttps://github.com/czq142857/NDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heed the Noise in Performance Evaluations in Neural Architecture Search. (arXiv:2202.02078v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2202.02078","description":"<p>Neural Architecture Search (NAS) has recently become a topic of great\ninterest. However, there is a potentially impactful issue within NAS that\nremains largely unrecognized: noise. Due to stochastic factors in neural\nnetwork initialization, training, and the chosen train/validation dataset\nsplit, the performance evaluation of a neural network architecture, which is\noften based on a single learning run, is also stochastic. This may have a\nparticularly large impact if a dataset is small. We therefore propose to reduce\nthis noise by evaluating architectures based on average performance over\nmultiple network training runs using different random seeds and\ncross-validation. We perform experiments for a combinatorial optimization\nformulation of NAS in which we vary noise reduction levels. We use the same\ncomputational budget for each noise level in terms of network training runs,\ni.e., we allow less architecture evaluations when averaging over more training\nruns. Multiple search algorithms are considered, including evolutionary\nalgorithms which generally perform well for NAS. We use two publicly available\ndatasets from the medical image segmentation domain where datasets are often\nlimited and variability among samples is often high. Our results show that\nreducing noise in architecture evaluations enables finding better architectures\nby all considered search algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dushatskiy_A/0/1/0/all/0/1\">Arkadiy Dushatskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A. N. Bosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification. (arXiv:2202.02472v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2202.02472","description":"<p>Deep learning (DL) has been widely investigated in a vast majority of\napplications in electroencephalography (EEG)-based brain-computer interfaces\n(BCIs), especially for motor imagery (MI) classification in the past five\nyears. The mainstream DL methodology for the MI-EEG classification exploits the\ntemporospatial patterns of EEG signals using convolutional neural networks\n(CNNs), which have been particularly successful in visual images. However,\nsince the statistical characteristics of visual images depart radically from\nEEG signals, a natural question arises whether an alternative network\narchitecture exists apart from CNNs. To address this question, we propose a\nnovel geometric deep learning (GDL) framework called Tensor-CSPNet, which\ncharacterizes spatial covariance matrices derived from EEG signals on symmetric\npositive definite (SPD) manifolds and fully captures the temporospatiofrequency\npatterns using existing deep neural networks on SPD manifolds, integrating with\nexperiences from many successful MI-EEG classifiers to optimize the framework.\nIn the experiments, Tensor-CSPNet attains or slightly outperforms the current\nstate-of-the-art performance on the cross-validation and holdout scenarios in\ntwo commonly-used MI-EEG datasets. Moreover, the visualization and\ninterpretability analyses also exhibit the validity of Tensor-CSPNet for the\nMI-EEG classification. To conclude, in this study, we provide a feasible answer\nto the question by generalizing the DL methodologies on SPD manifolds, which\nindicates the start of a specific GDL methodology for the MI-EEG\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ju_C/0/1/0/all/0/1\">Ce Ju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08227","description":"<p>Digitizing physical objects into the virtual world has the potential to\nunlock new research and applications in embodied AI and mixed reality. This\nwork focuses on recreating interactive digital twins of real-world articulated\nobjects, which can be directly imported into virtual environments. We introduce\nDitto to learn articulation model estimation and 3D geometry reconstruction of\nan articulated object through interactive perception. Given a pair of visual\nobservations of an articulated object before and after interaction, Ditto\nreconstructs part-level geometry and estimates the articulation model of the\nobject. We employ implicit neural representations for joint geometry and\narticulation modeling. Our experiments show that Ditto effectively builds\ndigital twins of articulated objects in a category-agnostic way. We also apply\nDitto to real-world objects and deploy the recreated digital twins in physical\nsimulation. Code and additional results are available at\nhttps://ut-austin-rpl.github.io/Ditto\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhenyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Cheng-Chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey with Quantitative Comparison of Image Analysis Methods for Microorganism Biovolume Measurements. (arXiv:2202.09020v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09020","description":"<p>With the acceleration of urbanization and living standards, microorganisms\nplay increasingly important roles in industrial production, bio-technique, and\nfood safety testing. Microorganism biovolume measurements are one of the\nessential parts of microbial analysis. However, traditional manual measurement\nmethods are time-consuming and challenging to measure the characteristics\nprecisely. With the development of digital image processing techniques, the\ncharacteristics of the microbial population can be detected and quantified. The\nchanging trend can be adjusted in time and provided a basis for the\nimprovement. The applications of the microorganism biovolume measurement method\nhave developed since the 1980s. More than 62 articles are reviewed in this\nstudy, and the articles are grouped by digital image segmentation methods with\nperiods. This study has high research significance and application value, which\ncan be referred to microbial researchers to have a comprehensive understanding\nof microorganism biovolume measurements using digital image analysis methods\nand potential applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Molecular Prior Distribution for Bayesian Inference Based on Wilson Statistics. (arXiv:2202.09388v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2202.09388","description":"<p>Background and Objective: Wilson statistics describe well the power spectrum\nof proteins at high frequencies. Therefore, it has found several applications\nin structural biology, e.g., it is the basis for sharpening steps used in\ncryogenic electron microscopy (cryo-EM). A recent paper gave the first rigorous\nproof of Wilson statistics based on a formalism of Wilson's original argument.\nThis new analysis also leads to statistical estimates of the scattering\npotential of proteins that reveal a correlation between neighboring Fourier\ncoefficients. Here we exploit these estimates to craft a novel prior that can\nbe used for Bayesian inference of molecular structures. Methods: We describe\nthe properties of the prior and the computation of its hyperparameters. We then\nevaluate the prior on two synthetic linear inverse problems, and compare\nagainst a popular prior in cryo-EM reconstruction at a range of SNRs. Results:\nWe show that the new prior effectively suppresses noise and fills-in low SNR\nregions in the spectral domain. Furthermore, it improves the resolution of\nestimates on the problems considered for a wide range of SNR and produces\nFourier Shell Correlation curves that are insensitive to masking effects.\nConclusions: We analyze the assumptions in the model, discuss relations to\nother regularization strategies, and postulate on potential implications for\nstructure determination in cryo-EM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Gilles_M/0/1/0/all/0/1\">Marc Aur&#xe8;le Gilles</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Singer_A/0/1/0/all/0/1\">Amit Singer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Cross-Layer Attention for Image Restoration. (arXiv:2203.03619v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03619","description":"<p>Non-local attention module has been proven to be crucial for image\nrestoration. Conventional non-local attention processes features of each layer\nseparately, so it risks missing correlation between features among different\nlayers. To address this problem, we propose Cross-Layer Attention (CLA) module\nin this paper. Instead of finding correlated key pixels within the same layer,\neach query pixel can attend to key pixels at previous layers of the network. In\norder to further enhance the learning capability and reduce the inference cost\nof CLA, we further propose Adaptive CLA, or ACLA, as an improved CLA. Two\nadaptive designs are proposed for ACLA: 1) adaptively selecting the keys for\nnon-local attention at each layer; 2) automatically searching for the insertion\nlocations for ACLA modules. By these two adaptive designs, ACLA dynamically\nselects the number of keys to be aggregated for non-local attention at layer.\nIn addition, ACLA searches for the optimal insert positions of ACLA modules by\na neural architecture search method to render a compact neural network with\ncompelling performance. Extensive experiments on image restoration tasks,\nincluding single image super-resolution, image denoising, image demosaicing,\nand image compression artifacts reduction, validate the effectiveness and\nefficiency of ACLA. The code of CLA and ACLA is available at\n\\url{https://github.com/SDL-ASU/ACLA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yancheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yingzhen Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers. (arXiv:2203.03682v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.03682","description":"<p>In this work, we consider the problem of learning a perception model for\nmonocular robot navigation using few annotated images. Using a Vision\nTransformer (ViT) pretrained with a label-free self-supervised method, we\nsuccessfully train a coarse image segmentation model for the Duckietown\nenvironment using 70 training images. Our model performs coarse image\nsegmentation at the 8x8 patch level, and the inference resolution can be\nadjusted to balance prediction granularity and real-time perception\nconstraints. We study how best to adapt a ViT to our task and environment, and\nfind that some lightweight architectures can yield good single-image\nsegmentation at a usable frame rate, even on CPU. The resulting perception\nmodel is used as the backbone for a simple yet robust visual servoing agent,\nwhich we deploy on a differential drive mobile robot to perform two tasks: lane\nfollowing and obstacle avoidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saavedra_Ruiz_M/0/1/0/all/0/1\">Miguel Saavedra-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_S/0/1/0/all/0/1\">Sacha Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1\">Liam Paull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive End-to-End Object Detection in Crowded Scenes. (arXiv:2203.07669v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07669","description":"<p>In this paper, we propose a new query-based detection framework for crowd\ndetection. Previous query-based detectors suffer from two drawbacks: first,\nmultiple predictions will be inferred for a single object, typically in crowded\nscenes; second, the performance saturates as the depth of the decoding stage\nincreases. Benefiting from the nature of the one-to-one label assignment rule,\nwe propose a progressive predicting method to address the above issues.\nSpecifically, we first select accepted queries prone to generate true positive\npredictions, then refine the rest noisy queries according to the previously\naccepted predictions. Experiments show that our method can significantly boost\nthe performance of query-based detectors in crowded scenes. Equipped with our\napproach, Sparse RCNN achieves 92.0\\% $\\text{AP}$, 41.4\\% $\\text{MR}^{-2}$ and\n83.2\\% $\\text{JI}$ on the challenging CrowdHuman \\cite{shao2018crowdhuman}\ndataset, outperforming the box-based method MIP \\cite{chu2020detection} that\nspecifies in handling crowded scenarios. Moreover, the proposed method, robust\nto crowdedness, can still obtain consistent improvements on moderately and\nslightly crowded datasets like CityPersons \\cite{zhang2017citypersons} and COCO\n\\cite{lin2014microsoft}. Code will be made publicly available at\nhttps://github.com/megvii-model/Iter-E2EDET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1\">Anlin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities. (arXiv:2203.14712v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14712","description":"<p>Assembly101 is a new procedural activity dataset featuring 4321 videos of\npeople assembling and disassembling 101 \"take-apart\" toy vehicles. Participants\nwork without fixed instructions, and the sequences feature rich and natural\nvariations in action ordering, mistakes, and corrections. Assembly101 is the\nfirst multi-view action dataset, with simultaneous static (8) and egocentric\n(4) recordings. Sequences are annotated with more than 100K coarse and 1M\nfine-grained action segments, and 18M 3D hand poses. We benchmark on three\naction understanding tasks: recognition, anticipation and temporal\nsegmentation. Additionally, we propose a novel task of detecting mistakes. The\nunique recording format and rich set of annotations allow us to investigate\ngeneralization to new toys, cross-view transfer, long-tailed distributions, and\npose vs. appearance. We envision that Assembly101 will serve as a new challenge\nto investigate various activity understanding problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sener_F/0/1/0/all/0/1\">Fadime Sener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_D/0/1/0/all/0/1\">Dibyadip Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelepov_D/0/1/0/all/0/1\">Daniel Shelepov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhania_D/0/1/0/all/0/1\">Dipika Singhania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Robert Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition. (arXiv:2203.16670v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16670","description":"<p>Intrinsic image decomposition is the process of recovering the image\nformation components (reflectance and shading) from an image. Previous methods\nemploy either explicit priors to constrain the problem or implicit constraints\nas formulated by their losses (deep learning). These methods can be negatively\ninfluenced by strong illumination conditions causing shading-reflectance\nleakages.\n</p>\n<p>Therefore, in this paper, an end-to-end edge-driven hybrid CNN approach is\nproposed for intrinsic image decomposition. Edges correspond to illumination\ninvariant gradients. To handle hard negative illumination transitions, a\nhierarchical approach is taken including global and local refinement layers. We\nmake use of attention layers to further strengthen the learning process.\n</p>\n<p>An extensive ablation study and large scale experiments are conducted showing\nthat it is beneficial for edge-driven hybrid IID networks to make use of\nillumination invariant descriptors and that separating global and local cues\nhelps in improving the performance of the network. Finally, it is shown that\nthe proposed method obtains state of the art performance and is able to\ngeneralise well to real world images. The project page with pretrained models,\nfinetuned models and network code can be found at\nhttps://ivi.fnwi.uva.nl/cv/pienet/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Partha Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaoglu_S/0/1/0/all/0/1\">Sezer Karaoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1\">Theo Gevers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00298","description":"<p>To make full use of computer vision technology in stores, it is required to\nconsider the actual needs that fit the characteristics of the retail scene.\nPursuing this goal, we introduce the United Retail Datasets (Unitail), a\nlarge-scale benchmark of basic visual tasks on products that challenges\nalgorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped\ninstances annotated, the Unitail offers a detection dataset to align product\nappearance better. Furthermore, it provides a gallery-style OCR dataset\ncontaining 1454 product categories, 30k text regions, and 21k transcriptions to\nenable robust reading on products and motivate enhanced product matching.\nBesides benchmarking the datasets using various state-of-the-arts, we customize\na new detector for product detection and provide a simple OCR-based matching\nsolution that verifies its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaiwang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_J/0/1/0/all/0/1\">Jiachen Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_U/0/1/0/all/0/1\">Uzair Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Logit Adjustment. (arXiv:2204.11822v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11822","description":"<p>Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses\nchallenges in recognizing novel classes in the test phase. The development of\ngenerative models enables current GZSL techniques to probe further into the\nsemantic-visual link, culminating in a two-stage form that includes a generator\nand a classifier. However, existing generation-based methods focus on enhancing\nthe generator's effect while neglecting the improvement of the classifier. In\nthis paper, we first analyze of two properties of the generated pseudo unseen\nsamples: bias and homogeneity. Then, we perform variational Bayesian inference\nto back-derive the evaluation metrics, which reflects the balance of the seen\nand unseen classes. As a consequence of our derivation, the aforementioned two\nproperties are incorporated into the classifier training as seen-unseen priors\nvia logit adjustment. The Zero-Shot Logit Adjustment further puts\nsemantic-based classifiers into effect in generation-based GZSL. Our\nexperiments demonstrate that the proposed technique achieves state-of-the-art\nwhen combined with the basic generator, and it can improve various generative\nZero-Shot Learning frameworks. Our codes are available on\nhttps://github.com/cdb342/IJCAI-2022-ZLA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dubing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study. (arXiv:2204.12037v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12037","description":"<p>Spatial-temporal representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions, which lack an unified guidance and analysis about\nwhy modern spatial-temporal representation learning methods are easily collapse\ninto data bias and have limited cognitive ability. Inspired by the strong\ninference ability of human-level agents, recent years have therefore witnessed\ngreat effort in developing causal reasoning paradigms to realize robust\nrepresentation and model learning with good cognitive ability. In this paper,\nwe conduct a comprehensive review of existing causal reasoning methods for\nspatial-temporal representation learning, covering fundamental theories,\nmodels, and datasets. The limitations of current methods and datasets are also\ndiscussed. Moreover, we propose some primary challenges, opportunities, and\nfuture research directions for benchmarking causal reasoning algorithms in\nspatial-temporal representation learning. This paper aims to provide a\ncomprehensive overview of this emerging field, attract attention, encourage\ndiscussions, bring to the forefront the urgency of developing novel causal\nreasoning methods, publicly available benchmarks, and consensus-building\nstandards for reliable spatial-temporal representation learning and related\nreal-world applications more efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yushen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Framework for Characterization of Tumor-Immune Spatial Relationships in Tumor Microenvironment. (arXiv:2204.12283v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2204.12283","description":"<p>Understanding the impact of tumor biology on the composition of nearby cells\noften requires characterizing the impact of biologically distinct tumor\nregions. Biomarkers have been developed to label biologically distinct tumor\nregions, but challenges arise because of differences in the spatial extent and\ndistribution of differentially labeled regions. In this work, we present a\nframework for systematically investigating the impact of distinct tumor regions\non cells near the tumor borders, accounting their cross spatial distributions.\nWe apply the framework to multiplex immunohistochemistry (mIHC) studies of\npancreatic cancer and show its efficacy in demonstrating how biologically\ndifferent tumor regions impact the immune response in the tumor\nmicroenvironment. Furthermore, we show that the proposed framework can be\nextended to largescale whole slide image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Hasan_M/0/1/0/all/0/1\">Mahmudul Hasan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kaczmarzyk_J/0/1/0/all/0/1\">Jakub R. Kaczmarzyk</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Paredes_D/0/1/0/all/0/1\">David Paredes</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Oblein_L/0/1/0/all/0/1\">Lyanne Oblein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Oentoro_J/0/1/0/all/0/1\">Jaymie Oentoro</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Abousamra_S/0/1/0/all/0/1\">Shahira Abousamra</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Horowitz_M/0/1/0/all/0/1\">Michael Horowitz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shroyer_K/0/1/0/all/0/1\">Kenneth R. Shroyer</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saltz_J/0/1/0/all/0/1\">Joel Saltz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Zooming for Multiple Instance Learning on Whole-Slide Images. (arXiv:2204.12454v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12454","description":"<p>Multiple Instance Learning (MIL) methods have become increasingly popular for\nclassifying giga-pixel sized Whole-Slide Images (WSIs) in digital pathology.\nMost MIL methods operate at a single WSI magnification, by processing all the\ntissue patches. Such a formulation induces high computational requirements, and\nconstrains the contextualization of the WSI-level representation to a single\nscale. A few MIL methods extend to multiple scales, but are computationally\nmore demanding. In this paper, inspired by the pathological diagnostic process,\nwe propose ZoomMIL, a method that learns to perform multi-level zooming in an\nend-to-end manner. ZoomMIL builds WSI representations by aggregating\ntissue-context information from multiple magnifications. The proposed method\noutperforms the state-of-the-art MIL methods in WSI classification on two large\ndatasets, while significantly reducing the computational demands with regard to\nFloating-Point Operations (FLOPs) and processing time by up to 40x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thandiackal_K/0/1/0/all/0/1\">Kevin Thandiackal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pati_P/0/1/0/all/0/1\">Pushpak Pati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaume_G/0/1/0/all/0/1\">Guillaume Jaume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrani_M/0/1/0/all/0/1\">Maria Gabrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-free few-shot learning via representation learning with weight averaging. (arXiv:2204.12466v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.12466","description":"<p>Recent studies on few-shot classification using transfer learning pose\nchallenges to the effectiveness and efficiency of episodic meta-learning\nalgorithms. Transfer learning approaches are a natural alternative, but they\nare restricted to few-shot classification. Moreover, little attention has been\non the development of probabilistic models with well-calibrated uncertainty\nfrom few-shot samples, except for some Bayesian episodic learning algorithms.\nTo tackle the aforementioned issues, we propose a new transfer learning method\nto obtain accurate and reliable models for few-shot regression and\nclassification. The resulting method does not require episodic meta-learning\nand is called meta-free representation learning (MFRL). MFRL first finds\nlow-rank representation generalizing well on meta-test tasks. Given the learned\nrepresentation, probabilistic linear models are fine-tuned with few-shot\nsamples to obtain models with well-calibrated uncertainty. The proposed method\nnot only achieves the highest accuracy on a wide range of few-shot learning\nbenchmark datasets but also correctly quantifies the prediction uncertainty. In\naddition, weight averaging and temperature scaling are effective in improving\nthe accuracy and reliability of few-shot learning in existing meta-learning\nalgorithms with a wide range of learning paradigms and model architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chi-Guhn Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-fine Q-attention with Tree Expansion. (arXiv:2204.12471v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.12471","description":"<p>Coarse-to-fine Q-attention enables sample-efficient robot manipulation by\ndiscretizing the translation space in a coarse-to-fine manner, where the\nresolution gradually increases at each layer in the hierarchy. Although\neffective, Q-attention suffers from \"coarse ambiguity\" - when voxelization is\nsignificantly coarse, it is not feasible to distinguish similar-looking objects\nwithout first inspecting at a finer resolution. To combat this, we propose to\nenvision Q-attention as a tree that can be expanded and used to accumulate\nvalue estimates across the top-k voxels at each Q-attention depth. When our\nextension, Q-attention with Tree Expansion (QTE), replaces standard Q-attention\nin the Attention-driven Robot Manipulation (ARM) system, we are able to\naccomplish a larger set of tasks; especially on those that suffer from \"coarse\nambiguity\". In addition to evaluating our approach across 12 RLBench tasks, we\nalso show that the improved performance is visible in a real-world task\ninvolving small objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving. (arXiv:2204.13483v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13483","description":"<p>The new generation of 4D high-resolution imaging radar provides not only a\nhuge amount of point cloud but also additional elevation measurement, which has\na great potential of 3D sensing in autonomous driving. In this paper, we\nintroduce an autonomous driving dataset named TJ4DRadSet, including multi-modal\nsensors that are 4D radar, lidar, camera and GNSS, with about 40K frames in\ntotal. 7757 frames within 44 consecutive sequences in various driving scenarios\nare well annotated with 3D bounding boxes and track id. We provide a 4D\nradar-based 3D object detection baseline for our dataset to demonstrate the\neffectiveness of deep learning methods for 4D radar point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhixiong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xichan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_K/0/1/0/all/0/1\">Kai Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1\">Mengyue Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Libo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jie Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN. (arXiv:2204.14079v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.14079","description":"<p>Transfer learning of StyleGAN has recently shown great potential to solve\ndiverse tasks, especially in domain translation. Previous methods utilized a\nsource model by swapping or freezing weights during transfer learning, however,\nthey have limitations on visual quality and controlling source features. In\nother words, they require additional models that are computationally demanding\nand have restricted control steps that prevent a smooth transition. In this\npaper, we propose a new approach to overcome these limitations. Instead of\nswapping or freezing, we introduce a simple feature matching loss to improve\ngeneration quality. In addition, to control the degree of source features, we\ntrain a target model with the proposed strategy, FixNoise, to preserve the\nsource features only in a disentangled subspace of a target feature space.\nOwing to the disentangled feature space, our method can smoothly control the\ndegree of the source features in a single model. Extensive experiments\ndemonstrate that the proposed method can generate more consistent and realistic\nimages than previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongyeun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaehyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast. (arXiv:2204.14057v2 [cs.SD] CROSS LISTED)","link":"http://arxiv.org/abs/2204.14057","description":"<p>We present an approach to learn voice-face representations from the talking\nface videos, without any identity labels. Previous works employ cross-modal\ninstance discrimination tasks to establish the correlation of voice and face.\nThese methods neglect the semantic content of different videos, introducing\nfalse-negative pairs as training noise. Furthermore, the positive pairs are\nconstructed based on the natural correlation between audio clips and visual\nframes. However, this correlation might be weak or inaccurate in a large amount\nof real-world data, which leads to deviating positives into the contrastive\nparadigm. To address these issues, we propose the cross-modal prototype\ncontrastive learning (CMPC), which takes advantage of contrastive methods and\nresists adverse effects of false negatives and deviate positives. On one hand,\nCMPC could learn the intra-class invariance by constructing semantic-wise\npositives via unsupervised clustering in different modalities. On the other\nhand, by comparing the similarities of cross-modal instances from that of\ncross-modal prototypes, we dynamically recalibrate the unlearnable instances'\ncontribution to overall loss. Experiments show that the proposed approach\noutperforms state-of-the-art unsupervised methods on various voice-face\nassociation evaluation protocols. Additionally, in the low-shot supervision\nsetting, our method also has a significant improvement compared to previous\ninstance-wise contrastive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Boqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changjian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zheng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuxing Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}