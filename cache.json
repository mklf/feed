{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Putting GPT-3's Creativity to the (Alternative Uses) Test. (arXiv:2206.08932v1 [cs.AI])","link":"http://arxiv.org/abs/2206.08932","description":"<p>AI large language models have (co-)produced amazing written works from\nnewspaper articles to novels and poetry. These works meet the standards of the\nstandard definition of creativity: being original and useful, and sometimes\neven the additional element of surprise. But can a large language model\ndesigned to predict the next text fragment provide creative, out-of-the-box,\nresponses that still solve the problem at hand? We put Open AI's generative\nnatural language model, GPT-3, to the test. Can it provide creative solutions\nto one of the most commonly used tests in creativity research? We assessed\nGPT-3's creativity on Guilford's Alternative Uses Test and compared its\nperformance to previously collected human responses on expert ratings of\noriginality, usefulness and surprise of responses, flexibility of each set of\nideas as well as an automated method to measure creativity based on the\nsemantic distance between a response and the AUT object in question. Our\nresults show that -- on the whole -- humans currently outperform GPT-3 when it\ncomes to creative output. But, we believe it is only a matter of time before\nGPT-3 catches up on this particular task. We discuss what this work reveals\nabout human and AI creativity, creativity testing and our definition of\ncreativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_C/0/1/0/all/0/1\">Claire Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smal_I/0/1/0/all/0/1\">Iris Smal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baas_M/0/1/0/all/0/1\">Matthijs Baas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasman_R/0/1/0/all/0/1\">Raoul Grasman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maas_H/0/1/0/all/0/1\">Han van der Maas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making first order linear logic a generating grammar. (arXiv:2206.08955v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08955","description":"<p>It is known that different categorial grammars have surface representation in\na fragment of first order multiplicative linear logic. We show that the\nfragment of interest is equivalent to the recently introduced {\\it extended\ntensor type calculus}. This provides the former not only with some alternative\nsyntax and intuitive geometric representation, but also with an intrinsic\ndeductive system, which has been absent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slavnov_S/0/1/0/all/0/1\">Sergey Slavnov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla Handwritten Text Recognition (HTR) and Line Segmentation. (arXiv:2206.08977v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08977","description":"<p>We introduce a new dataset for offline Handwritten Text Recognition (HTR)\nfrom images of Bangla scripts comprising words, lines, and document-level\nannotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant\nto act as ground truth texts. These texts were subsequently used to generate\nthe annotations that were filled out by people with their handwriting. Our\ndataset includes 788 images of handwritten pages produced by approximately 150\ndifferent writers. It can be adopted as a basis for various handwriting\nclassification tasks such as end-to-end document recognition, word-spotting,\nword or line segmentation, and so on. We also propose a scheme to segment\nBangla handwritten document images into corresponding lines in an unsupervised\nmanner. Our line segmentation approach takes care of the variability involved\nin different writing styles, accurately segmenting complex handwritten text\nlines of curvilinear nature. Along with a bunch of pre-processing and\nmorphological operations, both Hough line and circle transforms were employed\nto distinguish different linear components. In order to arrange those\ncomponents into their corresponding lines, we followed an unsupervised\nclustering approach. The average success rate of our segmentation technique is\n81.57% in terms of FM metrics (similar to F-measure) with a mean Average\nPrecision (mAP) of 0.547.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md. Ataur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabassum_N/0/1/0/all/0/1\">Nazifa Tabassum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Mitu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_R/0/1/0/all/0/1\">Riya Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mohammad Khairul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Deep Multi-layered Dialectal Language Analysis: A Case Study of African-American English. (arXiv:2206.08978v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08978","description":"<p>Currently, natural language processing (NLP) models proliferate language\ndiscrimination leading to potentially harmful societal impacts as a result of\nbiased outcomes. For example, part-of-speech taggers trained on Mainstream\nAmerican English (MAE) produce non-interpretable results when applied to\nAfrican American English (AAE) as a result of language features not seen during\ntraining. In this work, we incorporate a human-in-the-loop paradigm to gain a\nbetter understanding of AAE speakers' behavior and their language use, and\nhighlight the need for dialectal language inclusivity so that native AAE\nspeakers can extensively interact with NLP systems while reducing feelings of\ndisenfranchisement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dacon_J/0/1/0/all/0/1\">Jamell Dacon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks. (arXiv:2206.09059v1 [cs.CL])","link":"http://arxiv.org/abs/2206.09059","description":"<p>Current state-of-the-art vision-and-language models are evaluated on tasks\neither individually or in a multi-task setting, overlooking the challenges of\ncontinually learning (CL) tasks as they arrive. Existing CL benchmarks have\nfacilitated research on task adaptation and mitigating \"catastrophic\nforgetting\", but are limited to vision-only and language-only tasks. We present\nCLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL\nsetting, and to systematically evaluate how upstream continual learning can\nrapidly generalize to new multimodal and unimodal tasks. CLiMB includes\nimplementations of several CL algorithms and a modified Vision-Language\nTransformer (ViLT) model that can be deployed on both multimodal and unimodal\ntasks. We find that common CL methods can help mitigate forgetting during\nmultimodal task learning, but do not enable cross-task knowledge transfer. We\nenvision that CLiMB will facilitate research on a new class of CL algorithms\nfor this challenging multimodal setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Ting-Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alva_L/0/1/0/all/0/1\">Leticia Leonor Pinto Alva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chochlakis_G/0/1/0/all/0/1\">Georgios Chochlakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Making the Most of BERT in Neural Machine Translation. (arXiv:1908.05672v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1908.05672","description":"<p>GPT-2 and BERT demonstrate the effectiveness of using pre-trained language\nmodels (LMs) on various natural language processing tasks. However, LM\nfine-tuning often suffers from catastrophic forgetting when applied to\nresource-rich tasks. In this work, we introduce a concerted training framework\n(CTNMT) that is the key to integrate the pre-trained LMs to neural machine\ntranslation (NMT). Our proposed CTNMT consists of three techniques: a)\nasymptotic distillation to ensure that the NMT model can retain the previous\npre-trained knowledge; b) a dynamic switching gate to avoid catastrophic\nforgetting of pre-trained knowledge; and c) a strategy to adjust the learning\npaces according to a scheduled policy. Our experiments in machine translation\nshow CTNMT gains of up to 3 BLEU score on the WMT14 English-German language\npair which even surpasses the previous state-of-the-art pre-training aided NMT\nby 1.4 BLEU score. While for the large WMT14 English-French task with 40\nmillions of sentence-pairs, our base model still significantly improves upon\nthe state-of-the-art Transformer big model by more than 1 BLEU score. The code\nand model can be downloaded from https://github.com/bytedance/neurst/\ntree/master/examples/ctnmt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiacheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Higher Criticism for Discriminating Word-Frequency Tables and Testing Authorship. (arXiv:1911.01208v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.01208","description":"<p>We adapt the Higher Criticism (HC) goodness-of-fit test to measure the\ncloseness between word-frequency tables. We apply this measure to authorship\nattribution challenges, where the goal is to identify the author of a document\nusing other documents whose authorship is known. The method is simple yet\nperforms well without handcrafting and tuning; reporting accuracy at the state\nof the art level in various current challenges. As an inherent side effect, the\nHC calculation identifies a subset of discriminating words. In practice, the\nidentified words have low variance across documents belonging to a corpus of\nhomogeneous authorship. We conclude that in comparing the similarity of a new\ndocument and a corpus of a single author, HC is mostly affected by words\ncharacteristic of the author and is relatively unaffected by topic structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kipnis_A/0/1/0/all/0/1\">Alon Kipnis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation. (arXiv:2004.14911v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.14911","description":"<p>There has been recent success in pre-training on monolingual data and\nfine-tuning on Machine Translation (MT), but it remains unclear how to best\nleverage a pre-trained model for a given MT task. This paper investigates the\nbenefits and drawbacks of freezing parameters, and adding new ones, when\nfine-tuning a pre-trained model on MT. We focus on 1) Fine-tuning a model\ntrained only on English monolingual data, BART. 2) Fine-tuning a model trained\non monolingual data from 25 languages, mBART. For BART we get the best\nperformance by freezing most of the model parameters, and adding extra\npositional embeddings. For mBART we match or outperform the performance of\nnaive fine-tuning for most language pairs with the encoder, and most of the\ndecoder, frozen. The encoder-decoder attention parameters are most important to\nfine-tune. When constraining ourselves to an out-of-domain training set for\nVietnamese to English we see the largest improvements over the fine-tuning\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stickland_A/0/1/0/all/0/1\">Asa Cooper Stickland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank. (arXiv:2009.14124v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.14124","description":"<p>Pretrained multilingual contextual representations have shown great success,\nbut due to the limits of their pretraining data, their benefits do not apply\nequally to all language varieties. This presents a challenge for language\nvarieties unfamiliar to these models, whose labeled \\emph{and unlabeled} data\nis too limited to train a monolingual model effectively. We propose the use of\nadditional language-specific pretraining and vocabulary augmentation to adapt\nmultilingual models to low-resource settings. Using dependency parsing of four\ndiverse low-resource language varieties as a case study, we show that these\nmethods significantly improve performance over baselines, especially in the\nlowest-resource cases, and demonstrate the importance of the relationship\nbetween such models' pretraining data and target language varieties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chau_E/0/1/0/all/0/1\">Ethan C. Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lucy H. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Style Transfer: A Review and Experimental Evaluation. (arXiv:2010.12742v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12742","description":"<p>The stylistic properties of text have intrigued computational linguistics\nresearchers in recent years. Specifically, researchers have investigated the\nText Style Transfer (TST) task, which aims to change the stylistic properties\nof the text while retaining its style independent content. Over the last few\nyears, many novel TST algorithms have been developed, while the industry has\nleveraged these algorithms to enable exciting TST applications. The field of\nTST research has burgeoned because of this symbiosis. This article aims to\nprovide a comprehensive review of recent research efforts on text style\ntransfer. More concretely, we create a taxonomy to organize the TST models and\nprovide a comprehensive summary of the state of the art. We review the existing\nevaluation methodologies for TST tasks and conduct a large-scale\nreproducibility study where we experimentally benchmark 19 state-of-the-art TST\nalgorithms on two publicly available datasets. Finally, we expand on current\ntrends and provide new perspectives on the new and exciting developments in the\nTST field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu C. Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FGNET-RH: Fine-Grained Named Entity Typing via Refinement in Hyperbolic Space. (arXiv:2101.11212v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.11212","description":"<p>Fine-Grained Named Entity Typing (FG-NET) aims at classifying the entity\nmentions into a wide range of entity types (usually hundreds) depending upon\nthe context. While distant supervision is the most common way to acquire\nsupervised training data, it brings in label noise, as it assigns type labels\nto the entity mentions irrespective of mentions context. In attempts to deal\nwith the label noise, leading research on the FG-NET assumes that the\nfine-grained entity typing data possesses a euclidean nature, which restraints\nthe ability of the existing models in combating the label noise. Given the fact\nthat the fine-grained type hierarchy exhibits a hierarchical structure, it\nmakes hyperbolic space a natural choice to model the FG-NET data. In this\nresearch, we propose FGNET-RH, a novel framework that benefits from the\nhyperbolic geometry in combination with the graph structures to perform entity\ntyping in a performance-enhanced fashion. FGNET-RH initially uses LSTM networks\nto encode the mention in relation with its context, later it forms a graph to\ndistill/refine the mention encodings in the hyperbolic space. Finally, the\nrefined mention encoding is used for entity typing. Experimentation using\ndifferent benchmark datasets shows that FGNET-RH improves the performance on\nFG-NET by up to 3.5-% in terms of strict accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Muhammad Asif Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Specializing Multilingual Language Models: An Empirical Study. (arXiv:2106.09063v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.09063","description":"<p>Pretrained multilingual language models have become a common tool in\ntransferring NLP capabilities to low-resource languages, often with\nadaptations. In this work, we study the performance, extensibility, and\ninteraction of two such adaptations: vocabulary augmentation and script\ntransliteration. Our evaluations on part-of-speech tagging, universal\ndependency parsing, and named entity recognition in nine diverse low-resource\nlanguages uphold the viability of these approaches while raising new questions\naround how to optimally adapt multilingual models to low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chau_E/0/1/0/all/0/1\">Ethan C. Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity of Sentence Representations in Multilingual LMs: Resolving Conflicting Literature and Case Study of Baltic Languages. (arXiv:2109.01207v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01207","description":"<p>Low-resource languages, such as Baltic languages, benefit from Large\nMultilingual Models (LMs) that possess remarkable cross-lingual transfer\nperformance capabilities. This work is an interpretation and analysis study\ninto cross-lingual representations of Multilingual LMs. Previous works\nhypothesized that these LMs internally project representations of different\nlanguages into a shared cross-lingual space. However, the literature produced\ncontradictory results. In this paper, we revisit the prior work claiming that\n\"BERT is not an Interlingua\" and show that different languages do converge to a\nshared space in such language models with another choice of pooling strategy or\nsimilarity index. Then, we perform cross-lingual representational analysis for\nthe two most popular multilingual LMs employing 378 pairwise language\ncomparisons. We discover that while most languages share joint cross-lingual\nspace, some do not. However, we observe that Baltic languages do belong to that\nshared space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Del_M/0/1/0/all/0/1\">Maksym Del</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishel_M/0/1/0/all/0/1\">Mark Fishel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Retriever-Ranker for dense text retrieval. (arXiv:2110.03611v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03611","description":"<p>Current dense text retrieval models face two typical challenges. First, they\nadopt a siamese dual-encoder architecture to encode queries and documents\nindependently for fast indexing and searching, while neglecting the\nfiner-grained term-wise interactions. This results in a sub-optimal recall\nperformance. Second, their model training highly relies on a negative sampling\ntechnique to build up the negative documents in their contrastive losses. To\naddress these challenges, we present Adversarial Retriever-Ranker (AR2), which\nconsists of a dual-encoder retriever plus a cross-encoder ranker. The two\nmodels are jointly optimized according to a minimax adversarial objective: the\nretriever learns to retrieve negative documents to cheat the ranker, while the\nranker learns to rank a collection of candidates including both the\nground-truth and the retrieved ones, as well as providing progressive direct\nfeedback to the dual-encoder retriever. Through this adversarial game, the\nretriever gradually produces harder negative documents to train a better\nranker, whereas the cross-encoder ranker provides progressive feedback to\nimprove retriever. We evaluate AR2 on three benchmarks. Experimental results\nshow that AR2 consistently and significantly outperforms existing dense\nretriever methods and achieves new state-of-the-art results on all of them.\nThis includes the improvements on Natural Questions R@5 to 77.9%(+2.1%),\nTriviaQA R@5 to 78.2%(+1.4), and MS-MARCO MRR@10 to 39.5%(+1.3%). Code and\nmodels are available at https://github.com/microsoft/AR2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice Conversion Can Improve ASR in Very Low-Resource Settings. (arXiv:2111.02674v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2111.02674","description":"<p>Voice conversion (VC) could be used to improve speech recognition systems in\nlow-resource languages by using it to augment limited training data. However,\nVC has not been widely used for this purpose because of practical issues such\nas compute speed and limitations when converting to and from unseen speakers.\nMoreover, it is still unclear whether a VC model trained on one well-resourced\nlanguage can be applied to speech from another low-resource language for the\naim of data augmentation. In this work we assess whether a VC system can be\nused cross-lingually to improve low-resource speech recognition. We combine\nseveral recent techniques to design and train a practical VC system in English,\nand then use this system to augment data for training speech recognition models\nin several low-resource languages. When using a sensible amount of VC augmented\ndata, speech recognition performance is improved in all four low-resource\nlanguages considered. We also show that VC-based augmentation is superior to\nSpecAugment (a widely used signal processing augmentation method) in the\nlow-resource languages considered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baas_M/0/1/0/all/0/1\">Matthew Baas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech. (arXiv:2111.04040v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2111.04040","description":"<p>Personalizing a speech synthesis system is a highly desired application,\nwhere the system can generate speech with the user's voice with rare enrolled\nrecordings. There are two main approaches to build such a system in recent\nworks: speaker adaptation and speaker encoding. On the one hand, speaker\nadaptation methods fine-tune a trained multi-speaker text-to-speech (TTS) model\nwith few enrolled samples. However, they require at least thousands of\nfine-tuning steps for high-quality adaptation, making it hard to apply on\ndevices. On the other hand, speaker encoding methods encode enrollment\nutterances into a speaker embedding. The trained TTS model can synthesize the\nuser's speech conditioned on the corresponding speaker embedding. Nevertheless,\nthe speaker encoder suffers from the generalization gap between the seen and\nunseen speakers.\n</p>\n<p>In this paper, we propose applying a meta-learning algorithm to the speaker\nadaptation method. More specifically, we use Model Agnostic Meta-Learning\n(MAML) as the training algorithm of a multi-speaker TTS model, which aims to\nfind a great meta-initialization to adapt the model to any few-shot speaker\nadaptation tasks quickly. Therefore, we can also adapt the meta-trained TTS\nmodel to unseen speakers efficiently. Our experiments compare the proposed\nmethod (Meta-TTS) with two baselines: a speaker adaptation method baseline and\na speaker encoding method baseline. The evaluation results show that Meta-TTS\ncan synthesize high speaker-similarity speech from few enrollment samples with\nfewer adaptation steps than the speaker adaptation baseline and outperforms the\nspeaker encoding baseline under the same training scheme. When the speaker\nencoder of the baseline is pre-trained with extra 8371 speakers of data,\nMeta-TTS can still outperform the baseline on LibriTTS dataset and achieve\ncomparable results on VCTK dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sung-Feng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chyi-Jiunn Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Da-Rong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Conditional Language Models without Catastrophic Forgetting. (arXiv:2112.00791v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.00791","description":"<p>Machine learning is shifting towards general-purpose pretrained generative\nmodels, trained in a self-supervised manner on large amounts of data, which can\nthen be applied to solve a large number of tasks. However, due to their generic\ntraining methodology, these models often fail to meet some of the downstream\nrequirements (e.g., hallucinations in abstractive summarization or style\nviolations in code generation). This raises the important question of how to\nadapt pre-trained generative models to meet all requirements without destroying\ntheir general capabilities (\"catastrophic forgetting\"). Recent work has\nproposed to solve this problem by representing task-specific requirements\nthrough energy-based models (EBMs) and approximating these EBMs using\ndistributional policy gradients (DPG). Despite its effectiveness, this approach\nis however limited to unconditional distributions. In this paper, we extend DPG\nto conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on\nfour different control objectives across three tasks (translation,\nsummarization and code generation) and two pretrained models (T5 and GPT-Neo).\nOur results show that fine-tuning using CDPG robustly moves these pretrained\nmodels closer towards meeting control objectives and -- in contrast with\nbaseline approaches -- does not result in catastrophic forgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1\">German Kruszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1\">Marc Dymetman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models. (arXiv:2112.11941v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11941","description":"<p>We introduce the CRASS (counterfactual reasoning assessment) data set and\nbenchmark utilizing questionized counterfactual conditionals as a novel and\npowerful tool to evaluate large language models. We present the data set design\nand benchmark that supports scoring against a crowd-validated human baseline.\nWe test six state-of-the-art models against our benchmark. Our results show\nthat it poses a valid challenge for these models and opens up considerable room\nfor their improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frohberg_J/0/1/0/all/0/1\">J&#xf6;rg Frohberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_F/0/1/0/all/0/1\">Frank Binder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making sense of electrical vehicle discussions using sentiment analysis on closely related news and user comments. (arXiv:2112.12327v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.12327","description":"<p>We used a token-wise and document-wise sentiment analysis using both\nunsupervised and supervised models applied to both news and user reviews\ndataset. And our token-wise sentiment analysis found a statistically\nsignificant difference in sentiment between the two groups (both of which were\nvery large N), our document-wise supervised sentiment analysis found no\nsignificant difference in sentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everts_J/0/1/0/all/0/1\">Josh Everts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TiltedBERT: Resource Adjustable Version of BERT. (arXiv:2201.03327v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03327","description":"<p>In this paper, a novel adjustable fine-tuning method is proposed that\nimproves the inference time of BERT model on downstream tasks. The proposed\nmethod detects the more important word vectors in each layer by the proposed\nAttention Context Contribution (ACC) metric and eliminates the less important\nword vectors by the proposed strategy. In the TiltedBERT method the model\nlearns to work with a considerably lower number of Floating Point Operations\n(FLOPs) than the original BERTbase model. The proposed method does not need\ntraining from scratch, and it can be generalized to other transformer-based\nmodels. The extensive experiments show that the word vectors in higher layers\nhave less contribution that can be eliminated and improve the inference time.\nExperimental results on extensive sentiment analysis, classification and\nregression datasets, and benchmarks like IMDB and GLUE showed that TiltedBERT\nis effective in various datasets. TiltedBERT improves the inference time of\nBERTbase up to 4.8 times with less than 0.75% accuracy drop on average. After\nthe fine-tuning by the offline-tuning property, the inference time of the model\ncan be adjusted for a wide range of Tilt-Rate selections. Also, A mathematical\nspeedup analysis is proposed to estimate TiltedBERT method's speedup\naccurately. With the help of this analysis, a proper Tilt-Rate value can be\nselected before fine-tuning and during offline-tuning phases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kachuee_S/0/1/0/all/0/1\">Sajjad Kachuee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifkhani_M/0/1/0/all/0/1\">Mohammad Sharifkhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TYPIC: A Corpus of Template-Based Diagnostic Comments on Argumentation. (arXiv:2201.06674v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06674","description":"<p>Providing feedback on the argumentation of the learner is essential for\ndeveloping critical thinking skills, however, it requires a lot of time and\neffort. To mitigate the overload on teachers, we aim to automate a process of\nproviding feedback, especially giving diagnostic comments which point out the\nweaknesses inherent in the argumentation. It is recommended to give specific\ndiagnostic comments so that learners can recognize the diagnosis without\nmisinterpretation. However, it is not obvious how the task of providing\nspecific diagnostic comments should be formulated. We present a formulation of\nthe task as template selection and slot filling to make an automatic evaluation\neasier and the behavior of the model more tractable. The key to the formulation\nis the possibility of creating a template set that is sufficient for practical\nuse. In this paper, we define three criteria that a template set should\nsatisfy: expressiveness, informativeness, and uniqueness, and verify the\nfeasibility of creating a template set that satisfies these criteria as a first\ntrial. We will show that it is feasible through an annotation study that\nconverts diagnostic comments given in a text to a template format. The corpus\nused in the annotation study is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naito_S/0/1/0/all/0/1\">Shoichi Naito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawada_S/0/1/0/all/0/1\">Shintaro Sawada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_C/0/1/0/all/0/1\">Chihiro Nakagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1\">Naoya Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_K/0/1/0/all/0/1\">Kenshi Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_I/0/1/0/all/0/1\">Iori Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mim_F/0/1/0/all/0/1\">Farjana Sultana Mim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Keshav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis. (arXiv:2201.08277v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08277","description":"<p>Sentiment analysis is one of the most widely studied applications in NLP, but\nmost work focuses on languages with large amounts of data. We introduce the\nfirst large-scale human-annotated Twitter sentiment dataset for the four most\nwidely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and\nYor\\`ub\\'a ) consisting of around 30,000 annotated tweets per language (and\n14,000 for Nigerian-Pidgin), including a significant fraction of code-mixed\ntweets. We propose text collection, filtering, processing and labeling methods\nthat enable us to create datasets for these low-resource languages. We evaluate\na rangeof pre-trained models and transfer strategies on the dataset. We find\nthat language-specific models and language-adaptivefine-tuning generally\nperform best. We release the datasets, trained models, sentiment lexicons, and\ncode to incentivizeresearch on sentiment analysis in under-represented\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Said Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1\">Bello Shehu Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullahi_S/0/1/0/all/0/1\">Saheed Salahudeen Abdullahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeorge_A/0/1/0/all/0/1\">Alipio Jeorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazdil_P/0/1/0/all/0/1\">Pavel Brazdil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias in Automated Speaker Recognition. (arXiv:2201.09486v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2201.09486","description":"<p>Automated speaker recognition uses data processing to identify speakers by\ntheir voice. Today, automated speaker recognition is deployed on billions of\nsmart devices and in services such as call centres. Despite their wide-scale\ndeployment and known sources of bias in related domains like face recognition\nand natural language processing, bias in automated speaker recognition has not\nbeen studied systematically. We present an in-depth empirical and analytical\nstudy of bias in the machine learning development workflow of speaker\nverification, a voice biometric and core task in automated speaker recognition.\nDrawing on an established framework for understanding sources of harm in\nmachine learning, we show that bias exists at every development stage in the\nwell-known VoxCeleb Speaker Recognition Challenge, including data generation,\nmodel building, and implementation. Most affected are female speakers and\nnon-US nationalities, who experience significant performance degradation.\nLeveraging the insights from our findings, we make practical recommendations\nfor mitigating bias in automated speaker recognition, and outline future\nresearch directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hutiri_W/0/1/0/all/0/1\">Wiebke Toussaint Hutiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_A/0/1/0/all/0/1\">Aaron Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Adversarial Concept Erasure. (arXiv:2201.12091v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12091","description":"<p>Modern neural models trained on textual data rely on pre-trained\nrepresentations that emerge without direct supervision. As these\nrepresentations are increasingly being used in real-world applications, the\ninability to \\emph{control} their content becomes an increasingly important\nproblem.\n</p>\n<p>We formulate the problem of identifying and erasing a linear subspace that\ncorresponds to a given concept, in order to prevent linear predictors from\nrecovering the concept. We model this problem as a constrained, linear minimax\ngame, and show that existing solutions are generally not optimal for this task.\nWe derive a closed-form solution for certain objectives, and propose a convex\nrelaxation, R-LACE, that works well for others. When evaluated in the context\nof binary gender removal, the method recovers a low-dimensional subspace whose\nremoval mitigates bias by intrinsic and extrinsic evaluation. We show that the\nmethod -- despite being linear -- is highly expressive, effectively mitigating\nbias in deep nonlinear classifiers while maintaining tractability and\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twiton_M/0/1/0/all/0/1\">Michael Twiton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuraHealth: An Automated Screening Pipeline to Detect Undiagnosed Cognitive Impairment in Electronic Health Records with Deep Learning and Natural Language Processing. (arXiv:2202.00478v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00478","description":"<p>Dementia related cognitive impairment (CI) is a neurodegenerative disorder,\naffecting over 55 million people worldwide and growing rapidly at the rate of\none new case every 3 seconds. 75% cases go undiagnosed globally with up to 90%\nin low-and-middle-income countries, leading to an estimated annual worldwide\ncost of USD 1.3 trillion, forecasted to reach 2.8 trillion by 2030. With no\ncure, a recurring failure of clinical trials, and a lack of early diagnosis,\nthe mortality rate is 100%. Information in electronic health records (EHR) can\nprovide vital clues for early detection of CI, but a manual review by experts\nis tedious and error prone. Several computational methods have been proposed,\nhowever, they lack an enhanced understanding of the linguistic context in\ncomplex language structures of EHR. Therefore, I propose a novel and more\naccurate framework, NeuraHealth, to identify patients who had no earlier\ndiagnosis. In NeuraHealth, using patient EHR from Mass General Brigham BioBank,\nI fine-tuned a bi-directional attention-based deep learning natural language\nprocessing model to classify sequences. The sequence predictions were used to\ngenerate structured features as input for a patient level regularized logistic\nregression model. This two-step framework creates high dimensionality,\noutperforming all existing state-of-the-art computational methods as well as\nclinical methods. Further, I integrate the models into a real-world product, a\nweb app, to create an automated EHR screening pipeline for scalable and\nhigh-speed discovery of undetected CI in EHR, making early diagnosis viable in\nmedical facilities and in regions with scarce health services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_T/0/1/0/all/0/1\">Tanish Tyagi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Magdamo_C/0/1/0/all/0/1\">Colin G. Magdamo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Noori_A/0/1/0/all/0/1\">Ayush Noori</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaozhi Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Deodhar_M/0/1/0/all/0/1\">Mayuresh Deodhar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhuoqiao Hong</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wendong Ge</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Elissa M. Ye</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sheu_Y/0/1/0/all/0/1\">Yi-han Sheu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Alabsi_H/0/1/0/all/0/1\">Haitham Alabsi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_L/0/1/0/all/0/1\">Laura Brenner</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Robbins_G/0/1/0/all/0/1\">Gregory K. Robbins</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_S/0/1/0/all/0/1\">Sahar Zafar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Benson_N/0/1/0/all/0/1\">Nicole Benson</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Moura_L/0/1/0/all/0/1\">Lidia Moura</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1\">John Hsu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_Pozo_A/0/1/0/all/0/1\">Alberto Serrano-Pozo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Prokopenko_D/0/1/0/all/0/1\">Dimitry Prokopenko</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Tanzi_R/0/1/0/all/0/1\">Rudolph E. Tanzi</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Hyman_B/0/1/0/all/0/1\">Bradley T.Hyman</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Blacker_D/0/1/0/all/0/1\">Deborah Blacker</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mukerji_S/0/1/0/all/0/1\">Shibani S. Mukerji</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Westover_M/0/1/0/all/0/1\">M. Brandon Westover</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sudeshna Das</a> (1) ((1) Massachusetts General Hospital, Boston, MA, (2) McCance Center for Brain Health, Boston, MA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09061","description":"<p>In the past few years, the emergence of pre-training models has brought\nuni-modal fields such as computer vision (CV) and natural language processing\n(NLP) to a new era. Substantial works have shown they are beneficial for\ndownstream uni-modal tasks and avoid training a new model from scratch. So can\nsuch pre-trained models be applied to multi-modal tasks? Researchers have\nexplored this problem and made significant progress. This paper surveys recent\nadvances and new frontiers in vision-language pre-training (VLP), including\nimage-text and video-text pre-training. To give readers a better overall grasp\nof VLP, we first review its recent advances from five aspects: feature\nextraction, model architecture, pre-training objectives, pre-training datasets,\nand downstream tasks. Then, we summarize the specific VLP models in detail.\nFinally, we discuss the new frontiers in VLP. To the best of our knowledge,\nthis is the first survey on VLP. We hope that this survey can shed light on\nfuture research in the VLP field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Base Question Answering by Case-based Reasoning over Subgraphs. (arXiv:2202.10610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10610","description":"<p>Question answering (QA) over knowledge bases (KBs) is challenging because of\nthe diverse, essentially unbounded, types of reasoning patterns needed.\nHowever, we hypothesize in a large KB, reasoning patterns required to answer a\nquery type reoccur for various entities in their respective subgraph\nneighborhoods. Leveraging this structural similarity between local\nneighborhoods of different subgraphs, we introduce a semiparametric model\n(CBR-SUBG) with (i) a nonparametric component that for each query, dynamically\nretrieves other similar $k$-nearest neighbor (KNN) training queries along with\nquery-specific subgraphs and (ii) a parametric component that is trained to\nidentify the (latent) reasoning patterns from the subgraphs of KNN queries and\nthen apply them to the subgraph of the target query. We also propose an\nadaptive subgraph collection strategy to select a query-specific compact\nsubgraph, allowing us to scale to full Freebase KB containing billions of\nfacts. We show that CBR-SUBG can answer queries requiring subgraph reasoning\npatterns and performs competitively with the best models on several KBQA\nbenchmarks. Our subgraph collection strategy also produces more compact\nsubgraphs (e.g. 55\\% reduction in size for WebQSP while increasing answer\nrecall by 4.85\\%)\\footnote{Code, model, and subgraphs are available at\n\\url{https://github.com/rajarshd/CBR-SUBG}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rajarshi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godbole_A/0/1/0/all/0/1\">Ameya Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Ankita Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tower_E/0/1/0/all/0/1\">Elliot Tower</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering. (arXiv:2203.04911v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04911","description":"<p>Spoken Question Answering (SQA) is to find the answer from a spoken document\ngiven a question, which is crucial for personal assistants when replying to the\nqueries from the users. Existing SQA methods all rely on Automatic Speech\nRecognition (ASR) transcripts. Not only does ASR need to be trained with\nmassive annotated data that are time and cost-prohibitive to collect for\nlow-resourced languages, but more importantly, very often the answers to the\nquestions include name entities or out-of-vocabulary words that cannot be\nrecognized correctly. Also, ASR aims to minimize recognition errors equally\nover all words, including many function words irrelevant to the SQA task.\nTherefore, SQA without ASR transcripts (textless) is always highly desired,\nalthough known to be very difficult.\n</p>\n<p>This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging\nunlabeled data for pre-training and fine-tuned by the SQA downstream task. The\ntime intervals of spoken answers can be directly predicted from spoken\ndocuments. We also release a new SQA benchmark corpus, NMSQA, for data with\nmore realistic scenarios. We empirically showed that DUAL yields results\ncomparable to those obtained by cascading ASR and text QA model and robust to\nreal-world data. Our code and model will be open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Ho-Lam Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lin-shan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A$^3$T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing. (arXiv:2203.09690v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.09690","description":"<p>Recently, speech representation learning has improved many speech-related\ntasks such as speech recognition, speech classification, and speech-to-text\ntranslation. However, all the above tasks are in the direction of speech\nunderstanding, but for the inverse direction, speech synthesis, the potential\nof representation learning is yet to be realized, due to the challenging nature\nof generating high-quality speech. To address this problem, we propose our\nframework, Alignment-Aware Acoustic-Text Pretraining (A$^3$T), which\nreconstructs masked acoustic signals with text input and acoustic-text\nalignment during training. In this way, the pretrained model can generate high\nquality reconstructed spectrogram, which can be applied to the speech editing\nand unseen speaker TTS directly. Experiments show A$^3$T outperforms SOTA\nmodels on speech editing, and improves multi-speaker speech synthesis without\nthe external speaker verification model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xintong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_M/0/1/0/all/0/1\">Mingbo Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition. (arXiv:2203.14222v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.14222","description":"<p>Although deep learning-based end-to-end Automatic Speech Recognition (ASR)\nhas shown remarkable performance in recent years, it suffers severe performance\nregression on test samples drawn from different data distributions. Test-time\nAdaptation (TTA), previously explored in the computer vision area, aims to\nadapt the model trained on source domains to yield better predictions for test\nsamples, often out-of-domain, without accessing the source data. Here, we\npropose the Single-Utterance Test-time Adaptation (SUTA) framework for ASR,\nwhich is the first TTA study on ASR to our best knowledge. The single-utterance\nTTA is a more realistic setting that does not assume test data are sampled from\nidentical distribution and does not delay on-demand inference due to\npre-collection for the batch of adaptation data. SUTA consists of unsupervised\nobjectives with an efficient adaptation strategy. Empirical results demonstrate\nthat SUTA effectively improves the performance of the source ASR model\nevaluated on multiple out-of-domain target corpora and in-domain test samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT. (arXiv:2203.15610v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.15610","description":"<p>Self-supervised speech representation learning has shown promising results in\nvarious speech processing tasks. However, the pre-trained models, e.g., HuBERT,\nare storage-intensive Transformers, limiting their scope of applications under\nlow-resource settings. To this end, we propose LightHuBERT, a once-for-all\nTransformer compression framework, to find the desired architectures\nautomatically by pruning structured parameters. More precisely, we create a\nTransformer-based supernet that is nested with thousands of weight-sharing\nsubnets and design a two-stage distillation strategy to leverage the\ncontextualized latent representations from HuBERT. Experiments on automatic\nspeech recognition (ASR) and the SUPERB benchmark show the proposed LightHuBERT\nenables over $10^9$ architectures concerning the embedding dimension, attention\ndimension, head number, feed-forward network ratio, and network depth.\nLightHuBERT outperforms the original HuBERT on ASR and five SUPERB tasks with\nthe HuBERT size, achieves comparable performance to the teacher model in most\ntasks with a reduction of 29% parameters, and obtains a $3.5\\times$ compression\nratio in three SUPERB tasks, e.g., automatic speaker verification, keyword\nspotting, and intent classification, with a slight accuracy loss. The code and\npre-trained models are available at\nhttps://github.com/mechanicalsea/lighthubert.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_Q/0/1/0/all/0/1\">Qibing Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ao_J/0/1/0/all/0/1\">Junyi Ao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhixiang Xiong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks. (arXiv:2203.16040v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.16040","description":"<p>Because the performance of speech separation is excellent for speech in which\ntwo speakers completely overlap, research attention has been shifted to dealing\nwith more realistic scenarios. However, domain mismatch between training/test\nsituations due to factors, such as speaker, content, channel, and environment,\nremains a severe problem for speech separation. Speaker and environment\nmismatches have been studied in the existing literature. Nevertheless, there\nare few studies on speech content and channel mismatches. Moreover, the impacts\nof language and channel in these studies are mostly tangled. In this study, we\ncreate several datasets for various experiments. The results show that the\nimpacts of different languages are small enough to be ignored compared to the\nimpacts of different channels. In our experiments, training on data recorded by\nAndroid phones leads to the best generalizability. Moreover, we provide a new\nsolution for channel mismatch by evaluating projection, where the channel\nsimilarity can be measured and used to effectively select additional training\ndata to improve the performance of in-the-wild test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan-Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributed Transition Systems with Tags for Privacy Analysis. (arXiv:2204.02602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02602","description":"<p>We present a logical framework that formally models how a given private\ninformation P stored on a given database D, can get captured progressively, by\nan agent/adversary querying the database repeatedly. Named DLTTS (Distributed\nLabeled Tagged Transition System), the framework borrows ideas from several\ndomains: Probabilistic Automata of Segala, Probabilistic Concurrent Systems,\nand Probabilistic labelled transition systems. To every node on a DLTTS is\nattached a tag that represents the 'current' knowledge of the adversary,\nacquired from the responses of the answering mechanism of the DBMS to his/her\nqueries, at the nodes traversed earlier, along any given run; this knowledge is\ncompleted at the same node, with further relational deductions, possibly in\ncombination with 'public' information from other databases given in advance. A\n'blackbox' mechanism is also part of a DLTTS, and it is meant as an oracle; its\nrole is to tell if the private information has been deduced by the adversary at\nthe current node, and if so terminate the run. An additional special feature is\nthat the blackbox also gives information on how 'close', or how 'far', the\nknowledge of the adversary is, from the private information P , at the current\nnode. A metric is defined for that purpose, on the set of all 'type compatible'\ntuples from the given database, the data themselves being typed with the\nheaders of the base. Despite the transition systems flavor of our framework,\nthis metric is not 'behavioral' in the sense presented in some other works. It\nis exclusively database oriented, and allows to define new notions of adjacency\nand of indistinguishabilty between databases, more generally than those usually\nbased on the Hamming metric (and a restricted notion of adjacency). Examples\nare given all along to illustrate how our framework works.\n</p>\n<p>Keywords:Database, Privacy, Transition System, Probability, Distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anantharaman_S/0/1/0/all/0/1\">Siva Anantharaman</a> (LMV), <a href=\"http://arxiv.org/find/cs/1/au:+Frittella_S/0/1/0/all/0/1\">Sabine Frittella</a> (SDS), <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Benjamin Nguyen</a> (SDS)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioRED: A Rich Biomedical Relation Extraction Dataset. (arXiv:2204.04263v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04263","description":"<p>Automated relation extraction (RE) from biomedical literature is critical for\nmany downstream text mining applications in both research and real-world\nsettings. However, most existing benchmarking datasets for bio-medical RE only\nfocus on relations of a single type (e.g., protein-protein interactions) at the\nsentence level, greatly limiting the development of RE systems in biomedicine.\nIn this work, we first review commonly used named entity recognition (NER) and\nRE datasets. Then we present BioRED, a first-of-its-kind biomedical RE corpus\nwith multiple entity types (e.g., gene/protein, disease, chemical) and relation\npairs (e.g., gene-disease; chemical-chemical) at the document level, on a set\nof 600 PubMed abstracts. Further, we label each relation as describing either a\nnovel finding or previously known background knowledge, enabling automated\nalgorithms to differentiate between novel and background information. We assess\nthe utility of BioRED by benchmarking several existing state-of-the-art\nmethods, including BERT-based models, on the NER and RE tasks. Our results show\nthat while existing approaches can reach high performance on the NER task\n(F-score of 89.3%), there is much room for improvement for the RE task,\nespecially when extracting novel relations (F-score of 47.7%). Our experiments\nalso demonstrate that such a rich dataset can successfully facilitate the\ndevelopment of more accurate, efficient, and robust RE systems for biomedicine.\nThe BioRED dataset and annotation guideline are freely available at\nhttps://ftp.ncbi.nlm.nih.gov/pub/lu/BioRED/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Po-Ting Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arighi_C/0/1/0/all/0/1\">Cecilia N Arighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08790","description":"<p>Learning visual representations from natural language supervision has\nrecently shown great promise in a number of pioneering works. In general, these\nlanguage-augmented visual models demonstrate strong transferability to a\nvariety of datasets and tasks. However, it remains challenging to evaluate the\ntransferablity of these models due to the lack of easy-to-use evaluation\ntoolkits and public benchmarks. To tackle this, we build ELEVATER (Evaluation\nof Language-augmented Visual Task-level Transfer), the first benchmark and\ntoolkit for evaluating(pre-trained) language-augmented visual models. ELEVATER\nis composed of three components. (i) Datasets. As downstream evaluation suites,\nit consists of 20 image classification datasets and 35 object detection\ndatasets, each of which is augmented with external knowledge. (ii) Toolkit. An\nautomatic hyper-parameter tuning toolkit is developed to facilitate model\nevaluation on downstream tasks. (iii) Metrics. A variety of evaluation metrics\nare used to measure sample-efficiency (zero-shot and few-shot) and\nparameter-efficiency (linear probing and full model fine-tuning). We publicly\nrelease ELEVATER at https://computer-vision-in-the-wild.github.io/ELEVATER/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Ping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPT: Open Pre-trained Transformer Language Models. (arXiv:2205.01068v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01068","description":"<p>Large language models, which are often trained for hundreds of thousands of\ncompute days, have shown remarkable capabilities for zero- and few-shot\nlearning. Given their computational cost, these models are difficult to\nreplicate without significant capital. For the few that are available through\nAPIs, no access is granted to the full model weights, making them difficult to\nstudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-only\npre-trained transformers ranging from 125M to 175B parameters, which we aim to\nfully and responsibly share with interested researchers. We show that OPT-175B\nis comparable to GPT-3, while requiring only 1/7th the carbon footprint to\ndevelop. We are also releasing our logbook detailing the infrastructure\nchallenges we faced, along with code for experimenting with all of the released\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Susan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Moya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewan_C/0/1/0/all/0/1\">Christopher Dewan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shleifer_S/0/1/0/all/0/1\">Sam Shleifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1\">Punit Singh Koura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Anjali Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Multi-Domain False News and Underlying User Effects on Chinese Weibo. (arXiv:2205.03068v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2205.03068","description":"<p>False news that spreads on social media has proliferated over the past years\nand has led to multi-aspect threats in the real world. While there are studies\nof false news on specific domains (like politics or health care), little work\nis found comparing false news across domains. In this article, we investigate\nfalse news across nine domains on Weibo, the largest Twitter-like social media\nplatform in China, from 2009 to 2019. The newly collected data comprise 44,728\nposts in the nine domains, published by 40,215 users, and reposted over 3.4\nmillion times. Based on the distributions and spreads of the multi-domain\ndataset, we observe that false news in domains that are close to daily life\nlike health and medicine generated more posts but diffused less effectively\nthan those in other domains like politics, and that political false news had\nthe most effective capacity for diffusion. The widely diffused false news posts\non Weibo were associated strongly with certain types of users -- by gender,\nage, etc. Further, these posts provoked strong emotions in the reposts and\ndiffused further with the active engagement of false-news starters. Our\nfindings have the potential to help design false news detection systems in\nsuspicious news discovery, veracity prediction, and display and explanation.\nThe comparison of the findings on Weibo with those of existing work\ndemonstrates nuanced patterns, suggesting the need for more research on data\nfrom diverse platforms, countries, or languages to tackle the global issue of\nfalse news. The code and new anonymized dataset are available at\nhttps://github.com/ICTMCG/Characterizing-Weibo-Multi-Domain-False-News.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_H/0/1/0/all/0/1\">H. Russell Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media. (arXiv:2205.03296v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03296","description":"<p>Building models to detect vaccine attitudes on social media is challenging\nbecause of the composite, often intricate aspects involved, and the limited\navailability of annotated data. Existing approaches have relied heavily on\nsupervised training that requires abundant annotations and pre-defined aspect\ncategories. Instead, with the aim of leveraging the large amount of unannotated\ndata now available on vaccination, we propose a novel semi-supervised approach\nfor vaccine attitude detection, called VADet. A variational autoencoding\narchitecture based on language models is employed to learn from unlabelled data\nthe topical information of the domain. Then, the model is fine-tuned with a few\nmanually annotated examples of user attitudes. We validate the effectiveness of\nVADet on our annotated data and also on an existing vaccination corpus\nannotated with opinions on vaccines. Our results show that VADet is able to\nlearn disentangled stance and aspect topics, and outperforms existing\naspect-based sentiment analysis models on both stance detection and tweet\nclustering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lixing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniMorph 4.0: Universal Morphology. (arXiv:2205.03608v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03608","description":"<p>The Universal Morphology (UniMorph) project is a collaborative effort\nproviding broad-coverage instantiated normalized morphological inflection\ntables for hundreds of diverse world languages. The project comprises two major\nthrusts: a language-independent feature schema for rich morphological\nannotation and a type-level resource of annotated data in diverse languages\nrealizing that schema. This paper presents the expansions and improvements made\non several fronts over the last couple of years (since McCarthy et al. (2020)).\nCollaborative efforts by numerous linguists have added 67 new languages,\nincluding 30 endangered languages. We have implemented several improvements to\nthe extraction pipeline to tackle some issues, e.g. missing gender and macron\ninformation. We have also amended the schema to use a hierarchical structure\nthat is needed for morphological phenomena like multiple-argument agreement and\ncase stacking, while adding some missing morphological features to make the\nschema more inclusive. In light of the last UniMorph release, we also augmented\nthe database with morpheme segmentation for 16 languages. Lastly, this new\nrelease makes a push towards inclusion of derivational morphology in UniMorph\nby enriching the data and annotation schema with instances representing\nderivational processes from MorphyNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Batsuren_K/0/1/0/all/0/1\">Khuyagbaatar Batsuren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_S/0/1/0/all/0/1\">Salam Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kieras_W/0/1/0/all/0/1\">Witold Kiera&#x15b;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">G&#xe1;bor Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonard_B/0/1/0/all/0/1\">Brian Leonard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolai_G/0/1/0/all/0/1\">Garrett Nicolai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorman_K/0/1/0/all/0/1\">Kyle Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ate_Y/0/1/0/all/0/1\">Yustinus Ghanggo Ate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1\">Maria Ryskina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mielke_S/0/1/0/all/0/1\">Sabrina J. Mielke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budianskaya_E/0/1/0/all/0/1\">Elena Budianskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Khaissi_C/0/1/0/all/0/1\">Charbel El-Khaissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasser_M/0/1/0/all/0/1\">Michael Gasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_W/0/1/0/all/0/1\">William Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_M/0/1/0/all/0/1\">Mohit Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coler_M/0/1/0/all/0/1\">Matt Coler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samame_J/0/1/0/all/0/1\">Jaime Rafael Montoya Samame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camaiteri_D/0/1/0/all/0/1\">Delio Siticonatzi Camaiteri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_E/0/1/0/all/0/1\">Esa&#xfa; Zumaeta Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_D/0/1/0/all/0/1\">Didier L&#xf3;pez Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oncevay_A/0/1/0/all/0/1\">Arturo Oncevay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bautista_J/0/1/0/all/0/1\">Juan L&#xf3;pez Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_G/0/1/0/all/0/1\">Gema Celeste Silva Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ek_A/0/1/0/all/0/1\">Adam Ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guriel_D/0/1/0/all/0/1\">David Guriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dirix_P/0/1/0/all/0/1\">Peter Dirix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardy_J/0/1/0/all/0/1\">Jean-Philippe Bernardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherbakov_A/0/1/0/all/0/1\">Andrey Scherbakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayyr_ool_A/0/1/0/all/0/1\">Aziyana Bayyr-ool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zariquiey_R/0/1/0/all/0/1\">Roberto Zariquiey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheifer_K/0/1/0/all/0/1\">Karina Sheifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganieva_S/0/1/0/all/0/1\">Sofya Ganieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_H/0/1/0/all/0/1\">Hilaria Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karahoga_R/0/1/0/all/0/1\">Ritv&#xe1;n Karah&#xf3;&#x1e7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markantonatou_S/0/1/0/all/0/1\">Stella Markantonatou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlidis_G/0/1/0/all/0/1\">George Pavlidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plugaryov_M/0/1/0/all/0/1\">Matvey Plugaryov</a>, et al. (53 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improved Zero-shot Voice Conversion with Conditional DSVAE. (arXiv:2205.05227v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2205.05227","description":"<p>Disentangling content and speaking style information is essential for\nzero-shot non-parallel voice conversion (VC). Our previous study investigated a\nnovel framework with disentangled sequential variational autoencoder (DSVAE) as\nthe backbone for information decomposition. We have demonstrated that\nsimultaneous disentangling content embedding and speaker embedding from one\nutterance is feasible for zero-shot VC. In this study, we continue the\ndirection by raising one concern about the prior distribution of content branch\nin the DSVAE baseline. We find the random initialized prior distribution will\nforce the content embedding to reduce the phonetic-structure information during\nthe learning process, which is not a desired property. Here, we seek to achieve\na better content embedding with more phonetic information preserved. We propose\nconditional DSVAE, a new model that enables content bias as a condition to the\nprior modeling and reshapes the content embedding sampled from the posterior\ndistribution. In our experiment on the VCTK dataset, we demonstrate that\ncontent embeddings derived from the conditional DSVAE overcome the randomness\nand achieve a much better phoneme classification accuracy, a stabilized\nvocalization and a better zero-shot VC performance compared with the\ncompetitive DSVAE baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lian_J/0/1/0/all/0/1\">Jiachen Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chunlei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anumanchipalli_G/0/1/0/all/0/1\">Gopala Krishna Anumanchipalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Zero-shot Common Sense from Large Language Models for Robot 3D Scene Understanding. (arXiv:2206.04585v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2206.04585","description":"<p>Semantic 3D scene understanding is a problem of critical importance in\nrobotics. While significant advances have been made in simultaneous\nlocalization and mapping algorithms, robots are still far from having the\ncommon sense knowledge about household objects and their locations of an\naverage human. We introduce a novel method for leveraging common sense embedded\nwithin large language models for labelling rooms given the objects contained\nwithin. This algorithm has the added benefits of (i) requiring no task-specific\npre-training (operating entirely in the zero-shot regime) and (ii) generalizing\nto arbitrary room and object labels, including previously-unseen ones -- both\nof which are highly desirable traits in robotic scene understanding algorithms.\nThe proposed algorithm operates on 3D scene graphs produced by modern spatial\nperception systems, and we hope it will pave the way to more generalizable and\nscalable high-level 3D scene understanding for robotics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Siyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talak_R/0/1/0/all/0/1\">Rajat Talak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solution of DeBERTaV3 on CommonsenseQA. (arXiv:2206.05033v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05033","description":"<p>We report the performance of DeBERTaV3 on CommonsenseQA in this report. We\nsimply formalize the answer selection as a text classification for DeBERTaV3.\nThe strong natural language inference ability of DeBERTaV3 helps its single and\nensemble model set the new (w/o external knowledge) state-of-the-art on\nCommonsenseQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REKnow: Enhanced Knowledge for Joint Entity and Relation Extraction. (arXiv:2206.05123v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05123","description":"<p>Relation extraction is an important but challenging task that aims to extract\nall hidden relational facts from the text. With the development of deep\nlanguage models, relation extraction methods have achieved good performance on\nvarious benchmarks. However, we observe two shortcomings of previous methods:\nfirst, there is no unified framework that works well under various relation\nextraction settings; second, effectively utilizing external knowledge as\nbackground information is absent. In this work, we propose a knowledge-enhanced\ngenerative model to mitigate these two issues. Our generative model is a\nunified framework to sequentially generate relational triplets under various\nrelation extraction settings and explicitly utilizes relevant knowledge from\nKnowledge Graph (KG) to resolve ambiguities. Our model achieves superior\nperformance on multiple benchmarks and settings, including WebNLG, NYT10, and\nTACRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AHD ConvNet for Speech Emotion Classification. (arXiv:2206.05286v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.05286","description":"<p>Accomplishments in the field of artificial intelligence are utilized in the\nadvancement of computing and making of intelligent machines for facilitating\nmankind and improving user experience. Emotions are rudimentary for people,\naffecting thinking and ordinary exercises like correspondence, learning and\ndirection. Speech emotion recognition is domain of interest in this regard and\nin this work, we propose a novel mel spectrogram learning approach in which our\nmodel uses the datapoints to learn emotions from the given wav form voice notes\nin the popular CREMA-D dataset. Our model uses log mel-spectrogram as feature\nwith number of mels = 64. It took less training time compared to other\napproaches used to address the problem of emotion speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Asfand Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasir_D/0/1/0/all/0/1\">Danial Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawad_M/0/1/0/all/0/1\">Mohammad Hassan Jawad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for Simultaneous Speech Translation. (arXiv:2206.05807v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05807","description":"<p>Simultaneous speech translation (SimulST) systems aim at generating their\noutput with the lowest possible latency, which is normally computed in terms of\nAverage Lagging (AL). In this paper we highlight that, despite its widespread\nadoption, AL provides underestimated scores for systems that generate longer\npredictions compared to the corresponding references. We also show that this\nproblem has practical relevance, as recent SimulST systems have indeed a\ntendency to over-generate. As a solution, we propose LAAL (Length-Adaptive\nAverage Lagging), a modified version of the metric that takes into account the\nover-generation phenomenon and allows for unbiased evaluation of both\nunder-/over-generating systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition. (arXiv:2206.08317v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.08317","description":"<p>Transformers have recently dominated the ASR field. Although able to yield\ngood performance, they involve an autoregressive (AR) decoder to generate\ntokens one by one, which is computationally inefficient. To speed up inference,\nnon-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to\nenable parallel generation. However, due to an independence assumption within\nthe output tokens, performance of single-step NAR is inferior to that of AR\nmodels, especially with a large-scale corpus. There are two challenges to\nimproving single-step NAR: Firstly to accurately predict the number of output\ntokens and extract hidden variables; secondly, to enhance modeling of\ninterdependence between output tokens. To tackle both challenges, we propose a\nfast and accurate parallel transformer, termed Paraformer. This utilizes a\ncontinuous integrate-and-fire based predictor to predict the number of tokens\nand generate hidden variables. A glancing language model (GLM) sampler then\ngenerates semantic embeddings to enhance the NAR decoder's ability to model\ncontext interdependence. Finally, we design a strategy to generate negative\nsamples for minimum word error rate training to further improve performance.\nExperiments using the public AISHELL-1, AISHELL-2 benchmark, and an\nindustrial-level 20,000 hour task demonstrate that the proposed Paraformer can\nattain comparable performance to the state-of-the-art AR transformer, with more\nthan 10x speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLoughlin_I/0/1/0/all/0/1\">Ian McLoughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2206.08406","description":"<p>Tweets are the most concise form of communication in online social media,\nwherein a single tweet has the potential to make or break the discourse of the\nconversation. Online hate speech is more accessible than ever, and stifling its\npropagation is of utmost importance for social media companies and users for\ncongenial communication. Most of the research barring a recent few has focused\non classifying an individual tweet regardless of the tweet thread/context\nleading up to that point. One of the classical approaches to curb hate speech\nis to adopt a reactive strategy after the hate speech postage. The ex-post\nfacto strategy results in neglecting subtle posts that do not show the\npotential to instigate hate speech on their own but may portend in the\nsubsequent discussion ensuing in the post's replies. In this paper, we propose\nDRAGNET++, which aims to predict the intensity of hatred that a tweet can bring\nin through its reply chain in the future. It uses the semantic and propagating\nstructure of the tweet threads to maximize the contextual information leading\nup to and the fall of hate intensity at each subsequent tweet. We explore three\npublicly available Twitter datasets -- Anti-Racism contains the reply tweets of\na collection of social media discourse on racist remarks during US political\nand Covid-19 background; Anti-Social presents a dataset of 40 million tweets\namidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents\nTwitter datasets collated based on anti-Asian behaviours during COVID-19\npandemic. All the curated datasets consist of structural graph information of\nthe Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art\nbaselines significantly. It beats the best baseline by an 11% margin on the\nPerson correlation coefficient and a decrease of 25% on RMSE for the\nAnti-Racism dataset with a similar performance on the other two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qing Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_T/0/1/0/all/0/1\">Tharun Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Simultaneous Bone and Shadow Segmentation Network using Task Correspondence Consistency. (arXiv:2206.08936v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08936","description":"<p>Segmenting both bone surface and the corresponding acoustic shadow are\nfundamental tasks in ultrasound (US) guided orthopedic procedures. However,\nthese tasks are challenging due to minimal and blurred bone surface response in\nUS images, cross-machine discrepancy, imaging artifacts, and low\nsignal-to-noise ratio. Notably, bone shadows are caused by a significant\nacoustic impedance mismatch between the soft tissue and bone surfaces. To\nleverage this mutual information between these highly related tasks, we propose\na single end-to-end network with a shared transformer-based encoder and task\nindependent decoders for simultaneous bone and shadow segmentation. To share\ncomplementary features, we propose a cross task feature transfer block which\nlearns to transfer meaningful features from decoder of shadow segmentation to\nthat of bone segmentation and vice-versa. We also introduce a correspondence\nconsistency loss which makes sure that network utilizes the inter-dependency\nbetween the bone surface and its corresponding shadow to refine the\nsegmentation. Validation against expert annotations shows that the method\noutperforms the previous state-of-the-art for both bone surface and shadow\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rahman_A/0/1/0/all/0/1\">Aimon Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hacihaliloglu_I/0/1/0/all/0/1\">Ilker Hacihaliloglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation. (arXiv:2206.08948v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08948","description":"<p>We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based\nframework for panoptic segmentation designed around clustering. It rethinks the\nexisting transformer architectures used in segmentation and detection;\nCMT-DeepLab considers the object queries as cluster centers, which fill the\nrole of grouping the pixels when applied to segmentation. The clustering is\ncomputed with an alternating procedure, by first assigning pixels to the\nclusters by their feature affinity, and then updating the cluster centers and\npixel features. Together, these operations comprise the Clustering Mask\nTransformer (CMT) layer, which produces cross-attention that is denser and more\nconsistent with the final segmentation task. CMT-DeepLab improves the\nperformance over prior art significantly by 4.4% PQ, achieving a new\nstate-of-the-art of 55.7% PQ on the COCO test-dev set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qihang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Siyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Maxwell Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding. (arXiv:2206.08954v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08954","description":"<p>Recently, self-supervised learning (SSL) has achieved tremendous empirical\nadvancements in learning image representation. However, our understanding and\nknowledge of the representation are still limited. This work shows that the\nsuccess of the SOTA siamese-network-based SSL approaches is primarily based on\nlearning a representation of image patches. Particularly, we show that when we\nlearn a representation only for fixed-scale image patches and aggregate\ndifferent patch representations linearly for an image (instance), it can\nachieve on par or even better results than the baseline methods on several\nbenchmarks. Further, we show that the patch representation aggregation can also\nimprove various SOTA baseline methods by a large margin. We also establish a\nformal connection between the SSL objective and the image patches co-occurrence\nstatistics modeling, which supplements the prevailing invariance perspective.\nBy visualizing the nearest neighbors of different image patches in the\nembedding space and projection space, we show that while the projection has\nmore invariance, the embedding space tends to preserve more equivariance and\nlocality. Finally, we propose a hypothesis for the future direction based on\nthe discovery of this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardes_A/0/1/0/all/0/1\">Adrien Bardes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zengyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KitBit: A New AI Model for Solving Intelligence Tests and Numerical Series. (arXiv:2206.08965v1 [cs.AI])","link":"http://arxiv.org/abs/2206.08965","description":"<p>The resolution of intelligence tests, in particular numerical sequences, has\nbeen of great interest in the evaluation of AI systems. We present a new\ncomputational model called KitBit that uses a reduced set of algorithms and\ntheir combinations to build a predictive model that finds the underlying\npattern in numerical sequences, such as those included in IQ tests and others\nof much greater complexity. We present the fundamentals of the model and its\napplication in different cases. First, the system is tested on a set of number\nseries used in IQ tests collected from various sources. Next, our model is\nsuccessfully applied on the sequences used to evaluate the models reported in\nthe literature. In both cases, the system is capable of solving these types of\nproblems in less than a second using standard computing power. Finally,\nKitBit's algorithms have been applied for the first time to the complete set of\nentire sequences of the well-known OEIS database. We find a pattern in the form\nof a list of algorithms and predict the following terms in the largest number\nof series to date. These results demonstrate the potential of KitBit to solve\ncomplex problems that could be represented numerically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corsino_V/0/1/0/all/0/1\">V&#xed;ctor Corsino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilperez_J/0/1/0/all/0/1\">Jos&#xe9; Manuel Gilp&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_L/0/1/0/all/0/1\">Luis Herrera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiEarth 2022 -- The Champion Solution for the Matrix Completion Challenge via Multimodal Regression and Generation. (arXiv:2206.08970v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08970","description":"<p>Earth observation satellites have been continuously monitoring the earth\nenvironment for years at different locations and spectral bands with different\nmodalities. Due to complex satellite sensing conditions (e.g., weather, cloud,\natmosphere, orbit), some observations for certain modalities, bands, locations,\nand times may not be available. The MultiEarth Matrix Completion Challenge in\nCVPR 2022 [1] provides the multimodal satellite data for addressing such data\nsparsity challenges with the Amazon Rainforest as the region of interest. This\nwork proposes an adaptive real-time multimodal regression and generation\nframework and achieves superior performance on unseen test queries in this\nchallenge with an LPIPS of 0.2226, a PSNR of 123.0372, and an SSIM of 0.6347.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1\">Yuchuan Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jui-Hsin Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla Handwritten Text Recognition (HTR) and Line Segmentation. (arXiv:2206.08977v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08977","description":"<p>We introduce a new dataset for offline Handwritten Text Recognition (HTR)\nfrom images of Bangla scripts comprising words, lines, and document-level\nannotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant\nto act as ground truth texts. These texts were subsequently used to generate\nthe annotations that were filled out by people with their handwriting. Our\ndataset includes 788 images of handwritten pages produced by approximately 150\ndifferent writers. It can be adopted as a basis for various handwriting\nclassification tasks such as end-to-end document recognition, word-spotting,\nword or line segmentation, and so on. We also propose a scheme to segment\nBangla handwritten document images into corresponding lines in an unsupervised\nmanner. Our line segmentation approach takes care of the variability involved\nin different writing styles, accurately segmenting complex handwritten text\nlines of curvilinear nature. Along with a bunch of pre-processing and\nmorphological operations, both Hough line and circle transforms were employed\nto distinguish different linear components. In order to arrange those\ncomponents into their corresponding lines, we followed an unsupervised\nclustering approach. The average success rate of our segmentation technique is\n81.57% in terms of FM metrics (similar to F-measure) with a mean Average\nPrecision (mAP) of 0.547.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md. Ataur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabassum_N/0/1/0/all/0/1\">Nazifa Tabassum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Mitu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_R/0/1/0/all/0/1\">Riya Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mohammad Khairul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness. (arXiv:2206.08984v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08984","description":"<p>Magnetic Resonance Spectroscopic Imaging (MRSI) is a valuable tool for\nstudying metabolic activities in the human body, but the current applications\nare limited to low spatial resolutions. The existing deep learning-based MRSI\nsuper-resolution methods require training a separate network for each upscaling\nfactor, which is time-consuming and memory inefficient. We tackle this\nmulti-scale super-resolution problem using a Filter Scaling strategy that\nmodulates the convolution filters based on the upscaling factor, such that a\nsingle network can be used for various upscaling factors. Observing that each\nmetabolite has distinct spatial characteristics, we also modulate the network\nbased on the specific metabolite. Furthermore, our network is conditioned on\nthe weight of adversarial loss so that the perceptual sharpness of the\nsuper-resolved metabolic maps can be adjusted within a single network. We\nincorporate these network conditionings using a novel Multi-Conditional Module.\nThe experiments were carried out on a 1H-MRSI dataset from 15 high-grade glioma\npatients. Results indicate that the proposed network achieves the best\nperformance among several multi-scale super-resolution methods and can provide\nsuper-resolved metabolic maps with adjustable sharpness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dong_S/0/1/0/all/0/1\">Siyuan Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hangel_G/0/1/0/all/0/1\">Gilbert Hangel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogner_W/0/1/0/all/0/1\">Wolfgang Bogner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Widhalm_G/0/1/0/all/0/1\">Georg Widhalm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rossler_K/0/1/0/all/0/1\">Karl R&#xf6;ssler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trattnig_S/0/1/0/all/0/1\">Siegfried Trattnig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graaf_R/0/1/0/all/0/1\">Robin de Graaf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Onofrey_J/0/1/0/all/0/1\">John Onofrey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1\">James Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransResU-Net: Transformer based ResU-Net for Real-Time Colonoscopy Polyp Segmentation. (arXiv:2206.08985v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08985","description":"<p>Colorectal cancer (CRC) is one of the most common causes of cancer and\ncancer-related mortality worldwide. Performing colon cancer screening in a\ntimely fashion is the key to early detection. Colonoscopy is the primary\nmodality used to diagnose colon cancer. However, the miss rate of polyps,\nadenomas and advanced adenomas remains significantly high. Early detection of\npolyps at the precancerous stage can help reduce the mortality rate and the\neconomic burden associated with colorectal cancer. Deep learning-based\ncomputer-aided diagnosis (CADx) system may help gastroenterologists to identify\npolyps that may otherwise be missed, thereby improving the polyp detection\nrate. Additionally, CADx system could prove to be a cost-effective system that\nimproves long-term colorectal cancer prevention. In this study, we proposed a\ndeep learning-based architecture for automatic polyp segmentation, called\nTransformer ResU-Net (TransResU-Net). Our proposed architecture is built upon\nresidual blocks with ResNet-50 as the backbone and takes the advantage of\ntransformer self-attention mechanism as well as dilated convolution(s). Our\nexperimental results on two publicly available polyp segmentation benchmark\ndatasets showed that TransResU-Net obtained a highly promising dice score and a\nreal-time speed. With high efficacy in our performance metrics, we concluded\nthat TransResU-Net could be a strong benchmark for building a real-time polyp\ndetection system for the early diagnosis, treatment, and prevention of\ncolorectal cancer. The source code of the proposed TransResU-Net is publicly\navailable at https://github.com/nikhilroxtomar/TransResUNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tomar_N/0/1/0/all/0/1\">Nikhil Kumar Tomar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shergill_A/0/1/0/all/0/1\">Annie Shergill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rieders_B/0/1/0/all/0/1\">Brandon Rieders</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shadows Shed Light on 3D Objects. (arXiv:2206.08990v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08990","description":"<p>3D reconstruction is a fundamental problem in computer vision, and the task\nis especially challenging when the object to reconstruct is partially or fully\noccluded. We introduce a method that uses the shadows cast by an unobserved\nobject in order to infer the possible 3D volumes behind the occlusion. We\ncreate a differentiable image formation model that allows us to jointly infer\nthe 3D shape of an object, its pose, and the position of a light source. Since\nthe approach is end-to-end differentiable, we are able to integrate learned\npriors of object geometry in order to generate realistic 3D shapes of different\nobject categories. Experiments and visualizations show that the method is able\nto generate multiple possible solutions that are consistent with the\nobservation of the shadow. Our approach works even when the position of the\nlight source and object pose are both unknown. Our approach is also robust to\nreal-world images where ground-truth shadow mask is unknown.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruoshi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_S/0/1/0/all/0/1\">Sachit Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dennis Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stent_S/0/1/0/all/0/1\">Simon Stent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Group Synchronization via Quadratic Programming. (arXiv:2206.08994v1 [stat.ML])","link":"http://arxiv.org/abs/2206.08994","description":"<p>We propose a novel quadratic programming formulation for estimating the\ncorruption levels in group synchronization, and use these estimates to solve\nthis problem. Our objective function exploits the cycle consistency of the\ngroup and we thus refer to our method as detection and estimation of structural\nconsistency (DESC). This general framework can be extended to other algebraic\nand geometric structures. Our formulation has the following advantages: it can\ntolerate corruption as high as the information-theoretic bound, it does not\nrequire a good initialization for the estimates of group elements, it has a\nsimple interpretation, and under some mild conditions the global minimum of our\nobjective function exactly recovers the corruption levels. We demonstrate the\ncompetitive accuracy of our approach on both synthetic and real data\nexperiments of rotation averaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Shi_Y/0/1/0/all/0/1\">Yunpeng Shi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wyeth_C/0/1/0/all/0/1\">Cole Wyeth</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lerman_G/0/1/0/all/0/1\">Gilad Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion models as plug-and-play priors. (arXiv:2206.09012v1 [cs.LG])","link":"http://arxiv.org/abs/2206.09012","description":"<p>We consider the problem of inferring high-dimensional data $\\mathbf{x}$ in a\nmodel that consists of a prior $p(\\mathbf{x})$ and an auxiliary constraint\n$c(\\mathbf{x},\\mathbf{y})$. In this paper, the prior is an independently\ntrained denoising diffusion generative model. The auxiliary constraint is\nexpected to have a differentiable form, but can come from diverse sources. The\npossibility of such inference turns diffusion models into plug-and-play\nmodules, thereby allowing a range of potential applications in adapting models\nto new domains and tasks, such as conditional generation or image segmentation.\nThe structure of diffusion models allows us to perform approximate inference by\niterating differentiation through the fixed denoising network enriched with\ndifferent amounts of noise at each step. Considering many noised versions of\n$\\mathbf{x}$ in evaluation of its fitness is a novel search mechanism that may\nlead to new algorithms for solving combinatorial optimization problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graikos_A/0/1/0/all/0/1\">Alexandros Graikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1\">Nikolay Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Landscape Learning for Neural Network Inversion. (arXiv:2206.09027v1 [cs.CV])","link":"http://arxiv.org/abs/2206.09027","description":"<p>Many machine learning methods operate by inverting a neural network at\ninference time, which has become a popular technique for solving inverse\nproblems in computer vision, robotics, and graphics. However, these methods\noften involve gradient descent through a highly non-convex loss landscape,\ncausing the optimization process to be unstable and slow. We introduce a method\nthat learns a loss landscape where gradient descent is efficient, bringing\nmassive improvement and acceleration to the inversion process. We demonstrate\nthis advantage on a number of methods for both generative and discriminative\ntasks, including GAN inversion, adversarial defense, and 3D human pose\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruoshi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tendulkar_P/0/1/0/all/0/1\">Purva Tendulkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stop Overcomplicating Selective Classification: Use Max-Logit. (arXiv:2206.09034v1 [cs.LG])","link":"http://arxiv.org/abs/2206.09034","description":"<p>We tackle the problem of Selective Classification where the goal is to\nachieve the best performance on the desired coverages of the dataset. Recent\nstate-of-the-art selective methods come with architectural changes either via\nintroducing a separate selection head or an extra abstention logit. In this\npaper, we present surprising results for Selective Classification by confirming\nthat the superior performance of state-of-the-art methods is owed to training a\nmore generalizable classifier; however, their selection mechanism is\nsuboptimal. We argue that the selection mechanism should be rooted in the\nobjective function instead of a separately calculated score. Accordingly, in\nthis paper, we motivate an alternative selection strategy that is based on the\ncross entropy loss for the classification settings, namely, max of the logits.\nOur proposed selection strategy achieves better results by a significant\nmargin, consistently, across all coverages and all datasets, without any\nadditional computation. Finally, inspired by our superior selection mechanism,\nwe propose to further regularize the objective function with\nentropy-minimization. Our proposed max-logit selection with the modified loss\nfunction achieves new state-of-the-art results for Selective Classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Leo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Mohamed Osama Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajimirsadeghi_H/0/1/0/all/0/1\">Hossein Hajimirsadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdi_A/0/1/0/all/0/1\">Amir Abdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Validation of Vector Data using Oblique Images. (arXiv:2206.09038v1 [cs.CV])","link":"http://arxiv.org/abs/2206.09038","description":"<p>Oblique images are aerial photographs taken at oblique angles to the earth's\nsurface. Projections of vector and other geospatial data in these images depend\non camera parameters, positions of the geospatial entities, surface terrain,\nocclusions, and visibility. This paper presents a robust and scalable algorithm\nto detect inconsistencies in vector data using oblique images. The algorithm\nuses image descriptors to encode the local appearance of a geospatial entity in\nimages. These image descriptors combine color, pixel-intensity gradients,\ntexture, and steerable filter responses. A Support Vector Machine classifier is\ntrained to detect image descriptors that are not consistent with underlying\nvector data, digital elevation maps, building models, and camera parameters. In\nthis paper, we train the classifier on visible road segments and non-road data.\nThereafter, the trained classifier detects inconsistencies in vectors, which\ninclude both occluded and misaligned road segments. The consistent road\nsegments validate our vector, DEM, and 3-D model data for those areas while\ninconsistent segments point out errors. We further show that a search for\ndescriptors that are consistent with visible road segments in the neighborhood\nof a misaligned road yields the desired road alignment that is consistent with\npixels in the image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pragyana Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofek_E/0/1/0/all/0/1\">Eyal Ofek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimchi_G/0/1/0/all/0/1\">Gur Kimchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmented Imagefication: A Data-driven Fault Detection Method for Aircraft Air Data Sensors. (arXiv:2206.09055v1 [cs.CV])","link":"http://arxiv.org/abs/2206.09055","description":"<p>In this paper, a novel data-driven approach named Augmented Imagefication for\nFault detection (FD) of aircraft air data sensors (ADS) is proposed.\nExemplifying the FD problem of aircraft air data sensors, an online FD scheme\non edge device based on deep neural network (DNN) is developed. First, the\naircraft inertial reference unit measurements is adopted as equivalent inputs,\nwhich is scalable to different aircraft/flight cases. Data associated with 6\ndifferent aircraft/flight conditions are collected to provide diversity\n(scalability) in the training/testing database. Then Augmented Imagefication is\nproposed for the DNN-based prediction of flying conditions. The raw data are\nreshaped as a grayscale image for convolutional operation, and the necessity of\naugmentation is analyzed and pointed out. Different kinds of augmented method,\ni.e. Flip, Repeat, Tile and their combinations are discussed, the result shows\nthat the All Repeat operation in both axes of image matrix leads to the best\nperformance of DNN. The interpretability of DNN is studied based on Grad-CAM,\nwhich provide a better understanding and further solidifies the robustness of\nDNN. Next the DNN model, VGG-16 with augmented imagefication data is optimized\nfor mobile hardware deployment. After pruning of DNN, a lightweight model\n(98.79% smaller than original VGG-16) with high accuracy (slightly up by 0.27%)\nand fast speed (time delay is reduced by 87.54%) is obtained. And the\nhyperparameters optimization of DNN based on TPE is implemented and the best\ncombination of hyperparameters is determined (learning rate 0.001, iterative\nepochs 600, and batch size 100 yields the highest accuracy at 0.987). Finally,\na online FD deployment based on edge device, Jetson Nano, is developed and the\nreal time monitoring of aircraft is achieved. We believe that this method is\ninstructive for addressing the FD problems in other similar fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jinyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yiqun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_J/0/1/0/all/0/1\">Jianliang Ai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks. (arXiv:2206.09059v1 [cs.CL])","link":"http://arxiv.org/abs/2206.09059","description":"<p>Current state-of-the-art vision-and-language models are evaluated on tasks\neither individually or in a multi-task setting, overlooking the challenges of\ncontinually learning (CL) tasks as they arrive. Existing CL benchmarks have\nfacilitated research on task adaptation and mitigating \"catastrophic\nforgetting\", but are limited to vision-only and language-only tasks. We present\nCLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL\nsetting, and to systematically evaluate how upstream continual learning can\nrapidly generalize to new multimodal and unimodal tasks. CLiMB includes\nimplementations of several CL algorithms and a modified Vision-Language\nTransformer (ViLT) model that can be deployed on both multimodal and unimodal\ntasks. We find that common CL methods can help mitigate forgetting during\nmultimodal task learning, but do not enable cross-task knowledge transfer. We\nenvision that CLiMB will facilitate research on a new class of CL algorithms\nfor this challenging multimodal setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Ting-Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alva_L/0/1/0/all/0/1\">Leticia Leonor Pinto Alva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chochlakis_G/0/1/0/all/0/1\">Georgios Chochlakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design of Supervision-Scalable Learning Systems: Methodology and Performance Benchmarking. (arXiv:2206.09061v1 [cs.CV])","link":"http://arxiv.org/abs/2206.09061","description":"<p>The design of robust learning systems that offer stable performance under a\nwide range of supervision degrees is investigated in this work. We choose the\nimage classification problem as an illustrative example and focus on the design\nof modularized systems that consist of three learning modules: representation\nlearning, feature learning and decision learning. We discuss ways to adjust\neach module so that the design is robust with respect to different training\nsample numbers. Based on these ideas, we propose two families of learning\nsystems. One adopts the classical histogram of oriented gradients (HOG)\nfeatures while the other uses successive-subspace-learning (SSL) features. We\ntest their performance against LeNet-5, which is an end-to-end optimized neural\nnetwork, for MNIST and Fashion-MNIST datasets. The number of training samples\nper image class goes from the extremely weak supervision condition (i.e., 1\nlabeled sample per class) to the strong supervision condition (i.e., 4096\nlabeled sample per class) with gradual transition in between (i.e., $2^n$,\n$n=0, 1, \\cdots, 12$). Experimental results show that the two families of\nmodularized learning systems have more robust performance than LeNet-5. They\nboth outperform LeNet-5 by a large margin for small $n$ and have performance\ncomparable with that of LeNet-5 for large $n$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yijing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Free-form Lesion Synthesis Using a Partial Convolution Generative Adversarial Network for Enhanced Deep Learning Liver Tumor Segmentation. (arXiv:2206.09065v1 [eess.IV])","link":"http://arxiv.org/abs/2206.09065","description":"<p>Automatic deep learning segmentation models has been shown to improve both\nthe segmentation efficiency and the accuracy. However, training a robust\nsegmentation model requires considerably large labeled training samples, which\nmay be impractical. This study aimed to develop a deep learning framework for\ngenerating synthetic lesions that can be used to enhance network training. The\nlesion synthesis network is a modified generative adversarial network (GAN).\nSpecifically, we innovated a partial convolution strategy to construct an\nUnet-like generator. The discriminator is designed using Wasserstein GAN with\ngradient penalty and spectral normalization. A mask generation method based on\nprincipal component analysis was developed to model various lesion shapes. The\ngenerated masks are then converted into liver lesions through a lesion\nsynthesis network. The lesion synthesis framework was evaluated for lesion\ntextures, and the synthetic lesions were used to train a lesion segmentation\nnetwork to further validate the effectiveness of this framework. All the\nnetworks are trained and tested on the public dataset from LITS. The synthetic\nlesions generated by the proposed approach have very similar histogram\ndistributions compared to the real lesions for the two employed texture\nparameters, GLCM-energy and GLCM-correlation. The Kullback-Leibler divergence\nof GLCM-energy and GLCM-correlation were 0.01 and 0.10, respectively. Including\nthe synthetic lesions in the tumor segmentation network improved the\nsegmentation dice performance of U-Net significantly from 67.3% to 71.4%\n(p&lt;0.05). Meanwhile, the volume precision and sensitivity improve from 74.6% to\n76.0% (p=0.23) and 66.1% to 70.9% (p&lt;0.01), respectively. The synthetic data\nsignificantly improves the segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yingao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yidong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Dynamic Subspace Learners for Medical Image Analysis. (arXiv:2206.09068v1 [cs.CV])","link":"http://arxiv.org/abs/2206.09068","description":"<p>Learning similarity is a key aspect in medical image analysis, particularly\nin recommendation systems or in uncovering the interpretation of anatomical\ndata in images. Most existing methods learn such similarities in the embedding\nspace over image sets using a single metric learner. Images, however, have a\nvariety of object attributes such as color, shape, or artifacts. Encoding such\nattributes using a single metric learner is inadequate and may fail to\ngeneralize. Instead, multiple learners could focus on separate aspects of these\nattributes in subspaces of an overarching embedding. This, however, implies the\nnumber of learners to be found empirically for each new dataset. This work,\nDynamic Subspace Learners, proposes to dynamically exploit multiple learners by\nremoving the need of knowing apriori the number of learners and aggregating new\nsubspace learners during training. Furthermore, the visual interpretability of\nsuch subspace learning is enforced by integrating an attention module into our\nmethod. This integrated attention mechanism provides a visual insight of\ndiscriminative image features that contribute to the clustering of image sets\nand a visual explanation of the embedding features. The benefits of our\nattention-based dynamic subspace learners are evaluated in the application of\nimage clustering, image retrieval, and weakly supervised segmentation. Our\nmethod achieves competitive results with the performances of multiple learners\nbaselines and significantly outperforms the classification network in terms of\nclustering and retrieval scores on three different public benchmark datasets.\nMoreover, our attention maps offer a proxy-labels, which improves the\nsegmentation accuracy up to 15% in Dice scores when compared to\nstate-of-the-art interpretation techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+V_S/0/1/0/all/0/1\">Sukesh Adiga V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1\">Herve Lombaert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiamVGG: Visual Tracking using Deeper Siamese Networks. (arXiv:1902.02804v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1902.02804","description":"<p>Recently, we have seen a rapid development of Deep Neural Network (DNN) based\nvisual tracking solutions. Some trackers combine the DNN-based solutions with\nDiscriminative Correlation Filters (DCF) to extract semantic features and\nsuccessfully deliver the state-of-the-art tracking accuracy. However, these\nsolutions are highly compute-intensive, which require long processing time,\nresulting unsecured real-time performance. To deliver both high accuracy and\nreliable real-time performance, we propose a novel tracker called SiamVGG. It\ncombines a Convolutional Neural Network (CNN) backbone and a cross-correlation\noperator, and takes advantage of the features from exemplary images for more\naccurate object tracking.\n</p>\n<p>The architecture of SiamVGG is customized from VGG-16, with the parameters\nshared by both exemplary images and desired input video frames.\n</p>\n<p>We demonstrate the proposed SiamVGG on OTB-2013/50/100 and VOT 2015/2016/2017\ndatasets with the state-of-the-art accuracy while maintaining a decent\nreal-time performance of 50 FPS running on a GTX 1080Ti. Our design can achieve\n2% higher Expected Average Overlap (EAO) compared to the ECO and C-COT in\nVOT2017 Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HERS: Homomorphically Encrypted Representation Search. (arXiv:2003.12197v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.12197","description":"<p>We present a method to search for a probe (or query) image representation\nagainst a large gallery in the encrypted domain. We require that the probe and\ngallery images be represented in terms of a fixed-length representation, which\nis typical for representations obtained from learned networks. Our encryption\nscheme is agnostic to how the fixed-length representation is obtained and can\ntherefore be applied to any fixed-length representation in any application\ndomain. Our method, dubbed HERS (Homomorphically Encrypted Representation\nSearch), operates by (i) compressing the representation towards its estimated\nintrinsic dimensionality with minimal loss of accuracy (ii) encrypting the\ncompressed representation using the proposed fully homomorphic encryption\nscheme, and (iii) efficiently searching against a gallery of encrypted\nrepresentations directly in the encrypted domain, without decrypting them.\nNumerical results on large galleries of face, fingerprint, and object datasets\nsuch as ImageNet show that, for the first time, accurate and fast image search\nwithin the encrypted domain is feasible at scale (500 seconds; $275\\times$\nspeed up over state-of-the-art for encrypted search against a gallery of 100\nmillion). Code is available at\nhttps://github.com/human-analysis/hers-encrypted-image-search\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SS-IL: Separated Softmax for Incremental Learning. (arXiv:2003.13947v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.13947","description":"<p>We consider class incremental learning (CIL) problem, in which a learning\nagent continuously learns new classes from incrementally arriving training data\nbatches and aims to predict well on all the classes learned so far. The main\nchallenge of the problem is the catastrophic forgetting, and for the\nexemplar-memory based CIL methods, it is generally known that the forgetting is\ncommonly caused by the classification score bias that is injected due to the\ndata imbalance between the new classes and the old classes (in the\nexemplar-memory). While several methods have been proposed to correct such\nscore bias by some additional post-processing, e.g., score re-scaling or\nbalanced fine-tuning, no systematic analysis on the root cause of such bias has\nbeen done. To that end, we analyze that computing the softmax probabilities by\ncombining the output scores for all old and new classes could be the main cause\nof the bias. Then, we propose a new method, dubbed as Separated Softmax for\nIncremental Learning (SS-IL), that consists of separated softmax (SS) output\nlayer combined with task-wise knowledge distillation (TKD) to resolve such\nbias. Throughout our extensive experimental results on several large-scale CIL\nbenchmark datasets, we show our SS-IL achieves strong state-of-the-art accuracy\nthrough attaining much more balanced prediction scores across old and new\nclasses, without any additional post-processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1\">Hongjoon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1\">Jihwan Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Subin Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_H/0/1/0/all/0/1\">Hyeonsu Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyojun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided interactive image segmentation using machine learning and color based data set clustering. (arXiv:2005.07662v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.07662","description":"<p>We present a novel approach that combines machine learning based interactive\nimage segmentation using supervoxels with a clustering method for the automated\nidentification of similarly colored images in large data sets which enables a\nguided reuse of classifiers. Our approach solves the problem of significant\ncolor variability prevalent and often unavoidable in biological and medical\nimages which typically leads to deteriorated segmentation and quantification\naccuracy thereby greatly reducing the necessary training effort. This increase\nin efficiency facilitates the quantification of much larger numbers of images\nthereby enabling interactive image analysis for recent new technological\nadvances in high-throughput imaging. The presented methods are applicable for\nalmost any image type and represent a useful tool for image analysis tasks in\ngeneral.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friebel_A/0/1/0/all/0/1\">Adrian Friebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johann_T/0/1/0/all/0/1\">Tim Johann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drasdo_D/0/1/0/all/0/1\">Dirk Drasdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoehme_S/0/1/0/all/0/1\">Stefan Hoehme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Frequency and Image Space Learning for MRI Reconstruction and Analysis. (arXiv:2007.01441v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.01441","description":"<p>We propose neural network layers that explicitly combine frequency and image\nfeature representations and show that they can be used as a versatile building\nblock for reconstruction from frequency space data. Our work is motivated by\nthe challenges arising in MRI acquisition where the signal is a corrupted\nFourier transform of the desired image. The proposed joint learning schemes\nenable both correction of artifacts native to the frequency space and\nmanipulation of image space representations to reconstruct coherent image\nstructures at every layer of the network. This is in contrast to most current\ndeep learning approaches for image reconstruction that treat frequency and\nimage space features separately and often operate exclusively in one of the two\nspaces. We demonstrate the advantages of joint convolutional learning for a\nvariety of tasks, including motion correction, denoising, reconstruction from\nundersampled acquisitions, and combined undersampling and motion correction on\nsimulated and real world multicoil MRI data. The joint models produce\nconsistently high quality output images across all tasks and datasets. When\nintegrated into a state of the art unrolled optimization network with\nphysics-inspired data consistency constraints for undersampled reconstruction,\nthe proposed architectures significantly improve the optimization landscape,\nwhich yields an order of magnitude reduction of training time. This result\nsuggests that joint representations are particularly well suited for MRI\nsignals in deep learning networks. Our code and pretrained models are publicly\navailable at https://github.com/nalinimsingh/interlacer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nalini M. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Example Difficulty Using Variance of Gradients. (arXiv:2008.11600v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.11600","description":"<p>In machine learning, a question of great interest is understanding what\nexamples are challenging for a model to classify. Identifying atypical examples\nensures the safe deployment of models, isolates samples that require further\nhuman inspection and provides interpretability into model behavior. In this\nwork, we propose Variance of Gradients (VoG) as a valuable and efficient metric\nto rank data by difficulty and to surface a tractable subset of the most\nchallenging examples for human-in-the-loop auditing. We show that data points\nwith high VoG scores are far more difficult for the model to learn and\nover-index on corrupted or memorized examples. Further, restricting the\nevaluation to the test set instances with the lowest VoG improves the model's\ngeneralization performance. Finally, we show that VoG is a valuable and\nefficient ranking for out-of-distribution detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Action Recognition from Various Data Modalities: A Review. (arXiv:2012.11866v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.11866","description":"<p>Human Action Recognition (HAR) aims to understand human behavior and assign a\nlabel to each action. It has a wide range of applications, and therefore has\nbeen attracting increasing attention in the field of computer vision. Human\nactions can be represented using various data modalities, such as RGB,\nskeleton, depth, infrared, point cloud, event stream, audio, acceleration,\nradar, and WiFi signal, which encode different sources of useful yet distinct\ninformation and have various advantages depending on the application scenarios.\nConsequently, lots of existing works have attempted to investigate different\ntypes of approaches for HAR using various modalities. In this paper, we present\na comprehensive survey of recent progress in deep learning methods for HAR\nbased on the type of input data modality. Specifically, we review the current\nmainstream deep learning methods for single data modalities and multiple data\nmodalities, including the fusion-based and the co-learning-based frameworks. We\nalso present comparative results on several benchmark datasets for HAR,\ntogether with insightful observations and inspiring future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zehua Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Warping of Radar Data into Camera Image for Cross-Modal Supervision in Automotive Applications. (arXiv:2012.12809v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.12809","description":"<p>We present an approach to automatically generate semantic labels for real\nrecordings of automotive range-Doppler (RD) radar spectra. Such labels are\nrequired when training a neural network for object recognition from radar data.\nThe automatic labeling approach rests on the simultaneous recording of camera\nand lidar data in addition to the radar spectrum. By warping radar spectra into\nthe camera image, state-of-the-art object recognition algorithms can be applied\nto label relevant objects, such as cars, in the camera image. The warping\noperation is designed to be fully differentiable, which allows backpropagating\nthe gradient computed on the camera image through the warping operation to the\nneural network operating on the radar data. As the warping operation relies on\naccurate scene flow estimation, we further propose a novel scene flow\nestimation algorithm which exploits information from camera, lidar and radar\nsensors. The proposed scene flow estimation approach is compared against a\nstate-of-the-art scene flow algorithm, and it outperforms it by approximately\n30% w.r.t. mean average error. The feasibility of the overall framework for\nautomatic label generation for RD spectra is verified by evaluating the\nperformance of neural networks trained with the proposed framework for\nDirection-of-Arrival estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grimm_C/0/1/0/all/0/1\">Christopher Grimm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_T/0/1/0/all/0/1\">Tai Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warsitz_E/0/1/0/all/0/1\">Ernst Warsitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhoud_R/0/1/0/all/0/1\">Ridha Farhoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breddermann_T/0/1/0/all/0/1\">Tobias Breddermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeb_Umbach_R/0/1/0/all/0/1\">Reinhold Haeb-Umbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspectral Image Denoising via Multi-modal and Double-weighted Tensor Nuclear Norm. (arXiv:2101.07681v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07681","description":"<p>Hyperspectral images (HSIs) usually suffer from different types of pollution.\nThis severely reduces the quality of HSIs and limits the accuracy of subsequent\nprocessing tasks. HSI denoising can be modeled as a low-rank tensor denoising\nproblem. Tensor nuclear norm (TNN) induced by tensor singular value\ndecomposition plays an important role in this problem. In this letter, we first\nreconsider three inconspicuous but crucial phenomenons in TNN. In the Fourier\ntransform domain of HSIs, different frequency slices (FS) contain different\ninformation; different singular values (SVs) of each FS also represent\ndifferent information. The two physical phenomenons lie not only in the\nspectral mode but also in the spatial modes. Then based on them, we propose a\nmulti-modal and double-weighted TNN. It can adaptively shrink the FS and SVs\naccording to their physical meanings in all modes of HSIs. In the framework of\nthe alternating direction method of multipliers, we design an effective\nalternating iterative strategy to optimize our proposed model. Denoised\nexperiments on both synthetic and real HSI datasets demonstrate their\nsuperiority against related methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaozhen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Wenfeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Continual, Online, Self-Supervised Depth. (arXiv:2103.00369v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00369","description":"<p>Although depth extraction with passive sensors has seen remarkable\nimprovement with deep learning, these approaches may fail to obtain correct\ndepth if they are exposed to environments not observed during training. Online\nadaptation, where the neural network trains while deployed, with\nself-supervised learning provides a convenient solution as the network can\nlearn from the scene where it is deployed without external supervision.\nHowever, online adaptation causes a neural network to forget the past. Thus,\npast training is wasted and the network is not able to provide good results if\nit observes past scenes. This work deals with practical online-adaptation where\nthe input is online and temporally-correlated, and training is completely\nself-supervised. Regularization and replay-based methods without task\nboundaries are proposed to avoid catastrophic forgetting while adapting to\nonline data. Effort has been made to make the proposed approach suitable for\npractical use. We apply our method to both structure-from-motion and stereo\ndepth estimation. We evaluate our method on diverse public datasets that\ninclude outdoor, indoor and synthetic scenes. Qualitative and quantitative\nresults with both structure-from-motion and stereo show superior forgetting as\nwell as adaptation performance compared to recent methods. Furthermore, the\nproposed method incurs negligible overhead compared to fine-tuning for online\nadaptation, proving to be an adequate choice in terms of plasticity, stability\nand applicability. The proposed approach is more inline with the artificial\ngeneral intelligence paradigm as the neural network learns continually with no\nsupervision. Source code is available at https://github.com/umarKarim/cou_sfm\nand https://github.com/umarKarim/cou_stereo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Umar Karim Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANav: Efficient Terrain Segmentation for Robot Navigation in Unstructured Outdoor Environments. (arXiv:2103.04233v5 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.04233","description":"<p>We propose GANav, a novel group-wise attention mechanism to identify safe and\nnavigable regions in off-road terrains and unstructured environments from RGB\nimages. Our approach classifies terrains based on their navigability levels\nusing coarse-grained semantic segmentation. Our novel group-wise attention loss\nenables any backbone network to explicitly focus on the different groups'\nfeatures with low spatial resolution. Our design leads to efficient inference\nwhile maintaining a high level of accuracy compared to existing SOTA methods.\nOur extensive evaluations on the RUGD and RELLIS-3D datasets shows that GANav\nachieves an improvement over the SOTA mIoU by 2.25-39.05% on RUGD and\n5.17-19.06% on RELLIS-3D. We interface GANav with a deep reinforcement\nlearning-based navigation algorithm and highlight its benefits in terms of\nnavigation in real-world unstructured terrains. We integrate our GANav-based\nnavigation algorithm with ClearPath Jackal and Husky robots, and observe an\nincrease of 10% in terms of success rate, 2-47% in terms of selecting the\nsurface with the best navigability and a decrease of 4.6-13.9% in trajectory\nroughness. Further, GANav reduces the false positive rate of forbidden regions\nby 37.79%. Code, videos, and a full technical report are available at\nhttps://gamma.umd.edu/offroad/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1\">Tianrui Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1\">Divya Kothandaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyamoorthy_A/0/1/0/all/0/1\">Adarsh Jagan Sathyamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerakoon_K/0/1/0/all/0/1\">Kasun Weerakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highly Efficient Representation and Active Learning Framework and Its Application to Imbalanced Medical Image Classification. (arXiv:2103.05109v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.05109","description":"<p>We propose a highly data-efficient active learning framework for image\nclassification. Our novel framework combines: (1) unsupervised representation\nlearning of a Convolutional Neural Network and (2) the Gaussian Process (GP)\nmethod, in sequence to achieve highly data and label efficient classifications.\nMoreover, both elements are less sensitive to the prevalent and challenging\nclass imbalance issue, thanks to the (1) feature learned without labels and (2)\nthe Bayesian nature of GP. The GP-provided uncertainty estimates enable active\nlearning by ranking samples based on the uncertainty and selectively labeling\nsamples showing higher uncertainty. We apply this novel combination to the\nseverely imbalanced case of COVID-19 chest X-ray classification and the Nerthus\ncolonoscopy classification. We demonstrate that only . 10% of the labeled data\nis needed to reach the accuracy from training all available labels. We also\napplied our model architecture and proposed framework to a broader class of\ndatasets with expected success.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1\">Heng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hankyu Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Didari_S/0/1/0/all/0/1\">Sima Didari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jae Oh Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangert_P/0/1/0/all/0/1\">Patrick Bangert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Person Extreme Motion Prediction. (arXiv:2105.08825v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08825","description":"<p>Human motion prediction aims to forecast future poses given a sequence of\npast 3D skeletons. While this problem has recently received increasing\nattention, it has mostly been tackled for single humans in isolation. In this\npaper, we explore this problem when dealing with humans performing\ncollaborative tasks, we seek to predict the future motion of two interacted\npersons given two sequences of their past skeletons. We propose a novel cross\ninteraction attention mechanism that exploits historical information of both\npersons, and learns to predict cross dependencies between the two pose\nsequences. Since no dataset to train such interactive situations is available,\nwe collected ExPI (Extreme Pose Interaction), a new lab-based person\ninteraction dataset of professional dancers performing Lindy-hop dancing\nactions, which contains 115 sequences with 30K frames annotated with 3D body\nposes and shapes. We thoroughly evaluate our cross interaction network on ExPI\nand show that both in short- and long-term predictions, it consistently\noutperforms state-of-the-art methods for single-person motion prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1\">Xiaoyu Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Vision Transformers. (arXiv:2106.04560v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04560","description":"<p>Attention-based neural networks such as the Vision Transformer (ViT) have\nrecently attained state-of-the-art results on many computer vision benchmarks.\nScale is a primary ingredient in attaining excellent results, therefore,\nunderstanding a model's scaling properties is a key to designing future\ngenerations effectively. While the laws for scaling Transformer language models\nhave been studied, it is unknown how Vision Transformers scale. To address\nthis, we scale ViT models and data, both up and down, and characterize the\nrelationships between error rate, data, and compute. Along the way, we refine\nthe architecture and training of ViT, reducing memory consumption and\nincreasing accuracy of the resulting models. As a result, we successfully train\na ViT model with two billion parameters, which attains a new state-of-the-art\non ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot\ntransfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10\nexamples per class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge distillation: A good teacher is patient and consistent. (arXiv:2106.05237v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05237","description":"<p>There is a growing discrepancy in computer vision between large-scale models\nthat achieve state-of-the-art performance and models that are affordable in\npractical applications. In this paper we address this issue and significantly\nbridge the gap between these two types of models. Throughout our empirical\ninvestigation we do not aim to necessarily propose a new method, but strive to\nidentify a robust and effective recipe for making state-of-the-art large scale\nmodels affordable in practice. We demonstrate that, when performed correctly,\nknowledge distillation can be a powerful tool for reducing the size of large\nmodels without compromising their performance. In particular, we uncover that\nthere are certain implicit design choices, which may drastically affect the\neffectiveness of distillation. Our key contribution is the explicit\nidentification of these design choices, which were not previously articulated\nin the literature. We back up our findings by a comprehensive empirical study,\ndemonstrate compelling results on a wide range of vision datasets and, in\nparticular, obtain a state-of-the-art ResNet-50 model for ImageNet, which\nachieves 82.8% top-1 accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royer_A/0/1/0/all/0/1\">Am&#xe9;lie Royer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1\">Larisa Markeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physion: Evaluating Physical Prediction from Vision in Humans and Machines. (arXiv:2106.08261v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2106.08261","description":"<p>While current vision algorithms excel at many challenging tasks, it is\nunclear how well they understand the physical dynamics of real-world\nenvironments. Here we introduce Physion, a dataset and benchmark for rigorously\nevaluating the ability to predict how physical scenarios will evolve over time.\nOur dataset features realistic simulations of a wide range of physical\nphenomena, including rigid and soft-body collisions, stable multi-object\nconfigurations, rolling, sliding, and projectile motion, thus providing a more\ncomprehensive challenge than previous benchmarks. We used Physion to benchmark\na suite of models varying in their architecture, learning objective,\ninput-output structure, and training data. In parallel, we obtained precise\nmeasurements of human prediction behavior on the same set of scenarios,\nallowing us to directly evaluate how well any model could approximate human\nbehavior. We found that vision algorithms that learn object-centric\nrepresentations generally outperform those that do not, yet still fall far\nshort of human performance. On the other hand, graph neural networks with\ndirect access to physical state information both perform substantially better\nand make predictions that are more similar to those made by humans. These\nresults suggest that extracting physical representations of scenes is the main\nbottleneck to achieving human-level and human-like physical understanding in\nvision algorithms. We have publicly released all data and code to facilitate\nthe use of Physion to benchmark additional models in a fully reproducible\nmanner, enabling systematic evaluation of progress towards vision algorithms\nthat understand physical environments as robustly as people do.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bear_D/0/1/0/all/0/1\">Daniel M. Bear</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Elias Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mrowca_D/0/1/0/all/0/1\">Damian Mrowca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_F/0/1/0/all/0/1\">Felix J. Binder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1\">Hsiao-Yu Fish Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramod_R/0/1/0/all/0/1\">R.T. Pramod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holdaway_C/0/1/0/all/0/1\">Cameron Holdaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Sirui Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fan-Yun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanwisher_N/0/1/0/all/0/1\">Nancy Kanwisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1\">Daniel L.K. Yamins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Judith E. Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-Aware Network: Model Human's Top-Down Visual Perception Mechanism for Crowd Counting. (arXiv:2106.12163v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12163","description":"<p>Background noise and scale variation are common problems that have been long\nrecognized in crowd counting. Humans glance at a crowd image and instantly know\nthe approximate number of human and where they are through attention the crowd\nregions and the congestion degree of crowd regions with a global receptive\nfield. Hence, in this paper, we propose a novel feedback network with\nRegion-Aware block called RANet by modeling humans Top-Down visual perception\nmechanism. Firstly, we introduce a feedback architecture to generate priority\nmaps that provide prior about candidate crowd regions in input images. The\nprior enables the RANet pay more attention to crowd regions. Then we design\nRegion-Aware block that could adaptively encode the contextual information into\ninput images through global receptive field. More specifically, we scan the\nwhole input images and its priority maps in the form of column vector to obtain\na relevance matrix estimating their similarity. The relevance matrix obtained\nwould be utilized to build global relationships between pixels. Our method\noutperforms state-of-the-art crowd counting methods on several public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuehai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Badong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shaoyi Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral image denoising. (arXiv:2106.12489v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.12489","description":"<p>Low-rankness is important in the hyperspectral image (HSI) denoising tasks.\nThe tensor nuclear norm (TNN), defined based on the tensor singular value\ndecomposition, is a state-of-the-art method to describe the low-rankness of\nHSI. However, TNN ignores some physical meanings of HSI in tackling denoising\ntasks, leading to suboptimal denoising performance. In this paper, we propose\nthe multi-modal and frequency-weighted tensor nuclear norm (MFWTNN) and the\nnon-convex MFWTNN for HSI denoising tasks. Firstly, we investigate the physical\nmeaning of frequency slices and reconsider their weights to improve the\nlow-rank representation ability of TNN. Secondly, we consider the correlation\namong two spatial dimensions and the spectral dimension of HSI and combine the\nabove improvements to TNN to propose MFWTNN. Thirdly, we use non-convex\nfunctions to approximate the rank function of the frequency tensor and propose\nthe NonMFWTNN to relax the MFWTNN better. Besides, we adaptively choose bigger\nweights for slices mainly containing noise information and smaller weights for\nslices containing profile information. Finally, we develop the efficient\nalternating direction method of multiplier (ADMM) based algorithm to solve the\nproposed models, and the effectiveness of our models are substantiated in\nsimulated and real HSI datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1\">Xiaozhen Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPA: Object Placement Assessment Dataset. (arXiv:2107.01889v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01889","description":"<p>Image composition aims to generate realistic composite image by inserting an\nobject from one image into another background image, where the placement (e.g.,\nlocation, size, occlusion) of inserted object may be unreasonable, which would\nsignificantly degrade the quality of the composite image. Although some works\nattempted to learn object placement to create realistic composite images, they\ndid not focus on assessing the plausibility of object placement. In this paper,\nwe focus on object placement assessment task, which verifies whether a\ncomposite image is plausible in terms of the object placement. To accomplish\nthis task, we construct the first Object Placement Assessment (OPA) dataset\nconsisting of composite images and their rationality labels. We also propose a\nsimple yet effective baseline for this task. Dataset is available at\nhttps://github.com/bcmi/Object-Placement-Assessment-Dataset-OPA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangtong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.11191","description":"<p>Deep neural network approaches to inverse imaging problems have produced\nimpressive results in the last few years. In this paper, we consider the use of\ngenerative models in a variational regularisation approach to inverse problems.\nThe considered regularisers penalise images that are far from the range of a\ngenerative model that has learned to produce images similar to a training\ndataset. We name this family \\textit{generative regularisers}. The success of\ngenerative regularisers depends on the quality of the generative model and so\nwe propose a set of desired criteria to assess generative models and guide\nfuture research. In our numerical experiments, we evaluate three common\ngenerative models, autoencoders, variational autoencoders and generative\nadversarial networks, against our desired criteria. We also test three\ndifferent generative regularisers on the inverse problems of deblurring,\ndeconvolution, and tomography. We show that restricting solutions of the\ninverse problem to lie exactly in the range of a generative model can give good\nresults but that allowing small deviations from the range of the generator\nproduces more consistent results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duff_M/0/1/0/all/0/1\">Margaret Duff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_N/0/1/0/all/0/1\">Neill D. F. Campbell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehrhardt_M/0/1/0/all/0/1\">Matthias J. Ehrhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Anomaly Discovery From Unlabeled Videos via Normality Advantage and Self-Paced Refinement. (arXiv:2108.01975v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01975","description":"<p>While classic video anomaly detection (VAD) requires labeled normal videos\nfor training, emerging unsupervised VAD (UVAD) aims to discover anomalies\ndirectly from fully unlabeled videos. However, existing UVAD methods still rely\non shallow models to perform detection or initialization, and they are\nevidently inferior to classic VAD methods. This paper proposes a full deep\nneural network (DNN) based solution that can realize highly effective UVAD.\nFirst, we, for the first time, point out that deep reconstruction can be\nsurprisingly effective for UVAD, which inspires us to unveil a property named\n\"normality advantage\", i.e., normal events will enjoy lower reconstruction loss\nwhen DNN learns to reconstruct unlabeled videos. With this property, we propose\nLocalization based Reconstruction (LBR) as a strong UVAD baseline and a solid\nfoundation of our solution. Second, we propose a novel self-paced refinement\n(SPR) scheme, which is synthesized into LBR to conduct UVAD. Unlike ordinary\nself-paced learning that injects more samples in an easy-to-hard manner, the\nproposed SPR scheme gradually drops samples so that suspicious anomalies can be\nremoved from the learning process. In this way, SPR consolidates normality\nadvantage and enables better UVAD in a more proactive way. Finally, we further\ndesign a variant solution that explicitly takes the motion cues into account.\nThe solution evidently enhances the UVAD performance, and it sometimes even\nsurpasses the best classic VAD methods. Experiments show that our solution not\nonly significantly outperforms existing UVAD methods by a wide margin (5% to 9%\nAUROC), but also enables UVAD to catch up with the mainstream performance of\nclassic VAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiping Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chuanfu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chengkun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Shape Modelling with Global Coherence: An Inverse Spectral Approach. (arXiv:2108.02161v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02161","description":"<p>Many natural shapes have most of their characterizing features concentrated\nover a few regions in space. For example, humans and animals have distinctive\nhead shapes, while inorganic objects like chairs and airplanes are made of\nwell-localized functional parts with specific geometric features. Often, these\nfeatures are strongly correlated -- a modification of facial traits in a\nquadruped should induce changes to the body structure. However, in shape\nmodelling applications, these types of edits are among the hardest ones; they\nrequire high precision, but also a global awareness of the entire shape. Even\nin the deep learning era, obtaining manipulable representations that satisfy\nsuch requirements is an open problem posing significant constraints. In this\nwork, we address this problem by defining a data-driven model upon a family of\nlinear operators (variants of the mesh Laplacian), whose spectra capture global\nand local geometric properties of the shape at hand. Modifications to these\nspectra are translated to semantically valid deformations of the corresponding\nsurface. By explicitly decoupling the global from the local surface features,\nour pipeline allows to perform local edits while simultaneously maintaining a\nglobal stylistic coherence. We empirically demonstrate how our learning-based\nmodel generalizes to shape representations not seen at training time, and we\nsystematically analyze different choices of local operators over diverse shape\ncategories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pegoraro_M/0/1/0/all/0/1\">Marco Pegoraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1\">Simone Melzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellani_U/0/1/0/all/0/1\">Umberto Castellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Acquisition and Preparation for Dual-reference Deep Learning of Image Super-Resolution. (arXiv:2108.02348v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.02348","description":"<p>The performance of deep learning based image super-resolution (SR) methods\ndepend on how accurately the paired low and high resolution images for training\ncharacterize the sampling process of real cameras. Low and high resolution\n(LR$\\sim$HR) image pairs synthesized by degradation models (e.g., bicubic\ndownsampling) deviate from those in reality; thus the synthetically-trained\nDCNN SR models work disappointingly when being applied to real-world images. To\naddress this issue, we propose a novel data acquisition process to shoot a\nlarge set of LR$\\sim$HR image pairs using real cameras. The images are\ndisplayed on an ultra-high quality screen and captured at different\nresolutions. The resulting LR$\\sim$HR image pairs can be aligned at very high\nsub-pixel precision by a novel spatial-frequency dual-domain registration\nmethod, and hence they provide more appropriate training data for the learning\ntask of super-resolution. Moreover, the captured HR image and the original\ndigital image offer dual references to strengthen supervised learning.\nExperimental results show that training a super-resolution DCNN by our\nLR$\\sim$HR dataset achieves higher image quality than training it by other\ndatasets in the literature. Moreover, the proposed screen-capturing data\ncollection process can be automated; it can be carried out for any target\ncamera with ease and low cost, offering a practical way of tailoring the\ntraining of a DCNN SR model separately to each of the given cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shu_X/0/1/0/all/0/1\">Xiao Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation. (arXiv:2108.03429v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.03429","description":"<p>The success of neural networks on medical image segmentation tasks typically\nrelies on large labeled datasets for model training. However, acquiring and\nmanually labeling a large medical image set is resource-intensive, expensive,\nand sometimes impractical due to data sharing and privacy issues. To address\nthis challenge, we propose AdvChain, a generic adversarial data augmentation\nframework, aiming at improving both the diversity and effectiveness of training\ndata for medical image segmentation tasks. AdvChain augments data with dynamic\ndata augmentation, generating randomly chained photo-metric and geometric\ntransformations to resemble realistic yet challenging imaging variations to\nexpand training data. By jointly optimizing the data augmentation model and a\nsegmentation network during training, challenging examples are generated to\nenhance network generalizability for the downstream task. The proposed\nadversarial data augmentation does not rely on generative networks and can be\nused as a plug-in module in general segmentation networks. It is\ncomputationally efficient and applicable for both low-shot supervised and\nsemi-supervised learning. We analyze and evaluate the method on two MR image\nsegmentation tasks: cardiac segmentation and prostate segmentation with limited\nlabeled data. Results show that the proposed approach can alleviate the need\nfor labeled data while improving model generalization ability, indicating its\npractical value in medical imaging applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zeju Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_H/0/1/0/all/0/1\">Huaqi Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tarroni_G/0/1/0/all/0/1\">Giacomo Tarroni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid Automated Analysis of Skull Base Tumor Specimens Using Intraoperative Optical Imaging and Artificial Intelligence. (arXiv:2108.03555v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03555","description":"<p>Background: Accurate diagnosis of skull base tumors is essential for\nproviding personalized surgical treatment strategies. Intraoperative diagnosis\ncan be challenging due to tumor diversity and lack of intraoperative pathology\nresources.\n</p>\n<p>Objective: To develop an independent and parallel intraoperative pathology\nworkflow that can provide rapid and accurate skull base tumor diagnoses using\nlabel-free optical imaging and artificial intelligence.\n</p>\n<p>Method: We used a fiber laser-based, label-free, non-consumptive,\nhigh-resolution microscopy method ($&lt;$ 60 sec per 1 $\\times$ 1 mm$^\\text{2}$),\ncalled stimulated Raman histology (SRH), to image a consecutive, multicenter\ncohort of skull base tumor patients. SRH images were then used to train a\nconvolutional neural network (CNN) model using three representation learning\nstrategies: cross-entropy, self-supervised contrastive learning, and supervised\ncontrastive learning. Our trained CNN models were tested on a held-out,\nmulticenter SRH dataset.\n</p>\n<p>Results: SRH was able to image the diagnostic features of both benign and\nmalignant skull base tumors. Of the three representation learning strategies,\nsupervised contrastive learning most effectively learned the distinctive and\ndiagnostic SRH image features for each of the skull base tumor types. In our\nmulticenter testing set, cross-entropy achieved an overall diagnostic accuracy\nof 91.5%, self-supervised contrastive learning 83.9%, and supervised\ncontrastive learning 96.6%. Our trained model was able to identify tumor-normal\nmargins and detect regions of microscopic tumor infiltration in whole-slide SRH\nimages.\n</p>\n<p>Conclusion: SRH with trained artificial intelligence models can provide rapid\nand accurate intraoperative analysis of skull base tumor specimens to inform\nsurgical decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Abhishek Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzey_J/0/1/0/all/0/1\">Joseph Linzey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rushikesh S. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sung Jik Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sudharsan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alber_D/0/1/0/all/0/1\">Daniel Alber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondepudi_A/0/1/0/all/0/1\">Akhil Kondepudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urias_E/0/1/0/all/0/1\">Esteban Urias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandian_B/0/1/0/all/0/1\">Balaji Pandian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Holou_W/0/1/0/all/0/1\">Wajd Al-Holou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_S/0/1/0/all/0/1\">Steve Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">B. Gregory Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heth_J/0/1/0/all/0/1\">Jason Heth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freudiger_C/0/1/0/all/0/1\">Chris Freudiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalsa_S/0/1/0/all/0/1\">Siri Khalsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacione_D/0/1/0/all/0/1\">Donato Pacione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golfinos_J/0/1/0/all/0/1\">John G. Golfinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camelo_Piragua_S/0/1/0/all/0/1\">Sandra Camelo-Piragua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orringer_D/0/1/0/all/0/1\">Daniel A. Orringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollon_T/0/1/0/all/0/1\">Todd Hollon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble CNN and Uncertainty Modeling to Improve Automatic Identification/Segmentation of Multiple Sclerosis Lesions in Magnetic Resonance Imaging. (arXiv:2108.11791v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.11791","description":"<p>To date, several automated strategies for identification/segmentation of\nMultiple Sclerosis (MS) lesions with the use of Magnetic Resonance Imaging\n(MRI) have been presented, but they are outperformed by human experts, from\nwhom they act very differently. This is mainly due to: the ambiguity originated\nby MRI instabilities; peculiar variability of MS; non specificity of MRI\nregarding MS. Physicians partially manage the uncertainty generated by\nambiguity relying on radiological/clinical/anatomical background and\nexperience. To emulate human diagnosis, we present an automated framework for\nidentification/segmentation of MS lesions from MRI based on three pivotal\nconcepts: 1. the modelling of uncertainty; 2. the proposal of two, separately\ntrained, CNN, one optimized for lesions and the other for lesions with respect\nto the environment surrounding them, respectively repeated for axial, coronal\nand sagittal directions; 3. the definition of an ensemble classifier to merge\nthe information collected by different CNN. The proposed framework is trained,\nvalidated and tested on the 2016 MSSEG benchmark public data set from a single\nimaging modality, the FLuid-Attenuated Inversion Recovery (FLAIR). The\ncomparison with the ground-truth and with each of 7 human raters, proves that\nthere is no significant difference between the automated and the human raters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Placidi_G/0/1/0/all/0/1\">Giuseppe Placidi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iacoviello_D/0/1/0/all/0/1\">Daniela Iacoviello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mignosi_F/0/1/0/all/0/1\">Filippo Mignosi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Polsinelli_M/0/1/0/all/0/1\">Matteo Polsinelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication. (arXiv:2109.07644v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07644","description":"<p>Employing Vehicle-to-Vehicle communication to enhance perception performance\nin self-driving technology has attracted considerable attention recently;\nhowever, the absence of a suitable open dataset for benchmarking algorithms has\nmade it difficult to develop and assess cooperative perception technologies. To\nthis end, we present the first large-scale open simulated dataset for\nVehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464\nframes, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns\nin CARLA and a digital town of Culver City, Los Angeles. We then construct a\ncomprehensive benchmark with a total of 16 implemented models to evaluate\nseveral information fusion strategies~(i.e. early, late, and intermediate\nfusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose\na new Attentive Intermediate Fusion pipeline to aggregate information from\nmultiple connected vehicles. Our experiments show that the proposed pipeline\ncan be easily integrated with existing 3D LiDAR detectors and achieve\noutstanding performance even with large compression rates. To encourage more\nresearchers to investigate Vehicle-to-Vehicle perception, we will release the\ndataset, benchmark methods, and all related codes in\nhttps://mobility-lab.seas.ucla.edu/opv2v/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1\">Hao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Hilti SLAM Challenge Dataset. (arXiv:2109.11316v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.11316","description":"<p>Research in Simultaneous Localization and Mapping (SLAM) has made outstanding\nprogress over the past years. SLAM systems are nowadays transitioning from\nacademic to real world applications. However, this transition has posed new\ndemanding challenges in terms of accuracy and robustness. To develop new SLAM\nsystems that can address these challenges, new datasets containing cutting-edge\nhardware and realistic scenarios are required. We propose the Hilti SLAM\nChallenge Dataset. Our dataset contains indoor sequences of offices, labs, and\nconstruction environments and outdoor sequences of construction sites and\nparking areas. All these sequences are characterized by featureless areas and\nvarying illumination conditions that are typical in real-world scenarios and\npose great challenges to SLAM algorithms that have been developed in confined\nlab environments. Accurate sparse ground truth, at millimeter level, is\nprovided for each sequence. The sensor platform used to record the data\nincludes a number of visual, lidar, and inertial sensors, which are spatially\nand temporally calibrated. The purpose of this dataset is to foster the\nresearch in sensor fusion to develop SLAM algorithms that can be deployed in\ntasks where high accuracy and robustness are required, e.g., in construction\nenvironments. Many academic and industrial groups tested their SLAM systems on\nthe proposed dataset in the Hilti SLAM Challenge. The results of the challenge,\nwhich are summarized in this paper, show that the proposed dataset is an\nimportant asset in the development of new SLAM algorithms that are ready to be\ndeployed in the real-world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helmberger_M/0/1/0/all/0/1\">Michael Helmberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_K/0/1/0/all/0/1\">Kristian Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berner_B/0/1/0/all/0/1\">Beda Berner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Nitish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cioffi_G/0/1/0/all/0/1\">Giovanni Cioffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MD Loss: Efficient Training of 3D Seismic Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05319","description":"<p>Data-driven fault detection has been regarded as a 3D image segmentation\ntask. The models trained from synthetic data are difficult to generalize in\nsome surveys. Recently, training 3D fault segmentation using sparse manual 2D\nslices is thought to yield promising results, but manual labeling has many\nfalse negative labels (abnormal annotations), which is detrimental to training\nand consequently to detection performance. Motivated to train 3D fault\nsegmentation networks under sparse 2D labels while suppressing false negative\nlabels, we analyze the training process gradient and propose the Mask Dice (MD)\nloss. Moreover, the fault is an edge feature, and current encoder-decoder\narchitectures widely used for fault detection (e.g., U-shape network) are not\nconducive to edge representation. Consequently, Fault-Net is proposed, which is\ndesigned for the characteristics of faults, employs high-resolution propagation\nfeatures, and embeds MultiScale Compression Fusion block to fuse multi-scale\ninformation, which allows the edge information to be fully preserved during\npropagation and fusion, thus enabling advanced performance via few\ncomputational resources. Experimental demonstrates that MD loss supports the\ninclusion of human experience in training and suppresses false negative labels\ntherein, enabling baseline models to improve performance and generalize to more\nsurveys. Fault-Net is capable to provide a more stable and reliable\ninterpretation of faults, it uses extremely low computational resources and\ninference is significantly faster than other models. Our method indicates\noptimal performance in comparison with several mainstream methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yimin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Timing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shaoquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zongchao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixFace: Improving Face Verification Focusing on Fine-grained Conditions. (arXiv:2111.01717v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.01717","description":"<p>The performance of face recognition has become saturated for public benchmark\ndatasets such as LFW, CFP-FP, and AgeDB, owing to the rapid advances in CNNs.\nHowever, the effects of faces with various fine-grained conditions on FR models\nhave not been investigated because of the absence of such datasets. This paper\nanalyzes their effects in terms of different conditions and loss functions\nusing K-FACE, a recently introduced FR dataset with fine-grained conditions. We\npropose a novel loss function, MixFace, that combines classification and metric\nlosses. The superiority of MixFace in terms of effectiveness and robustness is\ndemonstrated experimentally on various benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Junuk Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sungbin Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joochan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Yongjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seonhoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Heung-Seon Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TorchGeo: Deep Learning With Geospatial Data. (arXiv:2111.08872v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08872","description":"<p>Remotely sensed geospatial data are critical for applications including\nprecision agriculture, urban planning, disaster monitoring and response, and\nclimate change research, among others. Deep learning methods are particularly\npromising for modeling many remote sensing tasks given the success of deep\nneural networks in similar computer vision tasks and the sheer volume of\nremotely sensed imagery available. However, the variance in data collection\nmethods and handling of geospatial metadata make the application of deep\nlearning methodology to remotely sensed data nontrivial. For example, satellite\nimagery often includes additional spectral bands beyond red, green, and blue\nand must be joined to other geospatial data sources that can have differing\ncoordinate systems, bounds, and resolutions. To help realize the potential of\ndeep learning for remote sensing applications, we introduce TorchGeo, a Python\nlibrary for integrating geospatial data into the PyTorch deep learning\necosystem. TorchGeo provides data loaders for a variety of benchmark datasets,\ncomposable datasets for generic geospatial data sources, samplers for\ngeospatial data, and transforms that work with multispectral imagery. TorchGeo\nis also the first library to provide pre-trained models for multispectral\nsatellite imagery (e.g., models that use all bands from the Sentinel-2\nsatellites), allowing for advances in transfer learning on downstream remote\nsensing tasks with limited labeled data. We use TorchGeo to create reproducible\nbenchmark results on existing datasets and benchmark our proposed method for\npreprocessing geospatial imagery on the fly. TorchGeo is open source and\navailable on GitHub: https://github.com/microsoft/torchgeo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stewart_A/0/1/0/all/0/1\">Adam J. Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corley_I/0/1/0/all/0/1\">Isaac A. Corley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan M. Lavista Ferres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Arindam Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Self and Semi-Supervised Methods for Remote Sensing Segmentation Tasks. (arXiv:2111.10079v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10079","description":"<p>Self- and semi-supervised machine learning techniques leverage unlabeled data\nfor improving downstream task performance. These methods are especially\nvaluable for remote sensing tasks where producing labeled ground truth datasets\ncan be prohibitively expensive but there is easy access to a wealth of\nunlabeled imagery. We perform a rigorous evaluation of SimCLR, a\nself-supervised method, and FixMatch, a semi-supervised method, on three remote\nsensing tasks: riverbed segmentation, land cover mapping, and flood mapping. We\nquantify performance improvements on these remote sensing segmentation tasks\nwhen additional imagery outside of the original supervised dataset is made\navailable for training. We also design experiments to test the effectiveness of\nthese techniques when the test set is domain shifted to sample different\ngeographic areas compared to the training and validation sets. We find that\nsuch techniques significantly improve generalization performance when labeled\ndata is limited and there are geographic domain shifts between the training\ndata and the validation/test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_C/0/1/0/all/0/1\">Chaitanya Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shashank Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquarella_V/0/1/0/all/0/1\">Valerie J. Pasquarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulshan_V/0/1/0/all/0/1\">Varun Gulshan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Segment-level Semantics for Online Phase Recognition from Surgical Videos. (arXiv:2111.11044v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11044","description":"<p>Automatic surgical phase recognition plays a vital role in robot-assisted\nsurgeries. Existing methods ignored a pivotal problem that surgical phases\nshould be classified by learning segment-level semantics instead of solely\nrelying on frame-wise information. This paper presents a segment-attentive\nhierarchical consistency network (SAHC) for surgical phase recognition from\nvideos. The key idea is to extract hierarchical high-level semantic-consistent\nsegments and use them to refine the erroneous predictions caused by ambiguous\nframes. To achieve it, we design a temporal hierarchical network to generate\nhierarchical high-level segments. Then, we introduce a hierarchical\nsegment-frame attention module to capture relations between the low-level\nframes and high-level segments. By regularizing the predictions of frames and\ntheir corresponding segments via a consistency loss, the network can generate\nsemantic-consistent segments and then rectify the misclassified predictions\ncaused by ambiguous low-level frames. We validate SAHC on two public surgical\nvideo datasets, i.e., the M2CAI16 challenge dataset and the Cholec80 dataset.\nExperimental results show that our method outperforms previous\nstate-of-the-arts and ablation studies prove the effectiveness of our proposed\nmodules. Our code has been released at: https://github.com/xmed-lab/SAHC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinpeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Distilled Self-Supervised Representation Learning. (arXiv:2111.12958v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12958","description":"<p>State-of-the-art frameworks in self-supervised learning have recently shown\nthat fully utilizing transformer-based models can lead to performance boost\ncompared to conventional CNN models. Striving to maximize the mutual\ninformation of two views of an image, existing works apply a contrastive loss\nto the final representations. Motivated by self-distillation in the supervised\nregime, we further exploit this by allowing the intermediate representations to\nlearn from the final layer via the contrastive loss. Through self-distillation,\nthe intermediate layers are better suited for instance discrimination, making\nthe performance of an early-exited sub-network not much degraded from that of\nthe full network. This renders the pretext task easier also for the final\nlayer, lead to better representations. Our method, Self-Distilled\nSelf-Supervised Learning (SDSSL), outperforms competitive baselines (SimCLR,\nBYOL and MoCo v3) using ViT on various tasks and datasets. In the linear\nevaluation and k-NN protocol, SDSSL not only leads to superior performance in\nthe final layers, but also in most of the lower layers. Furthermore, positive\nand negative alignments are used to explain how representations are formed more\neffectively. Code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jiho Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonhoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kiyoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jangho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13196","description":"<p>The canonical approach to video captioning dictates a caption generation\nmodel to learn from offline-extracted dense video features. These feature\nextractors usually operate on video frames sampled at a fixed frame rate and\nare often trained on image/video understanding tasks, without adaption to video\ncaptioning data. In this work, we present SwinBERT, an end-to-end\ntransformer-based model for video captioning, which takes video frame patches\ndirectly as inputs, and outputs a natural language description. Instead of\nleveraging multiple 2D/3D feature extractors, our method adopts a video\ntransformer to encode spatial-temporal representations that can adapt to\nvariable lengths of video input without dedicated design for different frame\nrates. Based on this model architecture, we show that video captioning can\nbenefit significantly from more densely sampled video frames as opposed to\nprevious successes with sparsely sampled video frames for video-and-language\nunderstanding tasks (e.g., video question answering). Moreover, to avoid the\ninherent redundancy in consecutive video frames, we propose adaptively learning\na sparse attention mask and optimizing it for task-specific performance\nimprovement through better long-range video sequence modeling. Through\nextensive experiments on 5 video captioning datasets, we show that SwinBERT\nachieves across-the-board performance improvements over previous methods, often\nby a large margin. The learned sparse attention masks in addition push the\nlimit to new state of the arts, and can be transferred between different video\nlengths and between different datasets. Code is available at\nhttps://github.com/microsoft/SwinBERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faisal Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yumao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HEAT: Holistic Edge Attention Transformer for Structured Reconstruction. (arXiv:2111.15143v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15143","description":"<p>This paper presents a novel attention-based neural network for structured\nreconstruction, which takes a 2D raster image as an input and reconstructs a\nplanar graph depicting an underlying geometric structure. The approach detects\ncorners and classifies edge candidates between corners in an end-to-end manner.\nOur contribution is a holistic edge classification architecture, which 1)\ninitializes the feature of an edge candidate by a trigonometric positional\nencoding of its end-points; 2) fuses image feature to each edge candidate by\ndeformable attention; 3) employs two weight-sharing Transformer decoders to\nlearn holistic structural patterns over the graph edge candidates; and 4) is\ntrained with a masked learning strategy. The corner detector is a variant of\nthe edge classification architecture, adapted to operate on pixels as corner\ncandidates. We conduct experiments on two structured reconstruction tasks:\noutdoor building architecture and indoor floorplan planar graph reconstruction.\nExtensive qualitative and quantitative evaluations demonstrate the superiority\nof our approach over the state of the art. Code and pre-trained models are\navailable at https://heat-structured-reconstruction.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiacheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1\">Yasutaka Furukawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition. (arXiv:2111.15361v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15361","description":"<p>Cross-Database Micro-Expression Recognition (CDMER) aims to develop the\nMicro-Expression Recognition (MER) methods that satisfy different conditions\n(equipment, subjects, and scenes) in practical application, i.e., the MER\nmethod of strong domain adaption ability. CDMER faces two obstacles: 1) the\nsevere feature distribution gap between the training and test databases and 2)\nthe feature representation bottleneck for micro-expression (ME) such local and\nsubtle facial expressions. To solve these obstacles, this paper proposes a\nnovel Transfer Group Sparse Regression method, namely TGSR, which seeks and\nselects those salient facial regions to 1) promote a more precise measurement\nof the difference between source and target databases by the operation in the\nfeature level to alleviate their difference better, and to 2) improve the\nextracted hand-crafted feature to be more effective and explicable for better\nMER. We use two public micro-expression databases, i.e., CASME II and SMIC, to\nevaluate the proposed TGSR. Experimental results show that TGSR learns the\ndiscriminative feature and outperforms most state-of-the-art\nsubspace-learning-based domain adaption methods for CDMER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xingxun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1\">Yuan Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenming Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiateng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mengting Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07225","description":"<p>The long-tailed class distribution in visual recognition tasks poses great\nchallenges for neural networks on how to handle the biased predictions between\nhead and tail classes, i.e., the model tends to classify tail classes as head\nclasses. While existing research focused on data resampling and loss function\nengineering, in this paper, we take a different perspective: the classification\nmargins. We study the relationship between the margins and logits\n(classification scores) and empirically observe the biased margins and the\nbiased logits are positively correlated. We propose MARC, a simple yet\neffective MARgin Calibration function to dynamically calibrate the biased\nmargins for unbiased logits. We validate MARC through extensive experiments on\ncommon long-tailed benchmarks including CIFAR-LT, ImageNet-LT, Places-LT, and\niNaturalist-LT. Experimental results demonstrate that our MARC achieves\nfavorable results on these benchmarks. In addition, MARC is extremely easy to\nimplement with just three lines of code. We hope this simple method will\nmotivate people to rethink the biased margins and biased logits in long-tailed\nvisual recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pure Noise to the Rescue of Insufficient Data: Improving Imbalanced Classification by Training on Random Noise Images. (arXiv:2112.08810v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08810","description":"<p>Despite remarkable progress on visual recognition tasks, deep neural-nets\nstill struggle to generalize well when training data is scarce or highly\nimbalanced, rendering them extremely vulnerable to real-world examples. In this\npaper, we present a surprisingly simple yet highly effective method to mitigate\nthis limitation: using pure noise images as additional training data. Unlike\nthe common use of additive noise or adversarial noise for data augmentation, we\npropose an entirely different perspective by directly training on pure random\nnoise images. We present a new Distribution-Aware Routing Batch Normalization\nlayer (DAR-BN), which enables training on pure noise images in addition to\nnatural images within the same network. This encourages generalization and\nsuppresses overfitting. Our proposed method significantly improves imbalanced\nclassification performance, obtaining state-of-the-art results on a large\nvariety of long-tailed image classification datasets (CIFAR-10-LT,\nCIFAR-100-LT, ImageNet-LT, Places-LT, and CelebA-5). Furthermore, our method is\nextremely simple and easy to use as a general new augmentation tool (on top of\nexisting augmentations), and can be incorporated in any training scheme. It\ndoes not require any specialized data generation or training procedures, thus\nkeeping training fast and efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zada_S/0/1/0/all/0/1\">Shiran Zada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benou_I/0/1/0/all/0/1\">Itay Benou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Gradient Descent: A Powerful and Principled Evasion Attack Against Neural Networks. (arXiv:2112.14232v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.14232","description":"<p>We propose new, more efficient targeted white-box attacks against deep neural\nnetworks. Our attacks better align with the attacker's goal: (1) tricking a\nmodel to assign higher probability to the target class than to any other class,\nwhile (2) staying within an $\\epsilon$-distance of the attacked input. First,\nwe demonstrate a loss function that explicitly encodes (1) and show that\nAuto-PGD finds more attacks with it. Second, we propose a new attack method,\nConstrained Gradient Descent (CGD), using a refinement of our loss function\nthat captures both (1) and (2). CGD seeks to satisfy both attacker objectives\n-- misclassification and bounded $\\ell_{p}$-norm -- in a principled manner, as\npart of the optimization, instead of via ad hoc post-processing techniques\n(e.g., projection or clipping). We show that CGD is more successful on CIFAR10\n(0.9--4.2%) and ImageNet (8.6--13.6%) than state-of-the-art attacks while\nconsuming less time (11.4--18.8%). Statistical tests confirm that our attack\noutperforms others against leading defenses on different datasets and values of\n$\\epsilon$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiran Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_K/0/1/0/all/0/1\">Keane Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1\">Lujo Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_M/0/1/0/all/0/1\">Michael K. Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharif_M/0/1/0/all/0/1\">Mahmood Sharif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Category Discovery. (arXiv:2201.02609v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02609","description":"<p>In this paper, we consider a highly general image recognition setting\nwherein, given a labelled and unlabelled set of images, the task is to\ncategorize all images in the unlabelled set. Here, the unlabelled images may\ncome from labelled classes or from novel ones. Existing recognition methods are\nnot able to deal with this setting, because they make several restrictive\nassumptions, such as the unlabelled instances only coming from known - or\nunknown - classes, and the number of unknown classes being known a-priori. We\naddress the more unconstrained setting, naming it 'Generalized Category\nDiscovery', and challenge all these assumptions. We first establish strong\nbaselines by taking state-of-the-art algorithms from novel category discovery\nand adapting them for this task. Next, we propose the use of vision\ntransformers with contrastive representation learning for this open-world\nsetting. We then introduce a simple yet effective semi-supervised $k$-means\nmethod to cluster the unlabelled data into seen and unseen classes\nautomatically, substantially outperforming the baselines. Finally, we also\npropose a new approach to estimate the number of classes in the unlabelled\ndata. We thoroughly evaluate our approach on public datasets for generic object\nclassification and on fine-grained datasets, leveraging the recent Semantic\nShift Benchmark suite. Project page at\nhttps://www.robots.ox.ac.uk/~vgg/research/gcd\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaze_S/0/1/0/all/0/1\">Sagar Vaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynaMixer: A Vision MLP Architecture with Dynamic Mixing. (arXiv:2201.12083v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12083","description":"<p>Recently, MLP-like vision models have achieved promising performances on\nmainstream visual recognition tasks. In contrast with vision transformers and\nCNNs, the success of MLP-like models shows that simple information fusion\noperations among tokens and channels can yield a good representation power for\ndeep recognition models. However, existing MLP-like models fuse tokens through\nstatic fusion operations, lacking adaptability to the contents of the tokens to\nbe mixed. Thus, customary information fusion procedures are not effective\nenough. To this end, this paper presents an efficient MLP-like network\narchitecture, dubbed DynaMixer, resorting to dynamic information fusion.\nCritically, we propose a procedure, on which the DynaMixer model relies, to\ndynamically generate mixing matrices by leveraging the contents of all the\ntokens to be mixed. To reduce the time complexity and improve the robustness, a\ndimensionality reduction technique and a multi-segment fusion mechanism are\nadopted. Our proposed DynaMixer model (97M parameters) achieves 84.3\\% top-1\naccuracy on the ImageNet-1K dataset without extra training data, performing\nfavorably against the state-of-the-art vision MLP models. When the number of\nparameters is reduced to 26M, it still achieves 82.7\\% top-1 accuracy,\nsurpassing the existing MLP-like models with a similar capacity. The code is\navailable at \\url{https://github.com/ziyuwwang/DynaMixer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proximal Denoiser for Convergent Plug-and-Play Optimization with Nonconvex Regularization. (arXiv:2201.13256v4 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2201.13256","description":"<p>Plug-and-Play (PnP) methods solve ill-posed inverse problems through\niterative proximal algorithms by replacing a proximal operator by a denoising\noperation. When applied with deep neural network denoisers, these methods have\nshown state-of-the-art visual performance for image restoration problems.\nHowever, their theoretical convergence analysis is still incomplete. Most of\nthe existing convergence results consider nonexpansive denoisers, which is\nnon-realistic, or limit their analysis to strongly convex data-fidelity terms\nin the inverse problem to solve. Recently, it was proposed to train the\ndenoiser as a gradient descent step on a functional parameterized by a deep\nneural network. Using such a denoiser guarantees the convergence of the PnP\nversion of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this\npaper, we show that this gradient denoiser can actually correspond to the\nproximal operator of another scalar function. Given this new result, we exploit\nthe convergence theory of proximal algorithms in the nonconvex setting to\nobtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM\n(Alternating Direction Method of Multipliers). When built on top of a smooth\ngradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target\nstationary points of an explicit functional. These convergence results are\nconfirmed with numerical experiments on deblurring, super-resolution and\ninpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Hurault_S/0/1/0/all/0/1\">Samuel Hurault</a>, <a href=\"http://arxiv.org/find/math/1/au:+Leclaire_A/0/1/0/all/0/1\">Arthur Leclaire</a>, <a href=\"http://arxiv.org/find/math/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification. (arXiv:2202.00580v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.00580","description":"<p>Federated learning (FL) has rapidly risen in popularity due to its promise of\nprivacy and efficiency. Previous works have exposed privacy vulnerabilities in\nthe FL pipeline by recovering user data from gradient updates. However,\nexisting attacks fail to address realistic settings because they either 1)\nrequire toy settings with very small batch sizes, or 2) require unrealistic and\nconspicuous architecture modifications. We introduce a new strategy that\ndramatically elevates existing attacks to operate on batches of arbitrarily\nlarge size, and without architectural modifications. Our model-agnostic\nstrategy only requires modifications to the model parameters sent to the user,\nwhich is a realistic threat model in many scenarios. We demonstrate the\nstrategy in challenging large-scale settings, obtaining high-fidelity data\nextraction in both cross-device and cross-silo federated learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make Some Noise: Reliable and Efficient Single-Step Adversarial Training. (arXiv:2202.01181v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01181","description":"<p>Recently, Wong et al. showed that adversarial training with single-step FGSM\nleads to a characteristic failure mode named catastrophic overfitting (CO), in\nwhich a model becomes suddenly vulnerable to multi-step attacks. They showed\nthat adding a random perturbation prior to FGSM (RS-FGSM) seemed to be\nsufficient to prevent CO. However, Andriushchenko and Flammarion observed that\nRS-FGSM still leads to CO for larger perturbations, and proposed an expensive\nregularizer (GradAlign) to avoid CO. In this work, we methodically revisit the\nrole of noise and clipping in single-step adversarial training. Contrary to\nprevious intuitions, we find that using a stronger noise around the clean\nsample combined with not clipping is highly effective in avoiding CO for large\nperturbation radii. Based on these observations, we then propose Noise-FGSM\n(N-FGSM) that, while providing the benefits of single-step adversarial\ntraining, does not suffer from CO. Empirical analyses on a large suite of\nexperiments show that N-FGSM is able to match or surpass the performance of\nprevious single-step methods while achieving a 3$\\times$ speed-up. Code can be\nfound in https://github.com/pdejorge/N-FGSM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jorge_P/0/1/0/all/0/1\">Pau de Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpi_R/0/1/0/all/0/1\">Riccardo Volpi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1\">Amartya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogez_G/0/1/0/all/0/1\">Gr&#xe9;gory Rogez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Images: Label Noise Transition Matrix Estimation for Tasks with Lower-Quality Features. (arXiv:2202.01273v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01273","description":"<p>The label noise transition matrix, denoting the transition probabilities from\nclean labels to noisy labels, is crucial for designing statistically robust\nsolutions. Existing estimators for noise transition matrices, e.g., using\neither anchor points or clusterability, focus on computer vision tasks that are\nrelatively easier to obtain high-quality representations. We observe that tasks\nwith lower-quality features fail to meet the anchor-point or clusterability\ncondition, due to the coexistence of both uninformative and informative\nrepresentations. To handle this issue, we propose a generic and practical\ninformation-theoretic approach to down-weight the less informative parts of the\nlower-quality features. This improvement is crucial to identifying and\nestimating the label noise transition matrix. The salient technical challenge\nis to compute the relevant information-theoretical metrics using only noisy\nlabels instead of clean ones. We prove that the celebrated $f$-mutual\ninformation measure can often preserve the order when calculated using noisy\nlabels. We then build our transition matrix estimator using this distilled\nversion of features. The necessity and effectiveness of the proposed method are\nalso demonstrated by evaluating the estimation error on a varied set of tabular\ndata and text classification tasks with lower-quality features. Code is\navailable at github.com/UCSC-REAL/BeyondImages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaowei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic defect segmentation by unsupervised anomaly learning. (arXiv:2202.02998v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02998","description":"<p>This paper addresses the problem of defect segmentation in semiconductor\nmanufacturing. The input of our segmentation is a scanning-electron-microscopy\n(SEM) image of the candidate defect region. We train a U-net shape network to\nsegment defects using a dataset of clean background images. The samples of the\ntraining phase are produced automatically such that no manual labeling is\nrequired. To enrich the dataset of clean background samples, we apply defect\nimplant augmentation. To that end, we apply a copy-and-paste of a random image\npatch in the clean specimen. To improve the robustness of the unlabeled data\nscenario, we train the features of the network with unsupervised learning\nmethods and loss functions. Our experiments show that we succeed to segment\nreal defects with high quality, even though our dataset contains no defect\nexamples. Our approach performs accurately also on the problem of supervised\nand labeled defect segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ofir_N/0/1/0/all/0/1\">Nati Ofir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yacobi_R/0/1/0/all/0/1\">Ran Yacobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granoviter_O/0/1/0/all/0/1\">Omer Granoviter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levant_B/0/1/0/all/0/1\">Boris Levant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtalrid_O/0/1/0/all/0/1\">Ore Shtalrid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency-Regularized Region-Growing Network for Semantic Segmentation of Urban Scenes with Point-Level Annotations. (arXiv:2202.03740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03740","description":"<p>Deep learning algorithms have obtained great success in semantic segmentation\nof very high-resolution (VHR) images. Nevertheless, training these models\ngenerally requires a large amount of accurate pixel-wise annotations, which is\nvery laborious and time-consuming to collect. To reduce the annotation burden,\nthis paper proposes a consistency-regularized region-growing network (CRGNet)\nto achieve semantic segmentation of VHR images with point-level annotations.\nThe key idea of CRGNet is to iteratively select unlabeled pixels with high\nconfidence to expand the annotated area from the original sparse points.\nHowever, since there may exist some errors and noises in the expanded\nannotations, directly learning from them may mislead the training of the\nnetwork. To this end, we further propose the consistency regularization\nstrategy, where a base classifier and an expanded classifier are employed.\nSpecifically, the base classifier is supervised by the original sparse\nannotations, while the expanded classifier aims to learn from the expanded\nannotations generated by the base classifier with the region-growing mechanism.\nThe consistency regularization is thereby achieved by minimizing the\ndiscrepancy between the predictions from both the base and the expanded\nclassifiers. We find such a simple regularization strategy is yet very useful\nto control the quality of the region-growing mechanism. Extensive experiments\non two benchmark datasets demonstrate that the proposed CRGNet significantly\noutperforms the existing state-of-the-art methods. Codes and pre-trained models\nare available online (https://github.com/YonghaoXu/CRGNet).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yonghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04533","description":"<p>Emerging Metaverse applications demand reliable, accurate, and photorealistic\nreproductions of human hands to perform sophisticated operations as if in the\nphysical world. While real human hand represents one of the most intricate\ncoordination between bones, muscle, tendon, and skin, state-of-the-art\ntechniques unanimously focus on modeling only the skeleton of the hand. In this\npaper, we present NIMBLE, a novel parametric hand model that includes the\nmissing key components, bringing 3D hand model to a new level of realism. We\nfirst annotate muscles, bones and skins on the recent Magnetic Resonance\nImaging hand (MRI-Hand) dataset and then register a volumetric template hand\nonto individual poses and subjects within the dataset. NIMBLE consists of 20\nbones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin\nmesh. Via iterative shape registration and parameter learning, it further\nproduces shape blend shapes, pose blend shapes, and a joint regressor. We\ndemonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.\nBy enforcing the inner bones and muscles to match anatomic and kinematic rules,\nNIMBLE can animate 3D hands to new poses at unprecedented realism. To model the\nappearance of skin, we further construct a photometric HandStage to acquire\nhigh-quality textures and normal maps to model wrinkles and palm print.\nFinally, NIMBLE also benefits learning-based hand pose and shape estimation by\neither synthesizing rich data or acting directly as a differentiable layer in\nthe inference network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zesong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yingwenqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09061","description":"<p>In the past few years, the emergence of pre-training models has brought\nuni-modal fields such as computer vision (CV) and natural language processing\n(NLP) to a new era. Substantial works have shown they are beneficial for\ndownstream uni-modal tasks and avoid training a new model from scratch. So can\nsuch pre-trained models be applied to multi-modal tasks? Researchers have\nexplored this problem and made significant progress. This paper surveys recent\nadvances and new frontiers in vision-language pre-training (VLP), including\nimage-text and video-text pre-training. To give readers a better overall grasp\nof VLP, we first review its recent advances from five aspects: feature\nextraction, model architecture, pre-training objectives, pre-training datasets,\nand downstream tasks. Then, we summarize the specific VLP models in detail.\nFinally, we discuss the new frontiers in VLP. To the best of our knowledge,\nthis is the first survey on VLP. We hope that this survey can shed light on\nfuture research in the VLP field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointMatch: A Consistency Training Framework for Weakly Supervised Semantic Segmentation of 3D Point Clouds. (arXiv:2202.10705v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10705","description":"<p>Semantic segmentation of point cloud usually relies on dense annotation that\nis exhausting and costly, so it attracts wide attention to investigate\nsolutions for the weakly supervised scheme with only sparse points annotated.\nExisting works start from the given labels and propagate them to highly-related\nbut unlabeled points, with the guidance of data, e.g. intra-point relation.\nHowever, it suffers from (i) the inefficient exploitation of data information,\nand (ii) the strong reliance on labels thus is easily suppressed when given\nmuch fewer annotations. Therefore, we propose a novel framework, PointMatch,\nthat stands on both data and label, by applying consistency regularization to\nsufficiently probe information from data itself and leveraging weak labels as\nassistance at the same time. By doing so, meaningful information can be learned\nfrom both data and label for better representation learning, which also enables\nthe model more robust to the extent of label sparsity. Simple yet effective,\nthe proposed PointMatch achieves the state-of-the-art performance under various\nweakly-supervised schemes on both ScanNet-v2 and S3DIS datasets, especially on\nthe settings with extremely sparse labels, e.g. surpassing SQN by 21.2% and\n17.2% on the 0.01% and 0.1% setting of ScanNet-v2, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yushuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shengcai Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zizheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Calibration for Object Detection and Segmentation. (arXiv:2202.12785v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12785","description":"<p>Calibrated confidence estimates obtained from neural networks are crucial,\nparticularly for safety-critical applications such as autonomous driving or\nmedical image diagnosis. However, although the task of confidence calibration\nhas been investigated on classification problems, thorough investigations on\nobject detection and segmentation problems are still missing. Therefore, we\nfocus on the investigation of confidence calibration for object detection and\nsegmentation models in this chapter. We introduce the concept of multivariate\nconfidence calibration that is an extension of well-known calibration methods\nto the task of object detection and segmentation. This allows for an extended\nconfidence calibration that is also aware of additional features such as\nbounding box/pixel position, shape information, etc. Furthermore, we extend the\nexpected calibration error (ECE) to measure miscalibration of object detection\nand segmentation models. We examine several network architectures on MS COCO as\nwell as on Cityscapes and show that especially object detection as well as\ninstance segmentation models are intrinsically miscalibrated given the\nintroduced definition of calibration. Using our proposed calibration methods,\nwe have been able to improve calibration so that it also has a positive impact\non the quality of segmentation masks as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuppers_F/0/1/0/all/0/1\">Fabian K&#xfc;ppers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haselhoff_A/0/1/0/all/0/1\">Anselm Haselhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronenberger_J/0/1/0/all/0/1\">Jan Kronenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Jonas Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Scene Flow Estimation with 4-D Automotive Radar. (arXiv:2203.01137v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01137","description":"<p>Scene flow allows autonomous vehicles to reason about the arbitrary motion of\nmultiple independent objects which is the key to long-term mobile autonomy.\nWhile estimating the scene flow from LiDAR has progressed recently, it remains\nlargely unknown how to estimate the scene flow from a 4-D radar - an\nincreasingly popular automotive sensor for its robustness against adverse\nweather and lighting conditions. Compared with the LiDAR point clouds, radar\ndata are drastically sparser, noisier and in much lower resolution. Annotated\ndatasets for radar scene flow are also in absence and costly to acquire in the\nreal world. These factors jointly pose the radar scene flow estimation as a\nchallenging problem. This work aims to address the above challenges and\nestimate scene flow from 4-D radar point clouds by leveraging self-supervised\nlearning. A robust scene flow estimation architecture and three novel losses\nare bespoken designed to cope with intractable radar data. Real-world\nexperimental results validate that our method is able to robustly estimate the\nradar scene flow in the wild and effectively supports the downstream task of\nmotion segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fangqiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhijun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yimin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jianning Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural Radiance Fields. (arXiv:2203.01762v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01762","description":"<p>Deep learning has shown great potential for modeling the physical dynamics of\ncomplex particle systems such as fluids. Existing approaches, however, require\nthe supervision of consecutive particle properties, including positions and\nvelocities. In this paper, we consider a partially observable scenario known as\nfluid dynamics grounding, that is, inferring the state transitions and\ninteractions within the fluid particle systems from sequential visual\nobservations of the fluid surface. We propose a differentiable two-stage\nnetwork named NeuroFluid. Our approach consists of (i) a particle-driven neural\nrenderer, which involves fluid physical properties into the volume rendering\nfunction, and (ii) a particle transition model optimized to reduce the\ndifferences between the rendered and the observed images. NeuroFluid provides\nthe first solution to unsupervised learning of particle-based fluid dynamics by\ntraining these two models jointly. It is shown to reasonably estimate the\nunderlying physics of fluids with different initial shapes, viscosity, and\ndensities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shanyan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Huayu Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-centric and memory-guided normality reconstruction for video anomaly detection. (arXiv:2203.03677v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03677","description":"<p>This paper addresses video anomaly detection problem for videosurveillance.\nDue to the inherent rarity and heterogeneity of abnormal events, the problem is\nviewed as a normality modeling strategy, in which our model learns\nobject-centric normal patterns without seeing anomalous samples during\ntraining. The main contributions consist in coupling pretrained object-level\naction features prototypes with a cosine distance-based anomaly estimation\nfunction, therefore extending previous methods by introducing additional\nconstraints to the mainstream reconstruction-based strategy. Our framework\nleverages both appearance and motion information to learn object-level behavior\nand captures prototypical patterns within a memory module. Experiments on\nseveral well-known datasets demonstrate the effectiveness of our method as it\noutperforms current state-of-the-art on most relevant spatio-temporal\nevaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bergaoui_K/0/1/0/all/0/1\">Khalil Bergaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naji_Y/0/1/0/all/0/1\">Yassine Naji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Setkov_A/0/1/0/all/0/1\">Aleksandr Setkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1\">Ang&#xe9;lique Loesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouiffes_M/0/1/0/all/0/1\">Mich&#xe8;le Gouiff&#xe8;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1\">Romaric Audigier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual MLP for Scoring Sport. (arXiv:2203.03990v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03990","description":"<p>Figure skating scoring is a challenging task because it requires judging\nplayers' technical moves as well as coordination with the background music.\nPrior learning-based work cannot solve it well for two reasons: 1) each move in\nfigure skating changes quickly, hence simply applying traditional frame\nsampling will lose a lot of valuable information, especially in a 3-5 minutes\nlasting video, so an extremely long-range representation learning is necessary;\n2) prior methods rarely considered the critical audio-visual relationship in\ntheir models. Thus, we introduce a multimodal MLP architecture, named\nSkating-Mixer. It extends the MLP-Mixer-based framework into a multimodal\nfashion and effectively learns long-term representations through our designed\nmemory recurrent unit (MRU). Aside from the model, we also collected a\nhigh-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8\ntypes of programs with 7 different rating metrics, overtaking other datasets in\nboth quantity and diversity. Experiments show the proposed method outperforms\nSOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In\naddition, we include an analysis applying our method to recent competitions\nthat occurred in Beijing 2022 Winter Olympic Games, proving our method has\nstrong robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jingfei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingchen Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1\">Tiantian Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuantai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaviAirway: a Bronchiole-sensitive Deep Learning-based Airway Segmentation Pipeline. (arXiv:2203.04294v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04294","description":"<p>Airway segmentation is essential for chest CT image analysis. However, it\nremains a challenging task because of the intrinsic complex tree-like structure\nand imbalanced sizes of airway branches. Current deep learning-based methods\nfocus on model structure design while the potential of training strategy and\nloss function have not been fully explored. Therefore, we present a simple yet\neffective airway segmentation pipeline, denoted NaviAirway, which finds finer\nbronchioles with a bronchiole-sensitive loss function and a\nhuman-vision-inspired iterative training strategy. Experimental results show\nthat NaviAirway outperforms existing methods, particularly in identification of\nhigher generation bronchioles and robustness to new CT scans. Besides,\nNaviAirway is general. It can be combined with different backbone models and\nsignificantly improve their performance. Moreover, we propose two new metrics\n(Branch Detected and Tree-length Detected) for a more comprehensive and fairer\nevaluation of deep learning-based airway segmentation approaches. NaviAirway\ncan generate airway roadmap for Navigation Bronchoscopy and can also be applied\nto other scenarios when segmenting fine and long tubular structures in\nbiomedical images. The code is publicly available on\nhttps://github.com/AntonotnaWang/NaviAirway.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1\">Andong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tam_T/0/1/0/all/0/1\">Terence Chi Chun Tam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poon_H/0/1/0/all/0/1\">Ho Ming Poon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_K/0/1/0/all/0/1\">Kun-Chang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_W/0/1/0/all/0/1\">Wei-Ning Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks. (arXiv:2203.04466v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.04466","description":"<p>Neural networks tend to achieve better accuracy with training if they are\nlarger -- even if the resulting models are overparameterized. Nevertheless,\ncarefully removing such excess parameters before, during, or after training may\nalso produce models with similar or even improved accuracy. In many cases, that\ncan be curiously achieved by heuristics as simple as removing a percentage of\nthe weights with the smallest absolute value -- even though magnitude is not a\nperfect proxy for weight relevance. With the premise that obtaining\nsignificantly better performance from pruning depends on accounting for the\ncombined effect of removing multiple weights, we revisit one of the classic\napproaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose\na tractable heuristic for solving the combinatorial extension of OBS, in which\nwe select weights for simultaneous removal, as well as a systematic update of\nthe remaining weights. Our selection method outperforms other methods under\nhigh sparsity, and the weight update is advantageous even when combined with\nthe other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_T/0/1/0/all/0/1\">Thiago Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1\">Srikumar Ramalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_S/0/1/0/all/0/1\">Shandian Zhe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Instance Domain Adaptation. (arXiv:2203.05028v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05028","description":"<p>Most existing studies on unsupervised domain adaptation (UDA) assume that\neach domain's training samples come with domain labels (e.g., painting, photo).\nSamples from each domain are assumed to follow the same distribution and the\ndomain labels are exploited to learn domain-invariant features via feature\nalignment. However, such an assumption often does not hold true -- there often\nexist numerous finer-grained domains (e.g., dozens of modern painting styles\nhave been developed, each differing dramatically from those of the classic\nstyles). Therefore, forcing feature distribution alignment across each\nartificially-defined and coarse-grained domain can be ineffective. In this\npaper, we address both single-source and multi-source UDA from a completely\ndifferent perspective, which is to view each instance as a fine domain. Feature\nalignment across domains is thus redundant. Instead, we propose to perform\ndynamic instance domain adaptation (DIDA). Concretely, a dynamic neural network\nwith adaptive convolutional kernels is developed to generate instance-adaptive\nresiduals to adapt domain-agnostic deep features to each individual instance.\nThis enables a shared classifier to be applied to both source and target domain\ndata without relying on any domain annotation. Further, instead of imposing\nintricate feature alignment losses, we adopt a simple semi-supervised learning\nparadigm using only a cross-entropy loss for both labeled source and pseudo\nlabeled target data. Our model, dubbed DIDA-Net, achieves state-of-the-art\nperformance on several commonly used single-source and multi-source UDA\ndatasets including Digits, Office-Home, DomainNet, Digit-Five, and PACS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhongying Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised segmentation of NO2 plumes from individual ships using TROPOMI satellite data. (arXiv:2203.06993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06993","description":"<p>Starting from 2021, the International Maritime Organization significantly\ntightened the $\\text{NO}_\\text{x}$ emission requirements for ships entering the\nBaltic and the North Sea waters. Since all methods currently used for the\nships' compliance monitoring are costly and require proximity to the ship, the\nperformance of global and continuous monitoring of the emission standards'\nfulfillment has been impossible up to now. A promising approach is the use of\nremote sensing with the recently launched TROPOMI/S5P satellite. Due to its\nunprecedentedly high spatial resolution, it allows for the visual distinction\nof $\\text{NO}_\\text{2}$ plumes of individual ships. To successfully deploy a\ncompliance monitoring system that is based on TROPOMI data, an automated\nprocedure for the attribution of $\\text{NO}_\\text{2}$ to individual ships has\nto be developed. However, due to the extremely low signal-to-noise ratio,\ninterference with the signal from other - often stronger - sources, and the\nabsence of ground truth, the task is very challenging.\n</p>\n<p>This is the first study proposing an application of supervised learning for\nthe segmentation of emission plumes produced by individual ships. As such, it\nis the first step towards an automated procedure for global ship compliance\nmonitoring using remote sensing data. To this end, we developed a feature\nconstruction method allowing the application of multivariate models on spatial\ndata. We applied several supervised-learning models and benchmark them towards\nexisting unsupervised solutions of ship-plume segmentation with TROPOMI\nsatellite data. We showed that the proposed approach leads to significant plume\nsegmentation improvement and a high correlation with the theoretically derived\nmeasure of the ship's emission potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurchaba_S/0/1/0/all/0/1\">Solomiia Kurchaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vliet_J/0/1/0/all/0/1\">Jasper van Vliet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_F/0/1/0/all/0/1\">Fons J. Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meulman_J/0/1/0/all/0/1\">Jacqueline J. Meulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veenman_C/0/1/0/all/0/1\">Cor J. Veenman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09550","description":"<p>Few-shot semantic segmentation aims at recognizing the object regions of\nunseen categories with only a few annotated examples as supervision. The key to\nfew-shot segmentation is to establish a robust semantic relationship between\nthe support and query images and to prevent overfitting. In this paper, we\npropose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle\nthe few-shot semantic segmentation problem. In MSHNet, we propose a new\nGenerative Prototype Similarity (GPS), which together with cosine similarity\ncan establish a strong semantic relation between the support and query images.\nThe locally generated prototype similarity based on global feature is logically\ncomplementary to the global cosine similarity based on local feature, and the\nrelationship between the query image and the supported image can be expressed\nmore comprehensively by using the two similarities simultaneously. In addition,\nwe propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge\nmulti-layer, multi-shot and multi-similarity hyperrelational features. MSHNet\nis built on the basis of similarity rather than specific category features,\nwhich can achieve more general unity and effectively reduce overfitting. On two\nbenchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet\nachieves new state-of-the-art performances on 1-shot and 5-shot semantic\nsegmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangwen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhe Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaobing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Miao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianghong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SepTr: Separable Transformer for Audio Spectrogram Processing. (arXiv:2203.09581v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09581","description":"<p>Following the successful application of vision transformers in multiple\ncomputer vision tasks, these models have drawn the attention of the signal\nprocessing community. This is because signals are often represented as\nspectrograms (e.g. through Discrete Fourier Transform) which can be directly\nprovided as input to vision transformers. However, naively applying\ntransformers to spectrograms is suboptimal. Since the axes represent distinct\ndimensions, i.e. frequency and time, we argue that a better approach is to\nseparate the attention dedicated to each axis. To this end, we propose the\nSeparable Transformer (SepTr), an architecture that employs two transformer\nblocks in a sequential manner, the first attending to tokens within the same\ntime interval, and the second attending to tokens within the same frequency\nbin. We conduct experiments on three benchmark data sets, showing that our\nseparable architecture outperforms conventional vision transformers and other\nstate-of-the-art methods. Unlike standard transformers, SepTr linearly scales\nthe number of trainable parameters with the input size, thus having a lower\nmemory footprint. Our code is available as open source at\nhttps://github.com/ristea/septr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GriTS: Grid table similarity metric for table structure recognition. (arXiv:2203.12555v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.12555","description":"<p>In this paper, we propose a new class of metric for table structure\nrecognition (TSR) evaluation, called grid table similarity (GriTS). Unlike\nprior metrics, GriTS evaluates the correctness of a predicted table directly in\nits natural form as a matrix. To create a similarity measure between matrices,\nwe generalize the two-dimensional largest common substructure (2D-LCS) problem,\nwhich is NP-hard, to the 2D most similar substructures (2D-MSS) problem and\npropose a polynomial-time heuristic for solving it. This algorithm produces\nboth an upper and a lower bound on the true similarity between matrices. We\nshow using evaluation on a large real-world dataset that in practice there is\nalmost no difference between these bounds. We compare GriTS to other metrics\nand empirically validate that matrix similarity exhibits more desirable\nbehavior than alternatives for TSR performance evaluation. Finally, GriTS\nunifies all three subtasks of cell topology recognition, cell location\nrecognition, and cell content recognition within the same framework, which\nsimplifies the evaluation and enables more meaningful comparisons across\ndifferent types of TSR approaches. Code will be released at\nhttps://github.com/microsoft/table-transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smock_B/0/1/0/all/0/1\">Brandon Smock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesala_R/0/1/0/all/0/1\">Rohith Pesala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_R/0/1/0/all/0/1\">Robin Abraham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-Guided Domain Adaptation for Face Presentation Attack Detection. (arXiv:2203.14565v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14565","description":"<p>Domain adaptation (DA) or domain generalization (DG) for face presentation\nattack detection (PAD) has attracted attention recently with its robustness\nagainst unseen attack scenarios. Existing DA/DG-based PAD methods, however,\nhave not yet fully explored the domain-specific style information that can\nprovide knowledge regarding attack styles (e.g., materials, background,\nillumination and resolution). In this paper, we introduce a novel Style-Guided\nDomain Adaptation (SGDA) framework for inference-time adaptive PAD.\nSpecifically, Style-Selective Normalization (SSN) is proposed to explore the\ndomain-specific style information within the high-order feature statistics. The\nproposed SSN enables the adaptation of the model to the target domain by\nreducing the style difference between the target and the source domains.\nMoreover, we carefully design Style-Aware Meta-Learning (SAML) to boost the\nadaptation ability, which simulates the inference-time adaptation with style\nselection process on virtual test domain. In contrast to previous domain\nadaptation approaches, our method does not require either additional auxiliary\nmodels (e.g., domain adaptors) or the unlabeled target domain during training,\nwhich makes our method more practical to PAD task. To verify our experiments,\nwe utilize the public datasets: MSU-MFSD, CASIA-FASD, OULU-NPU and Idiap\nREPLAYATTACK. In most assessments, the result demonstrates a notable gap of\nperformance compared to the conventional DA/DG-based PAD methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Eun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_W/0/1/0/all/0/1\">Woo-Jeoung Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1\">Kyungseo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15536","description":"<p>Our goal is to recover the 3D shape and pose of dogs from a single image.\nThis is a challenging task because dogs exhibit a wide range of shapes and\nappearances, and are highly articulated. Recent work has proposed to directly\nregress the SMAL animal model, with additional limb scale parameters, from\nimages. Our method, called BARC (Breed-Augmented Regression using\nClassification), goes beyond prior work in several important ways. First, we\nmodify the SMAL shape space to be more appropriate for representing dog shape.\nBut, even with a better shape model, the problem of regressing dog shape from\nan image is still challenging because we lack paired images with 3D ground\ntruth. To compensate for the lack of paired data, we formulate novel losses\nthat exploit information about dog breeds. In particular, we exploit the fact\nthat dogs of the same breed have similar body shapes. We formulate a novel\nbreed similarity loss consisting of two parts: One term encourages the shape of\ndogs from the same breed to be more similar than dogs of different breeds. The\nsecond one, a breed classification loss, helps to produce recognizable\nbreed-specific shapes. Through ablation studies, we find that our breed losses\nsignificantly improve shape accuracy over a baseline without them. We also\ncompare BARC qualitatively to WLDO with a perceptual study and find that our\napproach produces dogs that are significantly more realistic. This work shows\nthat a-priori information about genetic similarity can help to compensate for\nthe lack of 3D training data. This concept may be applicable to other animal\nspecies or groups of species. Our code is publicly available for research\npurposes at https://barc.is.tue.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rueegg_N/0/1/0/all/0/1\">Nadine Rueegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuffi_S/0/1/0/all/0/1\">Silvia Zuffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Generalization of BasicVSR++ to Video Deblurring and Denoising. (arXiv:2204.05308v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05308","description":"<p>The exploitation of long-term information has been a long-standing problem in\nvideo restoration. The recent BasicVSR and BasicVSR++ have shown remarkable\nperformance in video super-resolution through long-term propagation and\neffective alignment. Their success has led to a question of whether they can be\ntransferred to different video restoration tasks. In this work, we extend\nBasicVSR++ to a generic framework for video restoration tasks. In tasks where\ninputs and outputs possess identical spatial size, the input resolution is\nreduced by strided convolutions to maintain efficiency. With only minimal\nchanges from BasicVSR++, the proposed framework achieves compelling performance\nwith great efficiency in various video restoration tasks including video\ndeblurring and denoising. Notably, BasicVSR++ achieves comparable performance\nto Transformer-based approaches with up to 79% of parameter reduction and 44x\nspeedup. The promising results demonstrate the importance of propagation and\nalignment in video restoration tasks beyond just video super-resolution. Code\nand models are available at https://github.com/ckkelvinchan/BasicVSR_PlusPlus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kelvin C.K. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shangchen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08790","description":"<p>Learning visual representations from natural language supervision has\nrecently shown great promise in a number of pioneering works. In general, these\nlanguage-augmented visual models demonstrate strong transferability to a\nvariety of datasets and tasks. However, it remains challenging to evaluate the\ntransferablity of these models due to the lack of easy-to-use evaluation\ntoolkits and public benchmarks. To tackle this, we build ELEVATER (Evaluation\nof Language-augmented Visual Task-level Transfer), the first benchmark and\ntoolkit for evaluating(pre-trained) language-augmented visual models. ELEVATER\nis composed of three components. (i) Datasets. As downstream evaluation suites,\nit consists of 20 image classification datasets and 35 object detection\ndatasets, each of which is augmented with external knowledge. (ii) Toolkit. An\nautomatic hyper-parameter tuning toolkit is developed to facilitate model\nevaluation on downstream tasks. (iii) Metrics. A variety of evaluation metrics\nare used to measure sample-efficiency (zero-shot and few-shot) and\nparameter-efficiency (linear probing and full model fine-tuning). We publicly\nrelease ELEVATER at https://computer-vision-in-the-wild.github.io/ELEVATER/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Ping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Moment Retrieval from Text Queries via Single Frame Annotation. (arXiv:2204.09409v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09409","description":"<p>Video moment retrieval aims at finding the start and end timestamps of a\nmoment (part of a video) described by a given natural language query. Fully\nsupervised methods need complete temporal boundary annotations to achieve\npromising results, which is costly since the annotator needs to watch the whole\nmoment. Weakly supervised methods only rely on the paired video and query, but\nthe performance is relatively poor. In this paper, we look closer into the\nannotation process and propose a new paradigm called \"glance annotation\". This\nparadigm requires the timestamp of only one single random frame, which we refer\nto as a \"glance\", within the temporal boundary of the fully supervised\ncounterpart. We argue this is beneficial because comparing to weak supervision,\ntrivial cost is added yet more potential in performance is provided. Under the\nglance annotation setting, we propose a method named as Video moment retrieval\nvia Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input\nvideo into clips and contrasts between clips and queries, in which glance\nguided Gaussian distributed weights are assigned to all clips. Our extensive\nexperiments indicate that ViGA achieves better results than the\nstate-of-the-art weakly supervised methods by a large margin, even comparable\nto fully supervised methods in some cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ran Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tianwen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Pai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daskalaki_E/0/1/0/all/0/1\">Elena Daskalaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification. (arXiv:2204.09826v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09826","description":"<p>Recent advances in skeleton-based person re-identification (re-ID) obtain\nimpressive performance via either hand-crafted skeleton descriptors or skeleton\nrepresentation learning with deep learning paradigms. However, they typically\nrequire skeletal pre-modeling and label information for training, which leads\nto limited applicability of these methods. In this paper, we focus on\nunsupervised skeleton-based person re-ID, and present a generic Simple Masked\nContrastive learning (SimMC) framework to learn effective representations from\nunlabeled 3D skeletons for person re-ID. Specifically, to fully exploit\nskeleton features within each skeleton sequence, we first devise a masked\nprototype contrastive learning (MPC) scheme to cluster the most typical\nskeleton features (skeleton prototypes) from different subsequences randomly\nmasked from raw sequences, and contrast the inherent similarity between\nskeleton features and different prototypes to learn discriminative skeleton\nrepresentations without using any label. Then, considering that different\nsubsequences within the same sequence usually enjoy strong correlations due to\nthe nature of motion continuity, we propose the masked intra-sequence\ncontrastive learning (MIC) to capture intra-sequence pattern consistency\nbetween subsequences, so as to encourage learning more effective skeleton\nrepresentations for person re-ID. Extensive experiments validate that the\nproposed SimMC outperforms most state-of-the-art skeleton-based methods. We\nfurther show its scalability and efficiency in enhancing the performance of\nexisting models. Our codes are available at https://github.com/Kali-Hac/SimMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Object Parts for Semantic Segmentation. (arXiv:2204.13101v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13101","description":"<p>Progress in self-supervised learning has brought strong general image\nrepresentation learning methods. Yet so far, it has mostly focused on\nimage-level learning. In turn, tasks such as unsupervised image segmentation\nhave not benefited from this trend as they require spatially-diverse\nrepresentations. However, learning dense representations is challenging, as in\nthe unsupervised context it is not clear how to guide the model to learn\nrepresentations that correspond to various potential object categories. In this\npaper, we argue that self-supervised learning of object parts is a solution to\nthis issue. Object parts are generalizable: they are a priori independent of an\nobject definition, but can be grouped to form objects a posteriori. To this\nend, we leverage the recently proposed Vision Transformer's capability of\nattending to objects and combine it with a spatially dense clustering task for\nfine-tuning the spatial tokens. Our method surpasses the state-of-the-art on\nthree semantic segmentation benchmarks by 17%-3%, showing that our\nrepresentations are versatile under various object definitions. Finally, we\nextend this to fully unsupervised segmentation - which refrains completely from\nusing label information even at test-time - and demonstrate that a simple\nmethod for automatically merging discovered object parts based on community\ndetection yields substantial gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_A/0/1/0/all/0/1\">Adrian Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domino Saliency Metrics: Improving Existing Channel Saliency Metrics with Structural Information. (arXiv:2205.02131v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02131","description":"<p>Channel pruning is used to reduce the number of weights in a Convolutional\nNeural Network (CNN). Channel pruning removes slices of the weight tensor so\nthat the convolution layer remains dense. The removal of these weight slices\nfrom a single layer causes mismatching number of feature maps between layers of\nthe network. A simple solution is to force the number of feature map between\nlayers to match through the removal of weight slices from subsequent layers.\nThis additional constraint becomes more apparent in DNNs with branches where\nmultiple channels need to be pruned together to keep the network dense. Popular\npruning saliency metrics do not factor in the structural dependencies that\narise in DNNs with branches. We propose Domino metrics (built on existing\nchannel saliency metrics) to reflect these structural constraints. We test\nDomino saliency metrics against the baseline channel saliency metrics on\nmultiple networks with branches. Domino saliency metrics improved pruning rates\nin most tested networks and up to 25% in AlexNet on CIFAR-10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1\">Kaveena Persand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Andrew Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1\">David Gregg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReFine: Re-randomization before Fine-tuning for Cross-domain Few-shot Learning. (arXiv:2205.05282v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05282","description":"<p>Cross-domain few-shot learning (CD-FSL), where there are few target samples\nunder extreme differences between source and target domains, has recently\nattracted huge attention. For CD-FSL, recent studies generally have developed\ntransfer learning based approaches that pre-train a neural network on popular\nlabeled source domain datasets and then transfer it to target domain data.\nAlthough the labeled datasets may provide suitable initial parameters for the\ntarget data, the domain difference between the source and target might hinder\nthe fine-tuning on the target domain. This paper proposes a simple yet powerful\nmethod that re-randomizes the parameters fitted on the source domain before\nadapting to the target data. The re-randomization resets source-specific\nparameters of the source pre-trained model and thus facilitates fine-tuning on\nthe target domain, improving few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jaehoon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungnyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Namgyu Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GR-GAN: Gradual Refinement Text-to-image Generation. (arXiv:2205.11273v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11273","description":"<p>A good Text-to-Image model should not only generate high quality images, but\nalso ensure the consistency between the text and the generated image. Previous\nmodels failed to simultaneously fix both sides well. This paper proposes a\nGradual Refinement Generative Adversarial Network (GR-GAN) to alleviates the\nproblem efficiently. A GRG module is designed to generate images from low\nresolution to high resolution with the corresponding text constraints from\ncoarse granularity (sentence) to fine granularity (word) stage by stage, a ITM\nmodule is designed to provide image-text matching losses at both sentence-image\nlevel and word-region level for corresponding stages. We also introduce a new\nmetric Cross-Model Distance (CMD) for simultaneously evaluating image quality\nand image-text consistency. Experimental results show GR-GAN significant\noutperform previous models, and achieve new state-of-the-art on both FID and\nCMD. A detailed analysis demonstrates the efficiency of different generation\nstages in GR-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Assemble Geometric Shapes. (arXiv:2205.11809v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11809","description":"<p>Assembling parts into an object is a combinatorial problem that arises in a\nvariety of contexts in the real world and involves numerous applications in\nscience and engineering. Previous related work tackles limited cases with\nidentical unit parts or jigsaw-style parts of textured shapes, which greatly\nmitigate combinatorial challenges of the problem. In this work, we introduce\nthe more challenging problem of shape assembly, which involves textureless\nfragments of arbitrary shapes with indistinctive junctions, and then propose a\nlearning-based approach to solving it. We demonstrate the effectiveness on\nshape assembly tasks with various scenarios, including the ones with abnormal\nfragments (e.g., missing and distorted), the different number of fragments, and\ndifferent rotation discretization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhwi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungtaek Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyunsoo Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction. (arXiv:2206.00790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00790","description":"<p>Self-supervised learning for computer vision has achieved tremendous progress\nand improved many downstream vision tasks such as image classification,\nsemantic segmentation, and object detection. Among these, generative\nself-supervised vision learning approaches such as MAE and BEiT show promising\nperformance. However, their global masked reconstruction mechanism is\ncomputationally demanding. To address this issue, we propose local masked\nreconstruction (LoMaR), a simple yet effective approach that performs masked\nreconstruction within a small window of 7$\\times$7 patches on a simple\nTransformer encoder, improving the trade-off between efficiency and accuracy\ncompared to global masked reconstruction over the entire image. Extensive\nexperiments show that LoMaR reaches 84.1% top-1 accuracy on ImageNet-1K\nclassification, outperforming MAE by 0.5%. After finetuning the pretrained\nLoMaR on 384$\\times$384 images, it can reach 85.4% top-1 accuracy, surpassing\nMAE by 0.6%. On MS COCO, LoMaR outperforms MAE by 0.5 $\\text{AP}^\\text{box}$ on\nobject detection and 0.5 $\\text{AP}^\\text{mask}$ on instance segmentation.\nLoMaR is especially more computation-efficient on pretraining high-resolution\nimages, e.g., it is 3.1$\\times$ faster than MAE with 0.2% higher classification\naccuracy on pretraining 448$\\times$448 images. This local masked reconstruction\nlearning mechanism can be easily integrated into any other generative\nself-supervised learning approach. Our code is publicly available in\nhttps://github.com/junchen14/LoMaR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Ming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation. (arXiv:2206.01737v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.01737","description":"<p>Convolutional neural networks (CNNs) have achieved remarkable segmentation\naccuracy on benchmark datasets where training and test sets are from the same\ndomain, yet their performance can degrade significantly on unseen domains,\nwhich hinders the deployment of CNNs in many clinical scenarios. Most existing\nworks improve model out-of-domain (OOD) robustness by collecting multi-domain\ndatasets for training, which is expensive and may not always be feasible due to\nprivacy and logistical issues. In this work, we focus on improving model\nrobustness using a single-domain dataset only. We propose a novel data\naugmentation framework called MaxStyle, which maximizes the effectiveness of\nstyle augmentation for model OOD performance. It attaches an auxiliary\nstyle-augmented image decoder to a segmentation network for robust feature\nlearning and data augmentation. Importantly, MaxStyle augments data with\nimproved image style diversity and hardness, by expanding the style space with\nnoise and searching for the worst-case style composition of latent features via\nadversarial training. With extensive experiments on multiple public cardiac and\nprostate MR datasets, we demonstrate that MaxStyle leads to significantly\nimproved out-of-distribution robustness against unseen corruptions as well as\ncommon distribution shifts across multiple, different, unseen sites and unknown\nimage sequences under both low- and high-training data settings. The code can\nbe found at https://github.com/cherise215/MaxStyle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zeju Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinclair_M/0/1/0/all/0/1\">Matt Sinclair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NORPPA: NOvel Ringed seal re-identification by Pelage Pattern Aggregation. (arXiv:2206.02498v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02498","description":"<p>We propose a method for Saimaa ringed seal (Pusa hispida saimensis)\nre-identification. Access to large image volumes through camera trapping and\ncrowdsourcing provides novel possibilities for animal monitoring and\nconservation and calls for automatic methods for analysis, in particular, when\nre-identifying individual animals from the images. The proposed method NOvel\nRinged seal re-identification by Pelage Pattern Aggregation (NORPPA) utilizes\nthe permanent and unique pelage pattern of Saimaa ringed seals and\ncontent-based image retrieval techniques. First, the query image is\npreprocessed, and each seal instance is segmented. Next, the seal's pelage\npattern is extracted using a U-net encoder-decoder based method. Then,\nCNN-based affine invariant features are embedded and aggregated into Fisher\nVectors. Finally, the cosine distance between the Fisher Vectors is used to\nfind the best match from a database of known individuals. We perform extensive\nexperiments of various modifications of the method on a new challenging Saimaa\nringed seals re-identification dataset. The proposed method is shown to produce\nthe best re-identification accuracy on our dataset in comparisons with\nalternative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nepovinnykh_E/0/1/0/all/0/1\">Ekaterina Nepovinnykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelak_I/0/1/0/all/0/1\">Ilia Chelak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eerola_T/0/1/0/all/0/1\">Tuomas Eerola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalviainen_H/0/1/0/all/0/1\">Heikki K&#xe4;lvi&#xe4;inen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency. (arXiv:2206.03820v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03820","description":"<p>Intra-voxel incoherent motion (IVIM) analysis of fetal lungs\nDiffusion-Weighted MRI (DWI) data shows potential in providing quantitative\nimaging bio-markers that reflect, indirectly, diffusion and pseudo-diffusion\nfor non-invasive fetal lung maturation assessment. However, long acquisition\ntimes, due to the large number of different \"b-value\" images required for IVIM\nanalysis, precluded clinical feasibility. We introduce SUPER-IVIM-DC a\ndeep-neural-networks (DNN) approach which couples supervised loss with a\ndata-consistency term to enable IVIM analysis of DWI data acquired with a\nlimited number of b-values. We demonstrated the added-value of SUPER-IVIM-DC\nover both classical and recent DNN approaches for IVIM analysis through\nnumerical simulations, healthy volunteer study, and IVIM analysis of fetal lung\nmaturation from fetal DWI data. Our numerical simulations and healthy volunteer\nstudy show that SUPER-IVIM-DC estimates of the IVIM model parameters from\nlimited DWI data had lower normalized root mean-squared error compared to\nprevious DNN-based approaches. Further, SUPER-IVIM-DC estimates of the\npseudo-diffusion fraction parameter from limited DWI data of fetal lungs\ncorrelate better with gestational age compared to both to classical and\nDNN-based approaches (0.242 vs. -0.079 and 0.239). SUPER-IVIM-DC has the\npotential to reduce the long acquisition times associated with IVIM analysis of\nDWI data and to provide clinically feasible bio-markers for non-invasive fetal\nlung maturity assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korngut_N/0/1/0/all/0/1\">Noam Korngut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rotman_E/0/1/0/all/0/1\">Elad Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afacan_O/0/1/0/all/0/1\">Onur Afacan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurugol_S/0/1/0/all/0/1\">Sila Kurugol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaffrani_Reznikov_Y/0/1/0/all/0/1\">Yael Zaffrani-Reznikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemirovsky_Rotman_S/0/1/0/all/0/1\">Shira Nemirovsky-Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warfield_S/0/1/0/all/0/1\">Simon Warfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-based Object-centric Transformers for Efficient Video Generation. (arXiv:2206.04003v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04003","description":"<p>In this work, we present Patch-based Object-centric Video Transformer (POVT),\na novel region-based video generation architecture that leverages\nobject-centric information to efficiently model temporal dynamics in videos. We\nbuild upon prior work in video prediction via an autoregressive transformer\nover the discrete latent space of compressed videos, with an added modification\nto model object-centric information via bounding boxes. Due to better\ncompressibility of object-centric representations, we can improve training\nefficiency by allowing the model to only access object information for longer\nhorizon temporal information. When evaluated on various difficult\nobject-centric datasets, our method achieves better or equal performance to\nother video generation models, while remaining computationally more efficient\nand scalable. In addition, we show that our method is able to perform\nobject-centric controllability through bounding box manipulation, which may aid\ndownstream tasks such as video editing, or visual planning. Samples are\navailable at https://sites.google.com/view/povt-public\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wilson Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_R/0/1/0/all/0/1\">Ryo Okumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Two-stage Method for Non-extreme Value Salt-and-Pepper Noise Removal. (arXiv:2206.05520v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05520","description":"<p>There are several previous methods based on neural network can have great\nperformance in denoising salt and pepper noise. However, those methods are\nbased on a hypothesis that the value of salt and pepper noise is exactly 0 and\n255. It is not true in the real world. The result of those methods deviate\nsharply when the value is different from 0 and 255. To overcome this weakness,\nour method aims at designing a convolutional neural network to detect the noise\npixels in a wider range of value and then a filter is used to modify pixel\nvalue to 0, which is beneficial for further filtering. Additionally, another\nconvolutional neural network is used to conduct the denoising and restoration\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Renwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">YiKe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TileGen: Tileable, Controllable Material Generation and Capture. (arXiv:2206.05649v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2206.05649","description":"<p>Recent methods (e.g. MaterialGAN) have used unconditional GANs to generate\nper-pixel material maps, or as a prior to reconstruct materials from input\nphotographs. These models can generate varied random material appearance, but\ndo not have any mechanism to constrain the generated material to a specific\ncategory or to control the coarse structure of the generated material, such as\nthe exact brick layout on a brick wall. Furthermore, materials reconstructed\nfrom a single input photo commonly have artifacts and are generally not\ntileable, which limits their use in practical content creation pipelines. We\npropose TileGen, a generative model for SVBRDFs that is specific to a material\ncategory, always tileable, and optionally conditional on a provided input\nstructure pattern. TileGen is a variant of StyleGAN whose architecture is\nmodified to always produce tileable (periodic) material maps. In addition to\nthe standard \"style\" latent code, TileGen can optionally take a condition\nimage, giving a user direct control over the dominant spatial (and optionally\ncolor) features of the material. For example, in brick materials, the user can\nspecify a brick layout and the brick color, or in leather materials, the\nlocations of wrinkles and folds. Our inverse rendering approach can find a\nmaterial perceptually matching a single target photograph by optimization. This\nreconstruction can also be conditional on a user-provided pattern. The\nresulting materials are tileable, can be larger than the target image, and are\neditable by varying the condition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xilong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Milo&#x161; Ha&#x161;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaintre_V/0/1/0/all/0/1\">Valentin Deschaintre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantari_N/0/1/0/all/0/1\">Nima Kalantari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRNet: Decomposition and Reconstruction Network for Remote Physiological Measurement. (arXiv:2206.05687v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2206.05687","description":"<p>Remote photoplethysmography (rPPG) based physiological measurement has great\napplication values in affective computing, non-contact health monitoring,\ntelehealth monitoring, etc, which has become increasingly important especially\nduring the COVID-19 pandemic. Existing methods are generally divided into two\ngroups. The first focuses on mining the subtle blood volume pulse (BVP) signals\nfrom face videos, but seldom explicitly models the noises that dominate face\nvideo content. They are susceptible to the noises and may suffer from poor\ngeneralization ability in unseen scenarios. The second focuses on modeling\nnoisy data directly, resulting in suboptimal performance due to the lack of\nregularity of these severe random noises. In this paper, we propose a\nDecomposition and Reconstruction Network (DRNet) focusing on the modeling of\nphysiological features rather than noisy data. A novel cycle loss is proposed\nto constrain the periodicity of physiological information. Besides, a\nplug-and-play Spatial Attention Block (SAB) is proposed to enhance features\nalong with the spatial location information. Furthermore, an efficient Patch\nCropping (PC) augmentation strategy is proposed to synthesize augmented samples\nwith different noise and features. Extensive experiments on different public\ndatasets as well as the cross-database testing demonstrate the effectiveness of\nour approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuhang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gongping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Featurized Query R-CNN. (arXiv:2206.06258v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06258","description":"<p>The query mechanism introduced in the DETR method is changing the paradigm of\nobject detection and recently there are many query-based methods have obtained\nstrong object detection performance. However, the current query-based detection\npipelines suffer from the following two issues. Firstly, multi-stage decoders\nare required to optimize the randomly initialized object queries, incurring a\nlarge computation burden. Secondly, the queries are fixed after training,\nleading to unsatisfying generalization capability. To remedy the above issues,\nwe present featurized object queries predicted by a query generation network in\nthe well-established Faster R-CNN framework and develop a Featurized Query\nR-CNN. Extensive experiments on the COCO dataset show that our Featurized Query\nR-CNN obtains the best speed-accuracy trade-off among all R-CNN detectors,\nincluding the recent state-of-the-art Sparse R-CNN detector. The code is\navailable at {https://github.com/hustvl/Featurized-QueryRCNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tianheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images. (arXiv:2206.06665v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06665","description":"<p>Developing an AI-assisted gland segmentation method from histology images is\ncritical for automatic cancer diagnosis and prognosis; however, the high cost\nof pixel-level annotations hinders its applications to broader diseases.\nExisting weakly-supervised semantic segmentation methods in computer vision\nachieve degenerative results for gland segmentation, since the characteristics\nand problems of glandular datasets are different from general object datasets.\nWe observe that, unlike natural images, the key problem with histology images\nis the confusion of classes owning to morphological homogeneity and low color\ncontrast among different tissues. To this end, we propose a novel method Online\nEasy Example Mining (OEEM) that encourages the network to focus on credible\nsupervision signals rather than noisy signals, therefore mitigating the\ninfluence of inevitable false predictions in pseudo-masks. According to the\ncharacteristics of glandular datasets, we design a strong framework for gland\nsegmentation. Our results exceed many fully-supervised methods and\nweakly-supervised methods for gland segmentation over 4.4% and 6.04% at mIoU,\nrespectively. Code is available at https://github.com/xmed-lab/OEEM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yiduo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yiwen Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tianqi Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-based Classification Framework for Lung Tissues with Auxiliary Information. (arXiv:2206.06701v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.06701","description":"<p>Interstitial lung diseases are a large group of heterogeneous diseases\ncharacterized by different degrees of alveolitis and pulmonary fibrosis.\nAccurately diagnosing these diseases has significant guiding value for\nformulating treatment plans. Although previous work has produced impressive\nresults in classifying interstitial lung diseases, there is still room for\nimproving the accuracy of these techniques, mainly to enhance automated\ndecision-making. In order to improve the classification precision, our study\nproposes a convolutional neural networks-based framework with auxiliary\ninformation. Firstly, ILD images are added with their medical information by\nre-scaling the original image in Hounsfield Units. Secondly, a modified CNN\nmodel is used to produce a vector of classification probability for each\ntissue. Thirdly, location information of the input image, consisting of the\noccurrence frequencies of different diseases in the CT scans on certain\nlocations, is used to calculate a location weight vector. Finally, the Hadamard\nproduct between two vectors is used to produce a decision vector for the\nprediction. Compared to the state-of-the-art methods, the results using a\npublicly available ILD database show the potential of predicting these using\ndifferent auxiliary information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Huafeng Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_R/0/1/0/all/0/1\">Ruijie Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thiyagalingam_J/0/1/0/all/0/1\">Jeyarajan Thiyagalingam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coenen_F/0/1/0/all/0/1\">Frans Coenen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal. (arXiv:2206.06803v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06803","description":"<p>This work studies the joint rain and haze removal problem. In real-life\nscenarios, rain and haze, two often co-occurring common weather phenomena, can\ngreatly degrade the clarity and quality of the scene images, leading to a\nperformance drop in the visual applications, such as autonomous driving.\nHowever, jointly removing the rain and haze in scene images is ill-posed and\nchallenging, where the existence of haze and rain and the change of atmosphere\nlight, can both degrade the scene information. Current methods focus on the\ncontamination removal part, thus ignoring the restoration of the scene\ninformation affected by the change of atmospheric light. We propose a novel\ndeep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address\nthe aforementioned challenge. The ADU-Net produces both the contamination\nresidual and the scene residual to efficiently remove the rain and haze while\npreserving the fidelity of the scene information. Extensive experiments show\nour work outperforms the existing state-of-the-art methods by a considerable\nmargin in both synthetic data and real-world data benchmarks, including\nRainCityscapes, BID Rain, and SPA-Data. For instance, we improve the\nstate-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data,\nrespectively.\n</p>\n<p>Codes will be made available freely to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yaojun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shengyong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnimeSR: Learning Real-World Super-Resolution Models for Animation Videos. (arXiv:2206.07038v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07038","description":"<p>This paper studies the problem of real-world video super-resolution (VSR) for\nanimation videos, and reveals three key improvements for practical animation\nVSR. First, recent real-world super-resolution approaches typically rely on\ndegradation simulation using basic operators without any learning capability,\nsuch as blur, noise, and compression. In this work, we propose to learn such\nbasic operators from real low-quality animation videos, and incorporate the\nlearned ones into the degradation generation pipeline. Such\nneural-network-based basic operators could help to better capture the\ndistribution of real degradations. Second, a large-scale high-quality animation\nvideo dataset, AVC, is built to facilitate comprehensive training and\nevaluations for animation VSR. Third, we further investigate an efficient\nmulti-scale network structure. It takes advantage of the efficiency of\nunidirectional recurrent networks and the effectiveness of sliding-window-based\nmethods. Thanks to the above delicate designs, our method, AnimeSR, is capable\nof restoring real-world low-quality animation videos effectively and\nefficiently, achieving superior performance to previous state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical Report for Argoverse2 Challenge 2022 -- Motion Forecasting Task. (arXiv:2206.07934v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07934","description":"<p>We propose a motion forecasting model called BANet, which means\nBoundary-Aware Network, and it is a variant of LaneGCN. We believe that it is\nnot enough to use only the lane centerline as input to obtain the embedding\nfeatures of the vector map nodes. The lane centerline can only provide the\ntopology of the lanes, and other elements of the vector map also contain rich\ninformation. For example, the lane boundary can provide traffic rule constraint\ninformation such as whether it is possible to change lanes which is very\nimportant. Therefore, we achieved better performance by encoding more vector\nmap elements in the motion forecasting model.We report our results on the 2022\nArgoverse2 Motion Forecasting challenge and rank 1st on the test leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Honglin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel Importance Matters in Few-Shot Image Classification. (arXiv:2206.08126v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08126","description":"<p>Few-Shot Learning (FSL) requires vision models to quickly adapt to brand-new\nclassification tasks with a shift in task distribution. Understanding the\ndifficulties posed by this task distribution shift is central to FSL. In this\npaper, we show that a simple channel-wise feature transformation may be the key\nto unraveling this secret from a channel perspective. When facing novel\nfew-shot tasks in the test-time datasets, this transformation can greatly\nimprove the generalization ability of learned image representations, while\nbeing agnostic to the choice of training algorithms and datasets. Through an\nin-depth analysis of this transformation, we find that the difficulty of\nrepresentation transfer in FSL stems from the severe channel bias problem of\nimage representations: channels may have different importance in different\ntasks, while convolutional neural networks are likely to be insensitive, or\nrespond incorrectly to such a shift. This points out a core problem of the\ngeneralization ability of modern vision systems and needs further attention in\nthe future. Our code is available at\nhttps://github.com/Frankluox/Channel_Importance_FSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FWD: Real-time Novel View Synthesis with Forward Warping and Depth. (arXiv:2206.08355v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08355","description":"<p>Novel view synthesis (NVS) is a challenging task requiring systems to\ngenerate photorealistic images of scenes from new viewpoints, where both\nquality and speed are important for applications. Previous image-based\nrendering (IBR) methods are fast, but have poor quality when input views are\nsparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give\nimpressive results but are not real-time. In our paper, we propose a\ngeneralizable NVS method with sparse inputs, called FWD, which gives\nhigh-quality synthesis in real-time. With explicit depth and differentiable\nrendering, it achieves competitive results to the SOTA methods with 130-1000x\nspeedup and better perceptual quality. If available, we can seamlessly\nintegrate sensor depth during either training or inference to improve image\nquality while retaining real-time speed. With the growing prevalence of depths\nsensors, we hope that methods making use of depth will become increasingly\nuseful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_A/0/1/0/all/0/1\">Ang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockwell_C/0/1/0/all/0/1\">Chris Rockwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Fourier-based Kernel and Nonlinearity Design for Equivariant Networks on Homogeneous Spaces. (arXiv:2206.08362v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08362","description":"<p>We introduce a unified framework for group equivariant networks on\nhomogeneous spaces derived from a Fourier perspective. We consider\ntensor-valued feature fields, before and after a convolutional layer. We\npresent a unified derivation of kernels via the Fourier domain by leveraging\nthe sparsity of Fourier coefficients of the lifted feature fields. The sparsity\nemerges when the stabilizer subgroup of the homogeneous space is a compact Lie\ngroup. We further introduce a nonlinear activation, via an elementwise\nnonlinearity on the regular representation after lifting and projecting back to\nthe field through an equivariant convolution. We show that other methods\ntreating features as the Fourier coefficients in the stabilizer subgroup are\nspecial cases of our activation. Experiments on $SO(3)$ and $SE(3)$ show\nstate-of-the-art performance in spherical vector field regression, point cloud\nclassification, and molecular completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinshuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiahui Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Neural Programs: Variational Learning of Image Grammars and Part-Whole Hierarchies. (arXiv:2206.08462v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08462","description":"<p>Human vision involves parsing and representing objects and scenes using\nstructured representations based on part-whole hierarchies. Computer vision and\nmachine learning researchers have recently sought to emulate this capability\nusing capsule networks, reference frames and active predictive coding, but a\ngenerative model formulation has been lacking. We introduce Recursive Neural\nPrograms (RNPs), which, to our knowledge, is the first neural generative model\nto address the part-whole hierarchy learning problem. RNPs model images as\nhierarchical trees of probabilistic sensory-motor programs that recursively\nreuse learned sensory-motor primitives to model an image within different\nreference frames, forming recursive image grammars. We express RNPs as\nstructured variational autoencoders (sVAEs) for inference and sampling, and\ndemonstrate parts-based parsing, sampling and one-shot transfer learning for\nMNIST, Omniglot and Fashion-MNIST datasets, demonstrating the model's\nexpressive power. Our results show that RNPs provide an intuitive and\nexplainable way of composing objects and scenes, allowing rich compositionality\nand intuitive interpretations of objects in terms of part-whole hierarchies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fisher_A/0/1/0/all/0/1\">Ares Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rajesh P.N. Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAVA: Template-free Animatable Volumetric Actors. (arXiv:2206.08929v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08929","description":"<p>Coordinate-based volumetric representations have the potential to generate\nphoto-realistic virtual avatars from images. However, virtual avatars also need\nto be controllable even to a novel pose that may not have been observed.\nTraditional techniques, such as LBS, provide such a function; yet it usually\nrequires a hand-designed body template, 3D scan data, and limited appearance\nmodels. On the other hand, neural representation has been shown to be powerful\nin representing visual details, but are under explored on deforming dynamic\narticulated actors. In this paper, we propose TAVA, a method to create T\nemplate-free Animatable Volumetric Actors, based on neural representations. We\nrely solely on multi-view data and a tracked skeleton to create a volumetric\nmodel of an actor, which can be animated at the test time given novel pose.\nSince TAVA does not require a body template, it is applicable to humans as well\nas other creatures such as animals. Furthermore, TAVA is designed such that it\ncan recover accurate dense correspondences, making it amenable to\ncontent-creation and editing tasks. Through extensive experiments, we\ndemonstrate that the proposed method generalizes well to novel poses as well as\nunseen views and showcase basic editing capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruilong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanke_J/0/1/0/all/0/1\">Julian Tanke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1\">Michael Zollhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Jurgen Gall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A machine-generated catalogue of Charon's craters and implications for the Kuiper belt. (arXiv:2206.08277v1 [astro-ph.EP] CROSS LISTED)","link":"http://arxiv.org/abs/2206.08277","description":"<p>In this paper we investigate Charon's craters size distribution using a deep\nlearning model. This is motivated by the recent results of Singer et al. (2019)\nwho, using manual cataloging, found a change in the size distribution slope of\ncraters smaller than 12 km in diameter, translating into a paucity of small\nKuiper Belt objects. These results were corroborated by Robbins and Singer\n(2021), but opposed by Morbidelli et al. (2021), necessitating an independent\nreview. Our MaskRCNN-based ensemble of models was trained on Lunar, Mercurian,\nand Martian crater catalogues and both optical and digital elevation images. We\nuse a robust image augmentation scheme to force the model to generalize and\ntransfer-learn into icy objects. With no prior bias or exposure to Charon, our\nmodel find best fit slopes of q =-1.47+-0.33 for craters smaller than 10 km,\nand q =-2.91+-0.51 for craters larger than 15 km. These values indicate a clear\nchange in slope around 15 km as suggested by Singer et al. (2019) and thus\nindependently confirm their conclusions. Our slopes however are both slightly\nflatter than those found more recently by Robbins and Singer (2021). Our\ntrained models and relevant codes are available online on\ngithub.com/malidib/ACID .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Ali_Dib_M/0/1/0/all/0/1\">Mohamad Ali-Dib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}