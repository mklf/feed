{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CaMEL: Mean Teacher Learning for Image Captioning. (arXiv:2202.10492v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10492","description":"<p>Describing images in natural language is a fundamental step towards the\nautomatic modeling of connections between the visual and textual modalities. In\nthis paper we present CaMEL, a novel Transformer-based architecture for image\ncaptioning. Our proposed approach leverages the interaction of two\ninterconnected language models that learn from each other during the training\nphase. The interplay between the two language models follows a mean teacher\nlearning paradigm with knowledge distillation. Experimentally, we assess the\neffectiveness of the proposed solution on the COCO dataset and in conjunction\nwith different visual feature extractors. When comparing with existing\nproposals, we demonstrate that our model provides state-of-the-art caption\nquality with a significantly reduced number of parameters. According to the\nCIDEr metric, we obtain a new state of the art on COCO when training without\nusing external data. The source code and trained models are publicly available\nat: https://github.com/aimagelab/camel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barraco_M/0/1/0/all/0/1\">Manuele Barraco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Base Question Answering by Case-based Reasoning over Subgraphs. (arXiv:2202.10610v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10610","description":"<p>Question answering (QA) over real-world knowledge bases (KBs) is challenging\nbecause of the diverse (essentially unbounded) types of reasoning patterns\nneeded. However, we hypothesize in a large KB, reasoning patterns required to\nanswer a query type reoccur for various entities in their respective subgraph\nneighborhoods. Leveraging this structural similarity between local\nneighborhoods of different subgraphs, we introduce a semiparametric model with\n(i) a nonparametric component that for each query, dynamically retrieves other\nsimilar $k$-nearest neighbor (KNN) training queries along with query-specific\nsubgraphs and (ii) a parametric component that is trained to identify the\n(latent) reasoning patterns from the subgraphs of KNN queries and then apply it\nto the subgraph of the target query. We also propose a novel algorithm to\nselect a query-specific compact subgraph from within the massive knowledge\ngraph (KG), allowing us to scale to full Freebase KG containing billions of\nedges. We show that our model answers queries requiring complex reasoning\npatterns more effectively than existing KG completion algorithms. The proposed\nmodel outperforms or performs competitively with state-of-the-art models on\nseveral KBQA benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rajarshi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godbole_A/0/1/0/all/0/1\">Ameya Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Ankita Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tower_E/0/1/0/all/0/1\">Elliot Tower</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Generating Counterfactuals for Relation Exaction. (arXiv:2202.10668v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10668","description":"<p>The goal of relation extraction (RE) is to extract the semantic relations\nbetween/among entities in the text. As a fundamental task in natural language\nprocessing, it is crucial to ensure the robustness of RE models. Despite the\nhigh accuracy current deep neural models have achieved in RE tasks, they are\neasily affected by spurious correlations. One solution to this problem is to\ntrain the model with counterfactually augmented data (CAD) such that it can\nlearn the causation rather than the confounding. However, no attempt has been\nmade on generating counterfactuals for RE tasks. In this paper, we formulate\nthe problem of automatically generating CAD for RE tasks from an entity-centric\nviewpoint, and develop a novel approach to derive contextual counterfactuals\nfor entities. Specifically, we exploit two elementary topological properties,\ni.e., the centrality and the shortest path, in syntactic and semantic\ndependency graphs, to first identify and then intervene on the contextual\ncausal features for entities. We conduct a comprehensive evaluation on four RE\ndatasets by combining our proposed approach with a variety of backbone RE\nmodels. The results demonstrate that our approach not only improves the\nperformance of the backbones, but also makes them more robust in the\nout-of-domain test.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Constituent Syntax for Coreference Resolution. (arXiv:2202.10710v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10710","description":"<p>Syntax has been shown to benefit Coreference Resolution from incorporating\nlong-range dependencies and structured information captured by syntax trees,\neither in traditional statistical machine learning based systems or recently\nproposed neural models. However, most leading systems use only dependency\ntrees. We argue that constituent trees also encode important information, such\nas explicit span-boundary signals captured by nested multi-word phrases, extra\nlinguistic labels and hierarchical structures useful for detecting anaphora. In\nthis work, we propose a simple yet effective graph-based method to incorporate\nconstituent syntactic structures. Moreover, we also explore to utilise\nhigher-order neighbourhood information to encode rich structures in constituent\ntrees. A novel message propagation mechanism is therefore proposed to enable\ninformation flow among elements in syntax trees. Experiments on the English and\nChinese portions of OntoNotes 5.0 benchmark show that our proposed model either\nbeats a strong baseline or achieves new state-of-the-art performance. (Code is\navailable at https://github.com/Fantabulous-J/Coref-Constituent-Graph)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Fan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Cross-lingual Speech Synthesis with Triplet Training Scheme. (arXiv:2202.10729v1 [cs.SD])","link":"http://arxiv.org/abs/2202.10729","description":"<p>Recent advances in cross-lingual text-to-speech (TTS) made it possible to\nsynthesize speech in a language foreign to a monolingual speaker. However,\nthere is still a large gap between the pronunciation of generated cross-lingual\nspeech and that of native speakers in terms of naturalness and intelligibility.\nIn this paper, a triplet training scheme is proposed to enhance the\ncross-lingual pronunciation by allowing previously unseen content and speaker\ncombinations to be seen during training. Proposed method introduces an extra\nfine-tune stage with triplet loss during training, which efficiently draws the\npronunciation of the synthesized foreign speech closer to those from the native\nanchor speaker, while preserving the non-native speaker's timbre. Experiments\nare conducted based on a state-of-the-art baseline cross-lingual TTS system and\nits enhanced variants. All the objective and subjective evaluations show the\nproposed method brings significant improvement in both intelligibility and\nnaturalness of the synthesized cross-lingual speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianhao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongbin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhiba Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wendi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kaimeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Heng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JAMES: Job Title Mapping with Multi-Aspect Embeddings and Reasoning. (arXiv:2202.10739v1 [cs.AI])","link":"http://arxiv.org/abs/2202.10739","description":"<p>One of the most essential tasks needed for various downstream tasks in career\nanalytics (e.g., career trajectory analysis, job mobility prediction, and job\nrecommendation) is Job Title Mapping (JTM), where the goal is to map\nuser-created (noisy and non-standard) job titles to predefined and standard job\ntitles. However, solving JTM is domain-specific and non-trivial due to its\ninherent challenges: (1) user-created job titles are messy, (2) different job\ntitles often overlap their job requirements, (3) job transition trajectories\nare inconsistent, and (4) the number of job titles in real world applications\nis large-scale. Toward this JTM problem, in this work, we propose a novel\nsolution, named as JAMES, that constructs three unique embeddings of a target\njob title: topological, semantic, and syntactic embeddings, together with\nmulti-aspect co-attention. In addition, we employ logical reasoning\nrepresentations to collaboratively estimate similarities between messy job\ntitles and standard job titles in the reasoning space. We conduct comprehensive\nexperiments against ten competing models on the large-scale real-world dataset\nwith more than 350,000 job titles. Our results show that JAMES significantly\noutperforms the best baseline by 10.06% in Precision@10 and by 17.52% in\nNDCG@10, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_M/0/1/0/all/0/1\">Michiharu Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jia Tracy Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekhtiari_H/0/1/0/all/0/1\">Hamoon Ekhtiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Thanh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CorefDRE: Document-level Relation Extraction with coreference resolution. (arXiv:2202.10744v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10744","description":"<p>Document-level relation extraction is to extract relation facts from a\ndocument consisting of multiple sentences, in which pronoun crossed sentences\nare a ubiquitous phenomenon against a single sentence. However, most of the\nprevious works focus more on mentions coreference resolution except for\npronouns, and rarely pay attention to mention-pronoun coreference and capturing\nthe relations. To represent multi-sentence features by pronouns, we imitate the\nreading process of humans by leveraging coreference information when\ndynamically constructing a heterogeneous graph to enhance semantic information.\nSince the pronoun is notoriously ambiguous in the graph, a mention-pronoun\ncoreference resolution is introduced to calculate the affinity between pronouns\nand corresponding mentions, and the noise suppression mechanism is proposed to\nreduce the noise caused by pronouns. Experiments on the public dataset, DocRED,\nDialogRE and MPDD, show that Coref-aware Doc-level Relation Extraction based on\nGraph Inference Network outperforms the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhongxuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qizhu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Systematic Generalization Through Modularity and Augmentation. (arXiv:2202.10745v1 [cs.AI])","link":"http://arxiv.org/abs/2202.10745","description":"<p>Systematic generalization is the ability to combine known parts into novel\nmeaning; an important aspect of efficient human learning, but a weakness of\nneural network learning. In this work, we investigate how two well-known\nmodeling principles -- modularity and data augmentation -- affect systematic\ngeneralization of neural networks in grounded language learning. We analyze how\nlarge the vocabulary needs to be to achieve systematic generalization and how\nsimilar the augmented data needs to be to the problem at hand. Our findings\nshow that even in the controlled setting of a synthetic benchmark, achieving\nsystematic generalization remains very difficult. After training on an\naugmented dataset with almost forty times more adverbs than the original\nproblem, a non-modular baseline is not able to systematically generalize to a\nnovel combination of a known verb and adverb. When separating the task into\ncognitive processes like perception and navigation, a modular neural network is\nable to utilize the augmented data and generalize more systematically,\nachieving 70% and 40% exact match increase over state-of-the-art on two gSCAN\ntests that have not previously been improved. We hope that this work gives\ninsight into the drivers of systematic generalization, and what we still need\nto improve for neural networks to learn more like humans do.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruis_L/0/1/0/all/0/1\">Laura Ruis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden Lake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VU-BERT: A Unified framework for Visual Dialog. (arXiv:2202.10787v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10787","description":"<p>The visual dialog task attempts to train an agent to answer multi-turn\nquestions given an image, which requires the deep understanding of interactions\nbetween the image and dialog history. Existing researches tend to employ the\nmodality-specific modules to model the interactions, which might be troublesome\nto use. To fill in this gap, we propose a unified framework for image-text\njoint embedding, named VU-BERT, and apply patch projection to obtain vision\nembedding firstly in visual dialog tasks to simplify the model. The model is\ntrained over two tasks: masked language modeling and next utterance retrieval.\nThese tasks help in learning visual concepts, utterances dependence, and the\nrelationships between these two modalities. Finally, our VU-BERT achieves\ncompetitive performance (0.7287 NDCG scores) on VisDial v1.0 Datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NU HLT at CMCL 2022 Shared Task: Multilingual and Crosslingual Prediction of Human Reading Behavior in Universal Language Space. (arXiv:2202.10855v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10855","description":"<p>In this paper, we present a unified model that works for both multilingual\nand crosslingual prediction of reading times of words in various languages. The\nsecret behind the success of this model is in the preprocessing step where all\nwords are transformed to their universal language representation via the\nInternational Phonetic Alphabet (IPA). To the best of our knowledge, this is\nthe first study to favorable exploit this phonological property of language for\nthe two tasks. Various feature types were extracted covering basic frequencies,\nn-grams, information theoretic, and psycholinguistically-motivated predictors\nfor model training. A finetuned Random Forest model obtained best performance\nfor both tasks with 3.8031 and 3.9065 MAE scores for mean first fixation\nduration (FFDAve) and mean total reading time (TRTAve) respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Persian Tokenizers. (arXiv:2202.10879v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10879","description":"<p>Tokenization plays a significant role in the process of lexical analysis.\nTokens become the input for other natural language processing tasks, like\nsemantic parsing and language modeling. Natural Language Processing in Persian\nis challenging due to Persian's exceptional cases, such as half-spaces. Thus,\nit is crucial to have a precise tokenizer for Persian. This article provides a\nnovel work by introducing the most widely used tokenizers for Persian and\ncomparing and evaluating their performance on Persian texts using a simple\nalgorithm with a pre-tagged Persian dependency dataset. After evaluating\ntokenizers with the F1-Score, the hybrid version of the Farsi Verb and Hazm\nwith bounded morphemes fixing showed the best performance with an F1 score of\n98.97%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamali_D/0/1/0/all/0/1\">Danial Kamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janfada_B/0/1/0/all/0/1\">Behrooz Janfada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenasa_M/0/1/0/all/0/1\">Mohammad Ebrahim Shenasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minaei_Bidgoli_B/0/1/0/all/0/1\">Behrouz Minaei-Bidgoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Vision-Language Pre-Trained Models. (arXiv:2202.10936v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10936","description":"<p>As Transformer evolved, pre-trained models have advanced at a breakneck pace\nin recent years. They have dominated the mainstream techniques in natural\nlanguage processing (NLP) and computer vision (CV). How to adapt pre-training\nto the field of Vision-and-Language (V-L) learning and improve the performance\non downstream tasks becomes a focus of multimodal learning. In this paper, we\nreview the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As\nthe core content, we first briefly introduce several ways to encode raw images\nand texts to single-modal embeddings before pre-training. Then, we dive into\nthe mainstream architectures of VL-PTMs in modeling the interaction between\ntext and image representations. We further present widely-used pre-training\ntasks, after which we introduce some common downstream tasks. We finally\nconclude this paper and present some promising research directions. Our survey\naims to provide multimodal researchers a synthesis and pointer to related\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semi-Supervised Learning Approach with Two Teachers to Improve Breakdown Identification in Dialogues. (arXiv:2202.10948v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10948","description":"<p>Identifying breakdowns in ongoing dialogues helps to improve communication\neffectiveness. Most prior work on this topic relies on human annotated data and\ndata augmentation to learn a classification model. While quality labeled\ndialogue data requires human annotation and is usually expensive to obtain,\nunlabeled data is easier to collect from various sources. In this paper, we\npropose a novel semi-supervised teacher-student learning framework to tackle\nthis task. We introduce two teachers which are trained on labeled data and\nperturbed labeled data respectively. We leverage unlabeled data to improve\nclassification in student training where we employ two teachers to refine the\nlabeling of unlabeled data through teacher-student learning in a bootstrapping\nmanner. Through our proposed training approach, the student can achieve\nimprovements over single-teacher performance. Experimental results on the\nDialogue Breakdown Detection Challenge dataset DBDC5 and Learning to Identify\nFollow-Up Questions dataset LIF show that our approach outperforms all previous\npublished approaches as well as other supervised and semi-supervised baseline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Cluster Patterns for Abstractive Summarization. (arXiv:2202.10967v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10967","description":"<p>Nowadays, pre-trained sequence-to-sequence models such as BERTSUM and BART\nhave shown state-of-the-art results in abstractive summarization. In these\nmodels, during fine-tuning, the encoder transforms sentences to context vectors\nin the latent space and the decoder learns the summary generation task based on\nthe context vectors. In our approach, we consider two clusters of salient and\nnon-salient context vectors, using which the decoder can attend more to salient\ncontext vectors for summary generation. For this, we propose a novel clustering\ntransformer layer between the encoder and the decoder, which first generates\ntwo clusters of salient and non-salient vectors, and then normalizes and\nshirinks the clusters to make them apart in the latent space. Our experimental\nresult shows that the proposed model outperforms the existing BART model by\nlearning these distinct cluster patterns, improving up to 4% in ROUGE and 0.3%\nin BERTScore on average in CNN/DailyMail and XSUM data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Sung-Guk Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeong-Jae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+On_B/0/1/0/all/0/1\">Byung-Won On</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical and Spatio-temporal Hand Gesture Features for Sign Language Recognition using the Leap Motion Sensor. (arXiv:2202.11005v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11005","description":"<p>In modern society, people should not be identified based on their disability,\nrather, it is environments that can disable people with impairments.\nImprovements to automatic Sign Language Recognition (SLR) will lead to more\nenabling environments via digital technology. Many state-of-the-art approaches\nto SLR focus on the classification of static hand gestures, but communication\nis a temporal activity, which is reflected by many of the dynamic gestures\npresent. Given this, temporal information during the delivery of a gesture is\nnot often considered within SLR. The experiments in this work consider the\nproblem of SL gesture recognition regarding how dynamic gestures change during\ntheir delivery, and this study aims to explore how single types of features as\nwell as mixed features affect the classification ability of a machine learning\nmodel. 18 common gestures recorded via a Leap Motion Controller sensor provide\na complex classification problem. Two sets of features are extracted from a 0.6\nsecond time window, statistical descriptors and spatio-temporal attributes.\nFeatures from each set are compared by their ANOVA F-Scores and p-values,\narranged into bins grown by 10 features per step to a limit of the 250\nhighest-ranked features. Results show that the best statistical model selected\n240 features and scored 85.96% accuracy, the best spatio-temporal model\nselected 230 features and scored 80.98%, and the best mixed-feature model\nselected 240 features from each set leading to a classification accuracy of\n86.75%. When all three sets of results are compared (146 individual machine\nlearning models), the overall distribution shows that the minimum results are\nincreased when inputs are any number of mixed features compared to any number\nof either of the two single sets of features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1\">Jordan J. Bird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview on Machine Translation Evaluation. (arXiv:2202.11027v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11027","description":"<p>Since the 1950s, machine translation (MT) has become one of the important\ntasks of AI and development, and has experienced several different periods and\nstages of development, including rule-based methods, statistical methods, and\nrecently proposed neural network-based learning methods. Accompanying these\nstaged leaps is the evaluation research and development of MT, especially the\nimportant role of evaluation methods in statistical translation and neural\ntranslation research. The evaluation task of MT is not only to evaluate the\nquality of machine translation, but also to give timely feedback to machine\ntranslation researchers on the problems existing in machine translation itself,\nhow to improve and how to optimise. In some practical application fields, such\nas in the absence of reference translations, the quality estimation of machine\ntranslation plays an important role as an indicator to reveal the credibility\nof automatically translated target languages. This report mainly includes the\nfollowing contents: a brief history of machine translation evaluation (MTE),\nthe classification of research methods on MTE, and the the cutting-edge\nprogress, including human evaluation, automatic evaluation, and evaluation of\nevaluation methods (meta-evaluation). Manual evaluation and automatic\nevaluation include reference-translation based and reference-translation\nindependent participation; automatic evaluation methods include traditional\nn-gram string matching, models applying syntax and semantics, and deep learning\nmodels; evaluation of evaluation methods includes estimating the credibility of\nhuman evaluations, the reliability of the automatic evaluation, the reliability\nof the test set, etc. Advances in cutting-edge evaluation methods include\ntask-based evaluation, using pre-trained language models based on big data, and\nlightweight optimisation models using distillation techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Historical News Collections. (arXiv:2109.03438v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03438","description":"<p>In the last few years, open-domain question answering (ODQA) has advanced\nrapidly due to the development of deep learning techniques and the availability\nof large-scale QA datasets. However, the current datasets are essentially\ndesigned for synchronic document collections (e.g., Wikipedia). Temporal news\ncollections such as long-term news archives spanning several decades, are\nrarely used in training the models despite they are quite valuable for our\nsociety. To foster the research in the field of ODQA on such historical\ncollections, we present ArchivalQA, a large question answering dataset\nconsisting of 532,444 question-answer pairs which is designed for temporal news\nQA. We divide our dataset into four subparts based on the question difficulty\nlevels and the containment of temporal expressions, which we believe are useful\nfor training and testing ODQA systems characterized by different strengths and\nabilities. The novel QA dataset-constructing framework that we introduce can be\nalso applied to generate non-ambiguous questions of good quality over other\ntypes of temporal document collections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masatoshi Yoshikawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting Knowledge from Language Models for Event Extraction. (arXiv:2109.05190v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05190","description":"<p>Eliciting knowledge contained in language models via prompt-based learning\nhas shown great potential in many natural language processing tasks, such as\ntext classification and generation. Whereas, the applications for more complex\ntasks such as event extraction are less studied, since the design of prompt is\nnot straightforward due to the complicated types and arguments. In this paper,\nwe explore to elicit the knowledge from pre-trained language models for event\ntrigger detection and argument extraction. Specifically, we present various\njoint trigger/argument prompt methods, which can elicit more complementary\nknowledge by modeling the interactions between different triggers or arguments.\nThe experimental results on the benchmark dataset, namely ACE2005, show the\ngreat advantages of our proposed approach. In particular, our approach is\nsuperior to the recent advanced methods in the few-shot scenario where only a\nfew samples are used for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaju Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_J/0/1/0/all/0/1\">Jin Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Entity-Centric Questions Challenge Dense Retrievers. (arXiv:2109.08535v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08535","description":"<p>Open-domain question answering has exploded in popularity recently due to the\nsuccess of dense retrieval models, which have surpassed sparse models using\nonly a few supervised training examples. However, in this paper, we demonstrate\ncurrent dense models are not yet the holy grail of retrieval. We first\nconstruct EntityQuestions, a set of simple, entity-rich questions based on\nfacts from Wikidata (e.g., \"Where was Arve Furset born?\"), and observe that\ndense retrievers drastically underperform sparse methods. We investigate this\nissue and uncover that dense retrievers can only generalize to common entities\nunless the question pattern is explicitly observed during training. We discuss\ntwo simple solutions towards addressing this critical problem. First, we\ndemonstrate that data augmentation is unable to fix the generalization problem.\nSecond, we argue a more robust passage encoder helps facilitate better question\nadaptation using specialized question encoders. We hope our work can shed light\non the challenges in creating a robust, universal dense retriever that works\nwell across different input distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sciavolino_C/0/1/0/all/0/1\">Christopher Sciavolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency Structure for News Document Summarization. (arXiv:2109.11199v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11199","description":"<p>In this work, we develop a neural network based model which leverages\ndependency parsing to capture cross-positional dependencies and grammatical\nstructures. With the help of linguistic signals, sentence-level relations can\nbe correctly captured, thus improving news documents summarization performance.\nEmpirical studies demonstrate that this simple but effective method outperforms\nexisting works on the benchmark dataset. Extensive analyses examine different\nsettings and configurations of the proposed model which provide a good\nreference to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Congbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shubham Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingyu Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Have best of both worlds: two-pass hybrid and E2E cascading framework for speech recognition. (arXiv:2110.04891v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04891","description":"<p>Hybrid and end-to-end (E2E) systems have their individual advantages, with\ndifferent error patterns in the speech recognition results. By jointly modeling\naudio and text, the E2E model performs better in matched scenarios and scales\nwell with a large amount of paired audio-text training data. The modularized\nhybrid model is easier for customization, and better to make use of a massive\namount of unpaired text data. This paper proposes a two-pass hybrid and E2E\ncascading (HEC) framework to combine the hybrid and E2E model in order to take\nadvantage of both sides, with hybrid in the first pass and E2E in the second\npass. We show that the proposed system achieves 8-10% relative word error rate\nreduction with respect to each individual system. More importantly, compared\nwith the pure E2E system, we show the proposed system has the potential to keep\nthe advantages of hybrid system, e.g., customization and segmentation\ncapabilities. We also show the second pass E2E model in HEC is robust with\nrespect to the change in the first pass hybrid model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1\">Guoli Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazalov_V/0/1/0/all/0/1\">Vadim Mazalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Non-Autoregressive End-To-End Neural Modeling For English Mispronunciation Detection And Diagnosis. (arXiv:2111.00844v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00844","description":"<p>End-to-end (E2E) neural modeling has emerged as one predominant school of\nthought to develop computer-assisted language training (CAPT) systems, showing\ncompetitive performance to conventional pronunciation-scoring based methods.\nHowever, current E2E neural methods for CAPT are faced with at least two\npivotal challenges. On one hand, most of the E2E methods operate in an\nautoregressive manner with left-to-right beam search to dictate the\npronunciations of an L2 learners. This however leads to very slow inference\nspeed, which inevitably hinders their practical use. On the other hand, E2E\nneural methods are normally data greedy and meanwhile an insufficient amount of\nnonnative training data would often reduce their efficacy on mispronunciation\ndetection and diagnosis (MD&amp;D). In response, we put forward a novel MD&amp;D method\nthat leverages non-autoregressive (NAR) E2E neural modeling to dramatically\nspeed up the inference time while maintaining performance in line with the\nconventional E2E neural methods. In addition, we design and develop a\npronunciation modeling network stacked on top of the NAR E2E models of our\nmethod to further boost the effectiveness of MD&amp;D. Empirical experiments\nconducted on the L2-ARCTIC English dataset seems to validate the feasibility of\nour method, in comparison to some top-of-the-line E2E models and an iconic\npronunciation-scoring based method built on a DNN-HMM acoustic model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bi-Cheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Hsuan-Sheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yung-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Hallucination in Natural Language Generation. (arXiv:2202.03629v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03629","description":"<p>Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of deep learning technologies such as\nTransformer-based language models. This advancement has led to more fluent and\ncoherent natural language generation, naturally leading to development in\ndownstream tasks such as abstractive summarization, dialogue generation and\ndata-to-text generation. However, it is also investigated that such generation\nincludes hallucinated texts, which makes the performances of text generation\nfail to meet users' expectations in many real-world scenarios. In order to\naddress this issue, studies in evaluation and mitigation methods of\nhallucinations have been presented in various tasks, but have not been reviewed\nin a combined manner. In this survey, we provide a broad overview of the\nresearch progress and challenges in the hallucination problem of NLG. The\nsurvey is organized into two big divisions: (i) a general overview of metrics,\nmitigation methods, and future directions; (ii) task-specific research progress\nfor hallucinations in a large set of downstream tasks: abstractive\nsummarization, dialogue generation, generative question answering, data-to-text\ngeneration, and machine translation. This survey could facilitate collaborative\nefforts among researchers in these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geodesic Quantum Walks. (arXiv:2202.10235v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2202.10235","description":"<p>We propose a new family of discrete-spacetime quantum walks capable to\npropagate on any arbitrary triangulations. Moreover we also extend and\ngeneralize the duality principle introduced by one of the authors, linking\ncontinuous local deformations of a given triangulation and the inhomogeneity of\nthe local unitaries that guide the quantum walker. We proved that in the formal\ncontinuous limit, in both space and time, this new family of quantum walks\nconverges to the (1+2)D massless Dirac equation on curved manifolds. We believe\nthat this result has relevance in both modelling/simulating quantum transport\non discrete curved structures, such as fullerene molecules or dynamical causal\ntriangulation, and in addressing fast and efficient optimization problems in\nthe context of the curved space optimization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Molfetta_G/0/1/0/all/0/1\">Giuseppe Di Molfetta</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Deng_V/0/1/0/all/0/1\">Victor Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Classical-Quantum Convolutional Neural Network for Detecting Pneumonia from Chest Radiographs. (arXiv:2202.10452v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10452","description":"<p>While many quantum computing techniques for machine learning have been\nproposed, their performance on real-world datasets remains to be studied. In\nthis paper, we explore how a variational quantum circuit could be integrated\ninto a classical neural network for the problem of detecting pneumonia from\nchest radiographs. We substitute one layer of a classical convolutional neural\nnetwork with a variational quantum circuit to create a hybrid neural network.\nWe train both networks on an image dataset containing chest radiographs and\nbenchmark their performance. To mitigate the influence of different sources of\nrandomness in network training, we sample the results over multiple rounds. We\nshow that the hybrid network outperforms the classical network on different\nperformance measures, and that these improvements are statistically\nsignificant. Our work serves as an experimental demonstration of the potential\nof quantum computing to significantly improve neural network performance for\nreal-world, non-trivial problems relevant to society and industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_V/0/1/0/all/0/1\">Viraj Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawale_S/0/1/0/all/0/1\">Sanjesh Pawale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharat_A/0/1/0/all/0/1\">Amit Kharat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses. (arXiv:2202.10453v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10453","description":"<p>Although media content is increasingly produced, distributed, and consumed in\nmultiple combinations of modalities, how individual modalities contribute to\nthe perceived emotion of a media item remains poorly understood. In this paper\nwe present MusicVideos (MuVi), a novel dataset for affective multimedia content\nanalysis to study how the auditory and visual modalities contribute to the\nperceived emotion of media. The data were collected by presenting music videos\nto participants in three conditions: music, visual, and audiovisual.\nParticipants annotated the music videos for valence and arousal over time, as\nwell as the overall emotion conveyed. We present detailed descriptive\nstatistics for key measures in the dataset and the results of feature\nimportance analyses for each condition. Finally, we propose a novel transfer\nlearning architecture to train Predictive models Augmented with Isolated\nmodality Ratings (PAIR) and demonstrate the potential of isolated modality\nratings for enhancing multimodal emotion recognition. Our results suggest that\nperceptions of arousal are influenced primarily by auditory information, while\nperceptions of valence are more subjective and can be influenced by both visual\nand auditory information. The dataset is made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chua_P/0/1/0/all/0/1\">Phoebe Chua</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Makris_D/0/1/0/all/0/1\">Dimos Makris</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1\">Gemma Roig</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Agres_K/0/1/0/all/0/1\">Kat Agres</a> (4) ((1) Department of Information Systems and Analytics, National University of Singapore, (2) Singapore University of Technology and Design, (3) Goethe University Frankfurt, (4) Yong Siew Toh Conservatory of Music, National University of Singapore)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feasibility Study of Multi-Site Split Learning for Privacy-Preserving Medical Systems under Data Imbalance Constraints in COVID-19, X-Ray, and Cholesterol Dataset. (arXiv:2202.10456v1 [cs.LG])","link":"http://arxiv.org/abs/2202.10456","description":"<p>It seems as though progressively more people are in the race to upload\ncontent, data, and information online; and hospitals haven't neglected this\ntrend either. Hospitals are now at the forefront for multi-site medical data\nsharing to provide groundbreaking advancements in the way health records are\nshared and patients are diagnosed. Sharing of medical data is essential in\nmodern medical research. Yet, as with all data sharing technology, the\nchallenge is to balance improved treatment with protecting patient's personal\ninformation. This paper provides a novel split learning algorithm coined the\nterm, \"multi-site split learning\", which enables a secure transfer of medical\ndata between multiple hospitals without fear of exposing personal data\ncontained in patient records. It also explores the effects of varying the\nnumber of end-systems and the ratio of data-imbalance on the deep learning\nperformance. A guideline for the most optimal configuration of split learning\nthat ensures privacy of patient data whilst achieving performance is\nempirically given. We argue the benefits of our multi-site split learning\nalgorithm, especially regarding the privacy preserving factor, using CT scans\nof COVID-19 patients, X-ray bone scans, and cholesterol level medical data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ha_Y/0/1/0/all/0/1\">Yoo Jeong Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gusang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_M/0/1/0/all/0/1\">Minjae Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Seehwan Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Architecture Slimming Method for Network Pruning and Knowledge Distillation. (arXiv:2202.10461v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10461","description":"<p>Network pruning and knowledge distillation are two widely-known model\ncompression methods that efficiently reduce computation cost and model size. A\ncommon problem in both pruning and distillation is to determine compressed\narchitecture, i.e., the exact number of filters per layer and layer\nconfiguration, in order to preserve most of the original model capacity. In\nspite of the great advances in existing works, the determination of an\nexcellent architecture still requires human interference or tremendous\nexperimentations. In this paper, we propose an architecture slimming method\nthat automates the layer configuration process. We start from the perspective\nthat the capacity of the over-parameterized model can be largely preserved by\nfinding the minimum number of filters preserving the maximum parameter variance\nper layer, resulting in a thin architecture. We formulate the determination of\ncompressed architecture as a one-step orthogonal linear transformation, and\nintegrate principle component analysis (PCA), where the variances of filters in\nthe first several projections are maximized. We demonstrate the rationality of\nour analysis and the effectiveness of the proposed method through extensive\nexperiments. In particular, we show that under the same overall compression\nrate, the compressed architecture determined by our method shows significant\nperformance gain over baselines after pruning and distillation. Surprisingly,\nwe find that the resulting layer-wise compression rates correspond to the layer\nsensitivities found by existing works through tremendous experimentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_Z/0/1/0/all/0/1\">Zhipeng Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weihua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaMEL: Mean Teacher Learning for Image Captioning. (arXiv:2202.10492v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10492","description":"<p>Describing images in natural language is a fundamental step towards the\nautomatic modeling of connections between the visual and textual modalities. In\nthis paper we present CaMEL, a novel Transformer-based architecture for image\ncaptioning. Our proposed approach leverages the interaction of two\ninterconnected language models that learn from each other during the training\nphase. The interplay between the two language models follows a mean teacher\nlearning paradigm with knowledge distillation. Experimentally, we assess the\neffectiveness of the proposed solution on the COCO dataset and in conjunction\nwith different visual feature extractors. When comparing with existing\nproposals, we demonstrate that our model provides state-of-the-art caption\nquality with a significantly reduced number of parameters. According to the\nCIDEr metric, we obtain a new state of the art on COCO when training without\nusing external data. The source code and trained models are publicly available\nat: https://github.com/aimagelab/camel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barraco_M/0/1/0/all/0/1\">Manuele Barraco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Evolutionary Clustering. (arXiv:2202.10505v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10505","description":"<p>Deep clustering outperforms conventional clustering by mutually promoting\nrepresentation learning and cluster assignment. However, most existing deep\nclustering methods suffer from two major drawbacks. First, most cluster\nassignment methods are based on simple distance comparison and highly dependent\non the target distribution generated by a handcrafted nonlinear mapping. These\nfacts largely limit the possible performance that deep clustering methods can\nreach. Second, the clustering results can be easily guided towards wrong\ndirection by the misassigned samples in each cluster. The existing deep\nclustering methods are incapable of discriminating such samples. To address\nthese issues, a novel modular Self-Evolutionary Clustering (Self-EvoC)\nframework is constructed, which boosts the clustering performance by\nclassification in a self-supervised manner. Fuzzy theory is used to score the\nsample membership with probability which evaluates the intermediate clustering\nresult certainty of each sample. Based on which, the most reliable samples can\nbe selected and augmented. The augmented data are employed to fine-tune an\noff-the-shelf deep network classifier with the labels from the clustering,\nwhich results in a model to generate the target distribution. The proposed\nframework can efficiently discriminate sample outliers and generate better\ntarget distribution with the assistance of self-supervised classifier.\nExtensive experiments indicate that the Self-EvoC remarkably outperforms\nstate-of-the-art deep clustering methods on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Na Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qinyang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Implicit Hybrid Gradient Methods with Application to Adversarial Robustness. (arXiv:2202.10523v1 [cs.LG])","link":"http://arxiv.org/abs/2202.10523","description":"<p>Adversarial examples, crafted by adding imperceptible perturbations to\nnatural inputs, can easily fool deep neural networks (DNNs). One of the most\nsuccessful methods for training adversarially robust DNNs is solving a\nnonconvex-nonconcave minimax problem with an adversarial training (AT)\nalgorithm. However, among the many AT algorithms, only Dynamic AT (DAT) and You\nOnly Propagate Once (YOPO) guarantee convergence to a stationary point. In this\nwork, we generalize the stochastic primal-dual hybrid gradient algorithm to\ndevelop semi-implicit hybrid gradient methods (SI-HGs) for finding stationary\npoints of nonconvex-nonconcave minimax problems. SI-HGs have the convergence\nrate $O(1/K)$, which improves upon the rate $O(1/K^{1/2})$ of DAT and YOPO. We\ndevise a practical variant of SI-HGs, and show that it outperforms other AT\nalgorithms in terms of convergence speed and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Junghoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Sampling Rate: Harnessing Frame Coherence in Graphics Applications for Energy-Efficient GPUs. (arXiv:2202.10533v1 [cs.AR])","link":"http://arxiv.org/abs/2202.10533","description":"<p>In real-time rendering, a 3D scene is modelled with meshes of triangles that\nthe GPU projects to the screen. They are discretized by sampling each triangle\nat regular space intervals to generate fragments which are then added texture\nand lighting effects by a shader program. Realistic scenes require detailed\ngeometric models, complex shaders, high-resolution displays and high screen\nrefreshing rates, which all come at a great compute time and energy cost. This\ncost is often dominated by the fragment shader, which runs for each sampled\nfragment. Conventional GPUs sample the triangles once per pixel, however, there\nare many screen regions containing low variation that produce identical\nfragments and could be sampled at lower than pixel-rate with no loss in\nquality. Additionally, as temporal frame coherence makes consecutive frames\nvery similar, such variations are usually maintained from frame to frame. This\nwork proposes Dynamic Sampling Rate (DSR), a novel hardware mechanism to reduce\nredundancy and improve the energy efficiency in graphics applications. DSR\nanalyzes the spatial frequencies of the scene once it has been rendered. Then,\nit leverages the temporal coherence in consecutive frames to decide, for each\nregion of the screen, the lowest sampling rate to employ in the next frame that\nmaintains image quality. We evaluate the performance of a state-of-the-art\nmobile GPU architecture extended with DSR for a wide variety of applications.\nExperimental results show that DSR is able to remove most of the redundancy\ninherent in the color computations at fragment granularity, which brings\naverage speedups of 1.68x and energy savings of 40%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anglada_M/0/1/0/all/0/1\">Mart&#xed; Anglada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_E/0/1/0/all/0/1\">Enrique de Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcerisa_J/0/1/0/all/0/1\">Joan-Manuel Parcerisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragon_J/0/1/0/all/0/1\">Juan L. Arag&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1\">Antonio Gonz&#xe1;lez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReViVD: Exploration and Filtering of Trajectories in an Immersive Environment using 3D Shapes. (arXiv:2202.10545v1 [cs.HC])","link":"http://arxiv.org/abs/2202.10545","description":"<p>We present ReViVD, a tool for exploring and filtering large trajectory-based\ndatasets using virtual reality. ReViVD's novelty lies in using simple 3D shapes\n-- such as cuboids, spheres and cylinders -- as queries for users to select and\nfilter groups of trajectories. Building on this simple paradigm, more complex\nqueries can be created by combining previously made selection groups through a\nsystem of user-created Boolean operations. We demonstrate the use of ReViVD in\ndifferent application domains, from GPS position tracking to simulated data\n(e.g., turbulent particle flows and traffic simulation). Our results show the\nease of use and expressiveness of the 3D geometric shapes in a broad range of\nexploratory tasks. ReViVD was found to be particularly useful for progressively\nrefining selections to isolate outlying behaviors. It also acts as a powerful\ncommunication tool for conveying the structure of normally abstract datasets to\nan audience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Homps_F/0/1/0/all/0/1\">Fran&#xe7;ois Homps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beugin_Y/0/1/0/all/0/1\">Yohan Beugin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuillemot_R/0/1/0/all/0/1\">Romain Vuillemot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Leakage of Adversarial Training Models in Federated Learning Systems. (arXiv:2202.10546v1 [cs.LG])","link":"http://arxiv.org/abs/2202.10546","description":"<p>Adversarial Training (AT) is crucial for obtaining deep neural networks that\nare robust to adversarial attacks, yet recent works found that it could also\nmake models more vulnerable to privacy attacks. In this work, we further reveal\nthis unsettling property of AT by designing a novel privacy attack that is\npractically applicable to the privacy-sensitive Federated Learning (FL)\nsystems. Using our method, the attacker can exploit AT models in the FL system\nto accurately reconstruct users' private training images even when the training\nbatch size is large. Code is available at\nhttps://github.com/zjysteven/PrivayAttack_AT_FL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hai Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guidelines and evaluation for clinical explainable AI on medical image analysis. (arXiv:2202.10553v1 [cs.LG])","link":"http://arxiv.org/abs/2202.10553","description":"<p>Explainable artificial intelligence (XAI) is essential for enabling clinical\nusers to get informed decision support from AI and comply with evidence-based\nmedical practice. Applying XAI in clinical settings requires proper evaluation\ncriteria to ensure the explanation technique is both technically sound and\nclinically useful, but specific support is lacking to achieve this goal. To\nbridge the research gap, we propose the Clinical XAI Guidelines that consist of\nfive criteria a clinical XAI needs to be optimized for. The guidelines\nrecommend choosing an explanation form based on Guideline 1 (G1)\nUnderstandability and G2 Clinical relevance. For the chosen explanation form,\nits specific XAI technique should be optimized for G3 Truthfulness, G4\nInformative plausibility, and G5 Computational efficiency.\n</p>\n<p>Following the guidelines, we conducted a systematic evaluation on a novel\nproblem of multi-modal medical image explanation with two clinical tasks, and\nproposed new evaluation metrics accordingly. The evaluated 16 commonly-used\nheatmap XAI techniques were not suitable for clinical use due to their failure\nin \\textbf{G3} and \\textbf{G4}. Our evaluation demonstrated the use of Clinical\nXAI Guidelines to support the design and evaluation for clinically viable XAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weina Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatehi_M/0/1/0/all/0/1\">Mostafa Fatehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Learning techniques for object detection in high-resolution satellite images. (arXiv:2202.10554v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10554","description":"<p>Ensembling is a method that aims to maximize the detection performance by\nfusing individual detectors. While rarely mentioned in deep-learning articles\napplied to remote sensing, ensembling methods have been widely used to achieve\nhigh scores in recent data science com-petitions, such as Kaggle. The few\nremote sensing articles mentioning ensembling mainly focus on mid resolution\nimages and earth observation applications such as land use classification, but\nnever on Very High Resolution (VHR) images for defense-related applications or\nobject detection.This study aims at reviewing the most relevant ensembling\ntechniques to be used for object detection on very high resolution imagery and\nshows an example of the value of such techniques on a relevant operational\nuse-case (vehicle detection in desert areas).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vilhelm_A/0/1/0/all/0/1\">Arthur Vilhelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Limbert_M/0/1/0/all/0/1\">Matthieu Limbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audebert_C/0/1/0/all/0/1\">Cl&#xe9;ment Audebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceillier_T/0/1/0/all/0/1\">Tugdual Ceillier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Training Strategies for Deep-learning-based Precipitation Nowcasting and Estimation. (arXiv:2202.10555v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10555","description":"<p>Deep learning has been successfully applied to precipitation nowcasting. In\nthis work, we propose a pre-training scheme and a new loss function for\nimproving deep-learning-based nowcasting. First, we adapt U-Net, a widely-used\ndeep-learning model, for the two problems of interest here: precipitation\nnowcasting and precipitation estimation from radar images. We formulate the\nformer as a classification problem with three precipitation intervals and the\nlatter as a regression problem. For these tasks, we propose to pre-train the\nmodel to predict radar images in the near future without requiring ground-truth\nprecipitation, and we also propose the use of a new loss function for\nfine-tuning to mitigate the class imbalance problem. We demonstrate the\neffectiveness of our approach using radar images and precipitation datasets\ncollected from South Korea over seven years. It is highlighted that our\npre-training scheme and new loss function improve the critical success index\n(CSI) of nowcasting of heavy rainfall (at least 10 mm/hr) by up to 95.7% and\n43.6%, respectively, at a 5-hr lead time. We also demonstrate that our approach\nreduces the precipitation estimation error by up to 10.7%, compared to the\nconventional approach, for light rainfall (between 1 and 10 mm/hr). Lastly, we\nreport the sensitivity of our approach to different resolutions and a detailed\nanalysis of four cases of heavy rainfall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jihoon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyuhan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hyunjin Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seok-Geun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Seok-Woo Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kijung Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks. (arXiv:2202.10571v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10571","description":"<p>In the deep learning era, long video generation of high-quality still remains\nchallenging due to the spatio-temporal complexity and continuity of videos.\nExisting prior works have attempted to model video distribution by representing\nvideos as 3D grids of RGB values, which impedes the scale of generated videos\nand neglects continuous dynamics. In this paper, we found that the recent\nemerging paradigm of implicit neural representations (INRs) that encodes a\ncontinuous signal into a parameterized neural network effectively mitigates the\nissue. By utilizing INRs of video, we propose dynamics-aware implicit\ngenerative adversarial network (DIGAN), a novel generative adversarial network\nfor video generation. Specifically, we introduce (a) an INR-based video\ngenerator that improves the motion dynamics by manipulating the space and time\ncoordinates differently and (b) a motion discriminator that efficiently\nidentifies the unnatural motions without observing the entire long frame\nsequences. We demonstrate the superiority of DIGAN under various datasets,\nalong with multiple intriguing properties, e.g., long video synthesis, video\nextrapolation, and non-autoregressive video generation. For example, DIGAN\nimproves the previous state-of-the-art FVD score on UCF-101 by 30.7% and can be\ntrained on 128 frame videos of 128x128 resolution, 80 frames longer than the 48\nframes of the previous state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tack_J/0/1/0/all/0/1\">Jihoon Tack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Semantic-Assisted Outlier Removal for Large-scale Point Cloud Registration. (arXiv:2202.10579v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10579","description":"<p>With current trends in sensors (cheaper, more volume of data) and\napplications (increasing affordability for new tasks, new ideas in what 3D data\ncould be useful for); there is corresponding increasing interest in the ability\nto automatically, reliably, and cheaply, register together individual point\nclouds. The volume of data to handle, and still elusive need to have the\nregistration occur fully reliably and fully automatically, mean there is a need\nto innovate further. One largely untapped area of innovation is that of\nexploiting the {\\em semantic information} of the points in question. Points on\na tree should match points on a tree, for example, and not points on car.\nMoreover, such a natural restriction is clearly human-like - a human would\ngenerally quickly eliminate candidate regions for matching based on semantics.\nEmploying semantic information is not only efficient but natural. It is also\ntimely - due to the recent advances in semantic classification capabilities.\nThis paper advances this theme by demonstrating that state of the art\nregistration techniques, in particular ones that rely on \"preservation of\nlength under rigid motion\" as an underlying matching consistency constraint,\ncan be augmented with semantic information. Semantic identity is of course also\npreserved under rigid-motion, but also under wider motions present in a scene.\nWe demonstrate that not only the potential obstacle of cost of semantic\nsegmentation, and the potential obstacle of the unreliability of semantic\nsegmentation; are both no impediment to achieving both speed and accuracy in\nfully automatic registration of large scale point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_G/0/1/0/all/0/1\">Giang Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Huu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_A/0/1/0/all/0/1\">Alvaro Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilani_S/0/1/0/all/0/1\">Syed Zulqarnain Gilani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Syed M. S. Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Light Fields for Super-Resolution and Disparity Estimation. (arXiv:2202.10603v1 [eess.IV])","link":"http://arxiv.org/abs/2202.10603","description":"<p>Light field (LF) cameras record both intensity and directions of light rays,\nand encode 3D scenes into 4D LF images. Recently, many convolutional neural\nnetworks (CNNs) have been proposed for various LF image processing tasks.\nHowever, it is challenging for CNNs to effectively process LF images since the\nspatial and angular information are highly inter-twined with varying\ndisparities. In this paper, we propose a generic mechanism to disentangle these\ncoupled information for LF image processing. Specifically, we first design a\nclass of domain-specific convolutions to disentangle LFs from different\ndimensions, and then leverage these disentangled features by designing\ntask-specific modules. Our disentangling mechanism can well incorporate the LF\nstructure prior and effectively handle 4D LF data. Based on the proposed\nmechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp)\nfor spatial super-resolution, angular super-resolution and disparity\nestimation. Experimental results show that our networks achieve\nstate-of-the-art performance on all these three tasks, which demonstrates the\neffectiveness, efficiency, and generality of our disentangling mechanism.\nProject page: https://yingqianwang.github.io/DistgLF/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1\">Gaochang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_W/0/1/0/all/0/1\">Wei An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Sliced-Wasserstein Feature Sets for Illumination-invariant Face Recognition. (arXiv:2202.10642v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10642","description":"<p>We present a new method for face recognition from digital images acquired\nunder varying illumination conditions. The method is based on mathematical\nmodeling of local gradient distributions using the Radon Cumulative\nDistribution Transform (R-CDT). We demonstrate that lighting variations cause\ncertain types of deformations of local image gradient distributions which, when\nexpressed in R-CDT domain, can be modeled as a subspace. Face recognition is\nthen performed using a nearest subspace in R-CDT domain of local gradient\ndistributions. Experiment results demonstrate the proposed method outperforms\nother alternatives in several face recognition tasks with challenging\nillumination conditions. Python code implementing the proposed method is\navailable, which is integrated as a part of the software package PyTransKit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shifat_E_Rabbi_M/0/1/0/all/0/1\">Mohammad Shifat-E-Rabbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuwang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubaiyat_A/0/1/0/all/0/1\">Abu Hasnat Mohammad Rubaiyat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1\">Gustavo K. Rohde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Two-Branch Neural Network for Gait Recognition. (arXiv:2202.10645v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10645","description":"<p>Gait recognition, a promising long-distance biometric technology, has aroused\nintense interest in computer vision. Existing works on gait recognition can be\ndivided into appearance-based methods and model-based methods, which extract\nfeatures from silhouettes and skeleton data, respectively. However, since\nappearance-based methods are greatly affected by clothing changing and carrying\ncondition, and model-based methods are limited by the accuracy of pose\nestimation approaches, gait recognition remains challenging in practical\napplications. In order to integrate the merits of such two approaches, a\ntwo-branch neural network (NN)-based model is proposed in this paper. The\nmethod contains two branches, namely a CNN-based branch taking silhouettes as\ninput and a GCN-based branch taking skeletons as input. In addition, two\nmodifications are introduced into the GCN-based branch to boost the\nperformance. First, we present a simple fully connected graph convolution\noperator to integrate multi-scale graph convolutions and relieve dependence on\nnatural connections. Second, we deploy an attention module named STC-Att after\neach GCN block to learn spatial, temporal and channel-wise attention\nsimultaneously. We evaluated the proposed two-branch neural network on the\nCASIA-B dataset. The experimental results show that our method achieves\nstate-of-the-art performance in various conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Likai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Movies2Scenes: Learning Scene Representations Using Movie Similarities. (arXiv:2202.10650v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10650","description":"<p>Automatic understanding of movie-scenes is an important problem with multiple\ndownstream applications including video-moderation, search and recommendation.\nThe long-form nature of movies makes labeling of movie scenes a laborious task,\nwhich makes applying end-to-end supervised approaches for understanding\nmovie-scenes a challenging problem. Directly applying state-of-the-art visual\nrepresentations learned from large-scale image datasets for movie-scene\nunderstanding does not prove to be effective given the large gap between the\ntwo domains. To address these challenges, we propose a novel contrastive\nlearning approach that uses commonly available sources of movie-information\n(e.g., genre, synopsis, more-like-this information) to learn a general-purpose\nscene-representation. Using a new dataset (MovieCL30K) with 30,340 movies, we\ndemonstrate that our learned scene-representation surpasses existing\nstate-of-the-art results on eleven downstream tasks from multiple datasets. To\nfurther show the effectiveness of our scene-representation, we introduce\nanother new dataset (MCD) focused on large-scale video-moderation with 44,581\nclips containing sex, violence, and drug-use activities covering 18,330 movies\nand TV episodes, and show strong gains over existing state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shixing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xiaohan Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamid_R/0/1/0/all/0/1\">Raffay Hamid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges. (arXiv:2202.10659v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10659","description":"<p>This paper describes the third Affective Behavior Analysis in-the-wild (ABAW)\nCompetition, held in conjunction with IEEE International Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. The 3rd ABAW Competition is a\ncontinuation of the Competitions held at ICCV 2021, IEEE FG 2020 and IEEE CVPR\n2017 Conferences, and aims at automatically analyzing affect. This year the\nCompetition encompasses four Challenges: i) uni-task Valence-Arousal\nEstimation, ii) uni-task Expression Classification, iii) uni-task Action Unit\nDetection, and iv) Multi-Task-Learning. All the Challenges are based on a\ncommon benchmark database, Aff-Wild2, which is a large scale in-the-wild\ndatabase and the first one to be annotated in terms of valence-arousal,\nexpressions and action units. In this paper, we present the four Challenges,\nwith the utilized Competition corpora, we outline the evaluation metrics and\npresent the baseline systems along with their obtained results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Eye Detector Using Metric Learning for Iris on The Move. (arXiv:2202.10671v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10671","description":"<p>This paper proposes a fast eye detection method based on fully-convolutional\nSiamese networks for iris recognition. The iris on the move system requires to\ncapture high resolution iris images from a moving subject for iris recognition.\nTherefore, capturing images contains both eyes at high-frame-rate increases the\nchance of iris imaging. In order to output the authentication result in real\ntime, the system requires a fast eye detector extracting the left and right eye\nregions from the image. Our method extracts features of a partial face image\nand a reference eye image using Siamese network frameworks. Similarity heat\nmaps of both eyes are created by calculating the spatial cosine similarity\nbetween extracted features. Besides, we use CosFace as a loss function for\ntraining to discriminate the left and right eyes with high accuracy even with a\nshallow network. Experimental results show that our method trained by CosFace\nis fast and accurate compared with conventional generic object detection\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogino_Y/0/1/0/all/0/1\">Yuka Ogino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toizumi_T/0/1/0/all/0/1\">Takahiro Toizumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1\">Masato Tsukada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing is Living? Rethinking the Security of Facial Liveness Verification in the Deepfake Era. (arXiv:2202.10673v1 [cs.CR])","link":"http://arxiv.org/abs/2202.10673","description":"<p>Facial Liveness Verification (FLV) is widely used for identity authentication\nin many security-sensitive domains and offered as Platform-as-a-Service (PaaS)\nby leading cloud vendors. Yet, with the rapid advances in synthetic media\ntechniques (e.g., deepfake), the security of FLV is facing unprecedented\nchallenges, about which little is known thus far.\n</p>\n<p>To bridge this gap, in this paper, we conduct the first systematic study on\nthe security of FLV in real-world settings. Specifically, we present\nLiveBugger, a new deepfake-powered attack framework that enables customizable,\nautomated security evaluation of FLV. Leveraging LiveBugger, we perform a\ncomprehensive empirical assessment of representative FLV platforms, leading to\na set of interesting findings. For instance, most FLV APIs do not use\nanti-deepfake detection; even for those with such defenses, their effectiveness\nis concerning (e.g., it may detect high-quality synthesized videos but fail to\ndetect low-quality ones). We then conduct an in-depth analysis of the factors\nimpacting the attack performance of LiveBugger: a) the bias (e.g., gender or\nrace) in FLV can be exploited to select victims; b) adversarial training makes\ndeepfake more effective to bypass FLV; c) the input quality has a varying\ninfluence on different deepfake techniques to bypass FLV. Based on these\nfindings, we propose a customized, two-stage approach that can boost the attack\nsuccess rate by up to 70%. Further, we run proof-of-concept attacks on several\nrepresentative applications of FLV (i.e., the clients of FLV APIs) to\nillustrate the practical implications: due to the vulnerability of the APIs,\nmany downstream applications are vulnerable to deepfake. Finally, we discuss\npotential countermeasures to improve the security of FLV. Our findings have\nbeen confirmed by the corresponding vendors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changjiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhaohan Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shanqing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcing Local Feature Representation for Weakly-Supervised Dense Crowd Counting. (arXiv:2202.10681v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10681","description":"<p>Fully-supervised crowd counting is a laborious task due to the large amounts\nof annotations. Few works focus on weekly-supervised crowd counting, where only\nthe global crowd numbers are available for training. The main challenge of\nweekly-supervised crowd counting is the lack of local supervision information.\nTo address this problem, we propose a self-adaptive feature similarity learning\n(SFSL) network and a global-local consistency (GLC) loss to reinforce local\nfeature representation. We introduce a feature vector which represents the\nunbiased feature estimation of persons. The network updates the feature vector\nself-adaptively and utilizes the feature similarity for the regression of crowd\nnumbers. Besides, the proposed GLC loss leverages the consistency between the\nnetwork estimations from global and local areas. The experimental results\ndemonstrate that our proposed method based on different backbones narrows the\ngap between weakly-supervised and fully-supervised dense crowd counting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoshuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongtao Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cut and Continuous Paste towards Real-time Deep Fall Detection. (arXiv:2202.10687v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10687","description":"<p>Deep learning based fall detection is one of the crucial tasks for\nintelligent video surveillance systems, which aims to detect unintentional\nfalls of humans and alarm dangerous situations. In this work, we propose a\nsimple and efficient framework to detect falls through a single and small-sized\nconvolutional neural network. To this end, we first introduce a new image\nsynthesis method that represents human motion in a single frame. This\nsimplifies the fall detection task as an image classification task. Besides,\nthe proposed synthetic data generation method enables to generate a sufficient\namount of training dataset, resulting in satisfactory performance even with the\nsmall model. At the inference step, we also represent real human motion in a\nsingle image by estimating mean of input frames. In the experiment, we conduct\nboth qualitative and quantitative evaluations on URFD and AIHub airport\ndatasets to show the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sunhee Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ki_M/0/1/0/all/0/1\">Minsong Ki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sanghoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_B/0/1/0/all/0/1\">Byoung-Ki Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Object Aware Hybrid U-Net for Breast Tumour Annotation. (arXiv:2202.10691v1 [eess.IV])","link":"http://arxiv.org/abs/2202.10691","description":"<p>In the clinical settings, during digital examination of histopathological\nslides, the pathologist annotate the slides by marking the rough boundary\naround the suspected tumour region. The marking or annotation is generally\nrepresented as a polygonal boundary that covers the extent of the tumour in the\nslide. These polygonal markings are difficult to imitate through CAD techniques\nsince the tumour regions are heterogeneous and hence segmenting them would\nrequire exhaustive pixel wise ground truth annotation. Therefore, for CAD\nanalysis, the ground truths are generally annotated by pathologist explicitly\nfor research purposes. However, this kind of annotation which is generally\nrequired for semantic or instance segmentation is time consuming and tedious.\nIn this proposed work, therefore, we have tried to imitate pathologist like\nannotation by segmenting tumour extents by polygonal boundaries. For polygon\nlike annotation or segmentation, we have used Active Contours whose vertices or\nsnake points move towards the boundary of the object of interest to find the\nregion of minimum energy. To penalize the Active Contour we used modified U-Net\narchitecture for learning penalization values. The proposed hybrid deep\nlearning model fuses the modern deep learning segmentation algorithm with\ntraditional Active Contours segmentation technique. The model is tested against\nboth state-of-the-art semantic segmentation and hybrid models for performance\nevaluation against contemporary work. The results obtained show that the\npathologist like annotation could be achieved by developing such hybrid models\nthat integrate the domain knowledge through classical segmentation methods like\nActive Contours and global knowledge through semantic segmentation deep\nlearning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tripathi_S/0/1/0/all/0/1\">Suvidha Tripathi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal adversarial perturbation for remote sensing images. (arXiv:2202.10693v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10693","description":"<p>Recently, with the application of deep learning in the remote sensing image\n(RSI) field, the classification accuracy of the RSI has been greatly improved\ncompared with traditional technology. However, even state-of-the-art object\nrecognition convolutional neural networks are fooled by the universal\nadversarial perturbation (UAP). To verify that UAP makes the RSI classification\nmodel error classification, this paper proposes a novel method combining an\nencoder-decoder network with an attention mechanism. Firstly, the former can\nlearn the distribution of perturbations better, then the latter is used to find\nthe main regions concerned by the RSI classification model. Finally, the\ngenerated regions are used to fine-tune the perturbations making the model\nmisclassified with fewer perturbations. The experimental results show that the\nUAP can make the RSI misclassify, and the attack success rate (ASR) of our\nproposed method on the RSI data set is as high as 97.35%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaoxia Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensembling Handcrafted Features with Deep Features: An Analytical Study for Classification of Routine Colon Cancer Histopathological Nuclei Images. (arXiv:2202.10694v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10694","description":"<p>The use of Deep Learning (DL) based methods in medical histopathology images\nhave been one of the most sought after solutions to classify, segment, and\ndetect diseased biopsy samples. However, given the complex nature of medical\ndatasets due to the presence of intra-class variability and heterogeneity, the\nuse of complex DL models might not give the optimal performance up to the level\nwhich is suitable for assisting pathologists. Therefore, ensemble DL methods\nwith the scope of including domain agnostic handcrafted Features (HC-F)\ninspired this work. We have, through experiments, tried to highlight that a\nsingle DL network (domain-specific or state of the art pre-trained models)\ncannot be directly used as the base model without proper analysis with the\nrelevant dataset. We have used F1-measure, Precision, Recall, AUC, and\nCross-Entropy Loss to analyse the performance of our approaches. We observed\nfrom the results that the DL features ensemble bring a marked improvement in\nthe overall performance of the model, whereas, domain agnostic HC-F remains\ndormant on the performance of the DL models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Suvidha Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag of Visual Words (BoVW) with Deep Features -- Patch Classification Model for Limited Dataset of Breast Tumours. (arXiv:2202.10701v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10701","description":"<p>Currently, the computational complexity limits the training of high\nresolution gigapixel images using Convolutional Neural Networks. Therefore,\nsuch images are divided into patches or tiles. Since, these high resolution\npatches are encoded with discriminative information therefore; CNNs are trained\non these patches to perform patch-level predictions. However, the problem with\npatch-level prediction is that pathologist generally annotates at image-level\nand not at patch level. Due to this limitation most of the patches may not\ncontain enough class-relevant features. Through this work, we tried to\nincorporate patch descriptive capability within the deep framework by using Bag\nof Visual Words (BoVW) as a kind of regularisation to improve generalizability.\nUsing this hypothesis, we aim to build a patch based classifier to discriminate\nbetween four classes of breast biopsy image patches (normal, benign, \\textit{In\nsitu} carcinoma, invasive carcinoma). The task is to incorporate quality deep\nfeatures using CNN to describe relevant information in the images while\nsimultaneously discarding irrelevant information using Bag of Visual Words\n(BoVW). The proposed method passes patches obtained from WSI and microscopy\nimages through pre-trained CNN to extract features. BoVW is used as a feature\nselector to select most discriminative features among the CNN features.\nFinally, the selected feature sets are classified as one of the four classes.\nThe hybrid model provides flexibility in terms of choice of pre-trained models\nfor feature extraction. The pipeline is end-to-end since it does not require\npost processing of patch predictions to select discriminative patches. We\ncompared our observations with state-of-the-art methods like ResNet50,\nDenseNet169, and InceptionV3 on the BACH-2018 challenge dataset. Our proposed\nmethod shows better performance than all the three methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Suvidha Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuan_L/0/1/0/all/0/1\">Lee Hwee Kuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving In-Bed Pose Monitoring: A Fusion and Reconstruction Study. (arXiv:2202.10704v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10704","description":"<p>Recently, in-bed human pose estimation has attracted the interest of\nresearchers due to its relevance to a wide range of healthcare applications.\nCompared to the general problem of human pose estimation, in-bed pose\nestimation has several inherent challenges, the most prominent being frequent\nand severe occlusions caused by bedding. In this paper we explore the effective\nuse of images from multiple non-visual and privacy-preserving modalities such\nas depth, long-wave infrared (LWIR) and pressure maps for the task of in-bed\npose estimation in two settings. First, we explore the effective fusion of\ninformation from different imaging modalities for better pose estimation.\nSecondly, we propose a framework that can estimate in-bed pose estimation when\nvisible images are unavailable, and demonstrate the applicability of fusion\nmethods to scenarios where only LWIR images are available. We analyze and\ndemonstrate the effect of fusing features from multiple modalities. For this\npurpose, we consider four different techniques: 1) Addition, 2) Concatenation,\n3) Fusion via learned modal weights, and 4) End-to-end fully trainable\napproach; with a state-of-the-art pose estimation model. We also evaluate the\neffect of reconstructing a data-rich modality (i.e., visible modality) from a\nprivacy-preserving modality with data scarcity (i.e., long-wavelength infrared)\nfor in-bed human pose estimation. For reconstruction, we use a conditional\ngenerative adversarial network. We conduct ablative studies across different\ndesign decisions of our framework. This includes selecting features with\ndifferent levels of granularity, using different fusion techniques, and varying\nmodel parameters. Through extensive evaluations, we demonstrate that our method\nproduces on par or better results compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dayarathna_T/0/1/0/all/0/1\">Thisun Dayarathna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthukumarana_T/0/1/0/all/0/1\">Thamidu Muthukumarana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathnayaka_Y/0/1/0/all/0/1\">Yasiru Rathnayaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Chathura de Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pemasiri_A/0/1/0/all/0/1\">Akila Pemasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointMatch: A Consistency Training Framework for Weakly SupervisedSemantic Segmentation of 3D Point Clouds. (arXiv:2202.10705v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10705","description":"<p>Semantic segmentation of point cloud usually relies on dense annotation that\nis exhausting and costly, so it attracts wide attention to investigate\nsolutions for the weakly supervised scheme with only sparse points annotated.\nExisting works start from the given labels and propagate them to highly-related\nbut unlabeled points, with the guidance of data, e.g. intra-point relation.\nHowever, it suffers from (i) the inefficient exploitation of data information,\nand (ii) the strong reliance on labels thus is easily suppressed when given\nmuch fewer annotations. Therefore, we propose a novel framework, PointMatch,\nthat stands on both data and label, by applying consistency regularization to\nsufficiently probe information from data itself and leveraging weak labels as\nassistance at the same time. By doing so, meaningful information can be learned\nfrom both data and label for better representation learning, which also enables\nthe model more robust to the extent of label sparsity. Simple yet effective,\nthe proposed PointMatch achieves the state-of-the-art performance under various\nweakly-supervised schemes on both ScanNet-v2 and S3DIS datasets, especially on\nthe settings with extremely sparse labels, e.g. surpassing SQN by 21.2% and\n17.2% on the 0.01% and 0.1% setting of ScanNet-v2, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yushuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zizheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shengcai Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRel: Filter Pruning based on High Relevance between Activation Maps and Class Labels. (arXiv:2202.10716v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10716","description":"<p>This paper proposes an Information Bottleneck theory based filter pruning\nmethod that uses a statistical measure called Mutual Information (MI). The MI\nbetween filters and class labels, also called \\textit{Relevance}, is computed\nusing the filter's activation maps and the annotations. The filters having High\nRelevance (HRel) are considered to be more important. Consequently, the least\nimportant filters, which have lower Mutual Information with the class labels,\nare pruned. Unlike the existing MI based pruning methods, the proposed method\ndetermines the significance of the filters purely based on their corresponding\nactivation map's relationship with the class labels. Architectures such as\nLeNet-5, VGG-16, ResNet-56\\textcolor{myblue}{, ResNet-110 and ResNet-50 are\nutilized to demonstrate the efficacy of the proposed pruning method over MNIST,\nCIFAR-10 and ImageNet datasets. The proposed method shows the state-of-the-art\npruning results for LeNet-5, VGG-16, ResNet-56, ResNet-110 and ResNet-50\narchitectures. In the experiments, we prune 97.98 \\%, 84.85 \\%, 76.89\\%,\n76.95\\%, and 63.99\\% of Floating Point Operation (FLOP)s from LeNet-5, VGG-16,\nResNet-56, ResNet-110, and ResNet-50 respectively.} The proposed HRel pruning\nmethod outperforms recent state-of-the-art filter pruning methods. Even after\npruning the filters from convolutional layers of LeNet-5 drastically (i.e. from\n20, 50 to 2, 3, respectively), only a small accuracy drop of 0.52\\% is\nobserved. Notably, for VGG-16, 94.98\\% parameters are reduced, only with a drop\nof 0.36\\% in top-1 accuracy. \\textcolor{myblue}{ResNet-50 has shown a 1.17\\%\ndrop in the top-5 accuracy after pruning 66.42\\% of the FLOPs.} In addition to\npruning, the Information Plane dynamics of Information Bottleneck theory is\nanalyzed for various Convolutional Neural Network architectures with the effect\nof pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarvani_C/0/1/0/all/0/1\">CH Sarvani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorai_M/0/1/0/all/0/1\">Mrinmoy Ghorai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basha_S/0/1/0/all/0/1\">SH Shabbeer Basha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature reconstruction from incomplete tomographic data without detour. (arXiv:2202.10724v1 [eess.IV])","link":"http://arxiv.org/abs/2202.10724","description":"<p>In this paper, we consider the problem of feature reconstruction from\nincomplete x-ray CT data. Such problems occurs, e.g., as a result of dose\nreduction in the context medical imaging. Since image reconstruction from\nincomplete data is a severely ill-posed problem, the reconstructed images may\nsuffer from characteristic artefacts or missing features, and significantly\ncomplicate subsequent image processing tasks (e.g., edge detection or\nsegmentation). In this paper, we introduce a novel framework for the robust\nreconstruction of convolutional image features directly from CT data, without\nthe need of computing a reconstruction firs. Within our framework we use\nnon-linear (variational) regularization methods that can be adapted to a\nvariety of feature reconstruction tasks and to several limited data situations\n. In our numerical experiments, we consider several instances of edge\nreconstructions from angularly undersampled data and show that our approach is\nable to reliably reconstruct feature maps in this case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goppel_S/0/1/0/all/0/1\">Simon G&#xf6;ppel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frikel_J/0/1/0/all/0/1\">J&#xfc;rgen Frikel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution. (arXiv:2202.10753v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10753","description":"<p>Nowadays, thermal infrared satellite remote sensors enable to extract very\ninteresting information at large scale, in particular Land Surface Temperature\n(LST). However such data are limited in spatial and/or temporal resolutions\nwhich prevents from an analysis at fine scales. For example, MODIS satellite\nprovides daily acquisitions with 1Km spatial resolutions which is not\nsufficient to deal with highly heterogeneous environments as agricultural\nparcels. Therefore, image super-resolution is a crucial task to better exploit\nMODIS LSTs. This issue is tackled in this paper. We introduce a deep\nlearning-based algorithm, named Multi-residual U-Net, for super-resolution of\nMODIS LST single-images. Our proposed network is a modified version of U-Net\narchitecture, which aims at super-resolving the input LST image from 1Km to\n250m per pixel. The results show that our Multi-residual U-Net outperforms\nother state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_G/0/1/0/all/0/1\">Ganglin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh-Triet Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_A/0/1/0/all/0/1\">Aur&#xe9;lie Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corpetti_T/0/1/0/all/0/1\">Thomas Corpetti</a> (CNRS, LETG), <a href=\"http://arxiv.org/find/cs/1/au:+Granero_Belinchon_C/0/1/0/all/0/1\">Carlos Granero-Belinchon</a> (Lab-STICC_OSE, IMT Atlantique - MEE)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thinking the Fusion Strategy of Multi-reference Face Reenactment. (arXiv:2202.10758v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10758","description":"<p>In recent advances of deep generative models, face reenactment -manipulating\nand controlling human face, including their head movement-has drawn much\nattention for its wide range of applicability. Despite its strong\nexpressiveness, it is inevitable that the models fail to reconstruct or\naccurately generate unseen side of the face of a given single reference image.\nMost of existing methods alleviate this problem by learning appearances of\nhuman faces from large amount of data and generate realistic texture at\ninference time. Rather than completely relying on what generative models learn,\nwe show that simple extension by using multiple reference images significantly\nimproves generation quality. We show this by 1) conducting the reconstruction\ntask on publicly available dataset, 2) conducting facial motion transfer on our\noriginal dataset which consists of multi-person's head movement video\nsequences, and 3) using a newly proposed evaluation metric to validate that our\nmethod achieves better quantitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yashima_T/0/1/0/all/0/1\">Takuya Yashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narihira_T/0/1/0/all/0/1\">Takuya Narihira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kojima_T/0/1/0/all/0/1\">Tamaki Kojima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning based domain adaptation for mitochondria segmentation on EM volumes. (arXiv:2202.10773v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10773","description":"<p>Accurate segmentation of electron microscopy (EM) volumes of the brain is\nessential to characterize neuronal structures at a cell or organelle level.\nWhile supervised deep learning methods have led to major breakthroughs in that\ndirection during the past years, they usually require large amounts of\nannotated data to be trained, and perform poorly on other data acquired under\nsimilar experimental and imaging conditions. This is a problem known as domain\nadaptation, since models that learned from a sample distribution (or source\ndomain) struggle to maintain their performance on samples extracted from a\ndifferent distribution or target domain. In this work, we address the complex\ncase of deep learning based domain adaptation for mitochondria segmentation\nacross EM datasets from different tissues and species. We present three\nunsupervised domain adaptation strategies to improve mitochondria segmentation\nin the target domain based on (1) state-of-the-art style transfer between\nimages of both domains; (2) self-supervised learning to pre-train a model using\nunlabeled source and target images, and then fine-tune it only with the source\nlabels; and (3) multi-task neural network architectures trained end-to-end with\nboth labeled and unlabeled images. Additionally, we propose a new training\nstopping criterion based on morphological priors obtained exclusively in the\nsource domain. We carried out all possible cross-dataset experiments using\nthree publicly available EM datasets. We evaluated our proposed strategies on\nthe mitochondria semantic labels predicted on the target datasets. The methods\nintroduced here outperform the baseline methods and compare favorably to the\nstate of the art. In the absence of validation labels, monitoring our proposed\nmorphology-based metric is an intuitive and effective way to stop the training\nprocess and select in average optimal models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franco_Barranco_D/0/1/0/all/0/1\">Daniel Franco-Barranco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastor_Tronch_J/0/1/0/all/0/1\">Julio Pastor-Tronch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Marfil_A/0/1/0/all/0/1\">Aitor Gonzalez-Marfil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Barrutia_A/0/1/0/all/0/1\">Arrate Mu&#xf1;oz-Barrutia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arganda_Carreras_I/0/1/0/all/0/1\">Ignacio Arganda-Carreras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RuCLIP -- new models and experiments: a technical report. (arXiv:2202.10784v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10784","description":"<p>In the report we propose six new implementations of ruCLIP model trained on\nour 240M pairs. The accuracy results are compared with original CLIP model with\nRu-En translation (OPUS-MT) on 16 datasets from different domains. Our best\nimplementations outperform CLIP + OPUS-MT solution on most of the datasets in\nfew-show and zero-shot tasks. In the report we briefly describe the\nimplementations and concentrate on the conducted experiments. Inference\nexecution time comparison is also presented in the report.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shonenkov_A/0/1/0/all/0/1\">Alex Shonenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1\">Andrey Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Denis Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatyana Shavrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesakov_D/0/1/0/all/0/1\">Daniil Chesakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maltseva_A/0/1/0/all/0/1\">Anastasia Maltseva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenogenova_A/0/1/0/all/0/1\">Alena Fenogenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlov_I/0/1/0/all/0/1\">Igor Pavlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emelyanov_A/0/1/0/all/0/1\">Anton Emelyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markov_S/0/1/0/all/0/1\">Sergey Markov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakshandaeva_D/0/1/0/all/0/1\">Daria Bakshandaeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shybaeva_V/0/1/0/all/0/1\">Vera Shybaeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chertok_A/0/1/0/all/0/1\">Andrey Chertok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VU-BERT: A Unified framework for Visual Dialog. (arXiv:2202.10787v1 [cs.CL])","link":"http://arxiv.org/abs/2202.10787","description":"<p>The visual dialog task attempts to train an agent to answer multi-turn\nquestions given an image, which requires the deep understanding of interactions\nbetween the image and dialog history. Existing researches tend to employ the\nmodality-specific modules to model the interactions, which might be troublesome\nto use. To fill in this gap, we propose a unified framework for image-text\njoint embedding, named VU-BERT, and apply patch projection to obtain vision\nembedding firstly in visual dialog tasks to simplify the model. The model is\ntrained over two tasks: masked language modeling and next utterance retrieval.\nThese tasks help in learning visual concepts, utterances dependence, and the\nrelationships between these two modalities. Finally, our VU-BERT achieves\ncompetitive performance (0.7287 NDCG scores) on VisDial v1.0 Datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A-Eye: Driving with the Eyes of AI for Corner Case Generation. (arXiv:2202.10803v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10803","description":"<p>The overall goal of this work is to enrich training data for automated\ndriving with so called corner cases. In road traffic, corner cases are\ncritical, rare and unusual situations that challenge the perception by AI\nalgorithms. For this purpose, we present the design of a test rig to generate\nsynthetic corner cases using a human-in-the-loop approach. For the test rig, a\nreal-time semantic segmentation network is trained and integrated into the\ndriving simulation software CARLA in such a way that a human can drive on the\nnetwork's prediction. In addition, a second person gets to see the same scene\nfrom the original CARLA output and is supposed to intervene with the help of a\nsecond control unit as soon as the semantic driver shows dangerous driving\nbehavior. Interventions potentially indicate poor recognition of a critical\nscene by the segmentation network and then represents a corner case. In our\nexperiments, we show that targeted enrichment of training data with corner\ncases leads to improvements in pedestrian detection in safety relevant episodes\nin road traffic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kowol_K/0/1/0/all/0/1\">Kamil Kowol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bracke_S/0/1/0/all/0/1\">Stefan Bracke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-shot Scene Graph Generation. (arXiv:2202.10824v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10824","description":"<p>As a structured representation of the image content, the visual scene graph\n(visual relationship) acts as a bridge between computer vision and natural\nlanguage processing. Existing models on the scene graph generation task\nnotoriously require tens or hundreds of labeled samples. By contrast, human\nbeings can learn visual relationships from a few or even one example. Inspired\nby this, we design a task named One-Shot Scene Graph Generation, where each\nrelationship triplet (e.g., \"dog-has-head\") comes from only one labeled\nexample. The key insight is that rather than learning from scratch, one can\nutilize rich prior knowledge. In this paper, we propose Multiple Structured\nKnowledge (Relational Knowledge and Commonsense Knowledge) for the one-shot\nscene graph generation task. Specifically, the Relational Knowledge represents\nthe prior knowledge of relationships between entities extracted from the visual\ncontent, e.g., the visual relationships \"standing in\", \"sitting in\", and \"lying\nin\" may exist between \"dog\" and \"yard\", while the Commonsense Knowledge encodes\n\"sense-making\" knowledge like \"dog can guard yard\". By organizing these two\nkinds of knowledge in a graph structure, Graph Convolution Networks (GCNs) are\nused to extract knowledge-embedded semantic features of the entities. Besides,\ninstead of extracting isolated visual features from each entity generated by\nFaster R-CNN, we utilize an Instance Relation Transformer encoder to fully\nexplore their context information. Based on a constructed one-shot dataset, the\nexperimental results show that our method significantly outperforms existing\nstate-of-the-art methods by a large margin. Ablation studies also verify the\neffectiveness of the Instance Relation Transformer encoder and the Multiple\nStructured Knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Regularized Scene Graph Generation. (arXiv:2202.10826v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10826","description":"<p>Scene graph generation (SGG) is built on top of detected objects to predict\nobject pairwise visual relations for describing the image content abstraction.\nExisting works have revealed that if the links between objects are given as\nprior knowledge, the performance of SGG is significantly improved. Inspired by\nthis observation, in this article, we propose a relation regularized network\n(R2-Net), which can predict whether there is a relationship between two objects\nand encode this relation into object feature refinement and better SGG.\nSpecifically, we first construct an affinity matrix among detected objects to\nrepresent the probability of a relationship between two objects. Graph\nconvolution networks (GCNs) over this relation affinity matrix are then used as\nobject encoders, producing relation-regularized representations of objects.\nWith these relation-regularized features, our R2-Net can effectively refine\nobject labels and generate scene graphs. Extensive experiments are conducted on\nthe visual genome dataset for three SGG tasks (i.e., predicate classification,\nscene graph classification, and scene graph detection), demonstrating the\neffectiveness of our proposed method. Ablation studies also verify the key\nroles of our proposed components in performance improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting long-term temporal dynamics for video captioning. (arXiv:2202.10828v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10828","description":"<p>Automatically describing videos with natural language is a fundamental\nchallenge for computer vision and natural language processing. Recently,\nprogress in this problem has been achieved through two steps: 1) employing 2-D\nand/or 3-D Convolutional Neural Networks (CNNs) (e.g. VGG, ResNet or C3D) to\nextract spatial and/or temporal features to encode video contents; and 2)\napplying Recurrent Neural Networks (RNNs) to generate sentences to describe\nevents in videos. Temporal attention-based model has gained much progress by\nconsidering the importance of each video frame. However, for a long video,\nespecially for a video which consists of a set of sub-events, we should\ndiscover and leverage the importance of each sub-shot instead of each frame. In\nthis paper, we propose a novel approach, namely temporal and spatial LSTM\n(TS-LSTM), which systematically exploits spatial and temporal dynamics within\nvideo sequences. In TS-LSTM, a temporal pooling LSTM (TP-LSTM) is designed to\nincorporate both spatial and temporal information to extract long-term temporal\ndynamics within video sub-shots; and a stacked LSTM is introduced to generate a\nlist of words to describe the video. Experimental results obtained in two\npublic video captioning benchmarks indicate that our TS-LSTM outperforms the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqiu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SADN: Learned Light Field Image Compression with Spatial-Angular Decorrelation. (arXiv:2202.10837v1 [eess.IV])","link":"http://arxiv.org/abs/2202.10837","description":"<p>Light field image becomes one of the most promising media types for immersive\nvideo applications. In this paper, we propose a novel end-to-end\nspatial-angular-decorrelated network (SADN) for high-efficiency light field\nimage compression. Different from the existing methods that exploit either\nspatial or angular consistency in the light field image, SADN decouples the\nangular and spatial information by dilation convolution and stride convolution\nin spatial-angular interaction, and performs feature fusion to compress spatial\nand angular information jointly. To train a stable and robust algorithm, a\nlarge-scale dataset consisting of 7549 light field images is proposed and\nbuilt. The proposed method provides 2.137 times and 2.849 times higher\ncompression efficiency relative to H.266/VVC and H.265/HEVC inter coding,\nrespectively. It also outperforms the end-to-end image compression networks by\nan average of 79.6% bitrate saving with much higher subjective quality and\nlight field consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tong_K/0/1/0/all/0/1\">Kedeng Tong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_F/0/1/0/all/0/1\">Fan Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography. (arXiv:2202.10847v1 [eess.IV])","link":"http://arxiv.org/abs/2202.10847","description":"<p>Implicit neural representations (INRs) have achieved impressive results for\nscene reconstruction and computer graphics, where their performance has\nprimarily been assessed on reconstruction accuracy. However, in medical\nimaging, where the reconstruction problem is underdetermined and model\npredictions inform high-stakes diagnoses, uncertainty quantification of INR\ninference is critical. To that end, we study UncertaINR: a Bayesian\nreformulation of INR-based image reconstruction, for computed tomography (CT).\nWe test several Bayesian deep learning implementations of UncertaINR and find\nthat they achieve well-calibrated uncertainty, while retaining accuracy\ncompetitive with other classical, INR-based, and CNN-based reconstruction\ntechniques. In contrast to the best-performing prior approaches, UncertaINR\ndoes not require a large training dataset, but only a handful of validation\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisca Vasconcelos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1\">Bobby He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_N/0/1/0/all/0/1\">Nalini Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning classification of large-scale point clouds: A case study on cuneiform tablets. (arXiv:2202.10851v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10851","description":"<p>This paper introduces a novel network architecture for the classification of\nlarge-scale point clouds. The network is used to classify metadata from\ncuneiform tablets. As more than half a million tablets remain unprocessed, this\ncan help create an overview of the tablets. The network is tested on a\ncomparison dataset and obtains state-of-the-art performance. We also introduce\nnew metadata classification tasks on which the network shows promising results.\nFinally, we introduce the novel Maximum Attention visualization, demonstrating\nthat the trained network focuses on the intended features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagelskjaer_F/0/1/0/all/0/1\">Frederik Hagelskjaer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Consistent Local Superresolution for Medical Imaging. (arXiv:2202.10875v1 [eess.IV])","link":"http://arxiv.org/abs/2202.10875","description":"<p>In this work we propose a new paradigm of iterative model-based\nreconstruction algorithms for providing real-time solution for zooming-in and\nrefining a region of interest in medical and clinical tomographic (such as\nCT/MRI/PET, etc) images. This algorithmic framework is tailor for a clinical\nneed in medical imaging practice, that after a reconstruction of the full\ntomographic image, the clinician may believe that some critical parts of the\nimage are not clear enough, and may wish to see clearer these\nregions-of-interest. A naive approach (which is highly not recommended) would\nbe performing the global reconstruction of a higher resolution image, which has\ntwo major limitations: firstly, it is computationally inefficient, and\nsecondly, the image regularization is still applied globally which may\nover-smooth some local regions. Furthermore if one wish to fine-tune the\nregularization parameter for local parts, it would be computationally\ninfeasible in practice for the case of using global reconstruction. Our new\niterative approaches for such tasks are based on jointly utilizing the\nmeasurement information, efficient upsampling/downsampling across image spaces,\nand locally adjusted image prior for efficient and high-quality\npost-processing. The numerical results in low-dose X-ray CT image local zoom-in\ndemonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1\">Junqi Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coordinate-Aligned Multi-Camera Collaboration for Active Multi-Object Tracking. (arXiv:2202.10881v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10881","description":"<p>Active Multi-Object Tracking (AMOT) is a task where cameras are controlled by\na centralized system to adjust their poses automatically and collaboratively so\nas to maximize the coverage of targets in their shared visual field. In AMOT,\neach camera only receives partial information from its observation, which may\nmislead cameras to take locally optimal action. Besides, the global goal, i.e.,\nmaximum coverage of objects, is hard to be directly optimized. To address the\nabove issues, we propose a coordinate-aligned multi-camera collaboration system\nfor AMOT. In our approach, we regard each camera as an agent and address AMOT\nwith a multi-agent reinforcement learning solution. To represent the\nobservation of each agent, we first identify the targets in the camera view\nwith an image detector, and then align the coordinates of the targets in 3D\nenvironment. We define the reward of each agent based on both global coverage\nas well as four individual reward terms. The action policy of the agents is\nderived with a value-based Q-network. To the best of our knowledge, we are the\nfirst to study the AMOT task. To train and evaluate the efficacy of our system,\nwe build a virtual yet credible 3D environment, named \"Soccer Court\", to mimic\nthe real-world AMOT scenario. The experimental results show that our system\nachieves a coverage of 71.88%, outperforming the baseline method by 8.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zeyu Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhenbo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Perceiver. (arXiv:2202.10890v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10890","description":"<p>General perception systems such as Perceivers can process arbitrary\nmodalities in any combination and are able to handle up to a few hundred\nthousand inputs. They achieve this generality by exclusively using global\nattention operations. This however hinders them from scaling up to the inputs\nsizes required to process raw high-resolution images or video. In this paper,\nwe show that some degree of locality can be introduced back into these models,\ngreatly improving their efficiency while preserving their generality. To scale\nthem further, we introduce a self-supervised approach that enables learning\ndense low-dimensional positional embeddings for very large signals. We call the\nresulting model a Hierarchical Perceiver (HiP). HiP retains the ability to\nprocess arbitrary modalities, but now at higher-resolution and without any\nspecialized preprocessing, improving over flat Perceivers in both efficiency\nand accuracy on the ImageNet, Audioset and PASCAL VOC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoran_D/0/1/0/all/0/1\">Daniel Zoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recasens_A/0/1/0/all/0/1\">Adria Recasens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1\">Catalin Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier Henaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_R/0/1/0/all/0/1\">Relja Arandjelovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1\">Matt Botvinick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sound Adversarial Audio-Visual Navigation. (arXiv:2202.10910v1 [cs.SD])","link":"http://arxiv.org/abs/2202.10910","description":"<p>Audio-visual navigation task requires an agent to find a sound source in a\nrealistic, unmapped 3D environment by utilizing egocentric audio-visual\nobservations. Existing audio-visual navigation works assume a clean environment\nthat solely contains the target sound, which, however, would not be suitable in\nmost real-world applications due to the unexpected sound noise or intentional\ninterference. In this work, we design an acoustically complex environment in\nwhich, besides the target sound, there exists a sound attacker playing a\nzero-sum game with the agent. More specifically, the attacker can move and\nchange the volume and category of the sound to make the agent suffer from\nfinding the sounding object while the agent tries to dodge the attack and\nnavigate to the goal under the intervention. Under certain constraints to the\nattacker, we can improve the robustness of the agent towards unexpected sound\nattacks in audio-visual navigation. For better convergence, we develop a joint\ntraining mechanism by employing the property of a centralized critic with\ndecentralized actors. Experiments on two real-world 3D scan datasets, Replica,\nand Matterport3D, verify the effectiveness and the robustness of the agent\ntrained under our designed environment when transferred to the clean\nenvironment or the one containing sound attackers with random policy. Project:\n\\url{https://yyf17.github.io/SAAVN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yinfeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaohong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Vision-Language Pre-Trained Models. (arXiv:2202.10936v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10936","description":"<p>As Transformer evolved, pre-trained models have advanced at a breakneck pace\nin recent years. They have dominated the mainstream techniques in natural\nlanguage processing (NLP) and computer vision (CV). How to adapt pre-training\nto the field of Vision-and-Language (V-L) learning and improve the performance\non downstream tasks becomes a focus of multimodal learning. In this paper, we\nreview the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As\nthe core content, we first briefly introduce several ways to encode raw images\nand texts to single-modal embeddings before pre-training. Then, we dive into\nthe mainstream architectures of VL-PTMs in modeling the interaction between\ntext and image representations. We further present widely-used pre-training\ntasks, after which we introduce some common downstream tasks. We finally\nconclude this paper and present some promising research directions. Our survey\naims to provide multimodal researchers a synthesis and pointer to related\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Based Activations for Accurate Bias-Free Learning. (arXiv:2202.10943v1 [cs.LG])","link":"http://arxiv.org/abs/2202.10943","description":"<p>Bias mitigation in machine learning models is imperative, yet challenging.\nWhile several approaches have been proposed, one view towards mitigating bias\nis through adversarial learning. A discriminator is used to identify the bias\nattributes such as gender, age or race in question. This discriminator is used\nadversarially to ensure that it cannot distinguish the bias attributes. The\nmain drawback in such a model is that it directly introduces a trade-off with\naccuracy as the features that the discriminator deems to be sensitive for\ndiscrimination of bias could be correlated with classification. In this work we\nsolve the problem. We show that a biased discriminator can actually be used to\nimprove this bias-accuracy tradeoff. Specifically, this is achieved by using a\nfeature masking approach using the discriminator's gradients. We ensure that\nthe features favoured for the bias discrimination are de-emphasized and the\nunbiased features are enhanced during classification. We show that this simple\napproach works well to reduce bias as well as improve accuracy significantly.\nWe evaluate the proposed model on standard benchmarks. We improve the accuracy\nof the adversarial methods while maintaining or even improving the unbiasness\nand also outperform several other recent methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishabh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Vardhan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subtyping brain diseases from imaging data. (arXiv:2202.10945v1 [cs.LG])","link":"http://arxiv.org/abs/2202.10945","description":"<p>The imaging community has increasingly adopted machine learning (ML) methods\nto provide individualized imaging signatures related to disease diagnosis,\nprognosis, and response to treatment. Clinical neuroscience and cancer imaging\nhave been two areas in which ML has offered particular promise. However, many\nneurologic and neuropsychiatric diseases, as well as cancer, are often\nheterogeneous in terms of their clinical manifestations, neuroanatomical\npatterns or genetic underpinnings. Therefore, in such cases, seeking a single\ndisease signature might be ineffectual in delivering individualized precision\ndiagnostics. The current chapter focuses on ML methods, especially\nsemi-supervised clustering, that seek disease subtypes using imaging data. Work\nfrom Alzheimer Disease and its prodromal stages, psychosis, depression, autism,\nand brain cancer are discussed. Our goal is to provide the readers with a broad\noverview in terms of methodology and clinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Junhao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_E/0/1/0/all/0/1\">Erdem Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Gyujoon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwyer_D/0/1/0/all/0/1\">Dominique Dwyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazerooni_A/0/1/0/all/0/1\">Anahita Fathi Kazerooni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalousis_P/0/1/0/all/0/1\">Paris Alexandros Lalousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1\">Christos Davatzikos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Controller With the Hand Gestures Pinch and Grab for Picking Up and Placing Virtual Objects. (arXiv:2202.10964v1 [cs.HC])","link":"http://arxiv.org/abs/2202.10964","description":"<p>Grabbing virtual objects is one of the essential tasks for Augmented,\nVirtual, and Mixed Reality applications. Modern applications usually use a\nsimple pinch gesture for grabbing and moving objects. However, picking up\nobjects by pinching has disadvantages. It can be an unnatural gesture to pick\nup objects and prevents the implementation of other gestures which would be\nperformed with thumb and index. Therefore it is not the optimal choice for many\napplications. In this work, different implementations for grabbing and placing\nvirtual objects are proposed and compared. Performance and accuracy of the\nproposed techniques are measured and compared.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schafer_A/0/1/0/all/0/1\">Alexander Sch&#xe4;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_G/0/1/0/all/0/1\">Gerd Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Classification Model Performance on Chest X-Rays through Lung Segmentation. (arXiv:2202.10971v1 [eess.IV])","link":"http://arxiv.org/abs/2202.10971","description":"<p>Chest radiography is an effective screening tool for diagnosing pulmonary\ndiseases. In computer-aided diagnosis, extracting the relevant region of\ninterest, i.e., isolating the lung region of each radiography image, can be an\nessential step towards improved performance in diagnosing pulmonary disorders.\nMethods: In this work, we propose a deep learning approach to enhance abnormal\nchest x-ray (CXR) identification performance through segmentations. Our\napproach is designed in a cascaded manner and incorporates two modules: a deep\nneural network with criss-cross attention modules (XLSor) for localizing lung\nregion in CXR images and a CXR classification model with a backbone of a\nself-supervised momentum contrast (MoCo) model pre-trained on large-scale CXR\ndata sets. The proposed pipeline is evaluated on Shenzhen Hospital (SH) data\nset for the segmentation module, and COVIDx data set for both segmentation and\nclassification modules. Novel statistical analysis is conducted in addition to\nregular evaluation metrics for the segmentation module. Furthermore, the\nresults of the optimized approach are analyzed with gradient-weighted class\nactivation mapping (Grad-CAM) to investigate the rationale behind the\nclassification decisions and to interpret its choices. Results and Conclusion:\nDifferent data sets, methods, and scenarios for each module of the proposed\npipeline are examined for designing an optimized approach, which has achieved\nan accuracy of 0.946 in distinguishing abnormal CXR images (i.e., Pneumonia and\nCOVID-19) from normal ones. Numerical and visual validations suggest that\napplying automated segmentation as a pre-processing step for classification\nimproves the generalization capability and the performance of the\nclassification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Azimi_H/0/1/0/all/0/1\">Hilda Azimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jianxing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Pengcheng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asad_H/0/1/0/all/0/1\">Hala Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tremblay_S/0/1/0/all/0/1\">Stephane Tremblay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimation of Looming from LiDAR. (arXiv:2202.10972v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10972","description":"<p>Looming, traditionally defined as the relative expansion of objects in the\nobserver's retina, is a fundamental visual cue for perception of threat and can\nbe used to accomplish collision free navigation. The measurement of the looming\ncue is not only limited to vision, and can also be obtained from range sensors\nlike LiDAR (Light Detection and Ranging). In this article we present two\nmethods that process raw LiDAR data to estimate the looming cue. Using looming\nvalues we show how to obtain threat zones for collision avoidance tasks. The\nmethods are general enough to be suitable for any six-degree-of-freedom motion\nand can be implemented in real-time without the need for fine matching,\npoint-cloud registration, object classification or object segmentation.\nQuantitative results using the KITTI dataset shows advantages and limitations\nof the methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yepes_J/0/1/0/all/0/1\">Juan D. Yepes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_D/0/1/0/all/0/1\">Daniel Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Winning Solution to the iFLYTEK Challenge 2021 Cultivated Land Extraction from High-Resolution Remote Sensing Image. (arXiv:2202.10974v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10974","description":"<p>Extracting cultivated land accurately from high-resolution remote images is a\nbasic task for precision agriculture. This report introduces our solution to\nthe iFLYTEK challenge 2021 cultivated land extraction from high-resolution\nremote sensing image. The challenge requires segmenting cultivated land objects\nin very high-resolution multispectral remote sensing images. We established a\nhighly effective and efficient pipeline to solve this problem. We first divided\nthe original images into small tiles and separately performed instance\nsegmentation on each tile. We explored several instance segmentation algorithms\nthat work well on natural images and developed a set of effective methods that\nare applicable to remote sensing images. Then we merged the prediction results\nof all small tiles into seamless, continuous segmentation results through our\nproposed overlap-tile fusion strategy. We achieved the first place among 486\nteams in the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqiu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking perovskite crystallization via deep learning-based feature detection on 2D X-ray scattering data. (arXiv:2202.10983v1 [cs.CV])","link":"http://arxiv.org/abs/2202.10983","description":"<p>Understanding the processes of perovskite crystallization is essential for\nimproving the properties of organic solar cells. In situ real-time\ngrazing-incidence X-ray diffraction (GIXD) is a key technique for this task,\nbut it produces large amounts of data, frequently exceeding the capabilities of\ntraditional data processing methods. We propose an automated pipeline for the\nanalysis of GIXD images, based on the Faster R-CNN deep learning architecture\nfor object detection, modified to conform to the specifics of the scattering\ndata. The model exhibits high accuracy in detecting diffraction features on\nnoisy patterns with various experimental artifacts. We demonstrate our method\non real-time tracking of organic-inorganic perovskite structure crystallization\nand test it on two applications: 1. the automated phase identification and\nunit-cell determination of two coexisting phases of Ruddlesden-Popper 2D\nperovskites, and 2. the fast tracking of MAPbI$_3$ perovskite formation. By\ndesign, our approach is equally suitable for other crystalline thin-film\nmaterials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Starostin_V/0/1/0/all/0/1\">Vladimir Starostin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munteanu_V/0/1/0/all/0/1\">Valentin Munteanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greco_A/0/1/0/all/0/1\">Alessandro Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneschaurek_E/0/1/0/all/0/1\">Ekaterina Kneschaurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleli_A/0/1/0/all/0/1\">Alina Pleli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertram_F/0/1/0/all/0/1\">Florian Bertram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerlach_A/0/1/0/all/0/1\">Alexander Gerlach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinderhofer_A/0/1/0/all/0/1\">Alexander Hinderhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreiber_F/0/1/0/all/0/1\">Frank Schreiber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does prior knowledge in the form of multiple low-dose PET images (at different dose levels) improve standard-dose PET prediction?. (arXiv:2202.10998v1 [physics.med-ph])","link":"http://arxiv.org/abs/2202.10998","description":"<p>Reducing the injected dose would result in quality degradation and loss of\ninformation in PET imaging. To address this issue, deep learning methods have\nbeen introduced to predict standard PET images (S-PET) from the corresponding\nlow-dose versions (L-PET). The existing deep learning-based denoising methods\nsolely rely on a single dose level of PET images to predict the S-PET images.\nIn this work, we proposed to exploit the prior knowledge in the form of\nmultiple low-dose levels of PET images (in addition to the target low-dose\nlevel) to estimate the S-PET images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Sanaei_B/0/1/0/all/0/1\">Behnoush Sanaei</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Faghihi_R/0/1/0/all/0/1\">Reza Faghihi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Arabi_H/0/1/0/all/0/1\">Hossein Arabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Objective Dual Simplex-Mesh Based Deformable Image Registration for 3D Medical Images -- Proof of Concept. (arXiv:2202.11001v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11001","description":"<p>Reliably and physically accurately transferring information between images\nthrough deformable image registration with large anatomical differences is an\nopen challenge in medical image analysis. Most existing methods have two key\nshortcomings: first, they require extensive up-front parameter tuning to each\nspecific registration problem, and second, they have difficulty capturing large\ndeformations and content mismatches between images. There have however been\ndevelopments that have laid the foundation for potential solutions to both\nshortcomings. Towards the first shortcoming, a multi-objective optimization\napproach using the Real-Valued Gene-pool Optimal Mixing Evolutionary Algorithm\n(RV-GOMEA) has been shown to be capable of producing a diverse set of\nregistrations for 2D images in one run of the algorithm, representing different\ntrade-offs between conflicting objectives in the registration problem. This\nallows the user to select a registration afterwards and removes the need for\nup-front tuning. Towards the second shortcoming, a dual-dynamic grid\ntransformation model has proven effective at capturing large differences in 2D\nimages. These two developments have recently been accelerated through GPU\nparallelization, delivering large speed-ups. Based on this accelerated version,\nit is now possible to extend the approach to 3D images. Concordantly, this work\nintroduces the first method for multi-objective 3D deformable image\nregistration, using a 3D dual-dynamic grid transformation model based on\nsimplex meshes while still supporting the incorporation of annotated guidance\ninformation and multi-resolution schemes. Our proof-of-concept prototype shows\npromising results on synthetic and clinical 3D registration problems, forming\nthe foundation for a new, insightful method that can include bio-mechanical\nproperties in the registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andreadis_G/0/1/0/all/0/1\">Georgios Andreadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A.N. Bosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical and Spatio-temporal Hand Gesture Features for Sign Language Recognition using the Leap Motion Sensor. (arXiv:2202.11005v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11005","description":"<p>In modern society, people should not be identified based on their disability,\nrather, it is environments that can disable people with impairments.\nImprovements to automatic Sign Language Recognition (SLR) will lead to more\nenabling environments via digital technology. Many state-of-the-art approaches\nto SLR focus on the classification of static hand gestures, but communication\nis a temporal activity, which is reflected by many of the dynamic gestures\npresent. Given this, temporal information during the delivery of a gesture is\nnot often considered within SLR. The experiments in this work consider the\nproblem of SL gesture recognition regarding how dynamic gestures change during\ntheir delivery, and this study aims to explore how single types of features as\nwell as mixed features affect the classification ability of a machine learning\nmodel. 18 common gestures recorded via a Leap Motion Controller sensor provide\na complex classification problem. Two sets of features are extracted from a 0.6\nsecond time window, statistical descriptors and spatio-temporal attributes.\nFeatures from each set are compared by their ANOVA F-Scores and p-values,\narranged into bins grown by 10 features per step to a limit of the 250\nhighest-ranked features. Results show that the best statistical model selected\n240 features and scored 85.96% accuracy, the best spatio-temporal model\nselected 230 features and scored 80.98%, and the best mixed-feature model\nselected 240 features from each set leading to a classification accuracy of\n86.75%. When all three sets of results are compared (146 individual machine\nlearning models), the overall distribution shows that the minimum results are\nincreased when inputs are any number of mixed features compared to any number\nof either of the two single sets of features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1\">Jordan J. Bird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computing Multiple Image Reconstructions with a Single Hypernetwork. (arXiv:2202.11009v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11009","description":"<p>Deep learning based techniques achieve state-of-the-art results in a wide\nrange of image reconstruction tasks like compressed sensing. These methods\nalmost always have hyperparameters, such as the weight coefficients that\nbalance the different terms in the optimized loss function. The typical\napproach is to train the model for a hyperparameter setting determined with\nsome empirical or theoretical justification. Thus, at inference time, the model\ncan only compute reconstructions corresponding to the pre-determined\nhyperparameter values. In this work, we present a hypernetwork based approach,\ncalled HyperRecon, to train reconstruction models that are agnostic to\nhyperparameter settings. At inference time, HyperRecon can efficiently produce\ndiverse reconstructions, which would each correspond to different\nhyperparameter values. In this framework, the user is empowered to select the\nmost useful output(s) based on their own judgement. We demonstrate our method\nin compressed sensing, super-resolution and denoising tasks, using two\nlarge-scale and publicly-available MRI datasets. Our code is available at\nhttps://github.com/alanqrwang/hyperrecon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alan Q. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Visual-Inertial Localization With Application And Benchmark in Laparoscopic Surgery. (arXiv:2202.11075v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11075","description":"<p>We propose a novel method to tackle the visual-inertial localization problem\nfor constrained camera movements. We use residuals from the different\nmodalities to jointly optimize a global cost function. The residuals emerge\nfrom IMU measurements, stereoscopic feature points, and constraints on possible\nsolutions in SE(3). In settings where dynamic disturbances are frequent, the\nresiduals reduce the complexity of the problem and make localization feasible.\nWe verify the advantages of our method in a suitable medical use case and\nproduce a dataset capturing a minimally invasive surgery in the abdomen. Our\nnovel clinical dataset MITI is comparable to state-of-the-art evaluation\ndatasets, contains calibration and synchronization and is available at\nhttps://mediatum.ub.tum.<a href=\"/abs/de/1621941\">de/1621941</a>.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartwig_R/0/1/0/all/0/1\">Regine Hartwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostler_D/0/1/0/all/0/1\">Daniel Ostler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenthal_J/0/1/0/all/0/1\">Jean-Claude Rosenthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feussner_H/0/1/0/all/0/1\">Hubertus Feu&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilhelm_D/0/1/0/all/0/1\">Dirk Wilhelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollherr_D/0/1/0/all/0/1\">Dirk Wollherr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReorientBot: Learning Object Reorientation for Specific-Posed Placement. (arXiv:2202.11092v1 [cs.RO])","link":"http://arxiv.org/abs/2202.11092","description":"<p>Robots need the capability of placing objects in arbitrary, specific poses to\nrearrange the world and achieve various valuable tasks. Object reorientation\nplays a crucial role in this as objects may not initially be oriented such that\nthe robot can grasp and then immediately place them in a specific goal pose. In\nthis work, we present a vision-based manipulation system, ReorientBot, which\nconsists of 1) visual scene understanding with pose estimation and volumetric\nreconstruction using an onboard RGB-D camera; 2) learned waypoint selection for\nsuccessful and efficient motion generation for reorientation; 3) traditional\nmotion planning to generate a collision-free trajectory from the selected\nwaypoints. We evaluate our method using the YCB objects in both simulation and\nthe real world, achieving 93% overall success, 81% improvement in success rate,\nand 22% improvement in execution time compared to a heuristic approach. We\ndemonstrate extended multi-object rearrangement showing the general capability\nof the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1\">Kentaro Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroupViT: Semantic Segmentation Emerges from Text Supervision. (arXiv:2202.11094v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11094","description":"<p>Grouping and recognition are important components of visual scene\nunderstanding, e.g., for object detection and semantic segmentation. With\nend-to-end deep learning systems, grouping of image regions usually happens\nimplicitly via top-down supervision from pixel-level recognition labels.\nInstead, in this paper, we propose to bring back the grouping mechanism into\ndeep networks, which allows semantic segments to emerge automatically with only\ntext supervision. We propose a hierarchical Grouping Vision Transformer\n(GroupViT), which goes beyond the regular grid structure representation and\nlearns to group image regions into progressively larger arbitrary-shaped\nsegments. We train GroupViT jointly with a text encoder on a large-scale\nimage-text dataset via contrastive losses. With only text supervision and\nwithout any pixel-level annotations, GroupViT learns to group together semantic\nregions and successfully transfers to the task of semantic segmentation in a\nzero-shot manner, i.e., without any further fine-tuning. It achieves a\nzero-shot accuracy of 51.2% mIoU on the PASCAL VOC 2012 and 22.3% mIoU on\nPASCAL Context datasets, and performs competitively to state-of-the-art\ntransfer-learning methods requiring greater levels of supervision. Project page\nis available at https://jerryxu.net/GroupViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaDIS: Cataract Dataset for Image Segmentation. (arXiv:1906.11586v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1906.11586","description":"<p>Video feedback provides a wealth of information about surgical procedures and\nis the main sensory cue for surgeons. Scene understanding is crucial to\ncomputer assisted interventions (CAI) and to post-operative analysis of the\nsurgical procedure. A fundamental building block of such capabilities is the\nidentification and localization of surgical instruments and anatomical\nstructures through semantic segmentation. Deep learning has advanced semantic\nsegmentation techniques in the recent years but is inherently reliant on the\navailability of labelled datasets for model training. This paper introduces a\ndataset for semantic segmentation of cataract surgery videos complementing the\npublicly available CATARACTS challenge dataset. In addition, we benchmark the\nperformance of several state-of-the-art deep learning models for semantic\nsegmentation on the presented dataset. The dataset is publicly available at\nhttps://cataracts-semantic-segmentation2020.grand-challenge.org/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grammatikopoulou_M/0/1/0/all/0/1\">Maria Grammatikopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flouty_E/0/1/0/all/0/1\">Evangello Flouty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadkhodamohammadi_A/0/1/0/all/0/1\">Abdolrahim Kadkhodamohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quellec_G/0/1/0/all/0/1\">Gwenol&#x27;e Quellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_A/0/1/0/all/0/1\">Andre Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nehme_J/0/1/0/all/0/1\">Jean Nehme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luengo_I/0/1/0/all/0/1\">Imanol Luengo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Convolutional RNN for MRI Image Reconstruction. (arXiv:1912.00543v7 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1912.00543","description":"<p>Fast and accurate MRI image reconstruction from undersampled data is crucial\nin clinical practice. Deep learning based reconstruction methods have shown\npromising advances in recent years. However, recovering fine details from\nundersampled data is still challenging. In this paper, we introduce a novel\ndeep learning based method, Pyramid Convolutional RNN (PC-RNN), to reconstruct\nimages from multiple scales. Based on the formulation of MRI reconstruction as\nan inverse problem, we design the PC-RNN model with three convolutional RNN\n(ConvRNN) modules to iteratively learn the features in multiple scales. Each\nConvRNN module reconstructs images at different scales and the reconstructed\nimages are combined by a final CNN module in a pyramid fashion. The multi-scale\nConvRNN modules learn a coarse-to-fine image reconstruction. Unlike other\ncommon reconstruction methods for parallel imaging, PC-RNN does not employ coil\nsensitive maps for multi-coil data and directly model the multiple coils as\nmulti-channel inputs. The coil compression technique is applied to standardize\ndata with various coil numbers, leading to more efficient training. We evaluate\nour model on the fastMRI knee and brain datasets and the results show that the\nproposed model outperforms other methods and can recover more details. The\nproposed method is one of the winner solutions in the 2019 fastMRI competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1\">Eric Z. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Puyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_S/0/1/0/all/0/1\">Shanhui Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization. (arXiv:1912.07109v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.07109","description":"<p>We propose SDFDiff, a novel approach for image-based shape optimization using\ndifferentiable rendering of 3D shapes represented by signed distance functions\n(SDFs). Compared to other representations, SDFs have the advantage that they\ncan represent shapes with arbitrary topology, and that they guarantee\nwatertight surfaces. We apply our approach to the problem of multi-view 3D\nreconstruction, where we achieve high reconstruction quality and can capture\ncomplex topology of 3D objects. In addition, we employ a multi-resolution\nstrategy to obtain a robust optimization algorithm. We further demonstrate that\nour SDF-based differentiable renderer can be integrated with deep learning\nmodels, which opens up options for learning approaches on 3D objects without 3D\nsupervision. In particular, we apply our method to single-view 3D\nreconstruction and achieve state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Dantong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Defense by Latent Style Transformations. (arXiv:2006.09701v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.09701","description":"<p>Machine learning models have demonstrated vulnerability to adversarial\nattacks, more specifically misclassification of adversarial examples.\n</p>\n<p>In this paper, we investigate an attack-agnostic defense against adversarial\nattacks on high-resolution images by detecting suspicious inputs.\n</p>\n<p>The intuition behind our approach is that the essential characteristics of a\nnormal image are generally consistent with non-essential style transformations,\ne.g., slightly changing the facial expression of human portraits.\n</p>\n<p>In contrast, adversarial examples are generally sensitive to such\ntransformations.\n</p>\n<p>In our approach to detect adversarial instances, we propose an\nin\\underline{V}ertible \\underline{A}utoencoder based on the\n\\underline{S}tyleGAN2 generator via \\underline{A}dversarial training (VASA) to\ninverse images to disentangled latent codes that reveal hierarchical styles.\n</p>\n<p>We then build a set of edited copies with non-essential style transformations\nby performing latent shifting and reconstruction, based on the correspondences\nbetween latent codes and style transformations.\n</p>\n<p>The classification-based consistency of these edited copies is used to\ndistinguish adversarial instances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1\">Surya Nepal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1\">Alsharif Abuadbba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_C/0/1/0/all/0/1\">Carsten Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grobler_M/0/1/0/all/0/1\">Marthie Grobler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking Passengers and Baggage Items using Multi-camera Systems at Security Checkpoints. (arXiv:2007.07924v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.07924","description":"<p>We introduce a novel tracking-by-detection framework to track multiple\nobjects in overhead camera videos for airport checkpoint security scenarios\nwhere targets correspond to passengers and their baggage items. Our approach\nimproves object detection by employing a test-time data augmentation procedure\nthat provides multiple geometrically transformed images as inputs to a\nconvolutional neural network. We cluster the multiple detections generated by\nthe network using the mean-shift algorithm. The multiple hypothesis tracking\nalgorithm then keeps track of the temporal identifiers of the targets based on\nthe cluster centroids. Our method also incorporates a trajectory association\nmechanism to maintain the consistency of the temporal identifiers as passengers\ntravel across camera views. Finally, we also introduce a simple distance-based\nmatching mechanism to associate passengers with their luggage. An evaluation of\ndetection, tracking, and association performances on videos obtained from\nmultiple overhead cameras in a realistic airport checkpoint environment\ndemonstrates the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddique_A/0/1/0/all/0/1\">Abubakar Siddique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_H/0/1/0/all/0/1\">Henry Medeiros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real-Time Predictive Pedestrian Collision Warning Service for Cooperative Intelligent Transportation Systems Using 3D Pose Estimation. (arXiv:2009.10868v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.10868","description":"<p>Minimizing traffic accidents between vehicles and pedestrians is one of the\nprimary research goals in intelligent transportation systems. To achieve the\ngoal, pedestrian orientation recognition and prediction of pedestrian's\ncrossing or not-crossing intention play a central role. Contemporary approaches\ndo not guarantee satisfactory performance due to limited field-of-view, lack of\ngeneralization, and high computational complexity. To overcome these\nlimitations, we propose a real-time predictive pedestrian collision warning\nservice (P2CWS) for two tasks: pedestrian orientation recognition (100.53 FPS)\nand intention prediction (35.76 FPS). Our framework obtains satisfying\ngeneralization over multiple sites because of the proposed site-independent\nfeatures. At the center of the feature extraction lies 3D pose estimation. The\n3D pose analysis enables robust and accurate recognition of pedestrian\norientations and prediction of intentions over multiple sites. The proposed\nvision framework realizes 89.3% accuracy in the behavior recognition task on\nthe TUD dataset without any training process and 91.28% accuracy in intention\nprediction on our dataset achieving new state-of-the-art performance. To\ncontribute to the corresponding research community, we make our source codes\npublic which are available at https://github.com/Uehwan/VisionForPedestrian\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_U/0/1/0/all/0/1\">Ue-Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ka_D/0/1/0/all/0/1\">Dongho Ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Hwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Child-Computer Interaction with Mobile Devices: Recent Works, New Dataset, and Age Detection. (arXiv:2102.01405v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2102.01405","description":"<p>This article provides an overview of recent research in Child-Computer\nInteraction with mobile devices and describe our framework ChildCI intended\nfor: i) overcoming the lack of large-scale publicly available databases in the\narea, ii) generating a better understanding of the cognitive and neuromotor\ndevelopment of children along time, contrary to most previous studies in the\nliterature focused on a single-session acquisition, and iii) enabling new\napplications in e-Learning and e-Health through the acquisition of additional\ninformation such as the school grades and children's disorders, among others.\nOur framework includes a new mobile application, specific data acquisition\nprotocols, and a first release of the ChildCI dataset (ChildCIdb v1), which is\nplanned to be extended yearly to enable longitudinal studies.\n</p>\n<p>In our framework children interact with a tablet device, using both a pen\nstylus and the finger, performing different tasks that require different levels\nof neuromotor and cognitive skills. ChildCIdb is the first database in the\nliterature that comprises more than 400 children from 18 months to 8 years old,\nconsidering therefore the first three development stages of the Piaget's\ntheory. In addition, and as a demonstration of the potential of the ChildCI\nframework, we include experimental results for one of the many applications\nenabled by ChildCIdb: children age detection based on device interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_J/0/1/0/all/0/1\">Juan Carlos Ruiz-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herreros_Rodriguez_J/0/1/0/all/0/1\">Jaime Herreros-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Tapiador_S/0/1/0/all/0/1\">Sergio Romero-Tapiador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANav: Efficient Terrain Segmentation for Robot Navigation in Unstructured Outdoor Environments. (arXiv:2103.04233v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.04233","description":"<p>We propose GANav, a novel group-wise attention mechanism to identify safe and\nnavigable regions in off-road terrains and unstructured environments from RGB\nimages. Our approach classifies terrains based on their navigability levels\nusing coarse-grained semantic segmentation. Our novel group-wise attention loss\nenables any backbone network to explicitly focus on the different groups'\nfeatures with low spatial resolution. Our design leads to efficient inference\nwhile maintaining a high level of accuracy compared to existing SOTA methods.\nOur extensive evaluations on the RUGD and RELLIS-3D datasets shows that GANav\nachieves an improvement over the SOTA mIoU by 2.25-39.05% on RUGD and\n5.17-19.06% on RELLIS-3D. We interface GANav with a deep reinforcement\nlearning-based navigation algorithm and highlight its benefits in terms of\nnavigation in real-world unstructured terrains. We integrate our GANav-based\nnavigation algorithm with ClearPath Jackal and Husky robots, and observe an\nincrease of 10% in terms of success rate, 2-47% in terms of selecting the\nsurface with the best navigability and a decrease of 4.6-13.9% in trajectory\nroughness. Further, GANav reduces the false positive rate of forbidden regions\nby 37.79%. Code, videos, and a full technical report are available at\nhttps://gamma.umd.edu/offroad/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1\">Tianrui Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1\">Divya Kothandaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyamoorthy_A/0/1/0/all/0/1\">Adarsh Jagan Sathyamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerakoon_K/0/1/0/all/0/1\">Kasun Weerakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Illumination based Depth Sensing using Deep Superpixel and Soft Sampling Approximation. (arXiv:2103.12297v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12297","description":"<p>Dense depth map capture is challenging in existing active sparse illumination\nbased depth acquisition techniques, such as LiDAR. Various techniques have been\nproposed to estimate a dense depth map based on fusion of the sparse depth map\nmeasurement with the RGB image. Recent advances in hardware enable adaptive\ndepth measurements resulting in further improvement of the dense depth map\nestimation. In this paper, we study the topic of estimating dense depth from\ndepth sampling. The adaptive sparse depth sampling network is jointly trained\nwith a fusion network of an RGB image and sparse depth, to generate optimal\nadaptive sampling masks. We show that such adaptive sampling masks can\ngeneralize well to many RGB and sparse depth fusion algorithms under a variety\nof sampling rates (as low as $0.0625\\%$). The proposed adaptive sampling method\nis fully differentiable and flexible to be trained end-to-end with upstream\nperception algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qiqin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1\">Oliver Cossairt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Expression Recognition with Visual Transformers and Attentional Selective Fusion. (arXiv:2103.16854v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16854","description":"<p>Facial Expression Recognition (FER) in the wild is extremely challenging due\nto occlusions, variant head poses, face deformation and motion blur under\nunconstrained conditions. Although substantial progresses have been made in\nautomatic FER in the past few decades, previous studies were mainly designed\nfor lab-controlled FER. Real-world occlusions, variant head poses and other\nissues definitely increase the difficulty of FER on account of these\ninformation-deficient regions and complex backgrounds. Different from previous\npure CNNs based methods, we argue that it is feasible and practical to\ntranslate facial images into sequences of visual words and perform expression\nrecognition from a global perspective. Therefore, we propose the Visual\nTransformers with Feature Fusion (VTFF) to tackle FER in the wild by two main\nsteps. First, we propose the attentional selective fusion (ASF) for leveraging\ntwo kinds of feature maps generated by two-branch CNNs. The ASF captures\ndiscriminative information by fusing multiple features with the global-local\nattention. The fused feature maps are then flattened and projected into\nsequences of visual words. Second, inspired by the success of Transformers in\nnatural language processing, we propose to model relationships between these\nvisual words with the global self-attention. The proposed method is evaluated\non three public in-the-wild facial expression datasets (RAF-DB, FERPlus and\nAffectNet). Under the same settings, extensive experiments demonstrate that our\nmethod shows superior performance over other methods, setting new state of the\nart on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. The\ncross-dataset evaluation on CK+ shows the promising generalization capability\nof the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fuyan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fine-grained Visual Representations by Combining Contrastive Learning with Image Reconstruction and Attention-weighted Pooling. (arXiv:2104.04323v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04323","description":"<p>This paper presents Contrastive Reconstruction, ConRec - a self-supervised\nlearning algorithm that obtains image representations by jointly optimizing a\ncontrastive and a self-reconstruction loss. We showcase that state-of-the-art\ncontrastive learning methods (e.g. SimCLR) have shortcomings to capture\nfine-grained visual features in their representations. ConRec extends the\nSimCLR framework by adding (1) a self-reconstruction task and (2) an attention\nmechanism within the contrastive learning task. This is accomplished by\napplying a simple encoder-decoder architecture with two heads. We show that\nboth extensions contribute towards an improved vector representation for images\nwith fine-grained visual features. Combining those concepts, ConRec outperforms\nSimCLR and SimCLR with Attention-Pooling on fine-grained classification\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dippel_J/0/1/0/all/0/1\">Jonas Dippel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogler_S/0/1/0/all/0/1\">Steffen Vogler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohne_J/0/1/0/all/0/1\">Johannes H&#xf6;hne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-invariant scale-channel networks: Deep networks that generalise to previously unseen scales. (arXiv:2106.06418v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06418","description":"<p>The ability to handle large scale variations is crucial for many real world\nvisual tasks. A straightforward approach for handling scale in a deep network\nis to process an image at several scales simultaneously in a set of scale\nchannels. Scale invariance can then, in principle, be achieved by using weight\nsharing between the scale channels together with max or average pooling over\nthe outputs from the scale channels. The ability of such scale channel networks\nto generalise to scales not present in the training set over significant scale\nranges has, however, not previously been explored.\n</p>\n<p>In this paper, we present a systematic study of this methodology by\nimplementing different types of scale channel networks and evaluating their\nability to generalise to previously unseen scales. We develop a formalism for\nanalysing the covariance and invariance properties of scale channel networks,\nand explore how different design choices, unique to scaling transformations,\naffect the overall performance of scale channel networks. We first show that\ntwo previously proposed scale channel network designs do not generalise well to\nscales not present in the training set. We explain theoretically and\ndemonstrate experimentally why generalisation fails in these cases.\n</p>\n<p>We then propose a new type of foveated scale channel architecture}, where the\nscale channels process increasingly larger parts of the image with decreasing\nresolution. This new type of scale channel network is shown to generalise\nextremely well, provided sufficient image resolution and the absence of\nboundary effects. Our proposed FovMax and FovAvg networks perform almost\nidentically over a scale range of 8, also when training on single scale\ntraining data, and do also give improved performance when learning from\ndatasets with large scale variations in the small sample regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jansson_Y/0/1/0/all/0/1\">Ylva Jansson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1\">Tony Lindeberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03823","description":"<p>Deep learning-based computer-aided diagnosis is gradually deployed to review\nand analyze medical images. However, this paradigm is restricted in real-world\nclinical applications due to the poor robustness and generalization. The issue\nis more sinister with a lack of training data. In this paper, we address the\nchallenge from the transfer learning point of view. Different from the common\nsetting that transferring knowledge from the natural image domain to the\nmedical image domain, we find the knowledge from the same domain further boosts\nthe model robustness and generalization. Therefore, we propose a novel\ntwo-stage framework for robust generalized medical image segmentation. Firstly,\nan unsupervised tile-wise autoencoder pretraining architecture is proposed to\nlearn local and global knowledge. Secondly, the downstream segmentation model\ncoupled with an auxiliary reconstruction network is designed. The\nreconstruction branch encourages the model to capture more general semantic\nfeatures. Experiments of lung segmentation on multi chest X-ray datasets are\nconducted. Comprehensive results demonstrate the superior robustness of the\nproposed framework to corruption and high generalization performance on unseen\ndatasets, especially under the scenario of the limited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVC-onGoing: Signature Verification Competition. (arXiv:2108.06090v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06090","description":"<p>This article presents SVC-onGoing, an on-going competition for on-line\nsignature verification where researchers can easily benchmark their systems\nagainst the state of the art in an open common platform using large-scale\npublic databases, such as DeepSignDB and SVC2021_EvalDB, and standard\nexperimental protocols. SVC-onGoing is based on the ICDAR 2021 Competition on\nOn-Line Signature Verification (SVC 2021), which has been extended to allow\nparticipants anytime. The goal of SVC-onGoing is to evaluate the limits of\non-line signature verification systems on popular scenarios (office/mobile) and\nwriting inputs (stylus/finger) through large-scale public databases. Three\ndifferent tasks are considered in the competition, simulating realistic\nscenarios as both random and skilled forgeries are simultaneously considered on\neach task. The results obtained in SVC-onGoing prove the high potential of deep\nlearning methods in comparison with traditional methods. In particular, the\nbest signature verification system has obtained Equal Error Rate (EER) values\nof 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3). Future studies in the\nfield should be oriented to improve the performance of signature verification\nsystems on the challenging mobile scenarios of SVC-onGoing in which several\nmobile devices and the finger are used during the signature acquisition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Garcia_C/0/1/0/all/0/1\">Carlos Gonzalez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_J/0/1/0/all/0/1\">Juan Carlos Ruiz-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Tapiador_S/0/1/0/all/0/1\">Sergio Romero-Tapiador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rengifo_S/0/1/0/all/0/1\">Santiago Rengifo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caruana_M/0/1/0/all/0/1\">Miguel Caruana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiajia Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yecheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1\">Javier Galbally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Moises Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_M/0/1/0/all/0/1\">Miguel Angel Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Barrero_M/0/1/0/all/0/1\">Marta Gomez-Barrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodashinsky_I/0/1/0/all/0/1\">Ilya Hodashinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarin_K/0/1/0/all/0/1\">Konstantin Sarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slezkin_A/0/1/0/all/0/1\">Artem Slezkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardamova_M/0/1/0/all/0/1\">Marina Bardamova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svetlakov_M/0/1/0/all/0/1\">Mikhail Svetlakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_M/0/1/0/all/0/1\">Mohammad Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szucs_C/0/1/0/all/0/1\">Cintia Lia Szucs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovari_B/0/1/0/all/0/1\">Bence Kovari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulsmeyer_F/0/1/0/all/0/1\">Falk Pulsmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1\">Mohamad Wehbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1\">Dario Zanca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1\">Sumaiya Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sarthak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabin_S/0/1/0/all/0/1\">Suraiya Jabin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOTR: Face Landmark Localization Using Localization Transformer. (arXiv:2109.10057v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10057","description":"<p>This paper presents a novel Transformer-based facial landmark localization\nnetwork named Localization Transformer (LOTR). The proposed framework is a\ndirect coordinate regression approach leveraging a Transformer network to\nbetter utilize the spatial information in the feature map. An LOTR model\nconsists of three main modules: 1) a visual backbone that converts an input\nimage into a feature map, 2) a Transformer module that improves the feature\nrepresentation from the visual backbone, and 3) a landmark prediction head that\ndirectly predicts the landmark coordinates from the Transformer's\nrepresentation. Given cropped-and-aligned face images, the proposed LOTR can be\ntrained end-to-end without requiring any post-processing steps. This paper also\nintroduces the smooth-Wing loss function, which addresses the gradient\ndiscontinuity of the Wing loss, leading to better convergence than standard\nloss functions such as L1, L2, and Wing loss. Experimental results on the JD\nlandmark dataset provided by the First Grand Challenge of 106-Point Facial\nLandmark Localization indicate the superiority of LOTR over the existing\nmethods on the leaderboard and two recent heatmap-based approaches. On the WFLW\ndataset, the proposed LOTR framework demonstrates promising results compared\nwith several state-of-the-art methods. Additionally, we report the improvement\nin state-of-the-art face recognition performance when using our proposed LOTRs\nfor face alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Watchareeruetai_U/0/1/0/all/0/1\">Ukrit Watchareeruetai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommana_B/0/1/0/all/0/1\">Benjaphan Sommana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sanjana Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noinongyao_P/0/1/0/all/0/1\">Pavit Noinongyao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Ankush Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samacoits_A/0/1/0/all/0/1\">Aubin Samacoits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earp_S/0/1/0/all/0/1\">Samuel W.F. Earp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sritrakool_N/0/1/0/all/0/1\">Nakarin Sritrakool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Fusion Using Deep Image Priors. (arXiv:2110.09490v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09490","description":"<p>A significant number of researchers have applied deep learning methods to\nimage fusion. However, most works require a large amount of training data or\ndepend on pre-trained models or frameworks to capture features from source\nimages. This is inevitably hampered by a shortage of training data or a\nmismatch between the framework and the actual problem. Deep Image Prior (DIP)\nhas been introduced to exploit convolutional neural networks' ability to\nsynthesize the 'prior' in the input image. However, the original design of DIP\nis hard to be generalized to multi-image processing problems, particularly for\nimage fusion. Therefore, we propose a new image fusion technique that extends\nDIP to fusion tasks formulated as inverse problems. Additionally, we apply a\nmulti-channel approach to enhance DIP's effect further. The evaluation is\nconducted with several commonly used image fusion assessment metrics. The\nresults are compared with state-of-the-art image fusion methods. Our method\noutperforms these techniques for a range of metrics. In particular, it is shown\nto provide the best objective results for most metrics when applied to medical\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xudong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_P/0/1/0/all/0/1\">Paul Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achim_A/0/1/0/all/0/1\">Alin Achim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB Camera-based Physiological Sensing: Challenges and Future Directions. (arXiv:2110.13362v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13362","description":"<p>Numerous real-world applications have been driven by the recent algorithmic\nadvancement of artificial intelligence (AI). Healthcare is no exception and AI\ntechnologies have great potential to revolutionize the industry. Non-contact\ncamera-based physiological sensing, including remote photoplethysmography\n(rPPG), is a set of imaging methods that leverages ordinary RGB cameras (e.g.,\nwebcam or smartphone camera) to capture subtle changes in electromagnetic\nradiation (e.g., light) reflected by the body caused by physiological\nprocesses. RGB camera-based systems not only have the ability to measure the\nsignals without contact with the body but also have the opportunity to capture\nmultimodal information (e.g., facial expressions, activities and other context)\nfrom the same sensor. However, developing accessible, equitable and useful\ncamera-based physiological sensing systems comes with various challenges. In\nthis article, we identify four research challenges for the field of RGB\ncamera-based physiological sensing and broader AI driven healthcare communities\nand suggest future directions to tackle these. We believe solving these\nchallenges will help deliver accurate, equitable and generalizable AI systems\nfor healthcare that are practical in real-world and clinical contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shwetak Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Look at Spike-Timing-Dependent Plasticity Networks for Spatio-Temporal Feature Learning. (arXiv:2111.00791v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00791","description":"<p>We present new theoretical foundations for unsupervised\nSpike-Timing-Dependent Plasticity (STDP) learning in spiking neural networks\n(SNNs). In contrast to empirical parameter search used in most previous works,\nwe provide novel theoretical grounds for SNN and STDP parameter tuning which\nconsiderably reduces design time. Using our generic framework, we propose a\nclass of global, action-based and convolutional SNN-STDP architectures for\nlearning spatio-temporal features from event-based cameras. We assess our\nmethods on the N-MNIST, the CIFAR10-DVS and the IBM DVS128 Gesture datasets,\nall acquired with a real-world event camera. Using our framework, we report\nsignificant improvements in classification accuracy compared to both\nconventional state-of-the-art event-based feature descriptors (+8.2% on\nCIFAR10-DVS), and compared to state-of-the-art STDP-based systems (+9.3% on\nN-MNIST, +7.74% on IBM DVS128 Gesture). Our work contributes to both\nultra-low-power learning in neuromorphic edge devices, and towards a\nbiologically-plausible, optimization-based theory of cortical vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safa_A/0/1/0/all/0/1\">Ali Safa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ocket_I/0/1/0/all/0/1\">Ilja Ocket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourdoux_A/0/1/0/all/0/1\">Andr&#xe9; Bourdoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahli_H/0/1/0/all/0/1\">Hichem Sahli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catthoor_F/0/1/0/all/0/1\">Francky Catthoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gielen_G/0/1/0/all/0/1\">Georges Gielen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-Preserving Graph Kernel for Brain Network Classification. (arXiv:2111.10803v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.10803","description":"<p>This paper presents a novel graph-based kernel learning approach for\nconnectome analysis. Specifically, we demonstrate how to leverage the naturally\navailable structure within the graph representation to encode prior knowledge\nin the kernel. We first proposed a matrix factorization to directly extract\nstructural features from natural symmetric graph representations of connectome\ndata. We then used them to derive a structure-persevering graph kernel to be\nfed into the support vector machine. The proposed approach has the advantage of\nbeing clinically interpretable. Quantitative evaluations on challenging HIV\ndisease classification (DTI- and fMRI-derived connectome data) and emotion\nrecognition (EEG-derived connectome data) tasks demonstrate the superior\nperformance of our proposed methods against the state-of-the-art. Results\nshowed that relevant EEG-connectome information is primarily encoded in the\nalpha band during the emotion regulation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_Z/0/1/0/all/0/1\">Zhaoming Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kendre_A/0/1/0/all/0/1\">Aditya Kendre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leow_A/0/1/0/all/0/1\">Alex Leow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Texture Recognition using PDV Hashing and Dictionary Learning on Multi-scale Volume Local Binary Pattern. (arXiv:2111.12315v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12315","description":"<p>Spatial-temporal local binary pattern (STLBP) has been widely used in dynamic\ntexture recognition. STLBP often encounters the high-dimension problem as its\ndimension increases exponentially, so that STLBP could only utilize a small\nneighborhood. To tackle this problem, we propose a method for dynamic texture\nrecognition using PDV hashing and dictionary learning on multi-scale volume\nlocal binary pattern (PHD-MVLBP). Instead of forming very high-dimensional LBP\nhistogram features, it first uses hash functions to map the pixel difference\nvectors (PDVs) to binary vectors, then forms a dictionary using the derived\nbinary vector, and encodes them using the derived dictionary. In such a way,\nthe PDVs are mapped to feature vectors of the size of dictionary, instead of\nLBP histograms of very high dimension. Such an encoding scheme could extract\nthe discriminant information from videos in a much larger neighborhood\neffectively. The experimental results on two widely-used dynamic textures\ndatasets, DynTex++ and UCLA, show the superiority performance of the proposed\napproach over the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruxin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jianfeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Heng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Track Boosting and Synthetic Data Aided Drone Detection. (arXiv:2111.12389v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12389","description":"<p>This is the paper for the first place winning solution of the Drone vs. Bird\nChallenge, organized by AVSS 2021. As the usage of drones increases with\nlowered costs and improved drone technology, drone detection emerges as a vital\nobject detection task. However, detecting distant drones under unfavorable\nconditions, namely weak contrast, long-range, low visibility, requires\neffective algorithms. Our method approaches the drone detection problem by\nfine-tuning a YOLOv5 model with real and synthetically generated data using a\nKalman-based object tracker to boost detection confidence. Our results indicate\nthat augmenting the real data with an optimal subset of synthetic data can\nincrease the performance. Moreover, temporal information gathered by object\ntracking methods can increase performance further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1\">Fatih Cagatay Akyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eryuksel_O/0/1/0/all/0/1\">Ogulcan Eryuksel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozfuttu_K/0/1/0/all/0/1\">Kamil Anil Ozfuttu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altinuc_S/0/1/0/all/0/1\">Sinan Onur Altinuc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"n-CPS: Generalising Cross Pseudo Supervision to n Networks for Semi-Supervised Semantic Segmentation. (arXiv:2112.07528v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07528","description":"<p>We present n-CPS - a generalisation of the recent state-of-the-art cross\npseudo supervision (CPS) approach for the task of semi-supervised semantic\nsegmentation. In n-CPS, there are n simultaneously trained subnetworks that\nlearn from each other through one-hot encoding perturbation and consistency\nregularisation. We also show that ensembling techniques applied to subnetworks\noutputs can significantly improve the performance. To the best of our\nknowledge, n-CPS paired with CutMix outperforms CPS and sets the new\nstate-of-the-art for Pascal VOC 2012 with (1/16, 1/8, 1/4, and 1/2 supervised\nregimes) and Cityscapes (1/16 supervised).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Filipiak_D/0/1/0/all/0/1\">Dominik Filipiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tempczyk_P/0/1/0/all/0/1\">Piotr Tempczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cygan_M/0/1/0/all/0/1\">Marek Cygan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nearest neighbor search with compact codes: A decoder perspective. (arXiv:2112.09568v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09568","description":"<p>Modern approaches for fast retrieval of similar vectors on billion-scaled\ndatasets rely on compressed-domain approaches such as binary sketches or\nproduct quantization. These methods minimize a certain loss, typically the mean\nsquared error or other objective functions tailored to the retrieval problem.\nIn this paper, we re-interpret popular methods such as binary hashing or\nproduct quantizers as auto-encoders, and point out that they implicitly make\nsuboptimal assumptions on the form of the decoder. We design\nbackward-compatible decoders that improve the reconstruction of the vectors\nfrom the same codes, which translates to a better performance in nearest\nneighbor search. Our method significantly improves over binary hashing methods\nor product quantization on popular benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amara_K/0/1/0/all/0/1\">Kenza Amara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sablayrolles_A/0/1/0/all/0/1\">Alexandre Sablayrolles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; J&#xe9;gou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to scale hyperparameters for quickshift image segmentation. (arXiv:2201.09286v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09286","description":"<p>Quickshift is a popular algorithm for image segmentation, used as a\npreprocessing step in many applications. Unfortunately, it is quite challenging\nto understand the hyperparameters' influence on the number and shape of\nsuperpixels produced by the method. In this paper, we study theoretically a\nslightly modified version of the quickshift algorithm, with a particular\nemphasis on homogeneous image patches with i.i.d. pixel noise and sharp\nboundaries between such patches. Leveraging this analysis, we derive a simple\nheuristic to scale quickshift hyperparameters with respect to the image size,\nwhich we check empirically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal unsupervised brain image registration using edge maps. (arXiv:2202.04647v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.04647","description":"<p>Diffeomorphic deformable multi-modal image registration is a challenging task\nwhich aims to bring images acquired by different modalities to the same\ncoordinate space and at the same time to preserve the topology and the\ninvertibility of the transformation. Recent research has focused on leveraging\ndeep learning approaches for this task as these have been shown to achieve\ncompetitive registration accuracy while being computationally more efficient\nthan traditional iterative registration methods. In this work, we propose a\nsimple yet effective unsupervised deep learning-based {\\em multi-modal} image\nregistration approach that benefits from auxiliary information coming from the\ngradient magnitude of the image, i.e. the image edges, during the training. The\nintuition behind this is that image locations with a strong gradient are\nassumed to denote a transition of tissues, which are locations of high\ninformation value able to act as a geometry constraint. The task is similar to\nusing segmentation maps to drive the training, but the edge maps are easier and\nfaster to acquire and do not require annotations. We evaluate our approach in\nthe context of registering multi-modal (T1w to T2w) magnetic resonance (MR)\nbrain images of different subjects using three different loss functions that\nare said to assist multi-modal registration, showing that in all cases the\nauxiliary information leads to better results without compromising the runtime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sideri_Lampretsa_V/0/1/0/all/0/1\">Vasiliki Sideri-Lampretsa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision. (arXiv:2202.08360v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08360","description":"<p>Discriminative self-supervised learning allows training models on any random\ngroup of internet images, and possibly recover salient information that helps\ndifferentiate between the images. Applied to ImageNet, this leads to object\ncentric features that perform on par with supervised features on most\nobject-centric downstream tasks. In this work, we question if using this\nability, we can learn any salient and more representative information present\nin diverse unbounded set of images from across the globe. To do so, we train\nmodels on billions of random images without any data pre-processing or prior\nassumptions about what we want the model to learn. We scale our model size to\ndense 10 billion parameters to avoid underfitting on a large data size. We\nextensively study and validate our model performance on over 50 benchmarks\nincluding fairness, robustness to distribution shift, geographical diversity,\nfine grained recognition, image copy detection and many image classification\ndatasets. The resulting model, not only captures well semantic information, it\nalso captures information about artistic style and learns salient information\nsuch as geolocations and multilingual word embeddings based on visual content\nonly. More importantly, we discover that such model is more robust, more fair,\nless harmful and less biased than supervised models or models trained on object\ncentric datasets such as ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Priya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duval_Q/0/1/0/all/0/1\">Quentin Duval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seessel_I/0/1/0/all/0/1\">Isaac Seessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagun_L/0/1/0/all/0/1\">Levent Sagun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time. (arXiv:2202.08614v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08614","description":"<p>Implicit neural representations such as Neural Radiance Field (NeRF) have\nfocused mainly on modeling static objects captured under multi-view settings\nwhere real-time rendering can be achieved with smart data structures, e.g.,\nPlenOctree. In this paper, we present a novel Fourier PlenOctree (FPO)\ntechnique to tackle efficient neural modeling and real-time rendering of\ndynamic scenes captured under the free-view video (FVV) setting. The key idea\nin our FPO is a novel combination of generalized NeRF, PlenOctree\nrepresentation, volumetric fusion and Fourier transform. To accelerate FPO\nconstruction, we present a novel coarse-to-fine fusion scheme that leverages\nthe generalizable NeRF technique to generate the tree via spatial blending. To\ntackle dynamic scenes, we tailor the implicit network to model the Fourier\ncoefficients of timevarying density and color attributes. Finally, we construct\nthe FPO and train the Fourier coefficients directly on the leaves of a union\nPlenOctree structure of the dynamic sequence. We show that the resulting FPO\nenables compact memory overload to handle dynamic objects and supports\nefficient fine-tuning. Extensive experiments show that the proposed method is\n3000 times faster than the original NeRF and achieves over an order of\nmagnitude acceleration over SOTA while preserving high visual quality for the\nfree-viewpoint rendering of unseen dynamic scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiakai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinhang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanshun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniSyn: Synthesizing 360 Videos with Wide-baseline Panoramas. (arXiv:2202.08752v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08752","description":"<p>Immersive maps such as Google Street View and Bing Streetside provide\ntrue-to-life views with a massive collection of panoramas. However, these\npanoramas are only available at sparse intervals along the path they are taken,\nresulting in visual discontinuities during navigation. Prior art in view\nsynthesis is usually built upon a set of perspective images, a pair of\nstereoscopic images, or a monocular image, but barely examines wide-baseline\npanoramas, which are widely adopted in commercial platforms to optimize\nbandwidth and storage usage. In this paper, we leverage the unique\ncharacteristics of wide-baseline panoramas and present OmniSyn, a novel\npipeline for 360{\\deg} view synthesis between wide-baseline panoramas. OmniSyn\npredicts omnidirectional depth maps using a spherical cost volume and a\nmonocular skip connection, renders meshes in 360{\\deg} images, and synthesizes\nintermediate views with a fusion network. We demonstrate the effectiveness of\nOmniSyn via comprehensive experimental results including comparison with the\nstate-of-the-art methods on CARLA and Matterport datasets, ablation studies,\nand generalization studies on street views. We envision our work may inspire\nfuture research for this unheeded real-world task and eventually produce a\nsmoother experience for navigating immersive maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">David Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hane_C/0/1/0/all/0/1\">Christian H&#xe4;ne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Danhang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_A/0/1/0/all/0/1\">Amitabh Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruofei Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Snowflake Point Deconvolution for Point Cloud Completion and Generation with Skip-Transformer. (arXiv:2202.09367v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09367","description":"<p>Most existing point cloud completion methods suffered from discrete nature of\npoint clouds and unstructured prediction of points in local regions, which\nmakes it hard to reveal fine local geometric details. To resolve this issue, we\npropose SnowflakeNet with Snowflake Point Deconvolution (SPD) to generate the\ncomplete point clouds. SPD models the generation of complete point clouds as\nthe snowflake-like growth of points, where the child points are progressively\ngenerated by splitting their parent points after each SPD. Our insight of\nrevealing detailed geometry is to introduce skip-transformer in SPD to learn\npoint splitting patterns which can fit local regions the best. Skip-transformer\nleverages attention mechanism to summarize the splitting patterns used in\nprevious SPD layer to produce the splitting in current SPD layer. The locally\ncompact and structured point clouds generated by SPD precisely reveal the\nstructure characteristic of 3D shape in local patches, which enables us to\npredict highly detailed geometries. Moreover, since SPD is a general operation,\nwhich is not limited to completion, we further explore the applications of SPD\non other generative tasks, including point cloud auto-encoding, generation,\nsingle image reconstruction and upsampling. Our experimental results outperform\nthe state-of-the-art methods under widely used benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAGE: SLAM with Appearance and Geometry Prior for Endoscopy. (arXiv:2202.09487v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09487","description":"<p>In endoscopy, many applications (e.g., surgical navigation) would benefit\nfrom a real-time method that can simultaneously track the endoscope and\nreconstruct the dense 3D geometry of the observed anatomy from a monocular\nendoscopic video. To this end, we develop a Simultaneous Localization and\nMapping system by combining the learning-based appearance and optimizable\ngeometry priors and factor graph optimization. The appearance and geometry\npriors are explicitly learned in an end-to-end differentiable training pipeline\nto master the task of pair-wise image alignment, one of the core components of\nthe SLAM system. In our experiments, the proposed SLAM system is shown to\nrobustly handle the challenges of texture scarceness and illumination variation\nthat are commonly seen in endoscopy. The system generalizes well to unseen\nendoscopes and subjects and performs favorably compared with a state-of-the-art\nfeature-based SLAM system. The code repository is available at\nhttps://github.com/lppllppl920/SAGE-SLAM.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingtong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1\">Masaru Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory D. Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-step Point Moving Paths. (arXiv:2202.09507v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09507","description":"<p>Point cloud completion concerns to predict missing part for incomplete 3D\nshapes. A common strategy is to generate complete shape according to incomplete\ninput. However, unordered nature of point clouds will degrade generation of\nhigh-quality 3D shapes, as detailed topology and structure of unordered points\nare hard to be captured during the generative process using an extracted latent\ncode. We address this problem by formulating completion as point cloud\ndeformation process. Specifically, we design a novel neural network, named\nPMP-Net++, to mimic behavior of an earth mover. It moves each point of\nincomplete input to obtain a complete point cloud, where total distance of\npoint moving paths (PMPs) should be the shortest. Therefore, PMP-Net++ predicts\nunique PMP for each point according to constraint of point moving distances.\nThe network learns a strict and unique correspondence on point-level, and thus\nimproves quality of predicted complete shape. Moreover, since moving points\nheavily relies on per-point features learned by network, we further introduce a\ntransformer-enhanced representation learning network, which significantly\nimproves completion performance of PMP-Net++. We conduct comprehensive\nexperiments in shape completion, and further explore application on point cloud\nup-sampling, which demonstrate non-trivial improvement of PMP-Net++ over\nstate-of-the-art point cloud completion/up-sampling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsity Winning Twice: Better Robust Generalization from More Efficient Training. (arXiv:2202.09844v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09844","description":"<p>Recent studies demonstrate that deep networks, even robustified by the\nstate-of-the-art adversarial training (AT), still suffer from large robust\ngeneralization gaps, in addition to the much more expensive training costs than\nstandard training. In this paper, we investigate this intriguing problem from a\nnew perspective, i.e., injecting appropriate forms of sparsity during\nadversarial training. We introduce two alternatives for sparse adversarial\ntraining: (i) static sparsity, by leveraging recent results from the lottery\nticket hypothesis to identify critical sparse subnetworks arising from the\nearly training; (ii) dynamic sparsity, by allowing the sparse subnetwork to\nadaptively adjust its connectivity pattern (while sticking to the same sparsity\nratio) throughout training. We find both static and dynamic sparse methods to\nyield win-win: substantially shrinking the robust generalization gap and\nalleviating the robust overfitting, meanwhile significantly saving training and\ninference FLOPs. Extensive experiments validate our proposals with multiple\nnetwork architectures on diverse datasets, including CIFAR-10/100 and\nTiny-ImageNet. For example, our methods reduce robust generalization gap and\noverfitting by 34.44% and 4.02%, with comparable robust/standard accuracy\nboosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with\nResNet-18. Besides, our approaches can be organically combined with existing\nregularizers, establishing new state-of-the-art results in AT. Codes are\navailable in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandra_S/0/1/0/all/0/1\">Santosh Balachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning. (arXiv:2202.09982v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09982","description":"<p>One of the key challenges in visual Reinforcement Learning (RL) is to learn\npolicies that can generalize to unseen environments. Recently, data\naugmentation techniques aiming at enhancing data diversity have demonstrated\nproven performance in improving the generalization ability of learned policies.\nHowever, due to the sensitivity of RL training, naively applying data\naugmentation, which transforms each pixel in a task-agnostic manner, may suffer\nfrom instability and damage the sample efficiency, thus further exacerbating\nthe generalization performance. At the heart of this phenomenon is the diverged\naction distribution and high-variance value estimation in the face of augmented\nimages. To alleviate this issue, we propose Task-aware Lipschitz Data\nAugmentation (TLDA) for visual RL, which explicitly identifies the\ntask-correlated pixels with large Lipschitz constants, and only augments the\ntask-irrelevant pixels. To verify the effectiveness of TLDA, we conduct\nextensive experiments on DeepMind Control suite, CARLA and DeepMind\nManipulation tasks, showing that TLDA improves both sample efficiency in\ntraining time and generalization in test time. It outperforms previous\nstate-of-the-art methods across the 3 different visual control benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhecheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guozheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yao Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huazhe Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic CT Skull Generation for Transcranial MR Imaging-Guided Focused Ultrasound Interventions with Conditional Adversarial Networks. (arXiv:2202.10136v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10136","description":"<p>Transcranial MRI-guided focused ultrasound (TcMRgFUS) is a therapeutic\nultrasound method that focuses sound through the skull to a small region\nnoninvasively under MRI guidance. It is clinically approved to thermally ablate\nregions of the thalamus and is being explored for other therapies, such as\nblood brain barrier opening and neuromodulation. To accurately target\nultrasound through the skull, the transmitted waves must constructively\ninterfere at the target region. However, heterogeneity of the sound speed,\ndensity, and ultrasound attenuation in different individuals' skulls requires\npatient-specific estimates of these parameters for optimal treatment planning.\nCT imaging is currently the gold standard for estimating acoustic properties of\nan individual skull during clinical procedures, but CT imaging exposes patients\nto radiation and increases the overall number of imaging procedures required\nfor therapy. A method to estimate acoustic parameters in the skull without the\nneed for CT would be desirable. Here, we synthesized CT images from routinely\nacquired T1-weighted MRI by using a 3D patch-based conditional generative\nadversarial network and evaluated the performance of synthesized CT images for\ntreatment planning with transcranial focused ultrasound. We compared the\nperformance of synthetic CT to real CT images using Kranion and k-Wave acoustic\nsimulation. Our work demonstrates the feasibility of replacing real CT with the\nMR-synthesized CT for TcMRgFUS planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigona_M/0/1/0/all/0/1\">Michelle K. Sigona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manuel_T/0/1/0/all/0/1\">Thomas J. Manuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caskey_C/0/1/0/all/0/1\">Charles F. Caskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawant_B/0/1/0/all/0/1\">Benoit M. Dawant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo Numerical Methods for Diffusion Models on Manifolds. (arXiv:2202.09778v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2202.09778","description":"<p>Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality\nsamples such as image and audio samples. However, DDPMs require hundreds to\nthousands of iterations to produce final samples. Several prior works have\nsuccessfully accelerated DDPMs through adjusting the variance schedule (e.g.,\nImproved Denoising Diffusion Probabilistic Models) or the denoising equation\n(e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these\nacceleration methods cannot maintain the quality of samples and even introduce\nnew noise at a high speedup rate, which limit their practicability. To\naccelerate the inference process while keeping the sample quality, we provide a\nfresh perspective that DDPMs should be treated as solving differential\nequations on manifolds. Under such a perspective, we propose pseudo numerical\nmethods for diffusion models (PNDMs). Specifically, we figure out how to solve\ndifferential equations on manifolds and show that DDIMs are simple cases of\npseudo numerical methods. We change several classical numerical methods to\ncorresponding pseudo numerical methods and find that the pseudo linear\nmulti-step method is the best in most situations. According to our experiments,\nby directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can\ngenerate higher quality synthetic images with only 50 steps compared with\n1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps\n(by around 0.4 in FID) and have good generalization on different variance\nschedules. Our implementation is available at\nhttps://github.com/luping-liu/PNDM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Luping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhijie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}