{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.6","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-11-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Deep Keyphrase Completion. (arXiv:2111.01910v1 [cs.IR])","link":"http://arxiv.org/abs/2111.01910","description":"<p>Keyphrase provides accurate information of document content that is highly\ncompact, concise, full of meanings, and widely used for discourse\ncomprehension, organization, and text retrieval. Though previous studies have\nmade substantial efforts for automated keyphrase extraction and generation,\nsurprisingly, few studies have been made for \\textit{keyphrase completion}\n(KPC). KPC aims to generate more keyphrases for document (e.g. scientific\npublication) taking advantage of document content along with a very limited\nnumber of known keyphrases, which can be applied to improve text indexing\nsystem, etc. In this paper, we propose a novel KPC method with an\nencoder-decoder framework. We name it \\textit{deep keyphrase completion} (DKPC)\nsince it attempts to capture the deep semantic meaning of the document content\ntogether with known keyphrases via a deep learning framework. Specifically, the\nencoder and the decoder in DKPC play different roles to make full use of the\nknown keyphrases. The former considers the keyphrase-guiding factors, which\naggregates information of known keyphrases into context. On the contrary, the\nlatter considers the keyphrase-inhibited factor to inhibit semantically\nrepeated keyphrase generation. Extensive experiments on benchmark datasets\ndemonstrate the efficacy of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Huali Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Advantages of Interactive and Non-Interactive Models for Vector-Based Cross-Lingual Information Retrieval. (arXiv:2111.01992v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01992","description":"<p>Interactive and non-interactive model are the two de-facto standard\nframeworks in vector-based cross-lingual information retrieval (V-CLIR), which\nembed queries and documents in synchronous and asynchronous fashions,\nrespectively. From the retrieval accuracy and computational efficiency\nperspectives, each model has its own superiority and shortcoming. In this\npaper, we propose a novel framework to leverage the advantages of these two\nparadigms. Concretely, we introduce semi-interactive mechanism, which builds\nour model upon non-interactive architecture but encodes each document together\nwith its associated multilingual queries. Accordingly, cross-lingual features\ncan be better learned like an interactive model. Besides, we further transfer\nknowledge from a well-trained interactive model to ours by reusing its word\nembeddings and adopting knowledge distillation. Our model is initialized from a\nmultilingual pre-trained language model M-BERT, and evaluated on two\nopen-resource CLIR datasets derived from Wikipedia and an in-house dataset\ncollected from a real-world search engine. Extensive analyses reveal that our\nmethods significantly boost the retrieval accuracy while maintaining the\ncomputational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linlong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xiaoyu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_T/0/1/0/all/0/1\">Tianchi Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haibo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenPrompt: An Open-source Framework for Prompt-learning. (arXiv:2111.01998v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01998","description":"<p>Prompt-learning has become a new paradigm in modern natural language\nprocessing, which directly adapts pre-trained language models (PLMs) to\n$cloze$-style prediction, autoregressive modeling, or sequence to sequence\ngeneration, resulting in promising performances on various tasks. However, no\nstandard implementation framework of prompt-learning is proposed yet, and most\nexisting prompt-learning codebases, often unregulated, only provide limited\nimplementations for specific scenarios. Since there are many details such as\ntemplating strategy, initializing strategy, and verbalizing strategy, etc. need\nto be considered in prompt-learning, practitioners face impediments to quickly\nadapting the desired prompt learning methods to their applications. In this\npaper, we present {OpenPrompt}, a unified easy-to-use toolkit to conduct\nprompt-learning over PLMs. OpenPrompt is a research-friendly framework that is\nequipped with efficiency, modularity, and extendibility, and its combinability\nallows the freedom to combine different PLMs, task formats, and prompting\nmodules in a unified paradigm. Users could expediently deploy prompt-learning\nframeworks and evaluate the generalization of them on different NLP tasks\nwithout constraints. OpenPrompt is publicly released at {\\url{\nhttps://github.com/thunlp/OpenPrompt}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Speaker Role Identification in Air Traffic Communication Using Deep Learning Approaches. (arXiv:2111.02041v1 [cs.SD])","link":"http://arxiv.org/abs/2111.02041","description":"<p>Automatic spoken instruction understanding (SIU) of the controller-pilot\nconversations in the air traffic control (ATC) requires not only recognizing\nthe words and semantics of the speech but also determining the role of the\nspeaker. However, few of the published works on the automatic understanding\nsystems in air traffic communication focus on speaker role identification\n(SRI). In this paper, we formulate the SRI task of controller-pilot\ncommunication as a binary classification problem. Furthermore, the text-based,\nspeech-based, and speech and text based multi-modal methods are proposed to\nachieve a comprehensive comparison of the SRI task. To ablate the impacts of\nthe comparative approaches, various advanced neural network architectures are\napplied to optimize the implementation of text-based and speech-based methods.\nMost importantly, a multi-modal speaker role identification network (MMSRINet)\nis designed to achieve the SRI task by considering both the speech and textual\nmodality features. To aggregate modality features, the modal fusion module is\nproposed to fuse and squeeze acoustic and textual representations by modal\nattention mechanism and self-attention pooling layer, respectively. Finally,\nthe comparative approaches are validated on the ATCSpeech corpus collected from\na real-world ATC environment. The experimental results demonstrate that all the\ncomparative approaches are worked for the SRI task, and the proposed MMSRINet\nshows the competitive performance and robustness than the other methods on both\nseen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dongyue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02080","description":"<p>Large pretrained language models such as GPT-3 have the surprising ability to\ndo in-context learning, where the model learns to do a downstream task simply\nby conditioning on a prompt consisting of input-output examples. Without being\nexplicitly pretrained to do so, the language model learns from these examples\nduring its forward pass without parameter updates on \"out-of-distribution\"\nprompts. Thus, it is unclear what mechanism enables in-context learning. In\nthis paper, we study the role of the pretraining distribution on the emergence\nof in-context learning under a mathematical setting where the pretraining texts\nhave long-range coherence. Here, language model pretraining requires inferring\na latent document-level concept from the conditioning text to generate coherent\nnext tokens. At test time, this mechanism enables in-context learning by\ninferring the shared latent concept between prompt examples and applying it to\nmake a prediction on the test example. Concretely, we prove that in-context\nlearning occurs implicitly via Bayesian inference of the latent concept when\nthe pretraining distribution is a mixture of HMMs. This can occur despite the\ndistribution mismatch between prompts and pretraining data. In contrast to\nmessy large-scale pretraining datasets for in-context learning in natural\nlanguage, we generate a family of small-scale synthetic datasets (GINC) where\nTransformer and LSTM language models both exhibit in-context learning. Beyond\nthe theory which focuses on the effect of the pretraining distribution, we\nempirically find that scaling model size improves in-context accuracy even when\nthe pretraining loss is the same.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task. (arXiv:2111.02086v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02086","description":"<p>This report describes Microsoft's machine translation systems for the WMT21\nshared task on large-scale multilingual machine translation. We participated in\nall three evaluation tracks including Large Track and two Small Tracks where\nthe former one is unconstrained and the latter two are fully constrained. Our\nmodel submissions to the shared task were initialized with\nDeltaLM\\footnote{\\url{https://aka.ms/deltalm}}, a generic pre-trained\nmultilingual encoder-decoder model, and fine-tuned correspondingly with the\nvast collected parallel data and allowed data sources according to track\nsettings, together with applying progressive learning and iterative\nback-translation approaches to further improve the performance. Our final\nsubmissions ranked first on three tracks in terms of the automatic evaluation\nmetric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Evaluation and Moderation of Open-domain Dialogue Systems. (arXiv:2111.02110v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02110","description":"<p>In recent years, dialogue systems have attracted significant interests in\nboth academia and industry. Especially the discipline of open-domain dialogue\nsystems, aka chatbots, has gained great momentum. Yet, a long standing\nchallenge that bothers the researchers is the lack of effective automatic\nevaluation metrics, which results in significant impediment in the current\nresearch. Common practice in assessing the performance of open-domain dialogue\nmodels involves extensive human evaluation on the final deployed models, which\nis both time- and cost- intensive. Moreover, a recent trend in building\nopen-domain chatbots involve pre-training dialogue models with a large amount\nof social media conversation data. However, the information contained in the\nsocial media conversations may be offensive and inappropriate. Indiscriminate\nusage of such data can result in insensitive and toxic generative models. This\npaper describes the data, baselines and results obtained for the Track 5 at the\nDialogue System Technology Challenge 10 (DSTC10).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sadoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banchs_R/0/1/0/all/0/1\">Rafael Banchs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. (arXiv:2111.02114v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02114","description":"<p>Multi-modal language-vision models trained on hundreds of millions of\nimage-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable\ncapability to perform zero- or few-shot learning and transfer even in absence\nof per-sample labels on target image data. Despite this trend, to date there\nhas been no publicly available datasets of sufficient scale for training such\nmodels from scratch. To address this issue, in a community effort we build and\nrelease for public LAION-400M, a dataset with CLIP-filtered 400 million\nimage-text pairs, their CLIP embeddings and kNN indices that allow efficient\nsimilarity search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuhmann_C/0/1/0/all/0/1\">Christoph Schuhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vencu_R/0/1/0/all/0/1\">Richard Vencu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaumont_R/0/1/0/all/0/1\">Romain Beaumont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaczmarczyk_R/0/1/0/all/0/1\">Robert Kaczmarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullis_C/0/1/0/all/0/1\">Clayton Mullis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katta_A/0/1/0/all/0/1\">Aarush Katta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coombes_T/0/1/0/all/0/1\">Theo Coombes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1\">Jenia Jitsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komatsuzaki_A/0/1/0/all/0/1\">Aran Komatsuzaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lingua Custodia's participation at the WMT 2021 Machine Translation using Terminologies shared task. (arXiv:2111.02120v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02120","description":"<p>This paper describes Lingua Custodia's submission to the WMT21 shared task on\nmachine translation using terminologies. We consider three directions, namely\nEnglish to French, Russian, and Chinese. We rely on a Transformer-based\narchitecture as a building block, and we explore a method which introduces two\nmain changes to the standard procedure to handle terminologies. The first one\nconsists in augmenting the training data in such a way as to encourage the\nmodel to learn a copy behavior when it encounters terminology constraint terms.\nThe second change is constraint token masking, whose purpose is to ease copy\nbehavior learning and to improve model generalization. Empirical results show\nthat our method satisfies most terminology constraints while maintaining high\ntranslation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ailem_M/0/1/0/all/0/1\">Melissa Ailem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinghsu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qader_R/0/1/0/all/0/1\">Raheel Qader</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Klarna Product Page Dataset: A RealisticBenchmark for Web Representation Learning. (arXiv:2111.02168v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02168","description":"<p>This paper tackles the under-explored problem of DOM tree element\nrepresentation learning. We advance the field of machine learning-based web\nautomation and hope to spur further research regarding this crucial area with\ntwo contributions. First, we adapt several popular Graph-based Neural Network\nmodels and apply them to embed elements in website DOM trees. Second, we\npresent a large-scale and realistic dataset of webpages. By providing this\nopen-access resource, we lower the entry barrier to this area of research. The\ndataset contains $51,701$ manually labeled product pages from $8,175$ real\ne-commerce websites. The pages can be rendered entirely in a web browser and\nare suitable for computer vision applications. This makes it substantially\nricher and more diverse than other datasets proposed for element representation\nlearning, classification and prediction on the web. Finally, using our proposed\ndataset, we show that the embeddings produced by a Graph Convolutional Neural\nNetwork outperform representations produced by other state-of-the-art methods\nin a web element prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hotti_A/0/1/0/all/0/1\">Alexandra Hotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risuleo_R/0/1/0/all/0/1\">Riccardo Sven Risuleo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magureanu_S/0/1/0/all/0/1\">Stefan Magureanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_A/0/1/0/all/0/1\">Aref Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagergren_J/0/1/0/all/0/1\">Jens Lagergren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition. (arXiv:2111.02172v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02172","description":"<p>The audio-video based multimodal emotion recognition has attracted a lot of\nattention due to its robust performance. Most of the existing methods focus on\nproposing different cross-modal fusion strategies. However, these strategies\nintroduce redundancy in the features of different modalities without fully\nconsidering the complementary properties between modal information, and these\napproaches do not guarantee the non-loss of original semantic information\nduring intra- and inter-modal interactions. In this paper, we propose a novel\ncross-modal fusion network based on self-attention and residual structure\n(CFN-SR) for multimodal emotion recognition. Firstly, we perform representation\nlearning for audio and video modalities to obtain the semantic features of the\ntwo modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed\nthe features of the two modalities into the cross-modal blocks separately to\nensure efficient complementarity and completeness of information through the\nself-attention mechanism and residual structure. Finally, we obtain the output\nof emotions by splicing the obtained fused representation with the original\nrepresentation. To verify the effectiveness of the proposed method, we conduct\nexperiments on the RAVDESS dataset. The experimental results show that the\nproposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with\n26.30M parameters. Our code is available at\nhttps://github.com/skeletonNN/CFN-SR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziwang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiayin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiangling Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aimin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhibin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence Matching. (arXiv:2111.02188v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02188","description":"<p>This paper presents a deep neural architecture, for Natural Language Sentence\nMatching (NLSM) by adding a deep recursive encoder to BERT so called BERT with\nDeep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that\nBERT still does not capture the full complexity of text, so a deep recursive\nencoder is applied on top of BERT. Three Bi-LSTM layers with residual\nconnection are used to design a recursive encoder and an attention module is\nused on top of this encoder. To obtain the final vector, a pooling layer\nconsisting of average and maximum pooling is used. We experiment our model on\nfour benchmarks, SNLI, FarsTail, MultiNLI, SciTail, and a novel Persian\nreligious questions dataset. This paper focuses on improving the BERT results\nin the NLSM task. In this regard, comparisons between BERT-DRE and BERT are\nconducted, and it is shown that in all cases, BERT-DRE outperforms only BERT.\nThe BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and\nBERT-DRE architectures improved to 90.29% using the same dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavan_E/0/1/0/all/0/1\">Ehsan Tavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1\">Ali Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafi_M/0/1/0/all/0/1\">Maryam Najafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibak_S/0/1/0/all/0/1\">Saeed Bibak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training. (arXiv:2111.02194v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02194","description":"<p>Aspect-based sentiment analysis aims to identify the sentiment polarity of a\nspecific aspect in product reviews. We notice that about 30% of reviews do not\ncontain obvious opinion words, but still convey clear human-aware sentiment\norientation, which is known as implicit sentiment. However, recent neural\nnetwork-based approaches paid little attention to implicit sentiment entailed\nin the reviews. To overcome this issue, we adopt Supervised Contrastive\nPre-training on large-scale sentiment-annotated corpora retrieved from\nin-domain language resources. By aligning the representation of implicit\nsentiment expressions to those with the same sentiment label, the pre-training\nprocess leads to better capture of both implicit and explicit sentiment\norientation towards aspects in reviews. Experimental results show that our\nmethod achieves state-of-the-art performance on SemEval2014 benchmarks, and\ncomprehensive analysis validates its effectiveness on learning implicit\nsentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Embedding of Stories Into Collections of Independent Media. (arXiv:2111.02216v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02216","description":"<p>We look at how machine learning techniques that derive properties of items in\na collection of independent media can be used to automatically embed stories\ninto such collections. To do so, we use models that extract the tempo of songs\nto make a music playlist follow a narrative arc. Our work specifies an\nopen-source tool that uses pre-trained neural network models to extract the\nglobal tempo of a set of raw audio files and applies these measures to create a\nnarrative-following playlist. This tool is available at\nhttps://github.com/dylanashley/playlist-story-builder/releases/tag/v1.0.0\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1\">Dylan R. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrmann_V/0/1/0/all/0/1\">Vincent Herrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friggstad_Z/0/1/0/all/0/1\">Zachary Friggstad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1\">Kory W. Mathewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Case Study and Qualitative Analysis of Simple Cross-Lingual Opinion Mining. (arXiv:2111.02259v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02259","description":"<p>User-generated content from social media is produced in many languages,\nmaking it technically challenging to compare the discussed themes from one\ndomain across different cultures and regions. It is relevant for domains in a\nglobalized world, such as market research, where people from two nations and\nmarkets might have different requirements for a product. We propose a simple,\nmodern, and effective method for building a single topic model with sentiment\nanalysis capable of covering multiple languages simultanteously, based on a\npre-trained state-of-the-art deep neural network for natural language\nunderstanding. To demonstrate its feasibility, we apply the model to newspaper\narticles and user comments of a specific domain, i.e., organic food products\nand related consumption behavior. The themes match across languages.\nAdditionally, we obtain an high proportion of stable and domain-relevant\ntopics, a meaningful relation between topics and their respective textual\ncontents, and an interpretable representation for social media documents.\nMarketing can potentially benefit from our method, since it provides an\neasy-to-use means of addressing specific customer interests from different\nmarket regions around the globe. For reproducibility, we provide the code,\ndata, and results of our study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_W/0/1/0/all/0/1\">Wing Sheung Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiaoxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SERC: Syntactic and Semantic Sequence based Event Relation Classification. (arXiv:2111.02265v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02265","description":"<p>Temporal and causal relations play an important role in determining the\ndependencies between events. Classifying the temporal and causal relations\nbetween events has many applications, such as generating event timelines, event\nsummarization, textual entailment and question answering. Temporal and causal\nrelations are closely related and influence each other. So we propose a joint\nmodel that incorporates both temporal and causal features to perform causal\nrelation classification. We use the syntactic structure of the text for\nidentifying temporal and causal relations between two events from the text. We\nextract parts-of-speech tag sequence, dependency tag sequence and word sequence\nfrom the text. We propose an LSTM based model for temporal and causal relation\nclassification that captures the interrelations between the three encoded\nfeatures. Evaluation of our model on four popular datasets yields promising\nresults for temporal and causal relation classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkatachalam_K/0/1/0/all/0/1\">Kritika Venkatachalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutharaju_R/0/1/0/all/0/1\">Raghava Mutharaju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Annotator Bias Approximation on Crowdsourced Single-Label Sentiment Analysis. (arXiv:2111.02326v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02326","description":"<p>Sentiment analysis is often a crowdsourcing task prone to subjective labels\ngiven by many annotators. It is not yet fully understood how the annotation\nbias of each annotator can be modeled correctly with state-of-the-art methods.\nHowever, resolving annotator bias precisely and reliably is the key to\nunderstand annotators' labeling behavior and to successfully resolve\ncorresponding individual misconceptions and wrongdoings regarding the\nannotation task. Our contribution is an explanation and improvement for precise\nneural end-to-end bias modeling and ground truth estimation, which reduces an\nundesired mismatch in that regard of the existing state-of-the-art.\nClassification experiments show that it has potential to improve accuracy in\ncases where each sample is annotated only by one single annotator. We provide\nthe whole source code publicly and release an own domain-specific sentiment\ndataset containing 10,000 sentences discussing organic food products. These are\ncrawled from social media and are singly labeled by 10 non-expert annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szabo_D/0/1/0/all/0/1\">David Szabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_A/0/1/0/all/0/1\">Andreas Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_M/0/1/0/all/0/1\">Maria Luisa Ripoll Dominguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmer_C/0/1/0/all/0/1\">Christian Widmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wich_M/0/1/0/all/0/1\">Maximilian Wich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02358","description":"<p>We present a unified Vision-Language pretrained Model (VLMo) that jointly\nlearns a dual encoder and a fusion encoder with a modular Transformer network.\nSpecifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,\nwhere each block contains a pool of modality-specific experts and a shared\nself-attention layer. Because of the modeling flexibility of MoME, pretrained\nVLMo can be fine-tuned as a fusion encoder for vision-language classification\ntasks, or used as a dual encoder for efficient image-text retrieval. Moreover,\nwe propose a stagewise pre-training strategy, which effectively leverages\nlarge-scale image-only and text-only data besides image-text pairs.\nExperimental results show that VLMo achieves state-of-the-art results on\nvarious vision-language tasks, including VQA and NLVR2. The code and pretrained\nmodels are available at https://aka.ms/vlmo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HmBlogs: A big general Persian corpus. (arXiv:2111.02362v1 [cs.CL])","link":"http://arxiv.org/abs/2111.02362","description":"<p>This paper introduces the hmBlogs corpus for Persian, as a low resource\nlanguage. This corpus has been prepared based on a collection of nearly 20\nmillion blog posts over a period of about 15 years from a space of Persian\nblogs and includes more than 6.8 billion tokens. It can be claimed that this\ncorpus is currently the largest Persian corpus that has been prepared\nindependently for the Persian language. This corpus is presented in both raw\nand preprocessed forms, and based on the preprocessed corpus some word\nembedding models are produced. By the provided models, the hmBlogs is compared\nwith some of the most important corpora available in Persian, and the results\nshow the superiority of the hmBlogs corpus over the others. These evaluations\nalso present the importance and effects of corpora, evaluation datasets, model\nproduction methods, different hyperparameters and even the evaluation methods.\nIn addition to evaluating the corpus and its produced language models, this\nresearch also presents a semantic analogy dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khansari_H/0/1/0/all/0/1\">Hamzeh Motahari Khansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers. (arXiv:2111.02387v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02387","description":"<p>Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks are often\ndegraded significantly. In this paper, we present METER~(\\textbf{M}ultimodal\n\\textbf{E}nd-to-end \\textbf{T}ransform\\textbf{ER}), through which we\nsystematically investigate how to design and pre-train a fully\ntransformer-based VL model in an end-to-end manner. Specifically, we dissect\nthe model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT,\nSwin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion\n(e.g., merged attention vs. co-attention), architecture design (e.g.,\nencoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked\nimage modeling). We conduct comprehensive experiments on a wide range of VL\ntasks, and provide insights on how to train a performant VL transformer while\nmaintaining fast inference speed. Notably, METER~achieves an accuracy of\n77.64\\% on the VQAv2 test-std set using only 4M images for pre-training,\nsurpassing the state-of-the-art region-feature-based VinVL model by +1.04\\%,\nand outperforming the previous best fully transformer-based ALBEF model by\n+1.6\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanyun/0/1/0/all/0/1\">Nanyun</a> (Violet) <a href=\"http://arxiv.org/find/cs/1/au:+Peng/0/1/0/all/0/1\">Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Landscape of Relational Syllogistic Logics. (arXiv:1809.00656v2 [math.LO] UPDATED)","link":"http://arxiv.org/abs/1809.00656","description":"<p>This paper explores relational syllogistic logics, a family of logical\nsystems related to reasoning about relations in extensions of the classical\nsyllogistic. These are all decidable logical systems. We prove completeness\ntheorems and complexity results for a natural subfamily of relational\nsyllogistic logics, parametrized by constructors for terms and for sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Kruckman_A/0/1/0/all/0/1\">Alex Kruckman</a>, <a href=\"http://arxiv.org/find/math/1/au:+Moss_L/0/1/0/all/0/1\">Lawrence S. Moss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-level Neural Network for Implicit Causality Detection in Web Texts. (arXiv:1908.07822v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1908.07822","description":"<p>Mining causality from text is a complex and crucial natural language\nunderstanding task corresponding to the human cognition. Existing studies at\nits solution can be grouped into two primary categories: feature engineering\nbased and neural model based methods. In this paper, we find that the former\nhas incomplete coverage and inherent errors but provide prior knowledge; while\nthe latter leverages context information but causal inference of which is\ninsufficiency. To handle the limitations, we propose a novel causality\ndetection model named MCDN to explicitly model causal reasoning process, and\nfurthermore, to exploit the advantages of both methods. Specifically, we adopt\nmulti-head self-attention to acquire semantic feature at word level and develop\nthe SCRN to infer causality at segment level. To the best of our knowledge,\nwith regards to the causality tasks, this is the first time that the Relation\nNetwork is applied. The experimental results show that: 1) the proposed\napproach performs prominent performance on causality detection; 2) further\nanalysis manifests the effectiveness and robustness of MCDN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shining Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wanli Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenkun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xianglin Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Effective Positional Encoding for Transformers. (arXiv:2104.08698v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08698","description":"<p>Transformer models are permutation equivariant. To supply the order and type\ninformation of the input tokens, position and segment embeddings are usually\nadded to the input. Recent works proposed variations of positional encodings\nwith relative position encodings achieving better performance. Our analysis\nshows that the gain actually comes from moving positional information to\nattention layer from the input. Motivated by this, we introduce Decoupled\nPositional Attention for Transformers (DIET), a simple yet effective mechanism\nto encode position and segment information into the Transformer models. The\nproposed method has faster training and inference time, while achieving\ncompetitive performance on GLUE, XTREME and WMT benchmarks. We further\ngeneralize our method to long-range transformers and show performance gain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pu-Chin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_H/0/1/0/all/0/1\">Henry Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yin-Wen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferng_C/0/1/0/all/0/1\">Chun-Sung Ferng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Luna: Linear Unified Nested Attention. (arXiv:2106.01540v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.01540","description":"<p>The quadratic computational and memory complexities of the Transformer's\nattention mechanism have limited its scalability for modeling long sequences.\nIn this paper, we propose Luna, a linear unified nested attention mechanism\nthat approximates softmax attention with two nested linear attention functions,\nyielding only linear (as opposed to quadratic) time and space complexity.\nSpecifically, with the first attention function, Luna packs the input sequence\ninto a sequence of fixed length. Then, the packed sequence is unpacked using\nthe second attention function. As compared to a more traditional attention\nmechanism, Luna introduces an additional sequence with a fixed length as input\nand an additional corresponding output, which allows Luna to perform attention\noperation linearly, while also storing adequate contextual information. We\nperform extensive evaluations on three benchmarks of sequence modeling tasks:\nlong-context sequence modeling, neural machine translation and masked language\nmodeling for large-scale pretraining. Competitive or even better experimental\nresults demonstrate both the effectiveness and efficiency of Luna compared to a\nvariety\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.12566","description":"<p>The attention module, which is a crucial component in Transformer, cannot\nscale efficiently to long sequences due to its quadratic complexity. Many works\nfocus on approximating the dot-then-exponentiate softmax function in the\noriginal attention, leading to sub-quadratic or even linear-complexity\nTransformer architectures. However, we show that these methods cannot be\napplied to more powerful attention modules that go beyond the\ndot-then-exponentiate style, e.g., Transformers with relative positional\nencoding (RPE). Since in many state-of-the-art models, relative positional\nencoding is used as default, designing efficient Transformers that can\nincorporate RPE is appealing. In this paper, we propose a novel way to\naccelerate attention calculation for Transformers with RPE on top of the\nkernelized attention. Based upon the observation that relative positional\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\nattention with RPE can be calculated efficiently using Fast Fourier Transform\n(FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity.\nInterestingly, we further demonstrate that properly using relative positional\nencoding can mitigate the training instability problem of vanilla kernelized\nattention. On a wide range of tasks, we empirically show that our models can be\ntrained from scratch without any optimization issues. The learned model\nperforms better than many efficient Transformer variants and is faster than\nstandard Transformer in the long-sequence regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dinglan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Detoxification using Large Pre-trained Neural Models. (arXiv:2109.08914v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08914","description":"<p>We present two novel unsupervised methods for eliminating toxicity in text.\nOur first method combines two recent ideas: (1) guidance of the generation\nprocess with small style-conditional language models and (2) use of\nparaphrasing models to perform style transfer. We use a well-performing\nparaphraser guided by style-trained language models to keep the text content\nand remove toxicity. Our second method uses BERT to replace toxic words with\ntheir non-offensive synonyms. We make the method more flexible by enabling BERT\nto replace mask tokens with a variable number of words. Finally, we present the\nfirst large-scale comparative study of style transfer models on the task of\ntoxicity removal. We compare our models with a number of methods for style\ntransfer. The models are evaluated in a reference-free way using a combination\nof unsupervised style transfer metrics. Both methods we suggest yield new SOTA\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1\">David Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voronov_A/0/1/0/all/0/1\">Anton Voronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dementieva_D/0/1/0/all/0/1\">Daryna Dementieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logacheva_V/0/1/0/all/0/1\">Varvara Logacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozlova_O/0/1/0/all/0/1\">Olga Kozlova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semenov_N/0/1/0/all/0/1\">Nikita Semenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Media Reveals Urban-Rural Differences in Stress across China. (arXiv:2110.15726v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15726","description":"<p>Modeling differential stress expressions in urban and rural regions in China\ncan provide a better understanding of the effects of urbanization on\npsychological well-being in a country that has rapidly grown economically in\nthe last two decades. This paper studies linguistic differences in the\nexperiences and expressions of stress in urban-rural China from Weibo posts\nfrom over 65,000 users across 329 counties using hierarchical mixed-effects\nmodels. We analyzed phrases, topical themes, and psycho-linguistic word choices\nin Weibo posts mentioning stress to better understand appraisal differences\nsurrounding psychological stress in urban and rural communities in China; we\nthen compared them with large-scale polls from Gallup. After controlling for\nsocioeconomic and gender differences, we found that rural communities tend to\nexpress stress in emotional and personal themes such as relationships, health,\nand opportunity while users in urban areas express stress using relative,\ntemporal, and external themes such as work, politics, and economics. These\ndifferences exist beyond controlling for GDP and urbanization, indicating a\nfundamentally different lifestyle between rural and urban residents in very\nspecific environments, arguably having different sources of stress. We found\ncorroborative trends in physical, financial, and social wellness with\nurbanization in Gallup polls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jesse Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaidka_K/0/1/0/all/0/1\">Kokil Jaidka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_D/0/1/0/all/0/1\">Dandan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherman_G/0/1/0/all/0/1\">Garrick Sherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakhetiya_V/0/1/0/all/0/1\">Vinit Jakhetiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1\">Sharath Chandra Guntuku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying causal associations in tweets using deep learning: Use case on diabetes-related tweets from 2017-2021. (arXiv:2111.01225v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01225","description":"<p>Objective: Leveraging machine learning methods, we aim to extract both\nexplicit and implicit cause-effect associations in patient-reported,\ndiabetes-related tweets and provide a tool to better understand opinion,\nfeelings and observations shared within the diabetes online community from a\ncausality perspective. Materials and Methods: More than 30 million\ndiabetes-related tweets in English were collected between April 2017 and\nJanuary 2021. Deep learning and natural language processing methods were\napplied to focus on tweets with personal and emotional content. A\ncause-effect-tweet dataset was manually labeled and used to train 1) a\nfine-tuned Bertweet model to detect causal sentences containing a causal\nassociation 2) a CRF model with BERT based features to extract possible\ncause-effect associations. Causes and effects were clustered in a\nsemi-supervised approach and visualised in an interactive cause-effect-network.\nResults: Causal sentences were detected with a recall of 68% in an imbalanced\ndataset. A CRF model with BERT based features outperformed a fine-tuned BERT\nmodel for cause-effect detection with a macro recall of 68%. This led to 96,676\nsentences with cause-effect associations. \"Diabetes\" was identified as the\ncentral cluster followed by \"Death\" and \"Insulin\". Insulin pricing related\ncauses were frequently associated with \"Death\". Conclusions: A novel\nmethodology was developed to detect causal sentences and identify both explicit\nand implicit, single and multi-word cause and corresponding effect as expressed\nin diabetes-related tweets leveraging BERT-based architectures and visualised\nas cause-effect-network. Extracting causal associations on real-life, patient\nreported outcomes in social media data provides a useful complementary source\nof information in diabetes research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahne_A/0/1/0/all/0/1\">Adrian Ahne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tannier_X/0/1/0/all/0/1\">Xavier Tannier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizvi_M/0/1/0/all/0/1\">Md Imbessat Hassan Rizvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czernichow_T/0/1/0/all/0/1\">Thomas Czernichow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orchard_F/0/1/0/all/0/1\">Francisco Orchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bour_C/0/1/0/all/0/1\">Charline Bour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagherazzi_G/0/1/0/all/0/1\">Guy Fagherazzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Text-based Phishing Detection. (arXiv:2111.01676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01676","description":"<p>This paper reports on an experiment into text-based phishing detection using\nreadily available resources and without the use of semantics. The developed\nalgorithm is a modified version of previously published work that works with\nthe same tools. The results obtained in recognizing phishing emails are\nconsiderably better than the previously reported work; but the rate of text\nfalsely identified as phishing is slightly worse. It is expected that adding\nsemantic component will reduce the false positive rate while preserving the\ndetection accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gilchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1\">Julia M. Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Classifier Training Efficiency for Automatic Cyberbullying Detection with Feature Density. (arXiv:2111.01689v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01689","description":"<p>We study the effectiveness of Feature Density (FD) using different\nlinguistically-backed feature preprocessing methods in order to estimate\ndataset complexity, which in turn is used to comparatively estimate the\npotential performance of machine learning (ML) classifiers prior to any\ntraining. We hypothesise that estimating dataset complexity allows for the\nreduction of the number of required experiments iterations. This way we can\noptimize the resource-intensive training of ML models which is becoming a\nserious issue due to the increases in available dataset sizes and the ever\nrising popularity of models based on Deep Neural Networks (DNN). The problem of\nconstantly increasing needs for more powerful computational resources is also\naffecting the environment due to alarmingly-growing amount of CO2 emissions\ncaused by training of large-scale ML models. The research was conducted on\nmultiple datasets, including popular datasets, such as Yelp business review\ndataset used for training typical sentiment analysis models, as well as more\nrecent datasets trying to tackle the problem of cyberbullying, which, being a\nserious social problem, is also a much more sophisticated problem form the\npoint of view of linguistic representation. We use cyberbullying datasets\ncollected for multiple languages, namely English, Japanese and Polish. The\ndifference in linguistic complexity of datasets allows us to additionally\ndiscuss the efficacy of linguistically-backed word preprocessing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eronen_J/0/1/0/all/0/1\">Juuso Eronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smywinski_Pohl_A/0/1/0/all/0/1\">Aleksander Smywi&#x144;ski-Pohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leliwa_G/0/1/0/all/0/1\">Gniewosz Leliwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroczynski_M/0/1/0/all/0/1\">Michal Wroczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-11-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"3-D PET Image Generation with tumour masks using TGAN. (arXiv:2111.01866v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01866","description":"<p>Training computer-vision related algorithms on medical images for disease\ndiagnosis or image segmentation is difficult due to the lack of training data,\nlabeled samples, and privacy concerns. For this reason, a robust generative\nmethod to create synthetic data is highly sought after. However, most\nthree-dimensional image generators require additional image input or are\nextremely memory intensive. To address these issues we propose adapting video\ngeneration techniques for 3-D image generation. Using the temporal GAN (TGAN)\narchitecture, we show we are able to generate realistic head and neck PET\nimages. We also show that by conditioning the generator on tumour masks, we are\nable to control the geometry and location of the tumour in the generated\nimages. To test the utility of the synthetic images, we train a segmentation\nmodel using the synthetic images. Synthetic images conditioned on real tumour\nmasks are automatically segmented, and the corresponding real images are also\nsegmented. We evaluate the segmentations using the Dice score and find the\nsegmentation algorithm performs similarly on both datasets (0.65 synthetic\ndata, 0.70 real data). Various radionomic features are then calculated over the\nsegmented tumour volumes for each data set. A comparison of the real and\nsynthetic feature distributions show that seven of eight feature distributions\nhad statistically insignificant differences (p&gt;0.05). Correlation coefficients\nwere also calculated between all radionomic features and it is shown that all\nof the strong statistical correlations in the real data set are preserved in\nthe synthetic data set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bergen_R/0/1/0/all/0/1\">Robert V Bergen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajotte_J/0/1/0/all/0/1\">Jean-Francois Rajotte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yousefirizi_F/0/1/0/all/0/1\">Fereshteh Yousefirizi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klyuzhin_I/0/1/0/all/0/1\">Ivan S Klyuzhin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_R/0/1/0/all/0/1\">Raymond T. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Body Size and Depth Disambiguation in Multi-Person Reconstruction from Single Images. (arXiv:2111.01884v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01884","description":"<p>We address the problem of multi-person 3D body pose and shape estimation from\na single image. While this problem can be addressed by applying single-person\napproaches multiple times for the same scene, recent works have shown the\nadvantages of building upon deep architectures that simultaneously reason about\nall people in the scene in a holistic manner by enforcing, e.g., depth order\nconstraints or minimizing interpenetration among reconstructed bodies. However,\nexisting approaches are still unable to capture the size variability of people\ncaused by the inherent body scale and depth ambiguity. In this work, we tackle\nthis challenge by devising a novel optimization scheme that learns the\nappropriate body scale and relative camera pose, by enforcing the feet of all\npeople to remain on the ground floor. A thorough evaluation on MuPoTS-3D and\n3DPW datasets demonstrates that our approach is able to robustly estimate the\nbody translation and shape of multiple people while retrieving their spatial\narrangement, consistently improving current state-of-the-art, especially in\nscenes with people of very different heights\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ugrinovic_N/0/1/0/all/0/1\">Nicolas Ugrinovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1\">Adria Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanfeliu_A/0/1/0/all/0/1\">Alberto Sanfeliu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A dataset for multi-sensor drone detection. (arXiv:2111.01888v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01888","description":"<p>The use of small and remotely controlled unmanned aerial vehicles (UAVs), or\ndrones, has increased in recent years. This goes in parallel with misuse\nepisodes, with an evident threat to the safety of people or facilities. As a\nresult, the detection of UAV has also emerged as a research topic. Most studies\non drone detection fail to specify the type of acquisition device, the drone\ntype, the detection range, or the dataset. The lack of proper UAV detection\nstudies employing thermal infrared cameras is also an issue, despite its\nsuccess with other targets. Besides, we have not found any previous study that\naddresses the detection task as a function of distance to the target. Sensor\nfusion is indicated as an open research issue as well, although research in\nthis direction is scarce too. To counteract the mentioned issues and allow\nfundamental studies with a common public benchmark, we contribute with an\nannotated multi-sensor database for drone detection that includes infrared and\nvisible videos and audio files. The database includes three different drones,\nof different sizes and other flying objects that can be mistakenly detected as\ndrones, such as birds, airplanes or helicopters. In addition to using several\ndifferent sensors, the number of classes is higher than in previous studies. To\nallow studies as a function of the sensor-to-target distance, the dataset is\ndivided into three categories (Close, Medium, Distant) according to the\nindustry-standard Detect, Recognize and Identify (DRI) requirements, built on\nthe Johnson criteria. Given that the drones must be flown within visual range\ndue to regulations, the largest sensor-to-target distance for a drone is 200 m,\nand acquisitions are made in daylight. The data has been obtained at three\nairports in Sweden: Halmstad Airport (IATA code: HAD/ICAO code: ESMT),\nGothenburg City Airport (GSE/ESGP) and Malm\\\"o Airport (MMX/ESMS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svanstrom_F/0/1/0/all/0/1\">Fredrik Svanstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Englund_C/0/1/0/all/0/1\">Cristofer Englund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A high performance fingerprint liveness detection method based on quality related features. (arXiv:2111.01898v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01898","description":"<p>A new software-based liveness detection approach using a novel fingerprint\nparameterization based on quality related features is proposed. The system is\ntested on a highly challenging database comprising over 10,500 real and fake\nimages acquired with five sensors of different technologies and covering a wide\nrange of direct attack scenarios in terms of materials and procedures followed\nto generate the gummy fingers. The proposed solution proves to be robust to the\nmulti-scenario dataset, and presents an overall rate of 90% correctly\nclassified samples. Furthermore, the liveness detection method presented has\nthe added advantage over previously studied techniques of needing just one\nimage from a finger to decide whether it is real or fake. This last\ncharacteristic provides the method with very valuable features as it makes it\nless intrusive, more user friendly, faster and reduces its implementation\ncosts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1\">Javier Galbally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for identification and face, gender, expression recognition under constraints. (arXiv:2111.01930v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01930","description":"<p>Biometric recognition based on the full face is an extensive research area.\nHowever, using only partially visible faces, such as in the case of\nveiled-persons, is a challenging task. Deep convolutional neural network (CNN)\nis used in this work to extract the features from veiled-person face images. We\nfound that the sixth and the seventh fully connected layers, FC6 and FC7\nrespectively, in the structure of the VGG19 network provide robust features\nwith each of these two layers containing 4096 features. The main objective of\nthis work is to test the ability of deep learning based automated computer\nsystem to identify not only persons, but also to perform recognition of gender,\nage, and facial expressions such as eye smile. Our experimental results\nindicate that we obtain high accuracy for all the tasks. The best recorded\naccuracy values are up to 99.95% for identifying persons, 99.9% for gender\nrecognition, 99.9% for age recognition and 80.9% for facial expression (eye\nsmile) recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassanat_A/0/1/0/all/0/1\">Ahmad B. Hassanat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albustanji_A/0/1/0/all/0/1\">Abeer Albustanji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarawneh_A/0/1/0/all/0/1\">Ahmad S. Tarawneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alrashidi_M/0/1/0/all/0/1\">Malek Alrashidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alharbi_H/0/1/0/all/0/1\">Hani Alharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alanazi_M/0/1/0/all/0/1\">Mohammed Alanazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alghamdi_M/0/1/0/all/0/1\">Mansoor Alghamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhazi_I/0/1/0/all/0/1\">Ibrahim S Alkhazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasath_V/0/1/0/all/0/1\">V. B. Surya Prasath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting spatio-temporal layouts for compositional action recognition. (arXiv:2111.01936v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01936","description":"<p>Recognizing human actions is fundamentally a spatio-temporal reasoning\nproblem, and should be, at least to some extent, invariant to the appearance of\nthe human and the objects involved. Motivated by this hypothesis, in this work,\nwe take an object-centric approach to action recognition. Multiple works have\nstudied this setting before, yet it remains unclear (i) how well a carefully\ncrafted, spatio-temporal layout-based method can recognize human actions, and\n(ii) how, and when, to fuse the information from layout and appearance-based\nmodels. The main focus of this paper is compositional/few-shot action\nrecognition, where we advocate the usage of multi-head attention (proven to be\neffective for spatial reasoning) over spatio-temporal layouts, i.e.,\nconfigurations of object bounding boxes. We evaluate different schemes to\ninject video appearance information to the system, and benchmark our approach\non background cluttered action recognition. On the Something-Else and Action\nGenome datasets, we demonstrate (i) how to extend multi-head attention for\nspatio-temporal layout-based action recognition, (ii) how to improve the\nperformance of appearance-based models by fusion with layout-based models,\n(iii) that even on non-compositional background-cluttered video datasets, a\nfusion between layout- and appearance-based models improves the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radevski_G/0/1/0/all/0/1\">Gorjan Radevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarially Perturbed Wavelet-based Morphed Face Generation. (arXiv:2111.01965v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01965","description":"<p>Morphing is the process of combining two or more subjects in an image in\norder to create a new identity which contains features of both individuals.\nMorphed images can fool Facial Recognition Systems (FRS) into falsely accepting\nmultiple people, leading to failures in national security. As morphed image\nsynthesis becomes easier, it is vital to expand the research community's\navailable data to help combat this dilemma. In this paper, we explore\ncombination of two methods for morphed image generation, those of geometric\ntransformation (warping and blending to create morphed images) and photometric\nperturbation. We leverage both methods to generate high-quality adversarially\nperturbed morphs from the FERET, FRGC, and FRLL datasets. The final images\nretain high similarity to both input subjects while resulting in minimal\nartifacts in the visual domain. Images are synthesized by fusing the wavelet\nsub-bands from the two look-alike subjects, and then adversarially perturbed to\ncreate highly convincing imagery to deceive both humans and deep morph\ndetectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+OHaire_K/0/1/0/all/0/1\">Kelsey O&#x27;Haire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1\">Baaria Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1\">Poorya Aghdaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Glimpse Network: A Robust and Efficient Classification Architecture based on Recurrent Downsampled Attention. (arXiv:2111.02018v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02018","description":"<p>Most feedforward convolutional neural networks spend roughly the same efforts\nfor each pixel. Yet human visual recognition is an interaction between eye\nmovements and spatial attention, which we will have several glimpses of an\nobject in different regions. Inspired by this observation, we propose an\nend-to-end trainable Multi-Glimpse Network (MGNet) which aims to tackle the\nchallenges of high computation and the lack of robustness based on recurrent\ndownsampled attention mechanism. Specifically, MGNet sequentially selects\ntask-relevant regions of an image to focus on and then adaptively combines all\ncollected information for the final prediction. MGNet expresses strong\nresistance against adversarial attacks and common corruptions with less\ncomputation. Also, MGNet is inherently more interpretable as it explicitly\ninforms us where it focuses during each iteration. Our experiments on\nImageNet100 demonstrate the potential of recurrent downsampled attention\nmechanisms to improve a single feedforward manner. For example, MGNet improves\n4.76% accuracy on average in common corruptions with only 36.9% computational\ncost. Moreover, while the baseline incurs an accuracy drop to 7.6%, MGNet\nmanages to maintain 44.2% accuracy in the same PGD attack strength with\nResNet-50 backbone. Our code is available at\nhttps://github.com/siahuat0727/MGNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sia Huat Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Runpei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advancements in Self-Supervised Paradigms for Visual Feature Representation. (arXiv:2111.02042v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02042","description":"<p>We witnessed a massive growth in the supervised learning paradigm in the past\ndecade. Supervised learning requires a large amount of labeled data to reach\nstate-of-the-art performance. However, labeling the samples requires a lot of\nhuman annotation. To avoid the cost of labeling data, self-supervised methods\nwere proposed to make use of largely available unlabeled data. This study\nconducts a comprehensive and insightful survey and analysis of recent\ndevelopments in the self-supervised paradigm for feature representation. In\nthis paper, we investigate the factors affecting the usefulness of\nself-supervision under different settings. We present some of the key insights\nconcerning two different approaches in self-supervision, generative and\ncontrastive methods. We also investigate the limitations of supervised\nadversarial training and how self-supervision can help overcome those\nlimitations. We then move on to discuss the limitations and challenges in\neffectively using self-supervision for visual tasks. Finally, we highlight some\nopen problems and point out future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anand_M/0/1/0/all/0/1\">Mrinal Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Aditya Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Categorical Difference and Related Brain Regions of the Attentional Blink Effect. (arXiv:2111.02044v1 [cs.AI])","link":"http://arxiv.org/abs/2111.02044","description":"<p>Attentional blink (AB) is a biological effect, showing that for 200 to 500ms\nafter paying attention to one visual target, it is difficult to notice another\ntarget that appears next, and attentional blink magnitude (ABM) is a indicating\nparameter to measure the degree of this effect. Researchers have shown that\ndifferent categories of images can access the consciousness of human mind\ndifferently, and produce different ranges of ABM values. So in this paper, we\ncompare two different types of images, categorized as animal and object, by\npredicting ABM values directly from image features extracted from convolutional\nneural network (CNN), and indirectly from functional magnetic resonance imaging\n(fMRI) data. First, for two sets of images, we separately extract their average\nfeatures from layers of Alexnet, a classic model of CNN, then input the\nfeatures into a trained linear regression model to predict ABM values, and we\nfind higher-level instead of lower-level image features determine the\ncategorical difference in AB effect, and mid-level image features predict ABM\nvalues more correctly than low-level and high-level image features. Then we\nemploy fMRI data from different brain regions collected when the subjects\nviewed 50 test images to predict ABM values, and conclude that brain regions\ncovering relatively broader areas, like LVC, HVC and VC, perform better than\nother smaller brain regions, which means AB effect is more related to synthetic\nimpact of several visual brain regions than only one particular visual regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_R/0/1/0/all/0/1\">Renzhou Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiaohong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Point Set Resampling via Gradient Fields. (arXiv:2111.02045v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02045","description":"<p>3D point clouds acquired by scanning real-world objects or scenes have found\na wide range of applications including immersive telepresence, autonomous\ndriving, surveillance, etc. They are often perturbed by noise or suffer from\nlow density, which obstructs downstream tasks such as surface reconstruction\nand understanding. In this paper, we propose a novel paradigm of point set\nresampling for restoration, which learns continuous gradient fields of point\nclouds that converge points towards the underlying surface. In particular, we\nrepresent a point cloud via its gradient field -- the gradient of the\nlog-probability density function, and enforce the gradient field to be\ncontinuous, thus guaranteeing the continuity of the model for solvable\noptimization. Based on the continuous gradient fields estimated via a proposed\nneural network, resampling a point cloud amounts to performing gradient-based\nMarkov Chain Monte Carlo (MCMC) on the input noisy or sparse point cloud.\nFurther, we propose to introduce regularization into the gradient-based MCMC\nduring point cloud restoration, which essentially refines the intermediate\nresampled point cloud iteratively and accommodates various priors in the\nresampling process. Extensive experimental results demonstrate that the\nproposed point set resampling achieves the state-of-the-art performance in\nrepresentative restoration tasks including point cloud denoising and\nupsampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haolan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bi&#x27;an Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shitong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Image Feature Biases Exhibited by Deep CNN Models. (arXiv:2111.02058v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02058","description":"<p>In recent years, convolutional neural networks (CNNs) have been applied\nsuccessfully in many fields. However, such deep neural models are still\nregarded as black box in most tasks. One of the fundamental issues underlying\nthis problem is understanding which features are most influential in image\nrecognition tasks and how they are processed by CNNs. It is widely accepted\nthat CNN models combine low-level features to form complex shapes until the\nobject can be readily classified, however, several recent studies have argued\nthat texture features are more important than other features. In this paper, we\nassume that the importance of certain features varies depending on specific\ntasks, i.e., specific tasks exhibit a feature bias. We designed two\nclassification tasks based on human intuition to train deep neural models to\nidentify anticipated biases. We devised experiments comprising many tasks to\ntest these biases for the ResNet and DenseNet models. From the results, we\nconclude that (1) the combined effect of certain features is typically far more\ninfluential than any single feature; (2) in different tasks, neural models can\nperform different biases, that is, we can design a specific task to make a\nneural model biased toward a specific anticipated feature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dawei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yutang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Huanan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Sy Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoli Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep-Learning-Based Single-Image Height Reconstruction from Very-High-Resolution SAR Intensity Data. (arXiv:2111.02061v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02061","description":"<p>Originally developed in fields such as robotics and autonomous driving with\nimage-based navigation in mind, deep learning-based single-image depth\nestimation (SIDE) has found great interest in the wider image analysis\ncommunity. Remote sensing is no exception, as the possibility to estimate\nheight maps from single aerial or satellite imagery bears great potential in\nthe context of topographic reconstruction. A few pioneering investigations have\ndemonstrated the general feasibility of single image height prediction from\noptical remote sensing images and motivate further studies in that direction.\nWith this paper, we present the first-ever demonstration of deep learning-based\nsingle image height prediction for the other important sensor modality in\nremote sensing: synthetic aperture radar (SAR) data. Besides the adaptation of\na convolutional neural network (CNN) architecture for SAR intensity images, we\npresent a workflow for the generation of training data, and extensive\nexperimental results for different SAR imaging modes and test sites. Since we\nput a particular emphasis on transferability, we are able to confirm that deep\nlearning-based single-image height estimation is not only possible, but also\ntransfers quite well to unseen data, even if acquired by different imaging\nmodes and imaging parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Recla_M/0/1/0/all/0/1\">Michael Recla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Michael Schmitt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event and Activity Recognition in Video Surveillance for Cyber-Physical Systems. (arXiv:2111.02064v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02064","description":"<p>This chapter aims to aid the development of Cyber-Physical Systems (CPS) in\nautomated understanding of events and activities in various applications of\nvideo-surveillance. These events are mostly captured by drones, CCTVs or novice\nand unskilled individuals on low-end devices. Being unconstrained, these videos\nare immensely challenging due to a number of quality factors. We present an\nextensive account of the various approaches taken to solve the problem over the\nyears. This ranges from methods as early as Structure from Motion (SFM) based\napproaches to recent solution frameworks involving deep neural networks. We\nshow that the long-term motion patterns alone play a pivotal role in the task\nof recognizing an event. Consequently each video is significantly represented\nby a fixed number of key-frames using a graph-based approach. Only the temporal\nfeatures are exploited using a hybrid Convolutional Neural Network (CNN) +\nRecurrent Neural Network (RNN) architecture. The results we obtain are\nencouraging as they outperform standard temporal CNNs and are at par with those\nusing spatial information along with motion cues. Further exploring multistream\nmodels, we conceive a multi-tier fusion strategy for the spatial and temporal\nwings of a network. A consolidated representation of the respective individual\nprediction vectors on video and frame levels is obtained using a biased\nconflation technique. The fusion strategy endows us with greater rise in\nprecision on each stage as compared to the state-of-the-art methods, and thus a\npowerful consensus is achieved in classification. Results are recorded on four\nbenchmark datasets widely used in the domain of action recognition, namely CCV,\nHMDB, UCF-101 and KCV. It is inferable that focusing on better classification\nof the video sequences certainly leads to robust actuation of a system designed\nfor event surveillance and object cum activity tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhaumik_S/0/1/0/all/0/1\">Swarnabja Bhaumik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_P/0/1/0/all/0/1\">Prithwish Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1\">Partha Pratim Mohanta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Progressive Prototype Network for Generalized Zero-Shot Learning. (arXiv:2111.02073v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02073","description":"<p>Generalized Zero-Shot Learning (GZSL) aims to recognize new categories with\nauxiliary semantic information,e.g., category attributes. In this paper, we\nhandle the critical issue of domain shift problem, i.e., confusion between seen\nand unseen categories, by progressively improving cross-domain transferability\nand category discriminability of visual representations. Our approach, named\nDual Progressive Prototype Network (DPPN), constructs two types of prototypes\nthat record prototypical visual patterns for attributes and categories,\nrespectively. With attribute prototypes, DPPN alternately searches\nattribute-related local regions and updates corresponding attribute prototypes\nto progressively explore accurate attribute-region correspondence. This enables\nDPPN to produce visual representations with accurate attribute localization\nability, which benefits the semantic-visual alignment and representation\ntransferability. Besides, along with progressive attribute localization, DPPN\nfurther projects category prototypes into multiple spaces to progressively\nrepel visual representations from different categories, which boosts category\ndiscriminability. Both attribute and category prototypes are collaboratively\nlearned in a unified framework, which makes visual representations of DPPN\ntransferable and distinctive. Experiments on four benchmarks prove that DPPN\neffectively alleviates the domain shift problem in GZSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoqun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Shaobo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuejin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoyan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceQvec: Vector Quality Assessment for Face Biometrics based on ISO Compliance. (arXiv:2111.02078v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02078","description":"<p>In this paper we develop FaceQvec, a software component for estimating the\nconformity of facial images with each of the points contemplated in the ISO/IEC\n19794-5, a quality standard that defines general quality guidelines for face\nimages that would make them acceptable or unacceptable for use in official\ndocuments such as passports or ID cards. This type of tool for quality\nassessment can help to improve the accuracy of face recognition, as well as to\nidentify which factors are affecting the quality of a given face image and to\ntake actions to eliminate or reduce those factors, e.g., with postprocessing\ntechniques or re-acquisition of the image. FaceQvec consists of the automation\nof 25 individual tests related to different points contemplated in the\naforementioned standard, as well as other characteristics of the images that\nhave been considered to be related to facial quality. We first include the\nresults of the quality tests evaluated on a development dataset captured under\nrealistic conditions. We used those results to adjust the decision threshold of\neach test. Then we checked again their accuracy on a evaluation database that\ncontains new face images not seen during development. The evaluation results\ndemonstrate the accuracy of the individual tests for checking compliance with\nISO/IEC 19794-5. FaceQvec is available online\n(https://github.com/uam-biometrics/FaceQvec).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Ortega_J/0/1/0/all/0/1\">Javier Hernandez-Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Luis F. Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_de_Suso_J/0/1/0/all/0/1\">Jose Luis Gonzalez-de-Suso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamora_Martinez_F/0/1/0/all/0/1\">Francisco Zamora-Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence of image noise on crack detection performance of deep convolutional neural networks. (arXiv:2111.02079v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02079","description":"<p>Development of deep learning techniques to analyse image data is an expansive\nand emerging field. The benefits of tracking, identifying, measuring, and\nsorting features of interest from image data has endless applications for\nsaving cost, time, and improving safety. Much research has been conducted on\nclassifying cracks from image data using deep convolutional neural networks;\nhowever, minimal research has been conducted to study the efficacy of network\nperformance when noisy images are used. This paper will address the problem and\nis dedicated to investigating the influence of image noise on network accuracy.\nThe methods used incorporate a benchmark image data set, which is purposely\ndeteriorated with two types of noise, followed by treatment with image\nenhancement pre-processing techniques. These images, including their native\ncounterparts, are then used to train and validate two different networks to\nstudy the differences in accuracy and performance. Results from this research\nreveal that noisy images have a moderate to high impact on the network's\ncapability to accurately classify images despite the application of image\npre-processing. A new index has been developed for finding the most efficient\nmethod for classification in terms of computation timing and accuracy.\nConsequently, AlexNet was selected as the most efficient model based on the\nproposed index.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chianese_R/0/1/0/all/0/1\">Riccardo Chianese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Andy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharehbaghi_V/0/1/0/all/0/1\">Vahidreza Gharehbaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aravinthan_T/0/1/0/all/0/1\">Thiru Aravinthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noori_M/0/1/0/all/0/1\">Mohammad Noori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. (arXiv:2111.02114v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02114","description":"<p>Multi-modal language-vision models trained on hundreds of millions of\nimage-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable\ncapability to perform zero- or few-shot learning and transfer even in absence\nof per-sample labels on target image data. Despite this trend, to date there\nhas been no publicly available datasets of sufficient scale for training such\nmodels from scratch. To address this issue, in a community effort we build and\nrelease for public LAION-400M, a dataset with CLIP-filtered 400 million\nimage-text pairs, their CLIP embeddings and kNN indices that allow efficient\nsimilarity search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuhmann_C/0/1/0/all/0/1\">Christoph Schuhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vencu_R/0/1/0/all/0/1\">Richard Vencu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaumont_R/0/1/0/all/0/1\">Romain Beaumont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaczmarczyk_R/0/1/0/all/0/1\">Robert Kaczmarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullis_C/0/1/0/all/0/1\">Clayton Mullis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katta_A/0/1/0/all/0/1\">Aarush Katta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coombes_T/0/1/0/all/0/1\">Theo Coombes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1\">Jenia Jitsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komatsuzaki_A/0/1/0/all/0/1\">Aran Komatsuzaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient 3D Deep LiDAR Odometry. (arXiv:2111.02135v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02135","description":"<p>An efficient 3D point cloud learning architecture, named PWCLO-Net, for LiDAR\nodometry is first proposed in this paper. In this architecture, the\nprojection-aware representation of the 3D point cloud is proposed to organize\nthe raw 3D point cloud into an ordered data form to achieve efficiency. The\nPyramid, Warping, and Cost volume (PWC) structure for the LiDAR odometry task\nis built to estimate and refine the pose in a coarse-to-fine approach\nhierarchically and efficiently. A projection-aware attentive cost volume is\nbuilt to directly associate two discrete point clouds and obtain embedding\nmotion patterns. Then, a trainable embedding mask is proposed to weigh the\nlocal motion patterns to regress the overall pose and filter outlier points.\nThe trainable pose warp-refinement module is iteratively used with embedding\nmask optimized hierarchically to make the pose estimation more robust for\noutliers. The entire architecture is holistically optimized end-to-end to\nachieve adaptive learning of cost volume and mask, and all operations involving\npoint cloud sampling and grouping are accelerated by projection-aware 3D\nfeature learning methods. The superior performance and effectiveness of our\nLiDAR odometry architecture are demonstrated on KITTI odometry dataset. Our\nmethod outperforms all recent learning-based methods and even the\ngeometry-based approach, LOAM with mapping optimization, on most sequences of\nKITTI odometry dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinrui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Entropy-guided Reinforced Partial Convolutional Network for Zero-Shot Learning. (arXiv:2111.02139v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02139","description":"<p>Zero-Shot Learning (ZSL) aims to transfer learned knowledge from observed\nclasses to unseen classes via semantic correlations. A promising strategy is to\nlearn a global-local representation that incorporates global information with\nextra localities (i.e., small parts/regions of inputs). However, existing\nmethods discover localities based on explicit features without digging into the\ninherent properties and relationships among regions. In this work, we propose a\nnovel Entropy-guided Reinforced Partial Convolutional Network (ERPCNet), which\nextracts and aggregates localities progressively based on semantic relevance\nand visual correlations without human-annotated regions. ERPCNet uses\nreinforced partial convolution and entropy guidance; it not only discovers\nglobal-cooperative localities dynamically but also converges faster for policy\ngradient optimization. We conduct extensive experiments to demonstrate\nERPCNet's performance through comparisons with state-of-the-art methods under\nZSL and Generalized Zero-Shot Learning (GZSL) settings on four benchmark\ndatasets. We also show ERPCNet is time efficient and explainable through\nvisualization analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lina Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xianzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond PRNU: Learning Robust Device-Specific Fingerprint for Source Camera Identification. (arXiv:2111.02144v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02144","description":"<p>Source camera identification tools assist image forensic investigators to\nassociate an image in question with a suspect camera. Various techniques have\nbeen developed based on the analysis of the subtle traces left in the images\nduring the acquisition. The Photo Response Non Uniformity (PRNU) noise pattern\ncaused by sensor imperfections has been proven to be an effective way to\nidentify the source camera. The existing literature suggests that the PRNU is\nthe only fingerprint that is device-specific and capable of identifying the\nexact source device. However, the PRNU is susceptible to camera settings, image\ncontent, image processing operations, and counter-forensic attacks. A forensic\ninvestigator unaware of counter-forensic attacks or incidental image\nmanipulations is at the risk of getting misled. The spatial synchronization\nrequirement during the matching of two PRNUs also represents a major limitation\nof the PRNU. In recent years, deep learning based approaches have been\nsuccessful in identifying source camera models. However, the identification of\nindividual cameras of the same model through these data-driven approaches\nremains unsatisfactory. In this paper, we bring to light the existence of a new\nrobust data-driven device-specific fingerprint in digital images which is\ncapable of identifying the individual cameras of the same model. It is\ndiscovered that the new device fingerprint is location-independent, stochastic,\nand globally available, which resolve the spatial synchronization issue. Unlike\nthe PRNU, which resides in the high-frequency band, the new device fingerprint\nis extracted from the low and mid-frequency bands, which resolves the fragility\nissue that the PRNU is unable to contend with. Our experiments on various\ndatasets demonstrate that the new fingerprint is highly resilient to image\nmanipulations such as rotation, gamma correction, and aggressive JPEG\ncompression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manisha/0/1/0/all/0/1\">Manisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang-Tsun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xufeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotegar_K/0/1/0/all/0/1\">Karunakar A. Kotegar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Klarna Product Page Dataset: A RealisticBenchmark for Web Representation Learning. (arXiv:2111.02168v1 [cs.LG])","link":"http://arxiv.org/abs/2111.02168","description":"<p>This paper tackles the under-explored problem of DOM tree element\nrepresentation learning. We advance the field of machine learning-based web\nautomation and hope to spur further research regarding this crucial area with\ntwo contributions. First, we adapt several popular Graph-based Neural Network\nmodels and apply them to embed elements in website DOM trees. Second, we\npresent a large-scale and realistic dataset of webpages. By providing this\nopen-access resource, we lower the entry barrier to this area of research. The\ndataset contains $51,701$ manually labeled product pages from $8,175$ real\ne-commerce websites. The pages can be rendered entirely in a web browser and\nare suitable for computer vision applications. This makes it substantially\nricher and more diverse than other datasets proposed for element representation\nlearning, classification and prediction on the web. Finally, using our proposed\ndataset, we show that the embeddings produced by a Graph Convolutional Neural\nNetwork outperform representations produced by other state-of-the-art methods\nin a web element prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hotti_A/0/1/0/all/0/1\">Alexandra Hotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risuleo_R/0/1/0/all/0/1\">Riccardo Sven Risuleo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magureanu_S/0/1/0/all/0/1\">Stefan Magureanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_A/0/1/0/all/0/1\">Aref Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagergren_J/0/1/0/all/0/1\">Jens Lagergren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition. (arXiv:2111.02172v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02172","description":"<p>The audio-video based multimodal emotion recognition has attracted a lot of\nattention due to its robust performance. Most of the existing methods focus on\nproposing different cross-modal fusion strategies. However, these strategies\nintroduce redundancy in the features of different modalities without fully\nconsidering the complementary properties between modal information, and these\napproaches do not guarantee the non-loss of original semantic information\nduring intra- and inter-modal interactions. In this paper, we propose a novel\ncross-modal fusion network based on self-attention and residual structure\n(CFN-SR) for multimodal emotion recognition. Firstly, we perform representation\nlearning for audio and video modalities to obtain the semantic features of the\ntwo modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed\nthe features of the two modalities into the cross-modal blocks separately to\nensure efficient complementarity and completeness of information through the\nself-attention mechanism and residual structure. Finally, we obtain the output\nof emotions by splicing the obtained fused representation with the original\nrepresentation. To verify the effectiveness of the proposed method, we conduct\nexperiments on the RAVDESS dataset. The experimental results show that the\nproposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with\n26.30M parameters. Our code is available at\nhttps://github.com/skeletonNN/CFN-SR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziwang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiayin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiangling Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aimin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhibin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminator Synthesis: On reusing the other half of Generative Adversarial Networks. (arXiv:2111.02175v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02175","description":"<p>Generative Adversarial Networks have long since revolutionized the world of\ncomputer vision and, tied to it, the world of art. Arduous efforts have gone\ninto fully utilizing and stabilizing training so that outputs of the Generator\nnetwork have the highest possible fidelity, but little has gone into using the\nDiscriminator after training is complete. In this work, we propose to use the\nlatter and show a way to use the features it has learned from the training\ndataset to both alter an image and generate one from scratch. We name this\nmethod Discriminator Dreaming, and the full code can be found at\nhttps://github.com/PDillis/stylegan3-fun.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Porres_D/0/1/0/all/0/1\">Diego Porres</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Image Compression for Machine Perception. (arXiv:2111.02249v1 [eess.IV])","link":"http://arxiv.org/abs/2111.02249","description":"<p>Recent work has shown that learned image compression strategies can\noutperform standard hand-crafted compression algorithms that have been\ndeveloped over decades of intensive research on the rate-distortion trade-off.\nWith growing applications of computer vision, high quality image reconstruction\nfrom a compressible representation is often a secondary objective. Compression\nthat ensures high accuracy on computer vision tasks such as image segmentation,\nclassification, and detection therefore has the potential for significant\nimpact across a wide variety of settings. In this work, we develop a framework\nthat produces a compression format suitable for both human perception and\nmachine perception. We show that representations can be learned that\nsimultaneously optimize for compression and performance on core vision tasks.\nOur approach allows models to be trained directly from compressed\nrepresentations, and this approach yields increased performance on new tasks\nand in low-shot learning settings. We present results that improve upon\nsegmentation and detection performance compared to standard high quality JPGs,\nbut with representations that are four to ten times smaller in terms of bits\nper pixel. Further, unlike naive compression methods, at a level ten times\nsmaller than standard JEPGs, segmentation and detection models trained from our\nformat suffer only minor degradation in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Codevilla_F/0/1/0/all/0/1\">Felipe Codevilla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simard_J/0/1/0/all/0/1\">Jean Gabriel Simard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goroshin_R/0/1/0/all/0/1\">Ross Goroshin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pal_C/0/1/0/all/0/1\">Chris Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Cue Adaptive Emotion Recognition Network. (arXiv:2111.02273v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02273","description":"<p>Expressing and identifying emotions through facial and physical expressions\nis a significant part of social interaction. Emotion recognition is an\nessential task in computer vision due to its various applications and mainly\nfor allowing a more natural interaction between humans and machines. The common\napproaches for emotion recognition focus on analyzing facial expressions and\nrequires the automatic localization of the face in the image. Although these\nmethods can correctly classify emotion in controlled scenarios, such techniques\nare limited when dealing with unconstrained daily interactions. We propose a\nnew deep learning approach for emotion recognition based on adaptive multi-cues\nthat extract information from context and body poses, which humans commonly use\nin social interaction and communication. We compare the proposed approach with\nthe state-of-art approaches in the CAER-S dataset, evaluating different\ncomponents in a pipeline that reached an accuracy of 89.30%\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_W/0/1/0/all/0/1\">Willams Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1\">Cleber Zanchettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_L/0/1/0/all/0/1\">Lucas S. Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1\">Veronica Teichrieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of Deep Learning Models for the Prediction of Hand Hygiene Videos. (arXiv:2111.02322v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02322","description":"<p>This paper presents a comparison of various deep learning models such as\nException, Resnet-50, and Inception V3 for the classification and prediction of\nhand hygiene gestures, which were recorded in accordance with the World Health\nOrganization (WHO) guidelines. The dataset consists of six hand hygiene\nmovements in a video format, gathered for 30 participants. The network consists\nof pre-trained models with image net weights and a modified head of the model.\nAn accuracy of 37% (Xception model), 33% (Inception V3), and 72% (ResNet-50) is\nachieved in the classification report after the training of the models for 25\nepochs. ResNet-50 model clearly outperforms with correct class predictions. The\nmajor speed limitation can be overcome with the use of fast processing GPU for\nfuture work. A complete hand hygiene dataset along with other generic gestures\nsuch as one-hand movements (linear hand motion; circular hand rotation) will be\ntested with ResNet-50 architecture and the variants for health care workers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ML-PersRef: A Machine Learning-based Personalized Multimodal Fusion Approach for Referencing Outside Objects From a Moving Vehicle. (arXiv:2111.02327v1 [cs.HC])","link":"http://arxiv.org/abs/2111.02327","description":"<p>Over the past decades, the addition of hundreds of sensors to modern vehicles\nhas led to an exponential increase in their capabilities. This allows for novel\napproaches to interaction with the vehicle that go beyond traditional\ntouch-based and voice command approaches, such as emotion recognition, head\nrotation, eye gaze, and pointing gestures. Although gaze and pointing gestures\nhave been used before for referencing objects inside and outside vehicles, the\nmultimodal interaction and fusion of these gestures have so far not been\nextensively studied. We propose a novel learning-based multimodal fusion\napproach for referencing outside-the-vehicle objects while maintaining a long\ndriving route in a simulated environment. The proposed multimodal approaches\noutperform single-modality approaches in multiple aspects and conditions.\nMoreover, we also demonstrate possible ways to exploit behavioral differences\nbetween users when completing the referencing task to realize an adaptable\npersonalized system for each driver. We propose a personalization technique\nbased on the transfer-of-learning concept for exceedingly small data sizes to\nenhance prediction and adapt to individualistic referencing behavior. Our code\nis publicly available at https://github.com/amr-gomaa/ML-PersRef.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomaa_A/0/1/0/all/0/1\">Amr Gomaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_G/0/1/0/all/0/1\">Guillermo Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feld_M/0/1/0/all/0/1\">Michael Feld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LTD: Low Temperature Distillation for Robust Adversarial Training. (arXiv:2111.02331v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02331","description":"<p>Adversarial training has been widely used to enhance the robustness of the\nneural network models against adversarial attacks. However, there still a\nnotable gap between the nature accuracy and the robust accuracy. We found one\nof the reasons is the commonly used labels, one-hot vectors, hinder the\nlearning process for image recognition. In this paper, we proposed a method,\ncalled Low Temperature Distillation (LTD), which is based on the knowledge\ndistillation framework to generate the desired soft labels. Unlike the previous\nwork, LTD uses relatively low temperature in the teacher model, and employs\ndifferent, but fixed, temperatures for the teacher model and the student model.\nMoreover, we have investigated the methods to synergize the use of nature data\nand adversarial ones in LTD. Experimental results show that without extra\nunlabeled data, the proposed method combined with the previous work can achieve\n57.72\\% and 30.36\\% robust accuracy on CIFAR-10 and CIFAR-100 dataset\nrespectively, which is about 1.21\\% improvement of the state-of-the-art methods\nin average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Erh-Chung Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Che-Rung Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HS3: Learning with Proper Task Complexity in Hierarchically Supervised Semantic Segmentation. (arXiv:2111.02333v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02333","description":"<p>While deeply supervised networks are common in recent literature, they\ntypically impose the same learning objective on all transitional layers despite\ntheir varying representation powers.\n</p>\n<p>In this paper, we propose Hierarchically Supervised Semantic Segmentation\n(HS3), a training scheme that supervises intermediate layers in a segmentation\nnetwork to learn meaningful representations by varying task complexity. To\nenforce a consistent performance vs. complexity trade-off throughout the\nnetwork, we derive various sets of class clusters to supervise each\ntransitional layer of the network. Furthermore, we devise a fusion framework,\nHS3-Fuse, to aggregate the hierarchical features generated by these layers,\nwhich can provide rich semantic contexts and further enhance the final\nsegmentation. Extensive experiments show that our proposed HS3 scheme\nconsiderably outperforms vanilla deep supervision with no added inference cost.\nOur proposed HS3-Fuse framework further improves segmentation predictions and\nachieves state-of-the-art results on two large segmentation benchmarks: NYUD-v2\nand Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borse_S/0/1/0/all/0/1\">Shubhankar Borse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02358","description":"<p>We present a unified Vision-Language pretrained Model (VLMo) that jointly\nlearns a dual encoder and a fusion encoder with a modular Transformer network.\nSpecifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,\nwhere each block contains a pool of modality-specific experts and a shared\nself-attention layer. Because of the modeling flexibility of MoME, pretrained\nVLMo can be fine-tuned as a fusion encoder for vision-language classification\ntasks, or used as a dual encoder for efficient image-text retrieval. Moreover,\nwe propose a stagewise pre-training strategy, which effectively leverages\nlarge-scale image-only and text-only data besides image-text pairs.\nExperimental results show that VLMo achieves state-of-the-art results on\nvarious vision-language tasks, including VQA and NLVR2. The code and pretrained\nmodels are available at https://aka.ms/vlmo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subpixel Heatmap Regression for Facial Landmark Localization. (arXiv:2111.02360v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02360","description":"<p>Deep Learning models based on heatmap regression have revolutionized the task\nof facial landmark localization with existing models working robustly under\nlarge poses, non-uniform illumination and shadows, occlusions and\nself-occlusions, low resolution and blur. However, despite their wide adoption,\nheatmap regression approaches suffer from discretization-induced errors related\nto both the heatmap encoding and decoding process. In this work we show that\nthese errors have a surprisingly large negative impact on facial alignment\naccuracy. To alleviate this problem, we propose a new approach for the heatmap\nencoding and decoding process by leveraging the underlying continuous\ndistribution. To take full advantage of the newly proposed encoding-decoding\nmechanism, we also introduce a Siamese-based training that enforces heatmap\nconsistency across various geometric image transformations. Our approach offers\nnoticeable gains across multiple datasets setting a new state-of-the-art result\nin facial landmark localization. Code alongside the pretrained models will be\nmade available at https://www.adrianbulat.com/face-alignment\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1\">Adrian Bulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_E/0/1/0/all/0/1\">Enrique Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Salient Object Detection via Contrastive Features and Attention Modules. (arXiv:2111.02368v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02368","description":"<p>Video salient object detection aims to find the most visually distinctive\nobjects in a video. To explore the temporal dependencies, existing methods\nusually resort to recurrent neural networks or optical flow. However, these\napproaches require high computational cost, and tend to accumulate inaccuracies\nover time. In this paper, we propose a network with attention modules to learn\ncontrastive features for video salient object detection without the high\ncomputational temporal modeling techniques. We develop a non-local\nself-attention scheme to capture the global information in the video frame. A\nco-attention formulation is utilized to combine the low-level and high-level\nfeatures. We further apply the contrastive learning to improve the feature\nrepresentations, where foreground region pairs from the same video are pulled\ntogether, and foreground-background region pairs are pushed away in the latent\nspace. The intra-frame contrastive loss helps separate the foreground and\nbackground features, and the inter-frame contrastive loss improves the temporal\nconsistency. We conduct extensive experiments on several benchmark datasets for\nvideo salient object detection and unsupervised video object segmentation, and\nshow that the proposed method requires less computation, and performs favorably\nagainst the state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaojie Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaohui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers. (arXiv:2111.02387v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02387","description":"<p>Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks are often\ndegraded significantly. In this paper, we present METER~(\\textbf{M}ultimodal\n\\textbf{E}nd-to-end \\textbf{T}ransform\\textbf{ER}), through which we\nsystematically investigate how to design and pre-train a fully\ntransformer-based VL model in an end-to-end manner. Specifically, we dissect\nthe model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT,\nSwin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion\n(e.g., merged attention vs. co-attention), architecture design (e.g.,\nencoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked\nimage modeling). We conduct comprehensive experiments on a wide range of VL\ntasks, and provide insights on how to train a performant VL transformer while\nmaintaining fast inference speed. Notably, METER~achieves an accuracy of\n77.64\\% on the VQAv2 test-std set using only 4M images for pre-training,\nsurpassing the state-of-the-art region-feature-based VinVL model by +1.04\\%,\nand outperforming the previous best fully transformer-based ALBEF model by\n+1.6\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanyun/0/1/0/all/0/1\">Nanyun</a> (Violet) <a href=\"http://arxiv.org/find/cs/1/au:+Peng/0/1/0/all/0/1\">Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAST: Searching for a Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation. (arXiv:2111.02394v1 [cs.CV])","link":"http://arxiv.org/abs/2111.02394","description":"<p>We propose an accurate and efficient scene text detection framework, termed\nFAST (i.e., faster arbitrarily-shaped text detector). Different from recent\nadvanced text detectors that used hand-crafted network architectures and\ncomplicated post-processing, resulting in low inference speed, FAST has two new\ndesigns. (1) We search the network architecture by designing a network search\nspace and reward function carefully tailored for text detection, leading to\nmore powerful features than most networks that are searched for image\nclassification. (2) We design a minimalist representation (only has 1-channel\noutput) to model text with arbitrary shape, as well as a GPU-parallel\npost-processing to efficiently assemble text lines with negligible time\noverhead. Benefiting from these two designs, FAST achieves an excellent\ntrade-off between accuracy and efficiency on several challenging datasets. For\nexample, FAST-A0 yields 81.4% F-measure at 152 FPS on Total-Text, outperforming\nthe previous fastest method by 1.5 points and 70 FPS in terms of accuracy and\nspeed. With TensorRT optimization, the inference speed can be further\naccelerated to over 600 FPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">ZhiBo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Shared Latent Variables for Robots to Imitate Human Movements and Understand their Physical Limitations. (arXiv:1810.04879v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1810.04879","description":"<p>Assistive robotics and particularly robot coaches may be very helpful for\nrehabilitation healthcare. In this context, we propose a method based on\nGaussian Process Latent Variable Model (GP-LVM) to transfer knowledge between a\nphysiotherapist, a robot coach and a patient. Our model is able to map visual\nhuman body features to robot data in order to facilitate the robot learning and\nimitation. In addition , we propose to extend the model to adapt robots'\nunderstanding to patient's physical limitations during the assessment of\nrehabilitation exercises. Experimental evaluation demonstrates promising\nresults for both robot imitation and model adaptation according to the\npatients' limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devanne_M/0/1/0/all/0/1\">Maxime Devanne</a> (IMT Atlantique), <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1\">Sao Mai Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Evaluation: Fine-Grained CNN vs. Traditional CNN Classifiers. (arXiv:2003.11154v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.11154","description":"<p>To make the best use of the underlying minute and subtle differences,\nfine-grained classifiers collect information about inter-class variations. The\ntask is very challenging due to the small differences between the colors,\nviewpoint, and structure in the same class entities. The classification becomes\nmore difficult due to the similarities between the differences in viewpoint\nwith other classes and differences with its own. In this work, we investigate\nthe performance of the landmark general CNN classifiers, which presented\ntop-notch results on large scale classification datasets, on the fine-grained\ndatasets, and compare it against state-of-the-art fine-grained classifiers. In\nthis paper, we pose two specific questions: (i) Do the general CNN classifiers\nachieve comparable results to fine-grained classifiers? (ii) Do general CNN\nclassifiers require any specific information to improve upon the fine-grained\nones? Throughout this work, we train the general CNN classifiers without\nintroducing any aspect that is specific to fine-grained datasets. We show an\nextensive evaluation on six datasets to determine whether the fine-grained\nclassifier is able to elevate the baseline in their experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness. (arXiv:2006.13726v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.13726","description":"<p>Evaluating the robustness of a defense model is a challenging task in\nadversarial robustness research. Obfuscated gradients, a type of gradient\nmasking, have previously been found to exist in many defense methods and cause\na false signal of robustness. In this paper, we identify a more subtle\nsituation called Imbalanced Gradients that can also cause overestimated\nadversarial robustness. The phenomenon of imbalanced gradients occurs when the\ngradient of one term of the margin loss dominates and pushes the attack towards\nto a suboptimal direction. To exploit imbalanced gradients, we formulate a\nMargin Decomposition (MD) attack that decomposes a margin loss into individual\nterms and then explores the attackability of these terms separately via a\ntwo-stage process. We also propose a MultiTargeted and an ensemble version of\nour MD attack. By investigating 17 defense models proposed since 2018, we find\nthat 6 models are susceptible to imbalanced gradients and our MD attack can\ndecrease their robustness evaluated by the best baseline standalone attack by\nanother 2%. We also provide an in-depth analysis of the likely causes of\nimbalanced gradients and effective countermeasures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Linxi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hanxun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zejia Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine versus Human Attention in Deep Reinforcement Learning Tasks. (arXiv:2010.15942v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.15942","description":"<p>Deep reinforcement learning (RL) algorithms are powerful tools for solving\nvisuomotor decision tasks. However, the trained models are often difficult to\ninterpret, because they are represented as end-to-end deep neural networks. In\nthis paper, we shed light on the inner workings of such trained models by\nanalyzing the pixels that they attend to during task execution, and comparing\nthem with the pixels attended to by humans executing the same tasks. To this\nend, we investigate the following two questions that, to the best of our\nknowledge, have not been previously studied. 1) How similar are the visual\nrepresentations learned by RL agents and humans when performing the same task?\nand, 2) How do similarities and differences in these learned representations\nexplain RL agents' performance on these tasks? Specifically, we compare the\nsaliency maps of RL agents against visual attention models of human experts\nwhen learning to play Atari games. Further, we analyze how hyperparameters of\nthe deep RL algorithm affect the learned representations and saliency maps of\nthe trained agents. The insights provided have the potential to inform novel\nalgorithms for closing the performance gap between human experts and RL agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sihang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayhoe_M/0/1/0/all/0/1\">Mary Hayhoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballard_D/0/1/0/all/0/1\">Dana Ballard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Red Blood Cell Segmentation with Overlapping Cell Separation and Classification on Imbalanced Dataset. (arXiv:2012.01321v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.01321","description":"<p>Automated red blood cell (RBC) classification on blood smear images helps\nhematologists to analyze RBC lab results in a reduced time and cost. However,\noverlapping cells can cause incorrect predicted results, and so they have to be\nseparated into multiple single RBCs before classifying. To classify multiple\nclasses with deep learning, imbalance problems are common in medical imaging\nbecause normal samples are always higher than rare disease samples. This paper\npresents a new method to segment and classify RBCs from blood smear images,\nspecifically to tackle cell overlapping and data imbalance problems. Focusing\non overlapping cell separation, our segmentation process first estimates\nellipses to represent RBCs. The method detects the concave points and then\nfinds the ellipses using directed ellipse fitting. The accuracy from 20 blood\nsmear images was 0.889. Classification requires balanced training datasets.\nHowever, some RBC types are rare. The imbalance ratio of this dataset was\n34.538 for 12 RBC classes from 20,875 individual RBC samples. The use of\nmachine learning for RBC classification with an imbalanced dataset is hence\nmore challenging than many other applications. We analyzed techniques to deal\nwith this problem. The best accuracy and F1-score were 0.921 and 0.8679,\nrespectively, using EfficientNet-B1 with augmentation. Experimental results\nshowed that the weight balancing technique with augmentation had the potential\nto deal with imbalance problems by improving the F1-score on minority classes,\nwhile data augmentation significantly improved the overall classification\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Naruenatthanaset_K/0/1/0/all/0/1\">Korranat Naruenatthanaset</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palasuwan_A/0/1/0/all/0/1\">Attakorn Palasuwan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attack Agnostic Detection of Adversarial Examples via Random Subspace Analysis. (arXiv:2012.06405v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.06405","description":"<p>Whilst adversarial attack detection has received considerable attention, it\nremains a fundamentally challenging problem from two perspectives. First, while\nthreat models can be well-defined, attacker strategies may still vary widely\nwithin those constraints. Therefore, detection should be considered as an\nopen-set problem, standing in contrast to most current detection approaches.\nThese methods take a closed-set view and train binary detectors, thus biasing\ndetection toward attacks seen during detector training. Second, limited\ninformation is available at test time and typically confounded by nuisance\nfactors including the label and underlying content of the image. We address\nthese challenges via a novel strategy based on random subspace analysis. We\npresent a technique that utilizes properties of random projections to\ncharacterize the behavior of clean and adversarial examples across a diverse\nset of subspaces. The self-consistency (or inconsistency) of model activations\nis leveraged to discern clean from adversarial examples. Performance\nevaluations demonstrate that our technique ($AUC\\in[0.92, 0.98]$) outperforms\ncompeting detection strategies ($AUC\\in[0.30,0.79]$), while remaining truly\nagnostic to the attack strategy (for both targeted/untargeted attacks). It also\nrequires significantly less calibration data (composed only of clean examples)\nthan competing approaches to achieve this performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drenkow_N/0/1/0/all/0/1\">Nathan Drenkow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fendley_N/0/1/0/all/0/1\">Neil Fendley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Philippe Burlina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GTA: Global Temporal Attention for Video Action Understanding. (arXiv:2012.08510v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.08510","description":"<p>Self-attention learns pairwise interactions to model long-range dependencies,\nyielding great improvements for video action recognition. In this paper, we\nseek a deeper understanding of self-attention for temporal modeling in videos.\nWe first demonstrate that the entangled modeling of spatio-temporal information\nby flattening all pixels is sub-optimal, failing to capture temporal\nrelationships among frames explicitly. To this end, we introduce Global\nTemporal Attention (GTA), which performs global temporal attention on top of\nspatial attention in a decoupled manner. We apply GTA on both pixels and\nsemantically similar regions to capture temporal relationships at different\nlevels of spatial granularity. Unlike conventional self-attention that computes\nan instance-specific attention matrix, GTA directly learns a global attention\nmatrix that is intended to encode temporal structures that generalize across\ndifferent samples. We further augment GTA with a cross-channel multi-head\nfashion to exploit channel interactions for better temporal modeling. Extensive\nexperiments on 2D and 3D networks demonstrate that our approach consistently\nenhances temporal modeling and provides state-of-the-art performance on three\nvideo action recognition datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xitong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LS-HDIB: A Large Scale Handwritten Document Image Binarization Dataset. (arXiv:2101.11674v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11674","description":"<p>Handwritten document image binarization is challenging due to high\nvariability in the written content and complex background attributes such as\npage style, paper quality, stains, shadow gradients, and non-uniform\nillumination. While the traditional thresholding methods do not effectively\ngeneralize on such challenging real-world scenarios, deep learning-based\nmethods have performed relatively well when provided with sufficient training\ndata. However, the existing datasets are limited in size and diversity. This\nwork proposes LS-HDIB - a large-scale handwritten document image binarization\ndataset containing over a million document images that span numerous real-world\nscenarios. Additionally, we introduce a novel technique that uses a combination\nof adaptive thresholding and seamless cloning methods to create the dataset\nwith accurate ground truths. Through an extensive quantitative and qualitative\nevaluation over eight different deep learning based models, we demonstrate the\nenhancement in the performance of these models when trained on the LS-HDIB\ndataset and tested on unseen images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadekar_K/0/1/0/all/0/1\">Kaustubh Sadekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Ashish Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prajwal Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWAD: Domain Generalization by Seeking Flat Minima. (arXiv:2102.08604v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.08604","description":"<p>Domain generalization (DG) methods aim to achieve generalizability to an\nunseen target domain by using only training data from the source domains.\nAlthough a variety of DG methods have been proposed, a recent study shows that\nunder a fair evaluation protocol, called DomainBed, the simple empirical risk\nminimization (ERM) approach works comparable to or even outperforms previous\nmethods. Unfortunately, simply solving ERM on a complex, non-convex loss\nfunction can easily lead to sub-optimal generalizability by seeking sharp\nminima. In this paper, we theoretically show that finding flat minima results\nin a smaller domain generalization gap. We also propose a simple yet effective\nmethod, named Stochastic Weight Averaging Densely (SWAD), to find flat minima.\nSWAD finds flatter minima and suffers less from overfitting than does the\nvanilla SWA by a dense and overfit-aware stochastic weight sampling strategy.\nSWAD shows state-of-the-art performances on five DG benchmarks, namely PACS,\nVLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large\nmargins of +1.6% averagely on out-of-domain accuracy. We also compare SWAD with\nconventional generalization methods, such as data augmentation and consistency\nregularization methods, to verify that the remarkable performance improvements\nare originated from by seeking flat minima, not from better in-domain\ngeneralizability. Last but not least, SWAD is readily adaptable to existing DG\nmethods without modification; the combination of SWAD and an existing DG method\nfurther improves DG performances. Source code is available at\nhttps://github.com/khanrc/swad.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1\">Junbum Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Han-Cheol Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yunsung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stronger NAS with Weaker Predictors. (arXiv:2102.10490v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.10490","description":"<p>Neural Architecture Search (NAS) often trains and evaluates a large number of\narchitectures. Recent predictor-based NAS approaches attempt to alleviate such\nheavy computation costs with two key steps: sampling some\narchitecture-performance pairs and fitting a proxy accuracy predictor. Given\nlimited samples, these predictors, however, are far from accurate to locate top\narchitectures due to the difficulty of fitting the huge search space. This\npaper reflects on a simple yet crucial question: if our final goal is to find\nthe best architecture, do we really need to model the whole space well?. We\npropose a paradigm shift from fitting the whole architecture space using one\nstrong predictor, to progressively fitting a search path towards the\nhigh-performance sub-space through a set of weaker predictors. As a key\nproperty of the weak predictors, their probabilities of sampling better\narchitectures keep increasing. Hence we only sample a few well-performed\narchitectures guided by the previously learned predictor and estimate a new\nbetter weak predictor. This embarrassingly easy framework, dubbed WeakNAS,\nproduces coarse-to-fine iteration to gradually refine the ranking of sampling\nspace. Extensive experiments demonstrate that WeakNAS costs fewer samples to\nfind top-performance architectures on NAS-Bench-101 and NAS-Bench-201. Compared\nto state-of-the-art (SOTA) predictor-based NAS methods, WeakNAS outperforms all\nwith notable margins, e.g., requiring at least 7.5x less samples to find global\noptimal on NAS-Bench-101. WeakNAS can also absorb their ideas to boost\nperformance more. Further, WeakNAS strikes the new SOTA result of 81.3% in the\nImageNet MobileNet Search Space. The code is available at\nhttps://github.com/VITA-Group/WeakNAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ye Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03743","description":"<p>We investigate the reasons for the performance degradation incurred with\nbatch-independent normalization. We find that the prototypical techniques of\nlayer normalization and instance normalization both induce the appearance of\nfailure modes in the neural network's pre-activations: (i) layer normalization\ninduces a collapse towards channel-wise constant functions; (ii) instance\nnormalization induces a lack of variability in instance statistics, symptomatic\nof an alteration of the expressivity. To alleviate failure mode (i) without\naggravating failure mode (ii), we introduce the technique \"Proxy Normalization\"\nthat normalizes post-activations using a proxy distribution. When combined with\nlayer normalization or group normalization, this batch-independent\nnormalization emulates batch normalization's behavior and consistently matches\nor exceeds its performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1\">Antoine Labatie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1\">Dominic Masters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1\">Zach Eaton-Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Evaluation of Deep Active Learning on Image Classification Tasks. (arXiv:2106.15324v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15324","description":"<p>With the goal of making deep learning more label-efficient, a growing number\nof papers have been studying active learning (AL) for deep models. However,\nthere are a number of issues in the prevalent experimental settings, mainly\nstemming from a lack of unified implementation and benchmarking. Issues in the\ncurrent literature include sometimes contradictory observations on the\nperformance of different AL algorithms, unintended exclusion of important\ngeneralization approaches such as data augmentation and SGD for optimization, a\nlack of study of evaluation facets like the labeling efficiency of AL, and\nlittle or no clarity on the scenarios in which AL outperforms random sampling\n(RS). In this work, we present a unified re-implementation of state-of-the-art\nAL algorithms in the context of image classification via our new open-source AL\ntoolkit DISTIL, and we carefully study these issues as facets of effective\nevaluation. On the positive side, we show that AL techniques are $2\\times$ to\n$4\\times$ more label-efficient compared to RS with the use of data\naugmentation. Surprisingly, when data augmentation is included, there is no\nlonger a consistent gain in using BADGE, a state-of-the-art approach, over\nsimple uncertainty sampling. We then do a careful analysis of how existing\napproaches perform with varying amounts of redundancy and number of examples\nper class. Finally, we provide several insights for AL practitioners to\nconsider in future work, such as the effect of the AL batch size, the effect of\ninitialization, the importance of retraining the model at every round, and\nother insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1\">Durga Sivasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dani_A/0/1/0/all/0/1\">Apurva Dani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-Aware Multi-Grid Deep Homography Estimation with Contextual Correlation. (arXiv:2107.02524v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02524","description":"<p>Homography estimation is an important task in computer vision applications,\nsuch as image stitching, video stabilization, and camera calibration.\nTraditional homography estimation methods heavily depend on the quantity and\ndistribution of feature correspondences, leading to poor robustness in\nlow-texture scenes. The learning solutions, on the contrary, try to learn\nrobust deep features but demonstrate unsatisfying performance in the scenes\nwith low overlap rates. In this paper, we address these two problems\nsimultaneously by designing a contextual correlation layer (CCL). The CCL can\nefficiently capture the long-range correlation within feature maps and can be\nflexibly used in a learning framework. In addition, considering that a single\nhomography can not represent the complex spatial transformation in\ndepth-varying images with parallax, we propose to predict multi-grid homography\nfrom global to local. Moreover, we equip our network with a depth perception\ncapability, by introducing a novel depth-aware shape-preserved loss. Extensive\nexperiments demonstrate the superiority of our method over state-of-the-art\nsolutions in the synthetic benchmark dataset and real-world dataset. The codes\nand models will be available at\nhttps://github.com/nie-lang/Multi-Grid-Deep-Homography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization via Gradient Surgery. (arXiv:2108.01621v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.01621","description":"<p>In real-life applications, machine learning models often face scenarios where\nthere is a change in data distribution between training and test domains. When\nthe aim is to make predictions on distributions different from those seen at\ntraining, we incur in a domain generalization problem. Methods to address this\nissue learn a model using data from multiple source domains, and then apply\nthis model to the unseen target domain. Our hypothesis is that when training\nwith multiple domains, conflicting gradients within each mini-batch contain\ninformation specific to the individual domains which is irrelevant to the\nothers, including the test domain. If left untouched, such disagreement may\ndegrade generalization performance. In this work, we characterize the\nconflicting gradients emerging in domain shift scenarios and devise novel\ngradient agreement strategies based on gradient surgery to alleviate their\neffect. We validate our approach in image classification tasks with three\nmulti-domain datasets, showing the value of the proposed agreement strategy in\nenhancing the generalization capability of deep learning models in domain shift\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mansilla_L/0/1/0/all/0/1\">Lucas Mansilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echeveste_R/0/1/0/all/0/1\">Rodrigo Echeveste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milone_D/0/1/0/all/0/1\">Diego H. Milone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Data Uncertainty in Object Tracking Algorithms. (arXiv:2109.10521v2 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2109.10521","description":"<p>Methodologies for incorporating the uncertainties characteristic of\ndata-driven object detectors into object tracking algorithms are explored.\nObject tracking methods rely on measurement error models, typically in the form\nof measurement noise, false positive rates, and missed detection rates. Each of\nthese quantities, in general, can be dependent on object or measurement\nlocation. However, for detections generated from neural-network processed\ncamera inputs, these measurement error statistics are not sufficient to\nrepresent the primary source of errors, namely a dissimilarity between run-time\nsensor input and the training data upon which the detector was trained. To this\nend, we investigate incorporating data uncertainty into object tracking methods\nsuch as to improve the ability to track objects, and particularly those which\nout-of-distribution w.r.t. training data. The proposed methodologies are\nvalidated on an object tracking benchmark as well on experiments with a real\nautonomous aircraft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Muthali_A/0/1/0/all/0/1\">Anish Muthali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laine_F/0/1/0/all/0/1\">Forrest Laine</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomlin_C/0/1/0/all/0/1\">Claire Tomlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDRVideo-GAN: Deep Generative HDR Video Reconstruction. (arXiv:2110.11795v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.11795","description":"<p>High dynamic range (HDR) videos provide a more visually realistic experience\nthan the standard low dynamic range (LDR) videos. Despite having significant\nprogress in HDR imaging, it is still a challenging task to capture high-quality\nHDR video with a conventional off-the-shelf camera. Existing approaches rely\nentirely on using dense optical flow between the neighboring LDR sequences to\nreconstruct an HDR frame. However, they lead to inconsistencies in color and\nexposure over time when applied to alternating exposures with noisy frames. In\nthis paper, we propose an end-to-end GAN-based framework for HDR video\nreconstruction from LDR sequences with alternating exposures. We first extract\nclean LDR frames from noisy LDR video with alternating exposures with a\ndenoising network trained in a self-supervised setting. Using optical flow, we\nthen align the neighboring alternating-exposure frames to a reference frame and\nthen reconstruct high-quality HDR frames in a complete adversarial setting. To\nfurther improve the robustness and quality of generated frames, we incorporate\ntemporal stability-based regularization term along with content and style-based\nlosses in the cost function during the training procedure. Experimental results\ndemonstrate that our framework achieves state-of-the-art performance and\ngenerates superior quality HDR frames of a video over the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Anand_M/0/1/0/all/0/1\">Mrinal Anand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harilal_N/0/1/0/all/0/1\">Nidhin Harilal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_C/0/1/0/all/0/1\">Chandan Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskSplit: Self-supervised Meta-learning for Few-shot Semantic Segmentation. (arXiv:2110.12207v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12207","description":"<p>Just like other few-shot learning problems, few-shot segmentation aims to\nminimize the need for manual annotation, which is particularly costly in\nsegmentation tasks. Even though the few-shot setting reduces this cost for\nnovel test classes, there is still a need to annotate the training data. To\nalleviate this need, we propose a self-supervised training approach for\nlearning few-shot segmentation models. We first use unsupervised saliency\nestimation to obtain pseudo-masks on images. We then train a simple prototype\nbased model over different splits of pseudo masks and augmentations of images.\nOur extensive experiments show that the proposed approach achieves promising\nresults, highlighting the potential of self-supervised training. To the best of\nour knowledge this is the first work that addresses unsupervised few-shot\nsegmentation problem on natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amac_M/0/1/0/all/0/1\">Mustafa Sercan Amac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sencan_A/0/1/0/all/0/1\">Ahmet Sencan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baran_O/0/1/0/all/0/1\">Orhun Bugra Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikizler_Cinbis_N/0/1/0/all/0/1\">Nazli Ikizler-Cinbis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masking Modalities for Cross-modal Video Retrieval. (arXiv:2111.01300v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.01300","description":"<p>Pre-training on large scale unlabelled datasets has shown impressive\nperformance improvements in the fields of computer vision and natural language\nprocessing. Given the advent of large-scale instructional video datasets, a\ncommon strategy for pre-training video encoders is to use the accompanying\nspeech as weak supervision. However, as speech is used to supervise the\npre-training, it is never seen by the video encoder, which does not learn to\nprocess that modality. We address this drawback of current pre-training\nmethods, which fail to exploit the rich cues in spoken language. Our proposal\nis to pre-train a video encoder using all the available video modalities as\nsupervision, namely, appearance, sound, and transcribed speech. We mask an\nentire modality in the input and predict it using the other two modalities.\nThis encourages each modality to collaborate with the others, and our video\nencoder learns to process appearance and audio as well as speech. We show the\nsuperior performance of our \"modality masking\" pre-training approach for video\nretrieval on the How2R, YouCook2 and Condensed Movies datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabeur_V/0/1/0/all/0/1\">Valentin Gabeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Split Vision Transformer for COVID-19 CXR Diagnosis using Task-Agnostic Training. (arXiv:2111.01338v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.01338","description":"<p>Federated learning, which shares the weights of the neural network across\nclients, is gaining attention in the healthcare sector as it enables training\non a large corpus of decentralized data while maintaining data privacy. For\nexample, this enables neural network training for COVID-19 diagnosis on chest\nX-ray (CXR) images without collecting patient CXR data across multiple\nhospitals. Unfortunately, the exchange of the weights quickly consumes the\nnetwork bandwidth if highly expressive network architecture is employed.\nSo-called split learning partially solves this problem by dividing a neural\nnetwork into a client and a server part, so that the client part of the network\ntakes up less extensive computation resources and bandwidth. However, it is not\nclear how to find the optimal split without sacrificing the overall network\nperformance. To amalgamate these methods and thereby maximize their distinct\nstrengths, here we show that the Vision Transformer, a recently developed deep\nlearning architecture with straightforward decomposable configuration, is\nideally suitable for split learning without sacrificing performance. Even under\nthe non-independent and identically distributed data distribution which\nemulates a real collaboration between hospitals using CXR datasets from\nmultiple sources, the proposed framework was able to attain performance\ncomparable to data-centralized training. In addition, the proposed framework\nalong with heterogeneous multi-task clients also improves individual task\nperformances including the diagnosis of COVID-19, eliminating the need for\nsharing large weights with innumerable parameters. Our results affirm the\nsuitability of Transformer for collaborative learning in medical imaging and\npave the way forward for future real-world implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sangjoon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1\">Gwanghyun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jeongsol Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_B/0/1/0/all/0/1\">Boah Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Vision Transformers Perform Convolution?. (arXiv:2111.01353v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.01353","description":"<p>Several recent studies have demonstrated that attention-based networks, such\nas Vision Transformer (ViT), can outperform Convolutional Neural Networks\n(CNNs) on several computer vision tasks without using convolutional layers.\nThis naturally leads to the following questions: Can a self-attention layer of\nViT express any convolution operation? In this work, we prove that a single ViT\nlayer with image patches as the input can perform any convolution operation\nconstructively, where the multi-head attention mechanism and the relative\npositional encoding play essential roles. We further provide a lower bound on\nthe number of heads for Vision Transformers to express CNNs. Corresponding with\nour analysis, experimental results show that the construction in our proof can\nhelp inject convolutional bias into Transformers and significantly improve the\nperformance of ViT in low data regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HHP-Net: A light Heteroscedastic neural network for Head Pose estimation with uncertainty. (arXiv:2111.01440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.01440","description":"<p>In this paper we introduce a novel method to estimate the head pose of people\nin single images starting from a small set of head keypoints. To this purpose,\nwe propose a regression model that exploits keypoints computed automatically by\n2D pose estimation algorithms and outputs the head pose represented by yaw,\npitch, and roll. Our model is simple to implement and more efficient with\nrespect to the state of the art -- faster in inference and smaller in terms of\nmemory occupancy -- with comparable accuracy. Our method also provides a\nmeasure of the heteroscedastic uncertainties associated with the three angles,\nthrough an appropriately designed loss function; we show there is a correlation\nbetween error and uncertainty values, thus this extra source of information may\nbe used in subsequent computational steps. As an example application, we\naddress social interaction analysis in images: we propose an algorithm for a\nquantitative estimation of the level of interaction between people, starting\nfrom their head poses and reasoning on their mutual positions. The code is\navailable at https://github.com/cantarinigiorgio/HHP-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cantarini_G/0/1/0/all/0/1\">Giorgio Cantarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomenotti_F/0/1/0/all/0/1\">Federico Figari Tomenotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noceti_N/0/1/0/all/0/1\">Nicoletta Noceti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odone_F/0/1/0/all/0/1\">Francesca Odone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Knowledge Distillation From the Perspective of Model Calibration. (arXiv:2111.01684v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.01684","description":"<p>Recent years have witnessed dramatically improvements in the knowledge\ndistillation, which can generate a compact student model for better efficiency\nwhile retaining the model effectiveness of the teacher model. Previous studies\nfind that: more accurate teachers do not necessary make for better teachers due\nto the mismatch of abilities. In this paper, we aim to analysis the phenomenon\nfrom the perspective of model calibration. We found that the larger teacher\nmodel may be too over-confident, thus the student model cannot effectively\nimitate. While, after the simple model calibration of the teacher model, the\nsize of the teacher model has a positive correlation with the performance of\nthe student model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jincen Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-11-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}