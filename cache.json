{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Defending Compositionality in Emergent Languages. (arXiv:2206.04751v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04751","description":"<p>Compositionality has traditionally been understood as a major factor in\nproductivity of language and, more broadly, human cognition. Yet, recently,\nsome research started to question its status, showing that artificial neural\nnetworks are good at generalization even without noticeable compositional\nbehavior. We argue that some of these conclusions are too strong and/or\nincomplete. In the context of a two-agent communication game, we show that\ncompositionality indeed seems essential for successful generalization when the\nevaluation is done on a proper dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Auersperger_M/0/1/0/all/0/1\">Michal Auersperger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pecina_P/0/1/0/all/0/1\">Pavel Pecina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Case for a Single Model that can Both Generate Continuations and Fill in the Blank. (arXiv:2206.04812v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04812","description":"<p>The task of inserting text into a specified position in a passage, known as\nfill in the blank (FitB), is useful for a variety of applications where writers\ninteract with a natural language generation (NLG) system to craft text. While\nprevious work has tackled this problem with models trained specifically to do\nthe fill-in-the-blank task, a more useful model is one that can effectively\nperform _both_ FitB and continuation. In this work, we evaluate the feasibility\nof using a single model to do both tasks. We show that models pre-trained with\na FitB-style objective are capable of both tasks, while models pre-trained for\ncontinuation are not. Finally, we show how FitB models can be easily finetuned\nto allow for fine-grained control over the length and word choice of the\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugan_L/0/1/0/all/0/1\">Liam Dugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ask to Know More: Generating Counterfactual Explanations for Fake Claims. (arXiv:2206.04869v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04869","description":"<p>In this paper, we propose elucidating fact checking predictions using\ncounterfactual explanations to help people understand why a specific piece of\nnews was identified as fake. In this work, generating counterfactual\nexplanations for fake news involves three steps: asking good questions, finding\ncontradictions, and reasoning appropriately. We frame this research question as\ncontradicted entailment reasoning through question answering (QA). We first ask\nquestions towards the false claim and retrieve potential answers from the\nrelevant evidence documents. Then, we identify the most contradictory answer to\nthe false claim by use of an entailment classifier. Finally, a counterfactual\nexplanation is created using a matched QA pair with three different\ncounterfactual explanation forms. Experiments are conducted on the FEVER\ndataset for both system and human evaluations. Results suggest that the\nproposed approach generates the most helpful explanations compared to\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shih-Chieh Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yi-Li Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_A/0/1/0/all/0/1\">Aiping Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural Machine Translation. (arXiv:2206.04922v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04922","description":"<p>Chinese dialect text-to-speech(TTS) system usually can only be utilized by\nnative linguists, because the written form of Chinese dialects has different\ncharacters, idioms, grammar and usage from Mandarin, and even the local speaker\ncannot input a correct sentence. For Mandarin text inputs, Chinese dialect TTS\ncan only generate partly-meaningful speech with relatively poor prosody and\nnaturalness. To lower the bar of use and make it more practical in commercial,\nwe propose a novel Chinese dialect TTS frontend with a translation module. It\nhelps to convert Mandarin text into idiomatic expressions with correct\northography and grammar, so that the intelligibility and naturalness of the\nsynthesized speech can be improved. A non-autoregressive neural machine\ntranslation model with a glancing sampling strategy is proposed for the\ntranslation task. It is the first known work to incorporate translation with\nTTS frontend. Our experiments on Cantonese approve that the proposed frontend\ncan help Cantonese TTS system achieve a 0.27 improvement in MOS with Mandarin\ninputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wudi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junjie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiang Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RuCoCo: a new Russian corpus with coreference annotation. (arXiv:2206.04925v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04925","description":"<p>We present a new corpus with coreference annotation, Russian Coreference\nCorpus (RuCoCo). The goal of RuCoCo is to obtain a large number of annotated\ntexts while maintaining high inter-annotator agreement. RuCoCo contains news\ntexts in Russian, part of which were annotated from scratch, and for the rest\nthe machine-generated annotations were refined by human annotators. The size of\nour corpus is one million words and around 150,000 mentions. We make the corpus\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dobrovolskii_V/0/1/0/all/0/1\">Vladimir Dobrovolskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michurina_M/0/1/0/all/0/1\">Mariia Michurina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivoylova_A/0/1/0/all/0/1\">Alexandra Ivoylova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sort by Structure: Language Model Ranking as Dependency Probing. (arXiv:2206.04935v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04935","description":"<p>Making an informed choice of pre-trained language model (LM) is critical for\nperformance, yet environmentally costly, and as such widely underexplored. The\nfield of Computer Vision has begun to tackle encoder ranking, with promising\nforays into Natural Language Processing, however they lack coverage of\nlinguistic tasks such as structured prediction. We propose probing to rank LMs,\nspecifically for parsing dependencies in a given language, by measuring the\ndegree to which labeled trees are recoverable from an LM's contextualized\nembeddings. Across 46 typologically and architecturally diverse LM-language\npairs, our probing approach predicts the best LM choice 79% of the time using\norders of magnitude less compute than training a full parser. Within this\nstudy, we identify and analyze one recently proposed decoupled LM - RemBERT -\nand find it strikingly contains less inherent dependency information, but often\nyields the best parser after full fine-tuning. Without this outlier our\napproach identifies the best LM in 89% of cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1\">Max M&#xfc;ller-Eberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate, Evaluate, and Select: A Dialogue System with a Response Evaluator for Diversity-Aware Response Generation. (arXiv:2206.04937v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04937","description":"<p>We aim to overcome the lack of diversity in responses of current dialogue\nsystems and to develop a dialogue system that is engaging as a conversational\npartner. We propose a generator-evaluator model that evaluates multiple\nresponses generated by a response generator and selects the best response by an\nevaluator. By generating multiple responses, we obtain diverse responses. We\nconduct human evaluations to compare the output of the proposed system with\nthat of a baseline system. The results of the human evaluations showed that the\nproposed system's responses were often judged to be better than the baseline\nsystem's, and indicated the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakaeda_R/0/1/0/all/0/1\">Ryoma Sakaeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_D/0/1/0/all/0/1\">Daisuke Kawahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Borrowing or Codeswitching? Annotating for Finer-Grained Distinctions in Language Mixing. (arXiv:2206.04973v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04973","description":"<p>We present a new corpus of Twitter data annotated for codeswitching and\nborrowing between Spanish and English. The corpus contains 9,500 tweets\nannotated at the token level with codeswitches, borrowings, and named entities.\nThis corpus differs from prior corpora of codeswitching in that we attempt to\nclearly define and annotate the boundary between codeswitching and borrowing\nand do not treat common \"internet-speak\" ('lol', etc.) as codeswitching when\nused in an otherwise monolingual context. The result is a corpus that enables\nthe study and modeling of Spanish-English borrowing and codeswitching on\nTwitter in one dataset. We present baseline scores for modeling the labels of\nthis corpus using Transformer-based language models. The annotation itself is\nreleased with a CC BY 4.0 license, while the text it applies to is distributed\nin compliance with the Twitter terms of service.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mellado_E/0/1/0/all/0/1\">Elena Alvarez Mellado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised and Few-shot Parsing from Pretrained Language Models. (arXiv:2206.04980v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04980","description":"<p>Pretrained language models are generally acknowledged to be able to encode\nsyntax [Tenney et al., 2019, Jawahar et al., 2019, Hewitt and Manning, 2019].\nIn this article, we propose UPOA, an Unsupervised constituent Parsing model\nthat calculates an Out Association score solely based on the self-attention\nweight matrix learned in a pretrained language model as the syntactic distance\nfor span segmentation. We further propose an enhanced version, UPIO, which\nexploits both inside association and outside association scores for estimating\nthe likelihood of a span. Experiments with UPOA and UPIO disclose that the\nlinear projection matrices for the query and key in the self-attention\nmechanism play an important role in parsing. We therefore extend the\nunsupervised models to few-shot parsing models (FPOA, FPIO) that use a few\nannotated trees to learn better linear projection matrices for parsing.\nExperiments on the Penn Treebank demonstrate that our unsupervised parsing\nmodel UPIO achieves results comparable to the state of the art on short\nsentences (length &lt;= 10). Our few-shot parsing model FPIO trained with only 20\nannotated trees outperforms a previous few-shot parsing method trained with 50\nannotated trees. Experiments on cross-lingual parsing show that both\nunsupervised and few-shot parsing methods are better than previous methods on\nmost languages of SPMRL [Seddah et al., 2013].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhiyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building an Icelandic Entity Linking Corpus. (arXiv:2206.05014v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05014","description":"<p>In this paper, we present the first Entity Linking corpus for Icelandic. We\ndescribe our approach of using a multilingual entity linking model (mGENRE) in\ncombination with Wikipedia API Search (WAPIS) to label our data and compare it\nto an approach using WAPIS only. We find that our combined method reaches 53.9%\ncoverage on our corpus, compared to 30.9% using only WAPIS. We analyze our\nresults and explain the value of using a multilingual system when working with\nIcelandic. Additionally, we analyze the data that remain unlabeled, identify\npatterns and discuss why they may be more difficult to annotate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fri%7B%5Cdh%7Driksdottir_S/0/1/0/all/0/1\">Steinunn Rut Fri&#xf0;riksd&#xf3;ttir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eggertsson_V/0/1/0/all/0/1\">Valdimar &#xc1;g&#xfa;st Eggertsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johannesson_B/0/1/0/all/0/1\">Benedikt Geir J&#xf3;hannesson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danielsson_H/0/1/0/all/0/1\">Hjalti Dan&#xed;elsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loftsson_H/0/1/0/all/0/1\">Hrafn Loftsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Einarsson_H/0/1/0/all/0/1\">Hafsteinn Einarsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Yet Efficient Method for Adversarial Word-Substitute Attack. (arXiv:2206.05015v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05015","description":"<p>NLP researchers propose different word-substitute black-box attacks that can\nfool text classification models. In such attack, an adversary keeps sending\ncrafted adversarial queries to the target model until it can successfully\nachieve the intended outcome. State-of-the-art attack methods usually require\nhundreds or thousands of queries to find one adversarial example. In this\npaper, we study whether a sophisticated adversary can attack the system with\nmuch less queries. We propose a simple yet efficient method that can reduce the\naverage number of adversarial queries by 3-30 times and maintain the attack\neffectiveness. This research highlights that an adversary can fool a deep NLP\nmodel with much less cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianle Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frictional Authors. (arXiv:2206.05016v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05016","description":"<p>I present a method for text analysis based on an analogy with the dynamic\nfriction of sliding surfaces. One surface is an array of points with a\n'friction coefficient' derived from the distribution frequency of a text's\nalphabetic characters. The other surface is a test patch having points with\nthis friction coefficient equal to a median value. Examples are presented from\nan analysis of a broad range of public domain texts, and comparison is made to\nthe Flesch Reading Ease. Source code for the analysis program is provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gualtieri_D/0/1/0/all/0/1\">Devlin Gualtieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Conversational Systems: A Review of Current Advances, Gaps, and Opportunities. (arXiv:2206.05017v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05017","description":"<p>The concept of empathy is vital in human-agent systems as it contributes to\nmutual understanding, problem-solving and sustained relationships. Despite the\nincreasing adoption of conversational systems as one of the most significant\nevents in the recent decade, the emotional aspects require considerable\nimprovements, particularly in effectively displaying empathy. This paper\nprovides a critical review of this rapidly growing field by examining the\ncurrent advances in four dimensions: (i) conceptual empathy models and\nframeworks, (ii) the adopted empathy-related concepts, (iii) the datasets and\nalgorithmic techniques developed, and (iv) the evaluation strategies. The\nreview findings show that the most studies centred on the use of the\nEMPATHETICDIALOGUES dataset, and the text-based modality dominated research in\nthis field. Moreover, studies have focused mainly on extracting features from\nthe messages of both users and the conversational systems, with minimal\nemphasis on user modelling and profiling. For implementation in variegated\nreal-world domain settings, we recommend that future studies address the gaps\nin detecting and authenticating emotions at the entity level, handling\nmultimodal inputs, displaying more nuanced empathetic behaviours, and\nencompassing additional dialogue system features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raamkumar_A/0/1/0/all/0/1\">Aravind Sesagiri Raamkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinping Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going Beyond the Cookie Theft Picture Test: Detecting Cognitive Impairments using Acoustic Features. (arXiv:2206.05018v1 [cs.SD])","link":"http://arxiv.org/abs/2206.05018","description":"<p>Standardized tests play a crucial role in the detection of cognitive\nimpairment. Previous work demonstrated that automatic detection of cognitive\nimpairment is possible using audio data from a standardized picture description\ntask. The presented study goes beyond that, evaluating our methods on data\ntaken from two standardized neuropsychological tests, namely the German SKT and\na German version of the CERAD-NB, and a semi-structured clinical interview\nbetween a patient and a psychologist. For the tests, we focus on speech\nrecordings of three sub-tests: reading numbers (SKT 3), interference (SKT 7),\nand verbal fluency (CERAD-NB 1). We show that acoustic features from\nstandardized tests can be used to reliably discriminate cognitively impaired\nindividuals from non-impaired ones. Furthermore, we provide evidence that even\nfeatures extracted from random speech samples of the interview can be a\ndiscriminator of cognitive impairment. In our baseline experiments, we use\nOpenSMILE features and Support Vector Machine classifiers. In an improved\nsetup, we show that using wav2vec 2.0 features instead, we can achieve an\naccuracy of up to 85%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Braun_F/0/1/0/all/0/1\">Franziska Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erzigkeit_A/0/1/0/all/0/1\">Andreas Erzigkeit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehfeld_H/0/1/0/all/0/1\">Hartmut Lehfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillemacher_T/0/1/0/all/0/1\">Thomas Hillemacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solution of DeBERTaV3 on CommonsenseQA. (arXiv:2206.05033v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05033","description":"<p>We report the performance of DeBERTaV3 on CommonsenseQA in this report. We\nsimply formalize the answer selection as a text classification for DeBERTaV3.\nThe strong natural language inference ability of DeBERTaV3 helps its single and\nensemble model set the new state-of-the-art on CommonsenseQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Construction and Evaluation of the LEAFTOP Dataset of Automatically Extracted Nouns in 1480 Languages. (arXiv:2206.05034v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05034","description":"<p>The LEAFTOP (language extracted automatically from thousands of passages)\ndataset consists of nouns that appear in multiple places in the four gospels of\nthe New Testament. We use a naive approach -- probabilistic inference -- to\nidentify likely translations in 1480 other languages. We evaluate this process\nand find that it provides lexiconaries with accuracy from 42% (Korafe) to 99%\n(Runyankole), averaging 72% correct across evaluated languages. The process\ntranslates up to 161 distinct lemmas from Koine Greek (average 159). We\nidentify nouns which appear to be easy and hard to translate, language families\nwhere this technique works, and future possible improvements and extensions.\nThe claims to novelty are: the use of a Koine Greek New Testament as the source\nlanguage; using a fully-annotated manually-created grammatically parse of the\nsource text; a custom scraper for texts in the target languages; a new metric\nfor language similarity; a novel strategy for evaluation on low-resource\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1\">Greg Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molla_Aliod_D/0/1/0/all/0/1\">Diego Molla-Aliod</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment analysis on electricity twitter posts. (arXiv:2206.05042v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05042","description":"<p>In today's world, everyone is expressive in some way, and the focus of this\nproject is on people's opinions about rising electricity prices in United\nKingdom and India using data from Twitter, a micro-blogging platform on which\npeople post messages, known as tweets. Because many people's incomes are not\ngood and they have to pay so many taxes and bills, maintaining a home has\nbecome a disputed issue these days. Despite the fact that Government offered\nsubsidy schemes to compensate people electricity bills but it is not welcomed\nby people. In this project, the aim is to perform sentiment analysis on\npeople's expressions and opinions expressed on Twitter. In order to grasp the\nelectricity prices opinion, it is necessary to carry out sentiment analysis for\nthe government and consumers in energy market. Furthermore, text present on\nthese medias are unstructured in nature, so to process them we firstly need to\npre-process the data. There are so many feature extraction techniques such as\nBag of Words, TF-IDF (Term Frequency-Inverse Document Frequency), word\nembedding, NLP based features like word count. In this project, we analysed the\nimpact of feature TF-IDF word level on electricity bills dataset of sentiment\nanalysis. We found that by using TF-IDF word level performance of sentiment\nanalysis is 3-4 higher than using N-gram features. Analysis is done using four\nclassification algorithms including Naive Bayes, Decision Tree, Random Forest,\nand Logistic Regression and considering F-Score, Accuracy, Precision, and\nRecall performance parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_P/0/1/0/all/0/1\">Pardeep Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edalati_M/0/1/0/all/0/1\">Maryam Edalati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REKnow: Enhanced Knowledge for Joint Entity and Relation Extraction. (arXiv:2206.05123v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05123","description":"<p>Relation extraction is an important but challenging task that aims to extract\nall hidden relational facts from the text. With the development of deep\nlanguage models, relation extraction methods have achieved good performance on\nvarious benchmarks. However, we observe two shortcomings of previous methods:\nfirst, there is no unified framework that works well under various relation\nextraction settings; second, effectively utilizing external knowledge as\nbackground information is absent. In this work, we propose a knowledge-enhanced\ngenerative model to mitigate these two issues. Our generative model is a\nunified framework to sequentially generate relational triplets under various\nrelation extraction settings and explicitly utilizes relevant knowledge from\nKnowledge Graph (KG) to resolve ambiguities. Our model achieves superior\nperformance on multiple benchmarks and settings, including WebNLG, NYT10, and\nTACRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teacher Perception of Automatically Extracted Grammar Concepts for L2 Language Learning. (arXiv:2206.05154v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05154","description":"<p>One of the challenges of language teaching is how to organize the rules\nregarding syntax, semantics, or phonology of the language in a meaningful\nmanner. This not only requires pedagogical skills, but also requires a deep\nunderstanding of that language. While comprehensive materials to develop such\ncurricula are available in English and some broadly spoken languages, for many\nother languages, teachers need to manually create them in response to their\nstudents' needs. This process is challenging because i) it requires that such\nexperts be accessible and have the necessary resources, and ii) even if there\nare such experts, describing all the intricacies of a language is\ntime-consuming and prone to omission. In this article, we present an automatic\nframework that aims to facilitate this process by automatically discovering and\nvisualizing descriptions of different aspects of grammar. Specifically, we\nextract descriptions from a natural text corpus that answer questions about\nmorphosyntax (learning of word order, agreement, case marking, or word\nformation) and semantics (learning of vocabulary) and show illustrative\nexamples. We apply this method for teaching the Indian languages, Kannada and\nMarathi, which, unlike English, do not have well-developed pedagogical\nresources and, therefore, are likely to benefit from this exercise. To assess\nthe perceived utility of the extracted material, we enlist the help of language\neducators from schools in North America who teach these languages to perform a\nmanual evaluation. Overall, teachers find the materials to be interesting as a\nreference material for their own lesson preparation or even for learner\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Arun Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheshadri_A/0/1/0/all/0/1\">Ashwin Sheshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nominal Metaphor Generation with Multitask Learning. (arXiv:2206.05195v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05195","description":"<p>Nominal metaphors are frequently used in human language and have been shown\nto be effective in persuading, expressing emotion, and stimulating interest.\nThis paper tackles the problem of Chinese Nominal Metaphor (NM) generation. We\nintroduce a novel multitask framework, which jointly optimizes three tasks: NM\nidentification, NM component identification, and NM generation. The metaphor\nidentification module is able to perform a self-training procedure, which\ndiscovers novel metaphors from a large-scale unlabeled corpus for NM\ngeneration. The NM component identification module emphasizes components during\ntraining and conditions the generation on these NM components for more coherent\nresults. To train the NM identification and component identification modules,\nwe construct an annotated corpus consisting of 6.3k sentences that contain\ndiverse metaphorical patterns. Automatic metrics show that our method can\nproduce diverse metaphors with good readability, where 92\\% of them are novel\nmetaphorical comparisons. Human evaluation shows our model significantly\noutperforms baselines on consistency and creativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geurin_F/0/1/0/all/0/1\">Frank Geurin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction. (arXiv:2206.05224v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05224","description":"<p>The recent advances of deep learning have dramatically changed how machine\nlearning, especially in the domain of natural language processing, can be\napplied to legal domain. However, this shift to the data-driven approaches\ncalls for larger and more diverse datasets, which are nevertheless still small\nin number, especially in non-English languages. Here we present the first\nlarge-scale benchmark of Korean legal AI datasets, LBox Open, that consists of\none legal corpus, two classification tasks, two legal judgement prediction\n(LJP) tasks, and one summarization task. The legal corpus consists of 150k\nKorean precedents (264M tokens), of which 63k are sentenced in last 4 years and\n96k are from the first and the second level courts in which factual issues are\nreviewed. The two classification tasks are case names (10k) and statutes (3k)\nprediction from the factual description of individual cases. The LJP tasks\nconsist of (1) 11k criminal examples where the model is asked to predict fine\namount, imprisonment with labor, and imprisonment without labor ranges for the\ngiven facts, and (2) 5k civil examples where the inputs are facts and claim for\nrelief and outputs are the degrees of claim acceptance. The summarization task\nconsists of the Supreme Court precedents and the corresponding summaries. We\nalso release LCube, the first Korean legal language model trained on the legal\ncorpus from this study. Given the uniqueness of the Law of South Korea and the\ndiversity of the legal tasks covered in this work, we believe that LBox Open\ncontributes to the multilinguality of global legal research. LBox Open and\nLCube will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyoungyeon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hanuhl Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction. (arXiv:2206.05238v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05238","description":"<p>The most prominent tasks in emotion analysis are to assign emotions to texts\nand to understand how emotions manifest in language. An important observation\nfor natural language processing is that emotions can be communicated implicitly\nby referring to events alone, appealing to an empathetic, intersubjective\nunderstanding of events, even without explicitly mentioning an emotion name. In\npsychology, the class of emotion theories known as appraisal theories aims at\nexplaining the link between events and emotions. Appraisals can be formalized\nas variables that measure a cognitive evaluation by people living through an\nevent that they consider relevant. They include the assessment if an event is\nnovel, if the person considers themselves to be responsible, if it is in line\nwith the own goals, and many others. Such appraisals explain which emotions are\ndeveloped based on an event, e.g., that a novel situation can induce surprise\nor one with uncertain consequences could evoke fear. We analyze the suitability\nof appraisal theories for emotion analysis in text with the goal of\nunderstanding if appraisal concepts can reliably be reconstructed by\nannotators, if they can be predicted by text classifiers, and if appraisal\nconcepts help to identify emotion categories. To achieve that, we compile a\ncorpus by asking people to textually describe events that triggered particular\nemotions and to disclose their appraisals. Then, we ask readers to reconstruct\nemotions and appraisals from the text. This setup allows us to measure if\nemotions and appraisals can be recovered purely from text and provides a human\nbaseline to judge model's performance measures. Our comparison of text\nclassification methods to human annotators shows that both can reliably detect\nemotions and appraisals with similar performance. We further show that\nappraisal concepts improve the categorization of emotions in text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCon: A Self-Supervised Approach for Controlled Text Generation. (arXiv:2006.03535v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.03535","description":"<p>Pretrained Transformer-based language models (LMs) display remarkable natural\nlanguage generation capabilities. With their immense potential, controlling\ntext generation of such LMs is getting attention. While there are studies that\nseek to control high-level attributes (such as sentiment and topic) of\ngenerated text, there is still a lack of more precise control over its content\nat the word- and phrase-level. Here, we propose Content-Conditioner (CoCon) to\ncontrol an LM's output text with a content input, at a fine-grained level. In\nour self-supervised approach, the CoCon block learns to help the LM complete a\npartially-observed text sequence by conditioning with content inputs that are\nwithheld from the LM. Through experiments, we show that CoCon can naturally\nincorporate target content into generated texts and control high-level text\nattributes in a zero-shot manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alvin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew-Soon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pung_B/0/1/0/all/0/1\">Bill Pung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AxFormer: Accuracy-driven Approximation of Transformers for Faster, Smaller and more Accurate NLP Models. (arXiv:2010.03688v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.03688","description":"<p>Transformers have greatly advanced the state-of-the-art in Natural Language\nProcessing (NLP) in recent years, but present very large computation and\nstorage requirements. We observe that the design process of Transformers\n(pre-train a foundation model on a large dataset in a self-supervised manner,\nand subsequently fine-tune it for different downstream tasks) leads to\ntask-specific models that are highly over-parameterized, adversely impacting\nboth accuracy and inference efficiency. We propose AxFormer, a systematic\nframework that applies accuracy-driven approximations to create optimized\ntransformer models for a given downstream task. AxFormer combines two key\noptimizations -- accuracy-driven pruning and selective hard attention.\nAccuracy-driven pruning identifies and removes parts of the fine-tuned\ntransformer that hinder performance on the given downstream task. Sparse\nhard-attention optimizes attention blocks in selected layers by eliminating\nirrelevant word aggregations, thereby helping the model focus only on the\nrelevant parts of the input. In effect, AxFormer leads to models that are more\naccurate, while also being faster and smaller. Our experiments on GLUE and\nSQUAD tasks show that AxFormer models are up to 4.5% more accurate, while also\nbeing up to 2.5X faster and up to 3.2X smaller than conventional fine-tuned\nmodels. In addition, we demonstrate that AxFormer can be combined with previous\nefforts such as distillation or quantization to achieve further efficiency\ngains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagarajan_A/0/1/0/all/0/1\">Amrit Nagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1\">Sanchari Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_J/0/1/0/all/0/1\">Jacob R. Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Anand Raghunathan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTG: A Benchmark Suite for Multilingual Text Generation. (arXiv:2108.07140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07140","description":"<p>We introduce MTG, a new benchmark suite for training and evaluating\nmultilingual text generation. It is the first-proposed multilingual multiway\ntext generation dataset with the largest human-annotated data (400k). It\nincludes four generation tasks (story generation, question generation, title\ngeneration and text summarization) across five languages (English, German,\nFrench, Spanish and Chinese). The multiway setup enables testing knowledge\ntransfer capabilities for a model across languages and tasks. Using MTG, we\ntrain and analyze several popular multilingual generation models from different\naspects. Our benchmark suite fosters model performance enhancement with more\nhuman-annotated parallel data. It provides comprehensive evaluations with\ndiverse generation scenarios. Code and data are available at\n\\url{https://github.com/zide05/MTG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhenqiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xianze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison and Combination of Sentence Embeddings Derived from Different Supervision Signals. (arXiv:2202.02990v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02990","description":"<p>There have been many successful applications of sentence embedding methods.\nHowever, it has not been well understood what properties are captured in the\nresulting sentence embeddings depending on the supervision signals. In this\npaper, we focus on two types of sentence embedding methods with similar\narchitectures and tasks: one fine-tunes pre-trained language models on the\nnatural language inference task, and the other fine-tunes pre-trained language\nmodels on word prediction task from its definition sentence, and investigate\ntheir properties. Specifically, we compare their performances on semantic\ntextual similarity (STS) tasks using STS datasets partitioned from two\nperspectives: 1) sentence source and 2) superficial similarity of the sentence\npairs, and compare their performances on the downstream and probing tasks.\nFurthermore, we attempt to combine the two methods and demonstrate that\ncombining the two methods yields substantially better performance than the\nrespective methods on unsupervised STS tasks and downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsukagoshi_H/0/1/0/all/0/1\">Hayato Tsukagoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1\">Ryohei Sasano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koichi Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.07028","description":"<p>We study the problem of developing autonomous agents that can follow human\ninstructions to infer and perform a sequence of actions to complete the\nunderlying task. Significant progress has been made in recent years, especially\nfor tasks with short horizons. However, when it comes to long-horizon tasks\nwith extended sequences of actions, an agent can easily ignore some\ninstructions or get stuck in the middle of the long instructions and eventually\nfail the task. To address this challenge, we propose a model-agnostic\nmilestone-based task tracker (M-TRACK) to guide the agent and monitor its\nprogress. Specifically, we propose a milestone builder that tags the\ninstructions with navigation and interaction milestones which the agent needs\nto complete step by step, and a milestone checker that systemically checks the\nagent's progress in its current milestone and determines when to proceed to the\nnext. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and\n52% relative improvement in unseen success rate over two competitive base\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1\">Jihyung Kil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps. (arXiv:2203.09127v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09127","description":"<p>Pre-trained models (PTMs) have become a fundamental backbone for downstream\ntasks in natural language processing and computer vision. Despite initial gains\nthat were obtained by applying generic PTMs to geo-related tasks at Baidu Maps,\na clear performance plateau over time was observed. One of the main reasons for\nthis plateau is the lack of readily available geographic knowledge in generic\nPTMs. To address this problem, in this paper, we present ERNIE-GeoL, which is a\ngeography-and-language pre-trained model designed and developed for improving\nthe geo-related tasks at Baidu Maps. ERNIE-GeoL is elaborately designed to\nlearn a universal representation of geography-language by pre-training on\nlarge-scale data generated from a heterogeneous graph that contains abundant\ngeographic knowledge. Extensive quantitative and qualitative experiments\nconducted on large-scale real-world datasets demonstrate the superiority and\neffectiveness of ERNIE-GeoL. ERNIE-GeoL has already been deployed in production\nat Baidu Maps since April 2021, which significantly benefits the performance of\nvarious downstream tasks. This demonstrates that ERNIE-GeoL can serve as a\nfundamental backbone for a wide range of geo-related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jizhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yibo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_A/0/1/0/all/0/1\">An Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CompactIE: Compact Facts in Open Information Extraction. (arXiv:2205.02880v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02880","description":"<p>A major drawback of modern neural OpenIE systems and benchmarks is that they\nprioritize high coverage of information in extractions over compactness of\ntheir constituents. This severely limits the usefulness of OpenIE extractions\nin many downstream tasks. The utility of extractions can be improved if\nextractions are compact and share constituents. To this end, we study the\nproblem of identifying compact extractions with neural-based methods. We\npropose CompactIE, an OpenIE system that uses a novel pipelined approach to\nproduce compact extractions with overlapping constituents. It first detects\nconstituents of the extractions and then links them to build extractions. We\ntrain our system on compact extractions obtained by processing existing\nbenchmarks. Our experiments on CaRB and Wire57 datasets indicate that CompactIE\nfinds 1.5x-2x more compact extractions than previous systems, with high\nprecision, establishing a new state-of-the-art performance in OpenIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayat_F/0/1/0/all/0/1\">Farima Fatahi Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadish_H/0/1/0/all/0/1\">H.V. Jagadish</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph - Deep Learning: A Case Study in Question Answering in Aviation Safety Domain. (arXiv:2205.15952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15952","description":"<p>In the commercial aviation domain, there are a large number of documents,\nlike, accident reports (NTSB, ASRS) and regulatory directives (ADs). There is a\nneed for a system to access these diverse repositories efficiently in order to\nservice needs in the aviation industry, like maintenance, compliance, and\nsafety. In this paper, we propose a Knowledge Graph (KG) guided Deep Learning\n(DL) based Question Answering (QA) system for aviation safety. We construct a\nKnowledge Graph from Aircraft Accident reports and contribute this resource to\nthe community of researchers. The efficacy of this resource is tested and\nproved by the aforesaid QA system. Natural Language Queries constructed from\nthe documents mentioned above are converted into SPARQL (the interface language\nof the RDF graph database) queries and answered. On the DL side, we have two\ndifferent QA models: (i) BERT QA which is a pipeline of Passage Retrieval\n(Sentence-BERT based) and Question Answering (BERT based), and (ii) the\nrecently released GPT-3. We evaluate our system on a set of queries created\nfrom the accident reports. Our combined QA system achieves 9.3% increase in\naccuracy over GPT-3 and 40.3% increase over BERT QA. Thus, we infer that KG-DL\nperforms better than either singly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ankush Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gite_R/0/1/0/all/0/1\">Raj Gite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laddha_S/0/1/0/all/0/1\">Shreya Laddha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1\">Satyanarayan Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thind_P/0/1/0/all/0/1\">Prabhjit Thind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zele_R/0/1/0/all/0/1\">Rajesh Zele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_R/0/1/0/all/0/1\">Ravi Shankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04615","description":"<p>Language models demonstrate both quantitative improvement and new qualitative\ncapabilities with increasing scale. Despite their potentially transformative\nimpact, these new capabilities are as yet poorly characterized. In order to\ninform future research, prepare for disruptive new model capabilities, and\nameliorate socially harmful effects, it is vital that we understand the present\nand near-future capabilities and limitations of language models. To address\nthis challenge, we introduce the Beyond the Imitation Game benchmark\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442\nauthors across 132 institutions. Task topics are diverse, drawing problems from\nlinguistics, childhood development, math, common-sense reasoning, biology,\nphysics, social bias, software development, and beyond. BIG-bench focuses on\ntasks that are believed to be beyond the capabilities of current language\nmodels. We evaluate the behavior of OpenAI's GPT models, Google-internal dense\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\nacross model sizes spanning millions to hundreds of billions of parameters. In\naddition, a team of human expert raters performed all tasks in order to provide\na strong baseline. Findings include: model performance and calibration both\nimprove with scale, but are poor in absolute terms (and when compared with\nrater performance); performance is remarkably similar across model classes,\nthough with benefits from sparsity; tasks that improve gradually and\npredictably commonly involve a large knowledge or memorization component,\nwhereas tasks that exhibit \"breakthrough\" behavior at a critical scale often\ninvolve multiple steps or components, or brittle metrics; social bias typically\nincreases with scale in settings with ambiguous context, but this can be\nimproved with prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Aarohi Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Abhishek Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeb_A/0/1/0/all/0/1\">Abu Awal Md Shoeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_A/0/1/0/all/0/1\">Abubakar Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1\">Adam R. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garriga_Alonso_A/0/1/0/all/0/1\">Adri&#xe0; Garriga-Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kluska_A/0/1/0/all/0/1\">Agnieszka Kluska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Akshat Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1\">Alethea Power</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Alex Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alex Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocurek_A/0/1/0/all/0/1\">Alexander W. Kocurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safaya_A/0/1/0/all/0/1\">Ali Safaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1\">Ali Tazarv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1\">Alice Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_A/0/1/0/all/0/1\">Allen Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Aman Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_A/0/1/0/all/0/1\">Amanda Dsouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slone_A/0/1/0/all/0/1\">Ambrose Slone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahane_A/0/1/0/all/0/1\">Ameet Rahane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Anantharaman S. Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuhlmuller_A/0/1/0/all/0/1\">Andreas Stuhlm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+La_A/0/1/0/all/0/1\">Andrew La</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Angela Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_A/0/1/0/all/0/1\">Anh Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Animesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottardi_A/0/1/0/all/0/1\">Anna Gottardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norelli_A/0/1/0/all/0/1\">Antonio Norelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_A/0/1/0/all/0/1\">Anu Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholamidavoodi_A/0/1/0/all/0/1\">Arash Gholamidavoodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabassum_A/0/1/0/all/0/1\">Arfa Tabassum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirubarajan_A/0/1/0/all/0/1\">Arun Kirubarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullokandov_A/0/1/0/all/0/1\">Asher Mullokandov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrick_A/0/1/0/all/0/1\">Austin Herrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, et al. (394 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Extending Momentum Contrast with Cross Similarity Consistency Regularization. (arXiv:2206.04676v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04676","description":"<p>Contrastive self-supervised representation learning methods maximize the\nsimilarity between the positive pairs, and at the same time tend to minimize\nthe similarity between the negative pairs. However, in general the interplay\nbetween the negative pairs is ignored as they do not put in place special\nmechanisms to treat negative pairs differently according to their specific\ndifferences and similarities. In this paper, we present Extended Momentum\nContrast (XMoCo), a self-supervised representation learning method founded upon\nthe legacy of the momentum-encoder unit proposed in the MoCo family\nconfigurations. To this end, we introduce a cross consistency regularization\nloss, with which we extend the transformation consistency to dissimilar images\n(negative pairs). Under the cross consistency regularization rule, we argue\nthat semantic representations associated with any pair of images (positive or\nnegative) should preserve their cross-similarity under pretext transformations.\nMoreover, we further regularize the training loss by enforcing a uniform\ndistribution of similarity over the negative pairs across a batch. The proposed\nregularization can easily be added to existing self-supervised learning\nalgorithms in a plug-and-play fashion. Empirically, we report a competitive\nperformance on the standard Imagenet-1K linear head classification benchmark.\nIn addition, by transferring the learned representations to common downstream\ntasks, we show that using XMoCo with the prevalently utilized augmentations can\nlead to improvements in the performance of such tasks. We hope the findings of\nthis paper serve as a motivation for researchers to take into consideration the\nimportant interplay among the negative examples in self-supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seyfi_M/0/1/0/all/0/1\">Mehdi Seyfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Backdoor Attacks Survive Time-Varying Models?. (arXiv:2206.04677v1 [cs.CR])","link":"http://arxiv.org/abs/2206.04677","description":"<p>Backdoors are powerful attacks against deep neural networks (DNNs). By\npoisoning training data, attackers can inject hidden rules (backdoors) into\nDNNs, which only activate on inputs containing attack-specific triggers. While\nexisting work has studied backdoor attacks on a variety of DNN models, they\nonly consider static models, which remain unchanged after initial deployment.\n</p>\n<p>In this paper, we study the impact of backdoor attacks on a more realistic\nscenario of time-varying DNN models, where model weights are updated\nperiodically to handle drifts in data distribution over time. Specifically, we\nempirically quantify the \"survivability\" of a backdoor against model updates,\nand examine how attack parameters, data drift behaviors, and model update\nstrategies affect backdoor survivability. Our results show that one-shot\nbackdoor attacks (i.e., only poisoning training data once) do not survive past\na few model updates, even when attackers aggressively increase trigger size and\npoison ratio. To stay unaffected by model update, attackers must continuously\nintroduce corrupted data into the training pipeline. Together, these results\nindicate that when models are updated to learn new data, they also \"forget\"\nbackdoors as hidden, malicious features. The larger the distribution shift\nbetween old and new training data, the faster backdoors are forgotten.\nLeveraging these insights, we apply a smart learning rate scheduler to further\naccelerate backdoor forgetting during model updates, which prevents one-shot\nbackdoors from surviving past a single model update.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagoji_A/0/1/0/all/0/1\">Arjun Nitin Bhagoji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Ben Y. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution Samples. (arXiv:2206.04679v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04679","description":"<p>In this work, we propose to use out-of-distribution samples, i.e., unlabeled\nsamples coming from outside the target classes, to improve few-shot learning.\nSpecifically, we exploit the easily available out-of-distribution samples to\ndrive the classifier to avoid irrelevant features by maximizing the distance\nfrom prototypes to out-of-distribution samples while minimizing that of\nin-distribution samples (i.e., support, query data). Our approach is simple to\nimplement, agnostic to feature extractors, lightweight without any additional\ncost for pre-training, and applicable to both inductive and transductive\nsettings. Extensive experiments on various standard benchmarks demonstrate that\nthe proposed method consistently improves the performance of pretrained\nnetworks with different architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duong H. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi D. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc-Huy Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_R/0/1/0/all/0/1\">Rang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaussian Fourier Pyramid for Local Laplacian Filter. (arXiv:2206.04681v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04681","description":"<p>Multi-scale processing is essential in image processing and computer\ngraphics. Halos are a central issue in multi-scale processing. Several\nedge-preserving decompositions resolve halos, e.g., local Laplacian filtering\n(LLF), by extending the Laplacian pyramid to have an edge-preserving property.\nIts processing is costly; thus, an approximated acceleration of fast LLF was\nproposed to linearly interpolate multiple Laplacian pyramids. This paper\nfurther improves the accuracy by Fourier series expansion, named Fourier LLF.\nOur results showed that Fourier LLF has a higher accuracy for the same number\nof pyramids. Moreover, Fourier LLF exhibits parameter-adaptive property for\ncontent-adaptive filtering. The code is available at:\nhttps://norishigefukushima.github.io/GaussianFourierPyramid/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sumiya_Y/0/1/0/all/0/1\">Yuto Sumiya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otsuka_T/0/1/0/all/0/1\">Tomoki Otsuka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maeda_Y/0/1/0/all/0/1\">Yoshihiro Maeda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fukushima_N/0/1/0/all/0/1\">Norishige Fukushima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation. (arXiv:2206.04682v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04682","description":"<p>Accurately segmenting temporal frames of cine magnetic resonance imaging\n(MRI) is a crucial step in various real-time MRI guided cardiac interventions.\nTo achieve fast and accurate visual assistance, there are strict requirements\non the maximum latency and minimum throughput of the segmentation framework.\nState-of-the-art neural networks on this task are mostly hand-crafted to\nsatisfy these constraints while achieving high accuracy. On the other hand,\nwhile existing literature have demonstrated the power of neural architecture\nsearch (NAS) in automatically identifying the best neural architectures for\nvarious medical applications, they are mostly guided by accuracy, sometimes\nwith computation complexity, and the importance of real-time constraints are\noverlooked. A major challenge is that such constraints are non-differentiable\nand are thus not compatible with the widely used differentiable NAS frameworks.\nIn this paper, we present a strategy that directly handles real-time\nconstraints in a differentiable NAS framework named RT-DNAS. Experiments on\nextended 2017 MICCAI ACDC dataset show that compared with state-of-the-art\nmanually and automatically designed architectures, RT-DNAS is able to identify\nones with better accuracy while satisfying the real-time constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Q/0/1/0/all/0/1\">Qing Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_S/0/1/0/all/0/1\">Shunjie Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hao_C/0/1/0/all/0/1\">Callie Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuo_C/0/1/0/all/0/1\">Cheng Zhuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-consistent Restoration Network for Cataract Fundus Image Enhancement. (arXiv:2206.04684v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04684","description":"<p>Fundus photography is a routine examination in clinics to diagnose and\nmonitor ocular diseases. However, for cataract patients, the fundus image\nalways suffers quality degradation caused by the clouding lens. The degradation\nprevents reliable diagnosis by ophthalmologists or computer-aided systems. To\nimprove the certainty in clinical diagnosis, restoration algorithms have been\nproposed to enhance the quality of fundus images. Unfortunately, challenges\nremain in the deployment of these algorithms, such as collecting sufficient\ntraining data and preserving retinal structures. In this paper, to circumvent\nthe strict deployment requirement, a structure-consistent restoration network\n(SCR-Net) for cataract fundus images is developed from synthesized data that\nshares an identical structure. A cataract simulation model is firstly designed\nto collect synthesized cataract sets (SCS) formed by cataract fundus images\nsharing identical structures. Then high-frequency components (HFCs) are\nextracted from the SCS to constrain structure consistency such that the\nstructure preservation in SCR-Net is enforced. The experiments demonstrate the\neffectiveness of SCR-Net in the comparison with state-of-the-art methods and\nthe follow-up clinical applications. The code is available at\nhttps://github.com/liamheng/ArcNet-Medical-Image-Enhancement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Heng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haofeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shu_H/0/1/0/all/0/1\">Hai Shu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoling Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-based Clinical Assessment of Optic Nerve Head Robustness Superseding Biomechanical Testing. (arXiv:2206.04689v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04689","description":"<p>$\\mathbf{Purpose}$: To use artificial intelligence (AI) to: (1) exploit\nbiomechanical knowledge of the optic nerve head (ONH) from a relatively large\npopulation; (2) assess ONH robustness from a single optical coherence\ntomography (OCT) scan of the ONH; (3) identify what critical three-dimensional\n(3D) structural features make a given ONH robust.\n</p>\n<p>$\\mathbf{Design}$: Retrospective cross-sectional study.\n</p>\n<p>$\\mathbf{Methods}$: 316 subjects had their ONHs imaged with OCT before and\nafter acute intraocular pressure (IOP) elevation through ophthalmo-dynamometry.\nIOP-induced lamina-cribrosa deformations were then mapped in 3D and used to\nclassify ONHs. Those with LC deformations superior to 4% were considered\nfragile, while those with deformations inferior to 4% robust. Learning from\nthese data, we compared three AI algorithms to predict ONH robustness strictly\nfrom a baseline (undeformed) OCT volume: (1) a random forest classifier; (2) an\nautoencoder; and (3) a dynamic graph CNN (DGCNN). The latter algorithm also\nallowed us to identify what critical 3D structural features make a given ONH\nrobust.\n</p>\n<p>$\\mathbf{Results}$: All 3 methods were able to predict ONH robustness from 3D\nstructural information alone and without the need to perform biomechanical\ntesting. The DGCNN (area under the receiver operating curve [AUC]: 0.76 $\\pm$\n0.08) outperformed the autoencoder (AUC: 0.70 $\\pm$ 0.07) and the random forest\nclassifier (AUC: 0.69 $\\pm$ 0.05). Interestingly, to assess ONH robustness, the\nDGCNN mainly used information from the scleral canal and the LC insertion\nsites.\n</p>\n<p>$\\mathbf{Conclusions}$: We propose an AI-driven approach that can assess the\nrobustness of a given ONH solely from a single OCT scan of the ONH, and without\nthe need to perform biomechanical testing. Longitudinal studies should\nestablish whether ONH robustness could help us identify fast visual field loss\nprogressors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Braeu_F/0/1/0/all/0/1\">Fabian A. Braeu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chuangsuwanich_T/0/1/0/all/0/1\">Thanadet Chuangsuwanich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tun_T/0/1/0/all/0/1\">Tin A. Tun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thiery_A/0/1/0/all/0/1\">Alexandre H. Thiery</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aung_T/0/1/0/all/0/1\">Tin Aung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barbastathis_G/0/1/0/all/0/1\">George Barbastathis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Girard_M/0/1/0/all/0/1\">Micha&#xeb;l J.A. Girard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-MIA: COVID-19 Detection & Severity Analysis through Medical Imaging. (arXiv:2206.04732v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04732","description":"<p>This paper presents the baseline approach for the organized 2nd Covid-19\nCompetition, occurring in the framework of the AIMIA Workshop in the European\nConference on Computer Vision (ECCV 2022). It presents the COV19-CT-DB database\nwhich is annotated for COVID-19 detction, consisting of about 7,700 3-D CT\nscans. Part of the database consisting of Covid-19 cases is further annotated\nin terms of four Covid-19 severity conditions. We have split the database and\nthe latter part of it in training, validation and test datasets. The former two\ndatasets are used for training and validation of machine learning models, while\nthe latter will be used for evaluation of the developed models. The baseline\napproach consists of a deep learning approach, based on a CNN-RNN network and\nreport its performance on the COVID19-CT-DB database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arsenos_A/0/1/0/all/0/1\">Anastasios Arsenos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kollias_S/0/1/0/all/0/1\">Stefanos Kollias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Disentanglement of Negative-free Contrastive Learning. (arXiv:2206.04756v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04756","description":"<p>Negative-free contrastive learning has attracted a lot of attention with\nsimplicity and impressive performance for large-scale pretraining. But its\ndisentanglement property remains unexplored. In this paper, we take different\nnegative-free contrastive learning methods to study the disentanglement\nproperty of this genre of self-supervised methods empirically. We find the\nexisting disentanglement metrics fail to make meaningful measurements for the\nhigh-dimensional representation model so we propose a new disentanglement\nmetric based on Mutual Information between representation and data factors.\nWith the proposed metric, we benchmark the disentanglement property of\nnegative-free contrastive learning for the first time, on both popular\nsynthetic datasets and a real-world dataset CelebA. Our study shows that the\ninvestigated methods can learn a well-disentangled subset of representation. We\nextend the study of the disentangled representation learning to\nhigh-dimensional representation space and negative-free contrastive learning\nfor the first time. The implementation of the proposed metric is available at\n\\url{https://github.com/noahcao/disentanglement_lib_med}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jinkun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nai_R/0/1/0/all/0/1\">Ruiqian Nai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jialei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What should AI see? Using the Public's Opinion to Determine the Perception of an AI. (arXiv:2206.04776v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04776","description":"<p>Deep neural networks (DNN) have made impressive progress in the\ninterpretation of image data, so that it is conceivable and to some degree\nrealistic to use them in safety critical applications like automated driving.\nFrom an ethical standpoint, the AI algorithm should take into account the\nvulnerability of objects or subjects on the street that ranges from \"not at\nall\", e.g. the road itself, to \"high vulnerability\" of pedestrians. One way to\ntake this into account is to define the cost of confusion of one semantic\ncategory with another and use cost-based decision rules for the interpretation\nof probabilities, which are the output of DNNs. However, it is an open problem\nhow to define the cost structure, who should be in charge to do that, and\nthereby define what AI-algorithms will actually \"see\". As one possible answer,\nwe follow a participatory approach and set up an online survey to ask the\npublic to define the cost structure. We present the survey design and the data\nacquired along with an evaluation that also distinguishes between perspective\n(car passenger vs. external traffic participant) and gender. Using simulation\nbased $F$-tests, we find highly significant differences between the groups.\nThese differences have consequences on the reliable detection of pedestrians in\na safety critical distance to the self-driving car. We discuss the ethical\nproblems that are related to this approach and also discuss the problems\nemerging from human-machine interaction through the survey from a psychological\npoint of view. Finally, we include comments from industry leaders in the field\nof AI safety on the applicability of survey based elements in the design of AI\nfunctionalities in automated driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Robin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dardashti_R/0/1/0/all/0/1\">Radin Dardashti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_M/0/1/0/all/0/1\">Meike Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruggemann_D/0/1/0/all/0/1\">Dominik Br&#xfc;ggemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rucker_C/0/1/0/all/0/1\">Cilia R&#xfc;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_P/0/1/0/all/0/1\">Peter Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huger_F/0/1/0/all/0/1\">Fabian H&#xfc;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rummel_N/0/1/0/all/0/1\">Nikol Rummel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04779","description":"<p>Offline reinforcement learning has shown great promise in leveraging large\npre-collected datasets for policy learning, allowing agents to forgo\noften-expensive online data collection. However, to date, offline reinforcement\nlearning from has been relatively under-explored, and there is a lack of\nunderstanding of where the remaining challenges lie. In this paper, we seek to\nestablish simple baselines for continuous control in the visual domain. We show\nthat simple modifications to two state-of-the-art vision-based online\nreinforcement learning algorithms, DreamerV2 and DrQ-v2, suffice to outperform\nprior work and establish a competitive baseline. We rigorously evaluate these\nalgorithms on both existing offline datasets and a new testbed for offline\nreinforcement learning from visual observations that better represents the data\ndistributions present in real-world offline reinforcement learning problems,\nand open-source our code and data to facilitate progress in this important\ndomain. Finally, we present and analyze several key desiderata unique to\noffline RL from visual observations, including visual distractions and visually\nidentifiable changes in dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1\">Philip J. Ball</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1\">Tim G. J. Rudner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1\">Michael A. Osborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReFace: Real-time Adversarial Attacks on Face Recognition Systems. (arXiv:2206.04783v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04783","description":"<p>Deep neural network based face recognition models have been shown to be\nvulnerable to adversarial examples. However, many of the past attacks require\nthe adversary to solve an input-dependent optimization problem using gradient\ndescent which makes the attack impractical in real-time. These adversarial\nexamples are also tightly coupled to the attacked model and are not as\nsuccessful in transferring to different models. In this work, we propose\nReFace, a real-time, highly-transferable attack on face recognition models\nbased on Adversarial Transformation Networks (ATNs). ATNs model adversarial\nexample generation as a feed-forward neural network. We find that the white-box\nattack success rate of a pure U-Net ATN falls substantially short of\ngradient-based attacks like PGD on large face recognition datasets. We\ntherefore propose a new architecture for ATNs that closes this gap while\nmaintaining a 10000x speedup over PGD. Furthermore, we find that at a given\nperturbation magnitude, our ATN adversarial perturbations are more effective in\ntransferring to new face recognition models than PGD. ReFace attacks can\nsuccessfully deceive commercial face recognition services in a transfer attack\nsetting and reduce face identification accuracy from 82% to 16.4% for AWS\nSearchFaces API and Azure face verification accuracy from 91% to 50.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1\">Shehzeen Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huster_T/0/1/0/all/0/1\">Todd Huster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesterharm_C/0/1/0/all/0/1\">Chris Mesterharm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neekhara_P/0/1/0/all/0/1\">Paarth Neekhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1\">Kevin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jere_M/0/1/0/all/0/1\">Malhar Jere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_H/0/1/0/all/0/1\">Harshvardhan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Spatio-temporal Transformers for Egocentric 3D Pose Estimation. (arXiv:2206.04785v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04785","description":"<p>Egocentric 3D human pose estimation (HPE) from images is challenging due to\nsevere self-occlusions and strong distortion introduced by the fish-eye view\nfrom the head mounted camera. Although existing works use intermediate\nheatmap-based representations to counter distortion with some success,\naddressing self-occlusion remains an open problem. In this work, we leverage\ninformation from past frames to guide our self-attention-based 3D HPE\nestimation procedure -- Ego-STAN. Specifically, we build a spatio-temporal\nTransformer model that attends to semantically rich convolutional neural\nnetwork-based feature maps. We also propose feature map tokens: a new set of\nlearnable parameters to attend to these feature maps. Finally, we demonstrate\nEgo-STAN's superior performance on the xR-EgoPose dataset where it achieves a\n30.6% improvement on the overall mean per-joint position error, while leading\nto a 22% drop in parameters compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinman Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaai_K/0/1/0/all/0/1\">Kimathi Kaai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1\">Saad Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumi_N/0/1/0/all/0/1\">Norikatsu Sumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1\">Sirisha Rambhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1\">Paul Fieguth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition. (arXiv:2206.04790v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04790","description":"<p>We address the problem of data augmentation for video action recognition.\nStandard augmentation strategies in video are hand-designed and sample the\nspace of possible augmented data points either at random, without knowing which\naugmented points will be better, or through heuristics. We propose to learn\nwhat makes a good video for action recognition and select only high-quality\nsamples for augmentation. In particular, we choose video compositing of a\nforeground and a background video as the data augmentation process, which\nresults in diverse and realistic new samples. We learn which pairs of videos to\naugment without having to actually composite them. This reduces the space of\npossible augmentations, which has two advantages: it saves computational cost\nand increases the accuracy of the final trained classifier, as the augmented\npairs are of higher quality than average. We present experimental results on\nthe entire spectrum of training settings: few-shot, semi-supervised and fully\nsupervised. We observe consistent improvements across all of them over prior\nwork and baselines on Kinetics, UCF101, HMDB51, and achieve a new\nstate-of-the-art on settings with limited data. We see improvements of up to\n8.6% in the semi-supervised setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stable and memory-efficient image recovery using monotone operator learning (MOL). (arXiv:2206.04797v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04797","description":"<p>We introduce a monotone deep equilibrium learning framework for large-scale\ninverse problems in imaging. The proposed algorithm relies on forward-backward\nsplitting, where each iteration consists of a gradient descent involving the\nscore function and a conjugate gradient algorithm to encourage data\nconsistency. The score function is modeled as a monotone convolutional neural\nnetwork. The use of a monotone operator offers several benefits, including\nguaranteed convergence, uniqueness of fixed point, and robustness to input\nperturbations, similar to the use of convex priors in compressive sensing. In\naddition, the proposed formulation is significantly more memory-efficient than\nunrolled methods, which allows us to apply it to 3D problems that current\nunrolled algorithms cannot handle. Experiments show that the proposed scheme\ncan offer improved performance in 3D settings while being stable in the\npresence of input perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_A/0/1/0/all/0/1\">Aniket Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_M/0/1/0/all/0/1\">Mathews Jacob</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R4D: Utilizing Reference Objects for Long-Range Distance Estimation. (arXiv:2206.04831v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04831","description":"<p>Estimating the distance of objects is a safety-critical task for autonomous\ndriving. Focusing on short-range objects, existing methods and datasets neglect\nthe equally important long-range objects. In this paper, we introduce a\nchallenging and under-explored task, which we refer to as Long-Range Distance\nEstimation, as well as two datasets to validate new methods developed for this\ntask. We then proposeR4D, the first framework to accurately estimate the\ndistance of long-range objects by using references with known distances in the\nscene. Drawing inspiration from human perception, R4D builds a graph by\nconnecting a target object to all references. An edge in the graph encodes the\nrelative distance information between a pair of target and reference objects.\nAn attention module is then used to weigh the importance of reference objects\nand combine them into one target object distance prediction. Experiments on the\ntwo proposed datasets demonstrate the effectiveness and robustness of R4D by\nshowing significant improvements compared to existing baselines. We are looking\nto make the proposed dataset, Waymo OpenDataset - Long-Range Labels, available\npublicly at waymo.com/open/download.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabkab_M/0/1/0/all/0/1\">Maya Kabkab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Ruichi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Longlong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yurong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Autoencoders are Robust Data Augmentors. (arXiv:2206.04846v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04846","description":"<p>Deep neural networks are capable of learning powerful representations to\ntackle complex vision tasks but expose undesirable properties like the\nover-fitting issue. To this end, regularization techniques like image\naugmentation are necessary for deep neural networks to generalize well.\nNevertheless, most prevalent image augmentation recipes confine themselves to\noff-the-shelf linear transformations like scale, flip, and colorjitter. Due to\ntheir hand-crafted property, these augmentations are insufficient to generate\ntruly hard augmented examples. In this paper, we propose a novel perspective of\naugmentation to regularize the training process. Inspired by the recent success\nof applying masked image modeling to self-supervised learning, we adopt the\nself-supervised masked autoencoder to generate the distorted view of the input\nimages. We show that utilizing such model-based nonlinear transformation as\ndata augmentation can improve high-level recognition tasks. We term the\nproposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation\n(MRA). The extensive experiments on various image classification benchmarks\nverify the effectiveness of the proposed augmentation. Specifically, MRA\nconsistently enhances the performance on supervised, semi-supervised as well as\nfew-shot classification. The code will be available at\n\\url{https://github.com/haohang96/MRA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haohang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuangrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Face Recognition via Face Synthesis with Identity-Attribute Disentanglement. (arXiv:2206.04854v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04854","description":"<p>Heterogeneous Face Recognition (HFR) aims to match faces across different\ndomains (e.g., visible to near-infrared images), which has been widely applied\nin authentication and forensics scenarios. However, HFR is a challenging\nproblem because of the large cross-domain discrepancy, limited heterogeneous\ndata pairs, and large variation of facial attributes. To address these\nchallenges, we propose a new HFR method from the perspective of heterogeneous\ndata augmentation, named Face Synthesis with Identity-Attribute Disentanglement\n(FSIAD). Firstly, the identity-attribute disentanglement (IAD) decouples face\nimages into identity-related representations and identity-unrelated\nrepresentations (called attributes), and then decreases the correlation between\nidentities and attributes. Secondly, we devise a face synthesis module (FSM) to\ngenerate a large number of images with stochastic combinations of disentangled\nidentities and attributes for enriching the attribute diversity of synthetic\nimages. Both the original images and the synthetic ones are utilized to train\nthe HFR network for tackling the challenges and improving the performance of\nHFR. Extensive experiments on five HFR databases validate that FSIAD obtains\nsuperior performance than previous HFR approaches. Particularly, FSIAD obtains\n4.8% improvement over state of the art in terms of VR@FAR=0.01% on LAMP-HQ, the\nlargest HFR database so far.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chaoyou Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mandi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Yu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic image detection using scene and knowledge graphs. (arXiv:2206.04863v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04863","description":"<p>Sometimes the meaning conveyed by images goes beyond the list of objects they\ncontain; instead, images may express a powerful message to affect the viewers'\nminds. Inferring this message requires reasoning about the relationships\nbetween the objects, and general common-sense knowledge about the components.\nIn this paper, we use a scene graph, a graph representation of an image, to\ncapture visual components. In addition, we generate a knowledge graph using\nfacts extracted from ConceptNet to reason about objects and attributes. To\ndetect the symbols, we propose a neural network framework named SKG-Sym. The\nframework first generates the representations of the scene graph of the image\nand its knowledge graph using Graph Convolution Network. The framework then\nfuses the representations and uses an MLP to classify them. We extend the\nnetwork further to use an attention mechanism which learn the importance of the\ngraph representations. We evaluate our methods on a dataset of advertisements,\nand compare it with baseline symbolism classification methods (ResNet and VGG).\nResults show that our methods outperform ResNet in terms of F-score and the\nattention-based mechanism is competitive with VGG while it has much lower model\ncomplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalanat_N/0/1/0/all/0/1\">Nasrin Kalanat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Gender Gap in Face Recognition Accuracy Is a Hairy Problem. (arXiv:2206.04867v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04867","description":"<p>It is broadly accepted that there is a \"gender gap\" in face recognition\naccuracy, with females having higher false match and false non-match rates.\nHowever, relatively little is known about the cause(s) of this gender gap. Even\nthe recent NIST report on demographic effects lists \"analyze cause and effect\"\nunder \"what we did not do\". We first demonstrate that female and male\nhairstyles have important differences that impact face recognition accuracy. In\nparticular, compared to females, male facial hair contributes to creating a\ngreater average difference in appearance between different male faces. We then\ndemonstrate that when the data used to estimate recognition accuracy is\nbalanced across gender for how hairstyles occlude the face, the initially\nobserved gender gap in accuracy largely disappears. We show this result for two\ndifferent matchers, and analyzing images of Caucasians and of\nAfrican-Americans. These results suggest that future research on demographic\nvariation in accuracy should include a check for balanced quality of the test\ndata as part of the problem formulation. To promote reproducible research,\nmatchers, attribute classifiers, and datasets used in this research are/will be\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatta_A/0/1/0/all/0/1\">Aman Bhatta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albiero_V/0/1/0/all/0/1\">V&#xed;tor Albiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin W. Bowyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_M/0/1/0/all/0/1\">Michael C. King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 1st Data Science for Pavements Challenge. (arXiv:2206.04874v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04874","description":"<p>The Data Science for Pavement Challenge (DSPC) seeks to accelerate the\nresearch and development of automated vision systems for pavement condition\nmonitoring and evaluation by providing a platform with benchmarked datasets and\ncodes for teams to innovate and develop machine learning algorithms that are\npractice-ready for use by industry. The first edition of the competition\nattracted 22 teams from 8 countries. Participants were required to\nautomatically detect and classify different types of pavement distresses\npresent in images captured from multiple sources, and under different\nconditions. The competition was data-centric: teams were tasked to increase the\naccuracy of a predefined model architecture by utilizing various data\nmodification methods such as cleaning, labeling and augmentation. A real-time,\nonline evaluation system was developed to rank teams based on the F1 score.\nLeaderboard results showed the promise and challenges of machine for advancing\nautomation in pavement monitoring and evaluation. This paper summarizes the\nsolutions from the top 5 teams. These teams proposed innovations in the areas\nof data cleaning, annotation, augmentation, and detection parameter tuning. The\nF1 score for the top-ranked team was approximately 0.9. The paper concludes\nwith a review of different experiments that worked well for the current\nchallenge and those that did not yield any significant improvement in model\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behzadian_A/0/1/0/all/0/1\">Ashkan Behzadian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muturi_T/0/1/0/all/0/1\">Tanner Wambui Muturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hongmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_A/0/1/0/all/0/1\">Amanda Mullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owor_N/0/1/0/all/0/1\">Neema Jasika Owor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adu_Gyamfi_Y/0/1/0/all/0/1\">Yaw Adu-Gyamfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttlar_W/0/1/0/all/0/1\">William Buttlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamed_M/0/1/0/all/0/1\">Majidifard Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aboah_A/0/1/0/all/0/1\">Armstrong Aboah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensching_D/0/1/0/all/0/1\">David Mensching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robert_S/0/1/0/all/0/1\">Spragg Robert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corrigan_M/0/1/0/all/0/1\">Matthew Corrigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youtchef_J/0/1/0/all/0/1\">Jack Youtchef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshan_D/0/1/0/all/0/1\">Dave Eshan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Per-Shot Convex Hull Prediction By Recurrent Learning. (arXiv:2206.04877v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04877","description":"<p>Adaptive video streaming relies on the construction of efficient bitrate\nladders to deliver the best possible visual quality to viewers under bandwidth\nconstraints. The traditional method of content dependent bitrate ladder\nselection requires a video shot to be pre-encoded with multiple encoding\nparameters to find the optimal operating points given by the convex hull of the\nresulting rate-quality curves. However, this pre-encoding step is equivalent to\nan exhaustive search process over the space of possible encoding parameters,\nwhich causes significant overhead in terms of both computation and time\nexpenditure. To reduce this overhead, we propose a deep learning based method\nof content aware convex hull prediction. We employ a recurrent convolutional\nnetwork (RCN) to implicitly analyze the spatiotemporal complexity of video\nshots in order to predict their convex hulls. A two-step transfer learning\nscheme is adopted to train our proposed RCN-Hull model, which ensures\nsufficient content diversity to analyze scene complexity, while also making it\npossible capture the scene statistics of pristine source videos. Our\nexperimental results reveal that our proposed model yields better\napproximations of the optimal convex hulls, and offers competitive time savings\nas compared to existing approaches. On average, the pre-encoding time was\nreduced by 58.0% by our method, while the average Bjontegaard delta bitrate\n(BD-rate) of the predicted convex hulls against ground truth was 0.08%, while\nthe mean absolute deviation of the BD-rate distribution was 0.44%\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Paul_S/0/1/0/all/0/1\">Somdyuti Paul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norkin_A/0/1/0/all/0/1\">Andrey Norkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Foggy Scene Understanding via Self Spatial-Temporal Label Diffusion. (arXiv:2206.04879v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04879","description":"<p>Understanding foggy image sequence in the driving scenes is critical for\nautonomous driving, but it remains a challenging task due to the difficulty in\ncollecting and annotating real-world images of adverse weather. Recently, the\nself-training strategy has been considered a powerful solution for unsupervised\ndomain adaptation, which iteratively adapts the model from the source domain to\nthe target domain by generating target pseudo labels and re-training the model.\nHowever, the selection of confident pseudo labels inevitably suffers from the\nconflict between sparsity and accuracy, both of which will lead to suboptimal\nmodels. To tackle this problem, we exploit the characteristics of the foggy\nimage sequence of driving scenes to densify the confident pseudo labels.\nSpecifically, based on the two discoveries of local spatial similarity and\nadjacent temporal correspondence of the sequential image data, we propose a\nnovel Target-Domain driven pseudo label Diffusion (TDo-Dif) scheme. It employs\nsuperpixels and optical flows to identify the spatial similarity and temporal\ncorrespondence, respectively and then diffuses the confident but sparse pseudo\nlabels within a superpixel or a temporal corresponding pair linked by the flow.\nMoreover, to ensure the feature similarity of the diffused pixels, we introduce\nlocal spatial similarity loss and temporal contrastive loss in the model\nre-training stage. Experimental results show that our TDo-Dif scheme helps the\nadaptive model achieve 51.92% and 53.84% mean intersection-over-union (mIoU) on\ntwo publicly available natural foggy datasets (Foggy Zurich and Foggy Driving),\nwhich exceeds the state-of-the-art unsupervised domain adaptive semantic\nsegmentation methods. Models and data can be found at\nhttps://github.com/velor2012/TDo-Dif.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Liang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_S/0/1/0/all/0/1\">Shin&#x27;ichi Satoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers. (arXiv:2206.04881v1 [cs.CR])","link":"http://arxiv.org/abs/2206.04881","description":"<p>Backdoor attacks threaten Deep Neural Networks (DNNs). Towards stealthiness,\nresearchers propose clean-label backdoor attacks, which require the adversaries\nnot to alter the labels of the poisoned training datasets. Clean-label settings\nmake the attack more stealthy due to the correct image-label pairs, but some\nproblems still exist: first, traditional methods for poisoning training data\nare ineffective; second, traditional triggers are not stealthy which are still\nperceptible. To solve these problems, we propose a two-phase and image-specific\ntriggers generation method to enhance clean-label backdoor attacks. Our methods\nare (1) powerful: our triggers can both promote the two phases (i.e., the\nbackdoor implantation and activation phase) in backdoor attacks simultaneously;\n(2) stealthy: our triggers are generated from each image. They are\nimage-specific instead of fixed triggers. Extensive experiments demonstrate\nthat our approach can achieve a fantastic attack success rate~(98.98%) with low\npoisoning rate~(5%), high stealthiness under many evaluation metrics and is\nresistant to backdoor defense methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_N/0/1/0/all/0/1\">Nan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yajie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangbo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yu-an Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanxin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AntPivot: Livestream Highlight Detection via Hierarchical Attention Mechanism. (arXiv:2206.04888v1 [cs.MM])","link":"http://arxiv.org/abs/2206.04888","description":"<p>In recent days, streaming technology has greatly promoted the development in\nthe field of livestream. Due to the excessive length of livestream records,\nit's quite essential to extract highlight segments with the aim of effective\nreproduction and redistribution. Although there are lots of approaches proven\nto be effective in the highlight detection for other modals, the challenges\nexisting in livestream processing, such as the extreme durations, large topic\nshifts, much irrelevant information and so forth, heavily hamper the adaptation\nand compatibility of these methods. In this paper, we formulate a new task\nLivestream Highlight Detection, discuss and analyze the difficulties listed\nabove and propose a novel architecture AntPivot to solve this problem.\nConcretely, we first encode the original data into multiple views and model\ntheir temporal relations to capture clues in a hierarchical attention\nmechanism. Afterwards, we try to convert the detection of highlight clips into\nthe search for optimal decision sequences and use the fully integrated\nrepresentations to predict the final results in a dynamic-programming\nmechanism. Furthermore, we construct a fully-annotated dataset AntHighlight to\ninstantiate this task and evaluate the performance of our model. The extensive\nexperiments indicate the effectiveness and validity of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Maozong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors. (arXiv:2206.04901v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04901","description":"<p>Though Neural Radiance Field (NeRF) demonstrates compelling novel view\nsynthesis results, it is still unintuitive to edit a pre-trained NeRF because\nthe neural network's parameters and the scene geometry/appearance are often not\nexplicitly associated. In this paper, we introduce the first framework that\nenables users to remove unwanted objects or retouch undesired regions in a 3D\nscene represented by a pre-trained NeRF without any category-specific data and\ntraining. The user first draws a free-form mask to specify a region containing\nunwanted objects over a rendered view from the pre-trained NeRF. Our framework\nfirst transfers the user-provided mask to other rendered views and estimates\nguiding color and depth images within these transferred masked regions. Next,\nwe formulate an optimization problem that jointly inpaints the image content in\nall masked regions across multiple views by updating the NeRF model's\nparameters. We demonstrate our framework on diverse scenes and show it obtained\nvisual plausible and structurally consistent results across multiple views\nusing shorter time and less user manual efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao-Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_I/0/1/0/all/0/1\">I-Chao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bing-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out of Sight, Out of Mind: A Source-View-Wise Feature Aggregation for Multi-View Image-Based Rendering. (arXiv:2206.04906v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04906","description":"<p>To estimate the volume density and color of a 3D point in the multi-view\nimage-based rendering, a common approach is to inspect the consensus existence\namong the given source image features, which is one of the informative cues for\nthe estimation procedure. To this end, most of the previous methods utilize\nequally-weighted aggregation features. However, this could make it hard to\ncheck the consensus existence when some outliers, which frequently occur by\nocclusions, are included in the source image feature set. In this paper, we\npropose a novel source-view-wise feature aggregation method, which facilitates\nus to find out the consensus in a robust way by leveraging local structures in\nthe feature set. We first calculate the source-view-wise distance distribution\nfor each source feature for the proposed aggregation. After that, the distance\ndistribution is converted to several similarity distributions with the proposed\nlearnable similarity mapping functions. Finally, for each element in the\nfeature set, the aggregation features are extracted by calculating the weighted\nmeans and variances, where the weights are derived from the similarity\ndistributions. In experiments, we validate the proposed method on various\nbenchmark datasets, including synthetic and real image scenes. The experimental\nresults demonstrate that incorporating the proposed features improves the\nperformance by a large margin, resulting in the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_G/0/1/0/all/0/1\">Geonho Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1\">Chaehun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wee_D/0/1/0/all/0/1\">Dongyoon Wee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatchComplete: Learning Multi-Resolution Patch Priors for 3D Shape Completion on Unseen Categories. (arXiv:2206.04916v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04916","description":"<p>While 3D shape representations enable powerful reasoning in many visual and\nperception applications, learning 3D shape priors tends to be constrained to\nthe specific categories trained on, leading to an inefficient learning process,\nparticularly for general applications with unseen categories. Thus, we propose\nPatchComplete, which learns effective shape priors based on multi-resolution\nlocal patches, which are often more general than full shapes (e.g., chairs and\ntables often both share legs) and thus enable geometric reasoning about unseen\nclass categories. To learn these shared substructures, we learn\nmulti-resolution patch priors across all train categories, which are then\nassociated to input partial shape observations by attention across the patch\npriors, and finally decoded into a complete shape reconstruction. Such\npatch-based priors avoid overfitting to specific train categories and enable\nreconstruction on entirely unseen categories at test time. We demonstrate the\neffectiveness of our approach on synthetic ShapeNet data as well as challenging\nreal-scanned objects from ScanNet, which include noise and clutter, improving\nover state of the art in novel-category shape completion by 19.3% in chamfer\ndistance on ShapeNet, and 9.0% for ScanNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yuchen Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yinyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global Pose Estimation. (arXiv:2206.04927v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04927","description":"<p>Color-based two-hand 3D pose estimation in the global coordinate system is\nessential in many applications. However, there are very few datasets dedicated\nto this task and no existing dataset supports estimation in a non-laboratory\nenvironment. This is largely attributed to the sophisticated data collection\nprocess required for 3D hand pose annotations, which also leads to difficulty\nin obtaining instances with the level of visual diversity needed for estimation\nin the wild. Progressing towards this goal, a large-scale dataset Ego2Hands was\nrecently proposed to address the task of two-hand segmentation and detection in\nthe wild. The proposed composition-based data generation technique can create\ntwo-hand instances with quality, quantity and diversity that generalize well to\nunseen domains. In this work, we present Ego2HandsPose, an extension of\nEgo2Hands that contains 3D hand pose annotation and is the first dataset that\nenables color-based two-hand 3D tracking in unseen domains. To this end, we\ndevelop a set of parametric fitting algorithms to enable 1) 3D hand pose\nannotation using a single image, 2) automatic conversion from 2D to 3D hand\nposes and 3) accurate two-hand tracking with temporal consistency. We provide\nincremental quantitative analysis on the multi-stage pipeline and show that\ntraining on our dataset achieves state-of-the-art results that significantly\noutperforms other datasets for the task of egocentric two-hand global 3D pose\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fanqing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_T/0/1/0/all/0/1\">Tony Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Template: Topology-aware Reconstruction and Disentangled Generation of 3D Meshes. (arXiv:2206.04942v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04942","description":"<p>This paper introduces a novel framework called DTNet for 3D mesh\nreconstruction and generation via Disentangled Topology. Beyond previous works,\nwe learn a topology-aware neural template specific to each input then deform\nthe template to reconstruct a detailed mesh while preserving the learned\ntopology. One key insight is to decouple the complex mesh reconstruction into\ntwo sub-tasks: topology formulation and shape deformation. Thanks to the\ndecoupling, DT-Net implicitly learns a disentangled representation for the\ntopology and shape in the latent space. Hence, it can enable novel disentangled\ncontrols for supporting various shape generation applications, e.g., remix the\ntopologies of 3D objects, that are not achievable by previous reconstruction\nworks. Extensive experimental results demonstrate that our method is able to\nproduce high-quality meshes, particularly with diverse topologies, as compared\nwith the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Ka-Hei Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Multi-view Semi-supervised Clustering with Sample Pairwise Constraints. (arXiv:2206.04949v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04949","description":"<p>Multi-view clustering has attracted much attention thanks to the capacity of\nmulti-source information integration. Although numerous advanced methods have\nbeen proposed in past decades, most of them generally overlook the significance\nof weakly-supervised information and fail to preserve the feature properties of\nmultiple views, thus resulting in unsatisfactory clustering performance. To\naddress these issues, in this paper, we propose a novel Deep Multi-view\nSemi-supervised Clustering (DMSC) method, which jointly optimizes three kinds\nof losses during networks finetuning, including multi-view clustering loss,\nsemi-supervised pairwise constraint loss and multiple autoencoders\nreconstruction loss. Specifically, a KL divergence based multi-view clustering\nloss is imposed on the common representation of multi-view data to perform\nheterogeneous feature optimization, multi-view weighting and clustering\nprediction simultaneously. Then, we innovatively propose to integrate pairwise\nconstraints into the process of multi-view clustering by enforcing the learned\nmulti-view representation of must-link samples (cannot-link samples) to be\nsimilar (dissimilar), such that the formed clustering architecture can be more\ncredible. Moreover, unlike existing rivals that only preserve the encoders for\neach heterogeneous branch during networks finetuning, we further propose to\ntune the intact autoencoders frame that contains both encoders and decoders. In\nthis way, the issue of serious corruption of view-specific and view-shared\nfeature space could be alleviated, making the whole training procedure more\nstable. Through comprehensive experiments on eight popular image datasets, we\ndemonstrate that our proposed approach performs better than the\nstate-of-the-art multi-view and single-view competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yongqiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wenlong Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Deep Subspace Clustering with Entropy-norm. (arXiv:2206.04958v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04958","description":"<p>Auto-Encoder based deep subspace clustering (DSC) is widely used in computer\nvision, motion segmentation and image processing. However, it suffers from the\nfollowing three issues in the self-expressive matrix learning process: the\nfirst one is less useful information for learning self-expressive weights due\nto the simple reconstruction loss; the second one is that the construction of\nthe self-expression layer associated with the sample size requires\nhigh-computational cost; and the last one is the limited connectivity of the\nexisting regularization terms. In order to address these issues, in this paper\nwe propose a novel model named Self-Supervised deep Subspace Clustering with\nEntropy-norm (S$^{3}$CE). Specifically, S$^{3}$CE exploits a self-supervised\ncontrastive network to gain a more effetive feature vector. The local structure\nand dense connectivity of the original data benefit from the self-expressive\nlayer and additional entropy-norm constraint. Moreover, a new module with data\nenhancement is designed to help S$^{3}$CE focus on the key information of data,\nand improve the clustering performance of positive and negative instances\nthrough spectral clustering. Extensive experimental results demonstrate the\nsuperior performance of S$^{3}$CE in comparison to the state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_S/0/1/0/all/0/1\">Simin Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuesong Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression Recognition. (arXiv:2206.04975v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04975","description":"<p>Dynamic facial expression recognition (DFER) in the wild is an extremely\nchallenging task, due to a large number of noisy frames in the video sequences.\nPrevious works focus on extracting more discriminative features, but ignore\ndistinguishing the key frames from the noisy frames. To tackle this problem, we\npropose a noise-robust dynamic facial expression recognition network\n(NR-DFERNet), which can effectively reduce the interference of noisy frames on\nthe DFER task. Specifically, at the spatial stage, we devise a dynamic-static\nfusion module (DSF) that introduces dynamic features to static features for\nlearning more discriminative spatial features. To suppress the impact of target\nirrelevant frames, we introduce a novel dynamic class token (DCT) for the\ntransformer at the temporal stage. Moreover, we design a snippet-based filter\n(SF) at the decision stage to reduce the effect of too many neutral frames on\nnon-neutral sequence classification. Extensive experimental results demonstrate\nthat our NR-DFERNet outperforms the state-of-the-art methods on both the DFEW\nand AFEW benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_M/0/1/0/all/0/1\">Mingzhe Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaoqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+zhao_F/0/1/0/all/0/1\">Feng zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Layers Are Not Translation Equivariant. (arXiv:2206.04979v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04979","description":"<p>The purpose of this paper is to correct a misconception about convolutional\nneural networks (CNNs). CNNs are made up of convolutional layers which are\nshift equivariant due to weight sharing. However, contrary to popular belief,\nconvolutional layers are not translation equivariant, even when boundary\neffects are ignored and when pooling and subsampling are absent. This is\nbecause shift equivariance is a discrete symmetry while translation\nequivariance is a continuous symmetry. That discrete systems do not in general\ninherit continuous equivariances is a fundamental limitation of equivariant\ndeep learning. We discuss two implications of this fact. First, CNNs have\nachieved success in image processing despite not inheriting the translation\nequivariance of the physical systems they model. Second, using CNNs to solve\npartial differential equations (PDEs) will not result in translation\nequivariant solvers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McGreivy_N/0/1/0/all/0/1\">Nick McGreivy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakim_A/0/1/0/all/0/1\">Ammar Hakim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position Labels for Self-Supervised Vision Transformer. (arXiv:2206.04981v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04981","description":"<p>Position encoding is important for vision transformer (ViT) to capture the\nspatial structure of the input image. General efficacy has been proven in ViT.\nIn our work we propose to train ViT to recognize the 2D position encoding of\npatches of the input image, this apparently simple task actually yields a\nmeaningful self-supervisory task. Based on previous work on ViT position\nencoding, we propose two position labels dedicated to 2D images including\nabsolute position and relative position. Our position labels can be easily\nplugged into transformer, combined with the various current ViT variants. It\ncan work in two ways: 1.As an auxiliary training target for vanilla ViT (e.g.,\nViT-B and Swin-B) to improve model performance. 2. Combine the self-supervised\nViT (e.g., MAE) to provide a more powerful self-supervised signal for semantic\nfeature learning. Experiments demonstrate that solely due to the proposed\nself-supervised methods, Swin-B and ViT-B obtained improvements of 1.9% (top-1\nAcc) and 5.6% (top-1 Acc) on Mini-ImageNet, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhemin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinyi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subjective Quality Assessment for Images Generated by Computer Graphics. (arXiv:2206.05008v1 [cs.GR])","link":"http://arxiv.org/abs/2206.05008","description":"<p>With the development of rendering techniques, computer graphics generated\nimages (CGIs) have been widely used in practical application scenarios such as\narchitecture design, video games, simulators, movies, etc. Different from\nnatural scene images (NSIs), the distortions of CGIs are usually caused by poor\nrending settings and limited computation resources. What's more, some CGIs may\nalso suffer from compression distortions in transmission systems like cloud\ngaming and stream media. However, limited work has been put forward to tackle\nthe problem of computer graphics generated images' quality assessment (CG-IQA).\nTherefore, in this paper, we establish a large-scale subjective CG-IQA database\nto deal with the challenge of CG-IQA tasks. We collect 25,454 in-the-wild CGIs\nthrough previous databases and personal collection. After data cleaning, we\ncarefully select 1,200 CGIs to conduct the subjective experiment. Several\npopular no-reference image quality assessment (NR-IQA) methods are tested on\nour database. The experimental results show that the handcrafted-based methods\nachieve low correlation with subjective judgment and deep learning based\nmethods obtain relatively better performance, which demonstrates that the\ncurrent NR-IQA models are not suitable for CG-IQA tasks and more effective\nmodels are urgently needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Cross-Attention Improves Self-Supervised Visual Representation Learning. (arXiv:2206.05028v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05028","description":"<p>Unsupervised representation learning methods like SwAV are proved to be\neffective in learning visual semantics of a target dataset. The main idea\nbehind these methods is that different views of a same image represent the same\nsemantics. In this paper, we further introduce an add-on module to facilitate\nthe injection of the knowledge accounting for spatial cross correlations among\nthe samples. This in turn results in distilling intra-class information\nincluding feature level locations and cross similarities between same-class\ninstances. The proposed add-on can be added to existing methods such as the\nSwAV. We can later remove the add-on module for inference without any\nmodification of the learned weights. Through an extensive set of empirical\nevaluations, we verify that our method yields an improved performance in\ndetecting the class activation maps, top-1 classification accuracy, and\ndown-stream tasks such as object detection, with different configuration\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seyfi_M/0/1/0/all/0/1\">Mehdi Seyfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Generation with Multimodal Priors using Denoising Diffusion Probabilistic Models. (arXiv:2206.05039v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05039","description":"<p>Image synthesis under multi-modal priors is a useful and challenging task\nthat has received increasing attention in recent years. A major challenge in\nusing generative models to accomplish this task is the lack of paired data\ncontaining all modalities (i.e. priors) and corresponding outputs. In recent\nwork, a variational auto-encoder (VAE) model was trained in a weakly supervised\nmanner to address this challenge. Since the generative power of VAEs is usually\nlimited, it is difficult for this method to synthesize images belonging to\ncomplex distributions. To this end, we propose a solution based on a denoising\ndiffusion probabilistic models to synthesise images under multi-model priors.\nBased on the fact that the distribution over each time step in the diffusion\nmodel is Gaussian, in this work we show that there exists a closed-form\nexpression to the generate the image corresponds to the given modalities. The\nproposed solution does not require explicit retraining for all modalities and\ncan leverage the outputs of individual modalities to generate realistic images\naccording to different constraints. We conduct studies on two real-world\ndatasets to demonstrate the effectiveness of our approach\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_N/0/1/0/all/0/1\">Nithin Gopalakrishnan Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A GPU-Accelerated Light-field Super-resolution Framework Based on Mixed Noise Model and Weighted Regularization. (arXiv:2206.05047v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05047","description":"<p>This paper presents a GPU-accelerated computational framework for\nreconstructing high resolution (HR) LF images under a mixed Gaussian-Impulse\nnoise condition. The main focus is on developing a high-performance approach\nconsidering processing speed and reconstruction quality. From a statistical\nperspective, we derive a joint $\\ell^1$-$\\ell^2$ data fidelity term for\npenalizing the HR reconstruction error taking into account the mixed noise\nsituation. For regularization, we employ the weighted non-local total variation\napproach, which allows us to effectively realize LF image prior through a\nproper weighting scheme. We show that the alternating direction method of\nmultipliers algorithm (ADMM) can be used to simplify the computation complexity\nand results in a high-performance parallel computation on the GPU Platform. An\nextensive experiment is conducted on both synthetic 4D LF dataset and natural\nimage dataset to validate the proposed SR model's robustness and evaluate the\naccelerated optimizer's performance. The experimental results show that our\napproach achieves better reconstruction quality under severe mixed-noise\nconditions as compared to the state-of-the-art approaches. In addition, the\nproposed approach overcomes the limitation of the previous work in handling\nlarge-scale SR tasks. While fitting within a single off-the-shelf GPU, the\nproposed accelerator provides an average speedup of 2.46$\\times$ and\n1.57$\\times$ for $\\times 2$ and $\\times 3$ SR tasks, respectively. In addition,\na speedup of $77\\times$ is achieved as compared to CPU execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tran_T/0/1/0/all/0/1\">Trung-Hieu Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_K/0/1/0/all/0/1\">Kaicong Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simon_S/0/1/0/all/0/1\">Sven Simon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Generalized Expectation-Consistent Approximation for MRI Image Recovery. (arXiv:2206.05049v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05049","description":"<p>To solve inverse problems, plug-and-play (PnP) methods have been developed\nthat replace the proximal step in a convex optimization algorithm with a call\nto an application-specific denoiser, often implemented using a deep neural\nnetwork (DNN). Although such methods have been successful, they can be\nimproved. For example, denoisers are usually designed/trained to remove white\nGaussian noise, but the denoiser input error in PnP algorithms is usually far\nfrom white or Gaussian. Approximate message passing (AMP) methods provide white\nand Gaussian denoiser input error, but only when the forward operator is a\nlarge random matrix. In this work, for Fourier-based forward operators, we\npropose a PnP algorithm based on generalized expectation-consistent (GEC)\napproximation -- a close cousin of AMP -- that offers predictable error\nstatistics at each iteration, as well as a new DNN denoiser that leverages\nthose statistics. We apply our approach to magnetic resonance imaging (MRI)\nimage recovery and demonstrate its advantages over existing PnP and AMP\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shastri_S/0/1/0/all/0/1\">Saurav K. Shastri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmad_R/0/1/0/all/0/1\">Rizwan Ahmad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metzler_C/0/1/0/all/0/1\">Christopher A. Metzler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schniter_P/0/1/0/all/0/1\">Philip Schniter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A No-reference Quality Assessment Metric for Point Cloud Based on Captured Video Sequences. (arXiv:2206.05054v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05054","description":"<p>Point cloud is one of the most widely used digital formats of 3D models, the\nvisual quality of which is quite sensitive to distortions such as downsampling,\nnoise, and compression. To tackle the challenge of point cloud quality\nassessment (PCQA) in scenarios where reference is not available, we propose a\nno-reference quality assessment metric for colored point cloud based on\ncaptured video sequences. Specifically, three video sequences are obtained by\nrotating the camera around the point cloud through three specific orbits. The\nvideo sequences not only contain the static views but also include the\nmulti-frame temporal information, which greatly helps understand the human\nperception of the point clouds. Then we modify the ResNet3D as the feature\nextraction model to learn the correlation between the capture videos and\ncorresponding subjective quality scores. The experimental results show that our\nmethod outperforms most of the state-of-the-art full-reference and no-reference\nPCQA metrics, which validates the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yu Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning self-calibrated optic disc and cup segmentation from multi-rater annotations. (arXiv:2206.05092v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05092","description":"<p>The segmentation of optic disc(OD) and optic cup(OC) from fundus images is an\nimportant fundamental task for glaucoma diagnosis. In the clinical practice, it\nis often necessary to collect opinions from multiple experts to obtain the\nfinal OD/OC annotation. This clinical routine helps to mitigate the individual\nbias. But when data is multiply annotated, standard deep learning models will\nbe inapplicable. In this paper, we propose a novel neural network framework to\nlearn OD/OC segmentation from multi-rater annotations. The segmentation results\nare self-calibrated through the iterative optimization of multi-rater\nexpertness estimation and calibrated OD/OC segmentation. In this way, the\nproposed method can realize a mutual improvement of both tasks and finally\nobtain a refined segmentation result. Specifically, we propose Diverging\nModel(DivM) and Converging Model(ConM) to process the two tasks respectively.\nConM segments the raw image based on the multi-rater expertness map provided by\nDivM. DivM generates multi-rater expertness map from the segmentation mask\nprovided by ConM. The experiment results show that by recurrently running ConM\nand DivM, the results can be self-calibrated so as to outperform a range of\nstate-of-the-art(SOTA) multi-rater segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shang_F/0/1/0/all/0/1\">Fangxin Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dalu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wenshuo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Momentum Contrastive Clustering. (arXiv:2206.05093v1 [cs.LG])","link":"http://arxiv.org/abs/2206.05093","description":"<p>We present federated momentum contrastive clustering (FedMCC), a learning\nframework that can not only extract discriminative representations over\ndistributed local data but also perform data clustering. In FedMCC, a\ntransformed data pair passes through both the online and target networks,\nresulting in four representations over which the losses are determined. The\nresulting high-quality representations generated by FedMCC can outperform\nseveral existing self-supervised learning methods for linear evaluation and\nsemi-supervised learning tasks. FedMCC can easily be adapted to ordinary\ncentralized clustering through what we call momentum contrastive clustering\n(MCC). We show that MCC achieves state-of-the-art clustering accuracy results\nin certain datasets such as STL-10 and ImageNet-10. We also present a method to\nreduce the memory footprint of our clustering schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_R/0/1/0/all/0/1\">Runxuan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyuncu_E/0/1/0/all/0/1\">Erdem Koyuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimVP: Simpler yet Better Video Prediction. (arXiv:2206.05099v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05099","description":"<p>From CNN, RNN, to ViT, we have witnessed remarkable advancements in video\nprediction, incorporating auxiliary inputs, elaborate neural architectures, and\nsophisticated training strategies. We admire these progresses but are confused\nabout the necessity: is there a simple method that can perform comparably well?\nThis paper proposes SimVP, a simple video prediction model that is completely\nbuilt upon CNN and trained by MSE loss in an end-to-end fashion. Without\nintroducing any additional tricks and complicated strategies, we can achieve\nstate-of-the-art performance on five benchmark datasets. Through extended\nexperiments, we demonstrate that SimVP has strong generalization and\nextensibility on real-world datasets. The significant reduction of training\ncost makes it easier to scale to complex scenarios. We believe SimVP can serve\nas a solid baseline to stimulate the further development of video prediction.\nThe code is available at\n\\href{https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction}{Github}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhangyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saccade Mechanisms for Image Classification, Object Detection and Tracking. (arXiv:2206.05102v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05102","description":"<p>We examine how the saccade mechanism from biological vision can be used to\nmake deep neural networks more efficient for classification and object\ndetection problems. Our proposed approach is based on the ideas of\nattention-driven visual processing and saccades, miniature eye movements\ninfluenced by attention. We conduct experiments by analyzing: i) the robustness\nof different deep neural network (DNN) feature extractors to partially-sensed\nimages for image classification and object detection, and ii) the utility of\nsaccades in masking image patches for image classification and object tracking.\nExperiments with convolutional nets (ResNet-18) and transformer-based models\n(ViT, DETR, TransTrack) are conducted on several datasets (CIFAR-10, DAVSOD,\nMSCOCO, and MOT17). Our experiments show intelligent data reduction via\nlearning to mimic human saccades when used in conjunction with state-of-the-art\nDNNs for classification, detection, and tracking tasks. We observed minimal\ndrop in performance for the classification and detection tasks while only using\nabout 30\\% of the original sensor data. We discuss how the saccade mechanism\ncan inform hardware design via ``in-pixel'' processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farkya_S/0/1/0/all/0/1\">Saurabh Farkya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniels_Z/0/1/0/all/0/1\">Zachary Daniels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_A/0/1/0/all/0/1\">Aswin Nadamuni Raghavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piacentino_M/0/1/0/all/0/1\">Michael Piacentino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globally-Optimal Contrast Maximisation for Event Cameras. (arXiv:2206.05127v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05127","description":"<p>Event cameras are bio-inspired sensors that perform well in challenging\nillumination conditions and have high temporal resolution. However, their\nconcept is fundamentally different from traditional frame-based cameras. The\npixels of an event camera operate independently and asynchronously. They\nmeasure changes of the logarithmic brightness and return them in the highly\ndiscretised form of time-stamped events indicating a relative change of a\ncertain quantity since the last event. New models and algorithms are needed to\nprocess this kind of measurements. The present work looks at several motion\nestimation problems with event cameras. The flow of the events is modelled by a\ngeneral homographic warping in a space-time volume, and the objective is\nformulated as a maximisation of contrast within the image of warped events. Our\ncore contribution consists of deriving globally optimal solutions to these\ngenerally non-convex problems, which removes the dependency on a good initial\nguess plaguing existing methods. Our methods rely on branch-and-bound\noptimisation and employ novel and efficient, recursive upper and lower bounds\nderived for six different contrast estimation functions. The practical validity\nof our approach is demonstrated by a successful application to three different\nevent camera motion estimation problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Hyper-Dimensional Reconfiguration at the Edge using Hardware Accelerators. (arXiv:2206.05128v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05128","description":"<p>In this paper we present Hyper-Dimensional Reconfigurable Analytics at the\nTactical Edge (HyDRATE) using low-SWaP embedded hardware that can perform\nreal-time reconfiguration at the edge leveraging non-MAC (free of\nfloating-point MultiplyACcumulate operations) deep neural nets (DNN) combined\nwith hyperdimensional (HD) computing accelerators. We describe the algorithm,\ntrained quantized model generation, and simulated performance of a feature\nextractor free of multiply-accumulates feeding a hyperdimensional logic-based\nclassifier. Then we show how performance increases with the number of\nhyperdimensions. We describe the realized low-SWaP FPGA hardware and embedded\nsoftware system compared to traditional DNNs and detail the implemented\nhardware accelerators. We discuss the measured system latency and power, noise\nrobustness due to use of learnable quantization and HD computing, actual versus\nsimulated system performance for a video activity classification task and\ndemonstration of reconfiguration on this same dataset. We show that\nreconfigurability in the field is achieved by retraining only the feed-forward\nHD classifier without gradient descent backpropagation (gradient-free), using\nfew-shot learning of new classes at the edge. Initial work performed used LRCN\nDNN and is currently extended to use Two-stream DNN with improved performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kandaswamy_I/0/1/0/all/0/1\">Indhumathi Kandaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farkya_S/0/1/0/all/0/1\">Saurabh Farkya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniels_Z/0/1/0/all/0/1\">Zachary Daniels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wal_G/0/1/0/all/0/1\">Gooitzen van der Wal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_A/0/1/0/all/0/1\">Aswin Raghavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomnitz_M/0/1/0/all/0/1\">Michael Lomnitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isnardi_M/0/1/0/all/0/1\">Michael Isnardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piacentino_M/0/1/0/all/0/1\">Michael Piacentino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification. (arXiv:2206.05148v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05148","description":"<p>Deep learning models have shown their potential for several applications.\nHowever, most of the models are opaque and difficult to trust due to their\ncomplex reasoning - commonly known as the black-box problem. Some fields, such\nas medicine, require a high degree of transparency to accept and adopt such\ntechnologies. Consequently, creating explainable/interpretable models or\napplying post-hoc methods on classifiers to build trust in deep learning models\nare required. Moreover, deep learning methods can be used for segmentation\ntasks, which typically require hard-to-obtain, time-consuming\nmanually-annotated segmentation labels for training. This paper introduces\nthree inherently-explainable classifiers to tackle both of these problems as\none. The localisation heatmaps provided by the networks -- representing the\nmodels' focus areas and being used in classification decision-making -- can be\ndirectly interpreted, without requiring any post-hoc methods to derive\ninformation for model explanation. The models are trained by using the input\nimage and only the classification labels as ground-truth in a supervised\nfashion - without using any information about the location of the region of\ninterest (i.e. the segmentation labels), making the segmentation training of\nthe models weakly-supervised through classification labels. The final\nsegmentation is obtained by thresholding these heatmaps. The models were\nemployed for the task of multi-class brain tumour classification using two\ndifferent datasets, resulting in the best F1-score of 0.93 for the supervised\nclassification task while securing a median Dice score of 0.67$\\pm$0.08 for the\nweakly-supervised segmentation task. Furthermore, the obtained accuracy on a\nsubset of tumour-only images outperformed the state-of-the-art glioma tumour\ngrading binary classifiers with the best model achieving 98.7\\% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yassin_H/0/1/0/all/0/1\">Hadya Yassin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubost_F/0/1/0/all/0/1\">Florian Dubost</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Referring Image Matting. (arXiv:2206.05149v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05149","description":"<p>Image matting refers to extracting the accurate foregrounds in the image.\nCurrent automatic methods tend to extract all the salient objects in the image\nindiscriminately. In this paper, we propose a new task named Referring Image\nMatting (RIM), referring to extracting the meticulous alpha matte of the\nspecific object that can best match the given natural language description.\nHowever, prevalent visual grounding methods are all limited to the segmentation\nlevel, probably due to the lack of high-quality datasets for RIM. To fill the\ngap, we establish the first large-scale challenging dataset RefMatte by\ndesigning a comprehensive image composition and expression generation engine to\nproduce synthetic images on top of current public high-quality matting\nforegrounds with flexible logics and re-labelled diverse attributes. RefMatte\nconsists of 230 object categories, 47,500 images, 118,749 expression-region\nentities, and 474,996 expressions, which can be further extended easily in the\nfuture. Besides this, we also construct a real-world test set with manually\ngenerated phrase annotations consisting of 100 natural images to further\nevaluate the generalization of RIM models. We first define the task of RIM in\ntwo settings, i.e., prompt-based and expression-based, and then benchmark\nseveral representative methods together with specific model designs for image\nmatting. The results provide empirical insights into the limitations of\nexisting methods as well as possible solutions. We believe the new task RIM\nalong with the RefMatte dataset will open new research directions in this area\nand facilitate future studies. The dataset and code will be made publicly\navailable at https://github.com/JizhiziLi/RIM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEAT: Maneuver Extraction from Agent Trajectories. (arXiv:2206.05158v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05158","description":"<p>Advances in learning-based trajectory prediction are enabled by large-scale\ndatasets. However, in-depth analysis of such datasets is limited. Moreover, the\nevaluation of prediction models is limited to metrics averaged over all samples\nin the dataset. We propose an automated methodology that allows to extract\nmaneuvers (e.g., left turn, lane change) from agent trajectories in such\ndatasets. The methodology considers information about the agent dynamics and\ninformation about the lane segments the agent traveled along. Although it is\npossible to use the resulting maneuvers for training classification networks,\nwe exemplary use them for extensive trajectory dataset analysis and\nmaneuver-specific evaluation of multiple state-of-the-art trajectory prediction\nmodels. Additionally, an analysis of the datasets and an evaluation of the\nprediction models based on the agent dynamics is provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_J/0/1/0/all/0/1\">Julian Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_J/0/1/0/all/0/1\">Julian Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raba_D/0/1/0/all/0/1\">David Raba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welz_T/0/1/0/all/0/1\">Tobias Welz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Image Processing Pipeline for Camera Trap Time-Lapse Recordings. (arXiv:2206.05159v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05159","description":"<p>A new open-source image processing pipeline for analyzing camera trap\ntime-lapse recordings is described. This pipeline includes machine learning\nmodels to assist human-in-the-loop video segmentation and animal\nre-identification. We present some performance results and observations on the\nutility of this pipeline after using it in a year-long project studying the\nspatial ecology and social behavior of the gopher tortoise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hilton_M/0/1/0/all/0/1\">Michael L. Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamane_M/0/1/0/all/0/1\">Mark T. Yamane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knezevich_L/0/1/0/all/0/1\">Leah M. Knezevich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Feature Self-relation for Self-supervised Transformer. (arXiv:2206.05184v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05184","description":"<p>Learning representations with self-supervision for convolutional networks\n(CNN) has proven effective for vision tasks. As an alternative for CNN, vision\ntransformers (ViTs) emerge strong representation ability with the pixel-level\nself-attention and channel-level feed-forward networks. Recent works reveal\nthat self-supervised learning helps unleash the great potential of ViTs. Still,\nmost works follow self-supervised strategy designed for CNNs, e.g.,\ninstance-level discrimination of samples, but they ignore the unique properties\nof ViTs. We observe that modeling relations among pixels and channels\ndistinguishes ViTs from other networks. To enforce this property, we explore\nthe feature self-relations for training self-supervised ViTs. Specifically,\ninstead of conducting self-supervised learning solely on feature embeddings\nfrom multiple views, we utilize the feature self-relations, i.e.,\npixel/channel-level self-relations, for self-supervised learning. Self-relation\nbased learning further enhance the relation modeling ability of ViTs, resulting\nin strong representations that stably improve performance on multiple\ndownstream tasks. Our source code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong-Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shanghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Space of Deep Models. (arXiv:2206.05194v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05194","description":"<p>Embedding of large but redundant data, such as images or text, in a hierarchy\nof lower-dimensional spaces is one of the key features of representation\nlearning approaches, which nowadays provide state-of-the-art solutions to\nproblems once believed hard or impossible to solve. In this work, in a plot\ntwist with a strong meta aftertaste, we show how trained deep models are as\nredundant as the data they are optimized to process, and how it is therefore\npossible to use deep learning models to embed deep learning models. In\nparticular, we show that it is possible to use representation learning to learn\na fixed-size, low-dimensional embedding space of trained deep models and that\nsuch space can be explored by interpolation or optimization to attain\nready-to-use models. We find that it is possible to learn an embedding space of\nmultiple instances of the same architecture and of multiple architectures. We\naddress image classification and neural representation of signals, showing how\nour embedding space can be learnt so as to capture the notions of performance\nand 3D shape, respectively. In the Multi-Architecture setting we also show how\nan embedding trained only on a subset of architectures can learn to generate\nalready-trained instances of architectures it never sees instantiated at\ntraining time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berardi_G/0/1/0/all/0/1\">Gianluca Berardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luigi_L/0/1/0/all/0/1\">Luca De Luigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salti_S/0/1/0/all/0/1\">Samuele Salti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefano_L/0/1/0/all/0/1\">Luigi Di Stefano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClamNet: Using contrastive learning with variable depth Unets for medical image segmentation. (arXiv:2206.05225v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05225","description":"<p>Unets have become the standard method for semantic segmentation of medical\nimages, along with fully convolutional networks (FCN). Unet++ was introduced as\na variant of Unet, in order to solve some of the problems facing Unet and FCNs.\nUnet++ provided networks with an ensemble of variable depth Unets, hence\neliminating the need for professionals estimating the best suitable depth for a\ntask. While Unet and all its variants, including Unet++ aimed at providing\nnetworks that were able to train well without requiring large quantities of\nannotated data, none of them attempted to eliminate the need for pixel-wise\nannotated data altogether. Obtaining such data for each disease to be diagnosed\ncomes at a high cost. Hence such data is scarce. In this paper we use\ncontrastive learning to train Unet++ for semantic segmentation of medical\nimages using medical images from various sources including magnetic resonance\nimaging (MRI) and computed tomography (CT), without the need for pixel-wise\nannotations. Here we describe the architecture of the proposed model and the\ntraining method used. This is still a work in progress and so we abstain from\nincluding results in this paper. The results and the trained model would be\nmade available upon publication or in subsequent versions of this paper on\narxiv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Samayan Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahnawaz_S/0/1/0/all/0/1\">Sk Shahnawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Avigyan Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Diffraction Tomography based on 3D Physics-Inspired Neural Network (PINN). (arXiv:2206.05236v1 [physics.optics])","link":"http://arxiv.org/abs/2206.05236","description":"<p>Optical diffraction tomography (ODT) is an emerging 3D imaging technique that\nis used for the 3D reconstruction of the refractive index (RI) for\nsemi-transparent samples. Various inverse models have been proposed to\nreconstruct the 3D RI based on the holographic detection of different samples\nsuch as the Born and the Rytov approximations. However, such approximations\nusually suffer from the so-called missing-cone problem that results in an\nelongation of the final reconstruction along the optical axis. Different\niterative schemes have been proposed to solve the missing cone problem relying\non physical forward models and an error function that aims at filling in the\nk-space and thus eliminating the missing-cone problem and reaching better\nreconstruction accuracy. In this paper, we propose a different approach where a\n3D neural network (NN) is employed. The NN is trained with a cost function\nderived from a physical model based on the physics of optical wave propagation.\nThe 3D NN starts with an initial guess for the 3D RI reconstruction (i.e. Born,\nor Rytov) and aims at reconstructing better 3D reconstruction based on an error\nfunction. With this technique, the NN can be trained without any examples of\nthe relation between the ill-posed reconstruction (Born or Rytov) and the\nground truth (true shape).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Ayoub_A/0/1/0/all/0/1\">Ahmed B. Ayoub</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saba_A/0/1/0/all/0/1\">Amirhossein Saba</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gigli_C/0/1/0/all/0/1\">Carlo Gigli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Psaltis_D/0/1/0/all/0/1\">Demetri Psaltis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lost in Transmission: On the Impact of Networking Corruptions on Video Machine Learning Models. (arXiv:2206.05252v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05252","description":"<p>We study how networking corruptions--data corruptions caused by networking\nerrors--affect video machine learning (ML) models. We discover apparent\nnetworking corruptions in Kinetics-400, a benchmark video ML dataset. In a\nsimulation study, we investigate (1) what artifacts networking corruptions\ncause, (2) how such artifacts affect ML models, and (3) whether standard\nrobustness methods can mitigate their negative effects. We find that networking\ncorruptions cause visual and temporal artifacts (i.e., smeared colors or frame\ndrops). These networking corruptions degrade performance on a variety of video\nML tasks, but effects vary by task and dataset, depending on how much temporal\ncontext the tasks require. Lastly, we evaluate data augmentation--a standard\ndefense for data corruptions--but find that it does not recover performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Trenton Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Daniel Y. Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Spatial Invariance of Convolutional Networks for Object Counting. (arXiv:2206.05253v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05253","description":"<p>Previous work generally believes that improving the spatial invariance of\nconvolutional networks is the key to object counting. However, after verifying\nseveral mainstream counting networks, we surprisingly found too strict\npixel-level spatial invariance would cause overfit noise in the density map\ngeneration. In this paper, we try to use locally connected Gaussian kernels to\nreplace the original convolution filter to estimate the spatial position in the\ndensity map. The purpose of this is to allow the feature extraction process to\npotentially stimulate the density map generation process to overcome the\nannotation noise. Inspired by previous work, we propose a low-rank\napproximation accompanied with translation invariance to favorably implement\nthe approximation of massive Gaussian convolution. Our work points a new\ndirection for follow-up research, which should investigate how to properly\nrelax the overly strict pixel-level spatial invariance for object counting. We\nevaluate our methods on 4 mainstream object counting networks (i.e., MCNN,\nCSRNet, SANet, and ResNet-50). Extensive experiments were conducted on 7\npopular benchmarks for 3 applications (i.e., crowd, vehicle, and plant\ncounting). Experimental results show that our methods significantly outperform\nother state-of-the-art methods and achieve promising learning of the spatial\nposition of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhi-Qi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">JingKuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alexander G. Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces. (arXiv:2206.05257v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05257","description":"<p>Despite their high accuracies, modern complex image classifiers cannot be\ntrusted for sensitive tasks due to their unknown decision-making process and\npotential biases. Counterfactual explanations are very effective in providing\ntransparency for these black-box algorithms. Nevertheless, generating\ncounterfactuals that can have a consistent impact on classifier outputs and yet\nexpose interpretable feature changes is a very challenging task. We introduce a\nnovel method to generate causal and yet interpretable counterfactual\nexplanations for image classifiers using pretrained generative models without\nany re-training or conditioning. The generative models in this technique are\nnot bound to be trained on the same data as the target classifier. We use this\nframework to obtain contrastive and causal sufficiency and necessity scores as\nglobal explanations for black-box classifiers. On the task of face attribute\nclassification, we show how different attributes influence the classifier\noutput by providing both causal and contrastive feature attributions, and the\ncorresponding counterfactual images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alipour_K/0/1/0/all/0/1\">Kamran Alipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1\">Aditya Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_B/0/1/0/all/0/1\">Babak Salimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pazzani_M/0/1/0/all/0/1\">Michael Pazzani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Self-Supervised Learning More Robust Than Supervised Learning?. (arXiv:2206.05259v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05259","description":"<p>Self-supervised contrastive learning is a powerful tool to learn visual\nrepresentation without labels. Prior work has primarily focused on evaluating\nthe recognition accuracy of various pre-training algorithms, but has overlooked\nother behavioral aspects. In addition to accuracy, distributional robustness\nplays a critical role in the reliability of machine learning models. We design\nand conduct a series of robustness tests to quantify the behavioral differences\nbetween contrastive learning and supervised learning to downstream or\npre-training data distribution changes. These tests leverage data corruptions\nat multiple levels, ranging from pixel-level gamma distortion to patch-level\nshuffling and to dataset-level distribution shift. Our tests unveil intriguing\nrobustness behaviors of contrastive and supervised learning. On the one hand,\nunder downstream corruptions, we generally observe that contrastive learning is\nsurprisingly more robust than supervised learning. On the other hand, under\npre-training corruptions, we find contrastive learning vulnerable to patch\nshuffling and pixel intensity change, yet less sensitive to dataset-level\ndistribution change. We attempt to explain these results through the role of\ndata augmentation and feature space properties. Our insight has implications in\nimproving the downstream robustness of supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yuanyi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoran Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced Product of Experts for Long-Tailed Recognition. (arXiv:2206.05260v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05260","description":"<p>Many real-world recognition problems suffer from an imbalanced or long-tailed\nlabel distribution. Those distributions make representation learning more\nchallenging due to limited generalization over the tail classes. If the test\ndistribution differs from the training distribution, e.g. uniform versus\nlong-tailed, the problem of the distribution shift needs to be addressed. To\nthis aim, recent works have extended softmax cross-entropy using margin\nmodifications, inspired by Bayes' theorem. In this paper, we generalize several\napproaches with a Balanced Product of Experts (BalPoE), which combines a family\nof models with different test-time target distributions to tackle the imbalance\nin the data. The proposed experts are trained in a single stage, either jointly\nor independently, and fused seamlessly into a BalPoE. We show that BalPoE is\nFisher consistent for minimizing the balanced error and perform extensive\nexperiments to validate the effectiveness of our approach. Finally, we\ninvestigate the effect of Mixup in this setting, discovering that\nregularization is a key ingredient for learning calibrated experts. Our\nexperiments show that a regularized BalPoE can perform remarkably well in test\naccuracy and calibration metrics, leading to state-of-the-art results on\nCIFAR-100-LT, ImageNet-LT, and iNaturalist-2018 datasets. The code will be made\npublicly available upon paper acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aimar_E/0/1/0/all/0/1\">Emanuel Sanchez Aimar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonnarth_A/0/1/0/all/0/1\">Arvi Jonnarth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhlmann_M/0/1/0/all/0/1\">Marco Kuhlmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Balancing for Domain Generalization. (arXiv:2206.05263v1 [cs.LG])","link":"http://arxiv.org/abs/2206.05263","description":"<p>While machine learning models rapidly advance the state-of-the-art on various\nreal-world tasks, out-of-domain (OOD) generalization remains a challenging\nproblem given the vulnerability of these models to spurious correlations. While\ncurrent domain generalization methods usually focus on enforcing certain\ninvariance properties across different domains by new loss function designs, we\npropose a balanced mini-batch sampling strategy to reduce the domain-specific\nspurious correlations in the observed training distributions. More\nspecifically, we propose a two-phased method that 1) identifies the source of\nspurious correlations, and 2) builds balanced mini-batches free from spurious\ncorrelations by matching on the identified source. We provide an\nidentifiability guarantee of the source of spuriousness and show that our\nproposed approach provably samples from a balanced, spurious-free distribution\nover all training environments. Experiments are conducted on three computer\nvision datasets with documented spurious correlations, demonstrating\nempirically that our balanced mini-batch sampling strategy improves the\nperformance of four different established domain generalization model baselines\ncompared to the random mini-batch sampling strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?. (arXiv:2206.05266v1 [cs.LG])","link":"http://arxiv.org/abs/2206.05266","description":"<p>We investigate whether self-supervised learning (SSL) can improve online\nreinforcement learning (RL) from pixels. We extend the contrastive\nreinforcement learning framework (e.g., CURL) that jointly optimizes SSL and RL\nlosses and conduct an extensive amount of experiments with various\nself-supervised losses. Our observations suggest that the existing SSL\nframework for RL fails to bring meaningful improvement over the baselines only\ntaking advantage of image augmentation when the same amount of data and\naugmentation is used. We further perform an evolutionary search to find the\noptimal combination of multiple self-supervised losses for RL, but find that\neven such a loss combination fails to meaningfully outperform the methods that\nonly utilize carefully designed image augmentations. Often, the use of\nself-supervised losses under the existing framework lowered RL performances. We\nevaluate the approach in multiple different environments including a real-world\nrobot environment and confirm that no single self-supervised loss or image\naugmentation method can dominate all environments and that the current\nframework for joint optimization of SSL and RL is limited. Finally, we\nempirically investigate the pretraining framework for SSL + RL and the\nproperties of representations learned with different approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jinghuan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Street Crossing Aid Using Light-weight CNNs for the Visually Impaired. (arXiv:1909.09598v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.09598","description":"<p>In this paper, we address an issue that the visually impaired commonly face\nwhile crossing intersections and propose a solution that takes form as a mobile\napplication. The application utilizes a deep learning convolutional neural\nnetwork model, LytNetV2, to output necessary information that the visually\nimpaired may lack when without human companions or guide-dogs. A prototype of\nthe application runs on iOS devices of versions 11 or above. It is designed for\ncomprehensiveness, concision, accuracy, and computational efficiency through\ndelivering the two most important pieces of information, pedestrian traffic\nlight color and direction, required to cross the road in real-time.\nFurthermore, it is specifically aimed to support those facing financial burden\nas the solution takes the form of a free mobile application. Through the\nmodification and utilization of key principles in MobileNetV3 such as depthwise\nseperable convolutions and squeeze-excite layers, the deep neural network model\nachieves a classification accuracy of 96% and average angle error of 6.15\ndegrees, while running at a frame rate of 16.34 frames per second.\nAdditionally, the model is trained as an image classifier, allowing for a\nfaster and more accurate model. The network is able to outperform other methods\nsuch as object detection and non-deep learning algorithms in both accuracy and\nthoroughness. The information is delivered through both auditory signals and\nvibrations, and it has been tested on seven visually impaired and has received\nabove satisfactory responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samuel Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jung Hoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection. (arXiv:2010.14982v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.14982","description":"<p>Designing activity detection systems that can be successfully deployed in\ndaily-living environments requires datasets that pose the challenges typical of\nreal-world scenarios. In this paper, we introduce a new untrimmed daily-living\ndataset that features several real-world challenges: Toyota Smarthome Untrimmed\n(TSU). TSU contains a wide variety of activities performed in a spontaneous\nmanner. The dataset contains dense annotations including elementary, composite\nactivities and activities involving interactions with objects. We provide an\nanalysis of the real-world challenges featured by our dataset, highlighting the\nopen issues for detection algorithms. We show that current state-of-the-art\nmethods fail to achieve satisfactory performance on the TSU dataset. Therefore,\nwe propose a new baseline method for activity detection to tackle the novel\nchallenges provided by our dataset. This method leverages one modality (i.e.\noptic flow) to generate the attention weights to guide another modality (i.e\nRGB) to better detect the activity boundaries. This is particularly beneficial\nto detect activities characterized by high temporal variance. We show that the\nmethod we propose outperforms state-of-the-art methods on TSU and on another\npopular challenging dataset, Charades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Saurav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minciullo_L/0/1/0/all/0/1\">Luca Minciullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garattoni_L/0/1/0/all/0/1\">Lorenzo Garattoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1\">Gianpiero Francesca</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Region Video Transformers. (arXiv:2110.06915v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06915","description":"<p>Recently, video transformers have shown great success in video understanding,\nexceeding CNN performance; yet existing video transformer models do not\nexplicitly model objects, although objects can be essential for recognizing\nactions. In this work, we present Object-Region Video Transformers (ORViT), an\n\\emph{object-centric} approach that extends video transformer layers with a\nblock that directly incorporates object representations. The key idea is to\nfuse object-centric representations starting from early layers and propagate\nthem into the transformer-layers, thus affecting the spatio-temporal\nrepresentations throughout the network. Our ORViT block consists of two\nobject-level streams: appearance and dynamics. In the appearance stream, an\n\"Object-Region Attention\" module applies self-attention over the patches and\n\\emph{object regions}. In this way, visual object regions interact with uniform\npatch tokens and enrich them with contextualized object information. We further\nmodel object dynamics via a separate \"Object-Dynamics Module\", which captures\ntrajectory interactions, and show how to integrate the two streams. We evaluate\nour model on four tasks and five datasets: compositional and few-shot action\nrecognition on SomethingElse, spatio-temporal action detection on AVA, and\nstandard action recognition on Something-Something V2, Diving48 and\nEpic-Kitchen100. We show strong performance improvement across all tasks and\ndatasets considered, demonstrating the value of a model that incorporates\nobject representations into a transformer architecture. For code and pretrained\nmodels, visit the project page at \\url{https://roeiherz.github.io/ORViT/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1\">Roei Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Avraham_E/0/1/0/all/0/1\">Elad Ben-Avraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1\">Amir Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grafting Transformer on Automatically Designed Convolutional Neural Network for Hyperspectral Image Classification. (arXiv:2110.11084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11084","description":"<p>Hyperspectral image (HSI) classification has been a hot topic for decides, as\nhyperspectral images have rich spatial and spectral information and provide\nstrong basis for distinguishing different land-cover objects. Benefiting from\nthe development of deep learning technologies, deep learning based HSI\nclassification methods have achieved promising performance. Recently, several\nneural architecture search (NAS) algorithms have been proposed for HSI\nclassification, which further improve the accuracy of HSI classification to a\nnew level. In this paper, NAS and Transformer are combined for handling HSI\nclassification task for the first time. Compared with previous work, the\nproposed method has two main differences. First, we revisit the search spaces\ndesigned in previous HSI classification NAS methods and propose a novel hybrid\nsearch space, consisting of the space dominated cell and the spectrum dominated\ncell. Compared with search spaces proposed in previous works, the proposed\nhybrid search space is more aligned with the characteristic of HSI data, that\nis, HSIs have a relatively low spatial resolution and an extremely high\nspectral resolution. Second, to further improve the classification accuracy, we\nattempt to graft the emerging transformer module on the automatically designed\nconvolutional neural network (CNN) to add global information to local region\nfocused features learned by CNN. Experimental results on three public HSI\ndatasets show that the proposed method achieves much better performance than\ncomparison approaches, including manually designed network and NAS based HSI\nclassification methods. Especially on the most recently captured dataset\nHouston University, overall accuracy is improved by nearly 6 percentage points.\nCode is available at: https://github.com/Cecilia-xue/HyT-NAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xizhe Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zongwen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Ying Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.11191","description":"<p>The paper represents a method of a Convolution Neural Networks (CNN) model\nfor image classification with image preprocessing and hyperparameters tuning,\naiming at increasing the predictive performance for COVID-19 diagnosis while\navoiding deeper and thus more complex alternatives. Firstly, the CNN model\nincludes four similar convolutional layers followed by a flattening and two\ndense layers. This work proposes a less complex solution based on simply\nclassifying 2D slices of CT scans using a CNN model. Despite the simplicity in\narchitecture, the proposed CNN model showed improved quantitative results\nexceeding state-of-the-arts on the dataset of images, in terms of the macro F1\nscore. The results were achieved on the original CT slices of the dataset.\nSecondly, the original dataset was processed via anatomy-relevant masking of\nslices, removing non-representative slices from the CT volume, and\nhyperparameters tuning. For slice processing, a fixed-sized rectangular area\nwas used for cropping an anatomy-relevant region of interest in the images, and\na threshold based on the number of white pixels in binarized slices was\nemployed to remove non-representative slices from the 3D-CT scans. The CNN\nmodel with a learning rate schedule with exponential decay and slice flipping\ntechniques was deployed on the processed slices. The proposed method was used\nto make predictions on the 2D slices. For final diagnosis at a patient level,\nmajority voting was applied on the slices of each CT scan to make the\ndiagnosis. The macro F1 score of the proposed method well exceeded the baseline\napproach and other alternatives' scores on the validation set as well as on a\ntest partition of the previously unseen images from the COV19-CT-DB dataset\npartition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1\">Kenan Morani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unay_D/0/1/0/all/0/1\">Devrim Unay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection. (arXiv:2111.13336v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13336","description":"<p>In object detection, the detection backbone consumes more than half of the\noverall inference cost. Recent researches attempt to reduce this cost by\noptimizing the backbone architecture with the help of Neural Architecture\nSearch (NAS). However, existing NAS methods for object detection require\nhundreds to thousands of GPU hours of searching, making them impractical in\nfast-paced research and development. In this work, we propose a novel zero-shot\nNAS method to address this issue. The proposed method, named MAE-DET,\nautomatically designs efficient detection backbones via the Maximum Entropy\nPrinciple without training network parameters, reducing the architecture design\ncost to nearly zero yet delivering the state-of-the-art (SOTA) performance.\nUnder the hood, MAE-DET maximizes the differential entropy of detection\nbackbones, leading to a better feature extractor for object detection under the\nsame computational budgets. After merely one GPU day of fully automatic design,\nMAE-DET innovates SOTA detection backbones on multiple detection benchmark\ndatasets with little human intervention. Comparing to ResNet-50 backbone,\nMAE-DET is $+2.0\\%$ better in mAP when using the same amount of\nFLOPs/parameters, and is $1.54$ times faster on NVIDIA V100 at the same mAP.\nCode and pre-trained models are available at\nhttps://github.com/alibaba/lightweight-neuralarchitecture-search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMFlow: Learning Optical Flow via Global Matching. (arXiv:2111.13680v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13680","description":"<p>Learning-based optical flow estimation has been dominated with the pipeline\nof cost volume with convolutions for flow regression, which is inherently\nlimited to local correlations and thus is hard to address the long-standing\nchallenge of large displacements. To alleviate this, the state-of-the-art\nframework RAFT gradually improves its prediction quality by using a large\nnumber of iterative refinements, achieving remarkable performance but\nintroducing linearly increasing inference time. To enable both high accuracy\nand efficiency, we completely revamp the dominant flow regression pipeline by\nreformulating optical flow as a global matching problem, which identifies the\ncorrespondences by directly comparing feature similarities. Specifically, we\npropose a GMFlow framework, which consists of three main components: a\ncustomized Transformer for feature enhancement, a correlation and softmax layer\nfor global feature matching, and a self-attention layer for flow propagation.\nWe further introduce a refinement step that reuses GMFlow at higher feature\nresolution for residual flow prediction. Our new framework outperforms\n31-refinements RAFT on the challenging Sintel benchmark, while using only one\nrefinement and running faster, suggesting a new paradigm for accurate and\nefficient optical flow estimation. Code is available at\nhttps://github.com/haofeixu/gmflow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haofei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MC-Blur: A Comprehensive Benchmark for Image Deblurring. (arXiv:2112.00234v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00234","description":"<p>Blur artifacts can seriously degrade the visual quality of images, and\nnumerous deblurring methods have been proposed for specific scenarios. However,\nin most real-world images, blur is caused by different factors, e.g., motion\nand defocus. In this paper, we address how different deblurring methods perform\nin the case of multiple types of blur. For in-depth performance evaluation, we\nconstruct a new large-scale multi-cause image deblurring dataset (called\nMC-Blur), including real-world and synthesized blurry images with mixed factors\nof blurs. The images in the proposed MC-Blur dataset are collected using\ndifferent techniques: averaging sharp images captured by a 1000-fps high-speed\ncamera, convolving Ultra-High-Definition (UHD) sharp images with large-size\nkernels, adding defocus to images, and real-world blurry images captured by\nvarious camera models. Based on the MC-Blur dataset, we conduct extensive\nbenchmarking studies to compare SOTA methods in different scenarios, analyze\ntheir efficiency, and investigate the built dataset's capacity. These\nbenchmarking results provide a comprehensive overview of the advantages and\nlimitations of current deblurring methods, and reveal the advances of our\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenger_B/0/1/0/all/0/1\">Bjorn Stenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lumbar Bone Mineral Density Estimation from Chest X-ray Images: Anatomy-aware Attentive Multi-ROI Modeling. (arXiv:2201.01838v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.01838","description":"<p>Osteoporosis is a common chronic metabolic bone disease often under-diagnosed\nand under-treated due to the limited access to bone mineral density (BMD)\nexaminations, e.g. via Dual-energy X-ray Absorptiometry (DXA). This paper\nproposes a method to predict BMD from Chest X-ray (CXR), one of the most\ncommonly accessible and low-cost medical imaging examinations. Our method first\nautomatically detects Regions of Interest (ROIs) of local CXR bone structures.\nThen a multi-ROI deep model with transformer encoder is developed to exploit\nboth local and global information in the chest X-ray image for accurate BMD\nestimation. Our method is evaluated on 13719 CXR patient cases with ground\ntruth BMD measured by the gold standard DXA. The model predicted BMD has a\nstrong correlation with the ground truth (Pearson correlation coefficient 0.894\non lumbar 1). When applied in osteoporosis screening, it achieves a high\nclassification performance (average AUC of 0.968). As the first effort of using\nCXR scans to predict the BMD, the proposed algorithm holds strong potential for\nearly osteoporosis screening and public health promotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fakai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_K/0/1/0/all/0/1\">Kang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chang-Fu Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_S/0/1/0/all/0/1\">Shun Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Head and eye egocentric gesture recognition for human-robot interaction using eyewear cameras. (arXiv:2201.11500v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11500","description":"<p>Non-verbal communication plays a particularly important role in a wide range\nof scenarios in Human-Robot Interaction (HRI). Accordingly, this work addresses\nthe problem of human gesture recognition. In particular, we focus on head and\neye gestures, and adopt an egocentric (first-person) perspective using eyewear\ncameras. We argue that this egocentric view may offer a number of conceptual\nand technical benefits over scene- or robot-centric perspectives. A\nmotion-based recognition approach is proposed, which operates at two temporal\ngranularities. Locally, frame-to-frame homographies are estimated with a\nconvolutional neural network (CNN). The output of this CNN is input to a long\nshort-term memory (LSTM) to capture longer-term temporal visual relationships,\nwhich are relevant to characterize gestures. Regarding the configuration of the\nnetwork architecture, one particularly interesting finding is that using the\noutput of an internal layer of the homography CNN increases the recognition\nrate with respect to using the homography matrix itself. While this work\nfocuses on action recognition, and no robot or user study has been conducted\nyet, the system has been designed to meet real-time constraints. The\nencouraging results suggest that the proposed egocentric perspective is viable,\nand this proof-of-concept work provides novel and useful contributions to the\nexciting area of HRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marina_Miranda_J/0/1/0/all/0/1\">Javier Marina-Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traver_V/0/1/0/all/0/1\">V. Javier Traver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones. (arXiv:2202.07028v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.07028","description":"<p>We study the problem of developing autonomous agents that can follow human\ninstructions to infer and perform a sequence of actions to complete the\nunderlying task. Significant progress has been made in recent years, especially\nfor tasks with short horizons. However, when it comes to long-horizon tasks\nwith extended sequences of actions, an agent can easily ignore some\ninstructions or get stuck in the middle of the long instructions and eventually\nfail the task. To address this challenge, we propose a model-agnostic\nmilestone-based task tracker (M-TRACK) to guide the agent and monitor its\nprogress. Specifically, we propose a milestone builder that tags the\ninstructions with navigation and interaction milestones which the agent needs\nto complete step by step, and a milestone checker that systemically checks the\nagent's progress in its current milestone and determines when to proceed to the\nnext. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and\n52% relative improvement in unseen success rate over two competitive base\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1\">Jihyung Kil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image translation of Ultrasound to Pseudo Anatomical Display by CycleGAN. (arXiv:2202.08053v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.08053","description":"<p>Ultrasound is the second most used modality in medical imaging. It is cost\neffective, hazardless, portable and implemented routinely in numerous clinical\nprocedures. Nonetheless, image quality is characterized by granulated\nappearance, poor SNR and speckle noise. Specific for malignant tumors, the\nmargins are blurred and indistinct. Thus, there is a great need for improving\nultrasound image quality. We hypothesize that this can be achieved, using\nneural networks, by translation into a more realistic display which mimics an\nanatomical cut through the tissue. In order to achieve this goal, the\npreferable approach would be to use a set of paired images. However, this is\npractically impossible in our case. Therefore, Cycle Generative Adversarial\nNetwork (CycleGAN) was used, in order to learn each domain properties\nseparately and enforce cross domain cycle consistency. The two datasets which\nwere used for training the model were \"Breast Ultrasound Images\" (BUSI) and a\nset of optic images of poultry breast tissue samples acquired at our lab. The\ngenerated pseudo anatomical images provide improved visual discrimination of\nthe lesions with clearer border definition and pronounced contrast. In order to\nevaluate the preservation of the anatomical features, the lesions in the\nultrasonic images and the generated pseudo anatomical images were both\nautomatically segmented and compared. This comparison yielded median dice score\nof 0.91 for the benign tumors and 0.70 for the malignant ones. The median\nlesion center error was 0.58% and 3.27% for the benign and malignancies\nrespectively and the median area error index was 0.40% and 4.34% for the benign\nand malignancies respectively. In conclusion, these generated pseudo anatomical\nimages, which are presented in a more intuitive way, enhance tissue anatomy and\ncan potentially simplify the diagnosis and improve the clinical outcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Barkat_L/0/1/0/all/0/1\">Lilach Barkat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azhari_H/0/1/0/all/0/1\">Haim Azhari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs. (arXiv:2203.12344v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12344","description":"<p>We aim to understand how actions are performed and identify subtle\ndifferences, such as 'fold firmly' vs. 'fold gently'. To this end, we propose a\nmethod which recognizes adverbs across different actions. However, such\nfine-grained annotations are difficult to obtain and their long-tailed nature\nmakes it challenging to recognize adverbs in rare action-adverb compositions.\nOur approach therefore uses semi-supervised learning with multiple adverb\npseudo-labels to leverage videos with only action labels. Combined with\nadaptive thresholding of these pseudo-adverbs we are able to make efficient use\nof the available data while tackling the long-tailed distribution.\nAdditionally, we gather adverb annotations for three existing video retrieval\ndatasets, which allows us to introduce the new tasks of recognizing adverbs in\nunseen action-adverb compositions and unseen domains. Experiments demonstrate\nthe effectiveness of our method, which outperforms prior work in recognizing\nadverbs and semi-supervised works adapted for adverb recognition. We also show\nhow adverbs can relate fine-grained actions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1\">Hazel Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Plain Vision Transformer Backbones for Object Detection. (arXiv:2203.16527v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16527","description":"<p>We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone\nnetwork for object detection. This design enables the original ViT architecture\nto be fine-tuned for object detection without needing to redesign a\nhierarchical backbone for pre-training. With minimal adaptations for\nfine-tuning, our plain-backbone detector can achieve competitive results.\nSurprisingly, we observe: (i) it is sufficient to build a simple feature\npyramid from a single-scale feature map (without the common FPN design) and\n(ii) it is sufficient to use window attention (without shifting) aided with\nvery few cross-window propagation blocks. With plain ViT backbones pre-trained\nas Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the\nprevious leading methods that were all based on hierarchical backbones,\nreaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K\npre-training. We hope our study will draw attention to research on\nplain-backbone detectors. Code for ViTDet is available in Detectron2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Hanzi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1\">Ross Girshick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kaiming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Convolutional Neural Networks on Raspberry Pi for Image Classification. (arXiv:2204.00943v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00943","description":"<p>With the good performance of deep learning algorithms in the field of\ncomputer vision (CV), the convolutional neural network (CNN) architecture has\nbecome a main backbone of the computer vision task. With the widespread use of\nmobile devices, neural network models based on platforms with low computing\npower are gradually being paid attention. However, due to the limitation of\ncomputing power, deep learning algorithms are usually not available on mobile\ndevices. This paper proposes a lightweight convolutional neural network,\nTripleNet, which can operate easily on Raspberry Pi. Adopted from the concept\nof block connections in ThreshNet, the newly proposed network model compresses\nand accelerates the network model, reduces the amount of parameters of the\nnetwork, and shortens the inference time of each image while ensuring the\naccuracy. Our proposed TripleNet and other state-of-the-art (SOTA) neural\nnetworks perform image classification experiments with the CIFAR-10 and SVHN\ndatasets on Raspberry Pi. The experimental results show that, compared with\nGhostNet, MobileNet, ThreshNet, EfficientNet, and HarDNet, the inference time\nof TripleNet per image is shortened by 15%, 16%, 17%, 24%, and 30%,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1\">Rui-Yang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_J/0/1/0/all/0/1\">Jia-Hao Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jen-Shiun Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 6th AI City Challenge. (arXiv:2204.10380v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10380","description":"<p>The 6th edition of the AI City Challenge specifically focuses on problems in\ntwo domains where there is tremendous unlocked potential at the intersection of\ncomputer vision and artificial intelligence: Intelligent Traffic Systems (ITS),\nand brick and mortar retail businesses. The four challenge tracks of the 2022\nAI City Challenge received participation requests from 254 teams across 27\ncountries. Track 1 addressed city-scale multi-target multi-camera (MTMC)\nvehicle tracking. Track 2 addressed natural-language-based vehicle track\nretrieval. Track 3 was a brand new track for naturalistic driving analysis,\nwhere the data were captured by several cameras mounted inside the vehicle\nfocusing on driver safety, and the task was to classify driver actions. Track 4\nwas another new track aiming to achieve retail store automated checkout using\nonly a single view camera. We released two leader boards for submissions based\non different methods, including a public leader board for the contest, where no\nuse of external data is allowed, and a general leader board for all submitted\nresults. The top performance of participating teams established strong\nbaselines and even outperformed the state-of-the-art in the proposed challenge\ntracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naphade_M/0/1/0/all/0/1\">Milind Naphade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1\">David C. Anastasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yue Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammed Shaiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatachalapathy_A/0/1/0/all/0/1\">Archana Venkatachalapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1\">Vitaly Ablavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pranamesh Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alice Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shangru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Supervised Distillation for Continual Representation Learning. (arXiv:2205.05476v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05476","description":"<p>In this paper, we propose a novel training procedure for the continual\nrepresentation learning problem in which a neural network model is sequentially\nlearned to alleviate catastrophic forgetting in visual search tasks. Our\nmethod, called Contrastive Supervised Distillation (CSD), reduces feature\nforgetting while learning discriminative features. This is achieved by\nleveraging labels information in a distillation setting in which the student\nmodel is contrastively learned from the teacher model. Extensive experiments\nshow that CSD performs favorably in mitigating catastrophic forgetting by\noutperforming current state-of-the-art methods. Our results also provide\nfurther evidence that feature forgetting evaluated in visual retrieval tasks is\nnot as catastrophic as in classification tasks. Code at:\nhttps://github.com/NiccoBiondi/ContrastiveSupervisedDistillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barletti_T/0/1/0/all/0/1\">Tommaso Barletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_N/0/1/0/all/0/1\">Niccolo&#x27; Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pernici_F/0/1/0/all/0/1\">Federico Pernici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_M/0/1/0/all/0/1\">Matteo Bruni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods. (arXiv:2205.11508v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.11508","description":"<p>Self-Supervised Learning (SSL) surmises that inputs and pairwise positive\nrelationships are enough to learn meaningful representations. Although SSL has\nrecently reached a milestone: outperforming supervised methods in many\nmodalities\\dots the theoretical foundations are limited, method-specific, and\nfail to provide principled design guidelines to practitioners. In this paper,\nwe propose a unifying framework under the helm of spectral manifold learning to\naddress those limitations. Through the course of this study, we will rigorously\ndemonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous\nspectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al.\n</p>\n<p>This unification will then allow us to obtain (i) the closed-form optimal\nrepresentation for each method, (ii) the closed-form optimal network parameters\nin the linear regime for each method, (iii) the impact of the pairwise\nrelations used during training on each of those quantities and on downstream\ntask performances, and most importantly, (iv) the first theoretical bridge\nbetween contrastive and non-contrastive methods towards global and local\nspectral embedding methods respectively, hinting at the benefits and\nlimitations of each. For example, (i) if the pairwise relation is aligned with\nthe downstream task, any SSL method can be employed successfully and will\nrecover the supervised method, but in the low data regime, VICReg's invariance\nhyper-parameter should be high; (ii) if the pairwise relation is misaligned\nwith the downstream task, VICReg with small invariance hyper-parameter should\nbe preferred over SimCLR or BarlowTwins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Model Generalization for Monocular 3D Object Detection. (arXiv:2205.11664v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11664","description":"<p>Monocular 3D object detection (Mono3D) has achieved tremendous improvements\nwith emerging large-scale autonomous driving datasets and the rapid development\nof deep learning techniques. However, caused by severe domain gaps (e.g., the\nfield of view (FOV), pixel size, and object size among datasets), Mono3D\ndetectors have difficulty in generalization, leading to drastic performance\ndegradation on unseen domains. To solve these issues, we combine the\nposition-invariant transform and multi-scale training with the pixel-size depth\nstrategy to construct an effective unified camera-generalized paradigm (CGP).\nIt fully considers discrepancies in the FOV and pixel size of images captured\nby different cameras. Moreover, we further investigate the obstacle in\nquantitative metrics when cross-dataset inference through an exhaustive\nsystematic study. We discern that the size bias of prediction leads to a\ncolossal failure. Hence, we propose the 2D-3D geometry-consistent object\nscaling strategy (GCOS) to bridge the gap via an instance-level augment. Our\nmethod called DGMono3D achieves remarkable performance on all evaluated\ndatasets and surpasses the SoTA unsupervised domain adaptation scheme even\nwithout utilizing data on the target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DGSVis: Visual Analysis of Hierarchical Snapshots in Dynamic Graph. (arXiv:2205.13220v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2205.13220","description":"<p>Dynamic graph visualization attracts researchers' concentration as it\nrepresents time-varying relationships between entities in multiple domains\n(e.g., social media analysis, academic cooperation analysis, team sports\nanalysis). Integrating visual analytic methods is consequential in presenting,\ncomparing, and reviewing dynamic graphs. Even though dynamic graph\nvisualization is developed for many years, how to effectively visualize\nlarge-scale and time-intensive dynamic graph data with subtle changes is still\nchallenging for researchers. To provide an effective analysis method for this\ntype of dynamic graph data, we propose a snapshot generation algorithm\ninvolving Human-In-Loop to help users divide the dynamic graphs into\nmulti-granularity and hierarchical snapshots for further analysis. In addition,\nwe design a visual analysis prototype system (DGSVis) to assist users in\naccessing the dynamic graph insights effectively. DGSVis integrates a graphical\noperation interface to help users generate snapshots visually and\ninteractively. It is equipped with the overview and details for visualizing\nhierarchical snapshots of the dynamic graph data. To illustrate the usability\nand efficiency of our proposed methods for this type of dynamic graph data, we\nintroduce two case studies based on basketball player networks in a\ncompetition. In addition, we conduct an evaluation and receive exciting\nfeedback from experienced visualization experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baofeng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sujia Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jingwei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lvhan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ronghua Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guodao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Illumination Adaptive Transformer. (arXiv:2205.14871v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14871","description":"<p>Challenging illumination conditions (low light, underexposure and\noverexposure) in the real world not only cast an unpleasant visual appearance\nbut also taint the computer vision tasks. Existing light adaptive methods often\ndeal with each condition individually. What is more, most of them often operate\non a RAW image or over-simplify the camera image signal processing (ISP)\npipeline. By decomposing the light transformation pipeline into local and\nglobal ISP components, we propose a lightweight fast Illumination Adaptive\nTransformer (IAT) which comprises two transformer-style branches: local\nestimation branch and global ISP branch. While the local branch estimates the\npixel-wise local components relevant to illumination, the global branch defines\nlearnable quires that attend the whole image to decode the parameters. Our IAT\ncould also conduct both object detection and semantic segmentation under\nvarious light conditions. We have extensively evaluated IAT on multiple\nreal-world datasets on 2 low-level tasks and 3 high-level tasks. With only 90k\nparameters and 0.004s processing speed (excluding high-level module), our IAT\nhas consistently achieved superior performance over SOTA. Code is available at\nhttps://github.com/cuiziteng/Illumination-Adaptive-Transformer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Ziteng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1\">Lin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shenghan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CompleteDT: Point Cloud Completion with Dense Augment Inference Transformers. (arXiv:2205.14999v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14999","description":"<p>Point cloud completion task aims to predict the missing part of incomplete\npoint clouds and generate complete point clouds with details. In this paper, we\npropose a novel point cloud completion network, namely CompleteDT.\nSpecifically, features are learned from point clouds with different\nresolutions, which is sampled from the incomplete input, and are converted to a\nseries of \\textit{spots} based on the geometrical structure. Then, the Dense\nRelation Augment Module (DRA) based on the transformer is proposed to learn\nfeatures within \\textit{spots} and consider the correlation among these\n\\textit{spots}. The DRA consists of Point Local-Attention Module (PLA) and\nPoint Dense Multi-Scale Attention Module (PDMA), where the PLA captures the\nlocal information within the local \\textit{spots} by adaptively measuring\nweights of neighbors and the PDMA exploits the global relationship between\nthese \\textit{spots} in a multi-scale densely connected manner. Lastly, the\ncomplete shape is predicted from \\textit{spots} by the Multi-resolution Point\nFusion Module (MPF), which gradually generates complete point clouds from\n\\textit{spots}, and updates \\textit{spots} based on these generated point\nclouds. Experimental results show that, because the DRA based on the\ntransformer can learn the expressive features from the incomplete input and the\nMPF can fully explore these feature to predict the complete input, our method\nlargely outperforms the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shaokun Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xView3-SAR: Detecting Dark Fishing Activity Using Synthetic Aperture Radar Imagery. (arXiv:2206.00897v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00897","description":"<p>Unsustainable fishing practices worldwide pose a major threat to marine\nresources and ecosystems. Identifying vessels that evade monitoring systems --\nknown as \"dark vessels\" -- is key to managing and securing the health of marine\nenvironments. With the rise of satellite-based synthetic aperture radar (SAR)\nimaging and modern machine learning (ML), it is now possible to automate\ndetection of dark vessels day or night, under all-weather conditions. SAR\nimages, however, require domain-specific treatment and is not widely accessible\nto the ML community. Moreover, the objects (vessels) are small and sparse,\nchallenging traditional computer vision approaches. We present the largest\nlabeled dataset for training ML models to detect and characterize vessels from\nSAR. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the\nSentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The\nimages are annotated using a combination of automated and manual analysis.\nCo-located bathymetry and wind state rasters accompany every SAR image. We\nprovide an overview of the results from the xView3 Computer Vision Challenge,\nan international competition using xView3-SAR for ship detection and\ncharacterization at large scale. We release the data (https://iuu.xview.us/)\nand code (https://github.com/DIUx-xView) to support ongoing development and\nevaluation of ML approaches for this important application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paolo_F/0/1/0/all/0/1\">Fernando Paolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsu-ting Tim Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Ritwik Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_B/0/1/0/all/0/1\">Bryce Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1\">Nirav Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuster_D/0/1/0/all/0/1\">Daniel Kuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroodsma_D/0/1/0/all/0/1\">David Kroodsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunnmon_J/0/1/0/all/0/1\">Jared Dunnmon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images. (arXiv:2206.01256v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01256","description":"<p>In this paper, we propose PETRv2, a unified framework for 3D perception from\nmulti-view images. Based on PETR, PETRv2 explores the effectiveness of temporal\nmodeling, which utilizes the temporal information of previous frames to boost\n3D object detection. More specifically, we extend the 3D position embedding (3D\nPE) in PETR for temporal modeling. The 3D PE achieves the temporal alignment on\nobject position of different frames. A feature-guided position encoder is\nfurther introduced to improve the data adaptability of 3D PE. To support for\nhigh-quality BEV segmentation, PETRv2 provides a simply yet effective solution\nby adding a set of segmentation queries. Each segmentation query is responsible\nfor segmenting one specific patch of BEV map. PETRv2 achieves state-of-the-art\nperformance on 3D object detection and BEV segmentation. Detailed robustness\nanalysis is also conducted on PETR framework. We hope PETRv2 can serve as a\nstrong baseline for 3D perception. Code is available at\n\\url{https://github.com/megvii-research/PETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1\">Fan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIDNet: A Real-time Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02066","description":"<p>Two-branch network architecture has shown its efficiency and effectiveness\nfor real-time semantic segmentation tasks. However, direct fusion of low-level\ndetails and high-level semantics will lead to a phenomenon that the detailed\nfeatures are easily overwhelmed by surrounding contextual information, namely\novershoot in this paper, which limits the improvement of the accuracy of\nexisted two-branch models. In this paper, we bridge a connection between\nConvolutional Neural Network (CNN) and Proportional-Integral-Derivative (PID)\ncontroller and reveal that the two-branch network is nothing but a\nProportional-Integral (PI) controller, which inherently suffers from the\nsimilar overshoot issue. To alleviate this issue, we propose a novel\nthree-branch network architecture: PIDNet, which possesses three branches to\nparse the detailed, context and boundary information (derivative of semantics),\nrespectively, and employs boundary attention to guide the fusion of detailed\nand context branches in final stage. The family of PIDNets achieve the best\ntrade-off between inference speed and accuracy and their test accuracy\nsurpasses all the existed models with similar inference speed on Cityscapes,\nCamVid and COCO-Stuff datasets. Especially, PIDNet-S achieves 78.6% mIOU with\ninference speed of 93.2 FPS on Cityscapes test set and 80.1% mIOU with speed of\n153.7 FPS on CamVid test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zixiang Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1\">Shankar P. Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.03826","description":"<p>For unsupervised pretraining, mask-reconstruction pretraining (MRP)\napproaches randomly mask input patches and then reconstruct pixels or semantic\nfeatures of these masked patches via an auto-encoder. Then for a downstream\ntask, supervised fine-tuning the pretrained encoder remarkably surpasses the\nconventional supervised learning (SL) trained from scratch. However, it is\nstill unclear 1) how MRP performs semantic learning in the pretraining phase\nand 2) why it helps in downstream tasks. To solve these problems, we\ntheoretically show that on an auto-encoder of a two/one-layered convolution\nencoder/decoder, MRP can capture all discriminative semantics in the\npretraining dataset, and accordingly show its provable improvement over SL on\nthe classification downstream task. Specifically, we assume that pretraining\ndataset contains multi-view samples of ratio $1-\\mu$ and single-view samples of\nratio $\\mu$, where multi/single-view samples has multiple/single discriminative\nsemantics. Then for pretraining, we prove that 1) the convolution kernels of\nthe MRP encoder captures all discriminative semantics in the pretraining data;\nand 2) a convolution kernel captures at most one semantic. Accordingly, in the\ndownstream supervised fine-tuning, most semantics would be captured and\ndifferent semantics would not be fused together. This helps the downstream\nfine-tuned network to easily establish the relation between kernels and\nsemantic class labels. In this way, the fine-tuned encoder in MRP provably\nachieves zero test error with high probability for both multi-view and\nsingle-view test data. In contrast, as proved by~[3], conventional SL can only\nobtain a test accuracy between around $0.5\\mu$ for single-view test data. These\nresults together explain the benefits of MRP in downstream tasks. Experimental\nresults testify to multi-view data assumptions and our theoretical\nimplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiachun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Narrowing the Coordinate-frame Gap in Behavior Prediction Models: Distillation for Efficient and Accurate Scene-centric Motion Forecasting. (arXiv:2206.03970v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03970","description":"<p>Behavior prediction models have proliferated in recent years, especially in\nthe popular real-world robotics application of autonomous driving, where\nrepresenting the distribution over possible futures of moving agents is\nessential for safe and comfortable motion planning. In these models, the choice\nof coordinate frames to represent inputs and outputs has crucial trade offs\nwhich broadly fall into one of two categories. Agent-centric models transform\ninputs and perform inference in agent-centric coordinates. These models are\nintrinsically invariant to translation and rotation between scene elements, are\nbest-performing on public leaderboards, but scale quadratically with the number\nof agents and scene elements. Scene-centric models use a fixed coordinate\nsystem to process all agents. This gives them the advantage of sharing\nrepresentations among all agents, offering efficient amortized inference\ncomputation which scales linearly with the number of agents. However, these\nmodels have to learn invariance to translation and rotation between scene\nelements, and typically underperform agent-centric models. In this work, we\ndevelop knowledge distillation techniques between probabilistic motion\nforecasting models, and apply these techniques to close the gap in performance\nbetween agent-centric and scene-centric models. This improves scene-centric\nmodel performance by 13.2% on the public Argoverse benchmark, 7.8% on Waymo\nOpen Dataset and up to 9.4% on a large In-House dataset. These improved\nscene-centric models rank highly in public leaderboards and are up to 15 times\nmore efficient than their agent-centric teacher counterparts in busy scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">DiJia Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douillard_B/0/1/0/all/0/1\">Bertrand Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Cheolho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Benjamin Sapp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Score-based Generative Models for High-Resolution Image Synthesis. (arXiv:2206.04029v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04029","description":"<p>Score-based generative models (SGMs) have recently emerged as a promising\nclass of generative models. The key idea is to produce high-quality images by\nrecurrently adding Gaussian noises and gradients to a Gaussian sample until\nconverging to the target distribution, a.k.a. the diffusion sampling. To ensure\nstability of convergence in sampling and generation quality, however, this\nsequential sampling process has to take a small step size and many sampling\niterations (e.g., 2000). Several acceleration methods have been proposed with\nfocus on low-resolution generation. In this work, we consider the acceleration\nof high-resolution generation with SGMs, a more challenging yet more important\nproblem. We prove theoretically that this slow convergence drawback is\nprimarily due to the ignorance of the target distribution. Further, we\nintroduce a novel Target Distribution Aware Sampling (TDAS) method by\nleveraging the structural priors in space and frequency domains. Extensive\nexperiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can\nconsistently accelerate state-of-the-art SGMs, particularly on more challenging\nhigh resolution (1024x1024) image generation tasks by up to 18.4x, whilst\nlargely maintaining the synthesis quality. With fewer sampling iterations, TDAS\ncan still generate good quality images. In contrast, the existing methods\ndegrade drastically or even fails completely\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CASS: Cross Architectural Self-Supervision for Medical Image Analysis. (arXiv:2206.04170v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04170","description":"<p>Recent advances in Deep Learning and Computer Vision have alleviated many of\nthe bottlenecks, allowing algorithms to be label-free with better performance.\nSpecifically, Transformers provide a global perspective of the image, which\nConvolutional Neural Networks (CNN) lack by design. Here we present Cross\nArchitectural Self-Supervision, a novel self-supervised learning approach which\nleverages transformers and CNN simultaneously, while also being computationally\naccessible to general practitioners via easily available cloud services.\nCompared to existing state-of-the-art self-supervised learning approaches, we\nempirically show CASS trained CNNs, and Transformers gained an average of 8.5%\nwith 100% labelled data, 7.3% with 10% labelled data, and 11.5% with 1%\nlabelled data, across three diverse datasets. Notably, one of the employed\ndatasets included histopathology slides of an autoimmune disease, a topic\nunderrepresented in Medical Imaging and has minimal data. In addition, our\nfindings reveal that CASS is twice as efficient as other state-of-the-art\nmethods in terms of training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pranav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sizikova_E/0/1/0/all/0/1\">Elena Sizikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cirrone_J/0/1/0/all/0/1\">Jacopo Cirrone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Cues Lead to a Strong Multi-Object Tracker. (arXiv:2206.04656v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04656","description":"<p>For a long time, the most common paradigm in Multi-Object Tracking was\ntracking-by-detection (TbD), where objects are first detected and then\nassociated over video frames. For association, most models resource to motion\nand appearance cues. While still relying on these cues, recent approaches based\non, e.g., attention have shown an ever-increasing need for training data and\noverall complex frameworks. We claim that 1) strong cues can be obtained from\nlittle amounts of training data if some key design choices are applied, 2)\ngiven these strong cues, standard Hungarian matching-based association is\nenough to obtain impressive results. Our main insight is to identify key\ncomponents that allow a standard reidentification network to excel at\nappearance-based tracking. We extensively analyze its failure cases and show\nthat a combination of our appearance features with a simple motion model leads\nto strong tracking results. Our model achieves state-of-the-art performance on\nMOT17 and MOT20 datasets outperforming previous state-of-the-art trackers by up\nto 5.4pp in IDF1 and 4.4pp in HOTA. We will release the code and models after\nthe paper's acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seidenschwarz_J/0/1/0/all/0/1\">Jenny Seidenschwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braso_G/0/1/0/all/0/1\">Guillem Bras&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}