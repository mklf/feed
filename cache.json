{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.6","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Diachronic Text Mining Investigation of Therapeutic Candidates for COVID-19. (arXiv:2110.13971v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13971","description":"<p>Diachronic text mining has frequently been applied to long-term linguistic\nsurveys of word meaning and usage shifts over time. In this paper we apply\nshort-term diachronic text mining to a rapidly growing corpus of scientific\npublications on COVID-19 captured in the CORD-19 dataset in order to identify\nco-occurrences and analyze the behavior of potential candidate treatments. We\nused a data set associated with a COVID-19 drug re-purposing study from Oak\nRidge National Laboratory. This study identified existing candidate coronavirus\ntreatments, including drugs and approved compounds, which had been analyzed and\nranked according to their potential for blocking the ability of the SARS-COV-2\nvirus to invade human cells. We investigated the occurrence of these candidates\nin temporal instances of the CORD-19 corpus. We found that at least 25% of the\nidentified terms occurred in temporal instances of the corpus to the extent\nthat their frequency and contextual dynamics could be evaluated. We identified\nthree classes of behaviors: those where frequency and contextual shifts were\nsmall and positively correlated; those where there was no correlation between\nfrequency and contextual changes; and those where there was a negative\ncorrelation between frequency and contextual shift. We speculate that the\nlatter two patterns are indicative that a target candidate therapeutics is\nundergoing active evaluation. The patterns we detected demonstrate the\npotential benefits of using diachronic text mining techniques with a large\ndynamic text corpus to track drug-repurposing activities across international\nclinical and laboratory settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Powell_J/0/1/0/all/0/1\">James Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sentz_K/0/1/0/all/0/1\">Kari Sentz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks and Defenses for Social Network Text Processing Applications: Techniques, Challenges and Future Research Directions. (arXiv:2110.13980v1 [cs.CL])","link":"http://arxiv.org/abs/2110.13980","description":"<p>The growing use of social media has led to the development of several Machine\nLearning (ML) and Natural Language Processing(NLP) tools to process the\nunprecedented amount of social media content to make actionable decisions.\nHowever, these MLand NLP algorithms have been widely shown to be vulnerable to\nadversarial attacks. These vulnerabilities allow adversaries to launch a\ndiversified set of adversarial attacks on these algorithms in different\napplications of social media text processing. In this paper, we provide a\ncomprehensive review of the main approaches for adversarial attacks and\ndefenses in the context of social media applications with a particular focus on\nkey challenges and future research directions. In detail, we cover literature\non six key applications, namely (i) rumors detection, (ii) satires detection,\n(iii) clickbait &amp; spams identification, (iv) hate speech detection,\n(v)misinformation detection, and (vi) sentiment analysis. We then highlight the\nconcurrent and anticipated future research questions and provide\nrecommendations and directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alsmadi_I/0/1/0/all/0/1\">Izzat Alsmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazzal_M/0/1/0/all/0/1\">Mahmoud Nazzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khreishah_A/0/1/0/all/0/1\">Abdallah Khreishah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Algosaibi_A/0/1/0/all/0/1\">Abdulelah Algosaibi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connect-the-Dots: Bridging Semantics between Words and Definitions via Aligning Word Sense Inventories. (arXiv:2110.14091v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14091","description":"<p>Word Sense Disambiguation (WSD) aims to automatically identify the exact\nmeaning of one word according to its context. Existing supervised models\nstruggle to make correct predictions on rare word senses due to limited\ntraining data and can only select the best definition sentence from one\npredefined word sense inventory (e.g., WordNet). To address the data sparsity\nproblem and generalize the model to be independent of one predefined inventory,\nwe propose a gloss alignment algorithm that can align definition sentences\n(glosses) with the same meaning from different sense inventories to collect\nrich lexical knowledge. We then train a model to identify semantic equivalence\nbetween a target word in context and one of its glosses using these aligned\ninventories, which exhibits strong transfer capability to many WSD tasks.\nExperiments on benchmark datasets show that the proposed method improves\npredictions on both frequent and rare word senses, outperforming prior work by\n1.2% on the All-Words WSD Task and 4.3% on the Low-Shot WSD Task. Evaluation on\nWiC Task also indicates that our method can better capture word meanings in\ncontext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Verifiers to Solve Math Word Problems. (arXiv:2110.14168v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14168","description":"<p>State-of-the-art language models can match human performance on many tasks,\nbut they still struggle to robustly perform multi-step mathematical reasoning.\nTo diagnose the failures of current models and support research, we introduce\nGSM8K, a dataset of 8.5K high quality linguistically diverse grade school math\nword problems. We find that even the largest transformer models fail to achieve\nhigh test performance, despite the conceptual simplicity of this problem\ndistribution. To increase performance, we propose training verifiers to judge\nthe correctness of model completions. At test time, we generate many candidate\nsolutions and select the one ranked highest by the verifier. We demonstrate\nthat verification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cobbe_K/0/1/0/all/0/1\">Karl Cobbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosaraju_V/0/1/0/all/0/1\">Vineet Kosaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bavarian_M/0/1/0/all/0/1\">Mohammad Bavarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakano_R/0/1/0/all/0/1\">Reiichiro Nakano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesse_C/0/1/0/all/0/1\">Christopher Hesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Standing on the Shoulders of Predecessors: Meta-Knowledge Transfer for Knowledge Graphs. (arXiv:2110.14170v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14170","description":"<p>Knowledge graphs (KGs) have become widespread, and various knowledge graphs\nare constructed incessantly to support many in-KG and out-of-KG applications.\nDuring the construction of KGs, although new KGs may contain new entities with\nrespect to constructed KGs, some entity-independent knowledge can be\ntransferred from constructed KGs to new KGs. We call such knowledge\nmeta-knowledge, and refer to the problem of transferring meta-knowledge from\nconstructed (source) KGs to new (target) KGs to improve the performance of\ntasks on target KGs as meta-knowledge transfer for knowledge graphs. However,\nthere is no available general framework that can tackle meta-knowledge transfer\nfor both in-KG and out-of-KG tasks uniformly. Therefore, in this paper, we\npropose a framework, MorsE, which means conducting Meta-Learning for\nMeta-Knowledge Transfer via Knowledge Graph Embedding. MorsE represents the\nmeta-knowledge via Knowledge Graph Embedding and learns the meta-knowledge by\nMeta-Learning. Specifically, MorsE uses an entity initializer and a Graph\nNeural Network (GNN) modulator to entity-independently obtain entity embeddings\ngiven a KG and is trained following the meta-learning setting to gain the\nability of effectively obtaining embeddings. Experimental results on\nmeta-knowledge transfer for both in-KG and out-of-KG tasks show that MorsE is\nable to learn and transfer meta-knowledge between KGs effectively, and\noutperforms existing state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zonggang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity Enhanced Active Learning with Strictly Proper Scoring Rules. (arXiv:2110.14171v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14171","description":"<p>We study acquisition functions for active learning (AL) for text\nclassification. The Expected Loss Reduction (ELR) method focuses on a Bayesian\nestimate of the reduction in classification error, recently updated with Mean\nObjective Cost of Uncertainty (MOCU). We convert the ELR framework to estimate\nthe increase in (strictly proper) scores like log probability or negative mean\nsquare error, which we call Bayesian Estimate of Mean Proper Scores (BEMPS). We\nalso prove convergence results borrowing techniques used with MOCU. In order to\nallow better experimentation with the new acquisition functions, we develop a\ncomplementary batch AL algorithm, which encourages diversity in the vector of\nexpected changes in scores for unlabelled data. To allow high performance text\nclassifiers, we combine ensembling and dynamic validation set construction on\npretrained language models. Extensive experimental evaluation then explores how\nthese different acquisition functions perform. The results show that the use of\nmean square error and log probability with BEMPS yields robust acquisition\nfunctions, which consistently outperform the others tested.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidential Softmax for Sparse Multimodal Distributions in Deep Generative Models. (arXiv:2110.14182v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14182","description":"<p>Many applications of generative models rely on the marginalization of their\nhigh-dimensional output probability distributions. Normalization functions that\nyield sparse probability distributions can make exact marginalization more\ncomputationally tractable. However, sparse normalization functions usually\nrequire alternative loss functions for training since the log-likelihood is\nundefined for sparse probability distributions. Furthermore, many sparse\nnormalization functions often collapse the multimodality of distributions. In\nthis work, we present $\\textit{ev-softmax}$, a sparse normalization function\nthat preserves the multimodality of probability distributions. We derive its\nproperties, including its gradient in closed-form, and introduce a continuous\nfamily of approximations to $\\textit{ev-softmax}$ that have full support and\ncan be trained with probabilistic loss functions such as negative\nlog-likelihood and Kullback-Leibler divergence. We evaluate our method on a\nvariety of generative models, including variational autoencoders and\nauto-regressive architectures. Our method outperforms existing dense and sparse\nnormalization techniques in distributional accuracy. We demonstrate that\n$\\textit{ev-softmax}$ successfully reduces the dimensionality of probability\ndistributions while maintaining multimodality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Phil Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syllabic Quantity Patterns as Rhythmic Features for Latin Authorship Attribution. (arXiv:2110.14203v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14203","description":"<p>It is well known that, within the Latin production of written text, peculiar\nmetric schemes were followed not only in poetic compositions, but also in many\nprose works. Such metric patterns were based on so-called syllabic quantity,\ni.e., on the length of the involved syllables, and there is substantial\nevidence suggesting that certain authors had a preference for certain metric\npatterns over others. In this research we investigate the possibility to employ\nsyllabic quantity as a base for deriving rhythmic features for the task of\ncomputational authorship attribution of Latin prose texts. We test the impact\nof these features on the authorship attribution task when combined with other\ntopic-agnostic features. Our experiments, carried out on three different\ndatasets, using two different machine learning methods, show that rhythmic\nfeatures based on syllabic quantity are beneficial in discriminating among\nLatin prose authors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corbara_S/0/1/0/all/0/1\">Silvia Corbara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI. (arXiv:2110.14207v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14207","description":"<p>Many real-world problems require the combined application of multiple\nreasoning abilities employing suitable abstractions, commonsense knowledge, and\ncreative synthesis of problem-solving strategies. To help advance AI systems\ntowards such capabilities, we propose a new reasoning challenge, namely Fermi\nProblems (FPs), which are questions whose answers can only be approximately\nestimated because their precise computation is either impractical or\nimpossible. For example, \"How much would the sea level rise if all ice in the\nworld melted?\" FPs are commonly used in quizzes and interviews to bring out and\nevaluate the creative reasoning abilities of humans. To do the same for AI\nsystems, we present two datasets: 1) A collection of 1k real-world FPs sourced\nfrom quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate\ncomplexity to serve as a sandbox for the harder real-world challenge. In\naddition to question answer pairs, the datasets contain detailed solutions in\nthe form of an executable program and supporting facts, helping in supervision\nand evaluation of intermediate steps. We demonstrate that even extensively\nfine-tuned large scale language models perform poorly on these datasets, on\naverage making estimates that are off by two orders of magnitude. Our\ncontribution is thus the crystallization of several unsolved AI problems into a\nsingle, new challenge that we hope will spur further advances in building\nsystems that can reason.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Arjun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emoji-based Co-attention Network for Microblog Sentiment Analysis. (arXiv:2110.14227v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14227","description":"<p>Emojis are widely used in online social networks to express emotions,\nattitudes, and opinions. As emotional-oriented characters, emojis can be\nmodeled as important features of emotions towards the recipient or subject for\nsentiment analysis. However, existing methods mainly take emojis as heuristic\ninformation that fails to resolve the problem of ambiguity noise. Recent\nresearches have utilized emojis as an independent input to classify text\nsentiment but they ignore the emotional impact of the interaction between text\nand emojis. It results that the emotional semantics of emojis cannot be fully\nexplored. In this paper, we propose an emoji-based co-attention network that\nlearns the mutual emotional semantics between text and emojis on microblogs.\nOur model adopts the co-attention mechanism based on bidirectional long\nshort-term memory incorporating the text and emojis, and integrates a\nsqueeze-and-excitation block in a convolutional neural network classifier to\nincrease its sensitivity to emotional semantic features. Experimental results\nshow that the proposed method can significantly outperform several baselines\nfor sentiment analysis on short texts of social media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaowei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Honglei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic population-based meta-learning for multi-agent communication with natural language. (arXiv:2110.14241v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14241","description":"<p>In this work, our goal is to train agents that can coordinate with seen,\nunseen as well as human partners in a multi-agent communication environment\ninvolving natural language. Previous work using a single set of agents has\nshown great progress in generalizing to known partners, however it struggles\nwhen coordinating with unfamiliar agents. To mitigate that, recent work\nexplored the use of population-based approaches, where multiple agents interact\nwith each other with the goal of learning more generic protocols. These\nmethods, while able to result in good coordination between unseen partners,\nstill only achieve so in cases of simple languages, thus failing to adapt to\nhuman partners using natural language. We attribute this to the use of static\npopulations and instead propose a dynamic population-based meta-learning\napproach that builds such a population in an iterative manner. We perform a\nholistic evaluation of our method on two different referential games, and show\nthat our agents outperform all prior work when communicating with seen partners\nand humans. Furthermore, we analyze the natural language generation skills of\nour agents, where we find that our agents also outperform strong baselines.\nFinally, we test the robustness of our agents when communicating with\nout-of-population agents and carefully test the importance of each component of\nour method through ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1\">Marc Lanctot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1\">Angeliki Lazaridou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning. (arXiv:2110.14266v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14266","description":"<p>State-of-the-art approaches to reasoning and question answering over\nknowledge graphs (KGs) usually scale with the number of edges and can only be\napplied effectively on small instance-dependent subgraphs. In this paper, we\naddress this issue by showing that multi-hop and more complex logical reasoning\ncan be accomplished separately without losing expressive power. Motivated by\nthis insight, we propose an approach to multi-hop reasoning that scales\nlinearly with the number of relation types in the graph, which is usually\nsignificantly smaller than the number of edges or nodes. This produces a set of\ncandidate solutions that can be provably refined to recover the solution to the\noriginal problem. Our experiments on knowledge-based question answering show\nthat our approach solves the multi-hop MetaQA dataset, achieves a new\nstate-of-the-art on the more challenging WebQuestionsSP, is orders of magnitude\nmore scalable than competitive approaches, and can achieve compositional\ngeneralization out of the training distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atzeni_M/0/1/0/all/0/1\">Mattia Atzeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogojeska_J/0/1/0/all/0/1\">Jasmina Bogojeska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1\">Andreas Loukas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning For Prominence Detection In Children's Read Speech. (arXiv:2110.14273v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14273","description":"<p>The detection of perceived prominence in speech has attracted approaches\nranging from the design of linguistic knowledge-based acoustic features to the\nautomatic feature learning from suprasegmental attributes such as pitch and\nintensity contours. We present here, in contrast, a system that operates\ndirectly on segmented speech waveforms to learn features relevant to prominent\nword detection for children's oral fluency assessment. The chosen CRNN\n(convolutional recurrent neural network) framework, incorporating both\nword-level features and sequence information, is found to benefit from the\nperceptually motivated SincNet filters as the first convolutional layer. We\nfurther explore the benefits of the linguistic association between the prosodic\nevents of phrase boundary and prominence with different multi-task\narchitectures. Matching the previously reported performance on the same dataset\nof a random forest ensemble predictor trained on carefully chosen hand-crafted\nacoustic features, we evaluate further the possibly complementary information\nfrom hand-crafted acoustic and pre-trained lexical features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_M/0/1/0/all/0/1\">Mithilesh Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabu_K/0/1/0/all/0/1\">Kamini Sabu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_P/0/1/0/all/0/1\">Preeti Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Linguistic Distance help Language Classification? Assessing Hawrami-Zaza and Kurmanji-Sorani. (arXiv:2110.14398v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14398","description":"<p>To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a\nlanguage have been discussed and debated for a while among linguists active in\nstudying Iranian languages. The question of whether those languages/dialects\nbelong to the Kurdish language or if they are independent descendants of\nIranian languages was answered by MacKenzie (1961). However, a majority of\npeople who speak the dialects are against that answer. Their disapproval mainly\nseems to be based on the sociological, cultural, and historical relationship\namong the speakers of the dialects. While the case of Hawrami and Zaza has\nremained unexplored and under-examined, an almost unanimous agreement exists\nabout the classification of Kurmanji and Sorani as Kurdish dialects. The\nrelated studies to address the mentioned cases are primarily qualitative.\nHowever, computational linguistics could approach the question from a\nquantitative perspective. In this research, we look into three questions from a\nlinguistic distance point of view. First, how similar or dissimilar Hawrami and\nZaza are, considering no common geographical coexistence between the two.\nSecond, what about Kurmanji and Sorani that have geographical overlap. Finally,\nwhat is the distance among all these dialects, pair by pair? We base our\ncomputation on phonetic presentations of these dialects (languages), and we\ncalculate various linguistic distances among the pairs. We analyze the data and\ndiscuss the results to conclude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hossein Hassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FacTeR-Check: Semi-automated fact-checking through Semantic Similarity and Natural Language Inference. (arXiv:2110.14532v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14532","description":"<p>Our society produces and shares overwhelming amounts of information through\nthe Online Social Networks (OSNs). Within this environment, misinformation and\ndisinformation have proliferated, becoming a public safety concern on every\ncountry. Allowing the public and professionals to efficiently find reliable\nevidence about the factual veracity of a claim is crucial to mitigate this\nharmful spread. To this end, we propose FacTeR-Check, a multilingual\narchitecture for semi-automated fact-checking that can be used for either the\ngeneral public but also useful for fact-checking organisations. FacTeR-Check\nenables retrieving fact-checked information, unchecked claims verification and\ntracking dangerous information over social media. This architectures involves\nseveral modules developed to evaluate semantic similarity, to calculate natural\nlanguage inference and to retrieve information from Online Social Networks. The\nunion of all these modules builds a semi-automated fact-checking tool able of\nverifying new claims, to extract related evidence, and to track the evolution\nof a hoax on a OSN. While individual modules are validated on related\nbenchmarks (mainly MSTS and SICK), the complete architecture is validated using\na new dataset called NLI19-SP that is publicly released with COVID-19 related\nhoaxes and tweets from Spanish social media. Our results show state-of-the-art\nperformance on the individual benchmarks, as well as producing useful analysis\nof the evolution over time of 61 different hoaxes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Alejandro Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huertas_Tato_J/0/1/0/all/0/1\">Javier Huertas-Tato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huertas_Garcia_A/0/1/0/all/0/1\">&#xc1;lvaro Huertas-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villar_Rodriguez_G/0/1/0/all/0/1\">Guillermo Villar-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1\">David Camacho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndoNLI: A Natural Language Inference Dataset for Indonesian. (arXiv:2110.14566v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14566","description":"<p>We present IndoNLI, the first human-elicited NLI dataset for Indonesian. We\nadapt the data collection protocol for MNLI and collect nearly 18K sentence\npairs annotated by crowd workers and experts. The expert-annotated data is used\nexclusively as a test set. It is designed to provide a challenging test-bed for\nIndonesian NLI by explicitly incorporating various linguistic phenomena such as\nnumerical reasoning, structural changes, idioms, or temporal and spatial\nreasoning. Experiment results show that XLM-R outperforms other pre-trained\nmodels in our data. The best performance on the expert-annotated data is still\nfar below human performance (13.4% accuracy gap), suggesting that this test set\nis especially challenging. Furthermore, our analysis shows that our\nexpert-annotated data is more diverse and contains fewer annotation artifacts\nthan the crowd-annotated data. We hope this dataset can help accelerate\nprogress in Indonesian NLP research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_R/0/1/0/all/0/1\">Rahmad Mahendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louvan_S/0/1/0/all/0/1\">Samuel Louvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_F/0/1/0/all/0/1\">Fahrurrozi Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vania_C/0/1/0/all/0/1\">Clara Vania</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training. (arXiv:2102.08098v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.08098","description":"<p>Innovations in neural architectures have fostered significant breakthroughs\nin language modeling and computer vision. Unfortunately, novel architectures\noften result in challenging hyper-parameter choices and training instability if\nthe network parameters are not properly initialized. A number of\narchitecture-specific initialization schemes have been proposed, but these\nschemes are not always portable to new architectures. This paper presents\nGradInit, an automated and architecture agnostic method for initializing neural\nnetworks. GradInit is based on a simple heuristic; the norm of each network\nlayer is adjusted so that a single step of SGD or Adam with prescribed\nhyperparameters results in the smallest possible loss value. This adjustment is\ndone by introducing a scalar multiplier variable in front of each parameter\nblock, and then optimizing these variables using a simple numerical scheme.\nGradInit accelerates the convergence and test performance of many convolutional\narchitectures, both with or without skip connections, and even without\nnormalization layers. It also improves the stability of the original\nTransformer architecture for machine translation, enabling training it without\nlearning rate warmup using either Adam or SGD under a wide range of learning\nrates and momentum coefficients. Code is available at\nhttps://github.com/zhuchen03/gradinit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1\">Renkun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kezhi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining. (arXiv:2102.08473v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.08473","description":"<p>We present a self-supervised learning framework, COCO-LM, that pretrains\nLanguage Models by COrrecting and COntrasting corrupted text sequences.\nFollowing ELECTRA-style pretraining, COCO-LM employs an auxiliary language\nmodel to corrupt text sequences, upon which it constructs two new tasks for\npretraining the main model. The first token-level task, Corrective Language\nModeling, is to detect and correct tokens replaced by the auxiliary model, in\norder to better capture token-level semantics. The second sequence-level task,\nSequence Contrastive Learning, is to align text sequences originated from the\nsame source input while ensuring uniformity in the representation space.\nExperiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms\nrecent state-of-the-art pretrained models in accuracy, but also improves\npretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50% of\nits pretraining GPU hours. With the same pretraining steps of standard\nbase/large-sized models, COCO-LM outperforms the previous best models by 1+\nGLUE average points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_S/0/1/0/all/0/1\">Saurabh Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From partners to populations: A hierarchical Bayesian account of coordination and convention. (arXiv:2104.05857v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05857","description":"<p>Languages are powerful solutions to coordination problems: they provide\nstable, shared expectations about how the words we say correspond to the\nbeliefs and intentions in our heads. Yet language use in a variable and\nnon-stationary social environment requires linguistic representations to be\nflexible: old words acquire new ad hoc or partner-specific meanings on the fly.\nIn this paper, we introduce CHAI (Continual Hierarchical Adaptation through\nInference), a hierarchical Bayesian theory of coordination and convention\nformation that aims to reconcile the long-standing tension between these two\nbasic observations. We argue that the central computational problem of\ncommunication is not simply transmission, as in classical formulations, but\ncontinual learning and adaptation over multiple timescales. Partner-specific\ncommon ground quickly emerges from social inferences within dyadic\ninteractions, while community-wide social conventions are stable priors that\nhave been abstracted away from interactions with multiple partners. We present\nnew empirical data alongside simulations showing how our model provides a\ncomputational foundation for several phenomena that have posed a challenge for\nprevious accounts: (1) the convergence to more efficient referring expressions\nacross repeated interaction with the same partner, (2) the gradual transfer of\npartner-specific common ground to strangers, and (3) the influence of\ncommunicative context on which conventions eventually form.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_M/0/1/0/all/0/1\">Michael C. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_A/0/1/0/all/0/1\">Adele E. Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kenny Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Reordering for Modeling Latent Alignments in Sequence Transduction. (arXiv:2106.03257v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.03257","description":"<p>Despite success in many domains, neural models struggle in settings where\ntrain and test examples are drawn from different distributions. In particular,\nin contrast to humans, conventional sequence-to-sequence (seq2seq) models fail\nto generalize systematically, i.e., interpret sentences representing novel\ncombinations of concepts (e.g., text segments) seen in training. Traditional\ngrammar formalisms excel in such settings by implicitly encoding alignments\nbetween input and output segments, but are hard to scale and maintain. Instead\nof engineering a grammar, we directly model segment-to-segment alignments as\ndiscrete structured latent variables within a neural seq2seq model. To\nefficiently explore the large space of alignments, we introduce a reorder-first\nalign-later framework whose central component is a neural reordering module\nproducing {\\it separable} permutations. We present an efficient dynamic\nprogramming algorithm performing exact marginal inference of separable\npermutations, and, thus, enabling end-to-end differentiable training of our\nmodel. The resulting seq2seq model exhibits better systematic generalization\nthan standard models on synthetic problems and NLP tasks (i.e., semantic\nparsing and machine translation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kaizen: Continuously improving teacher using Exponential Moving Average for semi-supervised speech recognition. (arXiv:2106.07759v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2106.07759","description":"<p>In this paper, we introduce the Kaizen framework that uses a continuously\nimproving teacher to generate pseudo-labels for semi-supervised speech\nrecognition (ASR). The proposed approach uses a teacher model which is updated\nas the exponential moving average (EMA) of the student model parameters. We\ndemonstrate that it is critical for EMA to be accumulated with full-precision\nfloating point. The Kaizen framework can be seen as a continuous version of the\niterative pseudo-labeling approach for semi-supervised training. It is\napplicable for different training criteria, and in this paper we demonstrate\nits effectiveness for frame-level hybrid hidden Markov model-deep neural\nnetwork (HMM-DNN) systems as well as sequence-level Connectionist Temporal\nClassification (CTC) based models.\n</p>\n<p>For large scale real-world unsupervised public videos in UK English and\nItalian languages the proposed approach i) shows more than 10% relative word\nerror rate (WER) reduction over standard teacher-student training; ii) using\njust 10 hours of supervised data and a large amount of unsupervised data closes\nthe gap to the upper-bound supervised ASR system that uses 650h or 2700h\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1\">Vimal Manohar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zweig_G/0/1/0/all/0/1\">Geoffrey Zweig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BARTScore: Evaluating Generated Text as Text Generation. (arXiv:2106.11520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11520","description":"<p>A wide variety of NLP applications, such as machine translation,\nsummarization, and dialog, involve text generation. One major challenge for\nthese applications is how to evaluate whether such generated texts are actually\nfluent, accurate, or effective. In this work, we conceptualize the evaluation\nof generated text as a text generation problem, modeled using pre-trained\nsequence-to-sequence models. The general idea is that models trained to convert\nthe generated text to/from a reference output or the source text will achieve\nhigher scores when the generated text is better. We operationalize this idea\nusing BART, an encoder-decoder based pre-trained model, and propose a metric\nBARTScore with a number of variants that can be flexibly applied in an\nunsupervised fashion to evaluation of text from different perspectives (e.g.\ninformativeness, fluency, or factuality). BARTScore is conceptually simple and\nempirically effective. It can outperform existing top-scoring metrics in 16 of\n22 test settings, covering evaluation of 16 datasets (e.g., machine\ntranslation, text summarization) and 7 different perspectives (e.g.,\ninformativeness, factuality). Code to calculate BARTScore is available at\nhttps://github.com/neulab/BARTScore, and we have released an interactive\nleaderboard for meta-evaluation at\n<a href=\"http://explainaboard.nlpedia.ai/leaderboard/task-meval/\">this http URL</a> on the ExplainaBoard\nplatform, which allows us to interactively understand the strengths,\nweaknesses, and complementarity of each metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages. (arXiv:2108.07127v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07127","description":"<p>We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack. (arXiv:2109.02229v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02229","description":"<p>Over the past few years, various word-level textual attack approaches have\nbeen proposed to reveal the vulnerability of deep neural networks used in\nnatural language processing. Typically, these approaches involve an important\noptimization step to determine which substitute to be used for each word in the\noriginal input. However, current research on this step is still rather limited,\nfrom the perspectives of both problem-understanding and problem-solving. In\nthis paper, we address these issues by uncovering the theoretical properties of\nthe problem and proposing an efficient local search algorithm (LS) to solve it.\nWe establish the first provable approximation guarantee on solving the problem\nin general cases.Extensive experiments involving 5 NLP tasks, 8 datasets and 26\nNLP models show that LS can largely reduce the number of queries usually by an\norder of magnitude to achieve high attack success rates. Further experiments\nshow that the adversarial examples crafted by LS usually have higher quality,\nexhibit better transferability, and can bring more robustness improvement to\nvictim models by adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightSeq2: Accelerated Training for Transformer-based Models on GPUs. (arXiv:2110.05722v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05722","description":"<p>Transformer-based models have proven to be powerful in many natural language,\ncomputer vision, and speech recognition applications. It is expensive to train\nthese types of models due to unfixed input length, complex computation, and\nlarge numbers of parameters. Existing systems either only focus on efficient\ninference or optimize only BERT-like encoder models. In this paper, we present\nLightSeq2, a system for efficient training of Transformer-based models on GPUs.\nWe propose a series of GPU optimization techniques tailored to computation flow\nand memory access patterns of neural layers in Transformers. LightSeq2 supports\na variety of network architectures, including BERT (encoder-only), GPT\n(decoder-only), and Transformer (encoder-decoder). Our experiments on GPUs with\nvarying models and datasets show that LightSeq2 is 1.4-3.5x faster than\nprevious systems. In particular, it gains 308% training speedup compared with\nexisting systems on a large public machine translation benchmark (WMT14\nEnglish-German).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Ying Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xian Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Myelin: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.13005","description":"<p>In the last few years, the memory requirements to train state-of-the-art\nneural networks have far exceeded the DRAM capacities of modern hardware\naccelerators. This has necessitated the development of efficient algorithms to\ntrain these neural networks in parallel on large-scale GPU-based clusters.\nSince computation is relatively inexpensive on modern GPUs, designing and\nimplementing extremely efficient communication in these parallel training\nalgorithms is critical for extracting the maximum performance. This paper\npresents Myelin, a parallel deep learning framework that exploits asynchrony\nand message-driven execution to schedule neural network operations on each GPU,\nthereby reducing GPU idle time and maximizing hardware efficiency. By using the\nCPU memory as a scratch space for offloading data periodically during training,\nMyelin is able to reduce GPU memory consumption by four times. This allows us\nto increase the number of parameters per GPU by four times, thus reducing the\namount of communication and increasing performance by over 13%. When tested\nagainst large transformer models with 12-100 billion parameters on 48-384\nNVIDIA Tesla V100 GPUs, Myelin achieves a per-GPU throughput of 49.4-54.78% of\ntheoretical peak and reduces the training time by 22-37 days (15-25% speedup)\nas compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatele_A/0/1/0/all/0/1\">Abhinav Bhatele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Frequency Centric Defense Mechanisms against Adversarial Examples. (arXiv:2110.13935v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13935","description":"<p>Adversarial example (AE) aims at fooling a Convolution Neural Network by\nintroducing small perturbations in the input image.The proposed work uses the\nmagnitude and phase of the Fourier Spectrum and the entropy of the image to\ndefend against AE. We demonstrate the defense in two ways: by training an\nadversarial detector and denoising the adversarial effect. Experiments were\nconducted on the low-resolution CIFAR-10 and high-resolution ImageNet datasets.\nThe adversarial detector has 99% accuracy for FGSM and PGD attacks on the\nCIFAR-10 dataset. However, the detection accuracy falls to 50% for\nsophisticated DeepFool and Carlini &amp; Wagner attacks on ImageNet. We overcome\nthe limitation by using autoencoder and show that 70% of AEs are correctly\nclassified after denoising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sanket B. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raval_P/0/1/0/all/0/1\">Param Raval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khakhi_H/0/1/0/all/0/1\">Harin Khakhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raval_M/0/1/0/all/0/1\">Mehul S. Raval</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausalAF: Causal Autoregressive Flow for Goal-Directed Safety-Critical Scenes Generation. (arXiv:2110.13939v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13939","description":"<p>Goal-directed generation, aiming for solving downstream tasks by generating\ndiverse data, has a potentially wide range of applications in the real world.\nPrevious works tend to formulate goal-directed generation as a purely\ndata-driven problem, which directly searches or approximates the distribution\nof samples satisfying the goal. However, the generation ability of preexisting\nwork is heavily restricted by inefficient sampling, especially for sparse goals\nthat rarely show up in off-the-shelf datasets. For instance, generating\nsafety-critical traffic scenes with the goal of increasing the risk of\ncollision is critical to evaluate autonomous vehicles, but the rareness of such\nscenes is the biggest resistance. In this paper, we integrate causality as a\nprior into the safety-critical scene generation process and propose a\nflow-based generative framework - Causal Autoregressive Flow (CausalAF).\nCausalAF encourages the generative model to uncover and follow the causal\nrelationship among generated objects via novel causal masking operations\ninstead of searching the sample only from observational data. By learning the\ncause-and-effect mechanism of how the generated scene achieves the goal rather\nthan just learning correlations from data, CausalAF significantly improves the\nlearning efficiency. Extensive experiments on three heterogeneous traffic\nscenes illustrate that CausalAF requires much fewer optimization resources to\neffectively generate goal-directed scenes for safety evaluation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haohong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Uncertainty in Multi-Agent Trajectory Forecasting. (arXiv:2110.13947v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13947","description":"<p>Uncertainty modeling is critical in trajectory forecasting systems for both\ninterpretation and safety reasons. To better predict the future trajectories of\nmultiple agents, recent works have introduced interaction modules to capture\ninteractions among agents. This approach leads to correlations among the\npredicted trajectories. However, the uncertainty brought by such correlations\nis neglected. To fill this gap, we propose a novel concept, collaborative\nuncertainty(CU), which models the uncertainty resulting from the interaction\nmodule. We build a general CU-based framework to make a prediction model to\nlearn the future trajectory and the corresponding uncertainty. The CU-based\nframework is integrated as a plugin module to current state-of-the-art (SOTA)\nsystems and deployed in two special cases based on multivariate Gaussian and\nLaplace distributions. In each case, we conduct extensive experiments on two\nsynthetic datasets and two public, large-scale benchmarks of trajectory\nforecasting. The results are promising: 1) The results of synthetic datasets\nshow that CU-based framework allows the model to appropriately approximate the\nground-truth distribution. 2) The results of trajectory forecasting benchmarks\ndemonstrate that the CU-based framework steadily helps SOTA systems improve\ntheir performances. Especially, the proposed CU-based framework helps VectorNet\nimprove by 57cm regarding Final Displacement Error on nuScenes dataset. 3) The\nvisualization results of CU illustrate that the value of CU is highly related\nto the amount of the interactive information among agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Bohan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can't Fool Me: Adversarially Robust Transformer for Video Understanding. (arXiv:2110.13950v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13950","description":"<p>Deep neural networks have been shown to perform poorly on adversarial\nexamples. To address this, several techniques have been proposed to increase\nrobustness of a model for image classification tasks. However, in video\nunderstanding tasks, developing adversarially robust models is still\nunexplored. In this paper, we aim to bridge this gap. We first show that simple\nextensions of image based adversarially robust models slightly improve the\nworst-case performance. Further, we propose a temporal attention regularization\nscheme in Transformer to improve the robustness of attention modules to\nadversarial examples. We illustrate using a large-scale video data set\nYouTube-8M that the final model (A-ART) achieves close to non-adversarial\nperformance on its adversarial example set. We achieve 91% GAP on adversarial\nexamples, whereas baseline Transformer and simple adversarial extensions\nachieve 72.9% and 82% respectively, showing significant improvement in\nrobustness over the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1\">Divya Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Palash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Saurabh Sahu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based fully automatic assessment of open surgery suturing skills. (arXiv:2110.13972v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13972","description":"<p>The goal of this study was to develop new reliable open surgery suturing\nsimulation system for training medical students in situation where resources\nare limited or in the domestic setup. Namely, we developed an algorithm for\ntools and hands localization as well as identifying the interactions between\nthem based on simple webcam video data, calculating motion metrics for\nassessment of surgical skill. Twenty-five participants performed multiple\nsuturing tasks using our simulator. The YOLO network has been modified to a\nmulti-task network, for the purpose of tool localization and tool-hand\ninteraction detection. This was accomplished by splitting the YOLO detection\nheads so that they supported both tasks with minimal addition to computer\nrun-time. Furthermore, based on the outcome of the system, motion metrics were\ncalculated. These metrics included traditional metrics such as time and path\nlength as well as new metrics assessing the technique participants use for\nholding the tools. The dual-task network performance was similar to that of two\nnetworks, while computational load was only slightly bigger than one network.\nIn addition, the motion metrics showed significant differences between experts\nand novices. While video capture is an essential part of minimally invasive\nsurgery, it is not an integral component of open surgery. Thus, new algorithms,\nfocusing on the unique challenges open surgery videos present, are required. In\nthis study, a dual-task network was developed to solve both a localization task\nand a hand-tool interaction task. The dual network may be easily expanded to a\nmulti-task network, which may be useful for images with multiple layers and for\nevaluating the interaction between these different layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldbraikh_A/0/1/0/all/0/1\">Adam Goldbraikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_A/0/1/0/all/0/1\">Anne-Lise D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1\">Carla M. Pugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laufer_S/0/1/0/all/0/1\">Shlomi Laufer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHIP: CHannel Independence-based Pruning for Compact Neural Networks. (arXiv:2110.13981v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13981","description":"<p>Filter pruning has been widely used for neural network compression because of\nits enabled practical acceleration. To date, most of the existing filter\npruning works explore the importance of filters via using intra-channel\ninformation. In this paper, starting from an inter-channel perspective, we\npropose to perform efficient filter pruning using Channel Independence, a\nmetric that measures the correlations among different feature maps. The less\nindependent feature map is interpreted as containing less useful\ninformation$/$knowledge, and hence its corresponding filter can be pruned\nwithout affecting model capacity. We systematically investigate the\nquantification metric, measuring scheme and sensitiveness$/$reliability of\nchannel independence in the context of filter pruning. Our evaluation results\nfor different models on various datasets show the superior performance of our\napproach. Notably, on CIFAR-10 dataset our solution can bring $0.75\\%$ and\n$0.94\\%$ accuracy increase over baseline ResNet-56 and ResNet-110 models,\nrespectively, and meanwhile the model size and FLOPs are reduced by $42.8\\%$\nand $47.4\\%$ (for ResNet-56) and $48.3\\%$ and $52.1\\%$ (for ResNet-110),\nrespectively. On ImageNet dataset, our approach can achieve $40.8\\%$ and\n$44.8\\%$ storage and computation reductions, respectively, with $0.15\\%$\naccuracy increase over the baseline ResNet-50 model. The code is available at\nhttps://github.com/Eclipsess/CHIP_NeurIPS2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Miao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonouz_S/0/1/0/all/0/1\">Saman Zonouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Batch Normalization. (arXiv:2110.13989v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13989","description":"<p>Batch normalization (BN) is comprised of a normalization component followed\nby an affine transformation and has become essential for training deep neural\nnetworks. Standard initialization of each BN in a network sets the affine\ntransformation scale and shift to 1 and 0, respectively. However, after\ntraining we have observed that these parameters do not alter much from their\ninitialization. Furthermore, we have noticed that the normalization process can\nstill yield overly large values, which is undesirable for training. We revisit\nthe BN formulation and present a new initialization method and update approach\nfor BN to address the aforementioned issues. Experimental results using the\nproposed alterations to BN show statistically significant performance gains in\na variety of scenarios. The approach can be used with existing implementations\nat no additional computational cost. We also present a new online BN-based\ninput data normalization technique to alleviate the need for other offline or\nfixed methods. Source code is available at\nhttps://github.com/osu-cvl/revisiting-bn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jim Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_L/0/1/0/all/0/1\">Logan Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Local Temporal Information for Multimodal Scene Classification. (arXiv:2110.13992v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13992","description":"<p>Robust video scene classification models should capture the spatial\n(pixel-wise) and temporal (frame-wise) characteristics of a video effectively.\nTransformer models with self-attention which are designed to get contextualized\nrepresentations for individual tokens given a sequence of tokens, are becoming\nincreasingly popular in many computer vision tasks. However, the use of\nTransformer based models for video understanding is still relatively\nunexplored. Moreover, these models fail to exploit the strong temporal\nrelationships between the neighboring video frames to get potent frame-level\nrepresentations. In this paper, we propose a novel self-attention block that\nleverages both local and global temporal relationships between the video frames\nto obtain better contextualized representations for the individual frames. This\nenables the model to understand the video at various granularities. We\nillustrate the performance of our models on the large scale YoutTube-8M data\nset on the task of video categorization and further analyze the results to\nshowcase improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Saurabh Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Palash Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Data Augmentation Through Deep Relighting. (arXiv:2110.13996v1 [cs.CV])","link":"http://arxiv.org/abs/2110.13996","description":"<p>At the heart of the success of deep learning is the quality of the data.\nThrough data augmentation, one can train models with better generalization\ncapabilities and thus achieve greater results in their field of interest. In\nthis work, we explore how to augment a varied set of image datasets through\nrelighting so as to improve the ability of existing models to be invariant to\nillumination changes, namely for learned descriptors. We develop a tool, based\non an encoder-decoder network, that is able to quickly generate multiple\nvariations of the illumination of various input scenes whilst also allowing the\nuser to define parameters such as the angle of incidence and intensity. We\ndemonstrate that by training models on datasets that have been augmented with\nour pipeline, it is possible to achieve higher performance on localization\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chogovadze_G/0/1/0/all/0/1\">George Chogovadze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pautrat_R/0/1/0/all/0/1\">R&#xe9;mi Pautrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MisConv: Convolutional Neural Networks for Missing Data. (arXiv:2110.14010v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14010","description":"<p>Processing of missing data by modern neural networks, such as CNNs, remains a\nfundamental, yet unsolved challenge, which naturally arises in many practical\napplications, like image inpainting or autonomous vehicles and robots. While\nimputation-based techniques are still one of the most popular solutions, they\nfrequently introduce unreliable information to the data and do not take into\naccount the uncertainty of estimation, which may be destructive for a machine\nlearning model. In this paper, we present MisConv, a general mechanism, for\nadapting various CNN architectures to process incomplete images. By modeling\nthe distribution of missing values by the Mixture of Factor Analyzers, we cover\nthe spectrum of possible replacements and find an analytical formula for the\nexpected value of convolution operator applied to the incomplete image. The\nwhole framework is realized by matrix operations, which makes MisConv extremely\nefficient in practice. Experiments performed on various image processing tasks\ndemonstrate that MisConv achieves superior or comparable performance to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Przewiezlikowski_M/0/1/0/all/0/1\">Marcin Przewi&#x119;&#x17a;likowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1\">Marek &#x15a;mieja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struski_L/0/1/0/all/0/1\">&#x141;ukasz Struski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Integrated Pipeline of Segmentation Leading to Classification for Automated Detection of Breast Cancer from Breast Ultrasound Images. (arXiv:2110.14013v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14013","description":"<p>Breast cancer has become a symbol of tremendous concern in the modern world,\nas it is one of the major causes of cancer mortality worldwide. In this\nconcern, many people are frequently screening for breast cancer in order to be\nidentified early and avert mortality from the disease by receiving treatment.\nBreast Ultrasonography Images are frequently utilized by doctors to diagnose\nbreast cancer at an early stage. However, the complex artifacts and heavily\nnoised Breast Ultrasonography Images make detecting Breast Cancer a tough\nchallenge. Furthermore, the ever-increasing number of patients being screened\nfor Breast Cancer necessitates the use of automated Computer Aided Technology\nfor high accuracy diagnosis at a cheap cost and in a short period of time. The\ncurrent progress of Artificial Intelligence (AI) in the fields of Medical Image\nAnalysis and Health Care is a boon to humanity. In this study, we have proposed\na compact integrated automated pipelining framework which integrates\nultrasonography image preprocessing with Simple Linear Iterative Clustering\n(SLIC) to tackle the complex artifact of Breast Ultrasonography Images\ncomplementing semantic segmentation with Modified U-Net leading to Breast Tumor\nclassification with robust feature extraction using a transfer learning\napproach with pretrained VGG 16 model and densely connected neural network\narchitecture. The proposed automated pipeline can be effectively implemented to\nassist medical practitioners in making more accurate and timely diagnoses of\nbreast cancer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Inan_M/0/1/0/all/0/1\">Muhammad Sakib Khan Inan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alam_F/0/1/0/all/0/1\">Fahim Irfan Alam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_R/0/1/0/all/0/1\">Rizwan Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Local Effectiveness for Global robust training. (arXiv:2110.14030v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14030","description":"<p>Despite its popularity, deep neural networks are easily fooled. To alleviate\nthis deficiency, researchers are actively developing new training strategies,\nwhich encourage models that are robust to small input perturbations. Several\nsuccessful robust training methods have been proposed. However, many of them\nrely on strong adversaries, which can be prohibitively expensive to generate\nwhen the input dimension is high and the model structure is complicated. We\nadopt a new perspective on robustness and propose a novel training algorithm\nthat allows a more effective use of adversaries. Our method improves the model\nrobustness at each local ball centered around an adversary and then, by\ncombining these local balls through a global term, achieves overall robustness.\nWe demonstrate that, by maximizing the use of adversaries via focusing on local\nballs, we achieve high robust accuracy with weak adversaries. Specifically, our\nmethod reaches a similar robust accuracy level to the state of the art\napproaches trained on strong adversaries on MNIST, CIFAR-10 and CIFAR-100. As a\nresult, the overall training time is reduced. Furthermore, when trained with\nstrong adversaries, our method matches with the current state of the art on\nMNIST and outperforms them on CIFAR-10 and CIFAR-100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingyue Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge. (arXiv:2110.14032v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14032","description":"<p>Recently, a new trend of exploring sparsity for accelerating neural network\ntraining has emerged, embracing the paradigm of training on the edge. This\npaper proposes a novel Memory-Economic Sparse Training (MEST) framework\ntargeting for accurate and fast execution on edge devices. The proposed MEST\nframework consists of enhancements by Elastic Mutation (EM) and Soft Memory\nBound (&amp;S) that ensure superior accuracy at high sparsity ratios. Different\nfrom the existing works for sparse training, this current work reveals the\nimportance of sparsity schemes on the performance of sparse training in terms\nof accuracy as well as training speed on real edge devices. On top of that, the\npaper proposes to employ data efficiency for further acceleration of sparse\ntraining. Our results suggest that unforgettable examples can be identified\nin-situ even during the dynamic exploration of sparsity masks in the sparse\ntraining process, and therefore can be removed for further training speedup on\nedge devices. Comparing with state-of-the-art (SOTA) works on accuracy, our\nMEST increases Top-1 accuracy significantly on ImageNet when using the same\nunstructured sparsity scheme. Systematical evaluation on accuracy, training\nspeed, and memory footprint are conducted, where the proposed MEST framework\nconsistently outperforms representative SOTA works. A reviewer strongly against\nour work based on his false assumptions and misunderstandings. On top of the\nprevious submission, we employ data efficiency for further acceleration of\nsparse training. And we explore the impact of model sparsity, sparsity schemes,\nand sparse training algorithms on the number of removable training examples.\nOur codes are publicly available at: https://github.com/boone891214/MEST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenglun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zheng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14051","description":"<p>Machine learning models often encounter samples that are diverged from the\ntraining distribution. Failure to recognize an out-of-distribution (OOD)\nsample, and consequently assign that sample to an in-class label significantly\ncompromises the reliability of a model. The problem has gained significant\nattention due to its importance for safety deploying models in open-world\nsettings. Detecting OOD samples is challenging due to the intractability of\nmodeling all possible unknown distributions. To date, several research domains\ntackle the problem of detecting unfamiliar samples, including anomaly\ndetection, novelty detection, one-class learning, open set recognition, and\nout-of-distribution detection. Despite having similar and shared concepts,\nout-of-distribution, open-set, and anomaly detection have been investigated\nindependently. Accordingly, these research avenues have not cross-pollinated,\ncreating research barriers. While some surveys intend to provide an overview of\nthese approaches, they seem to only focus on a specific domain without\nexamining the relationship between different domains. This survey aims to\nprovide a cross-domain and comprehensive review of numerous eminent works in\nrespective areas while identifying their commonalities. Researchers can benefit\nfrom the overview of research advances in different fields and develop future\nmethodology synergistically. Furthermore, to the best of our knowledge, while\nthere are surveys in anomaly detection or one-class learning, there is no\ncomprehensive or up-to-date survey on out-of-distribution detection, which our\nsurvey covers extensively. Finally, having a unified cross-domain perspective,\nwe discuss and shed light on future lines of research, intending to bring these\nfields closer together.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_H/0/1/0/all/0/1\">Hossein Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoFiNet: Reliable Coarse-to-fine Correspondences for Robust Point Cloud Registration. (arXiv:2110.14076v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14076","description":"<p>We study the problem of extracting correspondences between a pair of point\nclouds for registration. For correspondence retrieval, existing works benefit\nfrom matching sparse keypoints detected from dense points but usually struggle\nto guarantee their repeatability. To address this issue, we present CoFiNet -\nCoarse-to-Fine Network which extracts hierarchical correspondences from coarse\nto fine without keypoint detection. On a coarse scale and guided by a weighting\nscheme, our model firstly learns to match down-sampled nodes whose vicinity\npoints share more overlap, which significantly shrinks the search space of a\nconsecutive stage. On a finer scale, node proposals are consecutively expanded\nto patches that consist of groups of points together with associated\ndescriptors. Point correspondences are then refined from the overlap areas of\ncorresponding patches, by a density-adaptive matching module capable to deal\nwith varying point density. Extensive evaluation of CoFiNet on both indoor and\noutdoor standard benchmarks shows our superiority over existing methods.\nEspecially on 3DLoMatch where point clouds share less overlap, CoFiNet\nsignificantly outperforms state-of-the-art approaches by at least 5% on\nRegistration Recall, with at most two-third of their parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1\">Mahdi Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Slobodan Ilic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Bisimulation Metric Learning. (arXiv:2110.14096v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14096","description":"<p>Learned representations in deep reinforcement learning (DRL) have to extract\ntask-relevant information from complex observations, balancing between\nrobustness to distraction and informativeness to the policy. Such stable and\nrich representations, often learned via modern function approximation\ntechniques, can enable practical application of the policy improvement theorem,\neven in high-dimensional continuous state-action spaces. Bisimulation metrics\noffer one solution to this representation learning problem, by collapsing\nfunctionally similar states together in representation space, which promotes\ninvariance to noise and distractors. In this work, we generalize value function\napproximation bounds for on-policy bisimulation metrics to non-optimal policies\nand approximate environment dynamics. Our theoretical results help us identify\nembedding pathologies that may occur in practical use. In particular, we find\nthat these issues stem from an underconstrained dynamics model and an unstable\ndependence of the embedding norm on the reward signal in environments with\nsparse rewards. Further, we propose a set of practical remedies: (i) a norm\nconstraint on the representation space, and (ii) an extension of prior\napproaches with intrinsic rewards and latent space regularization. Finally, we\nprovide evidence that the resulting method is not only more robust to sparse\nreward functions, but also able to solve challenging continuous control tasks\nwith observational distractions, where prior methods fail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kemertas_M/0/1/0/all/0/1\">Mete Kemertas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aumentado_Armstrong_T/0/1/0/all/0/1\">Tristan Aumentado-Armstrong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers. (arXiv:2110.14120v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14120","description":"<p>Adversarial patch attacks that craft the pixels in a confined region of the\ninput images show their powerful attack effectiveness in physical environments\neven with noises or deformations. Existing certified defenses towards\nadversarial patch attacks work well on small images like MNIST and CIFAR-10\ndatasets, but achieve very poor certified accuracy on higher-resolution images\nlike ImageNet. It is urgent to design both robust and effective defenses\nagainst such a practical and harmful attack in industry-level larger images. In\nthis work, we propose the certified defense methodology that achieves high\nprovable robustness for high-resolution images and largely improves the\npracticality for real adoption of the certified defense. The basic insight of\nour work is that the adversarial patch intends to leverage localized\nsuperficial important neurons (SIN) to manipulate the prediction results.\nHence, we leverage the SIN-based DNN compression techniques to significantly\nimprove the certified accuracy, by reducing the adversarial region searching\noverhead and filtering the prediction noises. Our experimental results show\nthat the certified accuracy is increased from 36.3% (the state-of-the-art\ncertified detection) to 60.4% on the ImageNet dataset, largely pushing the\ncertified defenses for practical use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Husheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaobing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Ling Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zidong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation. (arXiv:2110.14143v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14143","description":"<p>Natural language instructions for visual navigation often use scene\ndescriptions (e.g., \"bedroom\") and object references (e.g., \"green chairs\") to\nprovide a breadcrumb trail to a goal location. This work presents a\ntransformer-based vision-and-language navigation (VLN) agent that uses two\ndifferent visual encoders -- a scene classification network and an object\ndetector -- which produce features that match these two distinct types of\nvisual cues. In our method, scene features contribute high-level contextual\ninformation that supports object-level processing. With this design, our model\nis able to use vision-and-language pretraining (i.e., learning the alignment\nbetween images and text from large-scale web data) to substantially improve\nperformance on the Room-to-Room (R2R) and Room-Across-Room (RxR) benchmarks.\nSpecifically, our approach leads to improvements of 1.8% absolute in SPL on R2R\nand 3.7% absolute in SR on RxR. Our analysis reveals even larger gains for\nnavigation instructions that contain six or more object references, which\nfurther suggests that our approach is better able to use object features and\nalign them to references in the instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moudgil_A/0/1/0/all/0/1\">Abhinav Moudgil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Arjun Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_H/0/1/0/all/0/1\">Harsh Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physically Explainable CNN for SAR Image Classification. (arXiv:2110.14144v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14144","description":"<p>Integrating the special electromagnetic characteristics of Synthetic Aperture\nRadar (SAR) in deep neural networks is essential in order to enhance the\nexplainability and physics awareness of deep learning. In this paper, we\nfirstly propose a novel physics guided and injected neural network for SAR\nimage classification, which is mainly guided by explainable physics models and\ncan be learned with very limited labeled data. The proposed framework comprises\nthree parts: (1) generating physics guided signals using existing explainable\nmodels, (2) learning physics-aware features with physics guided network, and\n(3) injecting the physics-aware features adaptively to the conventional\nclassification deep learning model for prediction. The prior knowledge,\nphysical scattering characteristic of SAR in this paper, is injected into the\ndeep neural network in the form of physics-aware features which is more\nconducive to understanding the semantic labels of SAR image patches. A hybrid\nImage-Physics SAR dataset format is proposed, and both Sentinel-1 and Gaofen-3\nSAR data are taken for evaluation. The experimental results show that our\nproposed method substantially improve the classification performance compared\nwith the counterpart data-driven CNN. Moreover, the guidance of explainable\nphysics signals leads to explainability of physics-aware features and the\nphysics consistency of features are also preserved in the predictions. We deem\nthe proposed method would promote the development of physically explainable\ndeep learning in SAR image interpretation field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongling Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_X/0/1/0/all/0/1\">Xiwen Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dumitru_C/0/1/0/all/0/1\">Corneliu Octavian Dumitru</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Datcu_M/0/1/0/all/0/1\">Mihai Datcu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Comes Dancing with Collaborative Parsing-Flow Video Synthesis. (arXiv:2110.14147v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14147","description":"<p>Transferring human motion from a source to a target person poses great\npotential in computer vision and graphics applications. A crucial step is to\nmanipulate sequential future motion while retaining the appearance\ncharacteristic.Previous work has either relied on crafted 3D human models or\ntrained a separate model specifically for each target person, which is not\nscalable in practice.This work studies a more general setting, in which we aim\nto learn a \\emph{single} model to parsimoniously transfer motion from a source\nvideo to any target person given only one image of the person, named as\nCollaborative Parsing-Flow Network (CPF-Net). The paucity of information\nregarding the target person makes the task particularly challenging to\nfaithfully preserve the appearance in varying designated poses.To address this\nissue, CPF-Net integrates the structured human parsing and appearance flow to\nguide the realistic foreground synthesis which is merged into the background by\na spatio-temporal fusion module.In particular, CPF-Net decouples the problem\ninto stages of human parsing sequence generation, foreground sequence\ngeneration and final video generation. The human parsing generation stage\ncaptures both the pose and the body structure of the target. The appearance\nflow is beneficial to keep details in synthesized frames. The integration of\nhuman parsing and appearance flow effectively guides the generation of video\nframes with realistic appearance. Finally, the dedicated designed fusion\nnetwork ensure the temporal coherence. We further collect a large set of human\ndancing videos to push forward this research field. Both quantitative and\nqualitative results show our method substantially improves over previous\napproaches and is able to generate appealing and photo-realistic target videos\ngiven any input person image. All source code and dataset will be released at\nhttps://github.com/xiezhy6/CPF-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bowen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yubei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoye Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Wasserstein GANs without gradient penalties. (arXiv:2110.14150v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14150","description":"<p>We propose a stable method to train Wasserstein generative adversarial\nnetworks. In order to enhance stability, we consider two objective functions\nusing the $c$-transform based on Kantorovich duality which arises in the theory\nof optimal transport. We experimentally show that this algorithm can\neffectively enforce the Lipschitz constraint on the discriminator while other\nstandard methods fail to do so. As a consequence, our method yields an accurate\nestimation for the optimal discriminator and also for the Wasserstein distance\nbetween the true distribution and the generated one. Our method requires no\ngradient penalties nor corresponding hyperparameter tuning and is\ncomputationally more efficient than other methods. At the same time, it yields\ncompetitive generators of synthetic images based on the MNIST, F-MNIST, and\nCIFAR-10 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1\">Dohyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeoneung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1\">Guido Mont&#xfa;far</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_I/0/1/0/all/0/1\">Insoon Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying the key components in ResNet-50 for diabetic retinopathy grading from fundus images: a systematic investigation. (arXiv:2110.14160v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14160","description":"<p>Although deep learning based diabetic retinopathy (DR) classification methods\ntypically benefit from well-designed architectures of convolutional neural\nnetworks, the training setting also has a non-negligible impact on the\nprediction performance. The training setting includes various interdependent\ncomponents, such as objective function, data sampling strategy and data\naugmentation approach. To identify the key components in a standard deep\nlearning framework (ResNet-50) for DR grading, we systematically analyze the\nimpact of several major components. Extensive experiments are conducted on a\npublicly-available dataset EyePACS. We demonstrate that (1) the ResNet-50\nframework for DR grading is sensitive to input resolution, objective function,\nand composition of data augmentation, (2) using mean square error as the loss\nfunction can effectively improve the performance with respect to a\ntask-specific evaluation metric, namely the quadratically-weighted Kappa, (3)\nutilizing eye pairs boosts the performance of DR grading and (4) using data\nresampling to address the problem of imbalanced data distribution in EyePACS\nhurts the performance. Based on these observations and an optimal combination\nof the investigated components, our framework, without any specialized network\ndesign, achieves the state-of-the-art result (0.8631 for Kappa) on the EyePACS\ntest set (a total of 42670 fundus images) with only image-level labels. Our\ncodes and pre-trained model are available at\nhttps://github.com/YijinHuang/pytorch-classification\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yijin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Junyan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QU-net++: Image Quality Detection Framework for Segmentation of 3D Medical Image Stacks. (arXiv:2110.14181v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14181","description":"<p>Automated segmentation of pathological regions of interest has been shown to\naid prognosis and follow up treatment. However, accurate pathological\nsegmentations require high quality of annotated data that can be both cost and\ntime intensive to generate. In this work, we propose an automated two-step\nmethod that evaluates the quality of medical images from 3D image stacks using\na U-net++ model, such that images that can aid further training of the U-net++\nmodel can be detected based on the disagreement in segmentations produced from\nthe final two layers. Images thus detected can then be used to further fine\ntune the U-net++ model for semantic segmentation. The proposed QU-net++ model\nisolates around 10\\% of images per 3D stack and can scale across imaging\nmodalities to segment cysts in OCT images and ground glass opacity in Lung CT\nimages with Dice cores in the range 0.56-0.72. Thus, the proposed method can be\napplied for multi-modal binary segmentation of pathology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sohini Roychowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidential Softmax for Sparse Multimodal Distributions in Deep Generative Models. (arXiv:2110.14182v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14182","description":"<p>Many applications of generative models rely on the marginalization of their\nhigh-dimensional output probability distributions. Normalization functions that\nyield sparse probability distributions can make exact marginalization more\ncomputationally tractable. However, sparse normalization functions usually\nrequire alternative loss functions for training since the log-likelihood is\nundefined for sparse probability distributions. Furthermore, many sparse\nnormalization functions often collapse the multimodality of distributions. In\nthis work, we present $\\textit{ev-softmax}$, a sparse normalization function\nthat preserves the multimodality of probability distributions. We derive its\nproperties, including its gradient in closed-form, and introduce a continuous\nfamily of approximations to $\\textit{ev-softmax}$ that have full support and\ncan be trained with probabilistic loss functions such as negative\nlog-likelihood and Kullback-Leibler divergence. We evaluate our method on a\nvariety of generative models, including variational autoencoders and\nauto-regressive architectures. Our method outperforms existing dense and sparse\nnormalization techniques in distributional accuracy. We demonstrate that\n$\\textit{ev-softmax}$ successfully reduces the dimensionality of probability\ndistributions while maintaining multimodality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Phil Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Contrastive Learning Using Negative Samples with Diminished Semantics. (arXiv:2110.14189v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14189","description":"<p>Unsupervised learning has recently made exceptional progress because of the\ndevelopment of more effective contrastive learning methods. However, CNNs are\nprone to depend on low-level features that humans deem non-semantic. This\ndependency has been conjectured to induce a lack of robustness to image\nperturbations or domain shift. In this paper, we show that by generating\ncarefully designed negative samples, contrastive learning can learn more robust\nrepresentations with less dependence on such features. Contrastive learning\nutilizes positive pairs that preserve semantic information while perturbing\nsuperficial features in the training images. Similarly, we propose to generate\nnegative samples in a reversed way, where only the superfluous instead of the\nsemantic features are preserved. We develop two methods, texture-based and\npatch-based augmentations, to generate negative samples. These samples achieve\nbetter generalization, especially under out-of-domain settings. We also analyze\nour method and the generated texture-based samples, showing that texture\nfeatures are indispensable in classifying particular ImageNet classes and\nespecially finer classes. We also show that model bias favors texture and shape\nfeatures differently under different test settings. Our code, trained models,\nand ImageNet-Texture dataset can be found at\nhttps://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shlok Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Supervised Object Detection by Transferring Mask Prior and Semantic Similarity. (arXiv:2110.14191v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14191","description":"<p>Object detection has achieved promising success, but requires large-scale\nfully-annotated data, which is time-consuming and labor-extensive. Therefore,\nwe consider object detection with mixed supervision, which learns novel object\ncategories using weak annotations with the help of full annotations of existing\nbase object categories. Previous works using mixed supervision mainly learn the\nclass-agnostic objectness from fully-annotated categories, which can be\ntransferred to upgrade the weak annotations to pseudo full annotations for\nnovel categories. In this paper, we further transfer mask prior and semantic\nsimilarity to bridge the gap between novel categories and base categories.\nSpecifically, the ability of using mask prior to help detect objects is learned\nfrom base categories and transferred to novel categories. Moreover, the\nsemantic similarity between objects learned from base categories is transferred\nto denoise the pseudo full annotations for novel categories. Experimental\nresults on three benchmark datasets demonstrate the effectiveness of our method\nover existing methods. Codes are available at\nhttps://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smooth head tracking for virtual reality applications. (arXiv:2110.14193v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14193","description":"<p>In this work, we propose a new head-tracking solution for human-machine\nreal-time interaction with virtual 3D environments. This solution leverages\nRGBD data to compute virtual camera pose according to the movements of the\nuser's head. The process starts with the extraction of a set of facial features\nfrom the images delivered by the sensor. Such features are matched against\ntheir respective counterparts in a reference image for the computation of the\ncurrent head pose. Afterwards, a prediction approach is used to guess the most\nlikely next head move (final pose). Pythagorean Hodograph interpolation is then\nadapted to determine the path and local frames taken between the two poses. The\nresult is a smooth head trajectory that serves as an input to set the camera in\nvirtual scenes according to the user's gaze. The resulting motion model has the\nadvantage of being: continuous in time, it adapts to any frame rate of\nrendering; it is ergonomic, as it frees the user from wearing tracking markers;\nit is smooth and free from rendering jerks; and it is also torsion and\ncurvature minimizing as it produces a path with minimum bending energy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amamra_A/0/1/0/all/0/1\">Abdenour Amamra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Image to Imuge: Immunized Image Generation. (arXiv:2110.14196v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14196","description":"<p>We introduce Imuge, an image tamper resilient generative scheme for image\nself-recovery. The traditional manner of concealing image content within the\nimage are inflexible and fragile to diverse digital attack, i.e. image cropping\nand JPEG compression. To address this issue, we jointly train a U-Net backboned\nencoder, a tamper localization network and a decoder for image recovery. Given\nan original image, the encoder produces a visually indistinguishable immunized\nimage. At the recipient's side, the verifying network localizes the malicious\nmodifications, and the original content can be approximately recovered by the\ndecoder, despite the presence of the attacks. Several strategies are proposed\nto boost the training efficiency. We demonstrate that our method can recover\nthe details of the tampered regions with a high quality despite the presence of\nvarious kinds of attacks. Comprehensive ablation studies are conducted to\nvalidate our network designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haisheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoised Non-Local Neural Network for Semantic Segmentation. (arXiv:2110.14200v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14200","description":"<p>The non-local network has become a widely used technique for semantic\nsegmentation, which computes an attention map to measure the relationships of\neach pixel pair. However, most of the current popular non-local models tend to\nignore the phenomenon that the calculated attention map appears to be very\nnoisy, containing inter-class and intra-class inconsistencies, which lowers the\naccuracy and reliability of the non-local methods. In this paper, we\nfiguratively denote these inconsistencies as attention noises and explore the\nsolutions to denoise them. Specifically, we inventively propose a Denoised\nNon-Local Network (Denoised NL), which consists of two primary modules, i.e.,\nthe Global Rectifying (GR) block and the Local Retention (LR) block, to\neliminate the inter-class and intra-class noises respectively. First, GR adopts\nthe class-level predictions to capture a binary map to distinguish whether the\nselected two pixels belong to the same category. Second, LR captures the\nignored local dependencies and further uses them to rectify the unwanted\nhollows in the attention map. The experimental results on two challenging\nsemantic segmentation datasets demonstrate the superior performance of our\nmodel. Without any external training data, our proposed Denoised NL can achieve\nthe state-of-the-art performance of 83.5\\% and 46.69\\% mIoU on Cityscapes and\nADE20K, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisit Multimodal Meta-Learning through the Lens of Multi-Task Learning. (arXiv:2110.14202v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14202","description":"<p>Multimodal meta-learning is a recent problem that extends conventional\nfew-shot meta-learning by generalizing its setup to diverse multimodal task\ndistributions. This setup makes a step towards mimicking how humans make use of\na diverse set of prior skills to learn new skills. Previous work has achieved\nencouraging performance. In particular, in spite of the diversity of the\nmultimodal tasks, previous work claims that a single meta-learner trained on a\nmultimodal distribution can sometimes outperform multiple specialized\nmeta-learners trained on individual unimodal distributions. The improvement is\nattributed to knowledge transfer between different modes of task distributions.\nHowever, there is no deep investigation to verify and understand the knowledge\ntransfer between multimodal tasks. Our work makes two contributions to\nmultimodal meta-learning. First, we propose a method to quantify knowledge\ntransfer between tasks of different modes at a micro-level. Our quantitative,\ntask-level analysis is inspired by the recent transference idea from multi-task\nlearning. Second, inspired by hard parameter sharing in multi-task learning and\na new interpretation of related work, we propose a new multimodal meta-learner\nthat outperforms existing work by considerable margins. While the major focus\nis on multimodal meta-learning, our work also attempts to shed light on task\ninteraction in conventional meta-learning. The code for this project is\navailable at https://miladabd.github.io/KML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdollahzadeh_M/0/1/0/all/0/1\">Milad Abdollahzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malekzadeh_T/0/1/0/all/0/1\">Touba Malekzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1\">Ngai-Man Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural View Synthesis and Matching for Semi-Supervised Few-Shot Learning of 3D Pose. (arXiv:2110.14213v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14213","description":"<p>We study the problem of learning to estimate the 3D object pose from a few\nlabelled examples and a collection of unlabelled data. Our main contribution is\na learning framework, neural view synthesis and matching, that can transfer the\n3D pose annotation from the labelled to unlabelled images reliably, despite\nunseen 3D views and nuisance variations such as the object shape, texture,\nillumination or scene context. In our approach, objects are represented as 3D\ncuboid meshes composed of feature vectors at each mesh vertex. The model is\ninitialized from a few labelled images and is subsequently used to synthesize\nfeature representations of unseen 3D views. The synthesized views are matched\nwith the feature representations of unlabelled images to generate pseudo-labels\nof the 3D pose. The pseudo-labelled data is, in turn, used to train the feature\nextractor such that the features at each mesh vertex are more invariant across\nvarying 3D views of the object. Our model is trained in an EM-type manner\nalternating between increasing the 3D pose invariance of the feature extractor\nand annotating unlabelled data through neural view synthesis and matching. We\ndemonstrate the effectiveness of the proposed semi-supervised learning\nframework for 3D pose estimation on the PASCAL3D+ and KITTI datasets. We find\nthat our approach outperforms all baselines by a wide margin, particularly in\nan extreme few-shot setting where only 7 annotated images are given.\nRemarkably, we observe that our model also achieves an exceptional robustness\nin out-of-distribution scenarios that involve partial occlusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Angtian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Shenxiao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Classification: Knowledge Distillation using Multi-Object Impressions. (arXiv:2110.14215v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14215","description":"<p>Knowledge Distillation (KD) utilizes training data as a transfer set to\ntransfer knowledge from a complex network (Teacher) to a smaller network\n(Student). Several works have recently identified many scenarios where the\ntraining data may not be available due to data privacy or sensitivity concerns\nand have proposed solutions under this restrictive constraint for the\nclassification task. Unlike existing works, we, for the first time, solve a\nmuch more challenging problem, i.e., \"KD for object detection with zero\nknowledge about the training data and its statistics\". Our proposed approach\nprepares pseudo-targets and synthesizes corresponding samples (termed as\n\"Multi-Object Impressions\"), using only the pretrained Faster RCNN Teacher\nnetwork. We use this pseudo-dataset as a transfer set to conduct zero-shot KD\nfor object detection. We demonstrate the efficacy of our proposed method\nthrough several ablations and extensive experiments on benchmark datasets like\nKITTI, Pascal and COCO. Our approach with no training samples, achieves a\nrespectable mAP of 64.2% and 55.5% on the student with same and half capacity\nwhile performing distillation from a Resnet-18 Teacher of 73.3% mAP on KITTI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keswani_M/0/1/0/all/0/1\">Monish Keswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_S/0/1/0/all/0/1\">Sharan Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects. (arXiv:2110.14217v1 [cs.RO])","link":"http://arxiv.org/abs/2110.14217","description":"<p>The ability to grasp and manipulate transparent objects is a major challenge\nfor robots. Existing depth cameras have difficulty detecting, localizing, and\ninferring the geometry of such objects. We propose using neural radiance fields\n(NeRF) to detect, localize, and infer the geometry of transparent objects with\nsufficient accuracy to find and grasp them securely. We leverage NeRF's\nview-independent learned density, place lights to increase specular\nreflections, and perform a transparency-aware depth-rendering that we feed into\nthe Dex-Net grasp planner. We show how additional lights create specular\nreflections that improve the quality of the depth map, and test a setup for a\nrobot workcell equipped with an array of cameras to perform transparent object\nmanipulation. We also create synthetic and real datasets of transparent objects\nin real-world settings, including singulated objects, cluttered tables, and the\ntop rack of a dishwasher. In each setting we show that NeRF and Dex-Net are\nable to reliably compute robust grasps on transparent objects, achieving 90%\nand 100% grasp success rates in physical experiments on an ABB YuMi, on objects\nwhere baseline methods fail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1\">Jeffrey Ichnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avigal_Y/0/1/0/all/0/1\">Yahav Avigal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerr_J/0/1/0/all/0/1\">Justin Kerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RRNet: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2110.14223v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14223","description":"<p>Salient object detection (SOD) for optical remote sensing images (RSIs) aims\nat locating and extracting visually distinctive objects/regions from the\noptical RSIs. Despite some saliency models were proposed to solve the intrinsic\nproblem of optical RSIs (such as complex background and scale-variant objects),\nthe accuracy and completeness are still unsatisfactory. To this end, we propose\na relational reasoning network with parallel multi-scale attention for SOD in\noptical RSIs in this paper. The relational reasoning module that integrates the\nspatial and the channel dimensions is designed to infer the semantic\nrelationship by utilizing high-level encoder features, thereby promoting the\ngeneration of more complete detection results. The parallel multi-scale\nattention module is proposed to effectively restore the detail information and\naddress the scale variation of salient objects by using the low-level features\nrefined by multi-scale attention. Extensive experiments on two datasets\ndemonstrate that our proposed RRNet outperforms the existing state-of-the-art\nSOD competitors both qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1\">Runmin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yumo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Leyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2nd Place Solution for VisDA 2021 Challenge -- Universally Domain Adaptive Image Recognition. (arXiv:2110.14240v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14240","description":"<p>The Visual Domain Adaptation (VisDA) 2021 Challenge calls for unsupervised\ndomain adaptation (UDA) methods that can deal with both input distribution\nshift and label set variance between the source and target domains. In this\nreport, we introduce a universal domain adaptation (UniDA) method by\naggregating several popular feature extraction and domain adaptation schemes.\nFirst, we utilize VOLO, a Transformer-based architecture with state-of-the-art\nperformance in several visual tasks, as the backbone to extract effective\nfeature representations. Second, we modify the open-set classifier of OVANet to\nrecognize the unknown class with competitive accuracy and robustness. As shown\nin the leaderboard, our proposed UniDA method ranks the 2nd place with 48.56%\nACC and 70.72% AUROC in the VisDA 2021 Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haojin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaolin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yueming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_T/0/1/0/all/0/1\">Tengfei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilayer Lookahead: a Nested Version of Lookahead. (arXiv:2110.14254v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14254","description":"<p>In recent years, SGD and its variants have become the standard tool to train\nDeep Neural Networks. In this paper, we focus on the recently proposed variant\nLookahead, which improves upon SGD in a wide range of applications. Following\nthis success, we study an extension of this algorithm, the \\emph{Multilayer\nLookahead} optimizer, which recursively wraps Lookahead around itself. We prove\nthe convergence of Multilayer Lookahead with two layers to a stationary point\nof smooth non-convex functions with $O(\\frac{1}{\\sqrt{T}})$ rate. We also\njustify the improved generalization of both Lookahead over SGD, and of\nMultilayer Lookahead over Lookahead, by showing how they amplify the implicit\nregularization effect of SGD. We empirically verify our results and show that\nMultilayer Lookahead outperforms Lookahead on CIFAR-10 and CIFAR-100\nclassification tasks, and on GANs training on the MNIST dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pushkin_D/0/1/0/all/0/1\">Denys Pushkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barba_L/0/1/0/all/0/1\">Luis Barba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Important is Importance Sampling for Deep Budgeted Training?. (arXiv:2110.14283v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14283","description":"<p>Long iterative training processes for Deep Neural Networks (DNNs) are\ncommonly required to achieve state-of-the-art performance in many computer\nvision tasks. Importance sampling approaches might play a key role in budgeted\ntraining regimes, i.e. when limiting the number of training iterations. These\napproaches aim at dynamically estimating the importance of each sample to focus\non the most relevant and speed up convergence. This work explores this paradigm\nand how a budget constraint interacts with importance sampling approaches and\ndata augmentation techniques. We show that under budget restrictions,\nimportance sampling approaches do not provide a consistent improvement over\nuniform sampling. We suggest that, given a specific budget, the best course of\naction is to disregard the importance and introduce adequate data augmentation;\ne.g. when reducing the budget to a 30% in CIFAR-10/100, RICAP data augmentation\nmaintains accuracy, while importance sampling does not. We conclude from our\nwork that DNNs under budget restrictions benefit greatly from variety in the\ntraining set and that finding the right samples to train on is not the most\neffective strategy when balancing high performance with low computational\nrequirements. Source code available at https://git.io/JKHa3 .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arazo_E/0/1/0/all/0/1\">Eric Arazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortego_D/0/1/0/all/0/1\">Diego Ortego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albert_P/0/1/0/all/0/1\">Paul Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Sanity Checks for Saliency Maps. (arXiv:2110.14297v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14297","description":"<p>Saliency methods are a popular approach for model debugging and\nexplainability. However, in the absence of ground-truth data for what the\ncorrect maps should be, evaluating and comparing different approaches remains a\nlong-standing challenge. The sanity checks methodology of Adebayo et al\n[Neurips 2018] has sought to address this challenge. They argue that some\npopular saliency methods should not be used for explainability purposes since\nthe maps they produce are not sensitive to the underlying model that is to be\nexplained. Through a causal re-framing of their objective, we argue that their\nempirical evaluation does not fully establish these conclusions, due to a form\nof confounding introduced by the tasks they evaluate on. Through various\nexperiments on simple custom tasks we demonstrate that some of their\nconclusions may indeed be artifacts of the tasks more than a criticism of the\nsaliency methods themselves. More broadly, our work challenges the utility of\nthe sanity check methodology, and further highlights that saliency map\nevaluation beyond ad-hoc visual examination remains a fundamental challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yona_G/0/1/0/all/0/1\">Gal Yona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenfeld_D/0/1/0/all/0/1\">Daniel Greenfeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring the Class Conditional Response Map for Weakly Supervised Semantic Segmentation. (arXiv:2110.14309v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14309","description":"<p>Image-level weakly supervised semantic segmentation (WSSS) relies on class\nactivation maps (CAMs) for pseudo labels generation. As CAMs only highlight the\nmost discriminative regions of objects, the generated pseudo labels are usually\nunsatisfactory to serve directly as supervision. To solve this, most existing\napproaches follow a multi-training pipeline to refine CAMs for better\npseudo-labels, which includes: 1) re-training the classification model to\ngenerate CAMs; 2) post-processing CAMs to obtain pseudo labels; and 3) training\na semantic segmentation model with the obtained pseudo labels. However, this\nmulti-training pipeline requires complicated adjustment and additional time. To\naddress this, we propose a class-conditional inference strategy and an\nactivation aware mask refinement loss function to generate better pseudo labels\nwithout re-training the classifier. The class conditional inference-time\napproach is presented to separately and iteratively reveal the classification\nnetwork's hidden object activation to generate more complete response maps.\nFurther, our activation aware mask refinement loss function introduces a novel\nway to exploit saliency maps during segmentation training and refine the\nforeground object masks without suppressing background objects. Our method\nachieves superior WSSS results without requiring re-training of the classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weixuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-frequency image completion via a biologically-inspired sub-Riemannian model with frequency and phase. (arXiv:2110.14330v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14330","description":"<p>We present a novel cortically-inspired image completion algorithm. It uses a\nfive dimensional sub-Riemannian cortical geometry modelling the orientation,\nspatial frequency and phase selective behavior of the cells in the visual\ncortex. The algorithm extracts the orientation, frequency and phase information\nexisting in a given two dimensional corrupted input image via a Gabor transform\nand represent those values in terms of cortical cell output responses in the\nmodel geometry. Then it performs completion via a diffusion concentrated in a\nneighbourhood along the neural connections within the model geometry. The\ndiffusion models the activity propagation integrating orientation, frequency\nand phase features along the neural connections. Finally, the algorithm\ntransforms back the diffused and completed output responses back to the two\ndimensional image plane.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baspinar_E/0/1/0/all/0/1\">Emre Baspinar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature and Label Embedding Spaces Matter in Addressing Image Classifier Bias. (arXiv:2110.14336v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14336","description":"<p>This paper strives to address image classifier bias, with a focus on both\nfeature and label embedding spaces. Previous works have shown that spurious\ncorrelations from protected attributes, such as age, gender, or skin tone, can\ncause adverse decisions. To balance potential harms, there is a growing need to\nidentify and mitigate image classifier bias. First, we identify in the feature\nspace a bias direction. We compute class prototypes of each protected attribute\nvalue for every class, and reveal an existing subspace that captures the\nmaximum variance of the bias. Second, we mitigate biases by mapping image\ninputs to label embedding spaces. Each value of the protected attribute has its\nprojection head where classes are embedded through a latent vector\nrepresentation rather than a common one-hot encoding. Once trained, we further\nreduce in the feature space the bias effect by removing its direction.\nEvaluation on biased image datasets, for multi-class, multi-label and binary\nclassifications, shows the effectiveness of tackling both feature and label\nembedding spaces in improving the fairness of the classifier predictions, while\npreserving classification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thong_W/0/1/0/all/0/1\">William Thong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamLessMonoDepth: Monocular Depth Estimation with Unknown Camera Parameters. (arXiv:2110.14347v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14347","description":"<p>Perceiving 3D information is of paramount importance in many applications of\ncomputer vision. Recent advances in monocular depth estimation have shown that\ngaining such knowledge from a single camera input is possible by training deep\nneural networks to predict inverse depth and pose, without the necessity of\nground truth data. The majority of such approaches, however, require camera\nparameters to be fed explicitly during training. As a result, image sequences\nfrom wild cannot be used during training. While there exist methods which also\npredict camera intrinsics, their performance is not on par with novel methods\ntaking camera parameters as input. In this work, we propose a method for\nimplicit estimation of pinhole camera intrinsics along with depth and pose, by\nlearning from monocular image sequences alone. In addition, by utilizing\nefficient sub-pixel convolutions, we show that high fidelity depth estimates\ncan be obtained. We also embed pixel-wise uncertainty estimation into the\nframework, to emphasize the possible applicability of this work in practical\ndomain. Finally, we demonstrate the possibility of accurate prediction of depth\ninformation without prior knowledge of camera intrinsics, while outperforming\nthe existing state-of-the-art approaches on KITTI benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chanduri_S/0/1/0/all/0/1\">Sai Shyam Chanduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_Z/0/1/0/all/0/1\">Zeeshan Khan Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vozniak_I/0/1/0/all/0/1\">Igor Vozniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_C/0/1/0/all/0/1\">Christian M&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConAM: Confidence Attention Module for Convolutional Neural Networks. (arXiv:2110.14369v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14369","description":"<p>The so-called ``attention'' is an efficient mechanism to improve the\nperformance of convolutional neural networks. It uses contextual information to\nrecalibrate the input to strengthen the propagation of informative features.\nHowever, the majority of the attention mechanisms only consider either local or\nglobal contextual information, which is singular to extract features. Moreover,\nmany existing mechanisms directly use the contextual information to recalibrate\nthe input, which unilaterally enhances the propagation of the informative\nfeatures, but does not suppress the useless ones. This paper proposes a new\nattention mechanism module based on the correlation between local and global\ncontextual information and we name this correlation as confidence. The novel\nattention mechanism extracts the local and global contextual information\nsimultaneously, and calculates the confidence between them, then uses this\nconfidence to recalibrate the input pixels. The extraction of local and global\ncontextual information increases the diversity of features. The recalibration\nwith confidence suppresses useless information while enhancing the informative\none with fewer parameters. We use CIFAR-10 and CIFAR-100 in our experiments and\nexplore the performance of our method's components by sufficient ablation\nstudies. Finally, we compare our method with a various state-of-the-art\nconvolutional neural networks and the results show that our method completely\nsurpasses these models. We implement ConAM with the Python library, Pytorch,\nand the code and models will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Ziming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neri_F/0/1/0/all/0/1\">Ferrante Neri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition. (arXiv:2110.14373v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14373","description":"<p>Decomposing a scene into its shape, reflectance and illumination is a\nfundamental problem in computer vision and graphics. Neural approaches such as\nNeRF have achieved remarkable success in view synthesis, but do not explicitly\nperform decomposition and instead operate exclusively on radiance (the product\nof reflectance and illumination). Extensions to NeRF, such as NeRD, can perform\ndecomposition but struggle to accurately recover detailed illumination, thereby\nsignificantly limiting realism. We propose a novel reflectance decomposition\nnetwork that can estimate shape, BRDF, and per-image illumination given a set\nof object images captured under varying illumination. Our key technique is a\nnovel illumination integration network called Neural-PIL that replaces a costly\nillumination integral operation in the rendering with a simple network query.\nIn addition, we also learn deep low-dimensional priors on BRDF and illumination\nrepresentations using novel smooth manifold auto-encoders. Our decompositions\ncan result in considerably better BRDF and light estimates enabling more\naccurate novel view-synthesis and relighting compared to prior art. Project\npage: https://markboss.me/publication/2021-neural-pil/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boss_M/0/1/0/all/0/1\">Mark Boss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_R/0/1/0/all/0/1\">Raphael Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P.A. Lensch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Score: What Data Modalities Does Your Model Perceive?. (arXiv:2110.14375v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14375","description":"<p>Machine learning advances in the last decade have relied significantly on\nlarge-scale datasets that continue to grow in size. Increasingly, those\ndatasets also contain different data modalities. However, large multi-modal\ndatasets are hard to annotate, and annotations may contain biases that we are\noften unaware of. Deep-net-based classifiers, in turn, are prone to exploit\nthose biases and to find shortcuts. To study and quantify this concern, we\nintroduce the perceptual score, a metric that assesses the degree to which a\nmodel relies on the different subsets of the input features, i.e., modalities.\nUsing the perceptual score, we find a surprisingly consistent trend across four\npopular datasets: recent, more accurate state-of-the-art multi-modal models for\nvisual question-answering or visual dialog tend to perceive the visual data\nless than their predecessors. This trend is concerning as answers are hence\nincreasingly inferred from textual cues only. Using the perceptual score also\nhelps to analyze model biases by decomposing the score into data subset\ncontributions. We hope to spur a discussion on the perceptiveness of\nmulti-modal models and also hope to encourage the community working on\nmulti-modal classifiers to start quantifying perceptiveness via the proposed\nperceptual score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1\">Itai Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal-attentive Covariance Pooling Networks for Video Recognition. (arXiv:2110.14381v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14381","description":"<p>For video recognition task, a global representation summarizing the whole\ncontents of the video snippets plays an important role for the final\nperformance. However, existing video architectures usually generate it by using\na simple, global average pooling (GAP) method, which has limited ability to\ncapture complex dynamics of videos. For image recognition task, there exist\nevidences showing that covariance pooling has stronger representation ability\nthan GAP. Unfortunately, such plain covariance pooling used in image\nrecognition is an orderless representative, which cannot model spatio-temporal\nstructure inherent in videos. Therefore, this paper proposes a\nTemporal-attentive Covariance Pooling(TCP), inserted at the end of deep\narchitectures, to produce powerful video representations. Specifically, our TCP\nfirst develops a temporal attention module to adaptively calibrate\nspatio-temporal features for the succeeding covariance pooling, approximatively\nproducing attentive covariance representations. Then, a temporal covariance\npooling performs temporal pooling of the attentive covariance representations\nto characterize both intra-frame correlations and inter-frame\ncross-correlations of the calibrated features. As such, the proposed TCP can\ncapture complex temporal dynamics. Finally, a fast matrix power normalization\nis introduced to exploit geometry of covariance representations. Note that our\nTCP is model-agnostic and can be flexibly integrated into any video\narchitectures, resulting in TCPNet for effective video recognition. The\nextensive experiments on six benchmarks using various video architectures show\nour TCPNet is clearly superior to its counterparts, while having strong\ngeneralization\nability.$\\href{https://github.com/ZilinGao/Temporal-attentive-Covariance-Pooling-Networks-for-Video-Recognition}{\\textit{The\nsource code is publicly available.}}$\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zilin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bingbing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peihua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Traffic Forecasting on Traffic Moving Snippets. (arXiv:2110.14383v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14383","description":"<p>Advances in traffic forecasting technology can greatly impact urban mobility.\nIn the traffic4cast competition, the task of short-term traffic prediction is\ntackled in unprecedented detail, with traffic volume and speed information\navailable at 5 minute intervals and high spatial resolution. To improve\ngeneralization to unknown cities, as required in the 2021 extended challenge,\nwe propose to predict small quadratic city sections, rather than processing a\nfull-city-raster at once. At test time, breaking down the test data into\nspatially-cropped overlapping snippets improves stability and robustness of the\nfinal predictions, since multiple patches covering one cell can be processed\nindependently. With the performance on the traffic4cast test data and further\nexperiments on a validation set it is shown that patch-wise prediction indeed\nimproves accuracy. Further advantages can be gained with a Unet++ architecture\nand with an increasing number of patches per sample processed at test time. We\nconclude that our snippet-based method, combined with other successful network\narchitectures proposed in the competition, can leverage performance, in\nparticular on unseen cities. All source code is available at\nhttps://github.com/NinaWie/NeurIPS2021-traffic4cast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiedemann_N/0/1/0/all/0/1\">Nina Wiedemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raubal_M/0/1/0/all/0/1\">Martin Raubal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taylor Swift: Taylor Driven Temporal Modeling for Swift Future Frame Prediction. (arXiv:2110.14392v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14392","description":"<p>While recurrent neural networks (RNNs) demonstrate outstanding capabilities\nin future video frame prediction, they model dynamics in a discrete time space\nand sequentially go through all frames until the desired future temporal step\nis reached. RNNs are therefore prone to accumulate the error as the number of\nfuture frames increases. In contrast, partial differential equations (PDEs)\nmodel physical phenomena like dynamics in continuous time space, however,\ncurrent PDE-based approaches discretize the PDEs using e.g., the forward Euler\nmethod. In this work, we therefore propose to approximate the motion in a video\nby a continuous function using the Taylor series. To this end, we introduce\nTayloSwiftNet, a novel convolutional neural network that learns to estimate the\nhigher order terms of the Taylor series for a given input video. TayloSwiftNet\ncan swiftly predict any desired future frame in just one forward pass and\nchange the temporal resolution on-the-fly. The experimental results on various\ndatasets demonstrate the superiority of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourheydari_M/0/1/0/all/0/1\">Mohammad Saber Pourheydari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrami_E/0/1/0/all/0/1\">Emad Bahrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noroozi_M/0/1/0/all/0/1\">Mehdi Noroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separating Content and Style for Unsupervised Image-to-Image Translation. (arXiv:2110.14404v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14404","description":"<p>Unsupervised image-to-image translation aims to learn the mapping between two\nvisual domains with unpaired samples. Existing works focus on disentangling\ndomain-invariant content code and domain-specific style code individually for\nmultimodal purposes. However, less attention has been paid to interpreting and\nmanipulating the translated image. In this paper, we propose to separate the\ncontent code and style code simultaneously in a unified framework. Based on the\ncorrelation between the latent features and the high-level domain-invariant\ntasks, the proposed framework demonstrates superior performance in multimodal\ntranslation, interpretability and manipulation of the translated image.\nExperimental results show that the proposed approach outperforms the existing\nunsupervised image translation methods in terms of visual quality and\ndiversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Super Resolution for Foreground Images using U-Net and MR-CNN. (arXiv:2110.14413v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14413","description":"<p>Images play a vital role in understanding data through visual representation.\nIt gives a clear representation of the object in context. But if this image is\nnot clear it might not be of much use. Thus, the topic of Image Super\nResolution arose and many researchers have been working towards applying\nComputer Vision and Deep Learning Techniques to increase the quality of images.\nOne of the applications of Super Resolution is to increase the quality of\nPortrait Images. Portrait Images are images which mainly focus on capturing the\nessence of the main object in the frame, where the object in context is\nhighlighted whereas the background is occluded. When performing Super\nResolution the model tries to increase the overall resolution of the image. But\nin portrait images the foreground resolution is more important than that of the\nbackground. In this paper, the performance of a Convolutional Neural Network\n(CNN) architecture known as U-Net for Super Resolution combined with Mask\nRegion Based CNN (MR-CNN) for foreground super resolution is analysed. This\nanalysis is carried out based on Localized Super Resolution i.e. We pass the LR\nImages to a pre-trained Image Segmentation model (MR-CNN) and perform super\nresolution inference on the foreground or Segmented Images and compute the\nStructural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR)\nmetrics for comparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumaravelan_U/0/1/0/all/0/1\">Umashankar Kumaravelan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_N/0/1/0/all/0/1\">Nivedita M</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Neuron Pruning Purifies Backdoored Deep Models. (arXiv:2110.14430v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14430","description":"<p>As deep neural networks (DNNs) are growing larger, their requirements for\ncomputational resources become huge, which makes outsourcing training more\npopular. Training in a third-party platform, however, may introduce potential\nrisks that a malicious trainer will return backdoored DNNs, which behave\nnormally on clean samples but output targeted misclassifications whenever a\ntrigger appears at the test time. Without any knowledge of the trigger, it is\ndifficult to distinguish or recover benign DNNs from backdoored ones. In this\npaper, we first identify an unexpected sensitivity of backdoored DNNs, that is,\nthey are much easier to collapse and tend to predict the target label on clean\nsamples when their neurons are adversarially perturbed. Based on these\nobservations, we propose a novel model repairing method, termed Adversarial\nNeuron Pruning (ANP), which prunes some sensitive neurons to purify the\ninjected backdoor. Experiments show, even with only an extremely small amount\nof clean data (e.g., 1%), ANP effectively removes the injected backdoor without\ncausing obvious performance degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongxian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Teaching by Label Synthesis. (arXiv:2110.14432v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14432","description":"<p>In this paper, we consider the problem of iterative machine teaching, where a\nteacher provides examples sequentially based on the current iterative learner.\nIn contrast to previous methods that have to scan over the entire pool and\nselect teaching examples from it in each iteration, we propose a label\nsynthesis teaching framework where the teacher randomly selects input teaching\nexamples (e.g., images) and then synthesizes suitable outputs (e.g., labels)\nfor them. We show that this framework can avoid costly example selection while\nstill provably achieving exponential teachability. We propose multiple novel\nteaching algorithms in this framework. Finally, we empirically demonstrate the\nvalue of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1\">Liam Paull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Discriminator in GAN Compression: A Generator-discriminator Cooperative Compression Scheme. (arXiv:2110.14439v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14439","description":"<p>Recently, a series of algorithms have been explored for GAN compression,\nwhich aims to reduce tremendous computational overhead and memory usages when\ndeploying GANs on resource-constrained edge devices. However, most of the\nexisting GAN compression work only focuses on how to compress the generator,\nwhile fails to take the discriminator into account. In this work, we revisit\nthe role of discriminator in GAN compression and design a novel\ngenerator-discriminator cooperative compression scheme for GAN compression,\ntermed GCC. Within GCC, a selective activation discriminator automatically\nselects and activates convolutional channels according to a local capacity\nconstraint and a global coordination constraint, which help maintain the Nash\nequilibrium with the lightweight generator during the adversarial training and\navoid mode collapse. The original generator and discriminator are also\noptimized from scratch, to play as a teacher model to progressively refine the\npruned generator and the selective activation discriminator. A novel online\ncollaborative distillation scheme is designed to take full advantage of the\nintermediate feature of the teacher generator and discriminator to further\nboost the performance of the lightweight generator. Extensive experiments on\nvarious GAN-based generation tasks demonstrate the effectiveness and\ngeneralization of GCC. Among them, GCC contributes to reducing 80%\ncomputational costs while maintains comparable performance in image translation\ntasks. Our code and models are available at \\url{https://github.com/SJLeo/GCC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">ShaoJie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xudong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBIR using Pre-Trained Neural Networks. (arXiv:2110.14455v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14455","description":"<p>Much of the recent research work in image retrieval, has been focused around\nusing Neural Networks as the core component. Many of the papers in other domain\nhave shown that training multiple models, and then combining their outcomes,\nprovide good results. This is since, a single Neural Network model, may not\nextract sufficient information from the input. In this paper, we aim to follow\na different approach. Instead of the using a single model, we use a pretrained\nInception V3 model, and extract activation of its last fully connected layer,\nwhich forms a low dimensional representation of the image. This feature matrix,\nis then divided into branches and separate feature extraction is done for each\nbranch, to obtain multiple features flattened into a vector. Such individual\nvectors are then combined, to get a single combined feature. We make use of\nCUB200-2011 Dataset, which comprises of 200 birds classes to train the model\non. We achieved a training accuracy of 99.46% and validation accuracy of 84.56%\nfor the same. On further use of 3 branched global descriptors, we improve the\nvalidation accuracy to 88.89%. For this, we made use of MS-RMAC feature\nextraction method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alappat_A/0/1/0/all/0/1\">Agnel Lazar Alappat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakhate_P/0/1/0/all/0/1\">Prajwal Nakhate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suman_S/0/1/0/all/0/1\">Sagar Suman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandurkar_A/0/1/0/all/0/1\">Ambarish Chandurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimpalkhute_V/0/1/0/all/0/1\">Varad Pimpalkhute</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tapan Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand gesture detection in the hand movement test for the early diagnosis of dementia. (arXiv:2110.14461v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14461","description":"<p>Collecting hands data is important for many cognitive studies, especially for\nsenior participants who has no IT background. For example, alternating hand\nmovements and imitation of gestures are formal cognitive assessment in the\nearly detection of dementia. During data collection process, one of the key\nsteps is to detect whether the participants is following the instruction\ncorrectly to do the correct gestures. Meanwhile, re-searchers found a lot of\nproblems in TAS Test hand movement data collection process, where is\nchallenging to detect similar gestures and guarantee the quality of the\ncollect-ed images. We have implemented a hand gesture detector to detect the\ngestures per-formed in the hand movement tests, which enables us to monitor if\nthe participants are following the instructions correctly. In this research, we\nhave processed 20,000 images collected from TAS Test and labelled 6,450 images\nto detect different hand poses in the hand movement tests. This paper has the\nfollowing three contributions. Firstly, we compared the performance of\ndifferent network structures for hand poses detection. Secondly, we introduced\na transformer block in the state of art network and increased the\nclassification performance of the similar gestures. Thirdly, we have created\ntwo datasets and included 20 percent of blurred images in the dataset to\ninvestigate how different network structures were impacted by noisy data, then\nwe proposed a novel net-work to increase the detection accuracy to mediate the\ninfluence of the noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son N. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Quan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alty_J/0/1/0/all/0/1\">Jane Alty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Arbitrary Scale Super-Resolution Approach for 3-Dimensional Magnetic Resonance Image using Implicit Neural Representation. (arXiv:2110.14476v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14476","description":"<p>High Resolution (HR) medical images provide rich anatomical structure details\nto facilitate early and accurate diagnosis. In MRI, restricted by hardware\ncapacity, scan time, and patient cooperation ability, isotropic 3D HR image\nacquisition typically requests long scan time and, results in small spatial\ncoverage and low SNR. Recent studies showed that, with deep convolutional\nneural networks, isotropic HR MR images could be recovered from low-resolution\n(LR) input via single image super-resolution (SISR) algorithms. However, most\nexisting SISR methods tend to approach a scale-specific projection between LR\nand HR images, thus these methods can only deal with a fixed up-sampling rate.\nFor achieving different up-sampling rates, multiple SR networks have to be\nbuilt up respectively, which is very time-consuming and resource-intensive. In\nthis paper, we propose ArSSR, an Arbitrary Scale Super-Resolution approach for\nrecovering 3D HR MR images. In the ArSSR model, the reconstruction of HR images\nwith different up-scaling rates is defined as learning a continuous implicit\nvoxel function from the observed LR images. Then the SR task is converted to\nrepresent the implicit voxel function via deep neural networks from a set of\npaired HR-LR training examples. The ArSSR model consists of an encoder network\nand a decoder network. Specifically, the convolutional encoder network is to\nextract feature maps from the LR input images and the fully-connected decoder\nnetwork is to approximate the implicit voxel function. Due to the continuity of\nthe learned function, a single ArSSR model can achieve arbitrary up-sampling\nrate reconstruction of HR images from any input LR image after training.\nExperimental results on three datasets show that the ArSSR model can achieve\nstate-of-the-art SR performance for 3D HR MR image reconstruction while using a\nsingle trained model to achieve arbitrary up-sampling scales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qing Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yawen Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1\">Hongjiang Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PL-Net: Progressive Learning Network for Medical Image Segmentation. (arXiv:2110.14484v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14484","description":"<p>In recent years, segmentation methods based on deep convolutional neural\nnetworks (CNNs) have made state-of-the-art achievements for many medical\nanalysis tasks. However, most of these approaches improve performance by\noptimizing the structure or adding new functional modules of the U-Net, which\nignoring the complementation and fusion of the coarse-grained and fine-grained\nsemantic information. To solve the above problems, we propose a medical image\nsegmentation framework called progressive learning network (PL-Net), which\nincludes internal progressive learning (IPL) and external progressive learning\n(EPL). PL-Net has the following advantages: (1) IPL divides feature extraction\ninto two \"steps\", which can mix different size receptive fields and capture\nsemantic information from coarse to fine granularity without introducing\nadditional parameters; (2) EPL divides the training process into two \"stages\"\nto optimize parameters, and realizes the fusion of coarse-grained information\nin the previous stage and fine-grained information in the latter stage. We\nevaluate our method in different medical image analysis tasks, and the results\nshow that the segmentation performance of PL-Net is better than the\nstate-of-the-art methods of U-Net and its variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Junlong Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1\">Chengrui Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chaoqing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ming_Z/0/1/0/all/0/1\">Zhangqiang Ming</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_M/0/1/0/all/0/1\">Min Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Lightweight CNNs for Human-Nanodrone Proximity Interaction from Small Datasets using Background Randomization. (arXiv:2110.14491v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14491","description":"<p>We consider the task of visually estimating the pose of a human from images\nacquired by a nearby nano-drone; in this context, we propose a data\naugmentation approach based on synthetic background substitution to learn a\nlightweight CNN model from a small real-world training set. Experimental\nresults on data from two different labs proves that the approach improves\ngeneralization to unseen environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferri_M/0/1/0/all/0/1\">Marco Ferri</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mantegazza_D/0/1/0/all/0/1\">Dario Mantegazza</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Cereda_E/0/1/0/all/0/1\">Elia Cereda</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zimmerman_N/0/1/0/all/0/1\">Nicky Zimmerman</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Gambardella_L/0/1/0/all/0/1\">Luca M. Gambardella</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Palossi_D/0/1/0/all/0/1\">Daniele Palossi</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Guzzi_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Guzzi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Giusti_A/0/1/0/all/0/1\">Alessandro Giusti</a> (1) ((1) Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland, (2) University of Bonn, (3) Integrated Systems Laboratory (IIS), ETH Z&#xfc;rich, Switzerland)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenURL: A General Framework for Unsupervised Representation Learning. (arXiv:2110.14553v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14553","description":"<p>Recently unsupervised representation learning (URL) has achieved remarkable\nprogress in various scenarios. However, most methods are specifically designed\nbased on specific data characters or task assumptions. Based on the manifold\nassumption, we regard most URL problems as an embedding problem that seeks an\noptimal low-dimensional representation of the given high-dimensional data. We\nsplit the embedding process into two steps, data structural modeling and\nlow-dimensional embedding, and propose a general similarity-based framework\ncalled GenURL. Specifically, we provide a general method to model data\nstructures by adaptively combining graph distances on the feature space and\npredefined graphs, then propose robust loss functions to learn the\nlow-dimensional embedding. Combining with a specific pretext task, we can adapt\nGenURL to various URL tasks in a unified manner and achieve state-of-the-art\nperformance, including self-supervised visual representation learning,\nunsupervised knowledge distillation, graph embeddings, and dimension reduction.\nMoreover, ablation studies of loss functions and basic hyper-parameter settings\nin GenURL illustrate the data characters of various tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Geometric Perspective towards Neural Calibration via Sensitivity Decomposition. (arXiv:2110.14577v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14577","description":"<p>It is well known that vision classification models suffer from poor\ncalibration in the face of data distribution shifts. In this paper, we take a\ngeometric approach to this problem. We propose Geometric Sensitivity\nDecomposition (GSD) which decomposes the norm of a sample feature embedding and\nthe angular similarity to a target classifier into an instance-dependent and an\ninstance-independent component. The instance-dependent component captures the\nsensitive information about changes in the input while the instance-independent\ncomponent represents the insensitive information serving solely to minimize the\nloss on the training dataset. Inspired by the decomposition, we analytically\nderive a simple extension to current softmax-linear models, which learns to\ndisentangle the two components during training. On several common vision\nmodels, the disentangled model outperforms other calibration methods on\nstandard calibration metrics in the face of out-of-distribution (OOD) data and\ncorruption with significantly less complexity. Specifically, we surpass the\ncurrent state of the art by 30.8% relative improvement on corrupted CIFAR100 in\nExpected Calibration Error. Code available at\nhttps://github.com/GT-RIPL/Geometric-Sensitivity-Decomposition.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yung_D/0/1/0/all/0/1\">Dylan Yung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary Guided Context Aggregation for Semantic Segmentation. (arXiv:2110.14587v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14587","description":"<p>The recent studies on semantic segmentation are starting to notice the\nsignificance of the boundary information, where most approaches see boundaries\nas the supplement of semantic details. However, simply combing boundaries and\nthe mainstream features cannot ensure a holistic improvement of semantics\nmodeling. In contrast to the previous studies, we exploit boundary as a\nsignificant guidance for context aggregation to promote the overall semantic\nunderstanding of an image. To this end, we propose a Boundary guided Context\nAggregation Network (BCANet), where a Multi-Scale Boundary extractor (MSB)\nborrowing the backbone features at multiple scales is specifically designed for\naccurate boundary detection. Based on which, a Boundary guided Context\nAggregation module (BCA) improved from Non-local network is further proposed to\ncapture long-range dependencies between the pixels in the boundary regions and\nthe ones inside the objects. By aggregating the context information along the\nboundaries, the inner pixels of the same category achieve mutual gains and\ntherefore the intra-class consistency is enhanced. We conduct extensive\nexperiments on the Cityscapes and ADE20K databases, and comparable results are\nachieved with the state-of-the-art methods, clearly demonstrating the\neffectiveness of the proposed one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TMBuD: A dataset for urban scene building detection. (arXiv:2110.14590v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14590","description":"<p>Building recognition and 3D reconstruction of human made structures in urban\nscenarios has become an interesting and actual topic in the image processing\ndomain. For this research topic the Computer Vision and Augmented Reality areas\nintersect for creating a better understanding of the urban scenario for various\ntopics. In this paper we aim to introduce a dataset solution, the TMBuD, that\nis better fitted for image processing on human made structures for urban scene\nscenarios. The proposed dataset will allow proper evaluation of salient edges\nand semantic segmentation of images focusing on the street view perspective of\nbuildings. The images that form our dataset offer various street view\nperspectives of buildings from urban scenarios, which allows for evaluating\ncomplex algorithms. The dataset features 160 images of buildings from\nTimisoara, Romania, with a resolution of 768 x 1024 pixels each.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciprian_O/0/1/0/all/0/1\">Orhei Ciprian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silviu_V/0/1/0/all/0/1\">Vert Silviu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muguras_M/0/1/0/all/0/1\">Mocofan Muguras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radu_V/0/1/0/all/0/1\">Vasiu Radu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TA-Net: Topology-Aware Network for Gland Segmentation. (arXiv:2110.14593v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14593","description":"<p>Gland segmentation is a critical step to quantitatively assess the morphology\nof glands in histopathology image analysis. However, it is challenging to\nseparate densely clustered glands accurately. Existing deep learning-based\napproaches attempted to use contour-based techniques to alleviate this issue\nbut only achieved limited success. To address this challenge, we propose a\nnovel topology-aware network (TA-Net) to accurately separate densely clustered\nand severely deformed glands. The proposed TA-Net has a multitask learning\narchitecture and enhances the generalization of gland segmentation by learning\nshared representation from two tasks: instance segmentation and gland topology\nestimation. The proposed topology loss computes gland topology using gland\nskeletons and markers. It drives the network to generate segmentation results\nthat comply with the true gland topology. We validate the proposed approach on\nthe GlaS and CRAG datasets using three quantitative metrics, F1-score,\nobject-level Dice coefficient, and object-level Hausdorff distance. Extensive\nexperiments demonstrate that TA-Net achieves state-of-the-art performance on\nthe two datasets. TA-Net outperforms other approaches in the presence of\ndensely clustered glands.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vakanski_A/0/1/0/all/0/1\">Aleksandar Vakanski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"International Workshop on Continual Semi-Supervised Learning: Introduction, Benchmarks and Baselines. (arXiv:2110.14613v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14613","description":"<p>The aim of this paper is to formalize a new continual semi-supervised\nlearning (CSSL) paradigm, proposed to the attention of the machine learning\ncommunity via the IJCAI 2021 International Workshop on Continual\nSemi-Supervised Learning (CSSL-IJCAI), with the aim of raising field awareness\nabout this problem and mobilizing its effort in this direction. After a formal\ndefinition of continual semi-supervised learning and the appropriate training\nand testing protocols, the paper introduces two new benchmarks specifically\ndesigned to assess CSSL on two important computer vision tasks: activity\nrecognition and crowd counting. We describe the Continual Activity Recognition\n(CAR) and Continual Crowd Counting (CCC) challenges built upon those\nbenchmarks, the baseline models proposed for the challenges, and describe a\nsimple CSSL baseline which consists in applying batch self-training in temporal\nsessions, for a limited number of rounds. The results show that learning from\nunlabelled data streams is extremely challenging, and stimulate the search for\nmethods that can encode the dynamics of the data stream.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahbaz_A/0/1/0/all/0/1\">Ajmal Shahbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Mohammad Asiful Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannons_K/0/1/0/all/0/1\">Kevin Cannons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1\">Fabio Cuzzolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Edge Detection with Diverse Deep Supervision. (arXiv:1804.02864v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1804.02864","description":"<p>Semantic edge detection (SED), which aims at jointly extracting edges as well\nas their category information, has far-reaching applications in domains such as\nsemantic segmentation, object proposal generation, and object recognition. SED\nnaturally requires achieving two distinct supervision targets: locating fine\ndetailed edges and identifying high-level semantics. Our motivation comes from\nthe hypothesis that such distinct targets prevent state-of-the-art SED methods\nfrom effectively using deep supervision to improve results. To this end, we\npropose a novel fully convolutional neural network using diverse deep\nsupervision (DDS) within a multi-task framework where bottom layers aim at\ngenerating category-agnostic edges, while top layers are responsible for the\ndetection of category-aware semantic edges. To overcome the hypothesized\nsupervision challenge, a novel information converter unit is introduced, whose\neffectiveness has been extensively evaluated on SBD and Cityscapes datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">JiaWang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense. (arXiv:1908.02962v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1908.02962","description":"<p>Alternatively inferring on the visual facts and commonsense is fundamental\nfor an advanced VQA system. This ability requires models to go beyond the\nliteral understanding of commonsense. The system should not just treat objects\nas the entrance to query background knowledge, but fully ground commonsense to\nthe visual world and imagine the possible relationships between objects, e.g.,\n\"fork, can lift, food\". To comprehensively evaluate such abilities, we propose\na VQA benchmark, CRIC, which introduces new types of questions about\nCompositional Reasoning on vIsion and Commonsense, and an evaluation metric\nintegrating the correctness of answering and commonsense grounding. To collect\nsuch questions and rich additional annotations to support the metric, we also\npropose an automatic algorithm to generate question samples from the scene\ngraph associated with the images and the relevant knowledge graph. We further\nanalyze several representative types of VQA models on the CRIC dataset.\nExperimental results show that grounding the commonsense to the image region\nand joint reasoning on vision and commonsense are still challenging for current\napproaches. The dataset is available at https://cricvqa.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Compositions of Transformations in Contrastive Self-Supervised Learning. (arXiv:2003.04298v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.04298","description":"<p>In the image domain, excellent representations can be learned by inducing\ninvariance to content-preserving transformations via noise contrastive\nlearning. In this paper, we generalize contrastive learning to a wider set of\ntransformations, and their compositions, for which either invariance or\ndistinctiveness is sought. We show that it is not immediately obvious how\nexisting methods such as SimCLR can be extended to do so. Instead, we introduce\na number of formal requirements that all contrastive formulations must satisfy,\nand propose a practical construction which satisfies these requirements. In\norder to maximise the reach of this analysis, we express all components of\nnoise contrastive formulations as the choice of certain generalized\ntransformations of the data (GDTs), including data sampling. We then consider\nvideos as an example of data in which a large variety of transformations are\napplicable, accounting for the extra modalities -- for which we analyze audio\nand text -- and the dimension of time. We find that being invariant to certain\ntransformations and distinctive to others is critical to learning effective\nvideo representations, improving the state-of-the-art for multiple benchmarks\nby a large margin, and even surpassing supervised pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patrick_M/0/1/0/all/0/1\">Mandela Patrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsova_P/0/1/0/all/0/1\">Polina Kuznetsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1\">Ruth Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jo&#xe3;o F. Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zweig_G/0/1/0/all/0/1\">Geoffrey Zweig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction. (arXiv:2008.01645v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2008.01645","description":"<p>Data-driven problem solving in many real-world applications involves analysis\nof time-dependent multivariate data, for which dimensionality reduction (DR)\nmethods are often used to uncover the intrinsic structure and features of the\ndata. However, DR is usually applied to a subset of data that is either\nsingle-time-point multivariate or univariate time-series, resulting in the need\nto manually examine and correlate the DR results out of different data subsets.\nWhen the number of dimensions is large either in terms of the number of time\npoints or attributes, this manual task becomes too tedious and infeasible. In\nthis paper, we present MulTiDR, a new DR framework that enables processing of\ntime-dependent multivariate data as a whole to provide a comprehensive overview\nof the data. With the framework, we employ DR in two steps. When treating the\ninstances, time points, and attributes of the data as a 3D array, the first DR\nstep reduces the three axes of the array to two, and the second DR step\nvisualizes the data in a lower-dimensional space. In addition, by coupling with\na contrastive learning method and interactive visualizations, our framework\nenhances analysts' ability to interpret DR results. We demonstrate the\neffectiveness of our framework with four case studies using real-world\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fujiwara_T/0/1/0/all/0/1\">Takanori Fujiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shilpika/0/1/0/all/0/1\">Shilpika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakamoto_N/0/1/0/all/0/1\">Naohisa Sakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nonaka_J/0/1/0/all/0/1\">Jorji Nonaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1\">Keiji Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kwan-Liu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.16188","description":"<p>Extracting accurate foregrounds from natural images benefits many downstream\napplications such as film production and augmented reality. However, the furry\ncharacteristics and various appearance of the foregrounds, e.g., animal and\nportrait, challenge existing matting methods, which usually require extra user\ninputs such as trimap or scribbles. To resolve these problems, we study the\ndistinct roles of semantics and details for image matting and decompose the\ntask into two parallel sub-tasks: high-level semantic segmentation and\nlow-level details matting. Specifically, we propose a novel Glance and Focus\nMatting network (GFM), which employs a shared encoder and two separate decoders\nto learn both tasks in a collaborative manner for end-to-end natural image\nmatting. Besides, due to the limitation of available natural images in the\nmatting task, previous methods typically adopt composite images for training\nand evaluation, which result in limited generalization ability on real-world\nimages. In this paper, we investigate the domain gap issue between composite\nimages and real-world images systematically by conducting comprehensive\nanalyses of various discrepancies between the foreground and background images.\nWe find that a carefully designed composition route RSSN that aims to reduce\nthe discrepancies can lead to a better model with remarkable generalization\nability. Furthermore, we provide a benchmark containing 2,000 high-resolution\nreal-world animal images and 10,000 portrait images along with their manually\nlabeled alpha mattes to serve as a test bed for evaluating matting model's\ngeneralization ability on real-world images. Comprehensive empirical studies\nhave demonstrated that GFM outperforms state-of-the-art methods and effectively\nreduces the generalization error. The code and the datasets will be released at\nhttps://github.com/JizhiziLi/GFM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1\">Stephen J. Maybank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strict Enforcement of Conservation Laws and Invertibility in CNN-Based Super Resolution for Scientific Datasets. (arXiv:2011.05586v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.05586","description":"<p>Recently, deep Convolutional Neural Networks (CNNs) have revolutionized image\nsuper-resolution (SR), dramatically outperforming past methods for enhancing\nimage resolution. They could be a boon for the many scientific fields that\ninvolve image or gridded datasets: satellite remote sensing, radar meteorology,\nmedical imaging, numerical modeling etc. Unfortunately, while SR-CNNs produce\nvisually compelling outputs, they may break physical conservation laws when\napplied to scientific datasets. Here, a method for ``Downsampling Enforcement\"\nin SR-CNNs is proposed. A differentiable operator is derived that, when applied\nas the final transfer function of a CNN, ensures the high resolution outputs\nexactly reproduce the low resolution inputs under 2D-average downsampling while\nimproving performance of the SR schemes. The method is demonstrated across\nseven modern CNN-based SR schemes on several benchmark image datasets, and\napplications to weather radar, satellite imager, and climate model data are\nalso shown. The approach improves training time and performance while ensuring\nphysical consistency between the super-resolved and low resolution data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Geiss_A/0/1/0/all/0/1\">Andrew Geiss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hardin_J/0/1/0/all/0/1\">Joseph C. Hardin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v3 [physics.geo-ph] UPDATED)","link":"http://arxiv.org/abs/2011.10510","description":"<p>Deep neural networks (DNNs) can learn accurately from large quantities of\nlabeled input data, but often fail to do so when labelled data are scarce. DNNs\nsometimes fail to generalize ontest data sampled from different input\ndistributions. Unsupervised Deep Domain Adaptation (DDA)techniques have been\nproven useful when no labels are available, and when distribution shifts are\nobserved in the target domain (TD). In the present study, experiments are\nperformed on seismic images of the F3 block 3D dataset from offshore\nNetherlands (source domain; SD) and Penobscot 3D survey data from Canada\n(target domain; TD). Three geological classes from SD and TD that have similar\nreflection patterns are considered. A deep neural network architecture named\nEarthAdaptNet (EAN) is proposed to semantically segment the seismic images when\nfew classes have data scarcity, and we use a transposed residual unit to\nreplace the traditional dilated convolution in the decoder block. The EAN\nachieved a pixel-level accuracy &gt;84% and an accuracy of ~70% for the minority\nclasses, showing improved performance compared to existing architectures. In\naddition, we introduce the CORAL (Correlation Alignment) method to the EAN to\ncreate an unsupervised deep domain adaptation network (EAN-DDA) for the\nclassification of seismic reflections from F3 and Penobscot, to demonstrate\npossible approaches when labelled data are unavailable. Maximum class accuracy\nachieved was ~99% for class 2 of Penobscot, with an overall accuracy&gt;50%. Taken\ntogether, the EAN-DDA has the potential to classify target domain seismic\nfacies classes with high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Nasim_M/0/1/0/all/0/1\">M Quamer Nasim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maiti_T/0/1/0/all/0/1\">Tannistha Maiti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Srivastava_A/0/1/0/all/0/1\">Ayush Srivastava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_T/0/1/0/all/0/1\">Tarry Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Success and Simplicity: A Second Look at Transferable Targeted Attacks. (arXiv:2012.11207v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.11207","description":"<p>Achieving transferability of targeted attacks is reputed to be remarkably\ndifficult. Currently, state-of-the-art approaches are resource-intensive\nbecause they necessitate training model(s) for each target class with\nadditional data. In our investigation, we find, however, that simple\ntransferable attacks which require neither additional data nor model training\ncan achieve surprisingly high targeted transferability. This insight has been\noverlooked until now, mainly due to the widespread practice of unreasonably\nrestricting attack optimization to a limited number of iterations. In\nparticular, we, for the first time, identify that a simple logit loss can yield\ncompetitive results with the state of the arts. Our analysis spans a variety of\ntransfer settings, especially including three new, realistic settings: an\nensemble transfer setting with little model similarity, a worse-case setting\nwith low-ranked target classes, and also a real-world attack against the Google\nCloud Vision API. Results in these new settings demonstrate that the commonly\nadopted, easy settings cannot fully reveal the actual properties of different\nattacks and may cause misleading comparisons. We also show the usefulness of\nthe simple logit loss for generating targeted universal adversarial\nperturbations in a data-free and training-free manner. Overall, the aim of our\nanalysis is to inspire a more meaningful evaluation on targeted\ntransferability. Code is available at\nhttps://github.com/ZhengyuZhao/Targeted-Tansfer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhengyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuoran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_M/0/1/0/all/0/1\">Martha Larson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks. (arXiv:2102.02956v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.02956","description":"<p>State-of-the-art object detectors are vulnerable to localized patch hiding\nattacks, where an adversary introduces a small adversarial patch to make\ndetectors miss the detection of salient objects. The patch attacker can carry\nout a physical-world attack by printing and attaching an adversarial patch to\nthe victim object. In this paper, we propose DetectorGuard as the first general\nframework for building provably robust object detectors against localized patch\nhiding attacks. DetectorGuard is inspired by recent advancements in robust\nimage classification research; we ask: can we adapt robust image classifiers\nfor robust object detection? Unfortunately, due to their task difference, an\nobject detector naively adapted from a robust image classifier 1) may not\nnecessarily be robust in the adversarial setting or 2) even maintain decent\nperformance in the clean setting. To build a high-performance robust object\ndetector, we propose an objectness explaining strategy: we adapt a robust image\nclassifier to predict objectness for every image location and then explain each\nobjectness using the bounding boxes predicted by a conventional object\ndetector. If all objectness is well explained, we output the predictions made\nby the conventional object detector; otherwise, we issue an attack alert.\nNotably, 1) in the adversarial setting, we formally prove the end-to-end\nrobustness of DetectorGuard on certified objects, i.e., it either detects the\nobject or triggers an alert, against any patch hiding attacker within our\nthreat model; 2) in the clean setting, we have almost the same performance as\nstate-of-the-art object detectors. Our evaluation on the PASCAL VOC, MS COCO,\nand KITTI datasets further demonstrates that DetectorGuard achieves the first\nprovable robustness against localized patch hiding attacks at a negligible cost\n(&lt;1%) of clean performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training. (arXiv:2102.08098v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.08098","description":"<p>Innovations in neural architectures have fostered significant breakthroughs\nin language modeling and computer vision. Unfortunately, novel architectures\noften result in challenging hyper-parameter choices and training instability if\nthe network parameters are not properly initialized. A number of\narchitecture-specific initialization schemes have been proposed, but these\nschemes are not always portable to new architectures. This paper presents\nGradInit, an automated and architecture agnostic method for initializing neural\nnetworks. GradInit is based on a simple heuristic; the norm of each network\nlayer is adjusted so that a single step of SGD or Adam with prescribed\nhyperparameters results in the smallest possible loss value. This adjustment is\ndone by introducing a scalar multiplier variable in front of each parameter\nblock, and then optimizing these variables using a simple numerical scheme.\nGradInit accelerates the convergence and test performance of many convolutional\narchitectures, both with or without skip connections, and even without\nnormalization layers. It also improves the stability of the original\nTransformer architecture for machine translation, enabling training it without\nlearning rate warmup using either Adam or SGD under a wide range of learning\nrates and momentum coefficients. Code is available at\nhttps://github.com/zhuchen03/gradinit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1\">Renkun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kezhi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Discovery of Adaptive Attacks on Adversarial Defenses. (arXiv:2102.11860v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.11860","description":"<p>Reliable evaluation of adversarial defenses is a challenging task, currently\nlimited to an expert who manually crafts attacks that exploit the defense's\ninner workings or approaches based on an ensemble of fixed attacks, none of\nwhich may be effective for the specific defense at hand. Our key observation is\nthat adaptive attacks are composed of reusable building blocks that can be\nformalized in a search space and used to automatically discover attacks for\nunknown defenses. We evaluated our approach on 24 adversarial defenses and show\nthat it outperforms AutoAttack, the current state-of-the-art tool for reliable\nevaluation of adversarial defenses: our tool discovered significantly stronger\nattacks by producing 3.0\\%-50.8\\% additional adversarial examples for 10\nmodels, while obtaining attacks with slightly stronger or similar strength for\nthe remaining models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chengyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielik_P/0/1/0/all/0/1\">Pavol Bielik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsankov_P/0/1/0/all/0/1\">Petar Tsankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning. (arXiv:2103.10211v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10211","description":"<p>The quality of the image representations obtained from self-supervised\nlearning depends strongly on the type of data augmentations used in the\nlearning formulation. Recent papers have ported these methods from still images\nto videos and found that leveraging both audio and video signals yields strong\ngains; however, they did not find that spatial augmentations such as cropping,\nwhich are very important for still images, work as well for videos. In this\npaper, we improve these formulations in two ways unique to the spatio-temporal\naspect of videos. First, for space, we show that spatial augmentations such as\ncropping do work well for videos too, but that previous implementations, due to\nthe high processing and memory cost, could not do this at a scale sufficient\nfor it to work well. To address this issue, we first introduce Feature Crop, a\nmethod to simulate such augmentations much more efficiently directly in feature\nspace. Second, we show that as opposed to naive average pooling, the use of\ntransformer-based attention improves performance significantly, and is well\nsuited for processing feature crops. Combining both of our discoveries into a\nnew method, Space-Time Crop &amp; Attend (STiCA) we achieve state-of-the-art\nperformance across multiple video-representation learning benchmarks. In\nparticular, we achieve new state-of-the-art accuracies of 67.0% on HMDB-51 and\n93.1% on UCF-101 when pre-training on Kinetics-400.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patrick_M/0/1/0/all/0/1\">Mandela Patrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bernie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Joao Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMA-Net: A Cascaded Mutual Attention Network for Light Field Salient Object Detection. (arXiv:2105.00949v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00949","description":"<p>In the past few years, numerous deep learning methods have been proposed to\naddress the task of segmenting salient objects from RGB images. However, these\napproaches depending on single modality fail to achieve the state-of-the-art\nperformance on widely used light field salient object detection (SOD) datasets,\nwhich collect large-scale natural images and provide multiple modalities such\nas multi-view, micro-lens images and depth maps. Most recently proposed light\nfield SOD methods have acquired improving detecting accuracy, yet still predict\nrough objects' structures and perform slow inference speed. To this end, we\npropose CMA-Net, which consists of two novel cascaded mutual attention modules\naiming at fusing the high level features from the modalities of all-in-focus\nand depth. Our proposed CMA-Net outperforms 30 SOD methods on two widely\napplied light field benchmark datasets. Besides, the proposed CMA-Net is able\nto inference at the speed of 53 fps, thus being much faster than the\nstate-of-the-art multi-modal SOD methods. Extensive quantitative and\nqualitative experiments illustrate both the effectiveness and efficiency of our\nCMA-Net, inspiring future development of multi-modal learning for both the\nRGB-D and light field SOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoseContrast: Class-Agnostic Object Viewpoint Estimation in the Wild with Pose-Aware Contrastive Learning. (arXiv:2105.05643v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05643","description":"<p>Motivated by the need for estimating the 3D pose of arbitrary objects, we\nconsider the challenging problem of class-agnostic object viewpoint estimation\nfrom images only, without CAD model knowledge. The idea is to leverage features\nlearned on seen classes to estimate the pose for classes that are unseen, yet\nthat share similar geometries and canonical frames with seen classes. We train\na direct pose estimator in a class-agnostic way by sharing weights across all\nobject classes, and we introduce a contrastive learning method that has three\nmain ingredients: (i) the use of pre-trained, self-supervised, contrast-based\nfeatures; (ii) pose-aware data augmentations; (iii) a pose-aware contrastive\nloss. We experimented on Pascal3D+, ObjectNet3D and Pix3D in a cross-dataset\nfashion, with both seen and unseen classes. We report state-of-the-art results,\nincluding against methods that additionally use CAD models as input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-stream Network for Visual Recognition. (arXiv:2105.14734v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14734","description":"<p>Transformers with remarkable global representation capacities achieve\ncompetitive results for visual tasks, but fail to consider high-level local\npattern information in input images. In this paper, we present a generic\nDual-stream Network (DS-Net) to fully explore the representation capacity of\nlocal and global pattern features for image classification. Our DS-Net can\nsimultaneously calculate fine-grained and integrated features and efficiently\nfuse them. Specifically, we propose an Intra-scale Propagation module to\nprocess two different resolutions in each block and an Inter-Scale Alignment\nmodule to perform information interaction across features at dual scales.\nBesides, we also design a Dual-stream FPN (DS-FPN) to further enhance\ncontextual information for downstream dense predictions. Without bells and\nwhistles, the proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1\naccuracy on ImageNet-1k and achieves state-of-the-art performance over other\nVision Transformers and ResNets. For object detection and instance\nsegmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5%\nin terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art\nscheme, which significantly demonstrates its potential to be a general backbone\nin vision tasks. The code will be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Honghui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shumin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection. (arXiv:2106.00666v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00666","description":"<p>Can Transformer perform 2D object- and region-level recognition from a pure\nsequence-to-sequence perspective with minimal knowledge about the 2D spatial\nstructure? To answer this question, we present You Only Look at One Sequence\n(YOLOS), a series of object detection models based on the vanilla Vision\nTransformer with the fewest possible modifications, region priors, as well as\ninductive biases of the target task. We find that YOLOS pre-trained on the\nmid-sized ImageNet-1k dataset only can already achieve quite competitive\nperformance on the challenging COCO object detection benchmark, e.g.,\nYOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP\non COCO val. We also discuss the impacts as well as limitations of current\npre-train schemes and model scaling strategies for Transformer in vision\nthrough YOLOS. Code and pre-trained models are available at\nhttps://github.com/hustvl/YOLOS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Bencheng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiemin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jianwei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise2Score: Tweedie's Approach to Self-Supervised Image Denoising without Clean Images. (arXiv:2106.07009v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.07009","description":"<p>Recently, there has been extensive research interest in training deep\nnetworks to denoise images without clean reference. However, the representative\napproaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator\n(SURE), etc. seem to differ from one another and it is difficult to find the\ncoherent mathematical structure. To address this, here we present a novel\napproach, called Noise2Score, which reveals a missing link in order to unite\nthese seemingly different approaches. Specifically, we show that image\ndenoising problems without clean images can be addressed by finding the mode of\nthe posterior distribution and that the Tweedie's formula offers an explicit\nsolution through the score function (i.e. the gradient of log likelihood). Our\nmethod then uses the recent finding that the score function can be stably\nestimated from the noisy images using the amortized residual denoising\nautoencoder, the method of which is closely related to Noise2Noise or\nNose2Void. Our Noise2Score approach is so universal that the same network\ntraining can be used to remove noises from images that are corrupted by any\nexponential family distributions and noise parameters. Using extensive\nexperiments with Gaussian, Poisson, and Gamma noises, we show that Noise2Score\nsignificantly outperforms the state-of-the-art self-supervised denoising\nmethods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kwanyoung Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised GANs with Label Augmentation. (arXiv:2106.08601v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.08601","description":"<p>Recently, transformation-based self-supervised learning has been applied to\ngenerative adversarial networks (GANs) to mitigate catastrophic forgetting in\nthe discriminator by introducing a stationary learning environment. However,\nthe separate self-supervised tasks in existing self-supervised GANs cause a\ngoal inconsistent with generative modeling due to the fact that their\nself-supervised classifiers are agnostic to the generator distribution. To\naddress this problem, we propose a novel self-supervised GAN that unifies the\nGAN task with the self-supervised task by augmenting the GAN labels (real or\nfake) via self-supervision of data transformation. Specifically, the original\ndiscriminator and self-supervised classifier are unified into a label-augmented\ndiscriminator that predicts the augmented labels to be aware of both the\ngenerator distribution and the data distribution under every transformation,\nand then provide the discrepancy between them to optimize the generator.\nTheoretically, we prove that the optimal generator could converge to replicate\nthe real data distribution. Empirically, we show that the proposed method\nsignificantly outperforms previous self-supervised and data augmentation GANs\non both generative modeling and representation learning across benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers. (arXiv:2106.12620v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12620","description":"<p>The self-attention-based model, transformer, is recently becoming the leading\nbackbone in the field of computer vision. In spite of the impressive success\nmade by transformers in a variety of vision tasks, it still suffers from heavy\ncomputation and intensive memory costs. To address this limitation, this paper\npresents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\nWe start by observing a large amount of redundant computation, mainly spent on\nuncorrelated input patches, and then introduce an interpretable module to\ndynamically and gracefully drop these redundant patches. This novel framework\nis then extended to a hierarchical structure, where uncorrelated tokens at\ndifferent stages are gradually removed, resulting in a considerable shrinkage\nof computational cost. We include extensive experiments on both image and video\ntasks, where our method could deliver up to 1.4x speed-up for state-of-the-art\nmodels like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.\nMore importantly, contrary to other acceleration approaches, our method is\ninherently interpretable with substantial visual evidence, making vision\ntransformer closer to a more human-understandable architecture while being\nlighter. We demonstrate that the interpretability that naturally emerged in our\nframework can outperform the raw attention learned by the original visual\ntransformer, as well as those generated by off-the-shelf interpretation\nmethods, with both qualitative and quantitative results. Project Page:\n<a href=\"http://people.csail.mit.edu/bpan/ia-red/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bowen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Training of Neural Lumigraph Representations using Meta Learning. (arXiv:2106.14942v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14942","description":"<p>Novel view synthesis is a long-standing problem in machine learning and\ncomputer vision. Significant progress has recently been made in developing\nneural scene representations and rendering techniques that synthesize\nphotorealistic images from arbitrary views. These representations, however, are\nextremely slow to train and often also slow to render. Inspired by neural\nvariants of image-based rendering, we develop a new neural rendering approach\nwith the goal of quickly learning a high-quality representation which can also\nbe rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a\nunique combination of a neural shape representation and 2D CNN-based image\nfeature extraction, aggregation, and re-projection. To push representation\nconvergence times down to minutes, we leverage meta learning to learn neural\nshape and image feature priors which accelerate training. The optimized shape\nand image features can then be extracted using traditional graphics techniques\nand rendered in real time. We show that MetaNLR++ achieves similar or better\nnovel view synthesis results in a fraction of the time that competing methods\nrequire.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">Alexander W. Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellnhofer_P/0/1/0/all/0/1\">Petr Kellnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?. (arXiv:2107.00166v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00166","description":"<p>There have been long-standing controversies and inconsistencies over the\nexperiment setup and criteria for identifying the \"winning ticket\" in\nliterature. To reconcile such, we revisit the definition of lottery ticket\nhypothesis, with comprehensive and more rigorous conditions. Under our new\ndefinition, we show concrete evidence to clarify whether the winning ticket\nexists across the major DNN architectures and/or applications. Through\nextensive experiments, we perform quantitative analysis on the correlations\nbetween winning tickets and various experimental factors, and empirically study\nthe patterns of our observations. We find that the key training\nhyperparameters, such as learning rate and training epochs, as well as the\narchitecture characteristics such as capacities and residual connections, are\nall highly correlated with whether and when the winning tickets can be\nidentified. Based on our analysis, we summarize a guideline for parameter\nsettings in regards of specific architecture characteristics, which we hope to\ncatalyze the research progress on the topic of lottery ticket hypothesis. Our\ncodes are publicly available at:\nhttps://github.com/boone891214/sanity-check-LTH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images. (arXiv:2107.14449v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14449","description":"<p>Nonlinear inter-modality registration is often challenging due to the lack of\nobjective functions that are good proxies for alignment. Here we propose a\nsynthesis-by-registration method to convert this problem into an easier\nintra-modality task. We introduce a registration loss for weakly supervised\nimage translation between domains that does not require perfectly aligned\ntraining data. This loss capitalises on a registration U-Net with frozen\nweights, to drive a synthesis CNN towards the desired translation. We\ncomplement this loss with a structure preserving constraint based on\ncontrastive learning, which prevents blurring and content shifts due to\noverfitting. We apply this method to the registration of histological sections\nto MRI slices, a key step in 3D histology reconstruction. Results on two\ndifferent public datasets show improvements over registration based on mutual\ninformation (13% reduction in landmark error) and synthesis-based algorithms\nsuch as CycleGAN (11% reduction), and are comparable to a registration CNN with\nlabel supervision. Code and data are publicly available at\n\\url{https://github.com/acasamitjana/SynthByReg}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casamitjana_A/0/1/0/all/0/1\">Adri&#xe0; Casamitjana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Matteo Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00049","description":"<p>Contrastive self-supervised learning has shown impressive results in learning\nvisual representations from unlabeled images by enforcing invariance against\ndifferent data augmentations. However, the learned representations are often\ncontextually biased to the spurious scene correlations of different objects or\nobject and background, which may harm their generalization on the downstream\ntasks. To tackle the issue, we develop a novel object-aware contrastive\nlearning framework that first (a) localizes objects in a self-supervised manner\nand then (b) debias scene correlations via appropriate data augmentations\nconsidering the inferred object locations. For (a), we propose the contrastive\nclass activation map (ContraCAM), which finds the most discriminative regions\n(e.g., objects) in the image compared to the other images using the\ncontrastively trained models. We further improve the ContraCAM to detect\nmultiple objects and entire shapes via an iterative refinement procedure. For\n(b), we introduce two data augmentations based on ContraCAM, object-aware\nrandom crop and background mixup, which reduce contextual and background biases\nduring contrastive self-supervised learning, respectively. Our experiments\ndemonstrate the effectiveness of our representation learning framework,\nparticularly when trained under multi-object images or evaluated under the\nbackground (and distribution) shifted images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Sangwoo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyunwoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations. (arXiv:2108.01938v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.01938","description":"<p>Graph neural networks are increasingly becoming the go-to approach in various\nfields such as computer vision, computational biology and chemistry, where data\nare naturally explained by graphs. However, unlike traditional convolutional\nneural networks, deep graph networks do not necessarily yield better\nperformance than shallow graph networks. This behavior usually stems from the\nover-smoothing phenomenon. In this work, we propose a family of architectures\nto control this behavior by design. Our networks are motivated by numerical\nmethods for solving Partial Differential Equations (PDEs) on manifolds, and as\nsuch, their behavior can be explained by similar analysis. Moreover, as we\ndemonstrate using an extensive set of experiments, our PDE-motivated networks\ncan generalize and be effective for various types of problems from different\nfields. Our architectures obtain better or on par with the current\nstate-of-the-art results for problems that are typically approached using\ndifferent architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1\">Moshe Eliasof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1\">Eldad Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1\">Eran Treister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Road Scenes Segmentation Across Different Domains by Disentangling Latent Representations. (arXiv:2108.03021v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03021","description":"<p>Deep learning models obtain impressive accuracy in road scenes understanding,\nhowever they need a large quantity of labeled samples for their training.\nAdditionally, such models do not generalise well to environments where the\nstatistical properties of data do not perfectly match those of training scenes,\nand this can be a significant problem for intelligent vehicles. Hence, domain\nadaptation approaches have been introduced to transfer knowledge acquired on a\nlabel-abundant source domain to a related label-scarce target domain. In this\nwork, we design and carefully analyse multiple latent space-shaping\nregularisation strategies that work together to reduce the domain shift. More\nin detail, we devise a feature clustering strategy to increase domain\nalignment, a feature perpendicularity constraint to space apart features\nbelonging to different semantic classes, including those not present in the\ncurrent batch, and a feature norm alignment strategy to separate active and\ninactive channels. In addition, we propose a novel evaluation metric to capture\nthe relative performance of an adapted model with respect to supervised\ntraining. We validate our framework in driving scenarios, considering both\nsynthetic-to-real and real-to-real adaptation, outperforming previous\nfeature-level state-of-the-art methods on multiple road scenes benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1\">Francesco Barbato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer. (arXiv:2108.04444v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04444","description":"<p>Point cloud completion aims to predict a complete shape in high accuracy from\nits partial observation. However, previous methods usually suffered from\ndiscrete nature of point cloud and unstructured prediction of points in local\nregions, which makes it hard to reveal fine local geometric details on the\ncomplete shape. To resolve this issue, we propose SnowflakeNet with Snowflake\nPoint Deconvolution (SPD) to generate the complete point clouds. The\nSnowflakeNet models the generation of complete point clouds as the\nsnowflake-like growth of points in 3D space, where the child points are\nprogressively generated by splitting their parent points after each SPD. Our\ninsight of revealing detailed geometry is to introduce skip-transformer in SPD\nto learn point splitting patterns which can fit local regions the best.\nSkip-transformer leverages attention mechanism to summarize the splitting\npatterns used in the previous SPD layer to produce the splitting in the current\nSPD layer. The locally compact and structured point cloud generated by SPD is\nable to precisely capture the structure characteristic of 3D shape in local\npatches, which enables the network to predict highly detailed geometries, such\nas smooth regions, sharp edges and corners. Our experimental results outperform\nthe state-of-the-art point cloud completion methods under widely used\nbenchmarks. Code will be available at\nhttps://github.com/AllenXiangX/SnowflakeNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPNet: Cross-Parallel Network for Efficient Anomaly Detection. (arXiv:2108.04454v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04454","description":"<p>Anomaly detection in video streams is a challenging problem because of the\nscarcity of abnormal events and the difficulty of accurately annotating them.\nTo alleviate these issues, unsupervised learning-based prediction methods have\nbeen previously applied. These approaches train the model with only normal\nevents and predict a future frame from a sequence of preceding frames by use of\nencoder-decoder architectures so that they result in small prediction errors on\nnormal events but large errors on abnormal events. The architecture, however,\ncomes with the computational burden as some anomaly detection tasks require low\ncomputational cost without sacrificing performance. In this paper,\nCross-Parallel Network (CPNet) for efficient anomaly detection is proposed here\nto minimize computations without performance drops. It consists of N smaller\nparallel U-Net, each of which is designed to handle a single input frame, to\nmake the calculations significantly more efficient. Additionally, an\ninter-network shift module is incorporated to capture temporal relationships\namong sequential frames to enable more accurate future predictions.The\nquantitative results show that our model requires less computational cost than\nthe baseline U-Net while delivering equivalent performance in anomaly\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngsaeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jonghwan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">David Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Surface Crack Segmentation by Generating Pseudo-Labels using Localization with a Classifier and Thresholding. (arXiv:2109.00456v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00456","description":"<p>Surface cracks are a common sight on public infrastructure nowadays. Recent\nwork has been addressing this problem by supporting structural maintenance\nmeasures using machine learning methods. Those methods are used to segment\nsurface cracks from their background, making them easier to localize. However,\na common issue is that to create a well-functioning algorithm, the training\ndata needs to have detailed annotations of pixels that belong to cracks. Our\nwork proposes a weakly supervised approach that leverages a CNN classifier in a\nnovel way to create surface crack pseudo labels. First, we use the classifier\nto create a rough crack localization map by using its class activation maps and\na patch based classification approach and fuse this with a thresholding based\napproach to segment the mostly darker crack pixels. The classifier assists in\nsuppressing noise from the background regions, which commonly are incorrectly\nhighlighted as cracks by standard thresholding methods. Then, the pseudo labels\ncan be used in an end-to-end approach when training a standard CNN for surface\ncrack segmentation. Our method is shown to yield sufficiently accurate pseudo\nlabels. Those labels, incorporated into segmentation CNN training using\nmultiple recent crack segmentation architectures, achieve comparable\nperformance to fully supervised methods on four popular crack segmentation\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konig_J/0/1/0/all/0/1\">Jacob K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_M/0/1/0/all/0/1\">Mark Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannion_M/0/1/0/all/0/1\">Mike Mannion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrie_P/0/1/0/all/0/1\">Peter Barrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morison_G/0/1/0/all/0/1\">Gordon Morison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Compositional Feature Embedding and Similarity Metric for Ultra-Fine-Grained Visual Categorization. (arXiv:2109.12380v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12380","description":"<p>Fine-grained visual categorization (FGVC), which aims at classifying objects\nwith small inter-class variances, has been significantly advanced in recent\nyears. However, ultra-fine-grained visual categorization (ultra-FGVC), which\ntargets at identifying subclasses with extremely similar patterns, has not\nreceived much attention. In ultra-FGVC datasets, the samples per category are\nalways scarce as the granularity moves down, which will lead to overfitting\nproblems. Moreover, the difference among different categories is too subtle to\ndistinguish even for professional experts. Motivated by these issues, this\npaper proposes a novel compositional feature embedding and similarity metric\n(CECS). Specifically, in the compositional feature embedding module, we\nrandomly select patches in the original input image, and these patches are then\nreplaced by patches from the images of different categories or masked out. Then\nthe replaced and masked images are used to augment the original input images,\nwhich can provide more diverse samples and thus largely alleviate overfitting\nproblem resulted from limited training samples. Besides, learning with diverse\nsamples forces the model to learn not only the most discriminative features but\nalso other informative features in remaining regions, enhancing the\ngeneralization and robustness of the model. In the compositional similarity\nmetric module, a new similarity metric is developed to improve the\nclassification performance by narrowing the intra-category distance and\nenlarging the inter-category distance. Experimental results on two ultra-FGVC\ndatasets and one FGVC dataset with recent benchmark methods consistently\ndemonstrate that the proposed CECS method achieves the state of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaohua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Homography Estimation in Dynamic Surgical Scenes for Laparoscopic Camera Motion Extraction. (arXiv:2109.15098v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.15098","description":"<p>Current laparoscopic camera motion automation relies on rule-based approaches\nor only focuses on surgical tools. Imitation Learning (IL) methods could\nalleviate these shortcomings, but have so far been applied to oversimplified\nsetups. Instead of extracting actions from oversimplified setups, in this work\nwe introduce a method that allows to extract a laparoscope holder's actions\nfrom videos of laparoscopic interventions. We synthetically add camera motion\nto a newly acquired dataset of camera motion free da Vinci surgery image\nsequences through a novel homography generation algorithm. The synthetic camera\nmotion serves as a supervisory signal for camera motion estimation that is\ninvariant to object and tool motion. We perform an extensive evaluation of\nstate-of-the-art (SOTA) Deep Neural Networks (DNNs) across multiple compute\nregimes, finding our method transfers from our camera motion free da Vinci\nsurgery dataset to videos of laparoscopic interventions, outperforming\nclassical homography estimation approaches in both, precision by 41%, and\nruntime on a CPU by 43%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huber_M/0/1/0/all/0/1\">Martin Huber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bergeles_C/0/1/0/all/0/1\">Christos Bergeles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations. (arXiv:2110.01823v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01823","description":"<p>When compared to the image classification models, black-box adversarial\nattacks against video classification models have been largely understudied.\nThis could be possible because, with video, the temporal dimension poses\nsignificant additional challenges in gradient estimation. Query-efficient\nblack-box attacks rely on effectively estimated gradients towards maximizing\nthe probability of misclassifying the target video. In this work, we\ndemonstrate that such effective gradients can be searched for by parameterizing\nthe temporal structure of the search space with geometric transformations.\nSpecifically, we design a novel iterative algorithm Geometric TRAnsformed\nPerturbations (GEO-TRAP), for attacking video classification models. GEO-TRAP\nemploys standard geometric transformation operations to reduce the search space\nfor effective gradients into searching for a small group of parameters that\ndefine these operations. This group of parameters describes the geometric\nprogression of gradients, resulting in a reduced and structured search space.\nOur algorithm inherently leads to successful perturbations with surprisingly\nfew queries. For example, adversarial examples generated from GEO-TRAP have\nbetter attack success rates with ~73.55% fewer queries compared to the\nstate-of-the-art method for video adversarial attacks on the widely used Jester\ndataset. Overall, our algorithm exposes vulnerabilities of diverse video\nclassification models and achieves new state-of-the-art results under black-box\nsettings on two large datasets. Code is available here:\nhttps://github.com/sli057/Geo-TRAP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shitong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1\">Srikanth V. Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient large-scale image retrieval with deep feature orthogonality and Hybrid-Swin-Transformers. (arXiv:2110.03786v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03786","description":"<p>We present an efficient end-to-end pipeline for largescale landmark\nrecognition and retrieval. We show how to combine and enhance concepts from\nrecent research in image retrieval and introduce two architectures especially\nsuited for large-scale landmark identification. A model with deep orthogonal\nfusion of local and global features (DOLG) using an EfficientNet backbone as\nwell as a novel Hybrid-Swin-Transformer is discussed and details how to train\nboth architectures efficiently using a step-wise approach and a sub-center\narcface loss with dynamic margins are provided. Furthermore, we elaborate a\nnovel discriminative re-ranking methodology for image retrieval. The\nsuperiority of our approach was demonstrated by winning the recognition and\nretrieval track of the Google Landmark Competition 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henkel_C/0/1/0/all/0/1\">Christof Henkel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search. (arXiv:2110.05668v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05668","description":"<p>Most existing neural architecture search (NAS) benchmarks and algorithms\nprioritize performance on well-studied tasks, e.g., image classification on\nCIFAR and ImageNet. This makes the applicability of NAS approaches in more\ndiverse areas inadequately understood. In this paper, we present NAS-Bench-360,\na benchmark suite for evaluating state-of-the-art NAS methods for convolutional\nneural networks (CNNs). To construct it, we curate a collection of ten tasks\nspanning a diverse array of application domains, dataset sizes, problem\ndimensionalities, and learning objectives. By carefully selecting tasks that\ncan both interoperate with modern CNN-based search methods but that are also\nfar-afield from their original development domain, we can use NAS-Bench-360 to\ninvestigate the following central question: do existing state-of-the-art NAS\nmethods perform well on diverse tasks? Our experiments show that a modern NAS\nprocedure designed for image classification can indeed find good architectures\nfor tasks with other dimensionalities and learning objectives; however, the\nsame method struggles against more task-specific methods and performs\ncatastrophically poorly on classification in non-vision domains. The case for\nNAS robustness becomes even more dire in a resource-constrained setting, where\na recent NAS method provides little-to-no benefit over much simpler baselines.\nThese results demonstrate the need for a benchmark such as NAS-Bench-360 to\nhelp develop NAS approaches that work well on a variety of tasks, a crucial\ncomponent of a truly robust and automated pipeline. We conclude with a\ndemonstration of the kind of future research our suite of tasks will enable.\nAll data and code is made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Renbo Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1\">Mikhail Khodak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1\">Nicholas Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge. (arXiv:2110.07020v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07020","description":"<p>Kinship verification is the task of determining whether a parent-child,\nsibling, or grandparent-grandchild relationship exists between two people and\nis important in social media applications, forensic investigations, finding\nmissing children, and reuniting families. We demonstrate high quality kinship\nverification by participating in the 2021 Recognizing Families in the Wild\nchallenge which provides the largest publicly available dataset in the field.\nOur approach is among the top 3 winning entries in the competition. We ensemble\nmodels written by both human experts and OpenAI Codex. We make our models and\ncode publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strome_M/0/1/0/all/0/1\">Maxwell Benjamin Strome</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_I/0/1/0/all/0/1\">Ian Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_P/0/1/0/all/0/1\">Parker Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Roman Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagri_V/0/1/0/all/0/1\">Vaibhav Bagri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Newman Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse Problems Leveraging Pre-trained Contrastive Representations. (arXiv:2110.07439v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.07439","description":"<p>We study a new family of inverse problems for recovering representations of\ncorrupted data. We assume access to a pre-trained representation learning\nnetwork R(x) that operates on clean images, like CLIP. The problem is to\nrecover the representation of an image R(x), if we are only given a corrupted\nversion A(x), for some known forward operator A. We propose a supervised\ninversion method that uses a contrastive objective to obtain excellent\nrepresentations for highly corrupted images. Using a linear probe on our robust\nrepresentations, we achieve a higher accuracy than end-to-end supervised\nbaselines when classifying images with various types of distortions, including\nblurring, additive noise, and random pixel masking. We evaluate on a subset of\nImageNet and observe that our method is robust to varying levels of distortion.\nOur method outperforms end-to-end baselines even with a fraction of the labeled\ndata in a wide range of forward operators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravula_S/0/1/0/all/0/1\">Sriram Ravula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyrnis_G/0/1/0/all/0/1\">Georgios Smyrnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Matt Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuation of Famous Art with AI: A Conditional Adversarial Network Inpainting Approach. (arXiv:2110.09170v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09170","description":"<p>Much of the state-of-the-art in image synthesis inspired by real artwork are\neither entirely generative by filtered random noise or inspired by the transfer\nof style. This work explores the application of image inpainting to continue\nfamous artworks and produce generative art with a Conditional GAN. During the\ntraining stage of the process, the borders of images are cropped, leaving only\nthe centre. An inpainting GAN is then tasked with learning to reconstruct the\noriginal image from the centre crop by way of minimising both adversarial and\nabsolute difference losses, which are analysed by both their Fr\\'echet\nInception Distances and manual observations which are presented. Once the\nnetwork is trained, images are then resized rather than cropped and presented\nas input to the generator. Following the learning process, the generator then\ncreates new images by continuing from the edges of the original piece. Three\nexperiments are performed with datasets of 4766 landscape paintings\n(impressionism and romanticism), 1167 Ukiyo-e works from the Japanese Edo\nperiod, and 4968 abstract artworks. Results show that geometry and texture\n(including canvas and paint) as well as scenery such as sky, clouds, water,\nland (including hills and mountains), grass, and flowers are implemented by the\ngenerator when extending real artworks. In the Ukiyo-e experiments, it was\nobserved that features such as written text were generated even in cases where\nthe original image did not have any, due to the presence of an unpainted border\nwithin the input image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1\">Jordan J. Bird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization. (arXiv:2110.10832v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.10832","description":"<p>In Domain Generalization (DG) settings, models trained on a given set of\ntraining domains have notoriously chaotic performance on distribution shifted\ntest domains, and stochasticity in optimization (e.g. seed) plays a big role.\nThis makes deep learning models unreliable in real world settings. We first\nshow that a simple protocol for averaging model parameters along the\noptimization path, starting early during training, both significantly boosts\ndomain generalization and diminishes the impact of stochasticity by improving\nthe rank correlation between the in-domain validation accuracy and out-domain\ntest accuracy, which is crucial for reliable model selection. Next, we show\nthat an ensemble of independently trained models also has a chaotic behavior in\nthe DG setting. Taking advantage of our observation, we show that instead of\nensembling unaveraged models, ensembling moving average models (EoA) from\ndifferent runs does increase stability and further boosts performance. On the\nDomainBed benchmark, when using a ResNet-50 pre-trained on ImageNet, this\nensemble of averages achieves $88.6\\%$ on PACS, $79.1\\%$ on VLCS, $72.5\\%$ on\nOfficeHome, $52.3\\%$ on TerraIncognita, and $47.4\\%$ on DomainNet, an average\nof $68.0\\%$, beating ERM (w/o model averaging) by $\\sim 4\\%$. We also evaluate\na model that is pre-trained on a larger dataset, where we show EoA achieves an\naverage accuracy of $72.7\\%$, beating its corresponding ERM baseline by $5\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1\">Devansh Arpit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LARNet: Latent Action Representation for Human Action Synthesis. (arXiv:2110.10899v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.10899","description":"<p>We present LARNet, a novel end-to-end approach for generating human action\nvideos. A joint generative modeling of appearance and dynamics to synthesize a\nvideo is very challenging and therefore recent works in video synthesis have\nproposed to decompose these two factors. However, these methods require a\ndriving video to model the video dynamics. In this work, we propose a\ngenerative approach instead, which explicitly learns action dynamics in latent\nspace avoiding the need of a driving video during inference. The generated\naction dynamics is integrated with the appearance using a recurrent\nhierarchical structure which induces motion at different scales to focus on\nboth coarse as well as fine level action details. In addition, we propose a\nnovel mix-adversarial loss function which aims at improving the temporal\ncoherency of synthesized videos. We evaluate the proposed approach on four\nreal-world human action datasets demonstrating the effectiveness of the\nproposed approach in generating human actions. Code available at\nhttps://github.com/aayushjr/larnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biyani_N/0/1/0/all/0/1\">Naman Biyani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Aayush J Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1\">Shruti Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh S Rawat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks. (arXiv:2110.12509v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12509","description":"<p>Estimating the lung depth on x-ray images could provide both an accurate\nopportunistic lung volume estimation during clinical routine and improve image\ncontrast in modern structural chest imaging techniques like x-ray dark-field\nimaging. We present a method based on a convolutional neural network that\nallows a per-pixel lung thickness estimation and subsequent total lung capacity\nestimation. The network was trained and validated using 5250 simulated\nradiographs generated from 525 real CT scans. Furthermore, we are able to infer\nthe model trained with simulation data on real radiographs.\n</p>\n<p>For 35 patients, quantitative and qualitative evaluation was performed on\nstandard clinical radiographs. The ground-truth for each patient's total lung\nvolume was defined based on the patients' corresponding CT scan. The\nmean-absolute error between the estimated lung volume on the 35 real\nradiographs and groundtruth volume was 0.73 liter. Additionally, we predicted\nthe lung thicknesses on a synthetic dataset of 131 radiographs, where the\nmean-absolute error was 0.27 liter. The results show, that it is possible to\ntransfer the knowledge obtained in a simulation model to real x-ray images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schultheiss_M/0/1/0/all/0/1\">Manuel Schultheiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmette_P/0/1/0/all/0/1\">Philipp Schmette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellerer_T/0/1/0/all/0/1\">Thorsten Sellerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_R/0/1/0/all/0/1\">Rafael Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taphorn_K/0/1/0/all/0/1\">Kirsten Taphorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mechlem_K/0/1/0/all/0/1\">Korbinian Mechlem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birnbacher_L/0/1/0/all/0/1\">Lorenz Birnbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renger_B/0/1/0/all/0/1\">Bernhard Renger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_F/0/1/0/all/0/1\">Franz Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_D/0/1/0/all/0/1\">Daniela Pfeiffer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Graph Representation of Person-specific Cognitive Processes from Audio-visual Behaviours for Automatic Personality Recognition. (arXiv:2110.13570v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13570","description":"<p>This approach builds on two following findings in cognitive science: (i)\nhuman cognition partially determines expressed behaviour and is directly linked\nto true personality traits; and (ii) in dyadic interactions individuals'\nnonverbal behaviours are influenced by their conversational partner behaviours.\nIn this context, we hypothesise that during a dyadic interaction, a target\nsubject's facial reactions are driven by two main factors, i.e. their internal\n(person-specific) cognitive process, and the externalised nonverbal behaviours\nof their conversational partner. Consequently, we propose to represent the\ntarget subjects (defined as the listener) person-specific cognition in the form\nof a person-specific CNN architecture that has unique architectural parameters\nand depth, which takes audio-visual non-verbal cues displayed by the\nconversational partner (defined as the speaker) as input, and is able to\nreproduce the target subject's facial reactions. Each person-specific CNN is\nexplored by the Neural Architecture Search (NAS) and a novel adaptive loss\nfunction, which is then represented as a graph representation for recognising\nthe target subject's true personality. Experimental results not only show that\nthe produced graph representations are well associated with target subjects'\npersonality traits in both human-human and human-machine interaction scenarios,\nand outperform the existing approaches with significant advantages, but also\ndemonstrate that the proposed novel strategies such as adaptive loss, and the\nend-to-end vertices/edges feature learning, help the proposed approach in\nlearning more reliable personality representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Siyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zilong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_S/0/1/0/all/0/1\">Shashank Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valstar_M/0/1/0/all/0/1\">Michel Valstar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1\">Hatice Gunes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}