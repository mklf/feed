{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Grammatical Profiling for Semantic Change Detection. (arXiv:2109.10397v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10397","description":"<p>Semantics, morphology and syntax are strongly interdependent. However, the\nmajority of computational methods for semantic change detection use\ndistributional word representations which encode mostly semantics. We\ninvestigate an alternative method, grammatical profiling, based entirely on\nchanges in the morphosyntactic behaviour of words. We demonstrate that it can\nbe used for semantic change detection and even outperforms some distributional\nsemantic methods. We present an in-depth qualitative and quantitative analysis\nof the predictions made by our grammatical profiling system, showing that they\nare plausible and interpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giulianelli_M/0/1/0/all/0/1\">Mario Giulianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1\">Andrey Kutuzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pivovarova_L/0/1/0/all/0/1\">Lidia Pivovarova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RETRONLU: Retrieval Augmented Task-Oriented Semantic Parsing. (arXiv:2109.10410v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10410","description":"<p>While large pre-trained language models accumulate a lot of knowledge in\ntheir parameters, it has been demonstrated that augmenting it with\nnon-parametric retrieval-based memory has a number of benefits from accuracy\nimprovements to data efficiency for knowledge-focused tasks, such as question\nanswering. In this paper, we are applying retrieval-based modeling ideas to the\nproblem of multi-domain task-oriented semantic parsing for conversational\nassistants. Our approach, RetroNLU, extends a sequence-to-sequence model\narchitecture with a retrieval component, used to fetch existing similar\nexamples and provide them as an additional input to the model. In particular,\nwe analyze two settings, where we augment an input with (a) retrieved nearest\nneighbor utterances (utterance-nn), and (b) ground-truth semantic parses of\nnearest neighbor utterances (semparse-nn). Our technique outperforms the\nbaseline method by 1.5% absolute macro-F1, especially at the low resource\nsetting, matching the baseline model accuracy with only 40% of the data.\nFurthermore, we analyze the nearest neighbor retrieval component's quality,\nmodel sensitivity and break down the performance for semantic parses of\ndifferent utterance complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Adithya Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savenkov_D/0/1/0/all/0/1\">Denis Savenkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Would it Take to get Biomedical QA Systems into Practice?. (arXiv:2109.10415v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10415","description":"<p>Medical question answering (QA) systems have the potential to answer\nclinicians uncertainties about treatment and diagnosis on demand, informed by\nthe latest evidence. However, despite the significant progress in general QA\nmade by the NLP community, medical QA systems are still not widely used in\nclinical environments. One likely reason for this is that clinicians may not\nreadily trust QA system outputs, in part because transparency, trustworthiness,\nand provenance have not been key considerations in the design of such models.\nIn this paper we discuss a set of criteria that, if met, we argue would likely\nincrease the utility of biomedical QA systems, which may in turn lead to\nadoption of such systems in practice. We assess existing models, tasks, and\ndatasets with respect to these criteria, highlighting shortcomings of\npreviously proposed approaches and pointing toward what might be more usable QA\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kell_G/0/1/0/all/0/1\">Gregory Kell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_I/0/1/0/all/0/1\">Iain J. Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaun_A/0/1/0/all/0/1\">Andre Jaun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Debiasing Techniques for Intersectional Biases. (arXiv:2109.10441v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10441","description":"<p>Bias is pervasive in NLP models, motivating the development of automatic\ndebiasing techniques. Evaluation of NLP debiasing methods has largely been\nlimited to binary attributes in isolation, e.g., debiasing with respect to\nbinary gender or race, however many corpora involve multiple such attributes,\npossibly with higher cardinality. In this paper we argue that a truly fair\nmodel must consider `gerrymandering' groups which comprise not only single\nattributes, but also intersectional groups. We evaluate a form of\nbias-constrained model which is new to NLP, as well an extension of the\niterative nullspace projection technique which can handle multiple protected\nattributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Shivashankar Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness-aware Class Imbalanced Learning. (arXiv:2109.10444v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10444","description":"<p>Class imbalance is a common challenge in many NLP tasks, and has clear\nconnections to bias, in that bias in training data often leads to higher\naccuracy for majority groups at the expense of minority groups. However there\nhas traditionally been a disconnect between research on class-imbalanced\nlearning and mitigating bias, and only recently have the two been looked at\nthrough a common lens. In this work we evaluate long-tail learning methods for\ntweet sentiment and occupation classification, and extend a margin-loss based\napproach with methods to enforce fairness. We empirically show through\ncontrolled experiments that the proposed approaches help mitigate both class\nimbalance and demographic biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Shivashankar Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1\">Afshin Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results. (arXiv:2109.10453v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10453","description":"<p>Recent transformer-based approaches demonstrate promising results on\nrelational scientific information extraction. Existing datasets focus on\nhigh-level description of how research is carried out. Instead we focus on the\nsubtleties of how experimental associations are presented by building SciClaim,\na dataset of scientific claims drawn from Social and Behavior Science (SBS),\nPubMed, and CORD-19 papers. Our novel graph annotation schema incorporates not\nonly coarse-grained entity spans as nodes and relations as edges between them,\nbut also fine-grained attributes that modify entities and their relations, for\na total of 12,738 labels in the corpus. By including more label types and more\nthan twice the label density of previous datasets, SciClaim captures causal,\ncomparative, predictive, statistical, and proportional associations over\nexperimental variables along with their qualifications, subtypes, and evidence.\nWe extend work in transformer-based joint entity and relation extraction to\neffectively infer our schema, showing the promise of fine-grained knowledge\ngraphs in scientific claims and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_I/0/1/0/all/0/1\">Ian H. Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_S/0/1/0/all/0/1\">Scott E. Friedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable and Efficient MoE Training for Multitask Multilingual Models. (arXiv:2109.10465v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10465","description":"<p>The Mixture of Experts (MoE) models are an emerging class of sparsely\nactivated deep learning models that have sublinear compute costs with respect\nto their parameters. In contrast with dense models, the sparse architecture of\nMoE offers opportunities for drastically growing model size with significant\naccuracy gain while consuming much lower compute budget. However, supporting\nlarge scale MoE training also has its own set of system and modeling\nchallenges. To overcome the challenges and embrace the opportunities of MoE, we\nfirst develop a system capable of scaling MoE models efficiently to trillions\nof parameters. It combines multi-dimensional parallelism and heterogeneous\nmemory technologies harmoniously with MoE to empower 8x larger models on the\nsame hardware compared with existing work. Besides boosting system efficiency,\nwe also present new training methods to improve MoE sample efficiency and\nleverage expert pruning strategy to improve inference time efficiency. By\ncombining the efficient system and training methods, we are able to\nsignificantly scale up large multitask multilingual models for language\ngeneration which results in a great improvement in model accuracy. A model\ntrained with 10 billion parameters on 50 languages can achieve state-of-the-art\nperformance in Machine Translation (MT) and multilingual natural language\ngeneration tasks. The system support of efficient MoE training has been\nimplemented and open-sourced with the DeepSpeed library.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1\">Ammar Ahmad Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salinas_A/0/1/0/all/0/1\">Andres Felipe Cruz Salinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendy_A/0/1/0/all/0/1\">Amr Hendy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salience-Aware Event Chain Modeling for Narrative Understanding. (arXiv:2109.10475v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10475","description":"<p>Storytelling, whether via fables, news reports, documentaries, or memoirs,\ncan be thought of as the communication of interesting and related events that,\ntaken together, form a concrete process. It is desirable to extract the event\nchains that represent such processes. However, this extraction remains a\nchallenging problem. We posit that this is due to the nature of the texts from\nwhich chains are discovered. Natural language text interleaves a narrative of\nconcrete, salient events with background information, contextualization,\nopinion, and other elements that are important for a variety of necessary\ndiscourse and pragmatics acts but are not part of the principal chain of events\nbeing communicated. We introduce methods for extracting this principal chain\nfrom natural language text, by filtering away non-salient events and supportive\nsentences. We demonstrate the effectiveness of our methods at isolating\ncritical event chains by comparing their effect on downstream tasks. We show\nthat by pre-training large language models on our extracted chains, we obtain\nimprovements in two tasks that benefit from a clear understanding of event\nchains: narrative prediction and event-based temporal question answering. The\ndemonstrated improvements and ablative studies confirm that our extraction\nmethod isolates critical event chains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogueBERT: A Self-Supervised Learning based Dialogue Pre-training Encoder. (arXiv:2109.10480v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10480","description":"<p>With the rapid development of artificial intelligence, conversational bots\nhave became prevalent in mainstream E-commerce platforms, which can provide\nconvenient customer service timely. To satisfy the user, the conversational\nbots need to understand the user's intention, detect the user's emotion, and\nextract the key entities from the conversational utterances. However,\nunderstanding dialogues is regarded as a very challenging task. Different from\ncommon language understanding, utterances in dialogues appear alternately from\ndifferent roles and are usually organized as hierarchical structures. To\nfacilitate the understanding of dialogues, in this paper, we propose a novel\ncontextual dialogue encoder (i.e. DialogueBERT) based on the popular\npre-trained language model BERT. Five self-supervised learning pre-training\ntasks are devised for learning the particularity of dialouge utterances. Four\ndifferent input embeddings are integrated to catch the relationship between\nutterances, including turn embedding, role embedding, token embedding and\nposition embedding. DialogueBERT was pre-trained with 70 million dialogues in\nreal scenario, and then fine-tuned in three different downstream dialogue\nunderstanding tasks. Experimental results show that DialogueBERT achieves\nexciting results with 88.63% accuracy for intent recognition, 94.25% accuracy\nfor emotion recognition and 97.04% F1 score for named entity recognition, which\noutperforms several strong baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The NiuTrans Machine Translation Systems for WMT21. (arXiv:2109.10485v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10485","description":"<p>This paper describes NiuTrans neural machine translation systems of the WMT\n2021 news translation tasks. We made submissions to 9 language directions,\nincluding English$\\leftrightarrow$$\\{$Chinese, Japanese, Russian, Icelandic$\\}$\nand English$\\rightarrow$Hausa tasks. Our primary systems are built on several\neffective variants of Transformer, e.g., Transformer-DLCL, ODE-Transformer. We\nalso utilize back-translation, knowledge distillation, post-ensemble, and\niterative fine-tuning techniques to enhance the model performance further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuhan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Binghao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yingfeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yongyu Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zefan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuanjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chuanhao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1\">Yi Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Laohu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingnan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Canan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhongxiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the OBQA Context. (arXiv:2109.10497v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10497","description":"<p>In the open question answering (OBQA) task, how to select the relevant\ninformation from a large corpus is a crucial problem for reasoning and\ninference. Some datasets (e.g, HotpotQA) mainly focus on testing the model's\nreasoning ability at the sentence level. To overcome this challenge, many\nexisting frameworks use a deep learning model to select relevant passages and\nthen answer each question by matching a sentence in the corresponding passage.\nHowever, such frameworks require long inference time and fail to take advantage\nof the relationship between passages and sentences. In this work, we present a\nsimple yet effective framework to address these problems by jointly ranking\npassages and selecting sentences. We propose consistency and similarity\nconstraints to promote the correlation and interaction between passage ranking\nand sentence selection. In our experiments, we demonstrate that our framework\ncan achieve competitive results and outperform the baseline by 28\\% in terms of\nexact matching of relevant sentences on the HotpotQA dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperExpan: Taxonomy Expansion with Hyperbolic Representation Learning. (arXiv:2109.10500v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10500","description":"<p>Taxonomies are valuable resources for many applications, but the limited\ncoverage due to the expensive manual curation process hinders their general\napplicability. Prior works attempt to automatically expand existing taxonomies\nto improve their coverage by learning concept embeddings in Euclidean space,\nwhile taxonomies, inherently hierarchical, more naturally align with the\ngeometric properties of a hyperbolic space. In this paper, we present\nHyperExpan, a taxonomy expansion algorithm that seeks to preserve the structure\nof a taxonomy in a more expressive hyperbolic embedding space and learn to\nrepresent concepts and their relations with a Hyperbolic Graph Neural Network\n(HGNN). Specifically, HyperExpan leverages position embeddings to exploit the\nstructure of the existing taxonomies, and characterizes the concept profile\ninformation to support the inference on unseen concepts during training.\nExperiments show that our proposed HyperExpan outperforms baseline models with\nrepresentation learning in a Euclidean feature space and achieves\nstate-of-the-art performance on the taxonomy expansion benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tecnologica cosa: Modeling Storyteller Personalities in Boccaccio's Decameron. (arXiv:2109.10506v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10506","description":"<p>We explore Boccaccio's Decameron to see how digital humanities tools can be\nused for tasks that have limited data in a language no longer in contemporary\nuse: medieval Italian. We focus our analysis on the question: Do the different\nstorytellers in the text exhibit distinct personalities? To answer this\nquestion, we curate and release a dataset based on the authoritative edition of\nthe text. We use supervised classification methods to predict storytellers\nbased on the stories they tell, confirming the difficulty of the task, and\ndemonstrate that topic modeling can extract thematic storyteller \"profiles.\"\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">A. Feder Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniak_M/0/1/0/all/0/1\">Maria Antoniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migiel_M/0/1/0/all/0/1\">Marilyn Migiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Contextualized Document Representation. (arXiv:2109.10509v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10509","description":"<p>Several NLP tasks need the effective representation of text documents. Arora\net. al., 2017 demonstrate that simple weighted averaging of word vectors\nfrequently outperforms neural models. SCDV (Mekala et. al., 2017) further\nextends this from sentences to documents by employing soft and sparse\nclustering over pre-computed word vectors. However, both techniques ignore the\npolysemy and contextual character of words. In this paper, we address this\nissue by proposing SCDV+BERT(ctxd), a simple and effective unsupervised\nrepresentation that combines contextualized BERT (Devlin et al., 2019) based\nword embedding for word sense disambiguation with SCDV soft clustering\napproach. We show that our embeddings outperform original SCDV, pre-train BERT,\nand several other baselines on many classification datasets. We also\ndemonstrate our embeddings effectiveness on other tasks, such as concept\nmatching and sentence similarity. In addition, we show that SCDV+BERT(ctxd)\noutperforms fine-tune BERT and different embedding approaches in scenarios with\nlimited data and only few shots examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankur Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCM: A Fine-grained Comparison Model forMulti-turn Dialogue Reasoning. (arXiv:2109.10510v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10510","description":"<p>Despite the success of neural dialogue systems in achieving high performance\non the leader-board, they cannot meet users' requirements in practice, due to\ntheir poor reasoning skills. The underlying reason is that most neural dialogue\nmodels only capture the syntactic and semantic information, but fail to model\nthe logical consistency between the dialogue history and the generated\nresponse. Recently, a new multi-turn dialogue reasoning task has been proposed,\nto facilitate dialogue reasoning research. However, this task is challenging,\nbecause there are only slight differences between the illogical response and\nthe dialogue history. How to effectively solve this challenge is still worth\nexploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle\nthis problem. Inspired by human's behavior in reading comprehension, a\ncomparison mechanism is proposed to focus on the fine-grained differences in\nthe representation of each response candidate. Specifically, each candidate\nrepresentation is compared with the whole history to obtain a history\nconsistency representation. Furthermore, the consistency signals between each\ncandidate and the speaker's own history are considered to drive a model to\nprefer a candidate that is logically consistent with the speaker's history\nlogic. Finally, the above consistency representations are employed to output a\nranking list of the candidate responses for multi-turn dialogue reasoning.\nExperimental results on two public dialogue datasets show that our method\nobtains higher ranking scores than the baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards The Automatic Coding of Medical Transcripts to Improve Patient-Centered Communication. (arXiv:2109.10514v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10514","description":"<p>This paper aims to provide an approach for automatic coding of\nphysician-patient communication transcripts to improve patient-centered\ncommunication (PCC). PCC is a central part of high-quality health care. To\nimprove PCC, dialogues between physicians and patients have been recorded and\ntagged with predefined codes. Trained human coders have manually coded the\ntranscripts. Since it entails huge labor costs and poses possible human errors,\nautomatic coding methods should be considered for efficiency and effectiveness.\nWe adopted three machine learning algorithms (Na\\\"ive Bayes, Random Forest, and\nSupport Vector Machine) to categorize lines in transcripts into corresponding\ncodes. The result showed that there is evidence to distinguish the codes, and\nthis is considered to be sufficient for training of human annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gilchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1\">Julia Taylor Rayz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shields_C/0/1/0/all/0/1\">Cleveland G. Shields</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages. (arXiv:2109.10534v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10534","description":"<p>We explore the impact of leveraging the relatedness of languages that belong\nto the same family in NLP models using multilingual fine-tuning. We hypothesize\nand validate that multilingual fine-tuning of pre-trained language models can\nyield better performance on downstream NLP applications, compared to models\nfine-tuned on individual languages. A first of its kind detailed study is\npresented to track performance change as languages are added to a base language\nin a graded and greedy (in the sense of best boost of performance) manner;\nwhich reveals that careful selection of subset of related languages can\nsignificantly improve performance than utilizing all related languages. The\nIndo-Aryan (IA) language family is chosen for the study, the exact languages\nbeing Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script\nbarrier is crossed by simple rule-based transliteration of the text of all\nlanguages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL\nand two RoBERTa-based LMs, the last two being pre-trained by us. Low resource\nlanguages, such as Oriya and Punjabi, are found to be the largest beneficiaries\nof multilingual fine-tuning. Textual Entailment, Entity Classification, Section\nTitle Prediction, tasks of IndicGLUE and POS tagging form our test bed.\nCompared to monolingual fine tuning we get relative performance improvement of\nup to 150% in the downstream tasks. The surprise take-away is that for any\nlanguage there is a particular combination of other languages which yields the\nbest performance, and any additional language is in fact detrimental.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhamecha_T/0/1/0/all/0/1\">Tejas Indulal Dhamecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Rudra Murthy V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1\">Karthik Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing. (arXiv:2109.10540v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10540","description":"<p>Recent years pretrained language models (PLMs) hit a success on several\ndownstream tasks, showing their power on modeling language. To better\nunderstand and leverage what PLMs have learned, several techniques have emerged\nto explore syntactic structures entailed by PLMs. However, few efforts have\nbeen made to explore grounding capabilities of PLMs, which are also essential.\nIn this paper, we highlight the ability of PLMs to discover which token should\nbe grounded to which concept, if combined with our proposed\nerasing-then-awakening approach. Empirical studies on four datasets demonstrate\nthat our approach can awaken latent grounding which is understandable to human\nexperts, even if it is not exposed to such labels during training. More\nimportantly, our approach shows great potential to benefit downstream semantic\nparsing models. Taking text-to-SQL as a case study, we successfully couple our\napproach with two off-the-shelf parsers, obtaining an absolute improvement of\nup to 9.8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dejian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diarisation using Location tracking with agglomerative clustering. (arXiv:2109.10598v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10598","description":"<p>Previous works have shown that spatial location information can be\ncomplementary to speaker embeddings for a speaker diarisation task. However,\nthe models used often assume that speakers are fairly stationary throughout a\nmeeting. This paper proposes to relax this assumption, by explicitly modelling\nthe movements of speakers within an Agglomerative Hierarchical Clustering (AHC)\ndiarisation framework. Kalman filters, which track the locations of speakers,\nare used to compute log-likelihood ratios that contribute to the cluster\naffinity computations for the AHC merging and stopping decisions. Experiments\nshow that the proposed approach is able to yield improvements on a Microsoft\nrich meeting transcription task, compared to methods that do not use location\ninformation or that make stationarity assumptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jeremy H. M. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramovski_I/0/1/0/all/0/1\">Igor Abramovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset. (arXiv:2109.10604v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10604","description":"<p>While diverse question answering (QA) datasets have been proposed and\ncontributed significantly to the development of deep learning models for QA\ntasks, the existing datasets fall short in two aspects. First, we lack QA\ndatasets covering complex questions that involve answers as well as the\nreasoning processes to get the answers. As a result, the state-of-the-art QA\nresearch on numerical reasoning still focuses on simple calculations and does\nnot provide the mathematical expressions or evidences justifying the answers.\nSecond, the QA community has contributed much effort to improving the\ninterpretability of QA models. However, these models fail to explicitly show\nthe reasoning process, such as the evidence order for reasoning and the\ninteractions between different pieces of evidence. To address the above\nshortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset\nwith questions requiring numerical reasoning with compound mathematical\nexpressions. With NOAHQA, we develop an interpretable reasoning graph as well\nas the appropriate evaluation metric to measure the answer quality. We evaluate\nthe state-of-the-art QA models trained using existing QA datasets on NOAHQA and\nshow that the best among them can only achieve 55.5 exact match scores, while\nthe human performance is 89.7. We also present a new QA model for generating a\nreasoning graph where the reasoning graph metric still has a large gap compared\nwith that of humans, e.g., 28 scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sicheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVR: A test-bed for Visually Grounded Compositional Generalization with real images. (arXiv:2109.10613v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10613","description":"<p>While interest in models that generalize at test time to new compositions has\nrisen in recent years, benchmarks in the visually-grounded domain have thus far\nbeen restricted to synthetic images. In this work, we propose COVR, a new\ntest-bed for visually-grounded compositional generalization with real images.\nTo create COVR, we use real images annotated with scene graphs, and propose an\nalmost fully automatic procedure for generating question-answer pairs along\nwith a set of context images. COVR focuses on questions that require complex\nreasoning, including higher-order operations such as quantification and\naggregation. Due to the automatic generation process, COVR facilitates the\ncreation of compositional splits, where models at test time need to generalize\nto new concepts and compositions in a zero- or few-shot setting. We construct\ncompositional splits using COVR and demonstrate a myriad of cases where\nstate-of-the-art pre-trained language-and-vision models struggle to\ncompositionally generalize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1\">Ben Bogin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching and Controlling Global Semantics for Text Summarization. (arXiv:2109.10616v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10616","description":"<p>Recently, Transformer-based models have been proven effective in the\nabstractive summarization task by creating fluent and informative summaries.\nNevertheless, these models still suffer from the short-range dependency\nproblem, causing them to produce summaries that miss the key points of\ndocument. In this paper, we attempt to address this issue by introducing a\nneural topic model empowered with normalizing flow to capture the global\nsemantics of the document, which are then integrated into the summarization\nmodel. In addition, to avoid the overwhelming effect of global semantics on\ncontextualized representation, we introduce a mechanism to control the amount\nof global semantics supplied to the text generation module. Our method\noutperforms state-of-the-art summarization models on five common text\nsummarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and\nPubMed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Truc Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_T/0/1/0/all/0/1\">Tho Quan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Fair Representations. (arXiv:2109.10645v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10645","description":"<p>Trained classification models can unintentionally lead to biased\nrepresentations and predictions, which can reinforce societal preconceptions\nand stereotypes. Existing debiasing methods for classification models, such as\nadversarial training, are often expensive to train and difficult to optimise.\nIn this paper, we propose a method for mitigating bias in classifier training\nby incorporating contrastive learning, in which instances sharing the same\nclass label are encouraged to have similar representations, while instances\nsharing a protected attribute are forced further apart. In such a way our\nmethod learns representations which capture the task label in focused regions,\nwhile ensuring the protected attribute has diverse spread, and thus has limited\nimpact on prediction and thereby results in fairer models. Extensive\nexperimental results across four tasks in NLP and computer vision show (a) that\nour proposed method can achieve fairer representations and realises bias\nreductions compared with competitive baselines; and (b) that it can do so\nwithout sacrificing main task performance; (c) that it sets a new\nstate-of-the-art performance in one task despite reducing the bias. Finally,\nour method is conceptually simple and agnostic to network architectures, and\nincurs minimal additional compute cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_A/0/1/0/all/0/1\">Aili Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization. (arXiv:2109.10650v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10650","description":"<p>One of the most challenging aspects of current single-document news\nsummarization is that the summary often contains 'extrinsic hallucinations',\ni.e., facts that are not present in the source document, which are often\nderived via world knowledge. This causes summarization systems to act more like\nopen-ended language models tending to hallucinate facts that are erroneous. In\nthis paper, we mitigate this problem with the help of multiple supplementary\nresource documents assisting the task. We present a new dataset MiRANews and\nbenchmark existing summarization models. In contrast to multi-document\nsummarization, which addresses multiple events from several source documents,\nwe still aim at generating a summary for a single document. We show via data\nanalysis that it's not only the models which are to blame: more than 27% of\nfacts mentioned in the gold summaries of MiRANews are better grounded on\nassisting documents than in the main source articles. An error analysis of\ngenerated summaries from pretrained models fine-tuned on MiRANews reveals that\nthis has an even bigger effects on models: assisted summarization reduces 55%\nof hallucinations when compared to single-document summarization models trained\non the main article only. Our code and data are available at\nhttps://github.com/XinnuoXu/MiRANews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinnuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1\">Ioannis Konstas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10686","description":"<p>There remain many open questions pertaining to the scaling behaviour of\nTransformer architectures. These scaling decisions and findings can be\ncritical, as training runs often come with an associated computational cost\nwhich have both financial and/or environmental impact. The goal of this paper\nis to present scaling insights from pretraining and finetuning Transformers.\nWhile Kaplan et al. presents a comprehensive study of the scaling behaviour of\nTransformer language models, the scope is only on the upstream (pretraining)\nloss. Therefore, it is still unclear if these set of findings transfer to\ndownstream task within the context of the pretrain-finetune paradigm. The key\nfindings of this paper are as follows: (1) we show that aside from only the\nmodel size, model shape matters for downstream fine-tuning, (2) scaling\nprotocols operate differently at different compute regions, (3) widely adopted\nT5-base and T5-large sizes are Pareto-inefficient. To this end, we present\nimproved scaling protocols whereby our redesigned models achieve similar\ndownstream fine-tuning quality while having 50\\% fewer parameters and training\n40\\% faster compared to the widely adopted T5-base model. We publicly release\nover 100 pretrained checkpoints of different T5 configurations to facilitate\nfuture research and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulated Annealing for Emotional Dialogue Systems. (arXiv:2109.10715v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10715","description":"<p>Explicitly modeling emotions in dialogue generation has important\napplications, such as building empathetic personal companions. In this study,\nwe consider the task of expressing a specific emotion for dialogue generation.\nPrevious approaches take the emotion as an input signal, which may be ignored\nduring inference. We instead propose a search-based emotional dialogue system\nby simulated annealing (SA). Specifically, we first define a scoring function\nthat combines contextual coherence and emotional correctness. Then, SA\niteratively edits a general response and searches for a sentence with a higher\nscore, enforcing the presence of the desired emotion. We evaluate our system on\nthe NLPCC2017 dataset. Our proposed method shows 12% improvements in emotion\naccuracy compared with the previous state-of-the-art method, without hurting\nthe generation quality (measured by BLEU).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengzhang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Za&#xef;ane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Latency Incremental Text-to-Speech Synthesis with Distilled Context Prediction Network. (arXiv:2109.10724v1 [cs.SD])","link":"http://arxiv.org/abs/2109.10724","description":"<p>Incremental text-to-speech (TTS) synthesis generates utterances in small\nlinguistic units for the sake of real-time and low-latency applications. We\npreviously proposed an incremental TTS method that leverages a large\npre-trained language model to take unobserved future context into account\nwithout waiting for the subsequent segment. Although this method achieves\ncomparable speech quality to that of a method that waits for the future\ncontext, it entails a huge amount of processing for sampling from the language\nmodel at each time step. In this paper, we propose an incremental TTS method\nthat directly predicts the unobserved future context with a lightweight model,\ninstead of sampling words from the large-scale language model. We perform\nknowledge distillation from a GPT2-based context prediction network into a\nsimple recurrent model by minimizing a teacher-student loss defined between the\ncontext embedding vectors of those models. Experimental results show that the\nproposed method requires about ten times less inference time to achieve\ncomparable synthetic speech quality to that of our previous method, and it can\nperform incremental synthesis much faster than the average speaking speed of\nhuman English speakers, demonstrating the availability of our method to\nreal-time applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeki_T/0/1/0/all/0/1\">Takaaki Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamichi_S/0/1/0/all/0/1\">Shinnosuke Takamichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saruwatari_H/0/1/0/all/0/1\">Hiroshi Saruwatari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing. (arXiv:2109.10847v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10847","description":"<p>Recent progress in the Natural Language Processing domain has given us\nseveral State-of-the-Art (SOTA) pretrained models which can be finetuned for\nspecific tasks. These large models with billions of parameters trained on\nnumerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In\nthis paper, we discuss the need for a benchmark for cost and time effective\nsmaller models trained on a single GPU. This will enable researchers with\nresource constraints experiment with novel and innovative ideas on\ntokenization, pretraining tasks, architecture, fine tuning methods etc. We set\nup Small-Bench NLP, a benchmark for small efficient neural language models\ntrained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks\non the publicly available GLUE datasets and a leaderboard to track the progress\nof the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture\nachieves an average score of 81.53 which is comparable to that of BERT-Base's\n82.20 (110M parameters). Our models, code and leaderboard are available at\nhttps://github.com/smallbenchnlp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanakarajan_K/0/1/0/all/0/1\">Kamal Raj Kanakarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundumani_B/0/1/0/all/0/1\">Bhuvana Kundumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankarasubbu_M/0/1/0/all/0/1\">Malaikannan Sankarasubbu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10852","description":"<p>This paper presents Pix2Seq, a simple and generic framework for object\ndetection. Unlike existing approaches that explicitly integrate prior knowledge\nabout the task, we simply cast object detection as a language modeling task\nconditioned on the observed pixel inputs. Object descriptions (e.g., bounding\nboxes and class labels) are expressed as sequences of discrete tokens, and we\ntrain a neural net to perceive the image and generate the desired sequence. Our\napproach is based mainly on the intuition that if a neural net knows about\nwhere and what the objects are, we just need to teach it how to read them out.\nBeyond the use of task-specific data augmentations, our approach makes minimal\nassumptions about the task, yet it achieves competitive results on the\nchallenging COCO dataset, compared to highly specialized and well optimized\ndetection algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Saurabh Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BFClass: A Backdoor-free Text Classification Framework. (arXiv:2109.10855v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10855","description":"<p>Backdoor attack introduces artificial vulnerabilities into the model by\npoisoning a subset of the training data via injecting triggers and modifying\nlabels. Various trigger design strategies have been explored to attack text\nclassifiers, however, defending such attacks remains an open problem. In this\nwork, we propose BFClass, a novel efficient backdoor-free training framework\nfor text classification. The backbone of BFClass is a pre-trained discriminator\nthat predicts whether each token in the corrupted input was replaced by a\nmasked language model. To identify triggers, we utilize this discriminator to\nlocate the most suspicious token from each training sample and then distill a\nconcise set by considering their association strengths with particular labels.\nTo recognize the poisoned subset, we examine the training samples with these\nidentified triggers as the most suspicious token, and check if removing the\ntrigger will change the poisoned model's prediction. Extensive experiments\ndemonstrate that BFClass can identify all the triggers, remove 95% poisoned\ntraining samples with very limited false alarms, and achieve almost the same\nperformance as the models trained on the benign training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data. (arXiv:2109.10856v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10856","description":"<p>Existing text classification methods mainly focus on a fixed label set,\nwhereas many real-world applications require extending to new fine-grained\nclasses as the number of samples per label increases. To accommodate such\nrequirements, we introduce a new problem called coarse-to-fine grained\nclassification, which aims to perform fine-grained classification on coarsely\nannotated data. Instead of asking for new fine-grained human annotations, we\nopt to leverage label surface names as the only human guidance and weave in\nrich pre-trained generative language models into the iterative weak supervision\nstrategy. Specifically, we first propose a label-conditioned finetuning\nformulation to attune these generators for our task. Furthermore, we devise a\nregularization objective based on the coarse-fine label constraints derived\nfrom our problem setting, giving us even further improvements over the prior\nformulation. Our framework uses the fine-tuned generative models to sample\npseudo-training data for training the classifier, and bootstraps on real\nunlabeled data for model refinement. Extensive experiments and case studies on\ntwo real-world datasets demonstrate superior performance over SOTA zero-shot\nclassification baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Right Buttons: Adversarial Evaluation of Quality Estimation. (arXiv:2109.10859v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10859","description":"<p>Current Machine Translation (MT) systems achieve very good results on a\ngrowing variety of language pairs and datasets. However, they are known to\nproduce fluent translation outputs that can contain important meaning errors,\nthus undermining their reliability in practice. Quality Estimation (QE) is the\ntask of automatically assessing the performance of MT systems at test time.\nThus, in order to be useful, QE systems should be able to detect such errors.\nHowever, this ability is yet to be tested in the current evaluation practices,\nwhere QE systems are assessed only in terms of their correlation with human\njudgements. In this work, we bridge this gap by proposing a general methodology\nfor adversarial testing of QE for MT. First, we show that despite a high\ncorrelation with human judgements achieved by the recent SOTA, certain types of\nmeaning errors are still problematic for QE to detect. Second, we show that on\naverage, the ability of a given model to discriminate between\nmeaning-preserving and meaning-altering perturbations is predictive of its\noverall performance, thus potentially allowing for comparing QE systems without\nrelying on manual quality annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Or&#x103;san</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursively Summarizing Books with Human Feedback. (arXiv:2109.10862v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10862","description":"<p>A major challenge for scaling machine learning is training models to perform\ntasks that are very difficult or time-consuming for humans to evaluate. We\npresent progress on this problem on the task of abstractive summarization of\nentire fiction novels. Our method combines learning from human feedback with\nrecursive task decomposition: we use models trained on smaller parts of the\ntask to assist humans in giving feedback on the broader task. We collect a\nlarge volume of demonstrations and comparisons from human labelers, and\nfine-tune GPT-3 using behavioral cloning and reward modeling to do\nsummarization recursively. At inference time, the model first summarizes small\nsections of the book and then recursively summarizes these summaries to produce\na summary of the entire book. Our human labelers are able to supervise and\nevaluate the models quickly, despite not having read the entire books\nthemselves. Our resulting model generates sensible summaries of entire books,\neven matching the quality of human-written summaries in a few cases ($\\sim5\\%$\nof books). We achieve state-of-the-art results on the recent BookSum dataset\nfor book-length summarization. A zero-shot question-answering model using these\nsummaries achieves state-of-the-art results on the challenging NarrativeQA\nbenchmark for answering questions about books and movie scripts. We release\ndatasets of samples from our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1\">Daniel M. Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiennon_N/0/1/0/all/0/1\">Nissan Stiennon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_R/0/1/0/all/0/1\">Ryan Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1\">Paul Christiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTEANN: Estimating the Transparency of Orthographies with an Artificial Neural Network. (arXiv:1912.13321v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1912.13321","description":"<p>To transcribe spoken language to written medium, most alphabets enable an\nunambiguous sound-to-letter rule. However, some writing systems have distanced\nthemselves from this simple concept and little work exists in Natural Language\nProcessing (NLP) on measuring such distance. In this study, we use an\nArtificial Neural Network (ANN) model to evaluate the transparency between\nwritten words and their pronunciation, hence its name Orthographic Transparency\nEstimation with an ANN (OTEANN). Based on datasets derived from Wikimedia\ndictionaries, we trained and tested this model to score the percentage of\ncorrect predictions in phoneme-to-grapheme and grapheme-to-phoneme translation\ntasks. The scores obtained on 17 orthographies were in line with the\nestimations of other studies. Interestingly, the model also provided insight\ninto typical mistakes made by learners who only consider the phonemic rule in\nreading and writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjou_X/0/1/0/all/0/1\">Xavier Marjou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Wait, I'm Still Talking!\" Predicting the Dialogue Interaction Behavior Using Imagine-Then-Arbitrate Model. (arXiv:2002.09616v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2002.09616","description":"<p>Producing natural and accurate responses like human beings is the ultimate\ngoal of intelligent dialogue agents. So far, most of the past works concentrate\non selecting or generating one pertinent and fluent response according to\ncurrent query and its context. These models work on a one-to-one environment,\nmaking one response to one utterance each round. However, in real human-human\nconversations, human often sequentially sends several short messages for\nreadability instead of a long message in one turn. Thus messages will not end\nwith an explicit ending signal, which is crucial for agents to decide when to\nreply. So the first step for an intelligent dialogue agent is not replying but\ndeciding if it should reply at the moment. To address this issue, in this\npaper, we propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to\nhelp the agent decide whether to wait or to make a response directly. Our\nmethod has two imaginator modules and an arbitrator module. The two imaginators\nwill learn the agent's and user's speaking style respectively, generate\npossible utterances as the input of the arbitrator, combining with dialogue\nhistory. And the arbitrator decides whether to wait or to make a response to\nthe user directly. To verify the performance and effectiveness of our method,\nwe prepared two dialogue datasets and compared our approach with several\npopular models. Experimental results show that our model performs well on\naddressing ending prediction issue and outperforms baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xiaoming Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1\">Feng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00033","description":"<p>With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic has been\ndeclared one of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Addressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nTo address this gap, we release a large dataset of 16K manually annotated\ntweets for fine-grained disinformation analysis that (i) focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society, and (iii) covers Arabic,\nBulgarian, Dutch, and English. Finally, we show strong evaluation results using\npretrained Transformers, thus confirming the practical utility of the dataset\nin monolingual vs. multilingual, and single task vs. multitask settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1\">Alex Nikolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Homaid_A/0/1/0/all/0/1\">Abdulaziz Al-Homaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaghouani_W/0/1/0/all/0/1\">Wajdi Zaghouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caselli_T/0/1/0/all/0/1\">Tommaso Caselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danoe_G/0/1/0/all/0/1\">Gijs Danoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolk_F/0/1/0/all/0/1\">Friso Stolk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruntink_B/0/1/0/all/0/1\">Britt Bruntink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predict-then-Decide: A Predictive Approach for Wait or Answer Task in Dialogue Systems. (arXiv:2005.13119v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.13119","description":"<p>Different people have different habits of describing their intents in\nconversations. Some people tend to deliberate their intents in several\nsuccessive utterances, i.e., they use several consistent messages for\nreadability instead of a long sentence to express their question. This creates\na predicament faced by the application of dialogue systems, especially in\nreal-world industry scenarios, in which the dialogue system is unsure whether\nit should answer the query of user immediately or wait for further\nsupplementary input. Motivated by such an interesting predicament, we define a\nnovel Wait-or-Answer task for dialogue systems. We shed light on a new research\ntopic about how the dialogue system can be more intelligent to behave in this\nWait-or-Answer quandary. Further, we propose a predictive approach named\nPredict-then-Decide (PTD) to tackle this Wait-or-Answer task. More\nspecifically, we take advantage of a decision model to help the dialogue system\ndecide whether to wait or answer. The decision of decision model is made with\nthe assistance of two ancillary prediction models: a user prediction and an\nagent prediction. The user prediction model tries to predict what the user\nwould supplement and uses its prediction to persuade the decision model that\nthe user has some information to add, so the dialogue system should wait. The\nagent prediction model tries to predict the answer of the dialogue system and\nconvince the decision model that it is a superior choice to answer the query of\nuser immediately since the input of user has come to an end. We conduct our\nexperiments on two real-life scenarios and three public datasets. Experimental\nresults on five datasets show our proposed PTD approach significantly\noutperforms the existing models in solving this Wait-or-Answer problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xiaoming Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1\">Feng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIPFA: Generating IPA Pronunciation from Audio. (arXiv:2006.07573v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.07573","description":"<p>Transcribing spoken audio samples into the International Phonetic Alphabet\n(IPA) has long been reserved for experts. In this study, we examine the use of\nan Artificial Neural Network (ANN) model to automatically extract the IPA\nphonemic pronunciation of a word based on its audio pronunciation, hence its\nname Generating IPA Pronunciation From Audio (GIPFA). Based on the French\nWikimedia dictionary, we trained our model which then correctly predicted 75%\nof the IPA pronunciations tested. Interestingly, by studying inference errors,\nthe model made it possible to highlight possible errors in the dataset as well\nas to identify the closest phonemes in French.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjou_X/0/1/0/all/0/1\">Xavier Marjou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures. (arXiv:2007.08970v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.08970","description":"<p>While mainstream machine learning methods are known to have limited ability\nto compositionally generalize, new architectures and techniques continue to be\nproposed to address this limitation. We investigate state-of-the-art techniques\nand architectures in order to assess their effectiveness in improving\ncompositional generalization in semantic parsing tasks based on the SCAN and\nCFQ datasets. We show that masked language model (MLM) pre-training rivals\nSCAN-inspired architectures on primitive holdout splits. On a more complex\ncompositional task, we show that pre-training leads to significant improvements\nin performance vs. comparable non-pre-trained models, whereas architectures\nproposed to encourage compositional generalization on SCAN or in the area of\nalgorithm learning fail to lead to significant improvements. We establish a new\nstate of the art on the CFQ compositional generalization benchmark using MLM\npre-training together with an intermediate representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Furrer_D/0/1/0/all/0/1\">Daniel Furrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zee_M/0/1/0/all/0/1\">Marc van Zee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scales_N/0/1/0/all/0/1\">Nathan Scales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharli_N/0/1/0/all/0/1\">Nathanael Sch&#xe4;rli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Sound Change: Deep and Iterative Learning, Convolutional Neural Networks, and Language Change. (arXiv:2011.05463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.05463","description":"<p>This paper proposes a framework for modeling sound change that combines deep\nlearning and iterative learning. Acquisition and transmission of speech is\nmodeled by training generations of Generative Adversarial Networks (GANs) on\nunannotated raw speech data. The paper argues that several properties of sound\nchange emerge from the proposed architecture. GANs (Goodfellow et al. 2014\n<a href=\"/abs/1406.2661\">arXiv:1406.2661</a>, Donahue et al. 2019 <a href=\"/abs/1705.07904\">arXiv:1705.07904</a>) are uniquely appropriate\nfor modeling language change because the networks are trained on raw\nunsupervised acoustic data, contain no language-specific features and, as\nargued in Begu\\v{s} (2020 <a href=\"/abs/2006.03965\">arXiv:2006.03965</a>), encode phonetic and phonological\nrepresentations in their latent space and generate linguistically informative\ninnovative data. The first generation of networks is trained on the relevant\nsequences in human speech from TIMIT. The subsequent generations are not\ntrained on TIMIT, but on generated outputs from the previous generation and\nthus start learning from each other in an iterative learning task. The initial\nallophonic distribution is progressively being lost with each generation,\nlikely due to pressures from the global distribution of aspiration in the\ntraining data. The networks show signs of a gradual shift in phonetic targets\ncharacteristic of a gradual phonetic sound change. At endpoints, the outputs\nsuperficially resemble a phonological change -- rule loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Transferability of Adversarial Attacksagainst Neural Text Classifier. (arXiv:2011.08558v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.08558","description":"<p>Deep neural networks are vulnerable to adversarial attacks, where a small\nperturbation to an input alters the model prediction. In many cases, malicious\ninputs intentionally crafted for one model can fool another model. In this\npaper, we present the first study to systematically investigate the\ntransferability of adversarial examples for text classification models and\nexplore how various factors, including network architecture, tokenization\nscheme, word embedding, and model capacity, affect the transferability of\nadversarial examples. Based on these studies, we propose a genetic algorithm to\nfind an ensemble of models that can be used to induce adversarial examples to\nfool almost all existing models. Such adversarial examples reflect the defects\nof the learning process and the data bias in the training set. Finally, we\nderive word replacement rules that can be used for model diagnostics from these\nadversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liping Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach. (arXiv:2102.10242v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.10242","description":"<p>Reliable automatic evaluation of dialogue systems under an interactive\nenvironment has long been overdue. An ideal environment for evaluating dialog\nsystems, also known as the Turing test, needs to involve human interaction,\nwhich is usually not affordable for large-scale experiments. Though researchers\nhave attempted to use metrics (e.g., perplexity, BLEU) in language generation\ntasks or some model-based reinforcement learning methods (e.g., self-play\nevaluation) for automatic evaluation, these methods only show a very weak\ncorrelation with the actual human evaluation in practice. To bridge such a gap,\nwe propose a new framework named ENIGMA for estimating human evaluation scores\nbased on recent advances of off-policy evaluation in reinforcement learning.\nENIGMA only requires a handful of pre-collected experience data, and therefore\ndoes not involve human interaction with the target policy during the\nevaluation, making automatic evaluations feasible. More importantly, ENIGMA is\nmodel-free and agnostic to the behavior policies for collecting the experience\ndata (see details in Section 2), which significantly alleviates the technical\ndifficulties of modeling complex dialogue environments and human behaviors. Our\nexperiments show that ENIGMA significantly outperforms existing methods in\nterms of correlation with human evaluation scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Classifiers: Promises, Shortcomings, and Advances. (arXiv:2102.12452v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12452","description":"<p>Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple -- a classifier is trained to predict some\nlinguistic property from a model's representations -- and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological limitations of this approach. This article\ncritically reviews the probing classifiers framework, highlighting their\npromises, shortcomings, and advances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media. (arXiv:2104.05893v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05893","description":"<p>Online misinformation is a prevalent societal issue, with adversaries relying\non tools ranging from cheap fakes to sophisticated deep fakes. We are motivated\nby the threat scenario where an image is used out of context to support a\ncertain narrative. While some prior datasets for detecting image-text\ninconsistency generate samples via text manipulation, we propose a dataset\nwhere both image and text are unmanipulated but mismatched. We introduce\nseveral strategies for automatically retrieving convincing images for a given\ncaption, capturing cases with inconsistent entities or semantic context. Our\nlarge-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates\nthat machine-driven image repurposing is now a realistic threat, and (2)\nprovides samples that represent challenging instances of mismatch between text\nand image in news that are able to mislead humans. We benchmark several\nstate-of-the-art multimodal models on our dataset and analyze their performance\nacross different pretraining domains and visual backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Grace Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attention Free Transformer. (arXiv:2105.14103v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14103","description":"<p>We introduce Attention Free Transformer (AFT), an efficient variant of\nTransformers that eliminates the need for dot product self attention. In an AFT\nlayer, the key and value are first combined with a set of learned position\nbiases, the result of which is multiplied with the query in an element-wise\nfashion. This new operation has a memory complexity linear w.r.t. both the\ncontext size and the dimension of features, making it compatible to both large\ninput and model sizes. We also introduce AFT-local and AFT-conv, two model\nvariants that take advantage of the idea of locality and spatial weight sharing\nwhile maintaining global connectivity. We conduct extensive experiments on two\nautoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image\nrecognition task (ImageNet-1K classification). We show that AFT demonstrates\ncompetitive performance on all the benchmarks, while providing excellent\nefficiency at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbott_W/0/1/0/all/0/1\">Walter Talbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_N/0/1/0/all/0/1\">Nitish Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hanlin Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2107.07634","description":"<p>Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\nachieves a 12% relative reduction in the false reject ratios compared to the\nconventional multi-task learning with split branches and a bi-directional long\nshort-team memory decoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1\">Takuya Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Anmol Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1\">Chandra Dhir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04699","description":"<p>While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2109.05184","description":"<p>Internet memes have become powerful means to transmit political,\npsychological, and socio-cultural ideas. Although memes are typically humorous,\nrecent days have witnessed an escalation of harmful memes used for trolling,\ncyberbullying, and abuse. Detecting such memes is challenging as they can be\nhighly satirical and cryptic. Moreover, while previous work has focused on\nspecific aspects of memes such as hate speech and propaganda, there has been\nlittle work on harm in general. Here, we aim to bridge this gap. We focus on\ntwo tasks: (i)detecting harmful memes, and (ii)identifying the social entities\nthey target. We further extend a recently released HarMeme dataset, which\ncovered COVID-19, with additional memes and a new topic: US politics. To solve\nthese tasks, we propose MOMENTA (MultimOdal framework for detecting harmful\nMemEs aNd Their tArgets), a novel multimodal deep neural network that uses\nglobal and local perspectives to detect harmful memes. MOMENTA systematically\nanalyzes the local and the global perspective of the input meme (in both\nmodalities) and relates it to the background context. MOMENTA is interpretable\nand generalizable, and our experiments show that it outperforms several strong\nrivaling approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyword Extraction for Improved Document Retrieval in Conversational Search. (arXiv:2109.05979v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05979","description":"<p>Recent research has shown that mixed-initiative conversational search, based\non the interaction between users and computers to clarify and improve a query,\nprovides enormous advantages. Nonetheless, incorporating additional information\nprovided by the user from the conversation poses some challenges. In fact,\nfurther interactions could confuse the system as a user might use words\nirrelevant to the information need but crucial for correct sentence\nconstruction in the context of multi-turn conversations. To this aim, in this\npaper, we have collected two conversational keyword extraction datasets and\npropose an end-to-end document retrieval pipeline incorporating them.\nFurthermore, we study the performance of two neural keyword extraction models,\nnamely, BERT and sequence to sequence, in terms of extraction accuracy and\nhuman annotation. Finally, we study the effect of keyword extraction on the\nend-to-end neural IR performance and show that our approach beats\nstate-of-the-art IR models. We make the two datasets publicly available to\nfoster research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borisov_O/0/1/0/all/0/1\">Oleg Borisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1\">Mohammad Aliannejadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crestani_F/0/1/0/all/0/1\">Fabio Crestani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense-Focused Dialogues for Response Generation: An Empirical Study. (arXiv:2109.06427v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06427","description":"<p>Smooth and effective communication requires the ability to perform latent or\nexplicit commonsense inference. Prior commonsense reasoning benchmarks (such as\nSocialIQA and CommonsenseQA) mainly focus on the discriminative task of\nchoosing the right answer from a set of candidates, and do not involve\ninteractive language generation as in dialogue. Moreover, existing dialogue\ndatasets do not explicitly focus on exhibiting commonsense as a facet. In this\npaper, we present an empirical study of commonsense in dialogue response\ngeneration. We first auto-extract commonsensical dialogues from existing\ndialogue datasets by leveraging ConceptNet, a commonsense knowledge graph.\nFurthermore, building on social contexts/situations in SocialIQA, we collect a\nnew dialogue dataset with 25K dialogues aimed at exhibiting social commonsense\nin an interactive setting. We evaluate response generation models trained using\nthese datasets and find that models trained on both extracted and our collected\ndata produce responses that consistently exhibit more commonsense than\nbaselines. Finally we propose an approach for automatic evaluation of\ncommonsense that relies on features derived from ConceptNet and pre-trained\nlanguage and dialog models, and show reasonable correlation with human\nevaluation of responses' commonsense quality. We are releasing a subset of our\ncollected data, Commonsense-Dialogues, containing about 11K dialogs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Clinical Information Extraction with Transferred Contextual Embeddings. (arXiv:2109.07243v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07243","description":"<p>The Bidirectional Encoder Representations from Transformers (BERT) model has\nachieved the state-of-the-art performance for many natural language processing\n(NLP) tasks. Yet, limited research has been contributed to studying its\neffectiveness when the target domain is shifted from the pre-training corpora,\nfor example, for biomedical or clinical NLP applications. In this paper, we\napplied it to a widely studied a hospital information extraction (IE) task and\nanalyzed its performance under the transfer learning setting. Our application\nbecame the new state-of-the-art result by a clear margin, compared with a range\nof existing IE models. Specifically, on this nursing handover data set, the\nmacro-average F1 score from our model was 0.438, whilst the previous best deep\nlearning models had 0.416. In conclusion, we showed that BERT based\npre-training models can be transferred to health-related documents under mild\nconditions and with a proper fine-tuning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zimin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suominen_H/0/1/0/all/0/1\">Hanna Suominen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08818","description":"<p>Incorporating lexical knowledge into deep learning models has been proved to\nbe very effective for sequence labeling tasks. However, previous works commonly\nhave difficulty dealing with large-scale dynamic lexicons which often cause\nexcessive matching noise and problems of frequent updates. In this paper, we\npropose DyLex, a plug-in lexicon incorporation approach for BERT based sequence\nlabeling tasks. Instead of leveraging embeddings of words in the lexicon as in\nconventional methods, we adopt word-agnostic tag embeddings to avoid\nre-training the representation while updating the lexicon. Moreover, we employ\nan effective supervised lexical knowledge denoising method to smooth out\nmatching noise. Finally, we introduce a col-wise attention based knowledge\nfusion mechanism to guarantee the pluggability of the proposed framework.\nExperiments on ten datasets of three tasks show that the proposed framework\nachieves new SOTA, even with very large scale lexicons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1\">Guang-Yuan Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Joint Intent Detection and Slot Filling via Higher-order Attention. (arXiv:2109.08890v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08890","description":"<p>Intent detection (ID) and Slot filling (SF) are two major tasks in spoken\nlanguage understanding (SLU). Recently, attention mechanism has been shown to\nbe effective in jointly optimizing these two tasks in an interactive manner.\nHowever, latest attention-based works concentrated only on the first-order\nattention design, while ignoring the exploration of higher-order attention\nmechanisms. In this paper, we propose a BiLinear attention block, which\nleverages bilinear pooling to simultaneously exploit both the contextual and\nchannel-wise bilinear attention distributions to capture the second-order\ninteractions between the input intent or slot features. Higher and even\ninfinity order interactions are built by stacking numerous blocks and assigning\nExponential Linear Unit (ELU) to blocks. Before the decoding stage, we\nintroduce the Dynamic Feature Fusion Layer to implicitly fuse intent and slot\ninformation in a more effective way. Technically, instead of simply\nconcatenating intent and slot features, we first compute two correlation\nmatrices to weight on two features. Furthermore, we present Higher-order\nAttention Network for the SLU tasks. Experiments on two benchmark datasets show\nthat our approach yields improvements compared with the state-of-the-art\napproach. We also provide discussion to demonstrate the effectiveness of the\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongsheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10282","description":"<p>Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"An Ultra-Fast Method for Simulation of Realistic Ultrasound Images. (arXiv:2109.10353v1 [eess.IV])","link":"http://arxiv.org/abs/2109.10353","description":"<p>Convolutional neural networks (CNNs) have attracted a rapidly growing\ninterest in a variety of different processing tasks in the medical ultrasound\ncommunity. However, the performance of CNNs is highly reliant on both the\namount and fidelity of the training data. Therefore, scarce data is almost\nalways a concern, particularly in the medical field, where clinical data is not\neasily accessible. The utilization of synthetic data is a popular approach to\naddress this challenge. However, but simulating a large number of images using\npackages such as Field II is time-consuming, and the distribution of simulated\nimages is far from that of the real images. Herein, we introduce a novel\nultra-fast ultrasound image simulation method based on the Fourier transform\nand evaluate its performance in a lesion segmentation task. We demonstrate that\ndata augmentation using the images generated by the proposed method\nsubstantially outperforms Field II in terms of Dice similarity coefficient,\nwhile the simulation is almost 36000 times faster (both on CPU).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sharifzadeh_M/0/1/0/all/0/1\">Mostafa Sharifzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benali_H/0/1/0/all/0/1\">Habib Benali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust marginalization of baryonic effects for cosmological inference at the field level. (arXiv:2109.10360v1 [astro-ph.CO])","link":"http://arxiv.org/abs/2109.10360","description":"<p>We train neural networks to perform likelihood-free inference from\n$(25\\,h^{-1}{\\rm Mpc})^2$ 2D maps containing the total mass surface density\nfrom thousands of hydrodynamic simulations of the CAMELS project. We show that\nthe networks can extract information beyond one-point functions and power\nspectra from all resolved scales ($\\gtrsim 100\\,h^{-1}{\\rm kpc}$) while\nperforming a robust marginalization over baryonic physics at the field level:\nthe model can infer the value of $\\Omega_{\\rm m} (\\pm 4\\%)$ and $\\sigma_8 (\\pm\n2.5\\%)$ from simulations completely different to the ones used to train it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1\">Francisco Villaescusa-Navarro</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Genel_S/0/1/0/all/0/1\">Shy Genel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Angles_Alcazar_D/0/1/0/all/0/1\">Daniel Angles-Alcazar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Spergel_D/0/1/0/all/0/1\">David N. Spergel</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wandelt_B/0/1/0/all/0/1\">Benjamin Wandelt</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Thiele_L/0/1/0/all/0/1\">Leander Thiele</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nicola_A/0/1/0/all/0/1\">Andrina Nicola</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Matilla_J/0/1/0/all/0/1\">Jose Manuel Zorrilla Matilla</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Shao_H/0/1/0/all/0/1\">Helen Shao</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hassan_S/0/1/0/all/0/1\">Sultan Hassan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Narayanan_D/0/1/0/all/0/1\">Desika Narayanan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dave_R/0/1/0/all/0/1\">Romeel Dave</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Vogelsberger_M/0/1/0/all/0/1\">Mark Vogelsberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coast Sargassum Level Estimation from Smartphone Pictures. (arXiv:2109.10390v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10390","description":"<p>Since 2011, significant and atypical arrival of two species of surface\ndwelling algae, Sargassum natans and Sargassum Fluitans, have been detected in\nthe Mexican Caribbean. This massive accumulation of algae has had a great\nenvironmental and economic impact. Therefore, for the government, ecologists,\nand local businesses, it is important to keep track of the amount of sargassum\nthat arrives on the Caribbean coast. High-resolution satellite imagery is\nexpensive or may be time delayed. Therefore, we propose to estimate the amount\nof sargassum based on ground-level smartphone photographs. From the computer\nvision perspective, the problem is quite difficult since no information about\nthe 3D world is provided, in consequence, we have to model it as a\nclassification problem, where a set of five labels define the amount. For this\npurpose, we have built a dataset with more than one thousand examples from\npublic forums such as Facebook or Instagram and we have tested several\nstate-of-the-art convolutional networks. As a result, the VGG network trained\nunder fine-tuning showed the best performance. Even though the reached accuracy\ncould be improved with more examples, the current prediction distribution is\nnarrow, so the predictions are adequate for keeping a record and taking quick\necological actions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valeria_U/0/1/0/all/0/1\">Uriarte-Arcia Abril Valeria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_V/0/1/0/all/0/1\">Vasquez-Gomez Juan Irving</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hind_T/0/1/0/all/0/1\">Taud Hind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andres_G/0/1/0/all/0/1\">Garcia-Floriano Andres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elias_V/0/1/0/all/0/1\">Ventura-Molina Elias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Real-Time Facial Analysis System. (arXiv:2109.10393v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10393","description":"<p>Facial analysis is an active research area in computer vision, with many\npractical applications. Most of the existing studies focus on addressing one\nspecific task and maximizing its performance. For a complete facial analysis\nsystem, one needs to solve these tasks efficiently to ensure a smooth\nexperience. In this work, we present a system-level design of a real-time\nfacial analysis system. With a collection of deep neural networks for object\ndetection, classification, and regression, the system recognizes age, gender,\nfacial expression, and facial similarity for each person that appears in the\ncamera view. We investigate the parallelization and interplay of individual\ntasks. Results on common off-the-shelf architecture show that the system's\naccuracy is comparable to the state-of-the-art methods, and the recognition\nspeed satisfies real-time requirements. Moreover, we propose a multitask\nnetwork for jointly predicting the first three attributes, i.e., age, gender,\nand facial expression. Source code and trained models are available at\nhttps://github.com/mahehu/TUT-live-age-estimator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhikari_B/0/1/0/all/0/1\">Bishwo Adhikari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1\">Xingyang Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huttunen_H/0/1/0/all/0/1\">Heikki Huttunen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The First Vision For Vitals (V4V) Challenge for Non-Contact Video-Based Physiological Estimation. (arXiv:2109.10471v1 [cs.CY])","link":"http://arxiv.org/abs/2109.10471","description":"<p>Telehealth has the potential to offset the high demand for help during public\nhealth emergencies, such as the COVID-19 pandemic. Remote Photoplethysmography\n(rPPG) - the problem of non-invasively estimating blood volume variations in\nthe microvascular tissue from video - would be well suited for these\nsituations. Over the past few years a number of research groups have made rapid\nadvances in remote PPG methods for estimating heart rate from digital video and\nobtained impressive results. How these various methods compare in naturalistic\nconditions, where spontaneous behavior, facial expressions, and illumination\nchanges are present, is relatively unknown. To enable comparisons among\nalternative methods, the 1st Vision for Vitals Challenge (V4V) presented a\nnovel dataset containing high-resolution videos time-locked with varied\nphysiological signals from a diverse population. In this paper, we outline the\nevaluation protocol, the data used, and the results. V4V is to be held in\nconjunction with the 2021 International Conference on Computer Vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Revanur_A/0/1/0/all/0/1\">Ambareesh Revanur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciftci_U/0/1/0/all/0/1\">Umur A. Ciftci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lijun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeni_L/0/1/0/all/0/1\">Laszlo A. Jeni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotor Localization and Phase Mapping of Cardiac Excitation Waves using Deep Neural Networks. (arXiv:2109.10472v1 [physics.med-ph])","link":"http://arxiv.org/abs/2109.10472","description":"<p>The analysis of electrical impulse phenomena in cardiac muscle tissue is\nimportant for the diagnosis of heart rhythm disorders and other cardiac\npathophysiology. Cardiac mapping techniques acquire numerous local temporal\nmeasurements and combine them to visualize the spread of electrophysiological\nwave phenomena across the heart surface. However, low spatial resolutions,\nsparse measurement locations, noise and other artifacts make it challenging to\naccurately visualize spatio-temporal activity. For instance, electro-anatomical\ncatheter mapping is severely limited by the sparsity of the measurements and\noptical mapping is prone to noise and motion artifacts. In the past, several\napproaches have been proposed to obtain more reliable maps from noisy or sparse\nmapping data. Here, we demonstrate that deep learning can be used to compute\nphase maps and detect phase singularities from both noisy and sparse electrical\nmapping data with high precision and efficiency. The self-supervised deep\nlearning approach is fundamentally different from classical phase mapping\ntechniques. Rather than encoding a phase signal from time-series data, the\nnetwork instead learns to directly associate short spatio-temporal sequences of\nelectrical data with phase maps and the positions of phase singularities. Using\nthis method, we were able to accurately compute phase maps and locate rotor\ncores even from extremely sparse and noisy data, generated from both optical\nmapping experiments and computer simulations. Neural networks are a promising\nalternative to conventional phase mapping and rotor core localization methods,\nthat could be used in optical mapping studies in basic cardiovascular research\nas well as in the clinical setting for the analysis of atrial fibrillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Lebert_J/0/1/0/all/0/1\">Jan Lebert</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ravi_N/0/1/0/all/0/1\">Namita Ravi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fenton_F/0/1/0/all/0/1\">Flavio Fenton</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Christoph_J/0/1/0/all/0/1\">Jan Christoph</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVM3Det: A Novel Method for Multi-view Monocular 3D Detection. (arXiv:2109.10473v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10473","description":"<p>Monocular 3D object detection encounters occlusion problems in many\napplication scenarios, such as traffic monitoring, pedestrian monitoring, etc.,\nwhich leads to serious false negative. Multi-view object detection effectively\nsolves this problem by combining data from different perspectives. However, due\nto label confusion and feature confusion, the orientation estimation of\nmulti-view 3D object detection is intractable, which is important for object\ntracking and intention prediction. In this paper, we propose a novel multi-view\n3D object detection method named MVM3Det which simultaneously estimates the 3D\nposition and orientation of the object according to the multi-view monocular\ninformation. The method consists of two parts: 1) Position proposal network,\nwhich integrates the features from different perspectives into consistent\nglobal features through feature orthogonal transformation to estimate the\nposition. 2) Multi-branch orientation estimation network, which introduces\nfeature perspective pooling to overcome the two confusion problems during the\norientation estimation. In addition, we present a first dataset for multi-view\n3D object detection named MVM3D. Comparing with State-Of-The-Art (SOTA) methods\non our dataset and public dataset WildTrack, our method achieves very\ncompetitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haoran_L/0/1/0/all/0/1\">Li Haoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zicheng_D/0/1/0/all/0/1\">Duan Zicheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingjun_M/0/1/0/all/0/1\">Ma Mingjun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaran_C/0/1/0/all/0/1\">Chen Yaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiaqi_L/0/1/0/all/0/1\">Li Jiaqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dongbin_Z/0/1/0/all/0/1\">Zhao Dongbin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid detection and recognition of whole brain activity in a freely behaving Caenorhabditis elegans. (arXiv:2109.10474v1 [q-bio.QM])","link":"http://arxiv.org/abs/2109.10474","description":"<p>Advanced volumetric imaging methods and genetically encoded activity\nindicators have permitted a comprehensive characterization of whole brain\nactivity at single neuron resolution in \\textit{Caenorhabditis elegans}. The\nconstant motion and deformation of the mollusc nervous system, however, impose\na great challenge for a consistent identification of densely packed neurons in\na behaving animal. Here, we propose a cascade solution for long-term and rapid\nrecognition of head ganglion neurons in a freely moving \\textit{C. elegans}.\nFirst, potential neuronal regions from a stack of fluorescence images are\ndetected by a deep learning algorithm. Second, 2 dimensional neuronal regions\nare fused into 3 dimensional neuron entities. Third, by exploiting the neuronal\ndensity distribution surrounding a neuron and relative positional information\nbetween neurons, a multi-class artificial neural network transforms engineered\nneuronal feature vectors into digital neuronal identities. Under the constraint\nof a small number (20-40 volumes) of training samples, our bottom-up approach\nis able to process each volume - $1024 \\times 1024 \\times 18$ in voxels - in\nless than 1 second and achieves an accuracy of $91\\%$ in neuronal detection and\n$74\\%$ in neuronal recognition. Our work represents an important development\ntowards a rapid and fully automated algorithm for decoding whole brain activity\nunderlying natural animal behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_S/0/1/0/all/0/1\">Shang Wu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lang_C/0/1/0/all/0/1\">Chengtian Lang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wen_Q/0/1/0/all/0/1\">Quan Wen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_T/0/1/0/all/0/1\">Tianqi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Compositional Color Representations from Text. (arXiv:2109.10477v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10477","description":"<p>We consider the cross-modal task of producing color representations for text\nphrases. Motivated by the fact that a significant fraction of user queries on\nan image search engine follow an (attribute, object) structure, we propose a\ngenerative adversarial network that generates color profiles for such bigrams.\nWe design our pipeline to learn composition - the ability to combine seen\nattributes and objects to unseen pairs. We propose a novel dataset curation\npipeline from existing public sources. We describe how a set of phrases of\ninterest can be compiled using a graph propagation technique, and then mapped\nto images. While this dataset is specialized for our investigations on color,\nthe method can be extended to other visual dimensions where composition is of\ninterest. We provide detailed ablation studies that test the behavior of our\nGAN architecture with loss functions from the contrastive learning literature.\nWe show that the generative model achieves lower Frechet Inception Distance\nthan discriminative ones, and therefore predicts color profiles that better\nmatch those from real images. Finally, we demonstrate improved performance in\nimage retrieval and classification, indicating the crucial role that color\nplays in these downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_P/0/1/0/all/0/1\">Paridhi Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nihal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaddamanu_P/0/1/0/all/0/1\">Praneetha Vaddamanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raut_D/0/1/0/all/0/1\">Dhananjay Raut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaishay_S/0/1/0/all/0/1\">Shraiysh Vaishay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinay_V/0/1/0/all/0/1\">Vishwa Vinay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI in Osteoporosis. (arXiv:2109.10478v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10478","description":"<p>In this chapter we explore and evaluate methods for trabecular bone\ncharacterization and osteoporosis diagnosis with increased interest in sparse\napproximations. We first describe texture representation and classification\ntechniques, patch-based methods such as Bag of Keypoints, and more recent deep\nneural networks. Then we introduce the concept of sparse representations for\npattern recognition and we detail integrative sparse analysis methods and\nclassifier decision fusion methods. We report cross-validation results on\nosteoporosis datasets of bone radiographs and compare the results produced by\nthe different categories of methods. We conclude that advances in the AI and\nmachine learning fields have enabled the development of methods that can be\nused as diagnostic tools in clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makrogiannis_S/0/1/0/all/0/1\">Sokratis Makrogiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Keni Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Image Dehazing with An Independent Detail-Recovery Network. (arXiv:2109.10492v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10492","description":"<p>Single image dehazing is a prerequisite which affects the performance of many\ncomputer vision tasks and has attracted increasing attention in recent years.\nHowever, most existing dehazing methods emphasize more on haze removal but less\non the detail recovery of the dehazed images. In this paper, we propose a\nsingle image dehazing method with an independent Detail Recovery Network (DRN),\nwhich considers capturing the details from the input image over a separate\nnetwork and then integrates them into a coarse dehazed image. The overall\nnetwork consists of two independent networks, named DRN and the dehazing\nnetwork respectively. Specifically, the DRN aims to recover the dehazed image\ndetails through local and global branches respectively. The local branch can\nobtain local detail information through the convolution layer and the global\nbranch can capture more global information by the Smooth Dilated Convolution\n(SDC). The detail feature map is fused into the coarse dehazed image to obtain\nthe dehazed image with rich image details. Besides, we integrate the DRN, the\nphysical-model-based dehazing network and the reconstruction loss into an\nend-to-end joint learning framework. Extensive experiments on the public image\ndehazing datasets (RESIDE-Indoor, RESIDE-Outdoor and the TrainA-TestA)\nillustrate the effectiveness of the modules in the proposed method and show\nthat our method outperforms the state-of-the-art dehazing methods both\nquantitatively and qualitatively. The code is released in\nhttps://github.com/YanLi-LY/Dehazing-DRN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">De Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiande Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification. (arXiv:2109.10498v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10498","description":"<p>Person re-identification (re-ID) plays an important role in applications such\nas public security and video surveillance. Recently, learning from synthetic\ndata, which benefits from the popularity of synthetic data engine, has\nattracted attention from both academia and the public eye. However, existing\nsynthetic datasets are limited in quantity, diversity and realisticity, and\ncannot be efficiently used for generalizable re-ID problem. To address this\nchallenge, we construct and label a large-scale synthetic person dataset named\nFineGPR with fine-grained attribute distribution. Moreover, aiming to fully\nexploit the potential of FineGPR and promote the efficient training from\nmillions of synthetic data, we propose an attribute analysis pipeline AOST to\nlearn attribute distribution in target domain, then apply style transfer\nnetwork to eliminate the gap between synthetic and real-world data and thus is\nfreely deployed to new scenarios. Experiments conducted on benchmarks\ndemonstrate that FineGPR with AOST outperforms (or is on par with) existing\nreal and synthetic datasets, which suggests its feasibility for re-ID and\nproves the proverbial less-is-more principle. We hope this fine-grained dataset\ncould advance research towards re-ID in real scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Suncheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_G/0/1/0/all/0/1\">Guanjie You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1\">Mengyuan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuzhuo Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Optical Neuroimaging Denoising with Semantic Tasks. (arXiv:2109.10499v1 [eess.IV])","link":"http://arxiv.org/abs/2109.10499","description":"<p>Optical neuroimaging is a vital tool for understanding the brain structure\nand the connection between regions and nuclei. However, the image noise\nintroduced in the sample preparation and the imaging system hinders the\nextraction of the possible knowlege from the dataset, thus denoising for the\noptical neuroimaging is usually necessary. The supervised denoisng methods\noften outperform the unsupervised ones, but the training of the supervised\ndenoising models needs the corresponding clean labels, which is not always\navaiable due to the high labeling cost. On the other hand, those semantic\nlabels, such as the located soma positions, the reconstructed neuronal fibers,\nand the nuclei segmentation result, are generally available and accumulated\nfrom everyday neuroscience research. This work connects a supervised denoising\nand a semantic segmentation model together to form a end-to-end model, which\ncan make use of the semantic labels while still provides a denoised image as an\nintermediate product. We use both the supervised and the self-supervised models\nfor the denoising and introduce a new cost term for the joint denoising and the\nsegmentation setup. We test the proposed approach on both the synthetic data\nand the real-world data, including the optical neuroimaing dataset and the\nelectron microscope dataset. The result shows that the joint denoising result\noutperforms the one using the denoising method alone and the joint model\nbenefits the segmentation and other downstream task as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_T/0/1/0/all/0/1\">Tianfang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_Y/0/1/0/all/0/1\">Yue Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_A/0/1/0/all/0/1\">Anan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation. (arXiv:2109.10504v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10504","description":"<p>Self-supervised vision-and-language pretraining (VLP) aims to learn\ntransferable multi-modal representations from large-scale image-text data and\nto achieve strong performances on a broad scope of vision-language tasks after\nfinetuning. Previous mainstream VLP approaches typically adopt a two-step\nstrategy relying on external object detectors to encode images in a multi-modal\nTransformer framework, which suffer from restrictive object concept space,\nlimited image context and inefficient computation. In this paper, we propose an\nobject-aware end-to-end VLP framework, which directly feeds image grid features\nfrom CNNs into the Transformer and learns the multi-modal representations\njointly. More importantly, we propose to perform object knowledge distillation\nto facilitate learning cross-modal alignment at different semantic levels. To\nachieve that, we design two novel pretext tasks by taking object features and\ntheir semantic labels from external detectors as supervision: 1.) Object-guided\nmasked vision modeling task focuses on enforcing object-aware representation\nlearning in the multi-modal Transformer; 2.) Phrase-region alignment task aims\nto improve cross-modal alignment by utilizing the similarities between noun\nphrases and object labels in the linguistic space. Extensive experiments on a\nwide range of vision-language tasks demonstrate the efficacy of our proposed\nframework, and we achieve competitive or superior performances over the\nexisting pretraining strategies. The code is available in supplementary\nmaterials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_S/0/1/0/all/0/1\">Shao-yen Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Data Uncertainty in Object Tracking Algorithms. (arXiv:2109.10521v1 [eess.SY])","link":"http://arxiv.org/abs/2109.10521","description":"<p>Methodologies for incorporating the uncertainties characteristic of\ndata-driven object detectors into object tracking algorithms are explored.\nObject tracking methods rely on measurement error models, typically in the form\nof measurement noise, false positive rates, and missed detection rates. Each of\nthese quantities, in general, can be dependent on object or measurement\nlocation. However, for detections generated from neural-network processed\ncamera inputs, these measurement error statistics are not sufficient to\nrepresent the primary source of errors, namely a dissimilarity between run-time\nsensor input and the training data upon which the detector was trained. To this\nend, we investigate incorporating data uncertainty into object tracking methods\nsuch as to improve the ability to track objects, and particularly those which\nout-of-distribution w.r.t. training data. The proposed methodologies are\nvalidated on an object tracking benchmark as well on experiments with a real\nautonomous aircraft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Muthali_A/0/1/0/all/0/1\">Anish Muthali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laine_F/0/1/0/all/0/1\">Forrest Laine</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomlin_C/0/1/0/all/0/1\">Claire Tomlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Method For Adding Motion-Blur on Arbitrary Objects By using Auto-Segmentation and Color Compensation Techniques. (arXiv:2109.10524v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10524","description":"<p>When dynamic objects are captured by a camera, motion blur inevitably occurs.\nSuch a blur is sometimes considered as just a noise, however, it sometimes\ngives an important effect to add dynamism in the scene for photographs or\nvideos. Unlike the similar effects, such as defocus blur, which is now easily\ncontrolled even by smartphones, motion blur is still uncontrollable and makes\nundesired effects on photographs. In this paper, an unified framework to add\nmotion blur on per-object basis is proposed. In the method, multiple frames are\ncaptured without motion blur and they are accumulated to create motion blur on\ntarget objects. To capture images without motion blur, shutter speed must be\nshort, however, it makes captured images dark, and thus, a sensor gain should\nbe increased to compensate it. Since a sensor gain causes a severe noise on\nimage, we propose a color compensation algorithm based on non-linear filtering\ntechnique for solution. Another contribution is that our technique can be used\nto make HDR images for fast moving objects by using multi-exposure images. In\nthe experiments, effectiveness of the method is confirmed by ablation study\nusing several data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mikamo_M/0/1/0/all/0/1\">Michihiro Mikamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_R/0/1/0/all/0/1\">Ryo Furukawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawasaki_H/0/1/0/all/0/1\">Hiroshi Kawasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Multimodal Transformer to Summarize Videos. (arXiv:2109.10559v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10559","description":"<p>Although video summarization has achieved tremendous success benefiting from\nRecurrent Neural Networks (RNN), RNN-based methods neglect the global\ndependencies and multi-hop relationships among video frames, which limits the\nperformance. Transformer is an effective model to deal with this problem, and\nsurpasses RNN-based methods in several sequence modeling tasks, such as machine\ntranslation, video captioning, \\emph{etc}. Motivated by the great success of\ntransformer and the natural structure of video (frame-shot-video), a\nhierarchical transformer is developed for video summarization, which can\ncapture the dependencies among frame and shots, and summarize the video by\nexploiting the scene information formed by shots. Furthermore, we argue that\nboth the audio and visual information are essential for the video summarization\ntask. To integrate the two kinds of information, they are encoded in a\ntwo-stream scheme, and a multimodal fusion mechanism is developed based on the\nhierarchical transformer. In this paper, the proposed method is denoted as\nHierarchical Multimodal Transformer (HMT). Practically, extensive experiments\nshow that HMT surpasses most of the traditional, RNN-based and attention-based\nvideo summarization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Maoguo Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-supervised Learning. (arXiv:2109.10563v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10563","description":"<p>Due to difficulties in acquiring ground truth depth of equirectangular (360)\nimages, the quality and quantity of equirectangular depth data today is\ninsufficient to represent the various scenes in the world. Therefore, 360 depth\nestimation studies, which relied solely on supervised learning, are destined to\nproduce unsatisfactory results. Although self-supervised learning methods\nfocusing on equirectangular images (EIs) are introduced, they often have\nincorrect or non-unique solutions, causing unstable performance. In this paper,\nwe propose 360 monocular depth estimation methods which improve on the areas\nthat limited previous studies. First, we introduce a self-supervised 360 depth\nlearning method that only utilizes gravity-aligned videos, which has the\npotential to eliminate the needs for depth data during the training procedure.\nSecond, we propose a joint learning scheme realized by combining supervised and\nself-supervised learning. The weakness of each learning is compensated, thus\nleading to more accurate depth estimation. Third, we propose a non-local fusion\nblock, which retains global information encoded by vision transformer when\nreconstructing the depths. With the proposed methods, we successfully apply the\ntransformer to 360 depth estimations, to the best of our knowledge, which has\nnot been tried before. On several benchmarks, our approach achieves significant\nimprovements over previous works and establishes a state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_I/0/1/0/all/0/1\">IlWi Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyuk-Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_C/0/1/0/all/0/1\">Chae Eun Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation. (arXiv:2109.10595v1 [cs.GR])","link":"http://arxiv.org/abs/2109.10595","description":"<p>To the best of our knowledge, we first present a live system that generates\npersonalized photorealistic talking-head animation only driven by audio signals\nat over 30 fps. Our system contains three stages. The first stage is a deep\nneural network that extracts deep audio features along with a manifold\nprojection to project the features to the target person's speech space. In the\nsecond stage, we learn facial dynamics and motions from the projected audio\nfeatures. The predicted motions include head poses and upper body motions,\nwhere the former is generated by an autoregressive probabilistic model which\nmodels the head pose distribution of the target person. Upper body motions are\ndeduced from head poses. In the final stage, we generate conditional feature\nmaps from previous predictions and send them with a candidate image set to an\nimage-to-image translation network to synthesize photorealistic renderings. Our\nmethod generalizes well to wild audio and successfully synthesizes\nhigh-fidelity personalized facial details, e.g., wrinkles, teeth. Our method\nalso allows explicit control of head poses. Extensive qualitative and\nquantitative evaluations, along with user studies, demonstrate the superiority\nof our method over state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuanxun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Jinxiang Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Context-Aware Network for Abdominal Multi-organ Segmentation. (arXiv:2109.10601v1 [eess.IV])","link":"http://arxiv.org/abs/2109.10601","description":"<p>The contextual information, presented in abdominal CT scan, is relative\nconsistent. In order to make full use of the overall 3D context, we develop a\nwhole-volumebased coarse-to-fine framework for efficient and effective\nabdominal multi-organ segmentation. We propose a new efficientSegNet network,\nwhich is composed of encoder, decoder and context block. For the decoder\nmodule, anisotropic convolution with a k*k*1 intra-slice convolution and a\n1*1*k inter-slice convolution, is designed to reduce the computation burden.\nFor the context block, we propose strip pooling module to capture anisotropic\nand long-range contextual information, which exists in abdominal scene.\nQuantitative evaluation on the FLARE2021 validation cases, this method achieves\nthe average dice similarity coefficient (DSC) of 0.895 and average normalized\nsurface distance (NSD) of 0.775. The average running time is 9.8 s per case in\ninference phase, and maximum used GPU memory is 1017 MB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LDC-VAE: A Latent Distribution Consistency Approach to Variational AutoEncoders. (arXiv:2109.10640v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10640","description":"<p>Variational autoencoders (VAEs), as an important aspect of generative models,\nhave received a lot of research interests and reached many successful\napplications. However, it is always a challenge to achieve the consistency\nbetween the learned latent distribution and the prior latent distribution when\noptimizing the evidence lower bound (ELBO), and finally leads to an\nunsatisfactory performance in data generation. In this paper, we propose a\nlatent distribution consistency approach to avoid such substantial\ninconsistency between the posterior and prior latent distributions in ELBO\noptimizing. We name our method as latent distribution consistency VAE\n(LDC-VAE). We achieve this purpose by assuming the real posterior distribution\nin latent space as a Gibbs form, and approximating it by using our encoder.\nHowever, there is no analytical solution for such Gibbs posterior in\napproximation, and traditional approximation ways are time consuming, such as\nusing the iterative sampling-based MCMC. To address this problem, we use the\nStein Variational Gradient Descent (SVGD) to approximate the Gibbs posterior.\nMeanwhile, we use the SVGD to train a sampler net which can obtain efficient\nsamples from the Gibbs posterior. Comparative studies on the popular image\ngeneration datasets show that our method has achieved comparable or even better\nperformance than several powerful improvements of VAEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xinwen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Training for Cardiac Resynchronisation Therapy Response Prediction. (arXiv:2109.10641v1 [eess.IV])","link":"http://arxiv.org/abs/2109.10641","description":"<p>Evaluation of predictive deep learning (DL) models beyond conventional\nperformance metrics has become increasingly important for applications in\nsensitive environments like healthcare. Such models might have the capability\nto encode and analyse large sets of data but they often lack comprehensive\ninterpretability methods, preventing clinical trust in predictive outcomes.\nQuantifying uncertainty of a prediction is one way to provide such\ninterpretability and promote trust. However, relatively little attention has\nbeen paid to how to include such requirements into the training of the model.\nIn this paper we: (i) quantify the data (aleatoric) and model (epistemic)\nuncertainty of a DL model for Cardiac Resynchronisation Therapy response\nprediction from cardiac magnetic resonance images, and (ii) propose and perform\na preliminary investigation of an uncertainty-aware loss function that can be\nused to retrain an existing DL image-based classification model to encourage\nconfidence in correct predictions and reduce confidence in incorrect\npredictions. Our initial results are promising, showing a significant increase\nin the (epistemic) confidence of true positive predictions, with some evidence\nof a reduction in false negative confidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dawood_T/0/1/0/all/0/1\">Tareen Dawood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andlauer_R/0/1/0/all/0/1\">Robin Andlauer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sidhu_B/0/1/0/all/0/1\">Baldeep S. Sidhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gould_J/0/1/0/all/0/1\">Justin Gould</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Porter_B/0/1/0/all/0/1\">Bradley Porter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elliott_M/0/1/0/all/0/1\">Mark Elliott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehta_V/0/1/0/all/0/1\">Vishal Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rinaldi_C/0/1/0/all/0/1\">C. Aldo Rinaldi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Ant&#xf3;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Caption Enriched Samples for Improving Hateful Memes Detection. (arXiv:2109.10649v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10649","description":"<p>The recently introduced hateful meme challenge demonstrates the difficulty of\ndetermining whether a meme is hateful or not. Specifically, both unimodal\nlanguage models and multimodal vision-language models cannot reach the human\nlevel of performance. Motivated by the need to model the contrast between the\nimage content and the overlayed text, we suggest applying an off-the-shelf\nimage captioning tool in order to capture the first. We demonstrate that the\nincorporation of such automatic captions during fine-tuning improves the\nresults for various unimodal and multimodal models. Moreover, in the unimodal\ncase, continuing the pre-training of language models on augmented and original\ncaption pairs, is highly beneficial to the classification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blaier_E/0/1/0/all/0/1\">Efrat Blaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkiel_I/0/1/0/all/0/1\">Itzik Malkiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vehicle Behavior Prediction and Generalization Using Imbalanced Learning Techniques. (arXiv:2109.10656v1 [cs.RO])","link":"http://arxiv.org/abs/2109.10656","description":"<p>The use of learning-based methods for vehicle behavior prediction is a\npromising research topic. However, many publicly available data sets suffer\nfrom class distribution skews which limits learning performance if not\naddressed. This paper proposes an interaction-aware prediction model consisting\nof an LSTM autoencoder and SVM classifier. Additionally, an imbalanced learning\ntechnique, the multiclass balancing ensemble is proposed. Evaluations show that\nthe method enhances model performance, resulting in improved classification\naccuracy. Good generalization properties of learned models are important and\ntherefore a generalization study is done where models are evaluated on unseen\ntraffic data with dissimilar traffic behavior stemming from different road\nconfigurations. This is realized by using two distinct highway traffic\nrecordings, the publicly available NGSIM US-101 and I80 data sets. Moreover,\nmethods for encoding structural and static features into the learning process\nfor improved generalization are evaluated. The resulting methods show\nsubstantial improvements in classification as well as generalization\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Westny_T/0/1/0/all/0/1\">Theodor Westny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisk_E/0/1/0/all/0/1\">Erik Frisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olofsson_B/0/1/0/all/0/1\">Bj&#xf6;rn Olofsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TACTIC: Joint Rate-Distortion-Accuracy Optimisation for Low Bitrate Compression. (arXiv:2109.10658v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10658","description":"<p>We present TACTIC: Task-Aware Compression Through Intelligent Coding. Our\nlossy compression model learns based on the rate-distortion-accuracy trade-off\nfor a specific task. By considering what information is important for the\nfollow-on problem, the system trades off visual fidelity for good task\nperformance at a low bitrate. When compared against JPEG at the same bitrate,\nour approach is able to improve the accuracy of ImageNet subset classification\nby 4.5%. We also demonstrate the applicability of our approach to other\nproblems, providing a 3.4% accuracy and 4.9% mean IoU improvements in\nperformance over task-agnostic compression for semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kubiak_N/0/1/0/all/0/1\">Nikolina Kubiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1\">Simon Hadfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep neural network for multi-species fish detection using multiple acoustic cameras. (arXiv:2109.10664v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10664","description":"<p>Underwater acoustic cameras are high potential devices for many applications\nin ecology, notably for fisheries management and monitoring. However how to\nextract such data into high value information without a time-consuming entire\ndataset reading by an operator is still a challenge. Moreover the analysis of\nacoustic imaging, due to its low signal-to-noise ratio, is a perfect training\nground for experimenting with new approaches, especially concerning Deep\nLearning techniques. We present hereby a novel approach that takes advantage of\nboth CNN (Convolutional Neural Network) and classical CV (Computer Vision)\ntechniques, able to detect a generic class ''fish'' in acoustic video streams.\nThe pipeline pre-treats the acoustic images to extract 2 features, in order to\nlocalise the signals and improve the detection performances. To ensure the\nperformances from an ecological point of view, we propose also a two-step\nvalidation, one to validate the results of the trainings and one to test the\nmethod on a real-world scenario. The YOLOv3-based model was trained with data\nof fish from multiple species recorded by the two common acoustic cameras,\nDIDSON and ARIS, including species of high ecological interest, as Atlantic\nsalmon or European eels. The model we developed provides satisfying results\ndetecting almost 80% of fish and minimizing the false positive rate, however\nthe model is much less efficient for eel detections on ARIS videos. The first\nCNN pipeline for fish monitoring exploiting video data from two models of\nacoustic cameras satisfies most of the required features. Many challenges are\nstill present, such as the automation of fish species identification through a\nmulticlass model. 1 However the results point a new solution for dealing with\ncomplex data, such as sonar data, which can also be reapplied in other cases\nwhere the signal-to-noise ratio is a challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guglielmo_G/0/1/0/all/0/1\">Garcia Fernandez Guglielmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martignac_F/0/1/0/all/0/1\">Fran&#xe7;ois Martignac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevoux_M/0/1/0/all/0/1\">Marie Nevoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaulaton_L/0/1/0/all/0/1\">Laurent Beaulaton</a> (OFB), <a href=\"http://arxiv.org/find/cs/1/au:+Corpetti_T/0/1/0/all/0/1\">Thomas Corpetti</a> (LETG - Rennes)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training Based Unsupervised Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2109.10674v1 [eess.IV])","link":"http://arxiv.org/abs/2109.10674","description":"<p>With the advances of deep learning, many medical image segmentation studies\nachieve human-level performance when in fully supervised condition. However, it\nis extremely expensive to acquire annotation on every data in medical fields,\nespecially on magnetic resonance images (MRI) that comprise many different\ncontrasts. Unsupervised methods can alleviate this problem; however, the\nperformance drop is inevitable compared to fully supervised methods. In this\nwork, we propose a self-training based unsupervised-learning framework that\nperforms automatic segmentation of Vestibular Schwannoma (VS) and cochlea on\nhigh-resolution T2 scans. Our method consists of 4 main stages: 1)\nVS-preserving contrast conversion from contrast-enhanced T1 scan to\nhigh-resolution T2 scan, 2) training segmentation on generated T2 scans with\nannotations on T1 scans, and 3) Inferring pseudo-labels on non-annotated real\nT2 scans, and 4) boosting the generalizability of VS and cochlea segmentation\nby training with combined data (i.e., real T2 scans with pseudo-labels and\ngenerated T2 scans with true annotations). Our method showed mean Dice score\nand Average Symmetric Surface Distance (ASSD) of 0.8570 (0.0705) and 0.4970\n(0.3391) for VS, 0.8446 (0.0211) and 0.1513 (0.0314) for Cochlea on\nCrossMoDA2021 challenge validation phase leaderboard, outperforming most other\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shin_H/0/1/0/all/0/1\">Hyungseob Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hyeongyu Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sewon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jun_Y/0/1/0/all/0/1\">Yohan Jun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eo_T/0/1/0/all/0/1\">Taejoon Eo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dosik Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Video Localization with Learnable Moment Proposals. (arXiv:2109.10678v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10678","description":"<p>Given an untrimmed video and a natural language query, Natural Language Video\nLocalization (NLVL) aims to identify the video moment described by the query.\nTo address this task, existing methods can be roughly grouped into two groups:\n1) propose-and-rank models first define a set of hand-designed moment\ncandidates and then find out the best-matching one. 2) proposal-free models\ndirectly predict two temporal boundaries of the referential moment from frames.\nCurrently, almost all the propose-and-rank methods have inferior performance\nthan proposal-free counterparts. In this paper, we argue that propose-and-rank\napproach is underestimated due to the predefined manners: 1) Hand-designed\nrules are hard to guarantee the complete coverage of targeted segments. 2)\nDensely sampled candidate moments cause redundant computation and degrade the\nperformance of ranking process. To this end, we propose a novel model termed\nLPNet (Learnable Proposal Network for NLVL) with a fixed set of learnable\nmoment proposals. The position and length of these proposals are dynamically\nadjusted during training process. Moreover, a boundary-aware loss has been\nproposed to leverage frame-level information and further improve the\nperformance. Extensive ablations on two challenging NLVL benchmarks have\ndemonstrated the effectiveness of LPNet over existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shaoning Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jian Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Video-to-Video Translation Networks to Computational Fluid Dynamics. (arXiv:2109.10679v1 [physics.flu-dyn])","link":"http://arxiv.org/abs/2109.10679","description":"<p>In recent years, the evolution of artificial intelligence, especially deep\nlearning, has been remarkable, and its application to various fields has been\ngrowing rapidly. In this paper, I report the results of the application of\ngenerative adversarial networks (GANs), specifically video-to-video translation\nnetworks, to computational fluid dynamics (CFD) simulations. The purpose of\nthis research is to reduce the computational cost of CFD simulations with GANs.\nThe architecture of GANs in this research is a combination of the\nimage-to-image translation networks (the so-called \"pix2pix\") and Long\nShort-Term Memory (LSTM). It is shown that the results of high-cost and\nhigh-accuracy simulations (with high-resolution computational grids) can be\nestimated from those of low-cost and low-accuracy simulations (with\nlow-resolution grids). In particular, the time evolution of density\ndistributions in the cases of a high-resolution grid is reproduced from that in\nthe cases of a low-resolution grid through GANs, and the density inhomogeneity\nestimated from the image generated by GANs recovers the ground truth with good\naccuracy. Qualitative and quantitative comparisons of the results of the\nproposed method with those of several super-resolution algorithms are also\npresented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Kigure_H/0/1/0/all/0/1\">Hiromitsu Kigure</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10686","description":"<p>There remain many open questions pertaining to the scaling behaviour of\nTransformer architectures. These scaling decisions and findings can be\ncritical, as training runs often come with an associated computational cost\nwhich have both financial and/or environmental impact. The goal of this paper\nis to present scaling insights from pretraining and finetuning Transformers.\nWhile Kaplan et al. presents a comprehensive study of the scaling behaviour of\nTransformer language models, the scope is only on the upstream (pretraining)\nloss. Therefore, it is still unclear if these set of findings transfer to\ndownstream task within the context of the pretrain-finetune paradigm. The key\nfindings of this paper are as follows: (1) we show that aside from only the\nmodel size, model shape matters for downstream fine-tuning, (2) scaling\nprotocols operate differently at different compute regions, (3) widely adopted\nT5-base and T5-large sizes are Pareto-inefficient. To this end, we present\nimproved scaling protocols whereby our redesigned models achieve similar\ndownstream fine-tuning quality while having 50\\% fewer parameters and training\n40\\% faster compared to the widely adopted T5-base model. We publicly release\nover 100 pretrained checkpoints of different T5 configurations to facilitate\nfuture research and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Facial Forgery Artifacts with Parts-Based Detectors. (arXiv:2109.10688v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10688","description":"<p>Manipulated videos, especially those where the identity of an individual has\nbeen modified using deep neural networks, are becoming an increasingly relevant\nthreat in the modern day. In this paper, we seek to develop a generalizable,\nexplainable solution to detecting these manipulated videos. To achieve this, we\ndesign a series of forgery detection systems that each focus on one individual\npart of the face. These parts-based detection systems, which can be combined\nand used together in a single architecture, meet all of our desired criteria -\nthey generalize effectively between datasets and give us valuable insights into\nwhat the network is looking at when making its decision. We thus use these\ndetectors to perform detailed empirical analysis on the FaceForensics++,\nCeleb-DF, and Facebook Deepfake Detection Challenge datasets, examining not\njust what the detectors find but also collecting and analyzing useful related\nstatistics on the datasets themselves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwarcz_S/0/1/0/all/0/1\">Steven Schwarcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Surface Triangulation. (arXiv:2109.10695v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10695","description":"<p>Triangle meshes remain the most popular data representation for surface\ngeometry. This ubiquitous representation is essentially a hybrid one that\ndecouples continuous vertex locations from the discrete topological\ntriangulation. Unfortunately, the combinatorial nature of the triangulation\nprevents taking derivatives over the space of possible meshings of any given\nsurface. As a result, to date, mesh processing and optimization techniques have\nbeen unable to truly take advantage of modular gradient descent components of\nmodern optimization frameworks. In this work, we present a differentiable\nsurface triangulation that enables optimization for any per-vertex or per-face\ndifferentiable objective function over the space of underlying surface\ntriangulations. Our method builds on the result that any 2D triangulation can\nbe achieved by a suitably perturbed weighted Delaunay triangulation. We\ntranslate this result into a computational algorithm by proposing a soft\nrelaxation of the classical weighted Delaunay triangulation and optimizing over\nvertex weights and vertex locations. We extend the algorithm to 3D by\ndecomposing shapes into developable sets and differentiably meshing each set\nwith suitable boundary constraints. We demonstrate the efficacy of our method\non various planar and surface meshes on a range of difficult-to-optimize\nobjective functions. Our code can be found online:\nhttps://github.com/mrakotosaon/diff-surface-triangulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rakotosaona_M/0/1/0/all/0/1\">Marie-Julie Rakotosaona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Quantitative Comparison of Epistemic Uncertainty Maps Applied to Multi-Class Segmentation. (arXiv:2109.10702v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10702","description":"<p>Uncertainty assessment has gained rapid interest in medical image analysis. A\npopular technique to compute epistemic uncertainty is the Monte-Carlo (MC)\ndropout technique. From a network with MC dropout and a single input, multiple\noutputs can be sampled. Various methods can be used to obtain epistemic\nuncertainty maps from those multiple outputs. In the case of multi-class\nsegmentation, the number of methods is even larger as epistemic uncertainty can\nbe computed voxelwise per class or voxelwise per image. This paper highlights a\nsystematic approach to define and quantitatively compare those methods in two\ndifferent contexts: class-specific epistemic uncertainty maps (one value per\nimage, voxel and class) and combined epistemic uncertainty maps (one value per\nimage and voxel). We applied this quantitative analysis to a multi-class\nsegmentation of the carotid artery lumen and vessel wall, on a multi-center,\nmulti-scanner, multi-sequence dataset of (MR) images. We validated our analysis\nover 144 sets of hyperparameters of a model. Our main analysis considers the\nrelationship between the order of the voxels sorted according to their\nepistemic uncertainty values and the misclassification of the prediction. Under\nthis consideration, the comparison of combined uncertainty maps reveals that\nthe multi-class entropy and the multi-class mutual information statistically\nout-perform the other combined uncertainty maps under study. In a\nclass-specific scenario, the one-versus-all entropy statistically out-performs\nthe class-wise entropy, the class-wise variance and the one versus all mutual\ninformation. The class-wise entropy statistically out-performs the other\nclass-specific uncertainty maps in terms of calibration. We made a python\npackage available to reproduce our analysis on different data and tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camarasa_R/0/1/0/all/0/1\">Robin Camarasa</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Bos_D/0/1/0/all/0/1\">Daniel Bos</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Hendrikse_J/0/1/0/all/0/1\">Jeroen Hendrikse</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Nederkoorn_P/0/1/0/all/0/1\">Paul Nederkoorn</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Kooi_M/0/1/0/all/0/1\">M. Eline Kooi</a> (6), <a href=\"http://arxiv.org/find/cs/1/au:+Lugt_A/0/1/0/all/0/1\">Aad van der Lugt</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1\">Marleen de Bruijne</a> (1, 2 and 7), ((1) Biomedical Imaging Group Rotterdam, Department of Radiology and Nuclear Medicine, Erasmus MC, Rotterdam, The Netherlands, (2) Department of Radiology and Nuclear Medicine, Erasmus MC, Rotterdam, The Netherlands, (3) Department of Epidemiology, Erasmus MC, Rotterdam, The Netherlands, (4) Department of Radiology, University Medical Center Utrecht, Utrecht, The Netherlands, (5) Department of Neurology, Academic Medical Center University of Amsterdam, Amsterdam, The Netherlands, (6) Department of Radiology and Nuclear Medicine, CARIM School for Cardiovascular Diseases, Maastricht University Medical Center, Maastricht, The Netherlands, (7) Department of Computer Science, University of Copenhagen, Denmark)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Plane Adjustment of Orthopedic Intra-operative Flat Panel Detector CT-Volumes. (arXiv:2109.10731v1 [eess.IV])","link":"http://arxiv.org/abs/2109.10731","description":"<p>Purpose\n</p>\n<p>3D acquisitions are often acquired to assess the result in orthopedic trauma\nsurgery. With a mobile C-Arm system, these acquisitions can be performed\nintra-operatively. That reduces the number of required revision surgeries.\nHowever, due to the operation room setup, the acquisitions typically cannot be\nperformed such that the acquired volumes are aligned to the anatomical regions.\nThus, the multiplanar reconstructed (MPR) planes need to be adjusted manually\nduring the review of the volume. In this paper, we present a detailed study of\nmulti-task learning (MTL) regression networks to estimate the parameters of the\nMPR planes.\n</p>\n<p>Approach\n</p>\n<p>First, various mathematical descriptions for rotation, including Euler angle,\nquaternion, and matrix representation, are revised. Then, three different MTL\nnetwork architectures based on the PoseNet are compared with a single task\nlearning network.\n</p>\n<p>Results\n</p>\n<p>Using a matrix description rather than the Euler angle description, the\naccuracy of the regressed normals improves from $7.7^{\\circ}$ to $7.3^{\\circ}$\nin the mean value for single anatomies. The multi-head approach improves the\nregression of the plane position from $7.4mm$ to $6.1mm$, while the orientation\ndoes not benefit from this approach.\n</p>\n<p>Conclusions\n</p>\n<p>The results show that a multi-head approach can lead to slightly better\nresults than the individual tasks networks. The most important benefit of the\nMTL approach is that it is a single network for standard plane regression for\nall body regions with a reduced number of stored parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vicario_C/0/1/0/all/0/1\">Celia Martin Vicario</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kordon_F/0/1/0/all/0/1\">Florian Kordon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1\">Felix Denzinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barbari_J/0/1/0/all/0/1\">Jan Siad El Barbari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Privalov_M/0/1/0/all/0/1\">Maxim Privalov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Franke_J/0/1/0/all/0/1\">Jochen Franke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomas_S/0/1/0/all/0/1\">Sarina Thomas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kausch_L/0/1/0/all/0/1\">Lisa Kausch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kunze_H/0/1/0/all/0/1\">Holger Kunze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Animal inspired Application of a Variant of Mel Spectrogram for Seismic Data Processing. (arXiv:2109.10733v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10733","description":"<p>Predicting disaster events from seismic data is of paramount importance and\ncan save thousands of lives, especially in earthquake-prone areas and\nhabitations around volcanic craters. The drastic rise in the number of seismic\nmonitoring stations in recent years has allowed the collection of a huge\nquantity of data, outpacing the capacity of seismologists. Due to the complex\nnature of the seismological data, it is often difficult for seismologists to\ndetect subtle patterns with major implications. Machine learning algorithms\nhave been demonstrated to be effective in classification and prediction tasks\nfor seismic data. It has been widely known that some animals can sense\ndisasters like earthquakes from seismic signals well before the disaster\nstrikes. Mel spectrogram has been widely used for speech recognition as it\nscales the actual frequencies according to human hearing. In this paper, we\npropose a variant of the Mel spectrogram to scale the raw frequencies of\nseismic data to the hearing of such animals that can sense disasters from\nseismic signals. We are using a Computer vision algorithm along with clustering\nthat allows for the classification of unlabelled seismic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Samayan Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahnawaz_S/0/1/0/all/0/1\">Sk Shahnawaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing. (arXiv:2109.10737v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10737","description":"<p>Great diversity and photorealism have been achieved by unconditional GAN\nframeworks such as StyleGAN and its variations. In the meantime, persistent\nefforts have been made to enhance the semantic controllability of StyleGANs.\nFor example, a dozen of style manipulation methods have been recently proposed\nto perform attribute-conditioned style editing. Although some of these methods\nwork well in manipulating the style codes along one attribute, the control\naccuracy when jointly manipulating multiple attributes tends to be problematic.\nTo address these limitations, we propose a Dynamic Style Manipulation Network\n(DyStyle) whose structure and parameters vary by input samples, to perform\nnonlinear and adaptive manipulation of latent codes for flexible and precise\nattribute control. Additionally, a novel easy-to-hard training procedure is\nintroduced for efficient and stable training of the DyStyle network. Extensive\nexperiments have been conducted on faces and other objects. As a result, our\napproach demonstrates fine-grained disentangled edits along multiple numeric\nand binary attributes. Qualitative and quantitative comparisons with existing\nstyle manipulation methods verify the superiority of our method in terms of the\nattribute control accuracy and identity preservation without compromising the\nphotorealism. The advantage of our method is even more significant for joint\nmulti-attribute control. The source codes are made publicly available at\n\\href{https://github.com/phycvgan/DyStyle}{phycvgan/DyStyle}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingchuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1\">Miao Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zili Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Lane Change Prediction for Automated Driving Systems Using Multi-Task Attention-based Convolutional Neural Networks. (arXiv:2109.10742v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10742","description":"<p>Lane change (LC) is one of the safety-critical manoeuvres in highway driving\naccording to various road accident records. Thus, reliably predicting such\nmanoeuvre in advance is critical for the safe and comfortable operation of\nautomated driving systems. The majority of previous studies rely on detecting a\nmanoeuvre that has been already started, rather than predicting the manoeuvre\nin advance. Furthermore, most of the previous works do not estimate the key\ntimings of the manoeuvre (e.g., crossing time), which can actually yield more\nuseful information for the decision making in the ego vehicle. To address these\nshortcomings, this paper proposes a novel multi-task model to simultaneously\nestimate the likelihood of LC manoeuvres and the time-to-lane-change (TTLC). In\nboth tasks, an attention-based convolutional neural network (CNN) is used as a\nshared feature extractor from a bird's eye view representation of the driving\nenvironment. The spatial attention used in the CNN model improves the feature\nextraction process by focusing on the most relevant areas of the surrounding\nenvironment. In addition, two novel curriculum learning schemes are employed to\ntrain the proposed approach. The extensive evaluation and comparative analysis\nof the proposed method in existing benchmark datasets show that the proposed\nmethod outperforms state-of-the-art LC prediction models, particularly\nconsidering long-term prediction performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozaffari_S/0/1/0/all/0/1\">Sajjad Mozaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_E/0/1/0/all/0/1\">Eduardo Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dianati_M/0/1/0/all/0/1\">Mehrdad Dianati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceEraser: Removing Facial Parts for Augmented Reality. (arXiv:2109.10760v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10760","description":"<p>Our task is to remove all facial parts (e.g., eyebrows, eyes, mouth and\nnose), and then impose visual elements onto the ``blank'' face for augmented\nreality. Conventional object removal methods rely on image inpainting\ntechniques (e.g., EdgeConnect, HiFill) that are trained in a self-supervised\nmanner with randomly manipulated image pairs. Specifically, given a set of\nnatural images, randomly masked images are used as inputs and the raw images\nare treated as ground truths. Whereas, this technique does not satisfy the\nrequirements of facial parts removal, as it is hard to obtain ``ground-truth''\nimages with real ``blank'' faces. To address this issue, we propose a novel\ndata generation technique to produce paired training data that well mimic the\n``blank'' faces. In the mean time, we propose a novel network architecture for\nimproved inpainting quality for our task. Finally, we demonstrate various\nface-oriented augmented reality applications on top of our facial parts removal\nmodel. Our method has been integrated into commercial products and its\neffectiveness has been verified with unconstrained user inputs. The source\ncodes, pre-trained models and training data will be released for research\npurposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1\">Miao Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziyang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingchuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zili Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HybridSDF: Combining Free Form Shapes and Geometric Primitives for effective Shape Manipulation. (arXiv:2109.10767v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10767","description":"<p>CAD modeling typically involves the use of simple geometric primitives\nwhereas recent advances in deep-learning based 3D surface modeling have opened\nnew shape design avenues. Unfortunately, these advances have not yet been\naccepted by the CAD community because they cannot be integrated into\nengineering workflows. To remedy this, we propose a novel approach to\neffectively combining geometric primitives and free-form surfaces represented\nby implicit surfaces for accurate modeling that preserves interpretability,\nenforces consistency, and enables easy manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasu_S/0/1/0/all/0/1\">Subeesh Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talabot_N/0/1/0/all/0/1\">Nicolas Talabot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukoianov_A/0/1/0/all/0/1\">Artem Lukoianov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1\">Pierre Baque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donier_J/0/1/0/all/0/1\">Jonathan Donier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Variational Clustering Framework for Self-labeling of Large-scale Medical Images. (arXiv:2109.10777v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10777","description":"<p>We propose a Deep Variational Clustering (DVC) framework for unsupervised\nrepresentation learning and clustering of large-scale medical images. DVC\nsimultaneously learns the multivariate Gaussian posterior through the\nprobabilistic convolutional encoder and the likelihood distribution with the\nprobabilistic convolutional decoder; and optimizes cluster labels assignment.\nHere, the learned multivariate Gaussian posterior captures the latent\ndistribution of a large set of unlabeled images. Then, we perform unsupervised\nclustering on top of the variational latent space using a clustering loss. In\nthis approach, the probabilistic decoder helps to prevent the distortion of\ndata points in the latent space and to preserve the local structure of data\ngenerating distribution. The training process can be considered as a\nself-training process to refine the latent space and simultaneously optimizing\ncluster assignments iteratively. We evaluated our proposed framework on three\npublic datasets that represented different medical imaging modalities. Our\nexperimental results show that our proposed framework generalizes better across\ndifferent datasets. It achieves compelling results on several medical imaging\nbenchmarks. Thus, our approach offers potential advantages over conventional\ndeep unsupervised learning in real-world applications. The source code of the\nmethod and all the experiments are available publicly at:\nhttps://github.com/csfarzin/DVC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_F/0/1/0/all/0/1\">Farzin Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_M/0/1/0/all/0/1\">Mohammad Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elze_T/0/1/0/all/0/1\">Tobias Elze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1\">Mina Rezaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Cleaning Multiple Instance Learning: Refining Coarse Annotations on Single Whole-Slide Images. (arXiv:2109.10778v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10778","description":"<p>Annotating cancerous regions in whole-slide images (WSIs) of pathology\nsamples plays a critical role in clinical diagnosis, biomedical research, and\nmachine learning algorithms development. However, generating exhaustive and\naccurate annotations is labor-intensive, challenging, and costly. Drawing only\ncoarse and approximate annotations is a much easier task, less costly, and it\nalleviates pathologists' workload. In this paper, we study the problem of\nrefining these approximate annotations in digital pathology to obtain more\naccurate ones. Some previous works have explored obtaining machine learning\nmodels from these inaccurate annotations, but few of them tackle the refinement\nproblem where the mislabeled regions should be explicitly identified and\ncorrected, and all of them require a - often very large - number of training\nsamples. We present a method, named Label Cleaning Multiple Instance Learning\n(LC-MIL), to refine coarse annotations on a single WSI without the need of\nexternal training data. Patches cropped from a WSI with inaccurate labels are\nprocessed jointly with a MIL framework, and a deep-attention mechanism is\nleveraged to discriminate mislabeled instances, mitigating their impact on the\npredictive model and refining the segmentation. Our experiments on a\nheterogeneous WSI set with breast cancer lymph node metastasis, liver cancer,\nand colorectal cancer samples show that LC-MIL significantly refines the coarse\nannotations, outperforming the state-of-the-art alternatives, even while\nlearning from a single slide. These results demonstrate the LC-MIL is a\npromising, lightweight tool to provide fine-grained annotations from coarsely\nannotated pathology sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popel_A/0/1/0/all/0/1\">Aleksander S. Popel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulam_J/0/1/0/all/0/1\">Jeremias Sulam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural network relief: a pruning algorithm based on neural activity. (arXiv:2109.10795v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10795","description":"<p>Current deep neural networks (DNNs) are overparameterized and use most of\ntheir neuronal connections during inference for each task. The human brain,\nhowever, developed specialized regions for different tasks and performs\ninference with a small fraction of its neuronal connections. We propose an\niterative pruning strategy introducing a simple importance-score metric that\ndeactivates unimportant connections, tackling overparameterization in DNNs and\nmodulating the firing patterns. The aim is to find the smallest number of\nconnections that is still capable of solving a given task with comparable\naccuracy, i.e. a simpler subnetwork. We achieve comparable performance for\nLeNet architectures on MNIST, and significantly higher parameter compression\nthan state-of-the-art algorithms for VGG and ResNet architectures on\nCIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two\ndifferent optimizers considered -- Adam and SGD. The algorithm is not designed\nto minimize FLOPs when considering current hardware and software\nimplementations, although it performs reasonably when compared to the state of\nthe art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dekhovich_A/0/1/0/all/0/1\">Aleksandr Dekhovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tax_D/0/1/0/all/0/1\">David M.J. Tax</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sluiter_M/0/1/0/all/0/1\">Marcel H.F. Sluiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessa_M/0/1/0/all/0/1\">Miguel A. Bessa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DVC-P: Deep Video Compression with Perceptual Optimizations. (arXiv:2109.10849v1 [eess.IV])","link":"http://arxiv.org/abs/2109.10849","description":"<p>Recent years have witnessed the significant development of learning-based\nvideo compression methods, which aim at optimizing objective or perceptual\nquality and bit rates. In this paper, we introduce deep video compression with\nperceptual optimizations (DVC-P), which aims at increasing perceptual quality\nof decoded videos. Our proposed DVC-P is based on Deep Video Compression (DVC)\nnetwork, but improves it with perceptual optimizations. Specifically, a\ndiscriminator network and a mixed loss are employed to help our network trade\noff among distortion, perception and rate. Furthermore, nearest-neighbor\ninterpolation is used to eliminate checkerboard artifacts which can appear in\nsequences encoded with DVC frameworks. Thanks to these two improvements, the\nperceptual quality of decoded sequences is improved. Experimental results\ndemonstrate that, compared with the baseline DVC, our proposed method can\ngenerate videos with higher perceptual quality achieving 12.27% reduction in a\nperceptual BD-rate equivalent, on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Saiping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorriz_M/0/1/0/all/0/1\">Marc G&#xf3;rriz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_S/0/1/0/all/0/1\">Shuai Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fuzheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10852","description":"<p>This paper presents Pix2Seq, a simple and generic framework for object\ndetection. Unlike existing approaches that explicitly integrate prior knowledge\nabout the task, we simply cast object detection as a language modeling task\nconditioned on the observed pixel inputs. Object descriptions (e.g., bounding\nboxes and class labels) are expressed as sequences of discrete tokens, and we\ntrain a neural net to perceive the image and generate the desired sequence. Our\napproach is based mainly on the intuition that if a neural net knows about\nwhere and what the objects are, we just need to teach it how to read them out.\nBeyond the use of task-specific data augmentations, our approach makes minimal\nassumptions about the task, yet it achieves competitive results on the\nchallenging COCO dataset, compared to highly specialized and well optimized\ndetection algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Saurabh Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Batch norm with entropic regularization turns deterministic autoencoders into generative models. (arXiv:2002.10631v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2002.10631","description":"<p>The variational autoencoder is a well defined deep generative model that\nutilizes an encoder-decoder framework where an encoding neural network outputs\na non-deterministic code for reconstructing an input. The encoder achieves this\nby sampling from a distribution for every input, instead of outputting a\ndeterministic code per input. The great advantage of this process is that it\nallows the use of the network as a generative model for sampling from the data\ndistribution beyond provided samples for training. We show in this work that\nutilizing batch normalization as a source for non-determinism suffices to turn\ndeterministic autoencoders into generative models on par with variational ones,\nso long as we add a suitable entropic regularization to the training objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghose_A/0/1/0/all/0/1\">Amur Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashwan_A/0/1/0/all/0/1\">Abdullah Rashwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1\">Pascal Poupart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tighter risk certificates for neural networks. (arXiv:2007.12911v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.12911","description":"<p>This paper presents an empirical study regarding training probabilistic\nneural networks using training objectives derived from PAC-Bayes bounds. In the\ncontext of probabilistic neural networks, the output of training is a\nprobability distribution over network weights. We present two training\nobjectives, used here for the first time in connection with training neural\nnetworks. These two training objectives are derived from tight PAC-Bayes\nbounds. We also re-implement a previously used training objective based on a\nclassical PAC-Bayes bound, to compare the properties of the predictors learned\nusing the different training objectives. We compute risk certificates for the\nlearnt predictors, based on part of the data used to learn the predictors. We\nfurther experiment with different types of priors on the weights (both\ndata-free and data-dependent priors) and neural network architectures. Our\nexperiments on MNIST and CIFAR-10 show that our training methods produce\ncompetitive test set errors and non-vacuous risk bounds with much tighter\nvalues than previous results in the literature, showing promise not only to\nguide the learning algorithm through bounding the risk but also for model\nselection. These observations suggest that the methods studied here might be\ngood candidates for self-certified learning, in the sense of using the whole\ndata set for learning a predictor and certifying its risk on any unseen data\n(from the same distribution as the training data) potentially without the need\nfor holding out test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Ortiz_M/0/1/0/all/0/1\">Mar&#xed;a P&#xe9;rez-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivasplata_O/0/1/0/all/0/1\">Omar Rivasplata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1\">John Shawe-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection of Aedes aegypti Breeding Grounds Based on Deep Networks with Spatio-Temporal Consistency. (arXiv:2007.14863v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.14863","description":"<p>Every year, the Aedes aegypti mosquito infects millions of people with\ndiseases such as dengue, zika, chikungunya, and urban yellow fever. The main\nform to combat these diseases is to avoid mosquito reproduction by searching\nfor and eliminating the potential mosquito breeding grounds. In this work, we\nintroduce a comprehensive dataset of aerial videos, acquired with an unmanned\naerial vehicle, containing possible mosquito breeding sites. All frames of the\nvideo dataset were manually annotated with bounding boxes identifying all\nobjects of interest. This dataset was employed to develop an automatic\ndetection system of such objects based on deep convolutional networks. We\npropose the exploitation of the temporal information contained in the videos by\nthe incorporation, in the object detection pipeline, of a spatio-temporal\nconsistency module that can register the detected objects, minimizing most\nfalse-positive and false-negative occurrences. Using the ResNet-50-FPN as a\nbackbone, we achieve F$_1$-scores of 0.65 and 0.77 on the object-level\ndetection of `tires' and `water tanks', respectively, illustrating the system\ncapabilities to properly locate potential mosquito breeding objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passos_W/0/1/0/all/0/1\">Wesley L. Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_G/0/1/0/all/0/1\">Gabriel M. Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lima_A/0/1/0/all/0/1\">Amaro A. de Lima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netto_S/0/1/0/all/0/1\">Sergio L. Netto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_E/0/1/0/all/0/1\">Eduardo A. B. da Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Model Compression via Stage-wise Pruning. (arXiv:2011.04908v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.04908","description":"<p>Automated Machine Learning(Auto-ML) pruning methods aim at searching a\npruning strategy automatically to reduce the computational complexity of deep\nConvolutional Neural Networks(deep CNNs). However, some previous work found\nthat the results of many Auto-ML pruning methods cannot even surpass the\nresults of the uniformly pruning method. In this paper, the ineffectiveness of\nAuto-ML pruning which is caused by unfull and unfair training of the supernet\nis shown. A deep supernet suffers from unfull training because it contains too\nmany candidates. To overcome the unfull training, a stage-wise pruning(SWP)\nmethod is proposed, which splits a deep supernet into several stage-wise\nsupernets to reduce the candidate number and utilize inplace distillation to\nsupervise the stage training. Besides, A wide supernet is hit by unfair\ntraining since the sampling probability of each channel is unequal. Therefore,\nthe fullnet and the tinynet are sampled in each training iteration to ensure\neach channel can be overtrained. Remarkably, the proxy performance of the\nsubnets trained with SWP is closer to the actual performance than that of most\nof the previous Auto-ML pruning work. Experiments show that SWP achieves the\nstate-of-the-art on both CIFAR-10 and ImageNet under the mobile setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_J/0/1/0/all/0/1\">Jingtao Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSGCNet: A Pyramidal Scale and Global Context Guided Network for Dense Object Counting in Remote Sensing Images. (arXiv:2012.03597v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03597","description":"<p>Object counting, which aims to count the accurate number of object instances\nin images, has been attracting more and more attention. However, challenges\nsuch as large scale variation, complex background interference, and non-uniform\ndensity distribution greatly limit the counting accuracy, particularly striking\nin remote sensing imagery. To mitigate the above issues, this paper proposes a\nnovel framework for dense object counting in remote sensing images, which\nincorporates a pyramidal scale module (PSM) and a global context module (GCM),\ndubbed PSGCNet, where PSM is used to adaptively capture multi-scale information\nand GCM is to guide the model to select suitable scales generated from PSM.\nMoreover, a reliable supervision manner improved from Bayesian and Counting\nloss (BCL) is utilized to learn the density probability and then compute the\ncount expectation at each annotation. It can relieve non-uniform density\ndistribution to a certain extent. Extensive experiments on four remote sensing\ncounting datasets demonstrate the effectiveness of the proposed method and the\nsuperiority of it compared with state-of-the-arts. Additionally, experiments\nextended on four commonly used crowd counting datasets further validate the\ngeneralization ability of the model. Code is available at\nhttps://github.com/gaoguangshuai/PSGCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangshuai Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1\">Qi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter Efficient Multimodal Transformers for Video Representation Learning. (arXiv:2012.04124v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04124","description":"<p>The recent success of Transformers in the language domain has motivated\nadapting it to a multimodal setting, where a new visual model is trained in\ntandem with an already pretrained language model. However, due to the excessive\nmemory requirements from Transformers, existing work typically fixes the\nlanguage model and train only the vision module, which limits its ability to\nlearn cross-modal information in an end-to-end manner. In this work, we focus\non reducing the parameters of multimodal Transformers in the context of\naudio-visual video representation learning. We alleviate the high memory\nrequirement by sharing the parameters of Transformers across layers and\nmodalities; we decompose the Transformer into modality-specific and\nmodality-shared parts so that the model learns the dynamics of each modality\nboth individually and together, and propose a novel parameter sharing scheme\nbased on low-rank approximation. We show that our approach reduces parameters\nof the Transformers up to 97$\\%$, allowing us to train our model end-to-end\nfrom scratch. We also propose a negative sampling approach based on an instance\nsimilarity measured on the CNN embedding space that our model learns together\nwith the Transformers. To demonstrate our approach, we pretrain our model on\n30-second clips (480 frames) from Kinetics-700 and transfer it to audio-visual\nclassification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Temporal Sentence Grounding in Videos: Dataset and Metric. (arXiv:2101.09028v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.09028","description":"<p>Temporal Sentence Grounding in Videos (TSGV), i.e., grounding a natural\nlanguage sentence which indicates complex human activities in a long and\nuntrimmed video sequence, has received unprecedented attentions over the last\nfew years. Although each newly proposed method plausibly can achieve better\nperformance than previous ones, current TSGV models still tend to capture the\nmoment annotation biases and fail to take full advantage of multi-modal inputs.\nEven more incredibly, several extremely simple baselines without training can\nalso achieve state-of-the-art performance. In this paper, we take a closer look\nat the existing evaluation protocols for TSGV, and find that both the\nprevailing dataset splits and evaluation metrics are the devils to cause\nunreliable benchmarking. To this end, we propose to re-organize two widely-used\nTSGV benchmarks (ActivityNet Captions and Charades-STA). Specifically, we\ndeliberately make the ground-truth moment distribution different in the\ntraining and test splits, i.e., out-of-distribution (OOD) testing. Meanwhile,\nwe introduce a new evaluation metric dR@n,IoU@m to calibrate the basic IoU\nscores by penalizing on the bias-influenced moment predictions and alleviate\nthe inflating evaluations caused by the dataset annotation biases such as\noverlong ground-truth moments. Under our new evaluation protocol, we conduct\nextensive experiments and ablation studies on eight state-of-the-art TSGV\nmethods. All the results demonstrate that the re-organized dataset splits and\nnew metric can better monitor the progress in TSGV. Our reorganized datsets are\navailable at https://github.com/yytzsy/grounding_changing_distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yitian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xiaohan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask Guided Attention For Fine-Grained Patchy Image Classification. (arXiv:2102.02771v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.02771","description":"<p>In this work, we present a novel mask guided attention (MGA) method for\nfine-grained patchy image classification. The key challenge of fine-grained\npatchy image classification lies in two folds, ultra-fine-grained\ninter-category variances among objects and very few data available for\ntraining. This motivates us to consider employing more useful supervision\nsignal to train a discriminative model within limited training samples.\nSpecifically, the proposed MGA integrates a pre-trained semantic segmentation\nmodel that produces auxiliary supervision signal, i.e., patchy attention mask,\nenabling a discriminative representation learning. The patchy attention mask\ndrives the classifier to filter out the insignificant parts of images (e.g.,\ncommon features between different categories), which enhances the robustness of\nMGA for the fine-grained patchy image classification. We verify the\neffectiveness of our method on three publicly available patchy image datasets.\nExperimental results demonstrate that our MGA method achieves superior\nperformance on three datasets compared with the state-of-the-art methods. In\naddition, our ablation study shows that MGA improves the accuracy by 2.25% and\n2% on the SoyCultivarVein and BtfPIS datasets, indicating its practicality\ntowards solving the fine-grained patchy image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Mean Teacher for Semi-supervised Chest X-ray Classification. (arXiv:2103.03629v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.03629","description":"<p>The training of deep learning models generally requires a large amount of\nannotated data for effective convergence and generalisation. However, obtaining\nhigh-quality annotations is a laboursome and expensive process due to the need\nof expert radiologists for the labelling task. The study of semi-supervised\nlearning in medical image analysis is then of crucial importance given that it\nis much less expensive to obtain unlabelled images than to acquire images\nlabelled by expert radiologists. Essentially, semi-supervised methods leverage\nlarge sets of unlabelled data to enable better training convergence and\ngeneralisation than using only the small set of labelled images. In this paper,\nwe propose Self-supervised Mean Teacher for Semi-supervised (S$^2$MTS$^2$)\nlearning that combines self-supervised mean-teacher pre-training with\nsemi-supervised fine-tuning. The main innovation of S$^2$MTS$^2$ is the\nself-supervised mean-teacher pre-training based on the joint contrastive\nlearning, which uses an infinite number of pairs of positive query and key\nfeatures to improve the mean-teacher representation. The model is then\nfine-tuned using the exponential moving average teacher framework trained with\nsemi-supervised learning. We validate S$^2$MTS$^2$ on the multi-label\nclassification problems from Chest X-ray14 and CheXpert, and the multi-class\nclassification from ISIC2018, where we show that it outperforms the previous\nSOTA semi-supervised learning methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_F/0/1/0/all/0/1\">Filipe R. Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks on Camera-LiDAR Models for 3D Car Detection. (arXiv:2103.09448v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09448","description":"<p>Most autonomous vehicles (AVs) rely on LiDAR and RGB camera sensors for\nperception. Using these point cloud and image data, perception models based on\ndeep neural nets (DNNs) have achieved state-of-the-art performance in 3D\ndetection. The vulnerability of DNNs to adversarial attacks has been heavily\ninvestigated in the RGB image domain and more recently in the point cloud\ndomain, but rarely in both domains simultaneously. Multi-modal perception\nsystems used in AVs can be divided into two broad types: cascaded models which\nuse each modality independently, and fusion models which learn from different\nmodalities simultaneously. We propose a universal and physically realizable\nadversarial attack for each type, and study and contrast their respective\nvulnerabilities to attacks. We place a single adversarial object with specific\nshape and texture on top of a car with the objective of making this car evade\ndetection. Evaluating on the popular KITTI benchmark, our adversarial object\nmade the host vehicle escape detection by each model type more than 50% of the\ntime. The dense RGB input contributed more to the success of the adversarial\nattacks on both cascaded and fusion models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mazen Abdelfattah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kaiwen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_R/0/1/0/all/0/1\">Rabab Ward</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimising the selection of samples for robust lidar camera calibration. (arXiv:2103.12287v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12287","description":"<p>We propose a robust calibration pipeline that optimises the selection of\ncalibration samples for the estimation of calibration parameters that fit the\nentire scene. We minimise user error by automating the data selection process\naccording to a metric, called Variability of Quality (VOQ) that gives a score\nto each calibration set of samples. We show that this VOQ score is correlated\nwith the estimated calibration parameter's ability to generalise well to the\nentire scene, thereby overcoming the overfitting problems of existing\ncalibration algorithms. Our approach has the benefits of simplifying the\ncalibration process for practitioners of any calibration expertise level and\nproviding an objective measure of the quality for our calibration pipeline's\ninput and output data. We additionally use a novel method of assessing the\naccuracy of the calibration parameters. It involves computing reprojection\nerrors for the entire scene to ensure that the parameters are well fitted to\nall features in the scene. Our proposed calibration pipeline takes 90s, and\nobtains an average reprojection error of 1-1.2cm, with standard deviation of\n0.4-0.5cm over 46 poses evenly distributed in a scene. This process has been\nvalidated by experimentation on a high resolution, software definable lidar,\nBaraja Spectrum-Scan; and a low, fixed resolution lidar, Velodyne VLP-16. We\nhave shown that despite the vast differences in lidar technologies, our\nproposed approach manages to estimate robust calibration parameters for both.\nOur code and data set used for this paper are made available as open-source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_D/0/1/0/all/0/1\">Darren Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worrall_S/0/1/0/all/0/1\">Stewart Worrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1\">Mao Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohr_A/0/1/0/all/0/1\">Anton Lohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nebot_E/0/1/0/all/0/1\">Eduardo Nebot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media. (arXiv:2104.05893v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05893","description":"<p>Online misinformation is a prevalent societal issue, with adversaries relying\non tools ranging from cheap fakes to sophisticated deep fakes. We are motivated\nby the threat scenario where an image is used out of context to support a\ncertain narrative. While some prior datasets for detecting image-text\ninconsistency generate samples via text manipulation, we propose a dataset\nwhere both image and text are unmanipulated but mismatched. We introduce\nseveral strategies for automatically retrieving convincing images for a given\ncaption, capturing cases with inconsistent entities or semantic context. Our\nlarge-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates\nthat machine-driven image repurposing is now a realistic threat, and (2)\nprovides samples that represent challenging instances of mismatch between text\nand image in news that are able to mislead humans. We benchmark several\nstate-of-the-art multimodal models on our dataset and analyze their performance\nacross different pretraining domains and visual backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Grace Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based Person Re-identification without Bells and Whistles. (arXiv:2105.10678v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10678","description":"<p>Video-based person re-identification (Re-ID) aims at matching the video\ntracklets with cropped video frames for identifying the pedestrians under\ndifferent cameras. However, there exists severe spatial and temporal\nmisalignment for those cropped tracklets due to the imperfect detection and\ntracking results generated with obsolete methods. To address this issue, we\npresent a simple re-Detect and Link (DL) module which can effectively reduce\nthose unexpected noise through applying the deep learning-based detection and\ntracking on the cropped tracklets. Furthermore, we introduce an improved model\ncalled Coarse-to-Fine Axial-Attention Network (CF-AAN). Based on the typical\nNon-local Network, we replace the non-local module with three 1-D\nposition-sensitive axial attentions, in addition to our proposed coarse-to-fine\nstructure. With the developed CF-AAN, compared to the original non-local\noperation, we can not only significantly reduce the computation cost but also\nobtain the state-of-the-art performance (91.3% in rank-1 and 86.5% in mAP) on\nthe large-scale MARS dataset. Meanwhile, by simply adopting our DL module for\ndata alignment, to our surprise, several baseline models can achieve better or\ncomparable results with the current state-of-the-arts. Besides, we discover the\nerrors not only for the identity labels of tracklets but also for the\nevaluation protocol for the test data of MARS. We hope that our work can help\nthe community for the further development of invariant representation without\nthe hassle of the spatial and temporal alignment and dataset noise. The code,\ncorrected labels, evaluation protocol, and the aligned data will be available\nat https://github.com/jackie840129/CF-AAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chih-Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun-Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAT: 2D Semantics Assisted Training for 3D Visual Grounding. (arXiv:2105.11450v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11450","description":"<p>3D visual grounding aims at grounding a natural language description about a\n3D scene, usually represented in the form of 3D point clouds, to the targeted\nobject region. Point clouds are sparse, noisy, and contain limited semantic\ninformation compared with 2D images. These inherent limitations make the 3D\nvisual grounding problem more challenging. In this study, we propose 2D\nSemantics Assisted Training (SAT) that utilizes 2D image semantics in the\ntraining stage to ease point-cloud-language joint representation learning and\nassist 3D visual grounding. The main idea is to learn auxiliary alignments\nbetween rich, clean 2D object representations and the corresponding objects or\nmentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object\nlabel, image feature, and 2D geometric feature, as the extra input in training\nbut does not require such inputs during inference. By effectively utilizing 2D\nsemantics in training, our approach boosts the accuracy on the Nr3D dataset\nfrom 37.7% to 49.2%, which significantly surpasses the non-SAT baseline with\nthe identical network architecture and inference input. Our approach\noutperforms the state of the art by large margins on multiple 3D visual\ngrounding datasets, i.e., +10.4% absolute accuracy on Nr3D, +9.9% on Sr3D, and\n+5.6% on ScanRef.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attention Free Transformer. (arXiv:2105.14103v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14103","description":"<p>We introduce Attention Free Transformer (AFT), an efficient variant of\nTransformers that eliminates the need for dot product self attention. In an AFT\nlayer, the key and value are first combined with a set of learned position\nbiases, the result of which is multiplied with the query in an element-wise\nfashion. This new operation has a memory complexity linear w.r.t. both the\ncontext size and the dimension of features, making it compatible to both large\ninput and model sizes. We also introduce AFT-local and AFT-conv, two model\nvariants that take advantage of the idea of locality and spatial weight sharing\nwhile maintaining global connectivity. We conduct extensive experiments on two\nautoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image\nrecognition task (ImageNet-1K classification). We show that AFT demonstrates\ncompetitive performance on all the benchmarks, while providing excellent\nefficiency at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbott_W/0/1/0/all/0/1\">Walter Talbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_N/0/1/0/all/0/1\">Nitish Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hanlin Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anticipative Video Transformer. (arXiv:2106.02036v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02036","description":"<p>We propose Anticipative Video Transformer (AVT), an end-to-end\nattention-based video modeling architecture that attends to the previously\nobserved video in order to anticipate future actions. We train the model\njointly to predict the next action in a video sequence, while also learning\nframe feature encoders that are predictive of successive future frames'\nfeatures. Compared to existing temporal aggregation strategies, AVT has the\nadvantage of both maintaining the sequential progression of observed actions\nwhile still capturing long-range dependencies--both critical for the\nanticipation task. Through extensive experiments, we show that AVT obtains the\nbest reported performance on four popular action anticipation benchmarks:\nEpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads; and it wins\nfirst place in the EpicKitchens-100 CVPR'21 challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GasHisSDB: A New Gastric Histopathology Image Dataset for Computer Aided Diagnosis of Gastric Cancer. (arXiv:2106.02473v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02473","description":"<p>Gastric cancer has turned out to be the fifth most common cancer globally,\nand early detection of gastric cancer is essential to save lives.\nHistopathological examination of gastric cancer is the gold standard for the\ndiagnosis of gastric cancer. However, computer-aided diagnostic techniques are\nchallenging to evaluate due to the scarcity of publicly available gastric\nhistopathology image datasets.In this paper, a noble publicly available Gastric\nHistopathology Sub-size Image Database (GasHisSDB) is published to identify\nclassifiers' performance. Specifically, two types of data are included: normal\nand abnormal, with a total of 245,196 tissue case images.This study also\nperformed extensive experiments using traditional machine learning and deep\nlearning methods to prove that the methods of different periods have\ndiscrepancies on GasHisSDB. To the best of our knowledge, it is the first\npublicly available gastric cancer histopathology dataset containing a large\nnumber of images for weakly supervised learning. We believe that GasHisSDB can\nattract researchers to explore new algorithms for the automated diagnosis of\ngastric cancer, which can help physicians and patients in the clinical setting.\nGasHisSDB is available at the URL:https://gitee.com/neuhwm/GasHisSDB.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiquan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EAR-NET: Error Attention Refining Network For Retinal Vessel Segmentation. (arXiv:2107.01351v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.01351","description":"<p>The precise detection of blood vessels in retinal images is crucial to the\nearly diagnosis of the retinal vascular diseases, e.g., diabetic, hypertensive\nand solar retinopathies. Existing works often fail in predicting the abnormal\nareas, e.g, sudden brighter and darker areas and are inclined to predict a\npixel to background due to the significant class imbalance, leading to high\naccuracy and specificity while low sensitivity. To that end, we propose a novel\nerror attention refining network (ERA-Net) that is capable of learning and\npredicting the potential false predictions in a two-stage manner for effective\nretinal vessel segmentation. The proposed ERA-Net in the refine stage drives\nthe model to focus on and refine the segmentation errors produced in the\ninitial training stage. To achieve this, unlike most previous attention\napproaches that run in an unsupervised manner, we introduce a novel error\nattention mechanism which considers the differences between the ground truth\nand the initial segmentation masks as the ground truth to supervise the\nattention map learning. Experimental results demonstrate that our method\nachieves state-of-the-art performance on two common retinal blood vessel\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1\">Linglong Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TA2N: Two-Stage Action Alignment Network for Few-shot Action Recognition. (arXiv:2107.04782v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04782","description":"<p>Few-shot action recognition aims to recognize novel action classes (query)\nusing just a few samples (support). The majority of current approaches follow\nthe metric learning paradigm, which learns to compare the similarity between\nvideos. Recently, it has been observed that directly measuring this similarity\nis not ideal since different action instances may show distinctive temporal\ndistribution, resulting in severe misalignment issues across query and support\nvideos. In this paper, we arrest this problem from two distinct aspects --\naction duration misalignment and action evolution misalignment. We address them\nsequentially through a Two-stage Action Alignment Network (TA2N). The first\nstage locates the action by learning a temporal affine transform, which warps\neach video feature to its action duration while dismissing the\naction-irrelevant feature (e.g. background). Next, the second stage coordinates\nquery feature to match the spatial-temporal action evolution of support by\nperforming temporally rearrange and spatially offset prediction. Extensive\nexperiments on benchmark datasets show the potential of the proposed method in\nachieving state-of-the-art performance for few-shot action recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huabin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1\">Mengjuan Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Learning with Global Relatedness Decoupled-Distillation. (arXiv:2107.05583v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05583","description":"<p>Despite the success that metric learning based approaches have achieved in\nfew-shot learning, recent works reveal the ineffectiveness of their episodic\ntraining mode. In this paper, we point out two potential reasons for this\nproblem: 1) the random episodic labels can only provide limited supervision\ninformation, while the relatedness information between the query and support\nsamples is not fully exploited; 2) the meta-learner is usually constrained by\nthe limited contextual information of the local episode. To overcome these\nproblems, we propose a new Global Relatedness Decoupled-Distillation (GRDD)\nmethod using the global category knowledge and the Relatedness\nDecoupled-Distillation (RDD) strategy. Our GRDD learns new visual concepts\nquickly by imitating the habit of humans, i.e. learning from the deep knowledge\ndistilled from the teacher. More specifically, we first train a global learner\non the entire base subset using category labels as supervision to leverage the\nglobal context information of the categories. Then, the well-trained global\nlearner is used to simulate the query-support relatedness in global\ndependencies. Finally, the distilled global query-support relatedness is\nexplicitly used to train the meta-learner using the RDD strategy, with the goal\nof making the meta-learner more discriminative. The RDD strategy aims to\ndecouple the dense query-support relatedness into the groups of sparse\ndecoupled relatedness. Moreover, only the relatedness of a single support\nsample with other query samples is considered in each group. By distilling the\nsparse decoupled relatedness group by group, sharper relatedness can be\neffectively distilled to the meta-learner, thereby facilitating the learning of\na discriminative meta-learner. We conduct extensive experiments on the\nminiImagenet and CIFAR-FS datasets, which show the state-of-the-art performance\nof our GRDD method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanrong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shijie Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhengjun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVT: Point-Voxel Transformer for 3D Deep Learning. (arXiv:2108.06076v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06076","description":"<p>In this paper, we present an efficient and high-performance neural\narchitecture, termed Point-Voxel Transformer (PVT)for 3D deep learning, which\ndeeply integrates both 3D voxel-based and point-based self-attention\ncomputation to learn more discriminative features from 3D data. Specifically,\nwe conduct multi-head self-attention (MSA) computation in voxels to obtain the\nefficient learning pattern and the coarse-grained local features while\nperforming self-attention in points to provide finer-grained information about\nthe global context. In addition, to reduce the cost of MSA computation with\nhigh efficiency, we design a cyclic shifted boxing scheme by limiting the MSA\ncomputation to non-overlapping local box and also preserving cross-box\nconnection. Evaluated on classification benchmark, our method not only achieves\nstate-of-the-art accuracy of 94.0% (no voting) but outperforms previous\nTransformer-based models with 7x measured speedup on average. On part and\nsemantic segmentation, our model also obtains strong performance(86.5% and\n68.2% mIoU, respectively). For 3D object detection task, we replace the\nprimitives in Frustrum PointNet with PVT block and achieve an improvement of\n8.6% AP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Haocheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xinyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data. (arXiv:2108.09020v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.09020","description":"<p>Continual learning is the problem of learning and retaining knowledge through\ntime over multiple tasks and environments. Research has primarily focused on\nthe incremental classification setting, where new tasks/classes are added at\ndiscrete time intervals. Such an \"offline\" setting does not evaluate the\nability of agents to learn effectively and efficiently, since an agent can\nperform multiple learning epochs without any time limitation when a task is\nadded. We argue that \"online\" continual learning, where data is a single\ncontinuous stream without task boundaries, enables evaluating both information\nretention and online learning efficacy. In online continual learning, each\nincoming small batch of data is first used for testing and then added to the\ntraining set, making the problem truly online. Trained models are later\nevaluated on historical data to assess information retention. We introduce a\nnew benchmark for online continual visual learning that exhibits large scale\nand natural distribution shifts. Through a large-scale analysis, we identify\ncritical and previously unobserved phenomena of gradient-based optimization in\ncontinual learning, and propose effective strategies for improving\ngradient-based online continual learning with real data. The source code and\ndataset are available in: https://github.com/IntelLabs/continuallearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhipeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sener_O/0/1/0/all/0/1\">Ozan Sener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"External Knowledge enabled Text Visual Question Answering. (arXiv:2108.09717v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09717","description":"<p>The open-ended question answering task of Text-VQA requires reading and\nreasoning about local, often previously unseen, scene-text content of an image\nto generate answers. In this work, we propose the generalized use of external\nknowledge to augment our understanding of the said scene-text. We design a\nframework to extract, validate, and reason with knowledge using a standard\nmultimodal transformer for vision language understanding tasks. Through\nempirical evidence and qualitative results, we demonstrate how external\nknowledge can highlight instance-only cues and thus help deal with training\ndata bias, improve answer entity type correctness, and detect multiword named\nentities. We generate results comparable to the state-of-the-art on two\npublicly available datasets, under the constraints of similar upstream OCR\nsystems and training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arka Ujjal Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harit_G/0/1/0/all/0/1\">Gaurav Harit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transformer-based Semantic Segmentation Networks for Pathological Image Segmentation. (arXiv:2108.11993v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.11993","description":"<p>Histopathology has played an essential role in cancer diagnosis. With the\nrapid advances in convolutional neural networks (CNN). Various CNN-based\nautomated pathological image segmentation approaches have been developed in\ncomputer-assisted pathological image analysis. In the past few years,\nTransformer neural networks (Transformer) have shown the unique merit of\ncapturing the global long-distance dependencies across the entire image as a\nnew deep learning paradigm. Such merit is appealing for exploring spatially\nheterogeneous pathological images. However, there have been very few, if any,\nstudies that have systematically evaluated the current Transformer-based\napproaches in pathological image segmentation. To assess the performance of\nTransformer segmentation models on whole slide images (WSI), we quantitatively\nevaluated six prevalent transformer-based models on tumor segmentation, using\nthe widely used PAIP liver histopathological dataset. For a more comprehensive\nanalysis, we also compare the transformer-based models with six major\ntraditional CNN-based models. The results show that the Transformer-based\nmodels exhibit a general superior performance over the CNN-based models. In\nparticular, Segmenter, Swin-Transformer and TransUNet-all\ntransformer-based-came out as the best performers among the twelve evaluated\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths. (arXiv:2109.03310v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.03310","description":"<p>Malignant melanoma is a common skin cancer that is mostly curable before\nmetastasis -when growths spawn in organs away from the original site. Melanoma\nis the most dangerous type of skin cancer if left untreated due to the high\nrisk of metastasis. This paper presents Melatect, a machine learning (ML) model\nembedded in an iOS app that identifies potential malignant melanoma. Melatect\naccurately classifies lesions as malignant or benign over 96.6% of the time\nwith no apparent bias or overfitting. Using the Melatect app, users have the\nability to take pictures of skin lesions (moles) and subsequently receive a\nmole classification. The Melatect app provides a convenient way to get free\nadvice on lesions and track these lesions over time. A recursive computer image\nanalysis algorithm and modified MLOps pipeline was developed to create a model\nthat performs at a higher accuracy than existing models. Our training dataset\nincluded 18,400 images of benign and malignant lesions, including 18,000 from\nthe International Skin Imaging Collaboration (ISIC) archive, as well as 400\nimages gathered from local dermatologists; these images were augmented using\nDeepAugment, an AutoML tool, to 54,054 images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meel_V/0/1/0/all/0/1\">Vidushi Meel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bodepudi_A/0/1/0/all/0/1\">Asritha Bodepudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04699","description":"<p>While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAN3D: Fast 3D Medical Image Segmentation via Compact Context Aggregation. (arXiv:2109.05443v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.05443","description":"<p>Direct automatic segmentation of objects from 3D medical imaging, such as\nmagnetic resonance (MR) imaging, is challenging as it often involves accurately\nidentifying a number of individual objects with complex geometries within a\nlarge volume under investigation. To address these challenges, most deep\nlearning approaches typically enhance their learning capability by\nsubstantially increasing the complexity or the number of trainable parameters\nwithin their models. Consequently, these models generally require long\ninference time on standard workstations operating clinical MR systems and are\nrestricted to high-performance computing hardware due to their large memory\nrequirement. Further, to fit 3D dataset through these large models using\nlimited computer memory, trade-off techniques such as patch-wise training are\noften used which sacrifice the fine-scale geometric information from input\nimages which could be clinically significant for diagnostic purposes. To\naddress these challenges, we present a compact convolutional neural network\nwith a shallow memory footprint to efficiently reduce the number of model\nparameters required for state-of-art performance. This is critical for\npractical employment as most clinical environments only have low-end hardware\nwith limited computing power and memory. The proposed network can maintain data\nintegrity by directly processing large full-size 3D input volumes with no\npatches required and significantly reduces the computational time required for\nboth training and inference. We also propose a novel loss function with extra\nshape constraint to improve the accuracy for imbalanced classes in 3D MR\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_B/0/1/0/all/0/1\">Boyeong Woo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Siyu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marques_M/0/1/0/all/0/1\">Matthew Marques</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Engstrom_C/0/1/0/all/0/1\">Craig B. Engstrom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greer_P/0/1/0/all/0/1\">Peter B. Greer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crozier_S/0/1/0/all/0/1\">Stuart Crozier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dowling_J/0/1/0/all/0/1\">Jason A. Dowling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1\">Shekhar S. Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAFNe: A One-Stage Anchor-Free Deep Model for Oriented Object Detection. (arXiv:2109.06148v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06148","description":"<p>Object detection is a fundamental task in computer vision. While approaches\nfor axis-aligned bounding box detection have made substantial progress in\nrecent years, they perform poorly on oriented objects which are common in\nseveral real-world scenarios such as aerial view imagery and security camera\nfootage. In these cases, a large part of a predicted bounding box will,\nundesirably, cover non-object related areas. Therefore, oriented object\ndetection has emerged with the aim of generalizing object detection to\narbitrary orientations. This enables a tighter fit to oriented objects, leading\nto a better separation of bounding boxes especially in case of dense object\ndistributions. The vast majority of the work in this area has focused on\ncomplex two-stage anchor-based approaches. Anchors act as priors on the\nbounding box shape and require attentive hyper-parameter fine-tuning on a\nper-dataset basis, increased model size, and come with computational overhead.\nIn this work, we present DAFNe: A Dense one-stage Anchor-Free deep Network for\noriented object detection. As a one-stage model, DAFNe performs predictions on\na dense grid over the input image, being architecturally simpler and faster, as\nwell as easier to optimize than its two-stage counterparts. Furthermore, as an\nanchor-free model, DAFNe reduces the prediction complexity by refraining from\nemploying bounding box anchors. Moreover, we introduce an orientation-aware\ngeneralization of the center-ness function for arbitrarily oriented bounding\nboxes to down-weight low-quality predictions and a center-to-corner bounding\nbox prediction strategy that improves object localization performance. DAFNe\nimproves the prediction accuracy over the previous best one-stage anchor-free\nmodel results on DOTA 1.0 by 4.65% mAP, setting the new state-of-the-art\nresults by achieving 76.95% mAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_S/0/1/0/all/0/1\">Steven Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1\">Fabrizio Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoGG3D-Net: Locally Guided Global Descriptor Learning for 3D Place Recognition. (arXiv:2109.08336v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08336","description":"<p>Retrieval-based place recognition is an efficient and effective solution for\nenabling re-localization within a pre-built map or global data association for\nSimultaneous Localization and Mapping (SLAM). The accuracy of such an approach\nis heavily dependent on the quality of the extracted scene-level\nrepresentation. While end-to-end solutions, which learn a global descriptor\nfrom input point clouds, have demonstrated promising results, such approaches\nare limited in their ability to enforce desirable properties at the local\nfeature level. In this paper, we demonstrate that the inclusion of an\nadditional training signal (local consistency loss) can guide the network to\nlearning local features which are consistent across revisits, hence leading to\nmore repeatable global descriptors resulting in an overall improvement in place\nrecognition performance. We formulate our approach in an end-to-end trainable\narchitecture called LoGG3D-Net. Experiments on two large-scale public\nbenchmarks (KITTI and MulRan) show that our method achieves mean $F1_{max}$\nscores of $0.939$ and $0.968$ on KITTI and MulRan, respectively while operating\nin near real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vidanapathirana_K/0/1/0/all/0/1\">Kavisha Vidanapathirana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Milad Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1\">Peyman Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ElasticFace: Elastic Margin Loss for Deep Face Recognition. (arXiv:2109.09416v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09416","description":"<p>Learning discriminative face features plays a major role in building\nhigh-performing face recognition models. The recent state-of-the-art face\nrecognition solutions proposed to incorporate a fixed penalty margin on\ncommonly used classification loss function, softmax loss, in the normalized\nhypersphere to increase the discriminative power of face recognition models, by\nminimizing the intra-class variation and maximizing the inter-class variation.\nMarginal softmax losses, such as ArcFace and CosFace, assume that the geodesic\ndistance between and within the different identities can be equally learned\nusing a fixed margin. However, such a learning objective is not realistic for\nreal data with inconsistent inter-and intra-class variation, which might limit\nthe discriminative and generalizability of the face recognition model. In this\npaper, we relax the fixed margin constrain by proposing elastic margin loss\n(ElasticFace) that allows flexibility in the push for class separability. The\nmain idea is to utilize random margin values drawn from a normal distribution\nin each training iteration. This aims at giving the margin chances to extract\nand retract to allow space for flexible class separability learning. We\ndemonstrate the superiority of our elastic margin loss over ArcFace and CosFace\nlosses, using the same geometric transformation, on a large set of mainstream\nbenchmarks. From a wider perspective, our ElasticFace has advanced the\nstate-of-the-art face recognition performance on six out of nine mainstream\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation. (arXiv:2109.10115v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10115","description":"<p>We present a large-scale stereo RGB image object pose estimation dataset\nnamed the $\\textbf{StereOBJ-1M}$ dataset. The dataset is designed to address\nchallenging cases such as object transparency, translucency, and specular\nreflection, in addition to the common challenges of occlusion, symmetry, and\nvariations in illumination and environments. In order to collect data of\nsufficient scale for modern deep learning models, we propose a novel method for\nefficiently annotating pose data in a multi-view fashion that allows data\ncapturing in complex and flexible environments. Fully annotated with 6D object\nposes, our dataset contains over 396K frames and over 1.5M annotations of 18\nobjects recorded in 183 scenes constructed in 11 different environments. The 18\nobjects include 8 symmetric objects, 7 transparent objects, and 8 reflective\nobjects. We benchmark two state-of-the-art pose estimation frameworks on\nStereOBJ-1M as baselines for future work. We also propose a novel object-level\npose optimization method for computing 6D pose from keypoint predictions in\nmultiple images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwase_S/0/1/0/all/0/1\">Shun Iwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10282","description":"<p>Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}