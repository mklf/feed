{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Robust Task-Oriented Dialogue Generation with Contrastive Pre-training and Adversarial Filtering. (arXiv:2205.10363v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10363","description":"<p>Data artifacts incentivize machine learning models to learn non-transferable\ngeneralizations by taking advantage of shortcuts in the data, and there is\ngrowing evidence that data artifacts play a role for the strong results that\ndeep learning models achieve in recent natural language processing benchmarks.\nIn this paper, we focus on task-oriented dialogue and investigate whether\npopular datasets such as MultiWOZ contain such data artifacts. We found that by\nonly keeping frequent phrases in the training examples, state-of-the-art models\nperform similarly compared to the variant trained with full data, suggesting\nthey exploit these spurious correlations to solve the task. Motivated by this,\nwe propose a contrastive learning based framework to encourage the model to\nignore these cues and focus on learning generalisable patterns. We also\nexperiment with adversarial filtering to remove \"easy\" training instances so\nthat the model would focus on learning from the \"harder\" instances. We conduct\na number of generalization experiments -- e.g., cross-domain/dataset and\nadversarial tests -- to assess the robustness of our approach and found that it\nworks exceptionally well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinting Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Erfani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modernizing Open-Set Speech Language Identification. (arXiv:2205.10397v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10397","description":"<p>While most modern speech Language Identification methods are closed-set, we\nwant to see if they can be modified and adapted for the open-set problem. When\nswitching to the open-set problem, the solution gains the ability to reject an\naudio input when it fails to match any of our known language options. We tackle\nthe open-set task by adapting two modern-day state-of-the-art approaches to\nclosed-set language identification: the first using a CRNN with attention and\nthe second using a TDNN. In addition to enhancing our input feature embeddings\nusing MFCCs, log spectral features, and pitch, we will be attempting two\napproaches to out-of-set language detection: one using thresholds, and the\nother essentially performing a verification task. We will compare both the\nperformance of the TDNN and the CRNN, as well as our detection approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eyceoz_M/0/1/0/all/0/1\">Mustafa Eyceoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Justin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beigi_H/0/1/0/all/0/1\">Homayoon Beigi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Normalization of Temporal Expressions with Masked Language Models. (arXiv:2205.10399v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10399","description":"<p>The detection and normalization of temporal expressions is an important task\nand a preprocessing step for many applications. However, prior work on\nnormalization is rule-based, which severely limits the applicability in\nreal-world multilingual settings, due to the costly creation of new rules. We\npropose a novel neural method for normalizing temporal expressions based on\nmasked language modeling. Our multilingual method outperforms prior rule-based\nsystems in many languages, and in particular, for low-resource languages with\nperformance improvements of up to 35 F1 on average compared to the state of the\nart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strotgen_J/0/1/0/all/0/1\">Jannik Str&#xf6;tgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog. (arXiv:2205.10400v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10400","description":"<p>Research on (multi-domain) task-oriented dialog (TOD) has predominantly\nfocused on the English language, primarily due to the shortage of robust TOD\ndatasets in other languages, preventing the systematic investigation of\ncross-lingual transfer for this crucial NLP application area. In this work, we\nintroduce Multi2WOZ, a new multilingual multi-domain TOD dataset, derived from\nthe well-established English dataset MultiWOZ, that spans four typologically\ndiverse languages: Chinese, German, Arabic, and Russian. In contrast to\nconcurrent efforts, Multi2WOZ contains gold-standard dialogs in target\nlanguages that are directly comparable with development and test portions of\nthe English dataset, enabling reliable and comparative estimates of\ncross-lingual transfer performance for TOD. We then introduce a new framework\nfor multilingual conversational specialization of pretrained language models\n(PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream\nTOD tasks. Using such conversational PrLMs specialized for concrete target\nlanguages, we systematically benchmark a number of zero-shot and few-shot\ncross-lingual transfer approaches on two standard TOD tasks: Dialog State\nTracking and Response Retrieval. Our experiments show that, in most setups, the\nbest performance entails the combination of (I) conversational specialization\nin the target language and (ii) few-shot transfer for the concrete TOD task.\nMost importantly, we show that our conversational specialization in the target\nlanguage allows for an exceptionally sample-efficient few-shot transfer for\ndownstream TOD tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1\">Chia-Chien Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting COVID-19 Caseloads Using Unsupervised Embedding Clusters of Social Media Posts. (arXiv:2205.10408v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10408","description":"<p>We present a novel approach incorporating transformer-based language models\ninto infectious disease modelling. Text-derived features are quantified by\ntracking high-density clusters of sentence-level representations of Reddit\nposts within specific US states' COVID-19 subreddits. We benchmark these\nclustered embedding features against features extracted from other high-quality\ndatasets. In a threshold-classification task, we show that they outperform all\nother feature types at predicting upward trend signals, a significant result\nfor infectious disease modelling in areas where epidemiological data is\nunreliable. Subsequently, in a time-series forecasting task we fully utilise\nthe predictive power of the caseload and compare the relative strengths of\nusing different supplementary datasets as covariate feature sets in a\ntransformer-based time-series model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drinkall_F/0/1/0/all/0/1\">Felix Drinkall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Educational Tools for Mapuzugun. (arXiv:2205.10411v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10411","description":"<p>Mapuzugun is the language of the Mapuche people. Due to political and\nhistorical reasons, its number of speakers has decreased and the language has\nbeen excluded from the educational system in Chile and Argentina. For this\nreason, it is very important to support the revitalization of the Mapuzugun in\nall spaces and media of society. In this work we present a tool towards\nsupporting educational activities of Mapuzugun, tailored to the characteristics\nof the language. The tool consists of three parts: design and development of an\northography detector and converter; a morphological analyzer; and an informal\ntranslator. We also present a case study with Mapuzugun students showing\npromising results.\n</p>\n<p>Short Abstract in Mapuzuzgun: T\\\"ufachi k\\\"uzaw pegelfi ki\\~ne zugun\nk\\\"uzawpey\\\"um kelluaetew pu mapuzugun chillkatufe kimal kizu ta\\~ni zugun.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahumada_C/0/1/0/all/0/1\">Cristian Ahumada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_C/0/1/0/all/0/1\">Claudio Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Current Trends and Approaches in Synonyms Extraction: Potential Adaptation to Arabic. (arXiv:2205.10412v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10412","description":"<p>Extracting synonyms from dictionaries or corpora is gaining special attention\nas synonyms play an important role in improving NLP application performance.\nThis paper presents a survey of the different approaches and trends used in\nautomatically extracting the synonyms. These approaches can be divided into\nfour main categories. The first approach is to find the Synonyms using a\ntranslation graph. The second approach is to discover new transition pairs such\nas (Arabic-English) (English-France) then (Arabic-France). The third approach\nis to construct new WordNets by exploring synonymy graphs, and the fourth\napproach is to find similar words from corpora using Deep Learning methods,\nsuch as word embeddings and recently BERT models. The paper also presents a\ncomparative analysis between these approaches and highlights potential\nadaptation to generate synonyms automatically in the Arabic language as future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naser_Karajah_E/0/1/0/all/0/1\">Eman Naser-Karajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arman_N/0/1/0/all/0/1\">Nabil Arman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Down and Across: Introducing Crossword-Solving as a New NLP Benchmark. (arXiv:2205.10442v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10442","description":"<p>Solving crossword puzzles requires diverse reasoning capabilities, access to\na vast amount of knowledge about language and the world, and the ability to\nsatisfy the constraints imposed by the structure of the puzzle. In this work,\nwe introduce solving crossword puzzles as a new natural language understanding\ntask. We release the specification of a corpus of crossword puzzles collected\nfrom the New York Times daily crossword spanning 25 years and comprised of a\ntotal of around nine thousand puzzles. These puzzles include a diverse set of\nclues: historic, factual, word meaning, synonyms/antonyms, fill-in-the-blank,\nabbreviations, prefixes/suffixes, wordplay, and cross-lingual, as well as clues\nthat depend on the answers to other clues. We separately release the\nclue-answer pairs from these puzzles as an open-domain question answering\ndataset containing over half a million unique clue-answer pairs. For the\nquestion answering task, our baselines include several sequence-to-sequence and\nretrieval-based generative models. We also introduce a non-parametric\nconstraint satisfaction baseline for solving the entire crossword puzzle.\nFinally, we propose an evaluation framework which consists of several\ncomplementary performance metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_S/0/1/0/all/0/1\">Saurabh Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovaleva_O/0/1/0/all/0/1\">Olga Kovaleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1\">Namrata Shivagunde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for PETs: Using Distributional and Sentiment-Based Methods to Find Potentially Euphemistic Terms. (arXiv:2205.10451v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10451","description":"<p>This paper presents a linguistically driven proof of concept for finding\npotentially euphemistic terms, or PETs. Acknowledging that PETs tend to be\ncommonly used expressions for a certain range of sensitive topics, we make use\nof distributional similarities to select and filter phrase candidates from a\nsentence and rank them using a set of simple sentiment-based metrics. We\npresent the results of our approach tested on a corpus of sentences containing\neuphemisms, demonstrating its efficacy for detecting single and multi-word PETs\nfrom a broad range of topics. We also discuss future potential for\nsentiment-based methods on this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Patrick Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavidia_M/0/1/0/all/0/1\">Martha Gavidia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jing Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection. (arXiv:2205.10455v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10455","description":"<p>An important task for designing QA systems is answer sentence selection\n(AS2): selecting the sentence containing (or constituting) the answer to a\nquestion from a set of retrieved relevant documents. In this paper, we propose\nthree novel sentence-level transformer pre-training objectives that incorporate\nparagraph-level semantics within and across documents, to improve the\nperformance of transformers for AS2, and mitigate the requirement of large\nlabeled datasets. Our experiments on three public and one industrial AS2\ndatasets demonstrate the empirical superiority of our pre-trained transformers\nover baseline models such as RoBERTa and ELECTRA for AS2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liello_L/0/1/0/all/0/1\">Luca Di Liello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddhant Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training. (arXiv:2205.10471v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10471","description":"<p>Keyphrase generation is the task of automatically predicting keyphrases given\na piece of long text. Despite its recent flourishing, keyphrase generation on\nnon-English languages haven't been vastly investigated. In this paper, we call\nattention to a new setting named multilingual keyphrase generation and we\ncontribute two new datasets, EcommerceMKP and AcademicMKP, covering six\nlanguages. Technically, we propose a retrieval-augmented method for\nmultilingual keyphrase generation to mitigate the data shortage problem in\nnon-English languages. The retrieval-augmented model leverages keyphrase\nannotations in English datasets to facilitate generating keyphrases in\nlow-resource languages. Given a non-English passage, a cross-lingual dense\npassage retrieval module finds relevant English passages. Then the associated\nEnglish keyphrases serve as external knowledge for keyphrase generation in the\ncurrent language. Moreover, we develop a retriever-generator iterative training\nalgorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual\npassage retriever. Comprehensive experiments and ablations show that the\nproposed approach outperforms all baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepStruct: Pretraining of Language Models for Structure Prediction. (arXiv:2205.10475v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10475","description":"<p>We introduce a method for improving the structural understanding abilities of\nlanguage models. Unlike previous approaches that finetune the models with\ntask-specific augmentation, we pretrain language models on a collection of\ntask-agnostic corpora to generate structures from text. Our structure\npretraining enables zero-shot transfer of the learned knowledge that models\nhave about the structure tasks. We study the performance of this approach on 28\ndatasets, spanning 10 structure prediction tasks including open information\nextraction, joint entity and relation extraction, named entity recognition,\nrelation classification, semantic role labeling, event extraction, coreference\nresolution, factual probe, intent detection, and dialogue state tracking. We\nfurther enhance the pretraining with the task-specific training sets. We show\nthat a 10B parameter language model transfers non-trivially to most tasks and\nobtains state-of-the-art performance on 21 of 28 datasets that we evaluate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haoyun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DKG: A Descriptive Knowledge Graph for Explaining Relationships between Entities. (arXiv:2205.10479v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10479","description":"<p>In this paper, we propose Descriptive Knowledge Graph (DKG) - an open and\ninterpretable form of modeling relationships between entities. In DKGs,\nrelationships between entities are represented by relation descriptions. For\ninstance, the relationship between entities of machine learning and algorithm\ncan be described as \"Machine learning explores the study and construction of\nalgorithms that can learn from and make predictions on data.\" To construct\nDKGs, we propose a self-supervised learning method to extract relation\ndescriptions with the analysis of dependency patterns and a transformer-based\nrelation description synthesizing model to generate relation descriptions.\nExperiments demonstrate that our system can extract and generate high-quality\nrelation descriptions for explaining entity relationships.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kerui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Linking on Namesakes. (arXiv:2205.10498v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10498","description":"<p>We propose a simple and practical method of named entity linking (NEL), and\nexplore its features and performance on a dataset of ambiguous named entities -\nNamesakes. We represent knowledge base (KB) entity by a set of embeddings. Our\nobservations suggest that it is reasonable to keep a limited number of such\nembeddings, and that the number of mentions required to create a KB entity is\nimportant. We show that representations of entities in the knowledge base (KB)\ncan be adjusted using only KB data, and the adjustment improves NEL\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasilyev_O/0/1/0/all/0/1\">Oleg Vasilyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dauenhauer_A/0/1/0/all/0/1\">Alex Dauenhauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharnidharka_V/0/1/0/all/0/1\">Vedant Dharnidharka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_J/0/1/0/all/0/1\">John Bohannon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeper vs Wider: A Revisit of Transformer Configuration. (arXiv:2205.10505v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10505","description":"<p>Transformer-based models have delivered impressive results on many tasks,\nparticularly vision and language tasks. In many model training situations,\nconventional configurations are typically adopted. For example, we often set\nthe base model with hidden dimensions (i.e. model width) to be 768 and the\nnumber of transformer layers (i.e. model depth) to be 12. In this paper, we\nrevisit these conventional configurations. Through theoretical analysis and\nexperimental evaluation, we show that the masked autoencoder is effective in\nalleviating the over-smoothing issue in deep transformer training. Based on\nthis finding, we propose Bamboo, an idea of using deeper and narrower\ntransformer configurations, for masked autoencoder training. On ImageNet, with\nsuch a simple change in configuration, re-designed model achieves 87.1% top-1\naccuracy and outperforms SoTA models like MAE and BEiT. On language tasks,\nre-designed model outperforms BERT with default setting by 1.1 points on\naverage, on GLUE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianghai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Long Tailed Document-Level Relation Extraction via Easy Relation Augmentation and Contrastive Learning. (arXiv:2205.10511v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10511","description":"<p>Towards real-world information extraction scenario, research of relation\nextraction is advancing to document-level relation extraction(DocRE). Existing\napproaches for DocRE aim to extract relation by encoding various information\nsources in the long context by novel model architectures. However, the inherent\nlong-tailed distribution problem of DocRE is overlooked by prior work. We argue\nthat mitigating the long-tailed distribution problem is crucial for DocRE in\nthe real-world scenario. Motivated by the long-tailed distribution problem, we\npropose an Easy Relation Augmentation(ERA) method for improving DocRE by\nenhancing the performance of tailed relations. In addition, we further propose\na novel contrastive learning framework based on our ERA, i.e., ERACL, which can\nfurther improve the model performance on tailed relations and achieve\ncompetitive overall DocRE performance compared to the state-of-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yangkai Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese. (arXiv:2205.10517v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10517","description":"<p>Multilingual language models such as mBERT have seen impressive cross-lingual\ntransfer to a variety of languages, but many languages remain excluded from\nthese models. In this paper, we analyse the effect of pre-training with\nmonolingual data for a low-resource language that is not included in mBERT --\nMaltese -- with a range of pre-training set ups. We conduct evaluations with\nthe newly pre-trained models on three morphosyntactic tasks -- dependency\nparsing, part-of-speech tagging, and named-entity recognition -- and one\nsemantic classification task -- sentiment analysis. We also present a newly\ncreated corpus for Maltese, and determine the effect that the pre-training data\nsize and domain have on the downstream performance. Our results show that using\na mixture of pre-training domains is often superior to using Wikipedia text\nonly. We also find that a fraction of this corpus is enough to make significant\nleaps in performance over Wikipedia-trained models. We pre-train and compare\ntwo models on the new corpus: a monolingual BERT model trained from scratch\n(BERTu), and a further pre-trained multilingual BERT (mBERTu). The models\nachieve state-of-the-art performance on these tasks, despite the new corpus\nbeing considerably smaller than typically used corpora for high-resourced\nlanguages. On average, BERTu outperforms or performs competitively with mBERTu,\nand the largest gains are observed for higher-level tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Micallef_K/0/1/0/all/0/1\">Kurt Micallef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanti_M/0/1/0/all/0/1\">Marc Tanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plas_L/0/1/0/all/0/1\">Lonneke van der Plas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borg_C/0/1/0/all/0/1\">Claudia Borg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORAL: Contextual Response Retrievability Loss Function for Training Dialog Generation Models. (arXiv:2205.10558v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10558","description":"<p>Natural Language Generation (NLG) represents a large collection of tasks in\nthe field of NLP. While many of these tasks have been tackled well by the\ncross-entropy (CE) loss, the task of dialog generation poses a few unique\nchallenges for this loss function. First, CE loss assumes that for any given\ninput, the only possible output is the one available as the ground truth in the\ntraining dataset. In general, this is not true for any task, as there can be\nmultiple semantically equivalent sentences, each with a different surface form.\nThis problem gets exaggerated further for the dialog generation task, as there\ncan be multiple valid responses (for a given context) that not only have\ndifferent surface forms but are also not semantically equivalent. Second, CE\nloss does not take the context into consideration while processing the response\nand, hence, it treats all ground truths with equal importance irrespective of\nthe context. But, we may want our final agent to avoid certain classes of\nresponses (e.g. bland, non-informative or biased responses) and give relatively\nhigher weightage for more context-specific responses. To circumvent these\nshortcomings of the CE loss, in this paper, we propose a novel loss function,\nCORAL, that directly optimizes recently proposed estimates of human preference\nfor generated responses. Using CORAL, we can train dialog generation models\nwithout assuming non-existence of response other than the ground-truth. Also,\nthe CORAL loss is computed based on both the context and the response.\nExtensive comparisons on two benchmark datasets show that the proposed methods\noutperform strong state-of-the-art baseline models of different sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santra_B/0/1/0/all/0/1\">Bishal Santra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadia_R/0/1/0/all/0/1\">Ravi Ghadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_A/0/1/0/all/0/1\">Arpit Dwivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Sign Language Phoneme Clustering using HamNoSys Notation. (arXiv:2205.10560v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10560","description":"<p>Traditionally, sign language resources have been collected in controlled\nsettings for specific tasks involving supervised sign classification or\nlinguistic studies accompanied by specific annotation type. To date, very few\nwho explored signing videos found online on social media platforms as well as\nthe use of unsupervised methods applied to such resources. Due to the fact that\nthe field is striving to achieve acceptable model performance on the data that\ndiffers from that seen during training calls for more diversity in sign\nlanguage data, stepping away from the data obtained in controlled laboratory\nsettings. Moreover, since the sign language data collection and annotation\ncarries large overheads, it is desirable to accelerate the annotation process.\nConsidering the aforementioned tendencies, this paper takes the side of\nharvesting online data in a pursuit for automatically generating and annotating\nsign language corpora through phoneme clustering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mocialov_B/0/1/0/all/0/1\">Boris Mocialov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_G/0/1/0/all/0/1\">Graham Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1\">Helen Hastie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware Transformer Reranking. (arXiv:2205.10569v1 [cs.IR])","link":"http://arxiv.org/abs/2205.10569","description":"<p>Deep pre-trained language models (e,g. BERT) are effective at large-scale\ntext retrieval task. Existing text retrieval systems with state-of-the-art\nperformance usually adopt a retrieve-then-reranking architecture due to the\nhigh computational cost of pre-trained language models and the large corpus\nsize. Under such a multi-stage architecture, previous studies mainly focused on\noptimizing single stage of the framework thus improving the overall retrieval\nperformance. However, how to directly couple multi-stage features for\noptimization has not been well studied. In this paper, we design Hybrid List\nAware Transformer Reranking (HLATR) as a subsequent reranking module to\nincorporate both retrieval and reranking stage features. HLATR is lightweight\nand can be easily parallelized with existing text retrieval systems so that the\nreranking process can be performed in a single yet efficient processing.\nEmpirical experiments on two large-scale text retrieval datasets show that\nHLATR can efficiently improve the ranking performance of existing multi-stage\ntext retrieval methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_D/0/1/0/all/0/1\">Dingkun Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Autoregressive Neural Machine Translation: A Call for Clarity. (arXiv:2205.10577v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10577","description":"<p>Non-autoregressive approaches aim to improve the inference speed of\ntranslation models by only requiring a single forward pass to generate the\noutput sequence instead of iteratively producing each predicted token.\nConsequently, their translation quality still tends to be inferior to their\nautoregressive counterparts due to several issues involving output token\ninterdependence. In this work, we take a step back and revisit several\ntechniques that have been proposed for improving non-autoregressive translation\nmodels and compare their combined translation quality and speed implications\nunder third-party testing environments. We provide novel insights for\nestablishing strong baselines using length prediction or CTC-based architecture\nvariants and contribute standardized BLEU, chrF++, and TER scores using\nsacreBLEU on four translation tasks, which crucially have been missing as\ninconsistencies in the use of tokenized BLEU lead to deviations of up to 1.7\nBLEU points. Our open-sourced code is integrated into fairseq for\nreproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_R/0/1/0/all/0/1\">Robin M. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_T/0/1/0/all/0/1\">Telmo Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1\">Stephan Peitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loof_J/0/1/0/all/0/1\">Jonas L&#xf6;&#xf6;f</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibration of Natural Language Understanding Models with Venn--ABERS Predictors. (arXiv:2205.10586v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10586","description":"<p>Transformers, currently the state-of-the-art in natural language\nunderstanding (NLU) tasks, are prone to generate uncalibrated predictions or\nextreme probabilities, making the process of taking different decisions based\non their output relatively difficult. In this paper we propose to build several\ninductive Venn--ABERS predictors (IVAP), which are guaranteed to be well\ncalibrated under minimal assumptions, based on a selection of pre-trained\ntransformers. We test their performance over a set of diverse NLU tasks and\nshow that they are capable of producing well-calibrated probabilistic\npredictions that are uniformly spread over the [0,1] interval -- all while\nretaining the original model's predictive accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giovannotti_P/0/1/0/all/0/1\">Patrizio Giovannotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Natural Language Inference Generation with PDD: Prompt and Dynamic Demonstration. (arXiv:2205.10593v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10593","description":"<p>Natural Language Inference Generation task is to generate a text hypothesis\ngiven a text premise and a logical relation between the two. This task can be\nused in data augmentation and controllable text generation in practice. In this\npaper, we propose language models with prompt and dynamic demonstration\n(LM-PDD) to tackle this problem in few-shot settings. Our framework outperforms\nstandard fine-tuned models with low resource, achieving an average 8% absolute\nimprovement on SNLI and MNLI datasets, and the results on 13 natural language\nclassification tasks also show that our dynamic demonstration method has good\ngeneralizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shansan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. (arXiv:2205.10625v1 [cs.AI])","link":"http://arxiv.org/abs/2205.10625","description":"<p>We propose a novel prompting strategy, least-to-most prompting, that enables\nlarge language models to better perform multi-step reasoning tasks.\nLeast-to-most prompting first reduces a complex problem into a list of\nsubproblems, and then sequentially solves the subproblems, whereby solving a\ngiven subproblem is facilitated by the model's answers to previously solved\nsubproblems. Experiments on symbolic manipulation, compositional generalization\nand numerical reasoning demonstrate that least-to-most prompting can generalize\nto examples that are harder than those seen in the prompt context,\noutperforming other prompting-based approaches by a large margin. A notable\nempirical result is that the GPT-3 code-davinci-002 model with\nleast-to-most-prompting can solve the SCAN benchmark with an accuracy of 99.7%\nusing 14 examples. As a comparison, the neural-symbolic models in the\nliterature specialized for solving SCAN are trained with the full training set\nof more than 15,000 examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharli_N/0/1/0/all/0/1\">Nathanael Sch&#xe4;rli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scales_N/0/1/0/all/0/1\">Nathan Scales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bousquet_O/0/1/0/all/0/1\">Olivier Bousquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Speech Representation Learning: A Review. (arXiv:2205.10643v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10643","description":"<p>Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgholt_L/0/1/0/all/0/1\">Lasse Borgholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1\">Jakob D. Havtorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edin_J/0/1/0/all/0/1\">Joakim Edin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1\">Lars Maal&#xf8;e</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics. (arXiv:2205.10646v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10646","description":"<p>Few images on the Web receive alt-text descriptions that would make them\naccessible to blind and low vision (BLV) users. Image-based NLG systems have\nprogressed to the point where they can begin to address this persistent\nsocietal problem, but these systems will not be fully successful unless we\nevaluate them on metrics that guide their development correctly. Here, we argue\nagainst current referenceless metrics -- those that don't rely on\nhuman-generated ground-truth descriptions -- on the grounds that they do not\nalign with the needs of BLV users. The fundamental shortcoming of these metrics\nis that they cannot take context into account, whereas contextual information\nis highly valued by BLV users. To substantiate these claims, we present a study\nwith BLV participants who rated descriptions along a variety of dimensions. An\nin-depth analysis reveals that the lack of context-awareness makes current\nreferenceless metrics inadequate for advancing image accessibility, requiring a\nrethinking of referenceless evaluation metrics for image-based NLG systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1\">Elisa Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_C/0/1/0/all/0/1\">Cynthia Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooshmand_S/0/1/0/all/0/1\">Shayan Hooshmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1\">Eric Zelikman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Meredith Ringel Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Investigation of Commonsense Self-Supervision with Knowledge Graphs. (arXiv:2205.10661v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10661","description":"<p>Self-supervision based on the information extracted from large knowledge\ngraphs has been shown to improve the generalization of language models, in\nzero-shot evaluation on various downstream language reasoning tasks. Since\nthese improvements are reported in aggregate, however, little is known about\n(i) how to select the appropriate knowledge for solid performance across tasks,\n(ii) how to combine this knowledge with neural language models, and (iii) how\nthese pairings affect granular task performance. In this paper, we study the\neffect of knowledge sampling strategies and sizes that can be used to generate\nsynthetic data for adapting language models. We study the effect of different\nsynthetic datasets on language models with various architectures and sizes. The\nresulting models are evaluated against four task properties: domain overlap,\nanswer similarity, vocabulary overlap, and answer length. Our experiments show\nthat encoder-decoder models benefit from more data to learn from, whereas\nsampling strategies that balance across different aspects yield best\nperformance. Most of the improvement occurs on questions with short answers and\ndissimilar answer candidates, which corresponds to the characteristics of the\ndata used for pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiarui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1\">Alessandro Oltramari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Coreference Resolution for Dialogue Processing: Improving Mention-Linking on Real-Time Conversations. (arXiv:2205.10670v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10670","description":"<p>This paper suggests a direction of coreference resolution for online decoding\non actively generated input such as dialogue, where the model accepts an\nutterance and its past context, then finds mentions in the current utterance as\nwell as their referents, upon each dialogue turn. A baseline and four\nincremental-updated models adapted from the mention-linking paradigm are\nproposed for this new setting, which address different aspects including the\nsingletons, speaker-grounded encoding and cross-turn mention contextualization.\nOur approach is assessed on three datasets: Friends, OntoNotes, and BOLT.\nResults show that each aspect brings out steady improvement, and our best\nmodels outperform the baseline by over 10%, presenting an effective system for\nthis setting. Further analysis highlights the task characteristics, such as the\nsignificance of addressing the mention recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding. (arXiv:2205.10687v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10687","description":"<p>There is a growing body of work in recent years to develop pre-trained\nlanguage models (PLMs) for the Arabic language. This work concerns addressing\ntwo major problems in existing Arabic PLMs which constraint progress of the\nArabic NLU and NLG fields.First, existing Arabic PLMs are not well-explored and\ntheir pre-trainig can be improved significantly using a more methodical\napproach. Second, there is a lack of systematic and reproducible evaluation of\nthese models in the literature. In this work, we revisit both the pre-training\nand evaluation of Arabic PLMs. In terms of pre-training, we explore improving\nArabic LMs from three perspectives: quality of the pre-training data, size of\nthe model, and incorporating character-level information. As a result, we\nrelease three new Arabic BERT-style models ( JABER, Char-JABER, and SABER), and\ntwo T5-style models (AT5S and AT5B). In terms of evaluation, we conduct a\ncomprehensive empirical study to systematically evaluate the performance of\nexisting state-of-the-art models on ALUE that is a leaderboard-powered\nbenchmark for Arabic NLU tasks, and on a subset of the ARGEN benchmark for\nArabic NLG tasks. We show that our models significantly outperform existing\nArabic PLMs and achieve a new state-of-the-art performance on discriminative\nand generative Arabic NLU and NLG tasks. Our models and source code to\nreproduce of results will be made available shortly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yimeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagga_S/0/1/0/all/0/1\">Sunyam Bagga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_K/0/1/0/all/0/1\">Khalil Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinyu_D/0/1/0/all/0/1\">Duan Xinyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Life after BERT: What do Other Muppets Understand about Language?. (arXiv:2205.10696v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10696","description":"<p>Existing pre-trained transformer analysis works usually focus only on one or\ntwo model families at a time, overlooking the variability of the architecture\nand pre-training objectives. In our work, we utilize the oLMpics benchmark and\npsycholinguistic probing datasets for a diverse set of 29 models including T5,\nBART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for\nautoregressive models and evaluate GPT networks of different sizes. Our\nfindings show that none of these models can resolve compositional questions in\na zero-shot fashion, suggesting that this skill is not learnable using existing\npre-training objectives. Furthermore, we find that global model decisions such\nas architecture, directionality, size of the dataset, and pre-training\nobjective are not predictive of a model's linguistic capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kevin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1\">Namrata Shivagunde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase-level Textual Adversarial Attack with Label Preservation. (arXiv:2205.10710v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10710","description":"<p>Generating high-quality textual adversarial examples is critical for\ninvestigating the pitfalls of natural language processing (NLP) models and\nfurther promoting their robustness. Existing attacks are usually realized\nthrough word-level or sentence-level perturbations, which either limit the\nperturbation space or sacrifice fluency and textual quality, both affecting the\nattack effectiveness. In this paper, we propose Phrase-Level Textual\nAdversarial aTtack (PLAT) that generates adversarial samples through\nphrase-level perturbations. PLAT first extracts the vulnerable phrases as\nattack targets by a syntactic parser, and then perturbs them by a pre-trained\nblank-infilling model. Such flexible perturbation design substantially expands\nthe search space for more effective attacks without introducing too many\nmodifications, and meanwhile maintaining the textual fluency and grammaticality\nvia contextualized generation using surrounding texts. Moreover, we develop a\nlabel-preservation filter leveraging the likelihoods of language models\nfine-tuned on each class, rather than textual similarity, to rule out those\nperturbations that potentially alter the original class label for humans.\nExtensive experiments and human evaluation demonstrate that PLAT has a superior\nattack effectiveness as well as a better label consistency than strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yibin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dianqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Proof Generation via Iterative Backward Reasoning. (arXiv:2205.10714v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10714","description":"<p>We present IBR, an Iterative Backward Reasoning model to solve the proof\ngeneration tasks on rule-based Question Answering (QA), where models are\nrequired to reason over a series of textual rules and facts to find out the\nrelated proof path and derive the final answer. We handle the limitations of\nexisted works in two folds: 1) enhance the interpretability of reasoning\nprocedures with detailed tracking, by predicting nodes and edges in the proof\npath iteratively backward from the question; 2) promote the efficiency and\naccuracy via reasoning on the elaborate representations of nodes and history\npaths, without any intermediate texts that may introduce external noise during\nproof generation. There are three main modules in IBR, QA and proof strategy\nprediction to obtain the answer and offer guidance for the following procedure;\nparent node prediction to determine a node in the existing proof that a new\nchild node will link to; child node prediction to find out which new node will\nbe added to the proof. Experiments on both synthetic and paraphrased datasets\ndemonstrate that IBR has better in-domain performance as well as cross-domain\ntransferability than several strong baselines. Our code and models are\navailable at https://github.com/find-knowledge/IBR .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Hanhao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TWEET-FID: An Annotated Dataset for Multiple Foodborne Illness Detection Tasks. (arXiv:2205.10726v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10726","description":"<p>Foodborne illness is a serious but preventable public health problem -- with\ndelays in detecting the associated outbreaks resulting in productivity loss,\nexpensive recalls, public safety hazards, and even loss of life. While social\nmedia is a promising source for identifying unreported foodborne illnesses,\nthere is a dearth of labeled datasets for developing effective outbreak\ndetection models. To accelerate the development of machine learning-based\nmodels for foodborne outbreak detection, we thus present TWEET-FID\n(TWEET-Foodborne Illness Detection), the first publicly available annotated\ndataset for multiple foodborne illness incident detection tasks. TWEET-FID\ncollected from Twitter is annotated with three facets: tweet class, entity\ntype, and slot type, with labels produced by experts as well as by crowdsource\nworkers. We introduce several domain tasks leveraging these three facets: text\nrelevance classification (TRC), entity mention detection (EMD), and slot\nfilling (SF). We describe the end-to-end methodology for dataset design,\ncreation, and labeling for supporting model development for these tasks. A\ncomprehensive set of results for these tasks leveraging state-of-the-art\nsingle- and multi-task deep learning methods on the TWEET-FID dataset are\nprovided. This dataset opens opportunities for future research in foodborne\noutbreak detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruofan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dandan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartvigsen_T/0/1/0/all/0/1\">Thomas Hartvigsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rundensteiner_E/0/1/0/all/0/1\">Elke Rundensteiner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All Birds with One Stone: Multi-task Text Classification for Efficient Inference with One Forward Pass. (arXiv:2205.10744v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10744","description":"<p>Multi-Task Learning (MTL) models have shown their robustness, effectiveness,\nand efficiency for transferring learned knowledge across tasks. In real\nindustrial applications such as web content classification, multiple\nclassification tasks are predicted from the same input text such as a web\narticle. However, at the serving time, the existing multitask transformer\nmodels such as prompt or adaptor based approaches need to conduct N forward\npasses for N tasks with O(N) computation cost. To tackle this problem, we\npropose a scalable method that can achieve stronger performance with close to\nO(1) computation cost via only one forward pass. To illustrate real application\nusage, we release a multitask dataset on news topic and style classification.\nOur experiments show that our proposed method outperforms strong baselines on\nboth the GLUE benchmark and our news dataset. Our code and dataset are publicly\navailable at https://bit.ly/mtop-code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lelkes_A/0/1/0/all/0/1\">Adam D. Lelkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts. (arXiv:2205.10762v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10762","description":"<p>Neural Machine Translation systems built on top of Transformer-based\narchitectures are routinely improving the state-of-the-art in translation\nquality according to word-overlap metrics. However, a growing number of studies\nalso highlight the inherent gender bias that these models incorporate during\ntraining, which reflects poorly in their translations. In this work, we\ninvestigate whether these models can be instructed to fix their bias during\ninference using targeted, guided instructions as contexts. By translating\nrelevant contextual sentences during inference along with the input, we observe\nlarge improvements in reducing the gender bias in translations, across three\npopular test suites (WinoMT, BUG, SimpleGen). We further propose a novel metric\nto assess several large pretrained models (OPUS-MT, M2M-100) on their\nsensitivity towards using contexts during translation to correct their biases.\nOur approach requires no fine-tuning, and thus can be used easily in production\nsystems to de-bias translations from stereotypical gender-occupation bias. We\nhope our method, along with our metric, can be used to build better, bias-free\ntranslation systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shanya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidence for Hypodescent in Visual Semantic AI. (arXiv:2205.10764v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10764","description":"<p>We examine the state-of-the-art multimodal \"visual semantic\" model CLIP\n(\"Contrastive Language Image Pretraining\") for the rule of hypodescent, or\none-drop rule, whereby multiracial people are more likely to be assigned a\nracial or ethnic label corresponding to a minority or disadvantaged racial or\nethnic group than to the equivalent majority or advantaged group. A face\nmorphing experiment grounded in psychological research demonstrating\nhypodescent indicates that, at the midway point of 1,000 series of morphed\nimages, CLIP associates 69.7% of Black-White female images with a Black text\nlabel over a White text label, and similarly prefers Latina (75.8%) and Asian\n(89.1%) text labels at the midway point for Latina-White female and Asian-White\nfemale morphs, reflecting hypodescent. Additionally, assessment of the\nunderlying cosine similarities in the model reveals that association with White\nis correlated with association with \"person,\" with Pearson's rho as high as\n0.82 over a 21,000-image morph series, indicating that a White person\ncorresponds to the default representation of a person in CLIP. Finally, we show\nthat the stereotype-congruent pleasantness association of an image correlates\nwith association with the Black text label in CLIP, with Pearson's rho = 0.48\nfor 21,000 Black-White multiracial male images, and rho = 0.41 for Black-White\nmultiracial female images. CLIP is trained on English-language text gathered\nusing data collected from an American website (Wikipedia), and our findings\ndemonstrate that CLIP embeds the values of American racial hierarchy,\nreflecting the implicit and explicit beliefs that are present in human minds.\nWe contextualize these findings within the history and psychology of\nhypodescent. Overall, the data suggests that AI supervised using natural\nlanguage will, unless checked, learn biases that reflect racial hierarchies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaji_M/0/1/0/all/0/1\">Mahzarin R. Banaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models. (arXiv:2205.10770v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10770","description":"<p>Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tirumala_K/0/1/0/all/0/1\">Kushal Tirumala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markosyan_A/0/1/0/all/0/1\">Aram H. Markosyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Domain-adaptive Pre-training Approach for Language Bias Detection in News. (arXiv:2205.10773v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10773","description":"<p>Media bias is a multi-faceted construct influencing individual behavior and\ncollective decision-making. Slanted news reporting is the result of one-sided\nand polarized writing which can occur in various forms. In this work, we focus\non an important form of media bias, i.e. bias by word choice. Detecting biased\nword choices is a challenging task due to its linguistic complexity and the\nlack of representative gold-standard corpora. We present DA-RoBERTa, a new\nstate-of-the-art transformer-based model adapted to the media bias domain which\nidentifies sentence-level bias with an F1 score of 0.814. In addition, we also\ntrain, DA-BERT and DA-BART, two more transformer models adapted to the bias\ndomain. Our proposed domain-adapted models outperform prior bias detection\napproaches on the same data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krieger_J/0/1/0/all/0/1\">Jan-David Krieger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1\">Timo Spinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulshrestha_J/0/1/0/all/0/1\">Juhi Kulshrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Induction: From Few Examples to Natural Language Task Descriptions. (arXiv:2205.10782v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10782","description":"<p>Large language models are able to perform a task by conditioning on a few\ninput-output demonstrations - a paradigm known as in-context learning. We show\nthat language models can explicitly infer an underlying task from a few\ndemonstrations by prompting them to generate a natural language instruction\nthat fits the examples. To explore this ability, we introduce the instruction\ninduction challenge, compile a dataset consisting of 24 tasks, and define a\nnovel evaluation metric based on executing the generated instruction. We\ndiscover that, to a large extent, the ability to generate instructions does\nindeed emerge when using a model that is both large enough and aligned to\nfollow instructions; InstructGPT achieves 65.7% of human performance in our\nexecution-based metric, while the original GPT-3 model reaches only 9.8% of\nhuman performance. This surprising result suggests that instruction induction\nmight be a viable learning paradigm in and of itself, where instead of fitting\na set of latent continuous parameters to the data, one searches for the best\ndescription in the natural language hypothesis space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honovich_O/0/1/0/all/0/1\">Or Honovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Graph Enhanced BERT Model for Event Prediction. (arXiv:2205.10822v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10822","description":"<p>Predicting the subsequent event for an existing event context is an important\nbut challenging task, as it requires understanding the underlying relationship\nbetween events. Previous methods propose to retrieve relational features from\nevent graph to enhance the modeling of event correlation. However, the sparsity\nof event graph may restrict the acquisition of relevant graph information, and\nhence influence the model performance. To address this issue, we consider\nautomatically building of event graph using a BERT model. To this end, we\nincorporate an additional structured variable into BERT to learn to predict the\nevent connections in the training process. Hence, in the test process, the\nconnection relationship for unseen events can be predicted by the structured\nvariable. Results on two event prediction tasks: script event prediction and\nstory ending prediction, show that our approach can outperform state-of-the-art\nbaseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1\">Kai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10828","description":"<p>Recently, very large pre-trained models achieve state-of-the-art results in\nvarious natural language processing (NLP) tasks, but their size makes it more\nchallenging to apply them in resource-constrained environments. Compression\ntechniques allow to drastically reduce the size of the model and therefore its\ninference time with negligible impact on top-tier metrics. However, the general\nperformance hides a drastic performance drop on under-represented features,\nwhich could result in the amplification of biases encoded by the model. In this\nwork, we analyze the impacts of compression methods on Multilingual Neural\nMachine Translation models (MNMT) for various language groups and semantic\nfeatures by extensive analysis of compressed models on different NMT\nbenchmarks, e.g. FLORES-101, MT-Gender, and DiBiMT. Our experiments show that\nthe performance of under-represented languages drops significantly, while the\naverage BLEU metric slightly decreases. Interestingly, the removal of noisy\nmemorization with the compression leads to a significant improvement for some\nmedium-resource languages. Finally, we demonstrate that the compression\namplifies intrinsic gender and semantic biases, even in high-resource\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1\">Caroline Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Machine Translation with Hyper-Adapters. (arXiv:2205.10835v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10835","description":"<p>Multilingual machine translation suffers from negative interference across\nlanguages. A common solution is to relax parameter sharing with\nlanguage-specific modules like adapters. However, adapters of related languages\nare unable to transfer information, and their total number of parameters\nbecomes prohibitively expensive as the number of languages grows. In this work,\nwe overcome these drawbacks using hyper-adapters -- hyper-networks that\ngenerate adapters from language and layer embeddings. While past work had poor\nresults when scaling hyper-networks, we propose a rescaling fix that\nsignificantly improves convergence and enables training larger hyper-networks.\nWe find that hyper-adapters are more parameter efficient than regular adapters,\nreaching the same performance with up to 12 times less parameters. When using\nthe same number of parameters and FLOPS, our approach consistently outperforms\nregular adapters. Also, hyper-adapters converge faster than alternative\napproaches and scale better than regular dense networks. Our analysis shows\nthat hyper-adapters learn to encode language relatedness, enabling positive\ntransfer across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baziotis_C/0/1/0/all/0/1\">Christos Baziotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Mahalanobis Distance for Transformer-Based Out-of-Domain Detection. (arXiv:2101.03778v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.03778","description":"<p>Real-life applications, heavily relying on machine learning, such as dialog\nsystems, demand out-of-domain detection methods. Intent classification models\nshould be equipped with a mechanism to distinguish seen intents from unseen\nones so that the dialog agent is capable of rejecting the latter and avoiding\nundesired behavior. However, despite increasing attention paid to the task, the\nbest practices for out-of-domain intent detection have not yet been fully\nestablished.\n</p>\n<p>This paper conducts a thorough comparison of out-of-domain intent detection\nmethods. We prioritize the methods, not requiring access to out-of-domain data\nduring training, gathering of which is extremely time- and labor-consuming due\nto lexical and stylistic variation of user utterances. We evaluate multiple\ncontextual encoders and methods, proven to be efficient, on three standard\ndatasets for intent classification, expanded with out-of-domain utterances. Our\nmain findings show that fine-tuning Transformer-based encoders on in-domain\ndata leads to superior results. Mahalanobis distance, together with utterance\nrepresentations, derived from Transformer-based encoders, outperforms other\nmethods by a wide margin and establishes new state-of-the-art results for all\ndatasets.\n</p>\n<p>The broader analysis shows that the reason for success lies in the fact that\nthe fine-tuned Transformer is capable of constructing homogeneous\nrepresentations of in-domain utterances, revealing geometrical disparity to out\nof domain utterances. In turn, the Mahalanobis distance captures this disparity\neasily.\n</p>\n<p>The code is available in our GitHub repo:\nhttps://github.com/huawei-noah/noah-research/tree/master/Maha_OOD .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Podolskiy_A/0/1/0/all/0/1\">Alexander Podolskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipin_D/0/1/0/all/0/1\">Dmitry Lipin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bout_A/0/1/0/all/0/1\">Andrey Bout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Structured Feature Networks for Table Detection and Tabular Data Extraction from Scanned Financial Document Images. (arXiv:2102.10287v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.10287","description":"<p>Automatic table detection in PDF documents has achieved a great success but\ntabular data extraction are still challenging due to the integrity and noise\nissues in detected table areas. The accurate data extraction is extremely\ncrucial in finance area. Inspired by this, the aim of this research is\nproposing an automated table detection and tabular data extraction from\nfinancial PDF documents. We proposed a method that consists of three main\nprocesses, which are detecting table areas with a Faster R-CNN (Region-based\nConvolutional Neural Network) model with Feature Pyramid Network (FPN) on each\npage image, extracting contents and structures by a compounded layout\nsegmentation technique based on optical character recognition (OCR) and\nformulating regular expression rules for table header separation. The tabular\ndata extraction feature is embedded with rule-based filtering and restructuring\nfunctions that are highly scalable. We annotate a new Financial Documents\ndataset with table regions for the experiment. The excellent table detection\nperformance of the detection model is obtained from our customized dataset. The\nmain contributions of this paper are proposing the Financial Documents dataset\nwith table-area annotations, the superior detection model and the rule-based\nlayout segmentation technique for the tabular data extraction from PDF files.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Siwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengting Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yiwen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wanying Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiGCN: Label-interpretable Graph Convolutional Networks for Multi-label Text Classification. (arXiv:2103.14620v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14620","description":"<p>Multi-label text classification (MLTC) is an attractive and challenging task\nin natural language processing (NLP). Compared with single-label text\nclassification, MLTC has a wider range of applications in practice. In this\npaper, we propose a label-interpretable graph convolutional network model to\nsolve the MLTC problem by modeling tokens and labels as nodes in a\nheterogeneous graph. In this way, we are able to take into account multiple\nrelationships including token-level relationships. Besides, the model allows\nbetter interpretability for predicted labels as the token-label edges are\nexposed. We evaluate our method on four real-world datasets and it achieves\ncompetitive scores against selected baseline methods. Specifically, this model\nachieves a gain of 0.14 on the F1 score in the small label set MLTC, and 0.07\nin the large label set scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Aosong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzumura_T/0/1/0/all/0/1\">Toyotaro Suzumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Schema Curation via Causal Association Rule Mining. (arXiv:2104.08811v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08811","description":"<p>Event schemas are structured knowledge sources defining typical real-world\nscenarios (e.g., going to an airport). We present a framework for efficient\nhuman-in-the-loop construction of a schema library, based on a novel script\ninduction system and a well-crafted interface that allows non-experts to\n\"program\" complex event structures. Associated with this work we release a\nschema library: a machine readable resource of 232 detailed event schemas, each\nof which describe a distinct typical scenario in terms of its relevant\nsub-event structure (what happens in the scenario), participants (who plays a\nrole in the scenario), fine-grained typing of each participant, and the implied\nrelational constraints between them. We make our schema library and the\nSchemaBlocks interface available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_N/0/1/0/all/0/1\">Noah Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belyy_A/0/1/0/all/0/1\">Anton Belyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1\">Nils Holzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency Parsing as MRC-based Span-Span Prediction. (arXiv:2105.07654v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07654","description":"<p>Higher-order methods for dependency parsing can partially but not fully\naddress the issue that edges in dependency trees should be constructed at the\ntext span/subtree level rather than word level. In this paper, we propose a new\nmethod for dependency parsing to address this issue. The proposed method\nconstructs dependency trees by directly modeling span-span (in other words,\nsubtree-subtree) relations. It consists of two modules: the {\\it text span\nproposal module} which proposes candidate text spans, each of which represents\na subtree in the dependency tree denoted by (root, start, end); and the {\\it\nspan linking module}, which constructs links between proposed spans. We use the\nmachine reading comprehension (MRC) framework as the backbone to formalize the\nspan linking module, where one span is used as a query to extract the text\nspan/subtree it should be linked to. The proposed method has the following\nmerits: (1) it addresses the fundamental problem that edges in a dependency\ntree should be constructed between subtrees; (2) the MRC framework allows the\nmethod to retrieve missing spans in the span proposal stage, which leads to\nhigher recall for eligible spans. Extensive experiments on the PTB, CTB and\nUniversal Dependencies (UD) benchmarks demonstrate the effectiveness of the\nproposed method. The code is available at\n\\url{https://github.com/ShannonAI/mrc-for-dependency-parsing}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Leilei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conscious AI. (arXiv:2105.07879v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2105.07879","description":"<p>Recent advances in artificial intelligence (AI) have achieved human-scale\nspeed and accuracy for classification tasks. In turn, these capabilities have\nmade AI a viable replacement for many human activities that at their core\ninvolve classification, such as basic mechanical and analytical tasks in\nlow-level service jobs. Current systems do not need to be conscious to\nrecognize patterns and classify them. However, for AI to progress to more\ncomplicated tasks requiring intuition and empathy, it must develop capabilities\nsuch as metathinking, creativity, and empathy akin to human self-awareness or\nconsciousness. We contend that such a paradigm shift is possible only through a\nfundamental shift in the state of artificial intelligence toward consciousness,\na shift similar to what took place for humans through the process of natural\nselection and evolution. As such, this paper aims to theoretically explore the\nrequirements for the emergence of consciousness in AI. It also provides a\nprincipled understanding of how conscious AI can be detected and how it might\nbe manifested in contrast to the dominant paradigm that seeks to ultimately\ncreate machines that are linguistically indistinguishable from humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilzadeh_H/0/1/0/all/0/1\">Hadi Esmaeilzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaezi_R/0/1/0/all/0/1\">Reza Vaezi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEAP: Learnable Pruning for Transformer-based Models. (arXiv:2105.14636v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14636","description":"<p>Pruning is an effective method to reduce the memory footprint and\ncomputational cost associated with large natural language processing models.\nHowever, current pruning algorithms either only focus on one pruning category,\ne.g., structured pruning and unstructured, or need extensive hyperparameter\ntuning in order to get reasonable accuracy performance. To address these\nchallenges, we propose LEArnable Pruning (LEAP), an effective method to\ngradually prune the model based on thresholds learned by gradient descent.\nDifferent than previous learnable pruning methods, which utilize $L_0$ or $L_1$\npenalty to indirectly affect the final pruning ratio, LEAP introduces a novel\nregularization function, that directly interacts with the preset target pruning\nratio. Moreover, in order to reduce hyperparameter tuning, a novel adaptive\nregularization coefficient is deployed to control the regularization penalty\nadaptively. With the new regularization term and its associated adaptive\nregularization coefficient, LEAP is able to be applied for different pruning\ngranularity, including unstructured pruning, structured pruning, and hybrid\npruning, with minimal hyperparameter tuning. We apply LEAP for BERT models on\nQQP/MNLI/SQuAD for different pruning settings. Our result shows that for all\ndatasets, pruning granularity, and pruning ratios, LEAP achieves on-par or\nbetter results as compared to previous heavily hand-tuned methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Linjian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WALNUT: A Benchmark on Semi-weakly Supervised Learning for Natural Language Understanding. (arXiv:2108.12603v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12603","description":"<p>Building machine learning models for natural language understanding (NLU)\ntasks relies heavily on labeled data. Weak supervision has been proven valuable\nwhen large amount of labeled data is unavailable or expensive to obtain.\nExisting works studying weak supervision for NLU either mostly focus on a\nspecific task or simulate weak supervision signals from ground-truth labels. It\nis thus hard to compare different approaches and evaluate the benefit of weak\nsupervision without access to a unified and systematic benchmark with diverse\ntasks and real-world weak labeling rules. In this paper, we propose such a\nbenchmark, named WALNUT (semi-WeAkly supervised Learning for Natural language\nUnderstanding Testbed), to advocate and facilitate research on weak supervision\nfor NLU. WALNUT consists of NLU tasks with different types, including\ndocument-level and token-level prediction tasks. WALNUT is the first\nsemi-weakly supervised learning benchmark for NLU, where each task contains\nweak labels generated by multiple real-world weak sources, together with a\nsmall set of clean labels. We conduct baseline evaluations on WALNUT to\nsystematically evaluate the effectiveness of various weak supervision methods\nand model architectures. Our results demonstrate the benefit of weak\nsupervision for low-resource NLU tasks and highlight interesting patterns\nacross tasks. We expect WALNUT to stimulate further research on methodologies\nto leverage weak supervision more effectively. The benchmark and code for\nbaselines are available at \\url{aka.ms/walnut_benchmark}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamanolakis_G/0/1/0/all/0/1\">Giannis Karamanolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements. (arXiv:2109.01226v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01226","description":"<p>More predictable words are easier to process - they are read faster and\nelicit smaller neural signals associated with processing difficulty, most\nnotably, the N400 component of the event-related brain potential. Thus, it has\nbeen argued that prediction of upcoming words is a key component of language\ncomprehension, and that studying the amplitude of the N400 is a valuable way to\ninvestigate the predictions we make. In this study, we investigate whether the\nlinguistic predictions of computational language models or humans better\nreflect the way in which natural language stimuli modulate the amplitude of the\nN400. One important difference in the linguistic predictions of humans versus\ncomputational language models is that while language models base their\npredictions exclusively on the preceding linguistic context, humans may rely on\nother factors. We find that the predictions of three top-of-the-line\ncontemporary language models - GPT-3, RoBERTa, and ALBERT - match the N400 more\nclosely than human predictions. This suggests that the predictive processes\nunderlying the N400 may be more sensitive to the surface-level statistics of\nlanguage than previously thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1\">Seana Coulson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification. (arXiv:2109.07319v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07319","description":"<p>Automatic annotation of short-text data to a large number of target labels,\nreferred to as Short Text Extreme Classification, has found numerous\napplications including prediction of related searches and product\nrecommendation tasks. In this paper, we propose a convolutional architecture\nInceptionXML which is light-weight, yet powerful, and robust to the inherent\nlack of word-order in short-text queries encountered in search and\nrecommendation tasks. We demonstrate the efficacy of applying convolutions by\nrecasting the operation along the embedding dimension instead of the word\ndimension as applied in conventional CNNs for text classification. Towards\nscaling our model to datasets with millions of labels, we also propose\nInceptionXML+ framework which improves upon the shortcomings of the recently\nproposed dynamic hard-negative mining technique for label shortlisting by\nsynchronizing the label-shortlister and extreme classifier. InceptionXML+ not\nonly reduces the inference time to half but is also an order of magnitude\nsmaller than previous state-of-the-art Astec in terms of model size. Through\nour proposed models, we outperform all existing approaches on popular benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharbanda_S/0/1/0/all/0/1\">Siddhant Kharbanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Atmadeep Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palrecha_A/0/1/0/all/0/1\">Akash Palrecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Devaansh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babbar_R/0/1/0/all/0/1\">Rohit Babbar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Spread of Propaganda by Coordinated Communities on Social Media. (arXiv:2109.13046v3 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2109.13046","description":"<p>Large-scale manipulations on social media have two important characteristics:\n(i) use of propaganda to influence others, and (ii) adoption of coordinated\nbehavior to spread it and to amplify its impact. Despite the connection between\nthem, these two characteristics have so far been considered in isolation. Here\nwe aim to bridge this gap. In particular, we analyze the spread of propaganda\nand its interplay with coordinated behavior on a large Twitter dataset about\nthe 2019 UK general election. We first propose and evaluate several metrics for\nmeasuring the use of propaganda on Twitter. Then, we investigate the use of\npropaganda by different coordinated communities that participated in the online\ndebate. The combination of the use of propaganda and coordinated behavior\nallows us to uncover the authenticity and harmfulness of the different\ncommunities. Finally, we compare our measures of propaganda and coordination\nwith automation (i.e., bot) scores and Twitter suspensions, revealing\ninteresting trends. From a theoretical viewpoint, we introduce a methodology\nfor analyzing several important dimensions of online behavior that are seldom\nconjointly considered. From a practical viewpoint, we provide new insights into\nauthentic and inauthentic online activities during the 2019 UK general\nelection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hristakieva_K/0/1/0/all/0/1\">Kristina Hristakieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cresci_S/0/1/0/all/0/1\">Stefano Cresci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual GenQA: Open-Domain Question Answering with Answer Sentence Generation. (arXiv:2110.07150v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07150","description":"<p>Recent approaches for question answering systems have achieved impressive\nperformance on English by combining document-level retrieval with answer\ngeneration. These approaches, which we refer to as GenQA, are able to generate\nfull sentences, effectively answering both factoid and non-factoid questions.\nIn this paper, we extend GenQA beyond English and present the first\nCross-Lingual answer sentence generation system (CrossGenQA). Our system\nproduces natural, full-sentence answers to questions in several languages by\nexploiting passages written in multiple other languages. To foster further\ndevelopment on this topic, we introduce GenTyDiQA, an extension of the TyDiQA\ndataset with well-formed and complete answers for Arabic, Bengali, English,\nJapanese, and Russian questions. Using GenTyDiQA, we show that multi-language\nmodels outperform monolingual GenQA in the four non-English languages; for\nthree of them, our CrossGenQA system achieves the best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1\">Benjamin Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lind_E/0/1/0/all/0/1\">Eric Lind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models. (arXiv:2110.08173v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08173","description":"<p>Knowledge probing is crucial for understanding the knowledge transfer\nmechanism behind the pre-trained language models (PLMs). Despite the growing\nprogress of probing knowledge for PLMs in the general domain, specialised areas\nsuch as biomedical domain are vastly under-explored. To catalyse the research\nin this direction, we release a well-curated biomedical knowledge probing\nbenchmark, MedLAMA, which is constructed based on the Unified Medical Language\nSystem (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs\nand probing approaches on our benchmark, reaching at most 3% of acc@10. While\nhighlighting various sources of domain-specific challenges that amount to this\nunderwhelming performance, we illustrate that the underlying PLMs have a higher\npotential for probing tasks. To achieve this, we propose Contrastive-Probe, a\nnovel self-supervised contrastive probing approach, that adjusts the underlying\nPLMs without using any probing data. While Contrastive-Probe pushes the acc@10\nto 28%, the performance gap still remains notable. Our human expert evaluation\nsuggests that the probing performance of our Contrastive-Probe is still\nunder-estimated as UMLS still does not include the full spectrum of factual\nknowledge. We hope MedLAMA and Contrastive-Probe facilitate further\ndevelopments of more suited probing techniques for this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_C/0/1/0/all/0/1\">Charlotte Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DS-TOD: Efficient Domain Specialization for Task Oriented Dialog. (arXiv:2110.08395v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08395","description":"<p>Recent work has shown that self-supervised dialog-specific pretraining on\nlarge conversational datasets yields substantial gains over traditional\nlanguage modeling (LM) pretraining in downstream task-oriented dialog (TOD).\nThese approaches, however, exploit general dialogic corpora (e.g., Reddit) and\nthus presumably fail to reliably embed domain-specific knowledge useful for\nconcrete downstream TOD domains. In this work, we investigate the effects of\ndomain specialization of pretrained language models (PLMs) for TOD. Within our\nDS-TOD framework, we first automatically extract salient domain-specific terms,\nand then use them to construct DomainCC and DomainReddit -- resources that we\nleverage for domain-specific pretraining, based on (i) masked language modeling\n(MLM) and (ii) response selection (RS) objectives, respectively. We further\npropose a resource-efficient and modular domain specialization by means of\ndomain adapters -- additional parameter-light layers in which we encode the\ndomain knowledge. Our experiments with prominent TOD tasks -- dialog state\ntracking (DST) and response retrieval (RR) -- encompassing five domains from\nthe MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we\nshow that the light-weight adapter-based specialization (1) performs comparably\nto full fine-tuning in single domain setups and (2) is particularly suitable\nfor multi-domain specialization, where besides advantageous computational\nfootprint, it can offer better TOD performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1\">Chia-Chien Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Double Trouble: How to not explain a text classifier's decisions using counterfactuals synthesized by masked language models?. (arXiv:2110.11929v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.11929","description":"<p>A principle behind dozens of attribution methods is to take the prediction\ndifference between before-and-after an input feature (here, a token) is removed\nas its attribution. A popular Input Marginalization (IM) method (Kim et al.,\n2020) uses BERT to replace a token, yielding more plausible counterfactuals.\nWhile Kim et al. (2020) reported that IM is effective, we find this conclusion\nnot convincing as the DeletionBERT metric used in their paper is biased towards\nIM. Importantly, this bias exists in Deletion-based metrics, including\nInsertion, Sufficiency, and Comprehensiveness. Furthermore, our rigorous\nevaluation using 6 metrics and 3 datasets finds no evidence that IM is better\nthan a Leave-One-Out (LOO) baseline. We find two reasons why IM is not better\nthan LOO: (1) deleting a single word from the input only marginally reduces a\nclassifier's accuracy; and (2) a highly predictable word is always given\nnear-zero attribution, regardless of its true importance to the classifier. In\ncontrast, making LIME samples more natural via BERT consistently improves LIME\naccuracy under several ROAR metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Long Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pay attention to emoji: Feature Fusion Network with EmoGraph2vec Model for Sentiment Analysis. (arXiv:2110.14636v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14636","description":"<p>With the explosive growth of social media, opinionated postings with emojis\nhave increased explosively. Many emojis are used to express emotions,\nattitudes, and opinions. Emoji representation learning can be helpful to\nimprove the performance of emoji-related natural language processing tasks,\nespecially in text sentiment analysis. However, most studies have only utilized\nthe fixed descriptions provided by the Unicode Consortium without consideration\nof actual usage scenarios. As for the sentiment analysis task, many researchers\nignore the emotional impact of the interaction between text and emojis. It\nresults that the emotional semantics of emojis cannot be fully explored. In\nthis work, we propose a method called EmoGraph2vec to learn emoji\nrepresentations by constructing a co-occurrence graph network from social data\nand enriching the semantic information based on an external knowledge base\nEmojiNet to embed emoji nodes. Based on EmoGraph2vec model, we design a novel\nneural network to incorporate text and emoji information into sentiment\nanalysis, which uses a hybrid-attention module combined with TextCNN-based\nclassifier to improve performance. Experimental results show that the proposed\nmodel can outperform several baselines for sentiment analysis on benchmark\ndatasets. Additionally, we conduct a series of ablation and comparison\nexperiments to investigate the effectiveness and interpretability of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaowei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Honglei Lv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse-Aware Soft Prompting for Text Generation. (arXiv:2112.05717v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.05717","description":"<p>Current efficient fine-tuning methods (e.g., adapters, prefix-tuning, etc.)\nhave optimized conditional text generation via training a small set of extra\nparameters of the neural language model, while freezing the rest for\nefficiency. While showing strong performance on some generation tasks, they\ndon't generalize across all generation tasks. We show that soft-prompt based\nconditional text generation can be improved with simple and efficient methods\nthat simulate modeling the discourse structure of human written text. We\ninvestigate two design choices: First, we apply \\textit{hierarchical blocking}\non the prefix parameters to simulate a higher-level discourse structure of\nhuman written text. Second, we apply \\textit{attention sparsity} on the prefix\nparameters at different layers of the network and learn sparse transformations\non the softmax-function. We show that structured design of prefix parameters\nyields more coherent, faithful and relevant generations than the baseline\nprefix-tuning on all generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gor_V/0/1/0/all/0/1\">Vera Gor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards More Efficient Insertion Transformer with Fractional Positional Encoding. (arXiv:2112.06295v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06295","description":"<p>Auto-regressive neural sequence models have been shown to be effective across\ntext generation tasks. However, their left-to-right decoding order prevents\ngeneration from being parallelized. Insertion Transformer (Stern et al., 2019)\nis an attractive alternative that allows outputting multiple tokens in a single\ngeneration step. Nevertheless, due to the incompatibility between absolute\npositional encoding and insertion-based generation schemes, it needs to refresh\nthe encoding of every token in the generated partial hypothesis at each step,\nwhich could be costly. We design a novel reusable positional encoding scheme\nfor insertion transformers called Fractional Positional Encoding (FPE), which\nallows reusing representations calculated in previous steps. Empirical studies\non various text generation tasks demonstrate the effectiveness of FPE, which\nleads to floating-point operation reduction and latency improvements on batched\ndecoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhisong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Search with Mixed-Initiative -- Asking Good Clarification Questions backed-up by Passage Retrieval. (arXiv:2112.07308v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07308","description":"<p>We deal with the scenario of conversational search, where user queries are\nunder-specified or ambiguous. This calls for a mixed-initiative setup.\nUser-asks (queries) and system-answers, as well as system-asks (clarification\nquestions) and user response, in order to clarify her information needs. We\nfocus on the task of selecting the next clarification question, given the\nconversation context. Our method leverages passage retrieval from a background\ncontent to fine-tune two deep-learning models for ranking candidate\nclarification questions. We evaluated our method on two different use-cases.\nThe first is an open domain conversational search in a large web collection.\nThe second is a task-oriented customer-support setup. We show that our method\nperforms well on both use-cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mass_Y/0/1/0/all/0/1\">Yosi Mass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1\">Doron Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_A/0/1/0/all/0/1\">Asaf Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopnicki_D/0/1/0/all/0/1\">David Konopnicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Rich Self-Supervision for Biomedical Entity Linking. (arXiv:2112.07887v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07887","description":"<p>Entity linking faces significant challenges such as prolific variations and\nprevalent ambiguities, especially in high-value domains with myriad entities.\nStandard classification approaches suffer from the annotation bottleneck and\ncannot effectively handle unseen entities. Zero-shot entity linking has emerged\nas a promising direction for generalizing to new entities, but it still\nrequires example gold entity mentions during training and canonical\ndescriptions for all entities, both of which are rarely available outside of\nWikipedia. In this paper, we explore Knowledge-RIch Self-Supervision ($\\tt\nKRISS$) for biomedical entity linking, by leveraging readily available domain\nknowledge. In training, it generates self-supervised mention examples on\nunlabeled text using a domain ontology and trains a contextual encoder using\ncontrastive learning. For inference, it samples self-supervised mentions as\nprototypes for each entity and conducts linking by mapping the test mention to\nthe most similar prototype. Our approach can easily incorporate entity\ndescriptions and gold mention labels if available. We conducted extensive\nexperiments on seven standard datasets spanning biomedical literature and\nclinical notes. Without using any labeled information, our method produces $\\tt\nKRISSBERT$, a universal entity linker for four million UMLS entities that\nattains new state of the art, outperforming prior self-supervised methods by as\nmuch as 20 absolute points in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1\">Shikhar Vashishth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinfeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIN-X: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain. (arXiv:2112.08754v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08754","description":"<p>The field of natural language processing (NLP) has recently seen a large\nchange towards using pre-trained language models for solving almost any task.\nDespite showing great improvements in benchmark datasets for various tasks,\nthese models often perform sub-optimal in non-standard domains like the\nclinical domain where a large gap between pre-training documents and target\ndocuments is observed. In this paper, we aim at closing this gap with\ndomain-specific training of the language model and we investigate its effect on\na diverse set of downstream tasks and settings. We introduce the pre-trained\nCLIN-X (Clinical XLM-R) language models and show how CLIN-X outperforms other\npre-trained transformer models by a large margin for ten clinical concept\nextraction tasks from two languages. In addition, we demonstrate how the\ntransformer model can be further improved with our proposed task- and\nlanguage-agnostic model architecture based on ensembles over random splits and\ncross-sentence context. Our studies in low-resource and transfer settings\nreveal stable model performance despite a lack of annotated data with\nimprovements of up to 47 F1 points when only 250 labeled sentences are\navailable. Our results highlight the importance of specialized language models\nas CLIN-X for concept extraction in non-standard domains, but also show that\nour task-agnostic model architecture is robust across the tested tasks and\nlanguages so that domain- or task-specific adaptations are not required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strotgen_J/0/1/0/all/0/1\">Jannik Str&#xf6;tgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training. (arXiv:2201.08081v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08081","description":"<p>Language-based environment manipulation requires agents to manipulate the\nenvironment following natural language instructions, which is challenging due\nto the huge space of the environments. To address this challenge, various\napproaches have been proposed in recent work. Although these approaches work\nwell for their intended environments, they are difficult to generalize across\nenvironments. In this work, we propose LEMON, a general framework for\nlanguage-based environment manipulation tasks. Specifically, we first specify a\ngeneral approach for language-based environment manipulation tasks, which can\ndeal with various environments using the same generative language model. Then\nwe propose an execution-guided pre-training strategy to inject prior knowledge\nof environments to the language model with a pure synthetic pre-training\ncorpus. Experimental results on tasks including Alchemy, Scene, Tangrams,\nProPara and Recipes demonstrate the effectiveness of LEMON: it achieves new\nstate-of-the-art results on Alchemy, Scene, ProPara, and Recipes, and the\nexecution-guided pre-training strategy brings remarkable improvements on all\nexperimental tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.10890","description":"<p>Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is hard to implement, easy to overfit, and\nnot hardware-friendly. In this work, inspired by human education model, we\npropose a novel task, knowledge integration, to obtain a dense student model\n(OneS) as knowledgeable as one sparse MoE. We investigate this task by\nproposing a general training framework including knowledge gathering and\nknowledge distillation. Specifically, we first propose Singular Value\nDecomposition Knowledge Gathering (SVD-KG) to gather key knowledge from\ndifferent pretrained experts. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE. OneS can achieve $78.4\\%$ top-1\naccuracy with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms SoTA by $51.7\\%$\nusing the same architecture and training data. In addition, compared with the\nMoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to the\nhardware-friendly architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Language Models with Contrastive Explanations. (arXiv:2202.10419v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10419","description":"<p>Model interpretability methods are often used to explain NLP model decisions\non tasks such as text classification, where the output space is relatively\nsmall. However, when applied to language generation, where the output space\noften consists of tens of thousands of tokens, these methods are unable to\nprovide informative explanations. Language models must consider various\nfeatures to predict a token, such as its part of speech, number, tense, or\nsemantics. Existing explanation methods conflate evidence for all these\nfeatures into a single explanation, which is less interpretable for human\nunderstanding.\n</p>\n<p>To disentangle the different decisions in language modeling, we focus on\nexplaining language models contrastively: we look for salient input tokens that\nexplain why the model predicted one token instead of another. We demonstrate\nthat contrastive explanations are quantifiably better than non-contrastive\nexplanations in verifying major grammatical phenomena, and that they\nsignificantly improve contrastive model simulatability for human observers. We\nalso identify groups of contrastive decisions where the model uses similar\nevidence, and we are able to characterize what input tokens models use during\nvarious language generation decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Representation Learning for Continual Relation Extraction. (arXiv:2203.02721v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02721","description":"<p>Continual relation extraction (CRE) aims to continuously train a model on\ndata with new relations while avoiding forgetting old ones. Some previous work\nhas proved that storing a few typical samples of old relations and replaying\nthem when learning new relations can effectively avoid forgetting. However,\nthese memory-based methods tend to overfit the memory samples and perform\npoorly on imbalanced datasets. To solve these challenges, a consistent\nrepresentation learning method is proposed, which maintains the stability of\nthe relation embedding by adopting contrastive learning and knowledge\ndistillation when replaying memory. Specifically, supervised contrastive\nlearning based on a memory bank is first used to train each new task so that\nthe model can effectively learn the relation representation. Then, contrastive\nreplay is conducted of the samples in memory and makes the model retain the\nknowledge of historical relations through memory knowledge distillation to\nprevent the catastrophic forgetting of the old task. The proposed method can\nbetter learn consistent representations to alleviate forgetting effectively.\nExtensive experiments on FewRel and TACRED datasets show that our method\nsignificantly outperforms state-of-the-art baselines and yield strong\nrobustness on the imbalanced dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiangong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internet-augmented language models through few-shot prompting for open-domain question answering. (arXiv:2203.05115v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05115","description":"<p>In this work, we aim to capitalize on the unique few-shot capabilities of\nlarge-scale language models (LSLMs) to overcome some of their challenges with\nrespect to grounding to factual and up-to-date information. Motivated by\nsemi-parametric language models (LMs), which ground their decisions in external\nretrieved evidence, we use few-shot prompting to learn to condition LMs on\ninformation returned from the web using Google Search, a broad and constantly\nupdated knowledge source. Our approach does not involve fine-tuning or learning\nadditional parameters, thus making it applicable to any LM, offering therefore\na strong baseline. Indeed, we find that LMs conditioned on the web surpass\nperformance of closed-book models of similar, or even larger, model sizes in\nopen-domain question answering. Finally, we find that increasing the\ninference-time compute of models, achieved via using multiple retrieved\nevidences to generate multiple answers followed by a reranking stage that uses\nscores generated by the same LMs, leads to better performance and alleviates\nlower performance of smaller few-shot LMs. All in all, our findings suggest\nthat it might be beneficial to slow down the race towards the biggest model and\ninstead shift attention towards finding more effective ways to use models,\nincluding but not limited to, better prompting or increasing inference-time\ncompute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1\">Angeliki Lazaridou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gribovskaya_E/0/1/0/all/0/1\">Elena Gribovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stokowiec_W/0/1/0/all/0/1\">Wojciech Stokowiec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigorev_N/0/1/0/all/0/1\">Nikolai Grigorev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RED-ACE: Robust Error Detection for ASR using Confidence Embeddings. (arXiv:2203.07172v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07172","description":"<p>ASR Error Detection (AED) models aim to post-process the output of Automatic\nSpeech Recognition (ASR) systems, in order to detect transcription errors.\nModern approaches usually use text-based input, comprised solely of the ASR\ntranscription hypothesis, disregarding additional signals from the ASR model.\nInstead, we propose to utilize the ASR system's word-level confidence scores\nfor improving AED performance. Specifically, we add an ASR Confidence Embedding\n(ACE) layer to the AED model's encoder, allowing us to jointly encode the\nconfidence scores and the transcribed text into a contextualized\nrepresentation. Our experiments show the benefits of ASR confidence scores for\nAED, their complementary effect over the textual signal, as well as the\neffectiveness and robustness of ACE for combining these signals. To foster\nfurther research, we publish a novel AED dataset consisting of ASR outputs on\nthe LibriSpeech corpus with annotated transcription errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gekhman_Z/0/1/0/all/0/1\">Zorik Gekhman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zverinski_D/0/1/0/all/0/1\">Dina Zverinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallinson_J/0/1/0/all/0/1\">Jonathan Mallinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beryozkin_G/0/1/0/all/0/1\">Genady Beryozkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models. (arXiv:2203.07259v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07259","description":"<p>Transformer-based language models have become a key building block for\nnatural language processing. While these models are extremely accurate, they\ncan be too large and computationally intensive to run on standard deployments.\nA variety of compression methods, including distillation, quantization,\nstructured and unstructured pruning are known to decrease model size and\nincrease inference speed, with low accuracy loss. In this context, this paper's\ncontributions are two-fold. We perform an in-depth study of the\naccuracy-compression trade-off for unstructured weight pruning of BERT models.\nWe introduce Optimal BERT Surgeon (oBERT), an efficient and accurate weight\npruning method based on approximate second-order information, which we show to\nyield state-of-the-art results in both stages of language tasks: pre-training\nand fine-tuning. Specifically, oBERT extends existing work on unstructured\nsecond-order pruning by allowing for pruning blocks of weights, and by being\napplicable at the BERT scale. Second, we investigate the impact of this pruning\nmethod when compounding compression approaches to obtain highly compressed but\naccurate models for deployment on edge devices. These models significantly push\nboundaries of the current state-of-the-art sparse BERT models with respect to\nall metrics: model size, inference speed and task accuracy. For example,\nrelative to the dense BERT-base, we obtain 10x model size compression (in MB)\nwith &lt; 1% accuracy drop, 10x CPU-inference speedup with &lt; 2% accuracy drop, and\n29x CPU-inference speedup with &lt; 7.5% accuracy drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1\">Eldar Kurtic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1\">Elias Frantar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fineran_B/0/1/0/all/0/1\">Benjamin Fineran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goin_M/0/1/0/all/0/1\">Michael Goin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Learning for Few-Shot Dialogue State Tracking. (arXiv:2203.08568v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08568","description":"<p>Collecting and annotating task-oriented dialogues is time-consuming and\ncostly. Thus, zero and few shot learning for dialogue tasks presents an\nexciting opportunity. In this work, we propose an in-context (IC) learning\nframework for zero-shot and few-shot learning dialogue state tracking (DST),\nwhere a large pretrained language model (LM) takes a test instance and a few\nexemplars as input, and directly decodes the dialogue state without any\nparameter updates. This approach is more flexible and scalable than prior DST\nwork when adapting to new domains and scenarios. To better leverage a tabular\ndomain description in the LM prompt, we reformulate DST into a text-to-SQL\nproblem. We also propose a novel approach to retrieve annotated dialogues as\nexemplars. Empirical results on MultiWOZ show that our method IC-DST\nsubstantially outperforms previous fine-tuned state-of-the-art models in\nfew-shot settings. In addition, we test IC-DST in zero-shot settings, in which\nthe model only takes a fixed task instruction as input, finding that it\noutperforms previous zero-shot methods by a large margin on MultiWOZ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training. (arXiv:2203.09313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09313","description":"<p>Large-scale pre-training has shown remarkable performance in building\nopen-domain dialogue systems. However, previous works mainly focus on showing\nand evaluating the conversational performance of the released dialogue model,\nignoring the discussion of some key factors towards a powerful human-like\nchatbot, especially in Chinese scenarios. In this paper, we conduct extensive\nexperiments to investigate these under-explored factors, including data quality\ncontrol, model architecture designs, training approaches, and decoding\nstrategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese\ndialogue model with 2.8 billion parameters, and make our models and code\npublicly available. To our knowledge, EVA2.0 is the largest open-source Chinese\ndialogue model. Automatic and human evaluations show that our model\nsignificantly outperforms other open-source counterparts. We also discuss the\nlimitations of this work by presenting some failure cases and pose some future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jianzhu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibration of Machine Reading Systems at Scale. (arXiv:2203.10623v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10623","description":"<p>In typical machine learning systems, an estimate of the probability of the\nprediction is used to assess the system's confidence in the prediction. This\nconfidence measure is usually uncalibrated; i.e.\\ the system's confidence in\nthe prediction does not match the true probability of the predicted output. In\nthis paper, we present an investigation into calibrating open setting machine\nreading systems such as open-domain question answering and claim verification\nsystems. We show that calibrating such complex systems which contain discrete\nretrieval and deep reading components is challenging and current calibration\ntechniques fail to scale to these settings. We propose simple extensions to\nexisting calibration approaches that allows us to adapt them to these settings.\nOur experimental results reveal that the approach works well, and can be useful\nto selectively predict answers when question answering systems are posed with\nunanswerable or out-of-the-training distribution questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rajarshi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT models. (arXiv:2203.13778v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13778","description":"<p>Social media platforms are used by a large number of people prominently to\nexpress their thoughts and opinions. However, these platforms have contributed\nto a substantial amount of hateful and abusive content as well. Therefore, it\nis important to curb the spread of hate speech on these platforms. In India,\nMarathi is one of the most popular languages used by a wide audience. In this\nwork, we present L3Cube-MahaHate, the first major Hate Speech Dataset in\nMarathi. The dataset is curated from Twitter, annotated manually. Our dataset\nconsists of over 25000 distinct tweets labeled into four major classes i.e\nhate, offensive, profane, and not. We present the approaches used for\ncollecting and annotating the data and the challenges faced during the process.\nFinally, we present baseline classification results using deep learning models\nbased on CNN, LSTM, and Transformers. We explore mono-lingual and multi-lingual\nvariants of BERT like MahaBERT, IndicBERT, mBERT, and xlm-RoBERTa and show that\nmono-lingual models perform better than their multi-lingual counterparts. The\nMahaBERT model provides the best results on L3Cube-MahaHate Corpus. The data\nand models are available at https://github.com/l3cube-pune/MarathiNLP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velankar_A/0/1/0/all/0/1\">Abhishek Velankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_H/0/1/0/all/0/1\">Hrushikesh Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gore_A/0/1/0/all/0/1\">Amol Gore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salunke_S/0/1/0/all/0/1\">Shubham Salunke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filter-based Discriminative Autoencoders for Children Speech Recognition. (arXiv:2204.00164v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00164","description":"<p>Children speech recognition is indispensable but challenging due to the\ndiversity of children's speech. In this paper, we propose a filter-based\ndiscriminative autoencoder for acoustic modeling. To filter out the influence\nof various speaker types and pitches, auxiliary information of the speaker and\npitch features is input into the encoder together with the acoustic features to\ngenerate phonetic embeddings. In the training phase, the decoder uses the\nauxiliary information and the phonetic embedding extracted by the encoder to\nreconstruct the input acoustic features. The autoencoder is trained by\nsimultaneously minimizing the ASR loss and feature reconstruction error. The\nframework can make the phonetic embedding purer, resulting in more accurate\nsenone (triphone-state) scores. Evaluated on the test set of the CMU Kids\ncorpus, our system achieves a 7.8% relative WER reduction compared to the\nbaseline system. In the domain adaptation experiment, our system also\noutperforms the baseline system on the British-accent PF-STAR task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chiang-Lin Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Biomedical Entity Linking via Knowledge Base-Guided Pre-training and Synonyms-Aware Fine-tuning. (arXiv:2204.05164v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05164","description":"<p>Entities lie in the heart of biomedical natural language understanding, and\nthe biomedical entity linking (EL) task remains challenging due to the\nfine-grained and diversiform concept names. Generative methods achieve\nremarkable performances in general domain EL with less memory usage while\nrequiring expensive pre-training. Previous biomedical EL methods leverage\nsynonyms from knowledge bases (KB) which is not trivial to inject into a\ngenerative method. In this work, we use a generative approach to model\nbiomedical EL and propose to inject synonyms knowledge in it. We propose\nKB-guided pre-training by constructing synthetic samples with synonyms and\ndefinitions from KB and require the model to recover concept names. We also\npropose synonyms-aware fine-tuning to select concept names for training, and\npropose decoder prompt and multi-synonyms constrained prefix tree for\ninference. Our method achieves state-of-the-art results on several biomedical\nEL tasks without candidate selection which displays the effectiveness of\nproposed pre-training and fine-tuning strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoTEx: Explaining Model Decisions with Prototype Tensors. (arXiv:2204.05426v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05426","description":"<p>We present ProtoTEx, a novel white-box NLP classification architecture based\non prototype networks. ProtoTEx faithfully explains model decisions based on\nprototype tensors that encode latent clusters of training examples. At\ninference time, classification decisions are based on the distances between the\ninput text and the prototype tensors, explained via the training examples most\nsimilar to the most influential prototypes. We also describe a novel\ninterleaved training algorithm that effectively handles classes characterized\nby the absence of indicative features. On a propaganda detection task, ProtoTEx\naccuracy matches BART-large and exceeds BERT-large with the added benefit of\nproviding faithful explanations. A user study also shows that prototype-based\nexplanations help non-experts to better recognize propaganda in online news.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anubrata Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chitrank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovatchev_V/0/1/0/all/0/1\">Venelin Kovatchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-informed Question Answering with Heterogeneous Graph Transformer. (arXiv:2204.09655v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09655","description":"<p>Large neural language models are steadily contributing state-of-the-art\nperformance to question answering and other natural language and information\nprocessing tasks. These models are expensive to train. We propose to evaluate\nwhether such pre-trained models can benefit from the addition of explicit\nlinguistics information without requiring retraining from scratch.\n</p>\n<p>We present a linguistics-informed question answering approach that extends\nand fine-tunes a pre-trained transformer-based neural language model with\nsymbolic knowledge encoded with a heterogeneous graph transformer. We\nillustrate the approach by the addition of syntactic information in the form of\ndependency and constituency graphic structures connecting tokens and virtual\nvertices.\n</p>\n<p>A comparative empirical performance evaluation with BERT as its baseline and\nwith Stanford Question Answering Dataset demonstrates the competitiveness of\nthe proposed approach. We argue, in conclusion and in the light of further\nresults of preliminary experiments, that the approach is extensible to further\nlinguistics information including semantics and pragmatics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lok You Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bressan_S/0/1/0/all/0/1\">St&#xe9;phane Bressan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Hypergraph-based Nested Named Entity Recognition as Query-based Sequence Labeling. (arXiv:2204.11467v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11467","description":"<p>There has been a growing academic interest in the recognition of nested named\nentities in many domains. We tackle the task with a novel local\nhypergraph-based method: We first propose start token candidates and generate\ncorresponding queries with their surrounding context, then use a query-based\nsequence labeling module to form a local hypergraph for each candidate. An end\ntoken estimator is used to correct the hypergraphs and get the final\npredictions. Compared to span-based approaches, our method is free of the high\ncomputation cost of span sampling and the risk of losing long entities.\nSequential prediction makes it easier to leverage information in word order\ninside nested structures, and richer representations are built with a local\nhypergraph. Experiments show that our proposed method outperforms all the\nprevious hypergraph-based and sequence labeling approaches with large margins\non all four nested datasets. It achieves a new state-of-the-art F1 score on the\nACE 2004 dataset and competitive F1 scores with previous state-of-the-art\nmethods on three other nested NER datasets: ACE 2005, GENIA, and KBP 2017.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yukun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sen Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastRE: Towards Fast Relation Extraction with Convolutional Encoder and Improved Cascade Binary Tagging Framework. (arXiv:2205.02490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02490","description":"<p>Recent work for extracting relations from texts has achieved excellent\nperformance. However, most existing methods pay less attention to the\nefficiency, making it still challenging to quickly extract relations from\nmassive or streaming text data in realistic scenarios. The main efficiency\nbottleneck is that these methods use a Transformer-based pre-trained language\nmodel for encoding, which heavily affects the training speed and inference\nspeed. To address this issue, we propose a fast relation extraction model\n(FastRE) based on convolutional encoder and improved cascade binary tagging\nframework. Compared to previous work, FastRE employs several innovations to\nimprove efficiency while also keeping promising performance. Concretely, FastRE\nadopts a novel convolutional encoder architecture combined with dilated\nconvolution, gated unit and residual connection, which significantly reduces\nthe computation cost of training and inference, while maintaining the\nsatisfactory performance. Moreover, to improve the cascade binary tagging\nframework, FastRE first introduces a type-relation mapping mechanism to\naccelerate tagging efficiency and alleviate relation redundancy, and then\nutilizes a position-dependent adaptive thresholding strategy to obtain higher\ntagging accuracy and better model generalization. Experimental results\ndemonstrate that FastRE is well balanced between efficiency and performance,\nand achieves 3-10x training speed, 7-15x inference speed faster, and 1/100\nparameters compared to the state-of-the-art models, while the performance is\nstill competitive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guozheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiafeng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qiqing Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation. (arXiv:2205.03835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03835","description":"<p>In recent years, pre-trained models have become dominant in most natural\nlanguage processing (NLP) tasks. However, in the area of Automated Essay\nScoring (AES), pre-trained models such as BERT have not been properly used to\noutperform other deep learning models such as LSTM. In this paper, we introduce\na novel multi-scale essay representation for BERT that can be jointly learned.\nWe also employ multiple losses and transfer learning from out-of-domain essays\nto further improve the performance. Experiment results show that our approach\nderives much benefit from joint learning of multi-scale essay representation\nand obtains almost the state-of-the-art result among all deep learning models\nin the ASAP task. Our multi-scale essay representation also generalizes well to\nCommonLit Readability Prize data set, which suggests that the novel text\nrepresentation proposed in this paper may be a new and effective choice for\nlong-text tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruobing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hui Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chart Question Answering: State of the Art and Future Directions. (arXiv:2205.03966v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03966","description":"<p>Information visualizations such as bar charts and line charts are very common\nfor analyzing data and discovering critical insights. Often people analyze\ncharts to answer questions that they have in mind. Answering such questions can\nbe challenging as they often require a significant amount of perceptual and\ncognitive effort. Chart Question Answering (CQA) systems typically take a chart\nand a natural language question as input and automatically generate the answer\nto facilitate visual data analysis. Over the last few years, there has been a\ngrowing body of literature on the task of CQA. In this survey, we\nsystematically review the current state-of-the-art research focusing on the\nproblem of chart question answering. We provide a taxonomy by identifying\nseveral important dimensions of the problem domain including possible inputs\nand outputs of the task and discuss the advantages and limitations of proposed\nsolutions. We then summarize various evaluation techniques used in the surveyed\npapers. Finally, we outline the open challenges and future research\nopportunities related to chart question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavehzadeh_P/0/1/0/all/0/1\">Parsa Kavehzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">Ahmed Masry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Distributional Properties Drive Emergent In-Context Learning in Transformers. (arXiv:2205.05055v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.05055","description":"<p>Large transformer-based language models are able to perform few-shot learning\n(also known as in-context learning), without having been explicitly trained for\nit. We hypothesized that specific distributional properties of natural language\nmight drive this emergent phenomenon, as these characteristics might lead to a\nkind of interpolation between few-shot meta-training (designed to elicit rapid\nfew-shot learning) and standard supervised training (designed to elicit gradual\nin-weights learning). We also hypothesized that these distributional properties\ncould lead to emergent few-shot learning in domains outside of language.\nInspired by this idea, we ran a series of experiments on a standard image-based\nfew-shot dataset. We discovered that a number of data properties did indeed\npromote the emergence of few-shot learning in transformer models. All of these\nproperties are present in natural language -- burstiness, long-tailedness, and\nmany-to-one or one-to-many label mappings. The data influenced whether models\nwere biased towards either few-shot learning vs. memorizing information in\ntheir weights; models could generally perform well at only one or the other.\nHowever, we discovered that an additional distributional property could allow\nthe two capabilities to co-exist in the same model -- a skewed, Zipfian\ndistribution over classes -- which occurs in language as well. Notably,\ntraining data that could elicit few-shot learning in transformers were unable\nto elicit few-shot learning in recurrent models. In sum, we find that few-shot\nlearning emerges only from applying the right architecture to the right data\ndistribution; neither component is sufficient on its own.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1\">Pierre H. Richemond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">Jay McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Generalizability of Fine-Tuned Models for Fake News Detection. (arXiv:2205.07154v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.07154","description":"<p>The Covid-19 pandemic has caused a dramatic and parallel rise in dangerous\nmisinformation, denoted an `infodemic' by the CDC and WHO. Misinformation tied\nto the Covid-19 infodemic changes continuously; this can lead to performance\ndegradation of fine-tuned models due to concept drift. Degredation can be\nmitigated if models generalize well-enough to capture some cyclical aspects of\ndrifted data. In this paper, we explore generalizability of pre-trained and\nfine-tuned fake news detectors across 9 fake news datasets. We show that\nexisting models often overfit on their training dataset and have poor\nperformance on unseen data. However, on some subsets of unseen data that\noverlap with training data, models have higher accuracy. Based on this\nobservation, we also present KMeans-Proxy, a fast and effective method based on\nK-Means clustering for quickly identifying these overlapping subsets of unseen\ndata. KMeans-Proxy improves generalizability on unseen fake news datasets by\n0.1-0.2 f1-points across datasets. We present both our generalizability\nexperiments as well as KMeans-Proxy to further research in tackling the fake\nnews problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suprem_A/0/1/0/all/0/1\">Abhijit Suprem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_C/0/1/0/all/0/1\">Calton Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wojood: Nested Arabic Named Entity Corpus and Recognition using BERT. (arXiv:2205.09651v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09651","description":"<p>This paper presents Wojood, a corpus for Arabic nested Named Entity\nRecognition (NER). Nested entities occur when one entity mention is embedded\ninside another entity mention. Wojood consists of about 550K Modern Standard\nArabic (MSA) and dialect tokens that are manually annotated with 21 entity\ntypes including person, organization, location, event and date. More\nimportantly, the corpus is annotated with nested entities instead of the more\ncommon flat annotations. The data contains about 75K entities and 22.5% of\nwhich are nested. The inter-annotator evaluation of the corpus demonstrated a\nstrong agreement with Cohen's Kappa of 0.979 and an F1-score of 0.976. To\nvalidate our data, we used the corpus to train a nested NER model based on\nmulti-task learning and AraBERT (Arabic BERT). The model achieved an overall\nmicro F1-score of 0.884. Our corpus, the annotation guidelines, the source code\nand the pre-trained model are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalilia_M/0/1/0/all/0/1\">Mohammed Khalilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_S/0/1/0/all/0/1\">Sana Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Slot Tagging with Intent Features for Task Oriented Natural Language Understanding using BERT. (arXiv:2205.09732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09732","description":"<p>Recent joint intent detection and slot tagging models have seen improved\nperformance when compared to individual models. In many real-world datasets,\nthe slot labels and values have a strong correlation with their intent labels.\nIn such cases, the intent label information may act as a useful feature to the\nslot tagging model. In this paper, we examine the effect of leveraging intent\nlabel features through 3 techniques in the slot tagging task of joint intent\nand slot detection models. We evaluate our techniques on benchmark spoken\nlanguage datasets SNIPS and ATIS, as well as over a large private Bixby dataset\nand observe an improved slot-tagging performance over state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_S/0/1/0/all/0/1\">Shruthi Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_V/0/1/0/all/0/1\">Vignesh Kumar Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utkarsh/0/1/0/all/0/1\">Utkarsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarapanahalli_J/0/1/0/all/0/1\">Jayantha Gowda Sarapanahalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Dual Branch Prior-SegNet: CNN for Interventional CBCT using Planning Scan and Auxiliary Segmentation Loss. (arXiv:2205.10353v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10353","description":"<p>This paper proposes an extension to the Dual Branch Prior-Net for sparse view\ninterventional CBCT reconstruction incorporating a high quality planning scan.\nAn additional head learns to segment interventional instruments and thus guides\nthe reconstruction task. The prior scans are misaligned by up to +-5deg\nin-plane during training. Experiments show that the proposed model, Dual Branch\nPrior-SegNet, significantly outperforms any other evaluated model by &gt;2.8dB\nPSNR. It also stays robust wrt. rotations of up to +-5.5deg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ernst_P/0/1/0/all/0/1\">Philipp Ernst</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1\">Suhita Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rose_G/0/1/0/all/0/1\">Georg Rose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of stent under-expansion in calcified coronary arteries using machine-learning on intravascular optical coherence tomography. (arXiv:2205.10354v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10354","description":"<p>BACKGROUND Careful evaluation of the risk of stent under-expansions before\nthe intervention will aid treatment planning, including the application of a\npre-stent plaque modification strategy.\n</p>\n<p>OBJECTIVES It remains challenging to achieve a proper stent expansion in the\npresence of severely calcified coronary lesions. Building on our work in deep\nlearning segmentation, we created an automated machine learning approach that\nuses lesion attributes to predict stent under-expansion from pre-stent images,\nsuggesting the need for plaque modification.\n</p>\n<p>METHODS Pre- and post-stent intravascular optical coherence tomography image\ndata were obtained from 110 coronary lesions. Lumen and calcifications in\npre-stent images were segmented using deep learning, and numerous features per\nlesion were extracted. We analyzed stent expansion along the lesion, enabling\nframe, segmental, and whole-lesion analyses. We trained regression models to\npredict the poststent lumen area and then to compute the stent expansion index\n(SEI). Stents with an SEI &lt; or &gt;/= 80% were classified as \"under-expanded\" and\n\"well-expanded,\" respectively.\n</p>\n<p>RESULTS Best performance (root-mean-square-error = 0.04+/-0.02 mm2, r =\n0.94+/-0.04, p &lt; 0.0001) was achieved when we used features from both the lumen\nand calcification to train a Gaussian regression model for a segmental analysis\nover a segment length of 31 frames. Under-expansion classification results\n(AUC=0.85+/-0.02) were significantly improved over other approaches.\n</p>\n<p>CONCLUSIONS We used calcifications and lumen features to identify lesions at\nrisk of stent under-expansion. Results suggest that the use of pre-stent images\ncan inform physicians of the need to apply plaque modification approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gharaibeh_Y/0/1/0/all/0/1\">Yazan Gharaibeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Juhwan Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimin_V/0/1/0/all/0/1\">Vladislav N. Zimin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolluru_C/0/1/0/all/0/1\">Chaitanya Kolluru</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dallan_L/0/1/0/all/0/1\">Luis A. P. Dallan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_G/0/1/0/all/0/1\">Gabriel T. R. Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vergara_Martel_A/0/1/0/all/0/1\">Armando Vergara-Martel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Justin N. Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoori_A/0/1/0/all/0/1\">Ammar Hoori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_P/0/1/0/all/0/1\">Pengfei Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gamage_P/0/1/0/all/0/1\">Peshala T. Gamage</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_L/0/1/0/all/0/1\">Linxia Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bezerra_H/0/1/0/all/0/1\">Hiram G. Bezerra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Al_Kindi_S/0/1/0/all/0/1\">Sadeer Al-Kindi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilson_D/0/1/0/all/0/1\">David L. Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Quality Estimation: Creating Surrogate Models for Human Quality Ratings. (arXiv:2205.10355v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10355","description":"<p>Human ratings are abstract representations of segmentation quality. To\napproximate human quality ratings on scarce expert data, we train surrogate\nquality estimation models. We evaluate on a complex multi-class segmentation\nproblem, specifically glioma segmentation following the BraTS annotation\nprotocol. The training data features quality ratings from 15 expert\nneuroradiologists on a scale ranging from 1 to 6 stars for various\ncomputer-generated and manual 3D annotations. Even though the networks operate\non 2D images and with scarce training data, we can approximate segmentation\nquality within a margin of error comparable to human intra-rater reliability.\nSegmentation quality prediction has broad applications. While an understanding\nof segmentation quality is imperative for successful clinical translation of\nautomatic segmentation quality algorithms, it can play an essential role in\ntraining new segmentation models. Due to the split-second inference times, it\ncan be directly applied within a loss function or as a fully-automatic dataset\ncuration mechanism in a federated learning setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kofler_F/0/1/0/all/0/1\">Florian Kofler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_I/0/1/0/all/0/1\">Izabela Horvath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1\">Ezequiel de la Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LaMaster_J/0/1/0/all/0/1\">John LaMaster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finck_T/0/1/0/all/0/1\">Tom Finck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paetzold_J/0/1/0/all/0/1\">Johannes Paetzold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piraud_M/0/1/0/all/0/1\">Marie Piraud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1\">Jan Kirschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_C/0/1/0/all/0/1\">Claus Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1\">Benedikt Wiestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXPANSE: A Deep Continual / Progressive Learning System for Deep Transfer Learning. (arXiv:2205.10356v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10356","description":"<p>Deep transfer learning techniques try to tackle the limitations of deep\nlearning, the dependency on extensive training data and the training costs, by\nreusing obtained knowledge. However, the current DTL techniques suffer from\neither catastrophic forgetting dilemma (losing the previously obtained\nknowledge) or overly biased pre-trained models (harder to adapt to target data)\nin finetuning pre-trained models or freezing a part of the pre-trained model,\nrespectively. Progressive learning, a sub-category of DTL, reduces the effect\nof the overly biased model in the case of freezing earlier layers by adding a\nnew layer to the end of a frozen pre-trained model. Even though it has been\nsuccessful in many cases, it cannot yet handle distant source and target data.\nWe propose a new continual/progressive learning approach for deep transfer\nlearning to tackle these limitations. To avoid both catastrophic forgetting and\noverly biased-model problems, we expand the pre-trained model by expanding\npre-trained layers (adding new nodes to each layer) in the model instead of\nonly adding new layers. Hence the method is named EXPANSE. Our experimental\nresults confirm that we can tackle distant source and target data using this\ntechnique. At the same time, the final model is still valid on the source data,\nachieving a promising deep continual learning approach. Moreover, we offer a\nnew way of training deep learning models inspired by the human education\nsystem. We termed this two-step training: learning basics first, then adding\ncomplexities and uncertainties. The evaluation implies that the two-step\ntraining extracts more meaningful features and a finer basin on the error\nsurface since it can achieve better accuracy in comparison to regular training.\nEXPANSE (model expansion and two-step training) is a systematic continual\nlearning approach applicable to different problems and DL models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iman_M/0/1/0/all/0/1\">Mohammadreza Iman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John A. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branchinst_R/0/1/0/all/0/1\">Robert M. Branchinst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonlinear motion separation via untrained generator networks with disentangled latent space variables and applications to cardiac MRI. (arXiv:2205.10367v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10367","description":"<p>In this paper, a nonlinear approach to separate different motion types in\nvideo data is proposed. This is particularly relevant in dynamic medical\nimaging (e.g. PET, MRI), where patient motion poses a significant challenge due\nto its effects on the image reconstruction as well as for its subsequent\ninterpretation. Here, a new method is proposed where dynamic images are\nrepresented as the forward mapping of a sequence of latent variables via a\ngenerator neural network. The latent variables are structured so that temporal\nvariations in the data are represented via dynamic latent variables, which are\nindependent of static latent variables characterizing the general structure of\nthe frames. In particular, different kinds of motion are also characterized\nindependently of each other via latent space disentanglement using\none-dimensional prior information on all but one of the motion types. This\nrepresentation allows to freeze any selection of motion types, and to obtain\naccurate independent representations of other dynamics of interest. Moreover,\nthe proposed algorithm is training-free, i.e., all the network parameters are\nlearned directly from a single video. We illustrate the performance of this\nmethod on phantom and real-data MRI examples, where we successfully separate\nrespiratory and cardiac motion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abdullah/0/1/0/all/0/1\">Abdullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Holler_M/0/1/0/all/0/1\">Martin Holler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kunisch_K/0/1/0/all/0/1\">Karl Kunisch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_M/0/1/0/all/0/1\">Malena Sabate Landman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of Synthetic Colonoscopy Videos for Domain Randomization. (arXiv:2205.10368v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10368","description":"<p>An increasing number of colonoscopic guidance and assistance systems rely on\nmachine learning algorithms which require a large amount of high-quality\ntraining data. In order to ensure high performance, the latter has to resemble\na substantial portion of possible configurations. This particularly addresses\nvarying anatomy, mucosa appearance and image sensor characteristics which are\nlikely deteriorated by motion blur and inadequate illumination. The limited\namount of readily available training data hampers to account for all of these\npossible configurations which results in reduced generalization capabilities of\nmachine learning models. We propose an exemplary solution for synthesizing\ncolonoscopy videos with substantial appearance and anatomical variations which\nenables to learn discriminative domain-randomized representations of the\ninterior colon while mimicking real-world settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jagtap_A/0/1/0/all/0/1\">Abhishek Dinkar Jagtap</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heinrich_M/0/1/0/all/0/1\">Mattias Heinrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Himstedt_M/0/1/0/all/0/1\">Marian Himstedt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A SSIM Guided cGAN Architecture For Clinically Driven Generative Image Synthesis of Multiplexed Spatial Proteomics Channels. (arXiv:2205.10373v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10373","description":"<p>Here we present a structural similarity index measure (SSIM) guided\nconditional Generative Adversarial Network (cGAN) that generatively performs\nimage-to-image (i2i) synthesis to generate photo-accurate protein channels in\nmultiplexed spatial proteomics images. This approach can be utilized to\naccurately generate missing spatial proteomics channels that were not included\nduring experimental data collection either at the bench or the clinic.\nExperimental spatial proteomic data from the Human BioMolecular Atlas Program\n(HuBMAP) was used to generate spatial representations of missing proteins\nthrough a U-Net based image synthesis pipeline. HuBMAP channels were\nhierarchically clustered by the (SSIM) as a heuristic to obtain the minimal set\nneeded to recapitulate the underlying biology represented by the spatial\nlandscape of proteins. We subsequently prove that our SSIM based architecture\nallows for scaling of generative image synthesis to slides with up to 100\nchannels, which is better than current state of the art algorithms which are\nlimited to data with 11 channels. We validate these claims by generating a new\nexperimental spatial proteomics data set from human lung adenocarcinoma tissue\nsections and show that a model trained on HuBMAP can accurately synthesize\nchannels from our new data set. The ability to recapitulate experimental data\nfrom sparsely stained multiplexed histological slides containing spatial\nproteomic will have tremendous impact on medical diagnostics and drug\ndevelopment, and also raises important questions on the medical ethics of\nutilizing data produced by generative image synthesis in the clinical setting.\nThe algorithm that we present in this paper will allow researchers and\nclinicians to save time and costs in proteomics based histological staining\nwhile also increasing the amount of data that they can generate through their\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saurav_J/0/1/0/all/0/1\">Jillur Rahman Saurav</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nasr_M/0/1/0/all/0/1\">Mohammad Sadegh Nasr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koomey_P/0/1/0/all/0/1\">Paul Koomey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robben_M/0/1/0/all/0/1\">Michael Robben</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huber_M/0/1/0/all/0/1\">Manfred Huber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weidanz_J/0/1/0/all/0/1\">Jon Weidanz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ryan_B/0/1/0/all/0/1\">Br&#xed;d Ryan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruppin_E/0/1/0/all/0/1\">Eytan Ruppin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luber_J/0/1/0/all/0/1\">Jacob M. Luber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dynamic Weighted Tabular Method for Convolutional Neural Networks. (arXiv:2205.10386v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10386","description":"<p>Traditional Machine Learning (ML) models like Support Vector Machine, Random\nForest, and Logistic Regression are generally preferred for classification\ntasks on tabular datasets. Tabular data consists of rows and columns\ncorresponding to instances and features, respectively. Past studies indicate\nthat traditional classifiers often produce unsatisfactory results in complex\ntabular datasets. Hence, researchers attempt to use the powerful Convolutional\nNeural Networks (CNN) for tabular datasets. Recent studies propose several\ntechniques like SuperTML, Conditional GAN (CTGAN), and Tabular Convolution\n(TAC) for applying Convolutional Neural Networks (CNN) on tabular data. These\nmodels outperform the traditional classifiers and substantially improve the\nperformance on tabular data. This study introduces a novel technique, namely,\nDynamic Weighted Tabular Method (DWTM), that uses feature weights dynamically\nbased on statistical techniques to apply CNNs on tabular datasets. The method\nassigns weights dynamically to each feature based on their strength of\nassociativity to the class labels. Each data point is converted into images and\nfed to a CNN model. The features are allocated image canvas space based on\ntheir weights. The DWTM is an improvement on the previously mentioned methods\nas it dynamically implements the entire experimental setting rather than using\nthe static configuration provided in the previous methods. Furthermore, it uses\nthe novel idea of using feature weights to create image canvas space. In this\npaper, the DWTM is applied to six benchmarked tabular datasets and it achieves\noutstanding performance (i.e., average accuracy = 95%) on all of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_M/0/1/0/all/0/1\">Md Ifraham Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukta_M/0/1/0/all/0/1\">Md. Saddam Hossain Mukta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Ahmed Rafi Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing visual acuity in visual prostheses through a virtual-reality system. (arXiv:2205.10395v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10395","description":"<p>Current visual implants still provide very low resolution and limited field\nof view, thus limiting visual acuity in implanted patients. Developments of new\nstrategies of artificial vision simulation systems by harnessing new\nadvancements in technologies are of upmost priorities for the development of\nnew visual devices. In this work, we take advantage of virtual-reality software\npaired with a portable head-mounted display and evaluated the performance of\nnormally sighted participants under simulated prosthetic vision with variable\nfield of view and number of pixels. Our simulated prosthetic vision system\nallows simple experimentation in order to study the design parameters of future\nvisual prostheses. Ten normally sighted participants volunteered for a visual\nacuity study. Subjects were required to identify computer-generated Landolt-C\ngap orientation and different stimulus based on light perception,\ntime-resolution, light location and motion perception commonly used for visual\nacuity examination in the sighted. Visual acuity scores were recorded across\ndifferent conditions of number of electrodes and size of field of view. Our\nresults showed that of all conditions tested, a field of view of 20{\\deg} and\n1000 phosphenes of resolution proved the best, with a visual acuity of 1.3\nlogMAR. Furthermore, performance appears to be correlated with phosphene\ndensity, but showing a diminishing return when field of view is less than\n20{\\deg}. The development of new artificial vision simulation systems can be\nuseful to guide the development of new visual devices and the optimization of\nfield of view and resolution to provide a helpful and valuable visual aid to\nprofoundly or totally blind patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Garcia_M/0/1/0/all/0/1\">Melani Sanchez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morollon_Ruiz_R/0/1/0/all/0/1\">Roberto Morollon-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Cantin_R/0/1/0/all/0/1\">Ruben Martinez-Cantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_J/0/1/0/all/0/1\">Jose J. Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Jover_E/0/1/0/all/0/1\">Eduardo Fernandez-Jover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Contrastive and Supervised Learning for Video Super-Resolution Detection. (arXiv:2205.10406v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10406","description":"<p>Upscaled video detection is a helpful tool in multimedia forensics, but it is\na challenging task that involves various upscaling and compression algorithms.\nThere are many resolution-enhancement methods, including interpolation and\ndeep-learning-based super-resolution, and they leave unique traces. In this\nwork, we propose a new upscaled-resolution-detection method based on learning\nof visual representations using contrastive and cross-entropy losses. To\nexplain how the method detects videos, we systematically review the major\ncomponents of our framework - in particular, we show that most\ndata-augmentation approaches hinder the learning of the method. Through\nextensive experiments on various datasets, we demonstrate that our method\neffectively detects upscaling even in compressed videos and outperforms the\nstate-of-the-art alternatives. The code and models are publicly available at\nhttps://github.com/msu-video-group/SRDM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meshchaninov_V/0/1/0/all/0/1\">Viacheslav Meshchaninov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molodetskikh_I/0/1/0/all/0/1\">Ivan Molodetskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitriy Vatolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using machine learning on new feature sets extracted from 3D models of broken animal bones to classify fragments according to break agent. (arXiv:2205.10430v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10430","description":"<p>Distinguishing agents of bone modification at paleoanthropological sites is\nat the root of much of the research directed at understanding early hominin\nexploitation of large animal resources and the effects those subsistence\nbehaviors had on early hominin evolution. However, current methods,\nparticularly in the area of fracture pattern analysis as a signal of marrow\nexploitation, have failed to overcome equifinality. Furthermore, researchers\ndebate the replicability and validity of current and emerging methods for\nanalyzing bone modifications. Here we present a new approach to fracture\npattern analysis aimed at distinguishing bone fragments resulting from hominin\nbone breakage and those produced by carnivores. This new method uses 3D models\nof fragmentary bone to extract a much richer dataset that is more transparent\nand replicable than feature sets previously used in fracture pattern analysis.\nSupervised machine learning algorithms are properly used to classify bone\nfragments according to agent of breakage with average mean accuracy of 77%\nacross tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yezzi_Woodley_K/0/1/0/all/0/1\">Katrina Yezzi-Woodley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terwilliger_A/0/1/0/all/0/1\">Alexander Terwilliger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiafeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eric Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tappen_M/0/1/0/all/0/1\">Martha Tappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calder_J/0/1/0/all/0/1\">Jeff Calder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olver_P/0/1/0/all/0/1\">Peter J. Olver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Understanding Attribution Methods. (arXiv:2205.10435v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10435","description":"<p>Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods. Finally, we propose a post-processing smoothing step that\nsignificantly improves the performance of some attribution methods, and discuss\nits applicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Sukrut Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohle_M/0/1/0/all/0/1\">Moritz B&#xf6;hle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporally Precise Action Spotting in Soccer Videos Using Dense Detection Anchors. (arXiv:2205.10450v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10450","description":"<p>We present a model for temporally precise action spotting in videos, which\nuses a dense set of detection anchors, predicting a detection confidence and\ncorresponding fine-grained temporal displacement for each anchor. We experiment\nwith two trunk architectures, both of which are able to incorporate large\ntemporal contexts while preserving the smaller-scale features required for\nprecise localization: a one-dimensional version of a u-net, and a Transformer\nencoder (TE). We also suggest best practices for training models of this kind,\nby applying Sharpness-Aware Minimization (SAM) and mixup data augmentation. We\nachieve a new state-of-the-art on SoccerNet-v2, the largest soccer video\ndataset of its kind, with marked improvements in temporal localization.\nAdditionally, our ablations show: the importance of predicting the temporal\ndisplacements; the trade-offs between the u-net and TE trunks; and the benefits\nof training with SAM and mixup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soares_J/0/1/0/all/0/1\">Jo&#xe3;o V. B. Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Avijit Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_T/0/1/0/all/0/1\">Topojoy Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSO-Convolutional Neural Networks with Heterogeneous Learning Rate. (arXiv:2205.10456v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10456","description":"<p>Convolutional Neural Networks (ConvNets) have been candidly deployed in the\nscope of computer vision and related fields. Nevertheless, the dynamics of\ntraining of these neural networks lie still elusive: it is hard and\ncomputationally expensive to train them. A myriad of architectures and training\nstrategies have been proposed to overcome this challenge and address several\nproblems in image processing such as speech, image and action recognition as\nwell as object detection. In this article, we propose a novel Particle Swarm\nOptimization (PSO) based training for ConvNets. In such framework, the vector\nof weights of each ConvNet is typically cast as the position of a particle in\nphase space whereby PSO collaborative dynamics intertwines with Stochastic\nGradient Descent (SGD) in order to boost training performance and\ngeneralization. Our approach goes as follows: i) [warm-up phase] each ConvNet\nis trained independently via SGD; ii) [collaborative phase] ConvNets share\namong themselves their current vector of weights (or particle-position) along\nwith their gradient estimates of the Loss function. Distinct step sizes are\ncoined by distinct ConvNets. By properly blending ConvNets with large (possibly\nrandom) step-sizes along with more conservative ones, we propose an algorithm\nwith competitive performance with respect to other PSO-based approaches on\nCifar-10 (accuracy of 98.31%). These accuracy levels are obtained by resorting\nto only four ConvNets -- such results are expected to scale with the number of\ncollaborative ConvNets accordingly. We make our source codes available for\ndownload https://github.com/leonlha/PSO-ConvNet-Dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phong_N/0/1/0/all/0/1\">Nguyen Huu Phong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1\">Augusto Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1\">Bernardete Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Sensible Adversarial Learning of Deep Neural Networks for Image Classification. (arXiv:2205.10457v1 [cs.CR])","link":"http://arxiv.org/abs/2205.10457","description":"<p>The idea of robustness is central and critical to modern statistical\nanalysis. However, despite the recent advances of deep neural networks (DNNs),\nmany studies have shown that DNNs are vulnerable to adversarial attacks. Making\nimperceptible changes to an image can cause DNN models to make the wrong\nclassification with high confidence, such as classifying a benign mole as a\nmalignant tumor and a stop sign as a speed limit sign. The trade-off between\nrobustness and standard accuracy is common for DNN models. In this paper, we\nintroduce sensible adversarial learning and demonstrate the synergistic effect\nbetween pursuits of standard natural accuracy and robustness. Specifically, we\ndefine a sensible adversary which is useful for learning a robust model while\nkeeping high natural accuracy. We theoretically establish that the Bayes\nclassifier is the most robust multi-class classifier with the 0-1 loss under\nsensible adversarial learning. We propose a novel and efficient algorithm that\ntrains a robust model using implicit loss truncation. We apply sensible\nadversarial learning for large-scale image classification to a handwritten\ndigital image dataset called MNIST and an object recognition colored image\ndataset called CIFAR10. We have performed an extensive comparative study to\ncompare our method with other competitive methods. Our experiments empirically\ndemonstrate that our method is not sensitive to its hyperparameter and does not\ncollapse even with a small model capacity while promoting robustness against\nvarious attacks and keeping high natural accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungeum Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Omnidirectional Vision: A Survey and New Perspectives. (arXiv:2205.10468v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10468","description":"<p>Omnidirectional image (ODI) data is captured with a 360x180 field-of-view,\nwhich is much wider than the pinhole cameras and contains richer spatial\ninformation than the conventional planar images. Accordingly, omnidirectional\nvision has attracted booming attention due to its more advantageous performance\nin numerous applications, such as autonomous driving and virtual reality. In\nrecent years, the availability of customer-level 360 cameras has made\nomnidirectional vision more popular, and the advance of deep learning (DL) has\nsignificantly sparked its research and applications. This paper presents a\nsystematic and comprehensive review and analysis of the recent progress in DL\nmethods for omnidirectional vision. Our work covers four main contents: (i) An\nintroduction to the principle of omnidirectional imaging, the convolution\nmethods on the ODI, and datasets to highlight the differences and difficulties\ncompared with the 2D planar image data; (ii) A structural and hierarchical\ntaxonomy of the DL methods for omnidirectional vision; (iii) A summarization of\nthe latest novel learning strategies and applications; (iv) An insightful\ndiscussion of the challenges and open problems by highlighting the potential\nresearch directions to trigger more research in the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ai_H/0/1/0/all/0/1\">Hao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zidong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinjing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haotian Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yucheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Ling Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masterful: A Training Platform for Computer Vision Models. (arXiv:2205.10469v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10469","description":"<p>Masterful is a software platform to train deep learning computer vision\nmodels. Data and model architecture are inputs to the platform, and the output\nis a trained model. The platform's primary goal is to maximize a trained\nmodel's accuracy, which it achieves through its regularization and\nsemi-supervised learning implementations. The platform's secondary goal is to\nminimize the amount of manual experimentation typically required to tune\ntraining hyperparameters, which it achieves via multiple metalearning\nalgorithms which are custom built to control the platform's regularization and\nsemi-supervised learning implementations. The platform's tertiary goal is to\nminimize the computing resources required to train a model, which it achieves\nvia another set of metalearning algorithms which are purpose built to control\nTensorflow's optimization implementations. The platform builds on top of\nTensorflow's data management, architecture, automatic differentiation, and\noptimization implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wookey_S/0/1/0/all/0/1\">Samuel Wookey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_Y/0/1/0/all/0/1\">Yaoshiang Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rikert_T/0/1/0/all/0/1\">Tom Rikert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_J/0/1/0/all/0/1\">Juan David Gil Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beancur_J/0/1/0/all/0/1\">Juan Manuel Mu&#xf1;oz Beancur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cortes_S/0/1/0/all/0/1\">Santiago Cortes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tawil_R/0/1/0/all/0/1\">Ray Tawil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabin_A/0/1/0/all/0/1\">Aaron Sabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1\">Jack Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harper_T/0/1/0/all/0/1\">Travis Harper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gajendrakumar_N/0/1/0/all/0/1\">Nikhil Gajendrakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Subspace Clustering via Tensor Low-Rank Representation. (arXiv:2205.10481v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10481","description":"<p>In this letter, we propose a novel semi-supervised subspace clustering\nmethod, which is able to simultaneously augment the initial supervisory\ninformation and construct a discriminative affinity matrix. By representing the\nlimited amount of supervisory information as a pairwise constraint matrix, we\nobserve that the ideal affinity matrix for clustering shares the same low-rank\nstructure as the ideal pairwise constraint matrix. Thus, we stack the two\nmatrices into a 3-D tensor, where a global low-rank constraint is imposed to\npromote the affinity matrix construction and augment the initial pairwise\nconstraints synchronously. Besides, we use the local geometry structure of\ninput samples to complement the global low-rank prior to achieve better\naffinity matrix learning. The proposed model is formulated as a Laplacian graph\nregularized convex low-rank tensor representation problem, which is further\nsolved with an alternative iterative algorithm. In addition, we propose to\nrefine the affinity matrix with the augmented pairwise constraints.\nComprehensive experimental results on six commonly-used benchmark datasets\ndemonstrate the superiority of our method over state-of-the-art methods. The\ncode is publicly available at\nhttps://github.com/GuanxingLu/Subspace-Clustering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guanxing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping Emulation for Knowledge Distillation. (arXiv:2205.10490v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10490","description":"<p>This paper formalizes the source-blind knowledge distillation problem that is\nessential to federated learning. A new geometric perspective is presented to\nview such a problem as aligning generated distributions between the teacher and\nstudent. With its guidance, a new architecture MEKD is proposed to emulate the\ninverse mapping through generative adversarial training. Unlike mimicking\nlogits and aligning logit distributions, reconstructing the mapping from\nclassifier-logits has a geometric intuition of decreasing empirical distances,\nand theoretical guarantees using the universal function approximation and\noptimal mass transportation theories. A new algorithm is also proposed to train\nthe student model that reaches the teacher's performance source-blindly. On\nvarious benchmarks, MEKD outperforms existing source-blind KD methods,\nexplainable with ablation studies and visualized results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yuwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yiming Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriched Robust Multi-View Kernel Subspace Clustering. (arXiv:2205.10495v1 [cs.AI])","link":"http://arxiv.org/abs/2205.10495","description":"<p>Subspace clustering is to find underlying low-dimensional subspaces and\ncluster the data points correctly. In this paper, we propose a novel multi-view\nsubspace clustering method. Most existing methods suffer from two critical\nissues. First, they usually adopt a two-stage framework and isolate the\nprocesses of affinity learning, multi-view information fusion and clustering.\nSecond, they assume the data lies in a linear subspace which may fail in\npractice as most real-world datasets may have non-linearity structures. To\naddress the above issues, in this paper we propose a novel Enriched Robust\nMulti-View Kernel Subspace Clustering framework where the consensus affinity\nmatrix is learned from both multi-view data and spectral clustering. Due to the\nobjective and constraints which is difficult to optimize, we propose an\niterative optimization method which is easy to implement and can yield closed\nsolution in each step. Extensive experiments have validated the superiority of\nour method over state-of-the-art clustering methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Video Quality Assessment Models Sensitive to Frame Rate Distortions. (arXiv:2205.10501v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10501","description":"<p>We consider the problem of capturing distortions arising from changes in\nframe rate as part of Video Quality Assessment (VQA). Variable frame rate (VFR)\nvideos have become much more common, and streamed videos commonly range from 30\nframes per second (fps) up to 120 fps. VFR-VQA offers unique challenges in\nterms of distortion types as well as in making non-uniform comparisons of\nreference and distorted videos having different frame rates. The majority of\ncurrent VQA models require compared videos to be of the same frame rate, but\nare unable to adequately account for frame rate artifacts. The recently\nproposed Generalized Entropic Difference (GREED) VQA model succeeds at this\ntask, using natural video statistics models of entropic differences of temporal\nband-pass coefficients, delivering superior performance on predicting video\nquality changes arising from frame rate distortions. Here we propose a simple\nfusion framework, whereby temporal features from GREED are combined with\nexisting VQA models, towards improving model sensitivity towards frame rate\ndistortions. We find through extensive experiments that this feature fusion\nsignificantly boosts model performance on both HFR/VFR datasets as well as\nfixed frame rate (FFR) VQA databases. Our results suggest that employing\nefficient temporal representations can result much more robust and accurate VQA\nmodels when frame rate variations can occur.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Madhusudana_P/0/1/0/all/0/1\">Pavan C. Madhusudana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Birkbeck_N/0/1/0/all/0/1\">Neil Birkbeck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeper vs Wider: A Revisit of Transformer Configuration. (arXiv:2205.10505v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10505","description":"<p>Transformer-based models have delivered impressive results on many tasks,\nparticularly vision and language tasks. In many model training situations,\nconventional configurations are typically adopted. For example, we often set\nthe base model with hidden dimensions (i.e. model width) to be 768 and the\nnumber of transformer layers (i.e. model depth) to be 12. In this paper, we\nrevisit these conventional configurations. Through theoretical analysis and\nexperimental evaluation, we show that the masked autoencoder is effective in\nalleviating the over-smoothing issue in deep transformer training. Based on\nthis finding, we propose Bamboo, an idea of using deeper and narrower\ntransformer configurations, for masked autoencoder training. On ImageNet, with\nsuch a simple change in configuration, re-designed model achieves 87.1% top-1\naccuracy and outperforms SoTA models like MAE and BEiT. On language tasks,\nre-designed model outperforms BERT with default setting by 1.1 points on\naverage, on GLUE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianghai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Travel Time, Distance and Costs Optimization for Paratransit Operations using Graph Convolutional Neural Network. (arXiv:2205.10507v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10507","description":"<p>The provision of paratransit services is one option to meet the\ntransportation needs of Vulnerable Road Users (VRUs). Like any other means of\ntransportation, paratransit has obstacles such as high operational costs and\nlonger trip times. As a result, customers are dissatisfied, and paratransit\noperators have a low approval rating. Researchers have undertaken various\nstudies over the years to better understand the travel behaviors of paratransit\ncustomers and how they are operated. According to the findings of these\nresearches, paratransit operators confront the challenge of determining the\noptimal route for their trips in order to save travel time. Depending on the\nnature of the challenge, most research used different optimization techniques\nto solve these routing problems. As a result, the goal of this study is to use\nGraph Convolutional Neural Networks (GCNs) to assist paratransit operators in\nresearching various operational scenarios in a strategic setting in order to\noptimize routing, minimize operating costs and minimize their users' travel\ntime. The study was carried out by using a randomized simulated dataset to help\ndetermine the decision to make in terms of fleet composition and capacity under\ndifferent situations. For the various scenarios investigated, the GCN assisted\nin determining the minimum optimal gap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwakye_K/0/1/0/all/0/1\">Kelvin Kwakye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seong_Y/0/1/0/all/0/1\">Younho Seong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Sun Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing CoAtNet Predictions for Aiding Melanoma Detection. (arXiv:2205.10515v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10515","description":"<p>Melanoma is considered to be the most aggressive form of skin cancer. Due to\nthe similar shape of malignant and benign cancerous lesions, doctors spend\nconsiderably more time when diagnosing these findings. At present, the\nevaluation of malignancy is performed primarily by invasive histological\nexamination of the suspicious lesion. Developing an accurate classifier for\nearly and efficient detection can minimize and monitor the harmful effects of\nskin cancer and increase patient survival rates. This paper proposes a\nmulti-class classification task using the CoAtNet architecture, a hybrid model\nthat combines the depthwise convolution matrix operation of traditional\nconvolutional neural networks with the strengths of Transformer models and\nself-attention mechanics to achieve better generalization and capacity. The\nproposed multi-class classifier achieves an overall precision of 0.901, recall\n0.895, and AP 0.923, indicating high performance compared to other\nstate-of-the-art networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kvak_D/0/1/0/all/0/1\">Daniel Kvak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point is a Vector: A Feature Representation in Point Analysis. (arXiv:2205.10528v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10528","description":"<p>The irregularity and disorder of point clouds bring many challenges to point\ncloud analysis. PointMLP suggests that geometric information is not the only\ncritical point in point cloud analysis. It achieves promising result based on a\nsimple multi-layer perception (MLP) structure with geometric affine module.\nHowever, these MLP-like structures aggregate features only with fixed weights,\nwhile differences in the semantic information of different point features are\nignored. So we propose a novel Point-Vector Representation of the point feature\nto improve feature aggregation by using inductive bias. The direction of the\nintroduced vector representation can dynamically modulate the aggregation of\ntwo point features according to the semantic relationship. Based on it, we\ndesign a novel Point2Vector MLP architecture. Experiments show that it achieves\nstate-of-the-art performance on the classification task of ScanObjectNN\ndataset, with 1% increase, compared with the previous best method. We hope our\nmethod can help people better understand the role of semantic information in\npoint cloud analysis and lead to explore more and better feature\nrepresentations or other ways.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xin Deng</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">WengYu Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1\">Qing Ding</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">XinMing Zhang</a> (1) ((1) University of Science and Technology of China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Visual Classification using Self Assessment Classifier. (arXiv:2205.10529v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10529","description":"<p>Extracting discriminative features plays a crucial role in the fine-grained\nvisual classification task. Most of the existing methods focus on developing\nattention or augmentation mechanisms to achieve this goal. However, addressing\nthe ambiguity in the top-k prediction classes is not fully investigated. In\nthis paper, we introduce a Self Assessment Classifier, which simultaneously\nleverages the representation of the image and top-k prediction classes to\nreassess the classification results. Our method is inspired by continual\nlearning with coarse-grained and fine-grained classifiers to increase the\ndiscrimination of features in the backbone and produce attention maps of\ninformative areas on the image. In practice, our method works as an auxiliary\nbranch and can be easily integrated into different architectures. We show that\nby effectively addressing the ambiguity in the top-k prediction classes, our\nmethod achieves new state-of-the-art results on CUB200-2011, Stanford Dog, and\nFGVC Aircraft datasets. Furthermore, our method also consistently improves the\naccuracy of different existing fine-grained classifiers with a unified setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tuong Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Huy Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjiputra_E/0/1/0/all/0/1\">Erman Tjiputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang D. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation from A Stronger Teacher. (arXiv:2205.10536v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10536","description":"<p>Unlike existing knowledge distillation methods focus on the baseline\nsettings, where the teacher models and training strategies are not that strong\nand competing as state-of-the-art approaches, this paper presents a method\ndubbed DIST to distill better from a stronger teacher. We empirically find that\nthe discrepancy of predictions between the student and a stronger teacher may\ntend to be fairly severer. As a result, the exact match of predictions in KL\ndivergence would disturb the training and make existing methods perform poorly.\nIn this paper, we show that simply preserving the relations between the\npredictions of teacher and student would suffice, and propose a\ncorrelation-based loss to capture the intrinsic inter-class relations from the\nteacher explicitly. Besides, considering that different instances have\ndifferent semantic similarities to each class, we also extend this relational\nmatch to the intra-class level. Our method is simple yet practical, and\nextensive experiments demonstrate that it adapts well to various architectures,\nmodel sizes and training strategies, and can achieve state-of-the-art\nperformance consistently on image classification, object detection, and\nsemantic segmentation tasks. Code is available at:\nhttps://github.com/hunto/DIST_KD .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Feasibility and Generality of Patch-based Adversarial Attacks on Semantic Segmentation Problems. (arXiv:2205.10539v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10539","description":"<p>Deep neural networks were applied with success in a myriad of applications,\nbut in safety critical use cases adversarial attacks still pose a significant\nthreat. These attacks were demonstrated on various classification and detection\ntasks and are usually considered general in a sense that arbitrary network\noutputs can be generated by them.\n</p>\n<p>In this paper we will demonstrate through simple case studies both in\nsimulation and in real-life, that patch based attacks can be utilised to alter\nthe output of segmentation networks. Through a few examples and the\ninvestigation of network complexity, we will also demonstrate that the number\nof possible output maps which can be generated via patch-based attacks of a\ngiven size is typically smaller than the area they effect or areas which should\nbe attacked in case of practical applications.\n</p>\n<p>We will prove that based on these results most patch-based attacks cannot be\ngeneral in practice, namely they can not generate arbitrary output maps or if\nthey could, they are spatially limited and this limit is significantly smaller\nthan the receptive field of the patches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kontar_S/0/1/0/all/0/1\">Soma Kontar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_A/0/1/0/all/0/1\">Andras Horvath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improvements to Self-Supervised Representation Learning for Masked Image Modeling. (arXiv:2205.10546v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10546","description":"<p>This paper explores improvements to the masked image modeling (MIM) paradigm.\nThe MIM paradigm enables the model to learn the main object features of the\nimage by masking the input image and predicting the masked part by the unmasked\npart. We found the following three main directions for MIM to be improved.\nFirst, since both encoders and decoders contribute to representation learning,\nMIM uses only encoders for downstream tasks, which ignores the impact of\ndecoders on representation learning. Although the MIM paradigm already employs\nsmall decoders with asymmetric structures, we believe that continued reduction\nof decoder parameters is beneficial to improve the representational learning\ncapability of the encoder . Second, MIM solves the image prediction task by\ntraining the encoder and decoder together , and does not design a separate task\nfor the encoder . To further enhance the performance of the encoder when\nperforming downstream tasks, we designed the encoder for the tasks of\ncomparative learning and token position prediction. Third, since the input\nimage may contain background and other objects, and the proportion of each\nobject in the image varies, reconstructing the tokens related to the background\nor to other objects is not meaningful for MIM to understand the main object\nrepresentations. Therefore we use ContrastiveCrop to crop the input image so\nthat the input image contains as much as possible only the main objects. Based\non the above three improvements to MIM, we propose a new model, Contrastive\nMasked AutoEncoders (CMAE). We achieved a Top-1 accuracy of 65.84% on\ntinyimagenet using the ViT-B backbone, which is +2.89 outperforming the MAE of\ncompeting methods when all conditions are equal. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiawei Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuesong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yuanqi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Honggu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three-Dimensional Segmentation of the Left Ventricle in Late Gadolinium Enhanced MR Images of Chronic Infarction Combining Long- and Short-Axis Information. (arXiv:2205.10548v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10548","description":"<p>Automatic segmentation of the left ventricle (LV) in late gadolinium enhanced\n(LGE) cardiac MR (CMR) images is difficult due to the intensity heterogeneity\narising from accumulation of contrast agent in infarcted myocardium. In this\npaper, we present a comprehensive framework for automatic 3D segmentation of\nthe LV in LGE CMR images. Given myocardial contours in cine images as a priori\nknowledge, the framework initially propagates the a priori segmentation from\ncine to LGE images via 2D translational registration. Two meshes representing\nrespectively endocardial and epicardial surfaces are then constructed with the\npropagated contours. After construction, the two meshes are deformed towards\nthe myocardial edge points detected in both short-axis and long-axis LGE images\nin a unified 3D coordinate system. Taking into account the intensity\ncharacteristics of the LV in LGE images, we propose a novel parametric model of\nthe LV for consistent myocardial edge points detection regardless of\npathological status of the myocardium (infarcted or healthy) and of the type of\nthe LGE images (short-axis or long-axis). We have evaluated the proposed\nframework with 21 sets of real patient and 4 sets of simulated phantom data.\nBoth distance- and region-based performance metrics confirm the observation\nthat the framework can generate accurate and reliable results for myocardial\nsegmentation of LGE images. We have also tested the robustness of the framework\nwith respect to varied a priori segmentation in both practical and simulated\nsettings. Experimental results show that the proposed framework can greatly\ncompensate variations in the given a priori knowledge and consistently produce\naccurate segmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Ying Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ong_S/0/1/0/all/0/1\">Sim-Heng Ong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chai_P/0/1/0/all/0/1\">Ping Chai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teo_L/0/1/0/all/0/1\">Lynette L. Teo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Low_A/0/1/0/all/0/1\">Adrian F. Low</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robot Person Following in Uniform Crowd Environment. (arXiv:2205.10553v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10553","description":"<p>Person-tracking robots have many applications, such as in security, elderly\ncare, and socializing robots. Such a task is particularly challenging when the\nperson is moving in a Uniform crowd. Also, despite significant progress of\ntrackers reported in the literature, state-of-the-art trackers have hardly\naddressed person following in such scenarios. In this work, we focus on\nimproving the perceptivity of a robot for a person following task by developing\na robust and real-time applicable object tracker. We present a new robot person\ntracking system with a new RGB-D tracker, Deep Tracking with RGB-D (DTRD) that\nis resilient to tricky challenges introduced by the uniform crowd environment.\nOur tracker utilizes transformer encoder-decoder architecture with RGB and\ndepth information to discriminate the target person from similar distractors. A\nsubstantial amount of comprehensive experiments and results demonstrate that\nour tracker has higher performance in two quantitative evaluation metrics and\nconfirms its superiority over other SOTA trackers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghimire_A/0/1/0/all/0/1\">Adarsh Ghimire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoxiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Sajid Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dias_J/0/1/0/all/0/1\">Jorge Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cycle-GAN for eye-tracking. (arXiv:2205.10556v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10556","description":"<p>This manuscript presents a not typical implementation of the cycle generative\nadversarial networks (Cycle-GAN) method for eye-tracking tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rakhmatulin_I/0/1/0/all/0/1\">Ildar Rakhmatulin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Sign Language Phoneme Clustering using HamNoSys Notation. (arXiv:2205.10560v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10560","description":"<p>Traditionally, sign language resources have been collected in controlled\nsettings for specific tasks involving supervised sign classification or\nlinguistic studies accompanied by specific annotation type. To date, very few\nwho explored signing videos found online on social media platforms as well as\nthe use of unsupervised methods applied to such resources. Due to the fact that\nthe field is striving to achieve acceptable model performance on the data that\ndiffers from that seen during training calls for more diversity in sign\nlanguage data, stepping away from the data obtained in controlled laboratory\nsettings. Moreover, since the sign language data collection and annotation\ncarries large overheads, it is desirable to accelerate the annotation process.\nConsidering the aforementioned tendencies, this paper takes the side of\nharvesting online data in a pursuit for automatically generating and annotating\nsign language corpora through phoneme clustering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mocialov_B/0/1/0/all/0/1\">Boris Mocialov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_G/0/1/0/all/0/1\">Graham Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1\">Helen Hastie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADT-SSL: Adaptive Dual-Threshold for Semi-Supervised Learning. (arXiv:2205.10571v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10571","description":"<p>Semi-Supervised Learning (SSL) has advanced classification tasks by inputting\nboth labeled and unlabeled data to train a model jointly. However, existing SSL\nmethods only consider the unlabeled data whose predictions are beyond a fixed\nthreshold (e.g., 0.95), ignoring the valuable information from those less than\n0.95. We argue that these discarded data have a large proportion and are\nusually of hard samples, thereby benefiting the model training. This paper\nproposes an Adaptive Dual-Threshold method for Semi-Supervised Learning\n(ADT-SSL). Except for the fixed threshold, ADT extracts another class-adaptive\nthreshold from the labeled data to take full advantage of the unlabeled data\nwhose predictions are less than 0.95 but more than the extracted one.\nAccordingly, we engage CE and $L_2$ loss functions to learn from these two\ntypes of unlabeled data, respectively. For highly similar unlabeled data, we\nfurther design a novel similar loss to make the prediction of the model\nconsistency. Extensive experiments are conducted on benchmark datasets,\nincluding CIFAR-10, CIFAR-100, and SVHN. Experimental results show that the\nproposed ADT-SSL achieves state-of-the-art classification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zechen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan-Gen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive 3-D Framework for Automatic Quantification of Late Gadolinium Enhanced Cardiac Magnetic Resonance Images. (arXiv:2205.10572v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10572","description":"<p>Late gadolinium enhanced (LGE) cardiac magnetic resonance (CMR) can directly\nvisualize nonviable myocardium with hyperenhanced intensities with respect to\nnormal myocardium. For heart attack patients, it is crucial to facilitate the\ndecision of appropriate therapy by analyzing and quantifying their LGE CMR\nimages. To achieve accurate quantification, LGE CMR images need to be processed\nin two steps: segmentation of the myocardium followed by classification of\ninfarcts within the segmented myocardium. However, automatic segmentation is\ndifficult usually due to the intensity heterogeneity of the myocardium and\nintensity similarity between the infarcts and blood pool. Besides, the slices\nof an LGE CMR dataset often suffer from spatial and intensity distortions,\ncausing further difficulties in segmentation and classification. In this paper,\nwe present a comprehensive 3-D framework for automatic quantification of LGE\nCMR images. In this framework, myocardium is segmented with a novel method that\ndeforms coupled endocardial and epicardial meshes and combines information in\nboth short- and long-axis slices, while infarcts are classified with a\ngraph-cut algorithm incorporating intensity and spatial information. Moreover,\nboth spatial and intensity distortions are effectively corrected with specially\ndesigned countermeasures. Experiments with 20 sets of real patient data show\nvisually good segmentation and classification results that are quantitatively\nin strong agreement with those manually obtained by experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Ying Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ong_S/0/1/0/all/0/1\">Sim-Heng Ong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chai_P/0/1/0/all/0/1\">Ping Chai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teo_L/0/1/0/all/0/1\">Lynette L Teo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Low_A/0/1/0/all/0/1\">Adrian F Low</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-feature Co-learning for Image Inpainting. (arXiv:2205.10578v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10578","description":"<p>Image inpainting has achieved great advances by simultaneously leveraging\nimage structure and texture features. However, due to lack of effective\nmulti-feature fusion techniques, existing image inpainting methods still show\nlimited improvement. In this paper, we design a deep multi-feature co-learning\nnetwork for image inpainting, which includes Soft-gating Dual Feature Fusion\n(SDFF) and Bilateral Propagation Feature Aggregation (BPFA) modules. To be\nspecific, we first use two branches to learn structure features and texture\nfeatures separately. Then the proposed SDFF module integrates structure\nfeatures into texture features, and meanwhile uses texture features as an\nauxiliary in generating structure features. Such a co-learning strategy makes\nthe structure and texture features more consistent. Next, the proposed BPFA\nmodule enhances the connection from local feature to overall consistency by\nco-learning contextual attention, channel-wise information and feature space,\nwhich can further refine the generated structures and textures. Finally,\nextensive experiments are performed on benchmark datasets, including CelebA,\nPlaces2, and Paris StreetView. Experimental results demonstrate the superiority\nof the proposed method over the state-of-the-art. The source codes are\navailable at https://github.com/GZHU-DVL/MFCL-Inpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiayu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan-Gen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wenzhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aifeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Camouflaged Object Detection with Dual-Task Interactive Transformer. (arXiv:2205.10579v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10579","description":"<p>Camouflaged object detection intends to discover the concealed objects hidden\nin the surroundings. Existing methods follow the bio-inspired framework, which\nfirst locates the object and second refines the boundary. We argue that the\ndiscovery of camouflaged objects depends on the recurrent search for the object\nand the boundary. The recurrent processing makes the human tired and helpless,\nbut it is just the advantage of the transformer with global search ability.\nTherefore, a dual-task interactive transformer is proposed to detect both\naccurate position of the camouflaged object and its detailed boundary. The\nboundary feature is considered as Query to improve the camouflaged object\ndetection, and meanwhile the object feature is considered as Query to improve\nthe boundary detection. The camouflaged object detection and the boundary\ndetection are fully interacted by multi-head self-attention. Besides, to obtain\nthe initial object feature and boundary feature, transformer-based backbones\nare adopted to extract the foreground and background. The foreground is just\nobject, while foreground minus background is considered as boundary. Here, the\nboundary feature can be obtained from blurry boundary region of the foreground\nand background. Supervised by the object, the background and the boundary\nground truth, the proposed model achieves state-of-the-art performance in\npublic datasets. https://github.com/liuzywen/COD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhili Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comprehensive survey on semantic facial attribute editing using generative adversarial networks. (arXiv:2205.10587v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10587","description":"<p>Generating random photo-realistic images has experienced tremendous growth\nduring the past few years due to the advances of the deep convolutional neural\nnetworks and generative models. Among different domains, face photos have\nreceived a great deal of attention and a large number of face generation and\nmanipulation models have been proposed. Semantic facial attribute editing is\nthe process of varying the values of one or more attributes of a face image\nwhile the other attributes of the image are not affected. The requested\nmodifications are provided as an attribute vector or in the form of driving\nface image and the whole process is performed by the corresponding models. In\nthis paper, we survey the recent works and advances in semantic facial\nattribute editing. We cover all related aspects of these models including the\nrelated definitions and concepts, architectures, loss functions, datasets,\nevaluation metrics, and applications. Based on their architectures, the\nstate-of-the-art models are categorized and studied as encoder-decoder,\nimage-to-image, and photo-guided models. The challenges and restrictions of the\ncurrent state-of-the-art methods are discussed as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_M/0/1/0/all/0/1\">Maryam Saeedi Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farid_N/0/1/0/all/0/1\">Nastaran Moradzadeh Farid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadbagheri_N/0/1/0/all/0/1\">Najmeh Mohammadbagheri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facing the Void: Overcoming Missing Data in Multi-View Imagery. (arXiv:2205.10592v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10592","description":"<p>In some scenarios, a single input image may not be enough to allow the object\nclassification. In those cases, it is crucial to explore the complementary\ninformation extracted from images presenting the same object from multiple\nperspectives (or views) in order to enhance the general scene understanding\nand, consequently, increase the performance. However, this task, commonly\ncalled multi-view image classification, has a major challenge: missing data. In\nthis paper, we propose a novel technique for multi-view image classification\nrobust to this problem. The proposed method, based on state-of-the-art deep\nlearning-based approaches and metric learning, can be easily adapted and\nexploited in other applications and domains. A systematic evaluation of the\nproposed algorithm was conducted using two multi-view aerial-ground datasets\nwith very distinct properties. Results show that the proposed algorithm\nprovides improvements in multi-view image classification accuracy when compared\nto state-of-the-art methods. Code available at\n\\url{https://github.com/Gabriellm2003/remote_sensing_missing_data}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machado_G/0/1/0/all/0/1\">Gabriel Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_K/0/1/0/all/0/1\">Keiller Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_M/0/1/0/all/0/1\">Matheus Barros Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson Alex dos Santos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Myocardial Segmentation of Late Gadolinium Enhanced MR Images by Propagation of Contours from Cine MR Images. (arXiv:2205.10595v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10595","description":"<p>Automatic segmentation of myocardium in Late Gadolinium Enhanced (LGE)\nCardiac MR (CMR) images is often difficult due to the intensity heterogeneity\nresulting from accumulation of contrast agent in infarcted areas. In this\npaper, we propose an automatic segmentation framework that fully utilizes\nshared information between corresponding cine and LGE images of a same patient.\nGiven myocardial contours in cine CMR images, the proposed framework achieves\naccurate segmentation of LGE CMR images in a coarse-to-fine manner. Affine\nregistration is first performed between the corresponding cine and LGE image\npair, followed by nonrigid registration, and finally local deformation of\nmyocardial contours driven by forces derived from local features of the LGE\nimage. Experimental results on real patient data with expert outlined ground\ntruth show that the proposed framework can generate accurate and reliable\nresults for myocardial segmentation of LGE CMR images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Ying Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chai_P/0/1/0/all/0/1\">Ping Chai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Low_A/0/1/0/all/0/1\">Adrian Low</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ong_S/0/1/0/all/0/1\">Sim Heng Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brain Cortical Functional Gradients Predict Cortical Folding Patterns via Attention Mesh Convolution. (arXiv:2205.10605v1 [q-bio.NC])","link":"http://arxiv.org/abs/2205.10605","description":"<p>Since gyri and sulci, two basic anatomical building blocks of cortical\nfolding patterns, were suggested to bear different functional roles, a precise\nmapping from brain function to gyro-sulcal patterns can provide profound\ninsights into both biological and artificial neural networks. However, there\nlacks a generic theory and effective computational model so far, due to the\nhighly nonlinear relation between them, huge inter-individual variabilities and\na sophisticated description of brain function regions/networks distribution as\nmosaics, such that spatial patterning of them has not been considered. we\nadopted brain functional gradients derived from resting-state fMRI to embed the\n\"gradual\" change of functional connectivity patterns, and developed a novel\nattention mesh convolution model to predict cortical gyro-sulcal segmentation\nmaps on individual brains. The convolution on mesh considers the spatial\norganization of functional gradients and folding patterns on a cortical sheet\nand the newly designed channel attention block enhances the interpretability of\nthe contribution of different functional gradients to cortical folding\nprediction. Experiments show that the prediction performance via our model\noutperforms other state-of-the-art models. In addition, we found that the\ndominant functional gradients contribute less to folding prediction. On the\nactivation maps of the last layer, some well-studied cortical landmarks are\nfound on the borders of, rather than within, the highly activated regions.\nThese results and findings suggest that a specifically designed artificial\nneural network can improve the precision of the mapping between brain functions\nand cortical folding patterns, and can provide valuable insight of brain\nanatomy-function relation for neuroscience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+He_Z/0/1/0/all/0/1\">Zhibin He</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_C/0/1/0/all/0/1\">Changhe Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Human Pose Estimation Using Heatmap-Weighting Loss. (arXiv:2205.10611v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10611","description":"<p>Recent research on human pose estimation exploits complex structures to\nimprove performance on benchmark datasets, ignoring the resource overhead and\ninference speed when the model is actually deployed. In this paper, we lighten\nthe computation cost and parameters of the deconvolution head network in\nSimpleBaseline and introduce an attention mechanism that utilizes original,\ninter-level, and intra-level information to intensify the accuracy.\nAdditionally, we propose a novel loss function called heatmap weighting loss,\nwhich generates weights for each pixel on the heatmap that makes the model more\nfocused on keypoints. Experiments demonstrate our method achieves a balance\nbetween performance, resource volume, and inference speed. Specifically, our\nmethod can achieve 65.3 AP score on COCO test-dev, while the inference speed is\n55 FPS and 18 FPS on the mobile GPU and CPU, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiang Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Concealment: Free Lunch for Defending Adversarial Attacks. (arXiv:2205.10617v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10617","description":"<p>Recent studies show that the deep neural networks (DNNs) have achieved great\nsuccess in various tasks. However, even the \\emph{state-of-the-art} deep\nlearning based classifiers are extremely vulnerable to adversarial examples,\nresulting in sharp decay of discrimination accuracy in the presence of enormous\nunknown attacks. Given the fact that neural networks are widely used in the\nopen world scenario which can be safety-critical situations, mitigating the\nadversarial effects of deep learning methods has become an urgent need.\nGenerally, conventional DNNs can be attacked with a dramatically high success\nrate since their gradient is exposed thoroughly in the white-box scenario,\nmaking it effortless to ruin a well trained classifier with only imperceptible\nperturbations in the raw data space. For tackling this problem, we propose a\nplug-and-play layer that is training-free, termed as \\textbf{G}radient\n\\textbf{C}oncealment \\textbf{M}odule (GCM), concealing the vulnerable direction\nof gradient while guaranteeing the classification accuracy during the inference\ntime. GCM reports superior defense results on the ImageNet classification\nbenchmark, improving up to 63.41\\% top-1 attack robustness (AR) when faced with\nadversarial inputs compared to the vanilla DNNs. Moreover, we use GCM in the\nCVPR 2022 Robust Classification Challenge, currently achieving \\textbf{2nd}\nplace in Phase II with only a tiny version of ConvNext. The code will be made\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Sen Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiaxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pilot Study of Relating MYCN-Gene Amplification with Neuroblastoma-Patient CT Scans. (arXiv:2205.10619v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10619","description":"<p>Neuroblastoma is one of the most common cancers in infants, and the initial\ndiagnosis of this disease is difficult. At present, the MYCN gene amplification\n(MNA) status is detected by invasive pathological examination of tumor samples.\nThis is time-consuming and may have a hidden impact on children. To handle this\nproblem, we adopt multiple machine learning (ML) algorithms to predict the\npresence or absence of MYCN gene amplification. The dataset is composed of\nretrospective CT images of 23 neuroblastoma patients. Different from previous\nwork, we develop the algorithm without manually-segmented primary tumors which\nis time-consuming and not practical. Instead, we only need the coordinate of\nthe center point and the number of tumor slices given by a subspecialty-trained\npediatric radiologist. Specifically, CNN-based method uses pre-trained\nconvolutional neural network, and radiomics-based method extracts radiomics\nfeatures. Our results show that CNN-based method outperforms the\nradiomics-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_X/0/1/0/all/0/1\">Xiang Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_X/0/1/0/all/0/1\">Xuehua Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_J/0/1/0/all/0/1\">Jianbo Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints. (arXiv:2205.10636v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10636","description":"<p>Structured representations such as keypoints are widely used in pose\ntransfer, conditional image generation, animation, and 3D reconstruction.\nHowever, their supervised learning requires expensive annotation for each\ntarget domain. We propose a self-supervised method that learns to disentangle\nobject structure from the appearance with a graph of 2D keypoints linked by\nstraight edges. Both the keypoint location and their pairwise edge weights are\nlearned, given only a collection of images depicting the same object class. The\ngraph is interpretable, for example, AutoLink recovers the human skeleton\ntopology when applied to images showing people. Our key ingredients are i) an\nencoder that predicts keypoint locations in an input image, ii) a shared graph\nas a latent variable that links the same pairs of keypoints in every image,\niii) an intermediate edge map that combines the latent graph edge weights and\nkeypoint locations in a soft, differentiable manner, and iv) an inpainting\nobjective on randomly masked images. Although simpler, AutoLink outperforms\nexisting self-supervised methods on the established keypoint and pose\nestimation benchmarks and paves the way for structure-conditioned generative\nmodels on more diverse datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based out-of-distribution detection for clinically safe segmentation. (arXiv:2205.10650v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10650","description":"<p>In a clinical setting it is essential that deployed image processing systems\nare robust to the full range of inputs they might encounter and, in particular,\ndo not make confidently wrong predictions. The most popular approach to safe\nprocessing is to train networks that can provide a measure of their\nuncertainty, but these tend to fail for inputs that are far outside the\ntraining data distribution. Recently, generative modelling approaches have been\nproposed as an alternative; these can quantify the likelihood of a data sample\nexplicitly, filtering out any out-of-distribution (OOD) samples before further\nprocessing is performed. In this work, we focus on image segmentation and\nevaluate several approaches to network uncertainty in the far-OOD and near-OOD\ncases for the task of segmenting haemorrhages in head CTs. We find all of these\napproaches are unsuitable for safe segmentation as they provide confidently\nwrong predictions when operating OOD. We propose performing full 3D OOD\ndetection using a VQ-GAN to provide a compressed latent representation of the\nimage and a transformer to estimate the data likelihood. Our approach\nsuccessfully identifies images in both the far- and near-OOD cases. We find a\nstrong relationship between image likelihood and the quality of a model's\nsegmentation, making this approach viable for filtering images unsuitable for\nsegmentation. To our knowledge, this is the first time transformers have been\napplied to perform OOD detection on 3D image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graham_M/0/1/0/all/0/1\">Mark S Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tudosiu_P/0/1/0/all/0/1\">Petru-Daniel Tudosiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_P/0/1/0/all/0/1\">Paul Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinaya_W/0/1/0/all/0/1\">Walter Hugo Lopez Pinaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jean_Marie_U/0/1/0/all/0/1\">U Jean-Marie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mah_Y/0/1/0/all/0/1\">Yee Mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1\">James Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_R/0/1/0/all/0/1\">Rolf H J&#xe4;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werring_D/0/1/0/all/0/1\">David Werring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1\">Parashkev Nachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1\">M Jorge Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards real-time and energy efficient Siamese tracking -- a hardware-software approach. (arXiv:2205.10653v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10653","description":"<p>Siamese trackers have been among the state-of-the-art solutions in each\nVisual Object Tracking (VOT) challenge over the past few years. However, with\ngreat accuracy comes great computational complexity: to achieve real-time\nprocessing, these trackers have to be massively parallelised and are usually\nrun on high-end GPUs. Easy to implement, this approach is energy consuming, and\nthus cannot be used in many low-power applications. To overcome this, one can\nuse energy-efficient embedded devices, such as heterogeneous platforms joining\nthe ARM processor system with programmable logic (FPGA). In this work, we\npropose a hardware-software implementation of the well-known fully connected\nSiamese tracker (SiamFC). We have developed a quantised Siamese network for the\nFINN accelerator, using algorithm-accelerator co-design, and performed design\nspace exploration to achieve the best efficiency-to-energy ratio (determined by\nFPS and used resources). For our network, running in the programmable logic\npart of the Zynq UltraScale+ MPSoC ZCU104, we achieved the processing of almost\n50 frames-per-second with tracker accuracy on par with its floating point\ncounterpart, as well as the original SiamFC network. The complete tracking\nsystem, implemented in ARM with the network accelerated on FPGA, achieves up to\n17 fps. These results bring us towards bridging the gap between the highly\naccurate but energy-demanding algorithms and energy-efficient solutions ready\nto be used in low-power, edge systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Przewlocka_Rus_D/0/1/0/all/0/1\">Dominika Przewlocka-Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1\">Tomasz Kryjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swept-Angle Synthetic Wavelength Interferometry. (arXiv:2205.10655v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10655","description":"<p>We present a new imaging technique, swept-angle synthetic wavelength\ninterferometry, for full-field micron-scale 3D sensing. As in conventional\nsynthetic wavelength interferometry, our technique uses light consisting of two\noptical wavelengths, resulting in per-pixel interferometric measurements whose\nphase encodes scene depth. Our technique additionally uses a new type of light\nsource that, by emulating spatially-incoherent illumination, makes\ninterferometric measurements insensitive to global illumination effects that\nconfound depth information. The resulting technique combines the speed of\nfull-field interferometric setups with the robustness to global illumination of\nscanning interferometric setups. Overall, our technique can recover full-frame\ndepth at a spatial and axial resolution of a few micrometers using as few as 16\nmeasurements, resulting in fast acquisition at frame rates of 10 Hz. We build\nan experimental prototype and use it to demonstrate these capabilities, by\nscanning a variety of scenes that contain challenging light transport effects\nsuch as interreflections, subsurface scattering, and specularities. We validate\nthe accuracy of our measurements by showing that they closely match reference\nmeasurements from a full-field optical coherence tomography system, despite\nbeing captured at orders of magnitude faster acquisition times and while\noperating under strong ambient light.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotwal_A/0/1/0/all/0/1\">Alankar Kotwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levin_A/0/1/0/all/0/1\">Anat Levin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkioulekas_I/0/1/0/all/0/1\">Ioannis Gkioulekas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformers in 2022: An Update on Tiny ImageNet. (arXiv:2205.10660v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10660","description":"<p>The recent advances in image transformers have shown impressive results and\nhave largely closed the gap between traditional CNN architectures. The standard\nprocedure is to train on large datasets like ImageNet-21k and then finetune on\nImageNet-1k. After finetuning, researches will often consider the transfer\nlearning performance on smaller datasets such as CIFAR-10/100 but have left out\nTiny ImageNet. This paper offers an update on vision transformers' performance\non Tiny ImageNet. I include Vision Transformer (ViT) , Data Efficient Image\nTransformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin\nTransformers. In addition, Swin Transformers beats the current state-of-the-art\nresult with a validation accuracy of 91.35%. Code is available here:\nhttps://github.com/ehuynh1106/TinyImageNet-Transformers\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_E/0/1/0/all/0/1\">Ethan Huynh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariant Mesh Attention Networks. (arXiv:2205.10662v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10662","description":"<p>Equivariance to symmetries has proven to be a powerful inductive bias in deep\nlearning research. Recent works on mesh processing have concentrated on various\nkinds of natural symmetries, including translations, rotations, scaling, node\npermutations, and gauge transformations. To date, no existing architecture is\nequivariant to all of these transformations. Moreover, previous implementations\nhave not always applied these symmetry transformations to the test dataset.\nThis inhibits the ability to determine whether the model attains the claimed\nequivariance properties. In this paper, we present an attention-based\narchitecture for mesh data that is provably equivariant to all transformations\nmentioned above. We carry out experiments on the FAUST and TOSCA datasets, and\napply the mentioned symmetries to the test set only. Our results confirm that\nour proposed architecture is equivariant, and therefore robust, to these\nlocal/global transformations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sourya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_Posada_J/0/1/0/all/0/1\">Jose Gallego-Posada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vigano_F/0/1/0/all/0/1\">Francesco Vigan&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowbottom_J/0/1/0/all/0/1\">James Rowbottom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Taco Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer based Generative Adversarial Network for Liver Segmentation. (arXiv:2205.10663v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10663","description":"<p>Automated liver segmentation from radiology scans (CT, MRI) can improve\nsurgery and therapy planning and follow-up assessment in addition to\nconventional use for diagnosis and prognosis. Although convolutional neural\nnetworks (CNNs) have become the standard image segmentation tasks, more\nrecently this has started to change towards Transformers based architectures\nbecause Transformers are taking advantage of capturing long range dependence\nmodeling capability in signals, so called attention mechanism. In this study,\nwe propose a new segmentation approach using a hybrid approach combining the\nTransformer(s) with the Generative Adversarial Network (GAN) approach. The\npremise behind this choice is that the self-attention mechanism of the\nTransformers allows the network to aggregate the high dimensional feature and\nprovide global information modeling. This mechanism provides better\nsegmentation performance compared with traditional methods. Furthermore, we\nencode this generator into the GAN based architecture so that the discriminator\nnetwork in the GAN can classify the credibility of the generated segmentation\nmasks compared with the real masks coming from human (expert) annotations. This\nallows us to extract the high dimensional topology information in the mask for\nbiomedical image segmentation and provide more reliable segmentation results.\nOur model achieved a high dice coefficient of 0.9433, recall of 0.9515, and\nprecision of 0.9376 and outperformed other Transformer based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1\">Ugur Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antalek_M/0/1/0/all/0/1\">Matthew Antalek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1\">Elif Keles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borhani_A/0/1/0/all/0/1\">Amir Borhani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ladner_D/0/1/0/all/0/1\">Daniela Ladner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10667","description":"<p>Traditionally, extracting patterns from eye movement data relies on\nstatistics of different macro-events such as fixations and saccades. This\nrequires an additional preprocessing step to separate the eye movement\nsubtypes, often with a number of parameters on which the classification results\ndepend. Besides that, definitions of such macro events are formulated in\ndifferent ways by different researchers.\n</p>\n<p>We propose an application of a new class of features to the quantitative\nanalysis of personal eye movement trajectories structure. This new class of\nfeatures based on algebraic topology allows extracting patterns from different\nmodalities of gaze such as time series of coordinates and amplitudes, heatmaps,\nand point clouds in a unified way at all scales from micro to macro. We\nexperimentally demonstrate the competitiveness of the new class of features\nwith the traditional ones and their significant synergy while being used\ntogether for the person authentication task on the recently published eye\nmovement trajectories dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onuchin_A/0/1/0/all/0/1\">Arsenii Onuchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kachan_O/0/1/0/all/0/1\">Oleg Kachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy. (arXiv:2205.10683v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10683","description":"<p>Large convolutional neural networks (CNN) can be difficult to train in the\ndifferentially private (DP) regime, since the optimization algorithms require a\ncomputationally expensive operation, known as the per-sample gradient clipping.\nWe propose an efficient and scalable implementation of this clipping on\nconvolutional layers, termed as the mixed ghost clipping, that significantly\neases the private training in terms of both time and space complexities,\nwithout affecting the accuracy. The improvement in efficiency is rigorously\nstudied through the first complexity analysis for the mixed ghost clipping and\nexisting DP training algorithms.\n</p>\n<p>Extensive experiments on vision classification tasks, with large ResNet, VGG,\nand Vision Transformers, demonstrate that DP training with mixed ghost clipping\nadds $1\\sim 10\\%$ memory overhead and $&lt;2\\times$ slowdown to the standard\nnon-private training. Specifically, when training VGG19 on CIFAR10, the mixed\nghost clipping is $3\\times$ faster than state-of-the-art Opacus library with\n$18\\times$ larger maximum batch size. To emphasize the significance of\nefficient DP training on convolutional layers, we achieve 96.7\\% accuracy on\nCIFAR10 and 83.0\\% on CIFAR100 at $\\epsilon=1$ using BEiT, while the previous\nbest results are 94.8\\% and 67.4\\%, respectively. We open-source a privacy\nengine (\\url{https://github.com/JialinMao/private_CNN}) that implements DP\ntraining of CNN with a few lines of code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jialin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shiyun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Producing Histopathology Phantom Images using Generative Adversarial Networks to improve Tumor Detection. (arXiv:2205.10691v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10691","description":"<p>Advance in medical imaging is an important part in deep learning research.\nOne of the goals of computer vision is development of a holistic, comprehensive\nmodel which can identify tumors from histology slides obtained via biopsies. A\nmajor problem that stands in the way is lack of data for a few cancer-types. In\nthis paper, we ascertain that data augmentation using GANs can be a viable\nsolution to reduce the unevenness in the distribution of different cancer types\nin our dataset. Our demonstration showed that a dataset augmented to a 50%\nincrease causes an increase in tumor detection from 80% to 87.5%\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gautam_V/0/1/0/all/0/1\">Vidit Gautam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GL-RG: Global-Local Representation Granularity for Video Captioning. (arXiv:2205.10706v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10706","description":"<p>Video captioning is a challenging task as it needs to accurately transform\nvisual understanding into natural language description. To date,\nstate-of-the-art methods inadequately model global-local representation across\nvideo frames for caption generation, leaving plenty of room for improvement. In\nthis work, we approach the video captioning task from a new perspective and\npropose a GL-RG framework for video captioning, namely a\n\\textbf{G}lobal-\\textbf{L}ocal \\textbf{R}epresentation \\textbf{G}ranularity.\nOur GL-RG demonstrates three advantages over the prior efforts: 1) we\nexplicitly exploit extensive visual representations from different video ranges\nto improve linguistic expression; 2) we devise a novel global-local encoder to\nproduce rich semantic vocabulary to obtain a descriptive granularity of video\ncontents across frames; 3) we develop an incremental training strategy which\norganizes model learning in an incremental fashion to incur an optimal\ncaptioning behavior. Experimental results on the challenging MSR-VTT and MSVD\ndatasets show that our DL-RG outperforms recent state-of-the-art methods by a\nsignificant margin. Code is available at \\url{https://github.com/ylqi/GL-RG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Liqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongfang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Housekeep: Tidying Virtual Households using Commonsense Reasoning. (arXiv:2205.10712v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10712","description":"<p>We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the\nhome for embodied AI. In Housekeep, an embodied agent must tidy a house by\nrearranging misplaced objects without explicit instructions specifying which\nobjects need to be rearranged. Instead, the agent must learn from and is\nevaluated against human preferences of which objects belong where in a tidy\nhouse. Specifically, we collect a dataset of where humans typically place\nobjects in tidy and untidy houses constituting 1799 objects, 268 object\ncategories, 585 placements, and 105 rooms. Next, we propose a modular baseline\napproach for Housekeep that integrates planning, exploration, and navigation.\nIt leverages a fine-tuned large language model (LLM) trained on an internet\ntext corpus for effective planning. We show that our baseline agent generalizes\nto rearranging unseen objects in unknown environments. See our webpage for more\ndetails: https://yashkant.github.io/housekeep/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kant_Y/0/1/0/all/0/1\">Yash Kant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1\">Arun Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yenamandra_S/0/1/0/all/0/1\">Sriram Yenamandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1\">Igor Gilitschenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szot_A/0/1/0/all/0/1\">Andrew Szot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_H/0/1/0/all/0/1\">Harsh Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Visual Words for Interpretable Image Recognition. (arXiv:2205.10724v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10724","description":"<p>To interpret deep models' predictions, attention-based visual cues are widely\nused in addressing \\textit{why} deep models make such predictions. Beyond that,\nthe current research community becomes more interested in reasoning\n\\textit{how} deep models make predictions, where some prototype-based methods\nemploy interpretable representations with their corresponding visual cues to\nreveal the black-box mechanism of deep model behaviors. However, these\npioneering attempts only either learn the category-specific prototypes and\ndeteriorate their generalizing capacities, or demonstrate several illustrative\nexamples without a quantitative evaluation of visual-based interpretability\nwith further limitations on their practical usages. In this paper, we revisit\nthe concept of visual words and propose the Learnable Visual Words (LVW) to\ninterpret the model prediction behaviors with two novel modules: semantic\nvisual words learning and dual fidelity preservation. The semantic visual words\nlearning relaxes the category-specific constraint, enabling the general visual\nwords shared across different categories. Beyond employing the visual words for\nprediction to align visual words with the base model, our dual fidelity\npreservation also includes the attention guided semantic alignment that\nencourages the learned visual words to focus on the same conceptual regions for\nprediction. Experiments on six visual benchmarks demonstrate the superior\neffectiveness of our proposed LVW in both accuracy and model interpretation\nover the state-of-the-art methods. Moreover, we elaborate on various in-depth\nanalyses to further explore the learned visual words and the generalizability\nof our method for unseen categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenxiao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTAdapt: Optimal Transport-based Approach For Unsupervised Domain Adaptation. (arXiv:2205.10738v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10738","description":"<p>Unsupervised domain adaptation is one of the challenging problems in computer\nvision. This paper presents a novel approach to unsupervised domain adaptations\nbased on the optimal transport-based distance. Our approach allows aligning\ntarget and source domains without the requirement of meaningful metrics across\ndomains. In addition, the proposal can associate the correct mapping between\nsource and target domains and guarantee a constraint of topology between source\nand target domains. The proposed method is evaluated on different datasets in\nvarious problems, i.e. (i) digit recognition on MNIST, MNIST-M, USPS datasets,\n(ii) Object recognition on Amazon, Webcam, DSLR, and VisDA datasets, (iii)\nInsect Recognition on the IP102 dataset. The experimental results show that our\nproposed method consistently improves performance accuracy. Also, our framework\ncould be incorporated with any other CNN frameworks within an end-to-end deep\nnetwork design for recognition problems to improve their performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chappa_N/0/1/0/all/0/1\">Naga Venkata Sai Raviteja Chappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan Bac Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowling_A/0/1/0/all/0/1\">Ashley Dowling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Quasars, Galaxies, and Stars in the Mapping of the Universe Multi-modal Deep Learning. (arXiv:2205.10745v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10745","description":"<p>In this paper, the fourth version the Sloan Digital Sky Survey (SDSS-4), Data\nRelease 16 dataset was used to classify the SDSS dataset into galaxies, stars,\nand quasars using machine learning and deep learning architectures. We\nefficiently utilize both image and metadata in tabular format to build a novel\nmulti-modal architecture and achieve state-of-the-art results. In addition, our\nexperiments on transfer learning using Imagenet weights on five different\narchitectures (Resnet-50, DenseNet-121 VGG-16, Xception, and EfficientNet)\nreveal that freezing all layers and adding a final trainable layer may not be\nan optimal solution for transfer learning. It is hypothesized that higher the\nnumber of trainable layers, higher will be the training time and accuracy of\npredictions. It is also hypothesized that any subsequent increase in the number\nof training layers towards the base layers will not increase in accuracy as the\npre trained lower layers only help in low level feature extraction which would\nbe quite similar in all the datasets. Hence the ideal level of trainable layers\nneeds to be identified for each model in respect to the number of parameters.\nFor the tabular data, we compared classical machine learning algorithms\n(Logistic Regression, Random Forest, Decision Trees, Adaboost, LightGBM etc.,)\nwith artificial neural networks. Our works shed new light on transfer learning\nand multi-modal deep learning architectures. The multi-modal architecture not\nonly resulted in higher metrics (accuracy, precision, recall, F1 score) than\nmodels using only image data or tabular data. Furthermore, multi-modal\narchitecture achieved the best metrics in lesser training epochs and improved\nthe metrics on all classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ethiraj_S/0/1/0/all/0/1\">Sabeesh Ethiraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolla_B/0/1/0/all/0/1\">Bharath Kumar Bolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners. (arXiv:2205.10747v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10747","description":"<p>The goal of this work is to build flexible video-language models that can\ngeneralize to various video-to-text tasks from few examples, such as\ndomain-specific captioning, question answering, and future event prediction.\nExisting few-shot video-language learners focus exclusively on the encoder,\nresulting in the absence of a video-to-text decoder to handle generative tasks.\nVideo captioners have been pretrained on large-scale video-language datasets,\nbut they rely heavily on finetuning and lack the ability to generate text for\nunseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language\nLearner via Image and Language models, which demonstrates strong performance on\nfew-shot video-to-text tasks without the necessity of pretraining or finetuning\non any video datasets. We use the image-language models to translate the video\ncontent into frame captions, object, attribute, and event phrases, and compose\nthem into a temporal structure template. We then instruct a language model,\nwith a prompt containing a few in-context examples, to generate a target output\nfrom the composed content. The flexibility of prompting allows the model to\ncapture any form of text input, such as automatic speech recognition (ASR)\ntranscripts. Our experiments demonstrate the power of language models in\nunderstanding videos on a wide variety of video-language tasks, including video\ncaptioning, video question answering, video caption retrieval, and video future\nevent prediction. Especially, on video future event prediction, our few-shot\nmodel significantly outperforms state-of-the-art supervised models trained on\nlarge-scale video datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preparing data for pathological artificial intelligence with clinical-grade performance. (arXiv:2205.10748v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10748","description":"<p>[Purpose] The pathology is decisive for disease diagnosis, but relies heavily\non the experienced pathologists. Recently, pathological artificial intelligence\n(PAI) is thought to improve diagnostic accuracy and efficiency. However, the\nhigh performance of PAI based on deep learning in the laboratory generally\ncannot be reproduced in the clinic. [Methods] Because the data preparation is\nimportant for PAI, the paper has reviewed PAI-related studies in the PubMed\ndatabase published from January 2017 to February 2022, and 118 studies were\nincluded. The in-depth analysis of methods for preparing data is performed,\nincluding obtaining slides of pathological tissue, cleaning, screening, and\nthen digitizing. Expert review, image annotation, dataset division for model\ntraining and validation are also discussed. We further discuss the reasons why\nthe high performance of PAI is not reproducible in the clinical practices and\nshow some effective ways to improve clinical performances of PAI. [Results] The\nrobustness of PAI depend on randomized collection of representative disease\nslides, including rigorous quality control and screening, correction of digital\ndiscrepancies, reasonable annotation, and the amount of data. The digital\npathology is fundamental of clinical-grade PAI, and the techniques of data\nstandardization and weakly supervised learning methods based on whole slide\nimage (WSI) are effective ways to overcome obstacles of performance\nreproduction. [Conclusion] The representative data, the amount of labeling and\nconsistency from multi-centers is the key to performance reproduction. The\ndigital pathology for clinical diagnosis, data standardization and technique of\nWSI-based weakly supervised learning hopefully build clinical-grade PAI.\nKeywords: pathological artificial intelligence; data preparation;\nclinical-grade; deep learning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanqing Yang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yanhua Gao</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kuangsong Wang</a> (3 and 4), <a href=\"http://arxiv.org/find/eess/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a> (1) ((1) Department of Biomedical Engineering, School of Basic Medical Sciences, Central South University, Changsha, China,(2) Department of Ultrasound, Shaanxi Provincial People&#x27;s Hospital, Xi&#x27;an, China,(3) Department of Pathology, School of Basic Medical Sciences, Central South University, Changsha, China,(4) Department of Pathology, Xiangya Hospital, Central South University, Changsha, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real Time Detection Free Tracking of Multiple Objects Via Equilibrium Optimizer. (arXiv:2205.10756v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10756","description":"<p>Multiple objects tracking (MOT) is a difficult task, as it usually requires\nspecial hardware and higher computation complexity. In this work, we present a\nnew framework of MOT by using of equilibrium optimizer (EO) algorithm and\nreducing the resolution of the bounding boxes of the objects to solve such\nproblems in the detection free framework. First, in the first frame the target\nobjects are initialized and its size is computed, then its resolution is\nreduced if it is higher than a threshold, and then modeled by their kernel\ncolor histogram to establish a feature model. The Bhattacharya distances\nbetween the histogram of object models and other candidates are used as the\nfitness function to be optimized. Multiple agents are generated by EO,\naccording to the number of the target objects to be tracked. EO algorithm is\nused because of its efficiency and lower computation cost compared to other\nalgorithms in global optimization. Experimental results confirm that EO\nmulti-object tracker achieves satisfying tracking results then other trackers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Charef_Khodja_D/0/1/0/all/0/1\">Djemai Charef-Khodja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abida_T/0/1/0/all/0/1\">Toumi Abida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Feature Fusion via Graph Convolutional Network for Intracranial Artery Labeling. (arXiv:2205.10757v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10757","description":"<p>Intracranial arteries are critical blood vessels that supply the brain with\noxygenated blood. Intracranial artery labels provide valuable guidance and\nnavigation to numerous clinical applications and disease diagnoses. Various\nmachine learning algorithms have been carried out for automation in the\nanatomical labeling of cerebral arteries. However, the task remains challenging\nbecause of the high complexity and variations of intracranial arteries. This\nstudy investigates a novel graph convolutional neural network with deep feature\nfusion for cerebral artery labeling. We introduce stacked graph convolutions in\nan encoder-core-decoder architecture, extracting high-level representations\nfrom graph nodes and their neighbors. Furthermore, we efficiently aggregate\nintermediate features from different hierarchies to enhance the proposed\nmodel's representation capability and labeling performance. We perform\nextensive experiments on public datasets, in which the results prove the\nsuperiority of our approach over baselines by a clear margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaxin Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_P/0/1/0/all/0/1\">Peisheng Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Channel Attention Network for Brain Glioma Segmentation. (arXiv:2205.10758v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10758","description":"<p>A glioma is a malignant brain tumor that seriously affects cognitive\nfunctions and lowers patients' life quality. Segmentation of brain glioma is\nchallenging because of interclass ambiguities in tumor regions. Recently, deep\nlearning approaches have achieved outstanding performance in the automatic\nsegmentation of brain glioma. However, existing algorithms fail to exploit\nchannel-wise feature interdependence to select semantic attributes for glioma\nsegmentation. In this study, we implement a novel deep neural network that\nintegrates residual channel attention modules to calibrate intermediate\nfeatures for glioma segmentation. The proposed channel attention mechanism\nadaptively weights feature channel-wise to optimize the latent representation\nof gliomas. We evaluate our method on the established dataset BraTS2017.\nExperimental results indicate the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1\">Yiming Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_P/0/1/0/all/0/1\">Peisheng Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNNs are Myopic. (arXiv:2205.10760v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10760","description":"<p>We claim that Convolutional Neural Networks (CNNs) learn to classify images\nusing only small seemingly unrecognizable tiles. We show experimentally that\nCNNs trained only using such tiles can match or even surpass the performance of\nCNNs trained on full images. Conversely, CNNs trained on full images show\nsimilar predictions on small tiles. We also propose the first a priori\ntheoretical model for convolutional data sets that seems to explain this\nbehavior. This gives additional support to the long standing suspicion that\nCNNs do not need to understand the global structure of images to achieve\nstate-of-the-art accuracies. Surprisingly it also suggests that over-fitting is\nnot needed either.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madala_V/0/1/0/all/0/1\">Vamshi C. Madala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Shivkumar Chandrasekaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidence for Hypodescent in Visual Semantic AI. (arXiv:2205.10764v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10764","description":"<p>We examine the state-of-the-art multimodal \"visual semantic\" model CLIP\n(\"Contrastive Language Image Pretraining\") for the rule of hypodescent, or\none-drop rule, whereby multiracial people are more likely to be assigned a\nracial or ethnic label corresponding to a minority or disadvantaged racial or\nethnic group than to the equivalent majority or advantaged group. A face\nmorphing experiment grounded in psychological research demonstrating\nhypodescent indicates that, at the midway point of 1,000 series of morphed\nimages, CLIP associates 69.7% of Black-White female images with a Black text\nlabel over a White text label, and similarly prefers Latina (75.8%) and Asian\n(89.1%) text labels at the midway point for Latina-White female and Asian-White\nfemale morphs, reflecting hypodescent. Additionally, assessment of the\nunderlying cosine similarities in the model reveals that association with White\nis correlated with association with \"person,\" with Pearson's rho as high as\n0.82 over a 21,000-image morph series, indicating that a White person\ncorresponds to the default representation of a person in CLIP. Finally, we show\nthat the stereotype-congruent pleasantness association of an image correlates\nwith association with the Black text label in CLIP, with Pearson's rho = 0.48\nfor 21,000 Black-White multiracial male images, and rho = 0.41 for Black-White\nmultiracial female images. CLIP is trained on English-language text gathered\nusing data collected from an American website (Wikipedia), and our findings\ndemonstrate that CLIP embeds the values of American racial hierarchy,\nreflecting the implicit and explicit beliefs that are present in human minds.\nWe contextualize these findings within the history and psychology of\nhypodescent. Overall, the data suggests that AI supervised using natural\nlanguage will, unless checked, learn biases that reflect racial hierarchies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaji_M/0/1/0/all/0/1\">Mahzarin R. Banaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in Embedding Methods for Multi-Object Tracking: A Survey. (arXiv:2205.10766v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10766","description":"<p>Multi-object tracking (MOT) aims to associate target objects across video\nframes in order to obtain entire moving trajectories. With the advancement of\ndeep neural networks and the increasing demand for intelligent video analysis,\nMOT has gained significantly increased interest in the computer vision\ncommunity. Embedding methods play an essential role in object location\nestimation and temporal identity association in MOT. Unlike other computer\nvision tasks, such as image classification, object detection,\nre-identification, and segmentation, embedding methods in MOT have large\nvariations, and they have never been systematically analyzed and summarized. In\nthis survey, we first conduct a comprehensive overview with in-depth analysis\nfor embedding methods in MOT from seven different perspectives, including\npatch-level embedding, single-frame embedding, cross-frame joint embedding,\ncorrelation embedding, sequential embedding, tracklet embedding, and\ncross-track relational embedding. We further summarize the existing widely used\nMOT datasets and analyze the advantages of existing state-of-the-art methods\naccording to their embedding strategies. Finally, some critical yet\nunder-investigated areas and future research directions are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Instance Matting via Mutual Guidance and Multi-Instance Refinement. (arXiv:2205.10767v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10767","description":"<p>This paper introduces a new matting task called human instance matting (HIM),\nwhich requires the pertinent model to automatically predict a precise alpha\nmatte for each human instance. Straightforward combination of closely related\ntechniques, namely, instance segmentation, soft segmentation and\nhuman/conventional matting, will easily fail in complex cases requiring\ndisentangling mingled colors belonging to multiple instances along hairy and\nthin boundary structures. To tackle these technical challenges, we propose a\nhuman instance matting framework, called InstMatt, where a novel mutual\nguidance strategy working in tandem with a multi-instance refinement module is\nused, for delineating multi-instance relationship among humans with complex and\noverlapping boundaries if present. A new instance matting metric called\ninstance matting quality (IMQ) is proposed, which addresses the absence of a\nunified and fair means of evaluation emphasizing both instance recognition and\nmatting quality. Finally, we construct a HIM benchmark for evaluation, which\ncomprises of both synthetic and natural benchmark images. In addition to\nthorough experimental results on complex cases with multiple and overlapping\nhuman instances each has intricate boundaries, preliminary results are\npresented on general instance matting. Code and benchmark are available in\nhttps://github.com/nowsyn/InstMatt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Muti-expert Distribution Calibration for Long-tailed Video Classification. (arXiv:2205.10788v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10788","description":"<p>Most existing state-of-the-art video classification methods assume the\ntraining data obey a uniform distribution. However, video data in the real\nworld typically exhibit long-tail class distribution and imbalance, which\nextensively results in a model bias towards head class and leads to relatively\nlow performance on tail class. While the current long-tail classification\nmethods usually focus on image classification, adapting it to video data is not\na trivial extension. We propose an end-to-end multi-experts distribution\ncalibration method based on two-level distribution information to address these\nchallenges. The method jointly considers the distribution of samples in each\nclass (intra-class distribution) and the diverse distributions of overall data\n(inter-class distribution) to solve the problem of imbalanced data under\nlong-tailed distribution. By modeling this two-level distribution information,\nthe model can consider the head classes and the tail classes and significantly\ntransfer the knowledge from the head classes to improve the performance of the\ntail classes. Extensive experiments verify that our method achieves\nstate-of-the-art performance on the long-tailed video classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yufan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation via the Target-aware Transformer. (arXiv:2205.10793v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10793","description":"<p>Knowledge distillation becomes a de facto standard to improve the performance\nof small neural networks. Most of the previous works propose to regress the\nrepresentational features from the teacher to the student in a one-to-one\nspatial matching fashion. However, people tend to overlook the fact that, due\nto the architecture differences, the semantic information on the same spatial\nlocation usually vary. This greatly undermines the underlying assumption of the\none-to-one distillation approach. To this end, we propose a novel one-to-all\nspatial matching knowledge distillation approach. Specifically, we allow each\npixel of the teacher feature to be distilled to all spatial locations of the\nstudent features given its similarity, which is generated from a target-aware\ntransformer. Our approach surpasses the state-of-the-art methods by a\nsignificant margin on various computer vision benchmarks, such as ImageNet,\nPascal VOC and COCOStuff10k. Code will be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sihao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kaicheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReLU Fields: The Little Non-linearity That Could. (arXiv:2205.10824v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10824","description":"<p>In many recent works, multi-layer perceptions (MLPs) have been shown to be\nsuitable for modeling complex spatially-varying functions including images and\n3D scenes. Although the MLPs are able to represent complex scenes with\nunprecedented quality and memory footprint, this expressive power of the MLPs,\nhowever, comes at the cost of long training and inference times. On the other\nhand, bilinear/trilinear interpolation on regular grid based representations\ncan give fast training and inference times, but cannot match the quality of\nMLPs without requiring significant additional memory. Hence, in this work, we\ninvestigate what is the smallest change to grid-based representations that\nallows for retaining the high fidelity result of MLPs while enabling fast\nreconstruction and rendering times. We introduce a surprisingly simple change\nthat achieves this task -- simply allowing a fixed non-linearity (ReLU) on\ninterpolated grid values. When combined with coarse to-fine optimization, we\nshow that such an approach becomes competitive with the state-of-the-art. We\nreport results on radiance fields, and occupancy fields, and compare against\nmultiple existing alternatives. Code and data for the paper are available at\nhttps://geometry.cs.ucl.ac.uk/projects/2022/relu_fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karnewar_A/0/1/0/all/0/1\">Animesh Karnewar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1\">Oliver Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grad-CAM++ is Equivalent to Grad-CAM With Positive Gradients. (arXiv:2205.10838v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10838","description":"<p>The Grad-CAM algorithm provides a way to identify what parts of an image\ncontribute most to the output of a classifier deep network. The algorithm is\nsimple and widely used for localization of objects in an image, although some\nresearchers have point out its limitations, and proposed various alternatives.\nOne of them is Grad-CAM++, that according to its authors can provide better\nvisual explanations for network predictions, and does a better job at locating\nobjects even for occurrences of multiple object instances in a single image.\nHere we show that Grad-CAM++ is practically equivalent to a very simple\nvariation of Grad-CAM in which gradients are replaced with positive gradients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lerma_M/0/1/0/all/0/1\">Miguel Lerma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Mirtha Lucas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Visual Speech Analysis: A Survey. (arXiv:2205.10839v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10839","description":"<p>Visual speech, referring to the visual domain of speech, has attracted\nincreasing attention due to its wide applications, such as public security,\nmedical treatment, military defense, and film entertainment. As a powerful AI\nstrategy, deep learning techniques have extensively promoted the development of\nvisual speech learning. Over the past five years, numerous deep learning based\nmethods have been proposed to address various problems in this area, especially\nautomatic visual speech recognition and generation. To push forward future\nresearch on visual speech, this paper aims to present a comprehensive review of\nrecent progress in deep learning methods on visual speech analysis. We cover\ndifferent aspects of visual speech, including fundamental problems, challenges,\nbenchmark datasets, a taxonomy of existing methods, and state-of-the-art\nperformance. Besides, we also identify gaps in current research and discuss\ninspiring future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_C/0/1/0/all/0/1\">Changchong Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_G/0/1/0/all/0/1\">Gangyao Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Liang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Chenping Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietikainen_M/0/1/0/all/0/1\">Matti Pietik&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised U-net for few-shot learning of object segmentation in microscopy images. (arXiv:2205.10840v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10840","description":"<p>State-of-the-art segmentation performances are achieved by deep neural\nnetworks. Training these networks from only a few training examples is\nchallenging while producing annotated images that provide supervision is\ntedious. Recently, self-supervision, i.e. designing a neural pipeline providing\nsynthetic or indirect supervision, has proved to significantly increase\ngeneralization performances of models trained on few shots. This paper\nintroduces one such neural pipeline in the context of microscopic image\nsegmentation. By leveraging the rather simple content of these images a trainee\nnetwork can be mentored by a referee network which has been previously trained\non synthetically generated pairs of corrupted/correct region masks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deleruyelle_A/0/1/0/all/0/1\">Arnaud Deleruyelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Versari_C/0/1/0/all/0/1\">Cristian Versari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">John Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Road surface 3d reconstruction based on dense subpixel disparity map estimation. (arXiv:1807.01874v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1807.01874","description":"<p>Various 3D reconstruction methods have enabled civil engineers to detect\ndamage on a road surface. To achieve the millimetre accuracy required for road\ncondition assessment, a disparity map with subpixel resolution needs to be\nused. However, none of the existing stereo matching algorithms are specially\nsuitable for the reconstruction of the road surface. Hence in this paper, we\npropose a novel dense subpixel disparity estimation algorithm with high\ncomputational efficiency and robustness. This is achieved by first transforming\nthe perspective view of the target frame into the reference view, which not\nonly increases the accuracy of the block matching for the road surface but also\nimproves the processing speed. The disparities are then estimated iteratively\nusing our previously published algorithm where the search range is propagated\nfrom three estimated neighbouring disparities. Since the search range is\nobtained from the previous iteration, errors may occur when the propagated\nsearch range is not sufficient. Therefore, a correlation maxima verification is\nperformed to rectify this issue, and the subpixel resolution is achieved by\nconducting a parabola interpolation enhancement. Furthermore, a novel disparity\nglobal refinement approach developed from the Markov Random Fields and Fast\nBilateral Stereo is introduced to further improve the accuracy of the estimated\ndisparity map, where disparities are updated iteratively by minimising the\nenergy function that is related to their interpolated correlation polynomials.\nThe algorithm is implemented in C language with a near real-time performance.\nThe experimental results illustrate that the absolute error of the\nreconstruction varies from 0.1 mm to 3 mm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1\">Xiao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahnoun_N/0/1/0/all/0/1\">Naim Dahnoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pothole Detection Based on Disparity Transformation and Road Surface Modeling. (arXiv:1908.00894v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1908.00894","description":"<p>Pothole detection is one of the most important tasks for road maintenance.\nComputer vision approaches are generally based on either 2D road image analysis\nor 3D road surface modeling. However, these two categories are always used\nindependently. Furthermore, the pothole detection accuracy is still far from\nsatisfactory. Therefore, in this paper, we present a robust pothole detection\nalgorithm that is both accurate and computationally efficient. A dense\ndisparity map is first transformed to better distinguish between damaged and\nundamaged road areas. To achieve greater disparity transformation efficiency,\ngolden section search and dynamic programming are utilized to estimate the\ntransformation parameters. Otsu's thresholding method is then used to extract\npotential undamaged road areas from the transformed disparity map. The\ndisparities in the extracted areas are modeled by a quadratic surface using\nleast squares fitting. To improve disparity map modeling robustness, the\nsurface normal is also integrated into the surface modeling process.\nFurthermore, random sample consensus is utilized to reduce the effects caused\nby outliers. By comparing the difference between the actual and modeled\ndisparity maps, the potholes can be detected accurately. Finally, the point\nclouds of the detected potholes are extracted from the reconstructed 3D road\nsurface. The experimental results show that the successful detection accuracy\nof the proposed system is around 98.7% and the overall pixel-level accuracy is\napproximately 99.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgunalp_U/0/1/0/all/0/1\">Umar Ozgunalp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosking_B/0/1/0/all/0/1\">Brett Hosking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitas_I/0/1/0/all/0/1\">Ioannis Pitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGBT Salient Object Detection: A Large-scale Dataset and Benchmark. (arXiv:2007.03262v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.03262","description":"<p>Salient object detection in complex scenes and environments is a challenging\nresearch topic. Most works focus on RGB-based salient object detection, which\nlimits its performance of real-life applications when confronted with adverse\nconditions such as dark environments and complex backgrounds. Taking advantage\nof RGB and thermal infrared images becomes a new research direction for\ndetecting salient object in complex scenes recently, as thermal infrared\nspectrum imaging provides the complementary information and has been applied to\nmany computer vision tasks. However, current research for RGBT salient object\ndetection is limited by the lack of a large-scale dataset and comprehensive\nbenchmark. This work contributes such a RGBT image dataset named VT5000,\nincluding 5000 spatially aligned RGBT image pairs with ground truth\nannotations. VT5000 has 11 challenges collected in different scenes and\nenvironments for exploring the robustness of algorithms. With this dataset, we\npropose a powerful baseline approach, which extracts multi-level features\nwithin each modality and aggregates these features of all modalities with the\nattention mechanism, for accurate RGBT salient object detection. Extensive\nexperiments show that the proposed baseline approach outperforms the\nstate-of-the-art methods on VT5000 dataset and other two public datasets. In\naddition, we carry out a comprehensive analysis of different algorithms of RGBT\nsalient object detection on VT5000 dataset, and then make several valuable\nconclusions and provide some potential research directions for RGBT salient\nobject detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jieming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Multi Scale Neural Network for Crowd Counting. (arXiv:2007.14245v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.14245","description":"<p>Crowd Counting is a difficult but important problem in computer vision.\nConvolutional Neural Networks based on estimating the density map over the\nimage has been highly successful in this domain. However dense crowd counting\nremains an open problem because of severe occlusion and perspective view in\nwhich people can be present at various sizes. In this work, we propose a new\nnetwork which uses a ResNet based feature extractor, downsampling block which\nuses dilated convolutions and upsampling block using transposed convolutions.\nWe present a novel aggregation module which makes our network robust to the\nperspective view problem. We present the optimization details, loss functions\nand the algorithm used in our work. On evaluating on ShanghaiTech, UCF-CC-50\nand UCF-QNRF datasets using MSE and MAE as evaluation metrics, our network\noutperforms previous state of the art approaches while giving uncertainty\nestimates in a principled bayesian manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-Unet: A Dual-encoder U-Net for Image Splicing Forgery Detection and Localization. (arXiv:2012.01821v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01821","description":"<p>Recently, many detection methods based on convolutional neural networks\n(CNNs) have been proposed for image splicing forgery detection. Most of these\ndetection methods focus on the local patches or local objects. In fact, image\nsplicing forgery detection is a global binary classification task that\ndistinguishes the tampered and non-tampered regions by image fingerprints.\nHowever, some specific image contents are hardly retained by CNN-based\ndetection networks, but if included, would improve the detection accuracy of\nthe networks. To resolve these issues, we propose a novel network called\ndual-encoder U-Net (D-Unet) for image splicing forgery detection, which employs\nan unfixed encoder and a fixed encoder. The unfixed encoder autonomously learns\nthe image fingerprints that differentiate between the tampered and non-tampered\nregions, whereas the fixed encoder intentionally provides the direction\ninformation that assists the learning and detection of the network. This\ndual-encoder is followed by a spatial pyramid global-feature extraction module\nthat expands the global insight of D-Unet for classifying the tampered and\nnon-tampered regions more accurately. In an experimental comparison study of\nD-Unet and state-of-the-art methods, D-Unet outperformed the other methods in\nimage-level and pixel-level detection, without requiring pre-training or\ntraining on a large number of forgery images. Moreover, it was stably robust to\ndifferent attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ranglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiuli Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tasting the cake: evaluating self-supervised generalization on out-of-distribution multimodal MRI data. (arXiv:2103.15914v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15914","description":"<p>Self-supervised learning has enabled significant improvements on natural\nimage benchmarks. However, there is less work in the medical imaging domain in\nthis area. The optimal models have not yet been determined among the various\noptions. Moreover, little work has evaluated the current applicability limits\nof novel self-supervised methods. In this paper, we evaluate a range of current\ncontrastive self-supervised methods on out-of-distribution generalization in\norder to evaluate their applicability to medical imaging. We show that\nself-supervised models are not as robust as expected based on their results in\nnatural imaging benchmarks and can be outperformed by supervised learning with\ndropout. We also show that this behavior can be countered with extensive\naugmentation. Our results highlight the need for out-of-distribution\ngeneralization standards and benchmarks to adopt the self-supervised methods in\nthe medical imaging community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1\">Alex Fedorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geenjaar_E/0/1/0/all/0/1\">Eloy Geenjaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeRamus_T/0/1/0/all/0/1\">Thomas P. DeRamus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1\">Vince D. Calhoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1\">Sergey M. Plis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration. (arXiv:2104.01231v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.01231","description":"<p>Deep neural networks achieve high prediction accuracy when the train and test\ndistributions coincide. In practice though, various types of corruptions occur\nwhich deviate from this setup and cause severe performance degradations. Few\nmethods have been proposed to address generalization in the presence of\nunforeseen domain shifts. In particular, digital noise corruptions arise\ncommonly in practice during the image acquisition stage and present a\nsignificant challenge for current robustness approaches. In this paper, we\npropose a diverse Gaussian noise consistency regularization method for\nimproving robustness of image classifiers under a variety of noise corruptions\nwhile still maintaining high clean accuracy. We derive bounds to motivate and\nunderstand the behavior of our Gaussian noise consistency regularization using\na local loss landscape analysis. We show that this simple approach improves\nrobustness against various unforeseen noise corruptions by 4.2-18.4\\% over\nadversarial training and other strong diverse data augmentation baselines\nacross several benchmarks. Furthermore, when combined with state-of-the-art\ndiverse data augmentation techniques, we empirically show our method further\nimproves robustness by 0.6-3\\% and uncertainty calibration by 2.1-10.6\\% for\ncommon corruptions for several image classification benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_A/0/1/0/all/0/1\">Athanasios Tsiligkaridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Achieving Domain Generalization in Underwater Object Detection by Domain Mixup and Contrastive Learning. (arXiv:2104.02230v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02230","description":"<p>The performance of existing underwater object detection methods degrades\nseriously when facing domain shift caused by complicated underwater\nenvironments. Due to the limitation of the number of domains in the dataset,\ndeep detectors easily memorize a few seen domains, which leads to low\ngeneralization ability. There are two common ideas to improve the domain\ngeneralization performance. First, it can be inferred that the detector trained\non as many domains as possible is domain-invariant. Second, for the images with\nthe same semantic content in different domains, their hidden features should be\nequivalent. This paper further excavates these two ideas and proposes a domain\ngeneralization framework (named DMC) that learns how to generalize across\ndomains from Domain Mixup and Contrastive Learning. First, based on the\nformation of underwater images, an image in an underwater environment is the\nlinear transformation of another underwater environment. Thus, a style transfer\nmodel, which outputs a linear transformation matrix instead of the whole image,\nis proposed to transform images from one source domain to another, enriching\nthe domain diversity of the training data. Second, mixup operation interpolates\ndifferent domains on the feature level, sampling new domains on the domain\nmanifold. Third, contrastive loss is selectively applied to features from\ndifferent domains to force the model to learn domain invariant features but\nretain the discriminative capacity. With our method, detectors will be robust\nto domain shift. Also, a domain generalization benchmark S-UODAC2020 for\ndetection is set up to measure the performance of our method. Comprehensive\nexperiments on S-UODAC2020 and two object recognition benchmarks (PACS and\nVLCS) demonstrate that the proposed method is able to learn domain-invariant\nrepresentations, and outperforms other domain generalization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_P/0/1/0/all/0/1\">Pinhao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Linhui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_P/0/1/0/all/0/1\">Peipei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Runwei Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrivateSNN: Privacy-Preserving Spiking Neural Networks. (arXiv:2104.03414v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03414","description":"<p>How can we bring both privacy and energy-efficiency to a neural system? In\nthis paper, we propose PrivateSNN, which aims to build low-power Spiking Neural\nNetworks (SNNs) from a pre-trained ANN model without leaking sensitive\ninformation contained in a dataset. Here, we tackle two types of leakage\nproblems: 1) Data leakage is caused when the networks access real training data\nduring an ANN-SNN conversion process. 2) Class leakage is caused when\nclass-related features can be reconstructed from network parameters. In order\nto address the data leakage issue, we generate synthetic images from the\npre-trained ANNs and convert ANNs to SNNs using the generated images. However,\nconverted SNNs remain vulnerable to class leakage since the weight parameters\nhave the same (or scaled) value with respect to ANN parameters. Therefore, we\nencrypt SNN weights by training SNNs with a temporal spike-based learning rule.\nUpdating weight parameters with temporal data makes SNNs difficult to be\ninterpreted in the spatial domain. We observe that the encrypted PrivateSNN\neliminates data and class leakage issues with a slight performance drop (less\nthan ~2) and significant energy-efficiency gain (about 55x) compared to the\nstandard ANN. We conduct extensive experiments on various datasets including\nCIFAR10, CIFAR100, and TinyImageNet, highlighting the importance of\nprivacy-preserving SNN training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesha_Y/0/1/0/all/0/1\">Yeshwanth Venkatesha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Geometric Consistency for Monocular 3D Object Detection. (arXiv:2104.05858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05858","description":"<p>This paper investigates the geometric consistency for monocular 3D object\ndetection, which suffers from the ill-posed depth estimation. We first conduct\na thorough analysis to reveal how existing methods fail to consistently\nlocalize objects when different geometric shifts occur. In particular, we\ndesign a series of geometric manipulations to diagnose existing detectors and\nthen illustrate their vulnerability to consistently associate the depth with\nobject apparent sizes and positions. To alleviate this issue, we propose four\ngeometry-aware data augmentation approaches to enhance the geometric\nconsistency of the detectors. We first modify some commonly used data\naugmentation methods for 2D images so that they can maintain geometric\nconsistency in 3D spaces. We demonstrate such modifications are important. In\naddition, we propose a 3D-specific image perturbation method that employs the\ncamera movement. During the augmentation process, the camera system with the\ncorresponding image is manipulated, while the geometric visual cues for depth\nrecovery are preserved. We show that by using the geometric consistency\nconstraints, the proposed augmentation techniques lead to improvements on the\nKITTI and nuScenes monocular 3D detection benchmarks with state-of-the-art\nresults. In addition, we demonstrate that the augmentation methods are well\nsuited for semi-supervised training and cross-dataset generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1\">Qing Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1\">Botao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Weilong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOC3-Deep One Class Classification using Contradictions. (arXiv:2105.07636v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.07636","description":"<p>This paper introduces the notion of learning from contradictions (a.k.a\nUniversum learning) for deep one class classification problems. We formalize\nthis notion for the widely adopted one class large-margin loss, and propose the\nDeep One Class Classification using Contradictions (DOC3) algorithm. We show\nthat learning from contradictions incurs lower generalization error by\ncomparing the Empirical Rademacher Complexity (ERC) of DOC3 against its\ntraditional inductive learning counterpart. Our empirical results demonstrate\nthe efficacy of DOC3 compared to popular baseline algorithms on several\nreal-life data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_S/0/1/0/all/0/1\">Sauptik Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_B/0/1/0/all/0/1\">Bernardo Gonzalez Torres</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Vision Transformer. (arXiv:2105.07926v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07926","description":"<p>Recent advances on Vision Transformer (ViT) and its improved variants have\nshown that self-attention-based networks surpass traditional Convolutional\nNeural Networks (CNNs) in most vision tasks. However, existing ViTs focus on\nthe standard accuracy and computation cost, lacking the investigation of the\nintrinsic influence on model robustness and generalization. In this work, we\nconduct systematic evaluation on components of ViTs in terms of their impact on\nrobustness to adversarial examples, common corruptions and distribution shifts.\nWe find some components can be harmful to robustness. By using and combining\nrobust components as building blocks of ViTs, we propose Robust Vision\nTransformer (RVT), which is a new vision transformer and has superior\nperformance with strong robustness. We further propose two new plug-and-play\ntechniques called position-aware attention scaling and patch-wise augmentation\nto augment our RVT, which we abbreviate as RVT*. The experimental results on\nImageNet and six robustness benchmarks show the advanced robustness and\ngeneralization ability of RVT compared with previous ViTs and state-of-the-art\nCNNs. Furthermore, RVT-S* also achieves Top-1 rank on multiple robustness\nleaderboards including ImageNet-C and ImageNet-Sketch. The code will be\navailable at \\url{https://github.com/alibaba/easyrobust}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaofeng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Gege Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1\">Ranjie Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shaokai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White Paper Assistance: A Step Forward Beyond the Shortcut Learning. (arXiv:2106.04178v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04178","description":"<p>The promising performances of CNNs often overshadow the need to examine\nwhether they are doing in the way we are actually interested. We show through\nexperiments that even over-parameterized models would still solve a dataset by\nrecklessly leveraging spurious correlations, or so-called 'shortcuts'. To\ncombat with this unintended propensity, we borrow the idea of printer test page\nand propose a novel approach called White Paper Assistance. Our proposed method\ninvolves the white paper to detect the extent to which the model has preference\nfor certain characterized patterns and alleviates it by forcing the model to\nmake a random guess on the white paper. We show the consistent accuracy\nimprovements that are manifest in various architectures, datasets and\ncombinations with other techniques. Experiments have also demonstrated the\nversatility of our approach on fine-grained recognition, imbalanced\nclassification and robustness to corruptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianshu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiali Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization and Robustness Implications in Object-Centric Learning. (arXiv:2107.00637v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00637","description":"<p>The idea behind object-centric representation learning is that natural scenes\ncan better be modeled as compositions of objects and their relations as opposed\nto distributed representations. This inductive bias can be injected into neural\nnetworks to potentially improve systematic generalization and performance of\ndownstream tasks in scenes with multiple objects. In this paper, we train\nstate-of-the-art unsupervised models on five common multi-object datasets and\nevaluate segmentation metrics and downstream object property prediction. In\naddition, we study generalization and robustness by investigating the settings\nwhere either a single object is out of distribution -- e.g., having an unseen\ncolor, texture, or shape -- or global properties of the scene are altered --\ne.g., by occlusions, cropping, or increasing the number of objects. From our\nexperimental study, we find object-centric representations to be useful for\ndownstream tasks and generally robust to most distribution shifts affecting\nobjects. However, when the distribution shift affects the input in a less\nstructured manner, robustness in terms of segmentation and downstream task\nperformance may vary significantly across models and distribution shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_S/0/1/0/all/0/1\">Samuele Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vita_M/0/1/0/all/0/1\">Michele De Vita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI assisted method for efficiently generating breast ultrasound screening reports. (arXiv:2107.13431v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.13431","description":"<p>Background: Ultrasound is one of the preferred choices for early screening of\ndense breast cancer. Clinically, doctors have to manually write the screening\nreport which is time-consuming and laborious, and it is easy to miss and\nmiswrite. Aim: We proposed a new pipeline to automatically generate AI breast\nultrasound screening reports based on ultrasound images, aiming to assist\ndoctors in improving the efficiency of clinical screening and reducing\nrepetitive report writing. Methods: AI was used to efficiently generate\npersonalized breast ultrasound screening preliminary reports, especially for\nbenign and normal cases which account for the majority. Based on the\npreliminary AI report, doctors then make simple adjustments or corrections to\nquickly generate the final report. The approach has been trained and tested\nusing a database of 4809 breast tumor instances. Results: Experimental results\nindicate that this pipeline improves doctors' work efficiency by up to 90%,\nwhich greatly reduces repetitive work. Conclusion: Personalized report\ngeneration is more widely recognized by doctors in clinical practice compared\nwith non-intelligent reports based on fixed templates or containing options to\nfill in the blanks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shuang Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1\">Qiongyu Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Wenquan Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_D/0/1/0/all/0/1\">Desheng Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huabin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaobo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Distinctive \"Semantics\" in Super-Resolution Networks. (arXiv:2108.00406v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00406","description":"<p>Image super-resolution (SR) is a representative low-level vision problem.\nAlthough deep SR networks have achieved extraordinary success, we are still\nunaware of their working mechanisms. Specifically, whether SR networks can\nlearn semantic information, or just perform complex mapping function? What\nhinders SR networks from generalizing to real-world data? These questions not\nonly raise our curiosity, but also influence SR network development. In this\npaper, we make the primary attempt to answer the above fundamental questions.\nAfter comprehensively analyzing the feature representations (via dimensionality\nreduction and visualization), we successfully discover the distinctive\n\"semantics\" in SR networks, i.e., deep degradation representations (DDR), which\nrelate to image degradation instead of image content. We show that a\nwell-trained deep SR network is naturally a good descriptor of degradation\ninformation. Our experiments also reveal two key factors (adversarial learning\nand global residual) that influence the extraction of such semantics. We\nfurther apply DDR in several interesting applications (such as distortion\nidentification, blind SR and generalization evaluation) and achieve promising\nresults, demonstrating the correctness and effectiveness of our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CERL: A Unified Optimization Framework for Light Enhancement with Realistic Noise. (arXiv:2108.00478v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00478","description":"<p>Low-light images captured in the real world are inevitably corrupted by\nsensor noise. Such noise is spatially variant and highly dependent on the\nunderlying pixel intensity, deviating from the oversimplified assumptions in\nconventional denoising. Existing light enhancement methods either overlook the\nimportant impact of real-world noise during enhancement, or treat noise removal\nas a separate pre- or post-processing step. We present \\underline{C}oordinated\n\\underline{E}nhancement for \\underline{R}eal-world \\underline{L}ow-light Noisy\nImages (CERL), that seamlessly integrates light enhancement and noise\nsuppression parts into a unified and physics-grounded optimization framework.\nFor the real low-light noise removal part, we customize a self-supervised\ndenoising model that can easily be adapted without referring to clean\nground-truth images. For the light enhancement part, we also improve the design\nof a state-of-the-art backbone. The two parts are then joint formulated into\none principled plug-and-play optimization. Our approach is compared against\nstate-of-the-art low-light enhancement methods both qualitatively and\nquantitatively. Besides standard benchmarks, we further collect and test on a\nnew realistic low-light mobile photography dataset (RLMP), whose\nmobile-captured photos display heavier realistic noise than those taken by\nhigh-quality cameras. CERL consistently produces the most visually pleasing and\nartifact-free results across all experiments. Our RLMP dataset and codes are\navailable at: https://github.com/VITA-Group/CERL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Branch with Attention Network for Hand-Based Person Recognition. (arXiv:2108.02234v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02234","description":"<p>In this paper, we propose a novel hand-based person recognition method for\nthe purpose of criminal investigations since the hand image is often the only\navailable information in cases of serious crime such as sexual abuse. Our\nproposed method, Multi-Branch with Attention Network (MBA-Net), incorporates\nboth channel and spatial attention modules in branches in addition to a global\n(without attention) branch to capture global structural information for\ndiscriminative feature learning. The attention modules focus on the relevant\nfeatures of the hand image while suppressing the irrelevant backgrounds. In\norder to overcome the weakness of the attention mechanisms, equivariant to\npixel shuffling, we integrate relative positional encodings into the spatial\nattention module to capture the spatial positions of pixels. Extensive\nevaluations on two large multi-ethnic and publicly available hand datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nsurpassing the existing hand-based identification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eyes Tell All: Irregular Pupil Shapes Reveal GAN-generated Faces. (arXiv:2109.00162v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00162","description":"<p>Generative adversary network (GAN) generated high-realistic human faces have\nbeen used as profile images for fake social media accounts and are visually\nchallenging to discern from real ones. In this work, we show that GAN-generated\nfaces can be exposed via irregular pupil shapes. This phenomenon is caused by\nthe lack of physiological constraints in the GAN models. We demonstrate that\nsuch artifacts exist widely in high-quality GAN-generated faces and further\ndescribe an automatic method to extract the pupils from two eyes and analysis\ntheir shapes for exposing the GAN-generated faces. Qualitative and quantitative\nevaluations of our method suggest its simplicity and effectiveness in\ndistinguishing GAN-generated faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IFBiD: Inference-Free Bias Detection. (arXiv:2109.04374v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04374","description":"<p>This paper is the first to explore an automatic way to detect bias in deep\nconvolutional neural networks by simply looking at their weights. Furthermore,\nit is also a step towards understanding neural networks and how they work. We\nshow that it is indeed possible to know if a model is biased or not simply by\nlooking at its weights, without the model inference for an specific input. We\nanalyze how bias is encoded in the weights of deep networks through a toy\nexample using the Colored MNIST database and we also provide a realistic case\nstudy in gender detection from face images using state-of-the-art methods and\nexperimental resources. To do so, we generated two databases with 36K and 48K\nbiased models each. In the MNIST models we were able to detect whether they\npresented a strong or low bias with more than 99% accuracy, and we were also\nable to classify between four levels of bias with more than 70% accuracy. For\nthe face models, we achieved 90% accuracy in distinguishing between models\nbiased towards Asian, Black, or Caucasian ethnicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1\">Ignacio Serna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeAlcala_D/0/1/0/all/0/1\">Daniel DeAlcala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning is More Robust to Dataset Imbalance. (arXiv:2110.05025v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.05025","description":"<p>Self-supervised learning (SSL) is a scalable way to learn general visual\nrepresentations since it learns without labels. However, large-scale unlabeled\ndatasets in the wild often have long-tailed label distributions, where we know\nlittle about the behavior of SSL. In this work, we systematically investigate\nself-supervised learning under dataset imbalance. First, we find out via\nextensive experiments that off-the-shelf self-supervised representations are\nalready more robust to class imbalance than supervised representations. The\nperformance gap between balanced and imbalanced pre-training with SSL is\nsignificantly smaller than the gap with supervised learning, across sample\nsizes, for both in-domain and, especially, out-of-domain evaluation. Second,\ntowards understanding the robustness of SSL, we hypothesize that SSL learns\nricher features from frequent data: it may learn\nlabel-irrelevant-but-transferable features that help classify the rare classes\nand downstream tasks. In contrast, supervised learning has no incentive to\nlearn features irrelevant to the labels from frequent examples. We validate\nthis hypothesis with semi-synthetic experiments and theoretical analyses on a\nsimplified setting. Third, inspired by the theoretical insights, we devise a\nre-weighted regularization technique that consistently improves the SSL\nrepresentation quality on imbalanced datasets with several evaluation criteria,\nclosing the small gap between balanced and imbalanced datasets with the same\nnumber of examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composition and Style Attributes Guided Image Aesthetic Assessment. (arXiv:2111.04647v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.04647","description":"<p>The aesthetic quality of an image is defined as the measure or appreciation\nof the beauty of an image. Aesthetics is inherently a subjective property but\nthere are certain factors that influence it such as, the semantic content of\nthe image, the attributes describing the artistic aspect, the photographic\nsetup used for the shot, etc. In this paper we propose a method for the\nautomatic prediction of the aesthetics of an image that is based on the\nanalysis of the semantic content, the artistic style and the composition of the\nimage. The proposed network includes: a pre-trained network for semantic\nfeatures extraction (the Backbone); a Multi Layer Perceptron (MLP) network that\nrelies on the Backbone features for the prediction of image attributes (the\nAttributeNet); a self-adaptive Hypernetwork that exploits the attributes prior\nencoded into the embedding generated by the AttributeNet to predict the\nparameters of the target network dedicated to aesthetic estimation (the\nAestheticNet). Given an image, the proposed multi-network is able to predict:\nstyle and composition attributes, and aesthetic score distribution. Results on\nthree benchmark datasets demonstrate the effectiveness of the proposed method,\nwhile the ablation study gives a better understanding of the proposed network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Celona_L/0/1/0/all/0/1\">Luigi Celona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardi_M/0/1/0/all/0/1\">Marco Leonardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napoletano_P/0/1/0/all/0/1\">Paolo Napoletano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozza_A/0/1/0/all/0/1\">Alessandro Rozza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer. (arXiv:2111.12082v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12082","description":"<p>Remote photoplethysmography (rPPG), which aims at measuring heart activities\nand physiological signals from facial video without any contact, has great\npotential in many applications (e.g., remote healthcare and affective\ncomputing). Recent deep learning approaches focus on mining subtle rPPG clues\nusing convolutional neural networks with limited spatio-temporal receptive\nfields, which neglect the long-range spatio-temporal perception and interaction\nfor rPPG modeling. In this paper, we propose the PhysFormer, an end-to-end\nvideo transformer based architecture, to adaptively aggregate both local and\nglobal spatio-temporal features for rPPG representation enhancement. As key\nmodules in PhysFormer, the temporal difference transformers first enhance the\nquasi-periodic rPPG features with temporal difference guided global attention,\nand then refine the local spatio-temporal representation against interference.\nFurthermore, we also propose the label distribution learning and a curriculum\nlearning inspired dynamic constraint in frequency domain, which provide\nelaborate supervisions for PhysFormer and alleviate overfitting. Comprehensive\nexperiments are performed on four benchmark datasets to show our superior\nperformance on both intra- and cross-dataset testings. One highlight is that,\nunlike most transformer networks needed pretraining from large-scale datasets,\nthe proposed PhysFormer can be easily trained from scratch on rPPG datasets,\nwhich makes it promising as a novel transformer baseline for the rPPG\ncommunity. The codes will be released at\nhttps://github.com/ZitongYu/PhysFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jingang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating the Bias of Centered Objects in Common Datasets. (arXiv:2112.09195v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09195","description":"<p>Convolutional networks are considered shift invariant, but it was\ndemonstrated that their response may vary according to the exact location of\nthe objects. In this paper we will demonstrate that most commonly investigated\ndatasets have a bias, where objects are over-represented at the center of the\nimage during training. This bias and the boundary condition of these networks\ncan have a significant effect on the performance of these architectures and\ntheir accuracy drops significantly as an object approaches the boundary. We\nwill also demonstrate how this effect can be mitigated with data augmentation\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1\">Gergely Szabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_A/0/1/0/all/0/1\">Andras Horvath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial-Sketch Synthesis: A New Challenge. (arXiv:2112.15439v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15439","description":"<p>This paper aims to conduct a comprehensive study on facial-sketch synthesis\n(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,\nthere lacks a complete benchmark for assessing the development of FSS\nalgorithms over the last decade. We first introduce a high-quality dataset for\nFSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three\ntypes of sketch styles, image backgrounds, lighting conditions, skin colors,\nand facial attributes. FS2K differs from previous FSS datasets in difficulty,\ndiversity, and scalability and should thus facilitate the progress of FSS\nresearch. Second, we present the largest-scale FSS investigation by reviewing\n89 classical methods, including 25 handcrafted feature-based facial-sketch\nsynthesis approaches, 29 general translation methods, and 35 image-to-sketch\napproaches. Besides, we elaborate comprehensive experiments on the existing 19\ncutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.\nWith only two straightforward components, i.e., facial-aware masking and\nstyle-vector expansion, FSGAN surpasses the performance of all previous\nstate-of-the-art models on the proposed FS2K dataset by a large margin.\nFinally, we conclude with lessons learned over the past years and point out\nseveral unsolved challenges. Our code is available at\nhttps://github.com/DengPingFan/FSGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs. (arXiv:2201.06374v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06374","description":"<p>Blind face restoration is to recover a high-quality face image from unknown\ndegradations. As face image contains abundant contextual information, we\npropose a method, RestoreFormer, which explores fully-spatial attentions to\nmodel contextual information and surpasses existing works that use local\noperators. RestoreFormer has several benefits compared to prior arts. First,\nunlike the conventional multi-head self-attention in previous Vision\nTransformers (ViTs), RestoreFormer incorporates a multi-head cross-attention\nlayer to learn fully-spatial interactions between corrupted queries and\nhigh-quality key-value pairs. Second, the key-value pairs in ResotreFormer are\nsampled from a reconstruction-oriented high-quality dictionary, whose elements\nare rich in high-quality facial features specifically aimed for face\nreconstruction, leading to superior restoration results. Third, RestoreFormer\noutperforms advanced state-of-the-art methods on one synthetic dataset and\nthree real-world datasets, as well as produces images with better visual\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhouxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShapeFormer: Transformer-based Shape Completion via Sparse Representation. (arXiv:2201.10326v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10326","description":"<p>We present ShapeFormer, a transformer-based network that produces a\ndistribution of object completions, conditioned on incomplete, and possibly\nnoisy, point clouds. The resultant distribution can then be sampled to generate\nlikely completions, each exhibiting plausible shape details while being\nfaithful to the input. To facilitate the use of transformers for 3D, we\nintroduce a compact 3D representation, vector quantized deep implicit function,\nthat utilizes spatial sparsity to represent a close approximation of a 3D shape\nby a short sequence of discrete variables. Experiments demonstrate that\nShapeFormer outperforms prior art for shape completion from ambiguous partial\ninputs in terms of both completion quality and diversity. We also show that our\napproach effectively handles a variety of shape types, incomplete patterns, and\nreal-world scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xingguang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liqiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1\">Dani Lischinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.10890","description":"<p>Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is hard to implement, easy to overfit, and\nnot hardware-friendly. In this work, inspired by human education model, we\npropose a novel task, knowledge integration, to obtain a dense student model\n(OneS) as knowledgeable as one sparse MoE. We investigate this task by\nproposing a general training framework including knowledge gathering and\nknowledge distillation. Specifically, we first propose Singular Value\nDecomposition Knowledge Gathering (SVD-KG) to gather key knowledge from\ndifferent pretrained experts. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE. OneS can achieve $78.4\\%$ top-1\naccuracy with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms SoTA by $51.7\\%$\nusing the same architecture and training data. In addition, compared with the\nMoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to the\nhardware-friendly architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HCSC: Hierarchical Contrastive Selective Coding. (arXiv:2202.00455v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00455","description":"<p>Hierarchical semantic structures naturally exist in an image dataset, in\nwhich several semantically relevant image clusters can be further integrated\ninto a larger cluster with coarser-grained semantics. Capturing such structures\nwith image representations can greatly benefit the semantic understanding on\nvarious downstream tasks. Existing contrastive representation learning methods\nlack such an important model capability. In addition, the negative pairs used\nin these methods are not guaranteed to be semantically distinct, which could\nfurther hamper the structural correctness of learned image representations. To\ntackle these limitations, we propose a novel contrastive learning framework\ncalled Hierarchical Contrastive Selective Coding (HCSC). In this framework, a\nset of hierarchical prototypes are constructed and also dynamically updated to\nrepresent the hierarchical semantic structures underlying the data in the\nlatent space. To make image representations better fit such semantic\nstructures, we employ and further improve conventional instance-wise and\nprototypical contrastive learning via an elaborate pair selection scheme. This\nscheme seeks to select more diverse positive pairs with similar semantics and\nmore precise negative pairs with truly distinct semantics. On extensive\ndownstream tasks, we verify the superior performance of HCSC over\nstate-of-the-art contrastive methods, and the effectiveness of major model\ncomponents is proved by plentiful analytical studies. We build a comprehensive\nmodel zoo in Sec. D. Our source code and model weights are available at\nhttps://github.com/gyfastas/HCSC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xuanyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenbang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks. (arXiv:2202.05306v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.05306","description":"<p>We hypothesize that due to the greedy nature of learning in multi-modal deep\nneural networks, these models tend to rely on just one modality while\nunder-fitting the other modalities. Such behavior is counter-intuitive and\nhurts the models' generalization, as we observe empirically. To estimate the\nmodel's dependence on each modality, we compute the gain on the accuracy when\nthe model has access to it in addition to another modality. We refer to this\ngain as the conditional utilization rate. In the experiments, we consistently\nobserve an imbalance in conditional utilization rates between modalities,\nacross multiple tasks and architectures. Since conditional utilization rate\ncannot be computed efficiently during training, we introduce a proxy for it\nbased on the pace at which the model learns from each modality, which we refer\nto as the conditional learning speed. We propose an algorithm to balance the\nconditional learning speeds between modalities during training and demonstrate\nthat it indeed addresses the issue of greedy learning. The proposed algorithm\nimproves the model's generalization on three datasets: Colored MNIST,\nModelNet40, and NVIDIA Dynamic Hand Gesture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jastrzebski_S/0/1/0/all/0/1\">Stanis&#x142;aw Jastrz&#x119;bski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroupViT: Semantic Segmentation Emerges from Text Supervision. (arXiv:2202.11094v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11094","description":"<p>Grouping and recognition are important components of visual scene\nunderstanding, e.g., for object detection and semantic segmentation. With\nend-to-end deep learning systems, grouping of image regions usually happens\nimplicitly via top-down supervision from pixel-level recognition labels.\nInstead, in this paper, we propose to bring back the grouping mechanism into\ndeep networks, which allows semantic segments to emerge automatically with only\ntext supervision. We propose a hierarchical Grouping Vision Transformer\n(GroupViT), which goes beyond the regular grid structure representation and\nlearns to group image regions into progressively larger arbitrary-shaped\nsegments. We train GroupViT jointly with a text encoder on a large-scale\nimage-text dataset via contrastive losses. With only text supervision and\nwithout any pixel-level annotations, GroupViT learns to group together semantic\nregions and successfully transfers to the task of semantic segmentation in a\nzero-shot manner, i.e., without any further fine-tuning. It achieves a\nzero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on\nPASCAL Context datasets, and performs competitively to state-of-the-art\ntransfer-learning methods requiring greater levels of supervision. We\nopen-source our code at https://github.com/NVlabs/GroupViT .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Transformer Meets Robotic Grasping: Exploits Context for Efficient Grasp Detection. (arXiv:2202.11911v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.11911","description":"<p>In this paper, we present a transformer-based architecture, namely TF-Grasp,\nfor robotic grasp detection. The developed TF-Grasp framework has two elaborate\ndesigns making it well suitable for visual grasping tasks. The first key design\nis that we adopt the local window attention to capture local contextual\ninformation and detailed features of graspable objects. Then, we apply the\ncross window attention to model the long-term dependencies between distant\npixels. Object knowledge, environmental configuration, and relationships\nbetween different visual entities are aggregated for subsequent grasp\ndetection. The second key design is that we build a hierarchical\nencoder-decoder architecture with skip-connections, delivering shallow features\nfrom encoder to decoder to enable a multi-scale feature fusion. Due to the\npowerful attention mechanism, the TF-Grasp can simultaneously obtain the local\ninformation (i.e., the contours of objects), and model long-term connections\nsuch as the relationships between distinct visual concepts in clutter.\nExtensive computational experiments demonstrate that the TF-Grasp achieves\nsuperior results versus state-of-art grasping convolutional models and attain a\nhigher accuracy of 97.99% and 94.6% on Cornell and Jacquard grasping datasets,\nrespectively. Real-world experiments using a 7DoF Franka Emika Panda robot also\ndemonstrate its capability of grasping unseen objects in a variety of\nscenarios. The code and pre-trained models will be available at\nhttps://github.com/WangShaoSUN/grasp-transformer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhangli Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1\">Zhen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-scalable Transformer for Medical Image Segmentation: Architecture, Model Efficiency, and Benchmark. (arXiv:2203.00131v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.00131","description":"<p>Transformer, as a new generation of neural architecture, has demonstrated\nremarkable performance in natural language processing and computer vision.\nHowever, existing vision Transformers struggle to learn with limited medical\ndata and are unable to generalize on diverse medical image tasks. To tackle\nthese challenges, we present UTNetV2 as a data-scalable Transformer towards\ngeneralizable medical image segmentation. The key designs incorporate desirable\ninductive bias, hierarchical modeling with linear-complexity attention, and\nmulti-scale feature fusion in a spatially and semantically global manner.\nUTNetV2 can learn across tiny- to large-scale data without pre-training.\nExtensive experiments demonstrate the potential of UTNetV2 as a general\nsegmentation backbone, outperforming CNNs and vision Transformers on three\npublic datasets with multiple modalities (e.g., CT and MRI) and diverse medical\ntargets (e.g., healthy organ, diseased tissue, and tumor). We make the data\nprocessing, models and evaluation pipeline publicly available, offering solid\nbaselines and unbiased comparisons for promoting a wide range of downstream\nclinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zhennan Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving. (arXiv:2203.05056v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05056","description":"<p>Surround-view cameras are a primary sensor for automated driving, used for\nnear field perception. It is one of the most commonly used sensors in\ncommercial vehicles. Four fisheye cameras with a 190{\\deg} field of view cover\nthe 360{\\deg} around the vehicle. Due to its high radial distortion, the\nstandard algorithms do not extend easily. Previously, we released the first\npublic fisheye surround-view dataset named WoodScape. In this work, we release\na synthetic version of the surround-view dataset, covering many of its\nweaknesses and extending it. Firstly, it is not possible to obtain ground truth\nfor pixel-wise optical flow and depth. Secondly, WoodScape did not have all\nfour cameras simultaneously in order to sample diverse frames. However, this\nmeans that multi-camera algorithms cannot be designed, which is enabled in the\nnew dataset. We implemented surround-view fisheye geometric projections in\nCARLA Simulator matching WoodScape's configuration and created SynWoodScape. We\nrelease 80k images from the synthetic dataset with annotations for 10+ tasks.\nWe also release the baseline code and supporting scripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sekkat_A/0/1/0/all/0/1\">Ahmed Rida Sekkat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuis_Y/0/1/0/all/0/1\">Yohan Dupuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasseur_P/0/1/0/all/0/1\">Pascal Vasseur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honeine_P/0/1/0/all/0/1\">Paul Honeine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-generative Generalized Zero-shot Learning via Task-correlated Disentanglement and Controllable Samples Synthesis. (arXiv:2203.05335v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05335","description":"<p>Synthesizing pseudo samples is currently the most effective way to solve the\nGeneralized Zero-Shot Learning (GZSL) problem. Most models achieve competitive\nperformance but still suffer from two problems: (1) Feature confounding, the\noverall representations confound task-correlated and task-independent features,\nand existing models disentangle them in a generative way, but they are\nunreasonable to synthesize reliable pseudo samples with limited samples; (2)\nDistribution uncertainty, that massive data is needed when existing models\nsynthesize samples from the uncertain distribution, which causes poor\nperformance in limited samples of seen classes. In this paper, we propose a\nnon-generative model to address these problems correspondingly in two modules:\n(1) Task-correlated feature disentanglement, to exclude the task-correlated\nfeatures from task-independent ones by adversarial learning of domain adaption\ntowards reasonable synthesis; (2) Controllable pseudo sample synthesis, to\nsynthesize edge-pseudo and center-pseudo samples with certain characteristics\ntowards more diversity generated and intuitive transfer. In addation, to\ndescribe the new scene that is the limit seen class samples in the training\nprocess, we further formulate a new ZSL task named the 'Few-shot Seen class and\nZero-shot Unseen class learning' (FSZU). Extensive experiments on four\nbenchmarks verify that the proposed method is competitive in the GZSL and the\nFSZU tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yaogong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Pengbo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding. (arXiv:2203.08013v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08013","description":"<p>Natural language spatial video grounding aims to detect the relevant objects\nin video frames with descriptive sentences as the query. In spite of the great\nadvances, most existing methods rely on dense video frame annotations, which\nrequire a tremendous amount of human effort. To achieve effective grounding\nunder a limited annotation budget, we investigate one-shot video grounding, and\nlearn to ground natural language in all video frames with solely one frame\nlabeled, in an end-to-end manner. One major challenge of end-to-end one-shot\nvideo grounding is the existence of videos frames that are either irrelevant to\nthe language query or the labeled frames. Another challenge relates to the\nlimited supervision, which might result in ineffective representation learning.\nTo address these challenges, we designed an end-to-end model via Information\nTree for One-Shot video grounding (IT-OS). Its key module, the information\ntree, can eliminate the interference of irrelevant frames based on branch\nsearch and branch cropping techniques. In addition, several self-supervised\ntasks are proposed based on the information tree to improve the representation\nlearning under insufficient labeling. Experiments on the benchmark dataset\ndemonstrate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jiaxu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion. (arXiv:2203.09855v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09855","description":"<p>In this paper, we formulate a potentially valuable panoramic depth completion\n(PDC) task as panoramic 3D cameras often produce 360{\\deg} depth with missing\ndata in complex scenes. Its goal is to recover dense panoramic depths from raw\nsparse ones and panoramic RGB images. To deal with the PDC task, we train a\ndeep network that takes both depth and image as inputs for the dense panoramic\ndepth recovery. However, it needs to face a challenging optimization problem of\nthe network parameters due to its non-convex objective function. To address\nthis problem, we propose a simple yet effective approach termed M{^3}PT:\nmulti-modal masked pre-training. Specifically, during pre-training, we\nsimultaneously cover up patches of the panoramic RGB image and sparse depth by\nshared random mask, then reconstruct the sparse depth in the masked regions. To\nour best knowledge, it is the first time that we show the effectiveness of\nmasked pre-training in a multi-modal vision task, instead of the single-modal\ntask resolved by masked autoencoders (MAE). Different from MAE where\nfine-tuning completely discards the decoder part of pre-training, there is no\narchitectural difference between the pre-training and fine-tuning stages in our\nM$^{3}$PT as they only differ in the prediction density, which potentially\nmakes the transfer learning more convenient and effective. Extensive\nexperiments verify the effectiveness of M{^3}PT on three panoramic datasets.\nNotably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,\n51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.\nCodes and pre-trained models are available at\nhttps://github.com/anonymoustbd/MMMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Representation Learning as Multimodal Variational Inference. (arXiv:2203.11437v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11437","description":"<p>In this paper, we propose a probabilistic extension of the recent\nself-supervised learning (SSL) method, SimSiam. The proposed extension makes\nSimSiam uncertainty-aware by considering SimSiam as a generative model of\naugmented views and learning it in terms of variational inference. SimSiam\ntrains a model by maximizing the similarity between image representations of\ndifferent augmented views of the same image. The augmentation process sometimes\nproduces ambiguous images, and their representations potentially have\nuncertainty. Although the use of uncertainty-aware machine learning becoming\ncommon, such as in deep variational inference, SimSiam and other SSL methods\nare insufficiently uncertainty-aware, leading to limitations in the use of\naugmented ambiguous images. Our main contributions are twofold: Firstly, we\nclarify the theoretical relationship between non-contrastive SSL and multimodal\nvariational inference. Secondly, we introduce a novel SSL called variational\ninference SimSiam (VI-SimSiam), which incorporates uncertainty by involving\nspherical posterior distributions. The experiment results show that VI-SimSiam\noutperforms SimSiam in classification tasks in several datasets, such as\nImageNette and ImageWoof by successfully estimating the representation\nuncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_H/0/1/0/all/0/1\">Hiroki Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okada_M/0/1/0/all/0/1\">Masashi Okada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Fixed Sub-Center: A Better Way to Capture Data Complexity. (arXiv:2203.12928v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12928","description":"<p>Treating class with a single center may hardly capture data distribution\ncomplexities. Using multiple sub-centers is an alternative way to address this\nproblem. However, highly correlated sub-classes, the classifier's parameters\ngrow linearly with the number of classes, and lack of intra-class compactness\nare three typical issues that need to be addressed in existing multi-subclass\nmethods. To this end, we propose to use Fixed Sub-Center (F-SC), which allows\nthe model to create more discrepant sub-centers while saving memory and cutting\ncomputational costs considerably. The F-SC specifically, first samples a class\ncenter Ui for each class from a uniform distribution, and then generates a\nnormal distribution for each class, where the mean is equal to Ui. Finally, the\nsub-centers are sampled based on the normal distribution corresponding to each\nclass, and the sub-centers are fixed during the training process avoiding the\noverhead of gradient calculation. Moreover, F-SC penalizes the Euclidean\ndistance between the samples and their corresponding sub-centers, it helps\nremain intra-compactness. The experimental results show that F-SC significantly\nimproves the accuracy of both image classification and fine-grained recognition\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhemin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-distillation Augmented Masked Autoencoders for Histopathological Image Classification. (arXiv:2203.16983v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16983","description":"<p>Self-supervised learning (SSL) has drawn increasing attention in pathological\nimage analysis in recent years. However, the prevalent contrastive SSL is\nsuboptimal in feature representation under this scenario due to the homogeneous\nvisual appearance. Alternatively, masked autoencoders (MAE) build SSL from a\ngenerative paradigm. In this paper, we introduce MAE to pathological image\nanalysis and verify the effect of visible patches. Based on it, a novel SD-MAE\nmodel is proposed to enable a self-distillation augmented SSL on top of the raw\nMAE. Besides the reconstruction loss on masked image patches, SD-MAE further\nimposes the self-distillation loss on visible patches. It enhances the\nattention of the encoder, as shown by focusing more on fewer parts in\nvisualization results of attention map. We apply SD-MAE to the image\nclassification task on two pathological and one natural image datasets.\nExperiments demonstrate that SD-MAE performs highly competitive when compared\nwith leading contrastive SSL methods. The results, which are pre-trained using\na moderate size of pathological images, are also comparable to the method\npre-trained with two orders of magnitude more images. Our code will be released\nsoon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhineng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xieping Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDA GAN: Adversarial-Learning-based 3-D Seismic Data Interpolation and Reconstruction for Complex Missing. (arXiv:2204.03197v3 [physics.geo-ph] UPDATED)","link":"http://arxiv.org/abs/2204.03197","description":"<p>The interpolation and reconstruction of missing traces is a crucial step in\nseismic data processing, moreover it is also a highly ill-posed problem,\nespecially for complex cases such as high-ratio random discrete missing,\ncontinuous missing and missing in fault-rich or salt body surveys. These\ncomplex cases are rarely mentioned in current works. To cope with complex\nmissing cases, we propose Multi-Dimensional Adversarial GAN (MDA GAN), a novel\n3-D GAN framework. It employs three discriminators to ensure the consistency of\nthe reconstructed data with the original data distribution in each dimension.\nThe feature splicing module (FSM) is designed and embedded into the generator\nof this framework, which automatically splices the features of the unmissing\npart with those of the reconstructed part (missing part), thus fully preserving\nthe information of the unmissing part. To prevent pixel distortion in the\nseismic data caused by the adversarial learning process, we propose a new\nreconstruction loss Tanh Cross Entropy (TCE) loss to provide smoother\ngradients. We experimentally verified the effectiveness of the individual\ncomponents of the study and then tested the method on multiple publicly\navailable data. The method achieves reasonable reconstructions for up to 95% of\nrandom discrete missing, 100 traces of continuous missing and a mixture of both\ntypes with a cumulative 99.4% missing. In fault and salt body enriched surveys,\nMDA GAN still yields promising results for complex cases. Experimentally it has\nbeen demonstrated that our method achieves better performance than other\nmethods in both simple and complex cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Dou_Y/0/1/0/all/0/1\">Yimin Dou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Duan_H/0/1/0/all/0/1\">Hongjie Duan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_T/0/1/0/all/0/1\">Timing Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dong_L/0/1/0/all/0/1\">Lin Dong</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Huang_Z/0/1/0/all/0/1\">Zongchao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Prototype Prompt-tuning with Pre-trained Representation for Class Incremental Learning. (arXiv:2204.03410v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03410","description":"<p>Class incremental learning has attracted much attention, but most existing\nrelated works focus on fine-tuning the entire representation model, which\ninevitably results in much catastrophic forgetting. Instead of struggling to\nfight against such forgetting by replaying or distilling like most of the\nexisting methods, we take a novel pre-train-and-prompt-tuning paradigm to\nsequentially learn new visual concepts based on a fixed semantic-rich\npre-trained representation model. In detail, we incrementally prompt-tune\ncategory prototypes for classification and example prototypes to compensate for\nsemantic drift, the problem caused by learning bias at different learning\nphases. Extensive experiments conducted on the mainstream incremental learning\nbenchmarks demonstrate that our method outperforms other state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jieren Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianhua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haojian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunkuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glass Segmentation with RGB-Thermal Image Pairs. (arXiv:2204.05453v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05453","description":"<p>This paper proposes a new glass segmentation method utilizing paired RGB and\nthermal images. Due to the large difference between the transmission property\nof visible light and that of the thermal energy through the glass where most\nglass is transparent to the visible light but opaque to thermal energy, glass\nregions of a scene are made more distinguishable with a pair of RGB and thermal\nimages than solely with an RGB image. To exploit such a unique property, we\npropose a neural network architecture that effectively combines an RGB-thermal\nimage pair with a new multi-modal fusion module based on attention, and\nintegrate CNN and transformer to extract local features and long-range\ndependencies, respectively. As well, we have collected a new dataset containing\n5551 RGB-thermal image pairs with ground-truth segmentation annotations. The\nqualitative and quantitative evaluations demonstrate the effectiveness of the\nproposed approach on fusing RGB and thermal data for glass segmentation. Our\ncode and data are available at\nhttps://github.com/Dong-Huo/RGB-T-Glass-Segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Dong Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yee-Hong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Performance Limit of Scene Text Recognizer without Human Annotation. (arXiv:2204.07714v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07714","description":"<p>Scene text recognition (STR) attracts much attention over the years because\nof its wide application. Most methods train STR model in a fully supervised\nmanner which requires large amounts of labeled data. Although synthetic data\ncontributes a lot to STR, it suffers from the real-tosynthetic domain gap that\nrestricts model performance. In this work, we aim to boost STR models by\nleveraging both synthetic data and the numerous real unlabeled images,\nexempting human annotation cost thoroughly. A robust consistency regularization\nbased semi-supervised framework is proposed for STR, which can effectively\nsolve the instability issue due to domain inconsistency between synthetic and\nreal images. A character-level consistency regularization is designed to\nmitigate the misalignment between characters in sequence recognition. Extensive\nexperiments on standard text recognition benchmarks demonstrate the\neffectiveness of the proposed method. It can steadily improve existing STR\nmodels, and boost an STR model to achieve new state-of-the-art results. To our\nbest knowledge, this is the first consistency regularization based framework\nthat applies successfully to STR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Caiyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_S/0/1/0/all/0/1\">Seon-Min Rhee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jae-Joon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where and What: Driver Attention-based Object Detection. (arXiv:2204.12150v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12150","description":"<p>Human drivers use their attentional mechanisms to focus on critical objects\nand make decisions while driving. As human attention can be revealed from gaze\ndata, capturing and analyzing gaze information has emerged in recent years to\nbenefit autonomous driving technology. Previous works in this context have\nprimarily aimed at predicting \"where\" human drivers look at and lack knowledge\nof \"what\" objects drivers focus on. Our work bridges the gap between\npixel-level and object-level attention prediction. Specifically, we propose to\nintegrate an attention prediction module into a pretrained object detection\nframework and predict the attention in a grid-based style. Furthermore,\ncritical objects are recognized based on predicted attended-to areas. We\nevaluate our proposed method on two driver attention datasets, BDD-A and\nDR(eye)VE. Our framework achieves competitive state-of-the-art performance in\nthe attention prediction on both pixel-level and object-level but is far more\nefficient (75.3 GFLOPs less) in computation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yao Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kassautzki_N/0/1/0/all/0/1\">Naemi-Rebecca Kassautzki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1\">Wolfgang Fuhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Image Captioning. (arXiv:2204.13324v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13324","description":"<p>State-of-the-art image captioners can generate accurate sentences to describe\nimages in a sequence to sequence manner without considering the controllability\nand interpretability. This, however, is far from making image captioning widely\nused as an image can be interpreted in infinite ways depending on the target\nand the context at hand. Achieving controllability is important especially when\nthe image captioner is used by different people with different way of\ninterpreting the images. In this paper, we introduce a novel framework for\nimage captioning which can generate diverse descriptions by capturing the\nco-dependence between Part-Of-Speech tags and semantics. Our model decouples\ndirect dependence between successive variables. In this way, it allows the\ndecoder to exhaustively search through the latent Part-Of-Speech choices, while\nkeeping decoding speed proportional to the size of the POS vocabulary. Given a\ncontrol signal in the form of a sequence of Part-Of-Speech tags, we propose a\nmethod to generate captions through a Transformer network, which predicts words\nbased on the input Part-Of-Speech tag sequences. Experiments on publicly\navailable datasets show that our model significantly outperforms\nstate-of-the-art methods on generating diverse image captions with high\nqualities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maxwell_L/0/1/0/all/0/1\">Luka Maxwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVTR: Scene Text Recognition with a Single Visual Model. (arXiv:2205.00159v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00159","description":"<p>Dominant scene text recognition models commonly contain two building blocks,\na visual model for feature extraction and a sequence model for text\ntranscription. This hybrid architecture, although accurate, is complex and less\nefficient. In this study, we propose a Single Visual model for Scene Text\nrecognition within the patch-wise image tokenization framework, which dispenses\nwith the sequential modeling entirely. The method, termed SVTR, firstly\ndecomposes an image text into small patches named character components.\nAfterward, hierarchical stages are recurrently carried out by component-level\nmixing, merging and/or combining. Global and local mixing blocks are devised to\nperceive the inter-character and intra-character patterns, leading to a\nmulti-grained character component perception. Thus, characters are recognized\nby a simple linear prediction. Experimental results on both English and Chinese\nscene text recognition tasks demonstrate the effectiveness of SVTR. SVTR-L\n(Large) achieves highly competitive accuracy in English and outperforms\nexisting methods by a large margin in Chinese, while running faster. In\naddition, SVTR-T (Tiny) is an effective and much smaller model, which shows\nappealing speed at inference. The code is publicly available at\nhttps://github.com/PaddlePaddle/PaddleOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yongkun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhineng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Caiyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiaoting Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianlun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network. (arXiv:2205.01550v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01550","description":"<p>In recent years, with the development of computing resources and LiDAR, point\ncloud semantic segmentation has attracted many researchers. For the sparsity of\npoint clouds, although there is already a way to deal with sparse convolution,\nmulti-scale features are not considered. In this letter, we propose a feature\nextraction module based on multi-scale sparse convolution and a feature\nselection module based on channel attention and build a point cloud\nsegmentation network framework based on this. By introducing multi-scale sparse\nconvolution, the network could capture richer feature information based on\nconvolution kernels with different sizes, improving the segmentation result of\npoint cloud segmentation. Experimental results on Stanford large-scale 3-D\nIndoor Spaces(S3DIS) dataset and outdoor dataset(SemanticKITTI), demonstrate\neffectiveness and superiority of the proposed mothod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yunzheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lei Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simpler is Better: off-the-shelf Continual Learning Through Pretrained Backbones. (arXiv:2205.01586v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01586","description":"<p>In this short paper, we propose a baseline (off-the-shelf) for Continual\nLearning of Computer Vision problems, by leveraging the power of pretrained\nmodels. By doing so, we devise a simple approach achieving strong performance\nfor most of the common benchmarks. Our approach is fast since requires no\nparameters updates and has minimal memory requirements (order of KBytes). In\nparticular, the \"training\" phase reorders data and exploit the power of\npretrained models to compute a class prototype and fill a memory bank. At\ninference time we match the closest prototype through a knn-like approach,\nproviding us the prediction. We will see how this naive solution can act as an\noff-the-shelf continual learning system. In order to better consolidate our\nresults, we compare the devised pipeline with common CNN models and show the\nsuperiority of Vision Transformers, suggesting that such architectures have the\nability to produce features of higher quality. Moreover, this simple pipeline,\nraises the same questions raised by previous works \\cite{gdumb} on the\neffective progresses made by the CL community especially in the dataset\nconsidered and the usage of pretrained models. Code is live at\nhttps://github.com/francesco-p/off-the-shelf-cl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1\">Francesco Pelosin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Modeling Creative Processes for Algorithmic Painting. (arXiv:2205.01605v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.01605","description":"<p>This paper proposes a framework for computational modeling of artistic\npainting algorithms, inspired by human creative practices. Based on examples\nfrom expert artists and from the author's own experience, the paper argues that\ncreative processes often involve two important components: vague, high-level\ngoals (e.g., \"make a good painting\"), and exploratory processes for discovering\nnew ideas. This paper then sketches out possible computational mechanisms for\nimitating those elements of the painting process, including underspecified loss\nfunctions and iterative painting procedures with explicit task decompositions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1\">Aaron Hertzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressive Ptychography using Deep Image and Generative Priors. (arXiv:2205.02397v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02397","description":"<p>Ptychography is a well-established coherent diffraction imaging technique\nthat enables non-invasive imaging of samples at a nanometer scale. It has been\nextensively used in various areas such as the defense industry or materials\nscience. One major limitation of ptychography is the long data acquisition time\ndue to mechanical scanning of the sample; therefore, approaches to reduce the\nscan points are highly desired. However, reconstructions with less number of\nscan points lead to imaging artifacts and significant distortions, hindering a\nquantitative evaluation of the results. To address this bottleneck, we propose\na generative model combining deep image priors with deep generative priors. The\nself-training approach optimizes the deep generative neural network to create a\nsolution for a given dataset. We complement our approach with a prior acquired\nfrom a previously trained discriminator network to avoid a possible divergence\nfrom the desired output caused by the noise in the measurements. We also\nsuggest using the total variation as a complementary before combat artifacts\ndue to measurement noise. We analyze our approach with numerical experiments\nthrough different probe overlap percentages and varying noise levels. We also\ndemonstrate improved reconstruction accuracy compared to the state-of-the-art\nmethod and discuss the advantages and disadvantages of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barutcu_S/0/1/0/all/0/1\">Semih Barutcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gursoy_D/0/1/0/all/0/1\">Do&#x11f;a G&#xfc;rsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K. Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Scale Transfer Learning for Differentially Private Image Classification. (arXiv:2205.02973v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.02973","description":"<p>Differential Privacy (DP) provides a formal framework for training machine\nlearning models with individual example level privacy. In the field of deep\nlearning, Differentially Private Stochastic Gradient Descent (DP-SGD) has\nemerged as a popular private training algorithm. Unfortunately, the\ncomputational cost of training large-scale models with DP-SGD is substantially\nhigher than non-private training. This is further exacerbated by the fact that\nincreasing the number of parameters leads to larger degradation in utility with\nDP. In this work, we zoom in on the ImageNet dataset and demonstrate that,\nsimilar to the non-private case, pre-training over-parameterized models on a\nlarge public dataset can lead to substantial gains when the model is finetuned\nprivately. Moreover, by systematically comparing private and non-private models\nacross a range of large batch sizes, we find that similar to non-private\nsetting, choice of optimizer can further improve performance substantially with\nDP. By using LAMB optimizer with DP-SGD we saw improvement of up to 20$\\%$\npoints (absolute). Finally, we show that finetuning just the last layer for a\n\\emph{single step} in the full batch setting, combined with extremely\nsmall-scale (near-zero) initialization leads to both SOTA results of 81.7 $\\%$\nunder a wide privacy budget range of $\\epsilon \\in [4, 10]$ and $\\delta$ =\n$10^{-6}$ while minimizing the computational overhead substantially.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1\">Abhradeep Thakurta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1\">Alexey Kurakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HierAttn: Effectively Learn Representations from Stage Attention and Branch Attention for Skin Lesions Diagnosis. (arXiv:2205.04326v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.04326","description":"<p>Accurate and unbiased examinations of skin lesions are critical for the early\ndiagnosis and treatment of skin conditions and disorders. Visual features of\nskin lesions vary significantly because the skin images are collected from\npatients with different skin colours and morphologies by using dissimilar\nimaging equipment. Recent studies have reported ensembled convolutional neural\nnetworks (CNNs) to classify the images for early diagnosis of skin disorders.\nHowever, the practical use of these ensembled CNNs is limited because they are\nheavyweight and inadequate for using contextual information. Although\nlightweight networks (e.g., MobileNetV3 and EfficientNet) were developed to\nachieve parameters reduction for implementing deep neural networks on mobile\ndevices, insufficient depth of feature representation restricts the\nperformance. To address the existing limitations, we introduce a new lite and\neffective neural network, namely HierAttn. The HierAttn applies a novel\nstrategy to learn the local and global features by using multi-stage and\nmulti-branch attention mechanisms. The efficacy of HierAttn was evaluated by\nusing the dermoscopy images dataset ISIC2019 and smartphone photos dataset\nPAD-UFES-20. The experimental results show that HierAttn achieves the best\ntop-1 accuracy and AUC among the state-of-the-art lightweight networks. The\ncode is available at https://github.com/anthonyweidai/HierAttn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance. (arXiv:2205.05677v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05677","description":"<p>Marker-less monocular 3D human motion capture (MoCap) with scene interactions\nis a challenging research topic relevant for extended reality, robotics and\nvirtual avatar generation. Due to the inherent depth ambiguity of monocular\nsettings, 3D motions captured with existing methods often contain severe\nartefacts such as incorrect body-scene inter-penetrations, jitter and body\nfloating. To tackle these issues, we propose HULC, a new approach for 3D human\nMoCap which is aware of the scene geometry. HULC estimates 3D poses and dense\nbody-environment surface contacts for improved 3D localisations, as well as the\nabsolute scale of the subject. Furthermore, we introduce a 3D pose trajectory\noptimisation based on a novel pose manifold sampling that resolves erroneous\nbody-environment inter-penetrations. Although the proposed method requires less\nstructured inputs compared to existing scene-aware monocular MoCap algorithms,\nit produces more physically-plausible poses: HULC significantly and\nconsistently outperforms the existing approaches in various experiments and on\ndifferent metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topologically-Aware Deformation Fields for Single-View 3D Reconstruction. (arXiv:2205.06267v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06267","description":"<p>We present a framework for learning 3D object shapes and dense cross-object\n3D correspondences from just an unaligned category-specific image collection.\nThe 3D shapes are generated implicitly as deformations to a category-specific\nsigned distance field and are learned in an unsupervised manner solely from\nunaligned image collections and their poses without any 3D supervision.\nGenerally, image collections on the internet contain several intra-category\ngeometric and topological variations, for example, different chairs can have\ndifferent topologies, which makes the task of joint shape and correspondence\nestimation much more challenging. Because of this, prior works either focus on\nlearning each 3D object shape individually without modeling cross-instance\ncorrespondences or perform joint shape and correspondence estimation on\ncategories with minimal intra-category topological variations. We overcome\nthese restrictions by learning a topologically-aware implicit deformation field\nthat maps a 3D point in the object space to a higher dimensional point in the\ncategory-specific canonical space. At inference time, given a single image, we\nreconstruct the underlying 3D shape by first implicitly deforming each 3D point\nin the object space to the learned category-specific canonical space using the\ntopologically-aware deformation field and then reconstructing the 3D shape as a\ncanonical signed distance field. Both canonical shape and deformation field are\nlearned end-to-end in an inverse-graphics fashion using a learned recurrent ray\nmarcher (SRN) as a differentiable rendering module. Our approach, dubbed TARS,\nachieves state-of-the-art reconstruction fidelity on several datasets:\nShapeNet, Pascal3D+, CUB, and Pix3D chairs. Result videos and code at\nhttps://shivamduggal4.github.io/tars-3D/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duggal_S/0/1/0/all/0/1\">Shivam Duggal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Attention Memory Network for Video Object Segmentation. (arXiv:2205.08075v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08075","description":"<p>Semi-supervised video object segmentation is a fundamental yet Challenging\ntask in computer vision. Embedding matching based CFBI series networks have\nachieved promising results by foreground-background integration approach.\nDespite its superior performance, these works exhibit distinct shortcomings,\nespecially the false predictions caused by little appearance instances in first\nframe, even they could easily be recognized by previous frame. Moreover, they\nsuffer from object's occlusion and error drifts. In order to overcome the\nshortcomings , we propose Collaborative Attention Memory Network with an\nenhanced segmentation head. We introduce a object context scheme that\nexplicitly enhances the object information, which aims at only gathering the\npixels that belong to the same category as a given pixel as its context.\nAdditionally, a segmentation head with Feature Pyramid Attention(FPA) module is\nadopted to perform spatial pyramid attention structure on high-level output.\nFurthermore, we propose an ensemble network to combine STM network with all\nthese new refined CFBI network. Finally, we evaluated our approach on the 2021\nYoutube-VOS challenge where we obtain 6th place with an overall score of\n83.5\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhixing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1\">Junli Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Fei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yuandong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jinpeng Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Comparison Network for Remote Sensing Scene Classification. (arXiv:2205.08147v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08147","description":"<p>Remote sensing scene classification aims to assign a specific semantic label\nto a remote sensing image. Recently, convolutional neural networks have greatly\nimproved the performance of remote sensing scene classification. However, some\nconfused images may be easily recognized as the incorrect category, which\ngenerally degrade the performance. The differences between image pairs can be\nused to distinguish image categories. This paper proposed a pairwise comparison\nnetwork, which contains two main steps: pairwise selection and pairwise\nrepresentation. The proposed network first selects similar image pairs, and\nthen represents the image pairs with pairwise representations. The\nself-representation is introduced to highlight the informative parts of each\nimage itself, while the mutual-representation is proposed to capture the subtle\ndifferences between image pairs. Comprehensive experimental results on two\nchallenging datasets (AID, NWPU-RESISC45) demonstrate the effectiveness of the\nproposed network. The codes are provided in\nhttps://github.com/spectralpublic/PCNet.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiangtao_Z/0/1/0/all/0/1\">Zheng Xiangtao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaoqiang_L/0/1/0/all/0/1\">Lu Xiaoqiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Interactive Image Matting. (arXiv:2205.08324v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08324","description":"<p>Recent image matting studies are developing towards proposing trimap-free or\ninteractive methods for complete complex image matting tasks. Although avoiding\nthe extensive labors of trimap annotation, existing methods still suffer from\ntwo limitations: (1) For the single image with multiple objects, it is\nessential to provide extra interaction information to help determining the\nmatting target; (2) For transparent objects, the accurate regression of alpha\nmatte from RGB image is much more difficult compared with the opaque ones. In\nthis work, we propose a Unified Interactive image Matting method, named UIM,\nwhich solves the limitations and achieves satisfying matting results for any\nscenario. Specifically, UIM leverages multiple types of user interaction to\navoid the ambiguity of multiple matting targets, and we compare the pros and\ncons of different annotation types in detail. To unify the matting performance\nfor transparent and opaque objects, we decouple image matting into two stages,\ni.e., foreground segmentation and transparency prediction. Moreover, we design\na multi-scale attentive fusion module to alleviate the vagueness in the\nboundary region. Experimental results demonstrate that UIM achieves\nstate-of-the-art performance on the Composition-1K test set and a synthetic\nunified dataset. Our code and models will be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Stephen D.H. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">YiQi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Contrast Masked Autoencoders Are Powerful Pathological Representation Learners. (arXiv:2205.09048v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.09048","description":"<p>Based on digital whole slide scanning technique, artificial intelligence\nalgorithms represented by deep learning have achieved remarkable results in the\nfield of computational pathology. Compared with other medical images such as\nComputed Tomography (CT) or Magnetic Resonance Imaging (MRI), pathological\nimages are more difficult to annotate, thus there is an extreme lack of data\nsets that can be used for supervised learning. In this study, a self-supervised\nlearning (SSL) model, Global Contrast Masked Autoencoders (GCMAE), is proposed,\nwhich has the ability to represent both global and local domain-specific\nfeatures of whole slide image (WSI), as well as excellent cross-data transfer\nability. The Camelyon16 and NCTCRC datasets are used to evaluate the\nperformance of our model. When dealing with transfer learning tasks with\ndifferent data sets, the experimental results show that GCMAE has better linear\nclassification accuracy than MAE, which can reach 81.10% and 89.22%\nrespectively. Our method outperforms the previous state-of-the-art algorithm\nand even surpass supervised learning (improved by 3.86% on NCTCRC data sets).\nThe source code of this paper is publicly available at\nhttps://github.com/StarUniversus/gcmae\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Quan_H/0/1/0/all/0/1\">Hao Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Weixing Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_Q/0/1/0/all/0/1\">Qun Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_M/0/1/0/all/0/1\">Mingchen Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ruijie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_T/0/1/0/all/0/1\">Tingting Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_R/0/1/0/all/0/1\">Ruiqun Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xinghua Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_X/0/1/0/all/0/1\">Xiaoyu Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRT-ViT: TensorRT-oriented Vision Transformer. (arXiv:2205.09579v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09579","description":"<p>We revisit the existing excellent Transformers from the perspective of\npractical application. Most of them are not even as efficient as the basic\nResNets series and deviate from the realistic deployment scenario. It may be\ndue to the current criterion to measure computation efficiency, such as FLOPs\nor parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this\npaper directly treats the TensorRT latency on the specific hardware as an\nefficiency metric, which provides more comprehensive feedback involving\ncomputational capacity, memory cost, and bandwidth. Based on a series of\ncontrolled experiments, this work derives four practical guidelines for\nTensorRT-oriented and deployment-friendly network design, e.g., early CNN and\nlate Transformer at stage-level, early Transformer and late CNN at block-level.\nAccordingly, a family of TensortRT-oriented Transformers is presented,\nabbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT\nsignificantly outperforms existing ConvNets and vision Transformers with\nrespect to the latency/accuracy trade-off across diverse visual tasks, e.g.,\nimage classification, object detection and semantic segmentation. For example,\nat 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\\times$ faster than CSWin\nand 2.0$\\times$ faster than Twins. On the MS-COCO object detection task,\nTRT-ViT achieves comparable performance with Twins, while the inference speed\nis increased by 2.8$\\times$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiashi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Font Generation by Learning Fine-Grained Local Styles. (arXiv:2205.09965v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09965","description":"<p>Few-shot font generation (FFG), which aims to generate a new font with a few\nexamples, is gaining increasing attention due to the significant reduction in\nlabor cost. A typical FFG pipeline considers characters in a standard font\nlibrary as content glyphs and transfers them to a new target font by extracting\nstyle information from the reference glyphs. Most existing solutions explicitly\ndisentangle content and style of reference glyphs globally or component-wisely.\nHowever, the style of glyphs mainly lies in the local details, i.e. the styles\nof radicals, components, and strokes together depict the style of a glyph.\nTherefore, even a single character can contain different styles distributed\nover spatial locations. In this paper, we propose a new font generation\napproach by learning 1) the fine-grained local styles from references, and 2)\nthe spatial correspondence between the content and reference glyphs. Therefore,\neach spatial location in the content glyph can be assigned with the right\nfine-grained style. To this end, we adopt cross-attention over the\nrepresentation of the content glyphs as the queries and the representations of\nthe reference glyphs as the keys and values. Instead of explicitly\ndisentangling global or component-wise modeling, the cross-attention mechanism\ncan attend to the right local styles in the reference glyphs and aggregate the\nreference styles into a fine-grained style representation for the given content\nglyphs. The experiments show that the proposed method outperforms the\nstate-of-the-art methods in FFG. In particular, the user studies also\ndemonstrate the style consistency of our approach significantly outperforms\nprevious methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Licheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yiyang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhibin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Minhu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer with Multi-Stage Fusion. (arXiv:2205.10101v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10101","description":"<p>Measuring the perceptual quality of images automatically is an essential task\nin the area of computer vision, as degradations on image quality can exist in\nmany processes from image acquisition, transmission to enhancing. Many Image\nQuality Assessment(IQA) algorithms have been designed to tackle this problem.\nHowever, it still remains un settled due to the various types of image\ndistortions and the lack of large-scale human-rated datasets. In this paper, we\npropose a novel algorithm based on the Swin Transformer [31] with fused\nfeatures from multiple stages, which aggregates information from both local and\nglobal features to better predict the quality. To address the issues of\nsmall-scale datasets, relative rankings of images have been taken into account\ntogether with regression loss to simultaneously optimize the model.\nFurthermore, effective data augmentation strategies are also used to improve\nthe performance. In comparisons with previous works, experiments are carried\nout on two standard IQA datasets and a challenge dataset. The results\ndemonstrate the effectiveness of our work. The proposed method outperforms\nother methods on standard datasets and ranks 2nd in the no-reference track of\nNTIRE 2022 Perceptual Image Quality Assessment Challenge [53]. It verifies that\nour method is promising in solving diverse IQA problems and thus can be used to\nreal-word applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haotian Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xiaoxia Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yitian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuechao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Lean Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}