{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems. (arXiv:2206.01268v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01268","description":"<p>Recently, quite a few novel neural architectures were derived to solve math\nword problems by predicting expression trees. These architectures varied from\nseq2seq models, including encoders leveraging graph relationships combined with\ntree decoders. These models achieve good performance on various MWPs datasets\nbut perform poorly when applied to an adversarial challenge dataset, SVAMP. We\npresent a novel model MMTM that leverages multi-tasking and multi-decoder\nduring pre-training. It creates variant tasks by deriving labels using\npre-order, in-order and post-order traversal of expression trees, and uses\ntask-specific decoders in a multi-tasking framework. We leverage transformer\narchitectures with lower dimensionality and initialize weights from RoBERTa\nmodel. MMTM model achieves better mathematical reasoning ability and\ngeneralisability, which we demonstrate by outperforming the best state of the\nart baseline models from Seq2Seq, GTS, and Graph2Tree with a relative\nimprovement of 19.4% on an adversarial challenge dataset SVAMP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faldu_K/0/1/0/all/0/1\">Keyur Faldu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikani_P/0/1/0/all/0/1\">Prashant Kikani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Darshan Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multiset Version of Even-Odd Permutations Identity. (arXiv:2206.01291v1 [math.CO])","link":"http://arxiv.org/abs/2206.01291","description":"<p>In this paper, we give a new bijective proof of a multiset analogue of\neven-odd permutations identity. This multiset version is equivalent to the\noriginal coin arrangements lemma which is a key combinatorial lemma in the\nSherman's Proof of a conjecture of Feynman about an identity on paths in planar\ngraphs related to combinatorial solution of two dimensional Ising model in\nstatistical physics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Faal_H/0/1/0/all/0/1\">Hossein Teimoori Faal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Scientific Creativity with Retrieval across Knowledge Domains. (arXiv:2206.01328v1 [cs.IR])","link":"http://arxiv.org/abs/2206.01328","description":"<p>Exposure to ideas in domains outside a scientist's own may benefit her in\nreformulating existing research problems in novel ways and discovering new\napplication domains for existing solution ideas. While improved performance in\nscholarly search engines can help scientists efficiently identify relevant\nadvances in domains they may already be familiar with, it may fall short of\nhelping them explore diverse ideas \\textit{outside} such domains. In this paper\nwe explore the design of systems aimed at augmenting the end-user ability in\ncross-domain exploration with flexible query specification. To this end, we\ndevelop an exploratory search system in which end-users can select a portion of\ntext core to their interest from a paper abstract and retrieve papers that have\na high similarity to the user-selected core aspect but differ in terms of\ndomains. Furthermore, end-users can `zoom in' to specific domain clusters to\nretrieve more papers from them and understand nuanced differences within the\nclusters. Our case studies with scientists uncover opportunities and design\nimplications for systems aimed at facilitating cross-domain exploration and\ninspiration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyeonsu B. Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kevin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haw-Shiuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prein_T/0/1/0/all/0/1\">Thorben Prein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1\">Aniket Kittur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olivetti_E/0/1/0/all/0/1\">Elsa Olivetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plumber: A Modular Framework to Create Information Extraction Pipelines. (arXiv:2206.01442v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01442","description":"<p>Information Extraction (IE) tasks are commonly studied topics in various\ndomains of research. Hence, the community continuously produces multiple\ntechniques, solutions, and tools to perform such tasks. However, running those\ntools and integrating them within existing infrastructure requires time,\nexpertise, and resources. One pertinent task here is triples extraction and\nlinking, where structured triples are extracted from a text and aligned to an\nexisting Knowledge Graph (KG). In this paper, we present PLUMBER, the first\nframework that allows users to manually and automatically create suitable IE\npipelines from a community-created pool of tools to perform triple extraction\nand alignment on unstructured text. Our approach provides an interactive medium\nto alter the pipelines and perform IE tasks. A short video to show the working\nof the framework for different use-cases is available online under:\nhttps://www.youtube.com/watch?v=XC9rJNIUv8g\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaradeh_M/0/1/0/all/0/1\">Mohamad Yaser Jaradeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_M/0/1/0/all/0/1\">Markus Stocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Adaptive Pre-Training for Boosting Learning With Noisy Labels: A Study on Text Classification for African Languages. (arXiv:2206.01476v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01476","description":"<p>For high-resource languages like English, text classification is a\nwell-studied task. The performance of modern NLP models easily achieves an\naccuracy of more than 90% in many standard datasets for text classification in\nEnglish (Xie et al., 2019; Yang et al., 2019; Zaheer et al., 2020). However,\ntext classification in low-resource languages is still challenging due to the\nlack of annotated data. Although methods like weak supervision and\ncrowdsourcing can help ease the annotation bottleneck, the annotations obtained\nby these methods contain label noise. Models trained with label noise may not\ngeneralize well. To this end, a variety of noise-handling techniques have been\nproposed to alleviate the negative impact caused by the errors in the\nannotations (for extensive surveys see (Hedderich et al., 2021; Algan &amp; Ulusoy,\n2021)). In this work, we experiment with a group of standard noisy-handling\nmethods on text classification tasks with noisy labels. We study both simulated\nnoise and realistic noise induced by weak supervision. Moreover, we find\ntask-adaptive pre-training techniques (Gururangan et al., 2020) are beneficial\nfor learning with noisy labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael A. Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_F/0/1/0/all/0/1\">Fangzhou Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Topology Induction for Understanding Contextualized Representations. (arXiv:2206.01512v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01512","description":"<p>In this work, we study the representation space of contextualized embeddings\nand gain insight into the hidden topology of large language models. We show\nthere exists a network of latent states that summarize linguistic properties of\ncontextualized representations. Instead of seeking alignments to existing\nwell-defined annotations, we infer this latent network in a fully unsupervised\nway using a structured variational autoencoder. The induced states not only\nserve as anchors that mark the topology (neighbors and connectivity) of the\nrepresentation manifold but also reveal the internal mechanism of encoding\nsentences. With the induced network, we: (1). decompose the representation\nspace into a spectrum of latent states which encode fine-grained word meanings\nwith lexical, morphological, syntactic and semantic information; (2). show\nstate-state transitions encode rich phrase constructions and serve as the\nbackbones of the latent space. Putting the two together, we show that sentences\nare represented as a traversal over the latent network where state-state\ntransition chains encode syntactic templates and state-word emissions fill in\nthe content. We demonstrate these insights with extensive experiments and\nvisualizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acquiring and Modelling Abstract Commonsense Knowledge via Conceptualization. (arXiv:2206.01532v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01532","description":"<p>Conceptualization, or viewing entities and situations as instances of\nabstract concepts in mind and making inferences based on that, is a vital\ncomponent in human intelligence for commonsense reasoning. Although recent\nartificial intelligence has made progress in acquiring and modelling\ncommonsense, attributed to large neural language models and commonsense\nknowledge graphs (CKGs), conceptualization is yet to thoroughly be introduced,\nmaking current approaches ineffective to cover knowledge about countless\ndiverse entities and situations in the real world. To address the problem, we\nthoroughly study the possible role of conceptualization in commonsense\nreasoning, and formulate a framework to replicate human conceptual induction\nfrom acquiring abstract knowledge about abstract concepts. Aided by the\ntaxonomy Probase, we develop tools for contextualized conceptualization on\nATOMIC, a large-scale human annotated CKG. We annotate a dataset for the\nvalidity of conceptualizations for ATOMIC on both event and triple level,\ndevelop a series of heuristic rules based on linguistic features, and train a\nset of neural models, so as to generate and verify abstract knowledge. Based on\nthese components, a pipeline to acquire abstract knowledge is built. A large\nabstract CKG upon ATOMIC is then induced, ready to be instantiated to infer\nabout unseen entities or situations. Furthermore, experiments find directly\naugmenting data with abstract triples to be helpful in commonsense modelling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mutian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Opinion Mining: Summarizing Opinions of Customer Reviews. (arXiv:2206.01543v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01543","description":"<p>Customer reviews are vital for making purchasing decisions in the Information\nAge. Such reviews can be automatically summarized to provide the user with an\noverview of opinions. In this tutorial, we present various aspects of opinion\nsummarization that are useful for researchers and practitioners. First, we will\nintroduce the task and major challenges. Then, we will present existing opinion\nsummarization solutions, both pre-neural and neural. We will discuss how\nsummarizers can be trained in the unsupervised, few-shot, and supervised\nregimes. Each regime has roots in different machine learning methods, such as\nauto-encoding, controllable text generation, and variational inference.\nFinally, we will discuss resources and evaluation methods and conclude with the\nfuture directions. This three-hour tutorial will provide a comprehensive\noverview over major advances in opinion summarization. The listeners will be\nwell-equipped with the knowledge that is both useful for research and practical\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1\">Reinald Kim Amplayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazinskas_A/0/1/0/all/0/1\">Arthur Bra&#x17e;inskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshi Suhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy Qur'an Using a Post-Processed Ensemble of BERT-based Models. (arXiv:2206.01550v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01550","description":"<p>In recent years, we witnessed great progress in different tasks of natural\nlanguage understanding using machine learning. Question answering is one of\nthese tasks which is used by search engines and social media platforms for\nimproved user experience. Arabic is the language of the Holy Qur'an; the sacred\ntext for 1.8 billion people across the world. Arabic is a challenging language\nfor Natural Language Processing (NLP) due to its complex structures. In this\narticle, we describe our attempts at OSACT5 Qur'an QA 2022 Shared Task, which\nis a question answering challenge on the Holy Qur'an in Arabic. We propose an\nensemble learning model based on Arabic variants of BERT models. In addition,\nwe perform post-processing to enhance the model predictions. Our system\nachieves a Partial Reciprocal Rank (pRR) score of 56.6% on the official test\nset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ElKomy_M/0/1/0/all/0/1\">Mohammed ElKomy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarhan_A/0/1/0/all/0/1\">Amany M. Sarhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian. (arXiv:2206.01583v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01583","description":"<p>We present the shared task on artificial text detection in Russian, which is\norganized as a part of the Dialogue Evaluation initiative, held in 2022. The\nshared task dataset includes texts from 14 text generators, i.e., one human\nwriter and 13 text generative models fine-tuned for one or more of the\nfollowing generation tasks: machine translation, paraphrase generation, text\nsummarization, text simplification. We also consider back-translation and\nzero-shot generation approaches. The human-written texts are collected from\npublicly available resources across multiple domains. The shared task consists\nof two sub-tasks: (i) to determine if a given text is automatically generated\nor written by a human; (ii) to identify the author of a given text. The first\ntask is framed as a binary classification problem. The second task is a\nmulti-class classification problem. We provide count-based and BERT-based\nbaselines, along with the human evaluation on the first sub-task. A total of 30\nand 8 systems have been submitted to the binary and multi-class sub-tasks,\ncorrespondingly. Most teams outperform the baselines by a wide margin. We\npublicly release our codebase, human evaluation results, and other materials in\nour GitHub repository (https://github.com/dialogue-evaluation/RuATD).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamardina_T/0/1/0/all/0/1\">Tatiana Shamardina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernianskii_D/0/1/0/all/0/1\">Daniil Chernianskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenogenova_A/0/1/0/all/0/1\">Alena Fenogenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saidov_M/0/1/0/all/0/1\">Marat Saidov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valeeva_A/0/1/0/all/0/1\">Anastasiya Valeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smurov_I/0/1/0/all/0/1\">Ivan Smurov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Similar Questions From Naturally-occurring Business Conversations. (arXiv:2206.01585v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01585","description":"<p>Pre-trained contextualized embedding models such as BERT are a standard\nbuilding block in many natural language processing systems. We demonstrate that\nthe sentence-level representations produced by some off-the-shelf\ncontextualized embedding models have a narrow distribution in the embedding\nspace, and thus perform poorly for the task of identifying semantically similar\nquestions in real-world English business conversations. We describe a method\nthat uses appropriately tuned representations and a small set of exemplars to\ngroup questions of interest to business users in a visualization that can be\nused for data exploration or employee coaching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiliang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossouw_D/0/1/0/all/0/1\">David Rossouw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardiner_S/0/1/0/all/0/1\">Shayna Gardiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corston_Oliver_S/0/1/0/all/0/1\">Simon Corston-Oliver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArgRewrite V.2: an Annotated Argumentative Revisions Corpus. (arXiv:2206.01677v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01677","description":"<p>Analyzing how humans revise their writings is an interesting research\nquestion, not only from an educational perspective but also in terms of\nartificial intelligence. Better understanding of this process could facilitate\nmany NLP applications, from intelligent tutoring systems to supportive and\ncollaborative writing environments. Developing these applications, however,\nrequires revision corpora, which are not widely available. In this work, we\npresent ArgRewrite V.2, a corpus of annotated argumentative revisions,\ncollected from two cycles of revisions to argumentative essays about\nself-driving cars. Annotations are provided at different levels of purpose\ngranularity (coarse and fine) and scope (sentential and subsentential). In\naddition, the corpus includes the revision goal given to each writer, essay\nscores, annotation verification, pre- and post-study surveys collected from\nparticipants as meta-data. The variety of revision unit scope and purpose\ngranularity levels in ArgRewrite, along with the inclusion of new types of\nmeta-data, can make it a useful resource for research and applications that\ninvolve revision analysis. We demonstrate some potential applications of\nArgRewrite V.2 in the development of automatic revision purpose predictors, as\na training source and benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kashefi_O/0/1/0/all/0/1\">Omid Kashefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1\">Tazin Afrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_M/0/1/0/all/0/1\">Meghan Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olshefski_C/0/1/0/all/0/1\">Christopher Olshefski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godley_A/0/1/0/all/0/1\">Amanda Godley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwa_R/0/1/0/all/0/1\">Rebecca Hwa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward a realistic model of speech processing in the brain with self-supervised learning. (arXiv:2206.01685v1 [q-bio.NC])","link":"http://arxiv.org/abs/2206.01685","description":"<p>Several deep neural networks have recently been shown to generate activations\nsimilar to those of the brain in response to the same input. These algorithms,\nhowever, remain largely implausible: they require (1) extraordinarily large\namounts of data, (2) unobtainable supervised labels, (3) textual rather than\nraw sensory input, and / or (4) implausibly large memory (e.g. thousands of\ncontextual words). These elements highlight the need to identify algorithms\nthat, under these limitations, would suffice to account for both behavioral and\nbrain responses. Focusing on the issue of speech processing, we here\nhypothesize that self-supervised algorithms trained on the raw waveform\nconstitute a promising candidate. Specifically, we compare a recent\nself-supervised architecture, Wav2Vec 2.0, to the brain activity of 412\nEnglish, French, and Mandarin individuals recorded with functional Magnetic\nResonance Imaging (fMRI), while they listened to ~1h of audio books. Our\nresults are four-fold. First, we show that this algorithm learns brain-like\nrepresentations with as little as 600 hours of unlabelled speech -- a quantity\ncomparable to what infants can be exposed to during language acquisition.\nSecond, its functional hierarchy aligns with the cortical hierarchy of speech\nprocessing. Third, different training regimes reveal a functional\nspecialization akin to the cortex: Wav2Vec 2.0 learns sound-generic,\nspeech-specific and language-specific representations similar to those of the\nprefrontal and temporal cortices. Fourth, we confirm the similarity of this\nspecialization with the behavior of 386 additional participants. These\nelements, resulting from the largest neuroimaging benchmark to date, show how\nself-supervised learning can account for a rich organization of speech\nprocessing in the brain, and thus delineate a path to identify the laws of\nlanguage acquisition which shape the human brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Millet_J/0/1/0/all/0/1\">Juliette Millet</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Caucheteux_C/0/1/0/all/0/1\">Charlotte Caucheteux</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Orhan_P/0/1/0/all/0/1\">Pierre Orhan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Boubenec_Y/0/1/0/all/0/1\">Yves Boubenec</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pallier_C/0/1/0/all/0/1\">Christophe Pallier</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+King_J/0/1/0/all/0/1\">Jean-Remi King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Gender Bias in Word Embeddings of Gendered Languages Requires Disentangling Grammatical Gender Signals. (arXiv:2206.01691v1 [cs.CY])","link":"http://arxiv.org/abs/2206.01691","description":"<p>Does the grammatical gender of a language interfere when measuring the\nsemantic gender information captured by its word embeddings? A number of\nanomalous gender bias measurements in the embeddings of gendered languages\nsuggest this possibility. We demonstrate that word embeddings learn the\nassociation between a noun and its grammatical gender in grammatically gendered\nlanguages, which can skew social gender bias measurements. Consequently, word\nembedding post-processing methods are introduced to quantify, disentangle, and\nevaluate grammatical gender signals. The evaluation is performed on five\ngendered languages from the Germanic, Romance, and Slavic branches of the\nIndo-European language family. Our method reduces the strength of grammatical\ngender signals, which is measured in terms of effect size (Cohen's d), by a\nsignificant average of d = 1.3 for French, German, and Italian, and d = 0.56\nfor Polish and Spanish. Once grammatical gender is disentangled, the\nassociation between over 90% of 10,000 inanimate nouns and their assigned\ngrammatical gender weakens, and cross-lingual bias results from the Word\nEmbedding Association Test (WEAT) become more congruent with country-level\nimplicit bias measurements. The results further suggest that disentangling\ngrammatical gender signals from word embeddings may lead to improvement in\nsemantic machine learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabbaghi_S/0/1/0/all/0/1\">Shiva Omrani Sabbaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge. (arXiv:2206.01718v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01718","description":"<p>The Visual Question Answering (VQA) task aspires to provide a meaningful\ntestbed for the development of AI models that can jointly reason over visual\nand natural language inputs. Despite a proliferation of VQA datasets, this goal\nis hindered by a set of common limitations. These include a reliance on\nrelatively simplistic questions that are repetitive in both concepts and\nlinguistic structure, little world knowledge needed outside of the paired\nimage, and limited reasoning required to arrive at the correct answer. We\nintroduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about\n25K questions requiring a broad base of commonsense and world knowledge to\nanswer. In contrast to the existing knowledge-based VQA datasets, the questions\ngenerally cannot be answered by simply querying a knowledge base, and instead\nrequire some form of commonsense reasoning about the scene depicted in the\nimage. We demonstrate the potential of this new dataset through a detailed\nanalysis of its contents and baseline performance measurements over a variety\nof state-of-the-art vision-language models. Project page:\n<a href=\"http://a-okvqa.allenai.org/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_D/0/1/0/all/0/1\">Dustin Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marino_K/0/1/0/all/0/1\">Kenneth Marino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the \"Video\" in Video-Language Understanding. (arXiv:2206.01720v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01720","description":"<p>What makes a video task uniquely suited for videos, beyond what can be\nunderstood from a single image? Building on recent progress in self-supervised\nimage-language models, we revisit this question in the context of video and\nlanguage tasks. We propose the atemporal probe (ATP), a new model for\nvideo-language analysis which provides a stronger bound on the baseline\naccuracy of multimodal models constrained by image-level understanding. By\napplying this model to standard discriminative video and language tasks, such\nas video question answering and text-to-video retrieval, we characterize the\nlimitations and potential of current video-language benchmarks. We find that\nunderstanding of event temporality is often not necessary to achieve strong or\nstate-of-the-art performance, even compared with recent large-scale\nvideo-language models and in contexts intended to benchmark deeper video-level\nunderstanding. We also demonstrate how ATP can improve both video-language\ndataset and model design. We describe a technique for leveraging ATP to better\ndisentangle dataset subsets with a higher concentration of temporally\nchallenging data, improving benchmarking efficacy for causal and temporal\nunderstanding. Further, we show that effectively integrating ATP into full\nvideo-level temporal models can improve efficiency and state-of-the-art\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyzaguirre_C/0/1/0/all/0/1\">Crist&#xf3;bal Eyzaguirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1\">Juan Carlos Niebles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The geometry of integration in text classification RNNs. (arXiv:2010.15114v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.15114","description":"<p>Despite the widespread application of recurrent neural networks (RNNs) across\na variety of tasks, a unified understanding of how RNNs solve these tasks\nremains elusive. In particular, it is unclear what dynamical patterns arise in\ntrained RNNs, and how those patterns depend on the training dataset or task.\nThis work addresses these questions in the context of a specific natural\nlanguage processing task: text classification. Using tools from dynamical\nsystems analysis, we study recurrent networks trained on a battery of both\nnatural and synthetic text classification tasks. We find the dynamics of these\ntrained RNNs to be both interpretable and low-dimensional. Specifically, across\narchitectures and datasets, RNNs accumulate evidence for each class as they\nprocess the text, using a low-dimensional attractor manifold as the underlying\nmechanism. Moreover, the dimensionality and geometry of the attractor manifold\nare determined by the structure of the training dataset; in particular, we\ndescribe how simple word-count statistics computed on the training dataset can\nbe used to predict these properties. Our observations span multiple\narchitectures and datasets, reflecting a common mechanism RNNs employ to\nperform text classification. To the degree that integration of evidence towards\na decision is a common computational primitive, this work lays the foundation\nfor using dynamical systems techniques to study the inner workings of RNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aitken_K/0/1/0/all/0/1\">Kyle Aitken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasesh_V/0/1/0/all/0/1\">Vinay V. Ramasesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ankush Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sussillo_D/0/1/0/all/0/1\">David Sussillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheswaranathan_N/0/1/0/all/0/1\">Niru Maheswaranathan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation. (arXiv:2103.11878v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11878","description":"<p>Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT\nevaluation. They can neither distinguish document-level improvements in\ntranslation quality from sentence-level ones, nor identify the discourse\nphenomena that cause context-agnostic translations. This paper introduces a\nnovel automatic metric BlonDe to widen the scope of automatic MT evaluation\nfrom sentence to document level. BlonDe takes discourse coherence into\nconsideration by categorizing discourse-related spans and calculating the\nsimilarity-based F1 measure of categorized spans. We conduct extensive\ncomparisons on a newly constructed dataset BWB. The experimental results show\nthat BlonDe possesses better selectivity and interpretability at the\ndocument-level, and is more sensitive to document-level nuances. In a\nlarge-scale human study, BlonDe also achieves significantly higher Pearson's r\ncorrelation with human judgments compared to previous metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Token Pruning for Transformers. (arXiv:2107.00910v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00910","description":"<p>Deploying transformer models in practice is challenging due to their\ninference cost, which scales quadratically with input sequence length. To\naddress this, we present a novel Learned Token Pruning (LTP) method which\nadaptively removes unimportant tokens as an input sequence passes through\ntransformer layers. In particular, LTP prunes tokens with an attention score\nbelow a threshold value which is learned for each layer during training. Our\nthreshold-based method allows the length of the pruned sequence to vary\nadaptively based on the input sequence, and avoids algorithmically expensive\noperations such as top-k token selection. We extensively test the performance\nof LTP on GLUE tasks and show that our method outperforms the prior\nstate-of-the-art token pruning methods by up to ~2.5% higher accuracy with the\nsame amount of FLOPs. In particular, LTP achieves up to 2.1x FLOPs reduction\nwith less than 1% accuracy drop, which results in up to 1.9x and 2.0x\nthroughput improvement on Intel Haswell CPUs and NVIDIA V100 GPUs,\nrespectively. Furthermore, we demonstrate that LTP is more robust than prior\nmethods to variations on input sentence lengths. Our code has been developed in\nPyTorch and has been open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorsley_D/0/1/0/all/0/1\">David Thorsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_W/0/1/0/all/0/1\">Woosuk Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_J/0/1/0/all/0/1\">Joseph Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing. (arXiv:2110.07572v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07572","description":"<p>Semantic parsing is the task of producing a structured meaning representation\nfor natural language utterances or questions. Recent research has pointed out\nthat the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle\nto generalize systematically, i.e. to handle examples that require recombining\nknown knowledge in novel settings. In this work, we show that better systematic\ngeneralization can be achieved by producing the meaning representation (MR)\ndirectly as a graph and not as a sequence. To this end we propose LAGr, the\nLabeling Aligned Graphs algorithm that produces semantic parses by predicting\nnode and edge labels for a complete multi-layer input-aligned graph. The\nstrongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas\nweakly-supervised LAGr infers alignments for originally unaligned target graphs\nusing an approximate MAP inference procedure. On the COGS and CFQ compositional\ngeneralization benchmarks the strongly- and weakly- supervised LAGr algorithms\nachieve significant improvements upon the baseline seq2seq parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jambor_D/0/1/0/all/0/1\">Dora Jambor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v6 [q-bio.BM] UPDATED)","link":"http://arxiv.org/abs/2201.11147","description":"<p>Self-supervised protein language models have proved their effectiveness in\nlearning the proteins representations. With the increasing computational power,\ncurrent protein language models pre-trained with millions of diverse sequences\ncan advance the parameter scale from million-level to billion-level and achieve\nremarkable improvement. However, those prevailing approaches rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured\nknowledge facts for better protein representations. We argue that informative\nbiology knowledge in KGs can enhance protein representation with external\nknowledge. In this work, we propose OntoProtein, the first general framework\nthat makes use of structure in GO (Gene Ontology) into protein pre-training\nmodels. We construct a novel large-scale knowledge graph that consists of GO\nand its related proteins, and gene annotation texts or protein sequences\ndescribe all nodes in the graph. We propose novel contrastive learning with\nknowledge-aware negative sampling to jointly optimize the knowledge graph and\nprotein embedding during pre-training. Experimental results show that\nOntoProtein can surpass state-of-the-art methods with pre-trained protein\nlanguage models in TAPE benchmark and yield better performance compared with\nbaselines in protein-protein interaction and protein function prediction. Code\nand datasets are available in https://github.com/zjunlp/OntoProtein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hong_H/0/1/0/all/0/1\">Haosen Hong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lian_J/0/1/0/all/0/1\">Jiazhang Lian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02394","description":"<p>Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakhotiya_Y/0/1/0/all/0/1\">Yash Jakhotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1\">Ashwin Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep neural networks for fine-grained surveillance of overdose mortality. (arXiv:2202.12448v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12448","description":"<p>Surveillance of drug overdose deaths relies on death certificates for\nidentification of the substances that caused death. Drugs and drug classes can\nbe identified through the International Classification of Diseases, 10th\nRevision (ICD-10) codes present on death certificates. However, ICD-10 codes do\nnot always provide high levels of specificity in drug identification. To\nachieve more fine-grained identification of substances on a death certificate,\nthe free-text cause of death section, completed by the medical certifier, must\nbe analyzed. Current methods for analyzing free-text death certificates rely\nsolely on look-up tables for identifying specific substances, which must be\nfrequently updated and maintained. To improve identification of drugs on death\ncertificates, a deep learning named-entity recognition model was developed,\nwhich achieved an F1-score of 99.13%. This model can identify new drug\nmisspellings and novel substances that are not present on current surveillance\nlook-up tables, enhancing the surveillance of drug overdose deaths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ward_P/0/1/0/all/0/1\">Patrick J. Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1\">April M. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavova_S/0/1/0/all/0/1\">Svetla Slavova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liford_M/0/1/0/all/0/1\">Madison Liford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniels_L/0/1/0/all/0/1\">Lara Daniels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_R/0/1/0/all/0/1\">Ripley Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task. (arXiv:2202.13174v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13174","description":"<p>Biomedical machine reading comprehension (biomedical-MRC) aims to comprehend\ncomplex biomedical narratives and assist healthcare professionals in retrieving\ninformation from them. The high performance of modern neural network-based MRC\nsystems depends on high-quality, large-scale, human-annotated training\ndatasets. In the biomedical domain, a crucial challenge in creating such\ndatasets is the requirement for domain knowledge, inducing the scarcity of\nlabeled data and the need for transfer learning from the labeled\ngeneral-purpose (source) domain to the biomedical (target) domain. However,\nthere is a discrepancy in marginal distributions between the general-purpose\nand biomedical domains due to the variances in topics. Therefore,\ndirect-transferring of learned representations from a model trained on a\ngeneral-purpose domain to the biomedical domain can hurt the model's\nperformance. We present an adversarial learning-based domain adaptation\nframework for the biomedical machine reading comprehension task (BioADAPT-MRC),\na neural network-based method to address the discrepancies in the marginal\ndistributions between the general and biomedical domain datasets. BioADAPT-MRC\nrelaxes the need for generating pseudo labels for training a well-performing\nbiomedical-MRC model. We extensively evaluate the performance of BioADAPT-MRC\nby comparing it with the best existing methods on three widely used benchmark\nbiomedical-MRC datasets -- BioASQ-7b, BioASQ-8b, and BioASQ-9b. Our results\nsuggest that without using any synthetic or human-annotated data from the\nbiomedical domain, BioADAPT-MRC can achieve state-of-the-art performance on\nthese datasets. Availability: BioADAPT-MRC is freely available as an\nopen-source project at \\url{https://github.com/mmahbub/BioADAPT-MRC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1\">Maria Mahbub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sudarshan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1\">Edmon Begoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peterson_G/0/1/0/all/0/1\">Gregory D Peterson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RigoBERTa: A State-of-the-Art Language Model For Spanish. (arXiv:2205.10233v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10233","description":"<p>This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.\nRigoBERTa is trained over a well-curated corpus formed up from different\nsubcorpora with key features. It follows the DeBERTa architecture, which has\nseveral advantages over other architectures of similar size as BERT or RoBERTa.\nRigoBERTa performance is assessed over 13 NLU tasks in comparison with other\navailable Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa\noutperformed the three models in 10 out of the 13 tasks, achieving new\n\"State-of-the-Art\" results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serrano_A/0/1/0/all/0/1\">Alejandro Vaca Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subies_G/0/1/0/all/0/1\">Guillem Garcia Subies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamorano_H/0/1/0/all/0/1\">Helena Montoro Zamorano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Nuria Aldama Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samy_D/0/1/0/all/0/1\">Doaa Samy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1\">David Betancur Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandoval_A/0/1/0/all/0/1\">Antonio Moreno Sandoval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1\">Marta Guerrero Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_A/0/1/0/all/0/1\">Alvaro Barbero Jimenez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Connectivity Reveals Generalization Strategies. (arXiv:2205.12411v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12411","description":"<p>It is widely accepted in the mode connectivity literature that when two\nneural networks are trained similarly on the same data, they are connected by a\npath through parameter space over which test set accuracy is maintained. Under\nsome circumstances, including transfer learning from pretrained models, these\npaths are presumed to be linear. In contrast to existing results, we find that\namong text classifiers (trained on MNLI, QQP, and CoLA), some pairs of\nfinetuned models have large barriers of increasing loss on the linear paths\nbetween them. On each task, we find distinct clusters of models which are\nlinearly connected on the test loss surface, but are disconnected from models\noutside the cluster -- models that occupy separate basins on the surface. By\nmeasuring performance on specially-crafted diagnostic datasets, we find that\nthese clusters correspond to different generalization strategies: one cluster\nbehaves like a bag of words model under domain shift, while another cluster\nuses syntactic heuristics. Our work demonstrates how the geometry of the loss\nsurface can guide models towards different heuristic functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juneja_J/0/1/0/all/0/1\">Jeevesh Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rachit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"What Are Expected Queries in End-to-End Object Detection?. (arXiv:2206.01232v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01232","description":"<p>End-to-end object detection is rapidly progressed after the emergence of\nDETR. DETRs use a set of sparse queries that replace the dense candidate boxes\nin most traditional detectors. In comparison, the sparse queries cannot\nguarantee a high recall as dense priors. However, making queries dense is not\ntrivial in current frameworks. It not only suffers from heavy computational\ncost but also difficult optimization. As both sparse and dense queries are\nimperfect, then \\emph{what are expected queries in end-to-end object\ndetection}? This paper shows that the expected queries should be Dense Distinct\nQueries (DDQ). Concretely, we introduce dense priors back to the framework to\ngenerate dense queries. A duplicate query removal pre-process is applied to\nthese queries so that they are distinguishable from each other. The dense\ndistinct queries are then iteratively processed to obtain final sparse outputs.\nWe show that DDQ is stronger, more robust, and converges faster. It obtains\n44.5 AP on the MS COCO detection dataset with only 12 epochs. DDQ is also\nrobust as it outperforms previous methods on both object detection and instance\nsegmentation tasks on various datasets. DDQ blends advantages from traditional\ndense priors and recent end-to-end detectors. We hope it can serve as a new\nbaseline and inspires researchers to revisit the complementarity between\ntraditional methods and end-to-end detectors. The source code is publicly\navailable at \\url{https://github.com/jshilong/DDQ}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Portrait Stylization on the Edge. (arXiv:2206.01244v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01244","description":"<p>In this work we demonstrate real-time portrait stylization, specifically,\ntranslating self-portrait into cartoon or anime style on mobile devices. We\npropose a latency-driven differentiable architecture search method, maintaining\nrealistic generative quality. With our framework, we obtain $10\\times$\ncomputation reduction on the generative model and achieve real-time video\nstylization on off-the-shelf smartphone using mobile GPUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jiexiong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expressiveness and Learnability: A Unifying View for Evaluating Self-Supervised Learning. (arXiv:2206.01251v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01251","description":"<p>We propose a unifying view to analyze the representation quality of\nself-supervised learning (SSL) models without access to supervised labels,\nwhile being agnostic to the architecture, learning algorithm or data\nmanipulation used during training. We argue that representations can be\nevaluated through the lens of expressiveness and learnability. We propose to\nuse the Intrinsic Dimension (ID) to assess expressiveness and introduce Cluster\nLearnability (CL) to assess learnability. CL is measured as the learning speed\nof a KNN classifier trained to predict labels obtained by clustering the\nrepresentations with K-means. We thus combine CL and ID into a single\npredictor: CLID. Through a large-scale empirical study with a diverse family of\nSSL algorithms, we find that CLID better correlates with in-distribution model\nperformance than other competing recent evaluation schemes. We also benchmark\nCLID on out-of-domain generalization, where CLID serves as a predictor of the\ntransfer performance of SSL models on several classification tasks, yielding\nimprovements with respect to the competing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuchen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1\">Aristide Baratin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images. (arXiv:2206.01256v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01256","description":"<p>In this paper, we propose PETRv2, a unified framework for 3D perception from\nmulti-view images. Based on PETR, PETRv2 explores the effectiveness of temporal\nmodeling, which utilizes the temporal information of previous frames to boost\n3D object detection. More specifically, we extend the 3D position embedding (3D\nPE) in PETR for temporal modeling. The 3D PE achieves the temporal alignment on\nobject position of different frames. A feature-guided position encoder is\nfurther introduced to improve the data adaptability of 3D PE. To support for\nhigh-quality BEV segmentation, PETRv2 provides a simply yet effective solution\nby adding a set of segmentation queries. Each segmentation query is responsible\nfor segmenting one specific patch of BEV map. PETRv2 achieves state-of-the-art\nperformance on 3D object detection and BEV segmentation. Detailed robustness\nanalysis is also conducted on PETR framework. We hope PETRv2 can serve as a\nunified framework for 3D perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1\">Fan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Points2NeRF: Generating Neural Radiance Fields from 3D point cloud. (arXiv:2206.01290v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01290","description":"<p>Contemporary registration devices for 3D visual information, such as LIDARs\nand various depth cameras, capture data as 3D point clouds. In turn, such\nclouds are challenging to be processed due to their size and complexity.\nExisting methods address this problem by fitting a mesh to the point cloud and\nrendering it instead. This approach, however, leads to the reduced fidelity of\nthe resulting visualization and misses color information of the objects crucial\nin computer graphics applications. In this work, we propose to mitigate this\nchallenge by representing 3D objects as Neural Radiance Fields (NeRFs). We\nleverage a hypernetwork paradigm and train the model to take a 3D point cloud\nwith the associated color values and return a NeRF network's weights that\nreconstruct 3D objects from input 2D images. Our method provides efficient 3D\nobject representation and offers several advantages over the existing\napproaches, including the ability to condition NeRFs and improved\ngeneralization beyond objects seen in training. The latter we also confirmed in\nthe results of our empirical evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zimny_D/0/1/0/all/0/1\">D. Zimny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">T. Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1\">P. Spurek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lossless Compression of Point Cloud Sequences Using Sequence Optimized CNN Models. (arXiv:2206.01297v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01297","description":"<p>We propose a new paradigm for encoding the geometry of point cloud sequences,\nwhere the convolutional neural network (CNN) which estimates the encoding\ndistributions is optimized on several frames of the sequence to be compressed.\nWe adopt lightweight CNN structures, we perform training as part of the\nencoding process, and the CNN parameters are transmitted as part of the\nbitstream. The newly proposed encoding scheme operates on the octree\nrepresentation for each point cloud, encoding consecutively each octree\nresolution layer. At every octree resolution layer, the voxel grid is traversed\nsection-by-section (each section being perpendicular to a selected coordinate\naxis) and in each section the occupancies of groups of two-by-two voxels are\nencoded at once, in a single arithmetic coding operation. A context for the\nconditional encoding distribution is defined for each two-by-two group of\nvoxels, based on the information available about the occupancy of neighbor\nvoxels in the current and lower resolution layers of the octree. The CNN\nestimates the probability distributions of occupancy patterns of all voxel\ngroups from one section in four phases. In each new phase the contexts are\nupdated with the occupancies encoded in the previous phase, and each phase\nestimates the probabilities in parallel, providing a reasonable trade-off\nbetween the parallelism of processing and the informativeness of the contexts.\nThe CNN training time is comparable to the time spent in the remaining encoding\nsteps, leading to competitive overall encoding times. Bitrates and\nencoding-decoding times compare favorably with those of recently published\ncompression schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaya_E/0/1/0/all/0/1\">Emre Can Kaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabus_I/0/1/0/all/0/1\">Ioan Tabus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"H-EMD: A Hierarchical Earth Mover's Distance Method for Instance Segmentation. (arXiv:2206.01309v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01309","description":"<p>Deep learning (DL) based semantic segmentation methods have achieved\nexcellent performance in biomedical image segmentation, producing high quality\nprobability maps to allow extraction of rich instance information to facilitate\ngood instance segmentation. While numerous efforts were put into developing new\nDL semantic segmentation models, less attention was paid to a key issue of how\nto effectively explore their probability maps to attain the best possible\ninstance segmentation. We observe that probability maps by DL semantic\nsegmentation models can be used to generate many possible instance candidates,\nand accurate instance segmentation can be achieved by selecting from them a set\nof \"optimized\" candidates as output instances. Further, the generated instance\ncandidates form a well-behaved hierarchical structure (a forest), which allows\nselecting instances in an optimized manner. Hence, we propose a novel\nframework, called hierarchical earth mover's distance (H-EMD), for instance\nsegmentation in biomedical 2D+time videos and 3D images, which judiciously\nincorporates consistent instance selection with semantic-segmentation-generated\nprobability maps. H-EMD contains two main stages. (1) Instance candidate\ngeneration: capturing instance-structured information in probability maps by\ngenerating many instance candidates in a forest structure. (2) Instance\ncandidate selection: selecting instances from the candidate set for final\ninstance segmentation. We formulate a key instance selection problem on the\ninstance candidate forest as an optimization problem based on the earth mover's\ndistance (EMD), and solve it by integer linear programming. Extensive\nexperiments on eight biomedical video or 3D datasets demonstrate that H-EMD\nconsistently boosts DL semantic segmentation models and is highly competitive\nwith state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Peixian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianxu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madukoma_C/0/1/0/all/0/1\">Chinedu S. Madukoma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weninger_T/0/1/0/all/0/1\">Tim Weninger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrout_J/0/1/0/all/0/1\">Joshua D. Shrout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danny Z. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Unbiased Transferability for Domain Adaptation by Uncertainty Modeling. (arXiv:2206.01319v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01319","description":"<p>Domain adaptation (DA) aims to transfer knowledge learned from a labeled\nsource domain to an unlabeled or a less labeled but related target domain.\nIdeally, the source and target distributions should be aligned to each other\nequally to achieve unbiased knowledge transfer. However, due to the significant\nimbalance between the amount of annotated data in the source and target\ndomains, usually only the target distribution is aligned to the source domain,\nleading to adapting unnecessary source specific knowledge to the target domain,\ni.e., biased domain adaptation. To resolve this problem, in this work, we delve\ninto the transferability estimation problem in domain adaptation and propose a\nnon-intrusive Unbiased Transferability Estimation Plug-in (UTEP) by modeling\nthe uncertainty of a discriminator in adversarial-based DA methods to optimize\nunbiased transfer. We theoretically analyze the effectiveness of the proposed\napproach to unbiased transferability learning in DA. Furthermore, to alleviate\nthe impact of imbalanced annotated data, we utilize the estimated uncertainty\nfor pseudo label selection of unlabeled samples in the target domain, which\nhelps achieve better marginal and conditional distribution alignments between\ndomains. Extensive experimental results on a high variety of DA benchmark\ndatasets show that the proposed approach can be readily incorporated into\nvarious adversarial-based DA methods, achieving state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Haowen Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guile Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Fairness in Large-Scale Object Recognition by CrowdSourced Demographic Information. (arXiv:2206.01326v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01326","description":"<p>There has been increasing awareness of ethical issues in machine learning,\nand fairness has become an important research topic. Most fairness efforts in\ncomputer vision have been focused on human sensing applications and preventing\ndiscrimination by people's physical attributes such as race, skin color or age\nby increasing visual representation for particular demographic groups. We argue\nthat ML fairness efforts should extend to object recognition as well.\nBuildings, artwork, food and clothing are examples of the objects that define\nhuman culture. Representing these objects fairly in machine learning datasets\nwill lead to models that are less biased towards a particular culture and more\ninclusive of different traditions and values. There exist many research\ndatasets for object recognition, but they have not carefully considered which\nclasses should be included, or how much training data should be collected per\nclass. To address this, we propose a simple and general approach, based on\ncrowdsourcing the demographic composition of the contributors: we define fair\nrelevance scores, estimate them, and assign them to each class. We showcase its\napplication to the landmark recognition domain, presenting a detailed analysis\nand the final fairer landmark rankings. We present analysis which leads to a\nmuch fairer coverage of the world compared to existing datasets. The evaluation\ndataset was used for the 2021 Google Landmark Challenges, which was the first\nof a kind with an emphasis on fairness in generic object recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1\">Andr&#xe9; Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bingyi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askew_C/0/1/0/all/0/1\">Cam Askew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1\">Jack Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1\">Mike Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilla_N/0/1/0/all/0/1\">N&#x27;Mah Fodiatu Yilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1\">Tobias Weyand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RELAY: Robotic EyeLink AnalYsis of the EyeLink 1000 using an Artificial Eye. (arXiv:2206.01327v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01327","description":"<p>There is a widespread assumption that the peak velocities of visually guided\nsaccades in the dark are up to 10~\\% slower than those made in the light.\nStudies that questioned the impact of the surrounding brightness conditions,\ncome to differing conclusions, whether they have an influence or not and if so,\nin which manner. The problem is of a complex nature as the illumination\ncondition itself may not contribute to different measured peak velocities\nsolely but in combination with the estimation of the pupil size due to its\ndeformation during saccades or different gaze positions. Even the measurement\ntechnique of video-based eye tracking itself could play a significant role. To\ninvestigate this issue, we constructed a stepper motor driven artificial eye\nwith fixed pupil size to mimic human saccades with predetermined peak velocity\n\\&amp; amplitudes under three different brightness conditions with the EyeLink\n1000, one of the most common used eye trackers. The aim was to control the\npupil and brightness. With our device, an overall good accuracy and precision\nof the EyeLink 1000 could be confirmed. Furthermore, we could find that there\nis no artifact for pupil based eye tracking in relation to changing brightness\nconditions, neither for the pupil size nor for the peak velocities. What we\nfound, was a systematic, small, yet significant change of the measured pupil\nsizes as a function of different gaze directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felssberg_A/0/1/0/all/0/1\">Anna-Maria Fel&#xdf;berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strazdas_D/0/1/0/all/0/1\">Dominykas Strazdas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Scale Error Control in Low Light Image and Video Enhancement Using Equivariance. (arXiv:2206.01334v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01334","description":"<p>Image frames obtained in darkness are special. Just multiplying by a constant\ndoesn't restore the image. Shot noise, quantization effects and camera\nnon-linearities mean that colors and relative light levels are estimated\npoorly. Current methods learn a mapping using real dark-bright image pairs.\nThese are very hard to capture. A recent paper has shown that simulated data\npairs produce real improvements in restoration, likely because huge volumes of\nsimulated data are easy to obtain. In this paper, we show that respecting\nequivariance -- the color of a restored pixel should be the same, however the\nimage is cropped -- produces real improvements over the state of the art for\nrestoration. We show that a scale selection mechanism can be used to improve\nreconstructions. Finally, we show that our approach produces improvements on\nvideo restoration as well. Our methods are evaluated both quantitatively and\nqualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanzadeh_S/0/1/0/all/0/1\">Sara Aghajanzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Pulmonary Embolism from Computed Tomography Using Convolutional Neural Network. (arXiv:2206.01344v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01344","description":"<p>The clinical symptoms of pulmonary embolism (PE) are very diverse and\nnon-specific, which makes it difficult to diagnose. In addition, pulmonary\nembolism has multiple triggers and is one of the major causes of vascular\ndeath. Therefore, if it can be detected and treated quickly, it can\nsignificantly reduce the risk of death in hospitalized patients. In the\ndetection process, the cost of computed tomography pulmonary angiography (CTPA)\nis high, and angiography requires the injection of contrast agents, which\nincrease the risk of damage to the patient. Therefore, this study will use a\ndeep learning approach to detect pulmonary embolism in all patients who take a\nCT image of the chest using a convolutional neural network. With the proposed\npulmonary embolism detection system, we can detect the possibility of pulmonary\nembolism at the same time as the patient's first CT image, and schedule the\nCTPA test immediately, saving more than a week of CT image screening time and\nproviding timely diagnosis and treatment to the patient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Chia-Hung Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun-Chien Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chin Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks on Human Vision. (arXiv:2206.01365v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01365","description":"<p>This article presents an introduction to visual attention retargeting, its\nconnection to visual saliency, the challenges associated with it, and ideas for\nhow it can be approached. The difficulty of attention retargeting as a saliency\ninversion problem lies in the lack of one-to-one mapping between saliency and\nthe image domain, in addition to the possible negative impact of saliency\nalterations on image aesthetics. A few approaches from recent literature to\nsolve this challenging problem are reviewed, and several suggestions for future\ndevelopment are presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mateescu_V/0/1/0/all/0/1\">Victor A. Mateescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1\">Ivan V. Baji&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supernet Training for Federated Image Classification under System Heterogeneity. (arXiv:2206.01366v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01366","description":"<p>Efficient deployment of deep neural networks across many devices and resource\nconstraints, especially on edge devices, is one of the most challenging\nproblems in the presence of data-privacy preservation issues. Conventional\napproaches have evolved to either improve a single global model while keeping\neach local training data decentralized (i.e., data-heterogeneity) or to train a\nonce-for-all network that supports diverse architectural settings to address\nheterogeneous systems equipped with different computational capabilities (i.e.,\nmodel-heterogeneity). However, little research has considered both directions\nsimultaneously. In this work, we propose a novel framework to consider both\nscenarios, namely Federation of Supernet Training (FedSup), where clients send\nand receive a supernet whereby it contains all possible architectures sampled\nfrom itself. It is inspired by how averaging parameters in the model\naggregation stage of Federated Learning (FL) is similar to weight-sharing in\nsupernet training. Specifically, in the FedSup framework, a weight-sharing\napproach widely used in the training single shot model is combined with the\naveraging of Federated Learning (FedAvg). Under our framework, we present an\nefficient algorithm (E-FedSup) by sending the sub-model to clients in the\nbroadcast stage for reducing communication costs and training overhead. We\ndemonstrate several strategies to enhance supernet training in the FL\nenvironment and conduct extensive empirical evaluations. The resulting\nframework is shown to pave the way for the robustness of both data- and\nmodel-heterogeneity on several standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Learning Meets Transfer Learning: Application to Multi-site Prostate MRI Segmentation. (arXiv:2206.01369v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01369","description":"<p>Many medical datasets have recently been created for medical image\nsegmentation tasks, and it is natural to question whether we can use them to\nsequentially train a single model that (1) performs better on all these\ndatasets, and (2) generalizes well and transfers better to the unknown target\nsite domain. Prior works have achieved this goal by jointly training one model\non multi-site datasets, which achieve competitive performance on average but\nsuch methods rely on the assumption about the availability of all training\ndata, thus limiting its effectiveness in practical deployment. In this paper,\nwe propose a novel multi-site segmentation framework called\nincremental-transfer learning (ITL), which learns a model from multi-site\ndatasets in an end-to-end sequential fashion. Specifically, \"incremental\"\nrefers to training sequentially constructed datasets, and \"transfer\" is\nachieved by leveraging useful information from the linear combination of\nembedding features on each dataset. In addition, we introduce our ITL\nframework, where we train the network including a site-agnostic encoder with\npre-trained weights and at most two segmentation decoder heads. We also design\na novel site-level incremental loss in order to generalize well on the target\ndomain. Second, we show for the first time that leveraging our ITL training\nscheme is able to alleviate challenging catastrophic forgetting problems in\nincremental learning. We conduct experiments using five challenging benchmark\ndatasets to validate the effectiveness of our incremental-transfer learning\napproach. Our approach makes minimal assumptions on computation resources and\ndomain-specific expertise, and hence constitutes a strong starting point in\nmulti-site medical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jinlin Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1\">Kun Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siyuan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onofrey_J/0/1/0/all/0/1\">John Onofrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slot Order Matters for Compositional Scene Understanding. (arXiv:2206.01370v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01370","description":"<p>Empowering agents with a compositional understanding of their environment is\na promising next step toward solving long-horizon planning problems. On the one\nhand, we have seen encouraging progress on variational inference algorithms for\nobtaining sets of object-centric latent representations (\"slots\") from\nunstructured scene observations. On the other hand, generating scenes from\nslots has received less attention, in part because it is complicated by the\nlack of a canonical object order. A canonical object order is useful for\nlearning the object correlations necessary to generate physically plausible\nscenes similar to how raster scan order facilitates learning pixel correlations\nfor pixel-level autoregressive image generation. In this work, we address this\nlack by learning a fixed object order for a hierarchical variational\nautoencoder with a single level of autoregressive slots and a global scene\nprior. We cast autoregressive slot inference as a set-to-sequence modeling\nproblem. We introduce an auxiliary loss to train the slot prior to generate\nobjects in a fixed order. During inference, we align a set of inferred slots to\nthe object order obtained from a slot prior rollout. To ensure the rolled out\nobjects are meaningful for the given scene, we condition the prior on an\ninferred global summary of the input. Experiments on compositional environments\nand ablations demonstrate that our model with global prior, inference with\naligned slot order, and auxiliary loss achieves state-of-the-art sample\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Emami_P/0/1/0/all/0/1\">Patrick Emami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1\">Sanjay Ranka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1\">Anand Rangarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CF-YOLO: Cross Fusion YOLO for Object Detection in Adverse Weather with a High-quality Real Snow Dataset. (arXiv:2206.01381v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01381","description":"<p>Snow is one of the toughest adverse weather conditions for object detection\n(OD). Currently, not only there is a lack of snowy OD datasets to train\ncutting-edge detectors, but also these detectors have difficulties learning\nlatent information beneficial for detection in snow. To alleviate the two above\nproblems, we first establish a real-world snowy OD dataset, named RSOD.\nBesides, we develop an unsupervised training strategy with a distinctive\nactivation function, called $Peak \\ Act$, to quantitatively evaluate the effect\nof snow on each object. Peak Act helps grading the images in RSOD into\nfour-difficulty levels. To our knowledge, RSOD is the first quantitatively\nevaluated and graded snowy OD dataset. Then, we propose a novel Cross Fusion\n(CF) block to construct a lightweight OD network based on YOLOv5s (call\nCF-YOLO). CF is a plug-and-play feature aggregation module, which integrates\nthe advantages of Feature Pyramid Network and Path Aggregation Network in a\nsimpler yet more flexible form. Both RSOD and CF lead our CF-YOLO to possess an\noptimization ability for OD in real-world snow. That is, CF-YOLO can handle\nunfavorable detection problems of vagueness, distortion and covering of snow.\nExperiments show that our CF-YOLO achieves better detection results on RSOD,\ncompared to SOTAs. The code and dataset are available at\nhttps://github.com/qqding77/CF-YOLO-and-RSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1\">Qiqi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xuefeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Ding Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Falconn++: A Locality-sensitive Filtering Approach for Approximate Nearest Neighbor Search. (arXiv:2206.01382v1 [cs.DS])","link":"http://arxiv.org/abs/2206.01382","description":"<p>We present Falconn++, a novel locality-sensitive filtering (LSF) approach for\napproximate nearest neighbor search on angular distance. Falconn++ can filter\nout potential far away points in any hash bucket before querying, which results\nin higher quality candidates compared to other hashing-based solutions.\nTheoretically, Falconn++ asymptotically achieves lower query time complexity\nthan Falconn, an optimal locality-sensitive hashing scheme on angular distance.\nEmpirically, Falconn++ achieves a higher recall-speed tradeoff than Falconn on\nmany real-world data sets. Falconn++ is also competitive against HNSW, an\nefficient representative of graph-based solutions on high search recall\nregimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Ninh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End 3D Hand Pose Estimation from Stereo Cameras. (arXiv:2206.01384v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01384","description":"<p>This work proposes an end-to-end approach to estimate full 3D hand pose from\nstereo cameras. Most existing methods of estimating hand pose from stereo\ncameras apply stereo matching to obtain depth map and use depth-based solution\nto estimate hand pose. In contrast, we propose to bypass the stereo matching\nand directly estimate the 3D hand pose from the stereo image pairs. The\nproposed neural network architecture extends from any keypoint predictor to\nestimate the sparse disparity of the hand joints. In order to effectively train\nthe model, we propose a large scale synthetic dataset that is composed of\nstereo image pairs and ground truth 3D hand pose annotations. Experiments show\nthat the proposed approach outperforms the existing methods based on the stereo\ndepth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zehao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Liuhao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1\">Jonathan Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Structured Illumination Microscopy with a Neural Space-time Model. (arXiv:2206.01397v1 [physics.optics])","link":"http://arxiv.org/abs/2206.01397","description":"<p>Structured illumination microscopy (SIM) reconstructs a super-resolved image\nfrom multiple raw images; hence, acquisition speed is limited, making it\nunsuitable for dynamic scenes. We propose a new method, Speckle Flow SIM, that\nmodels sample motion during the data capture in order to reconstruct dynamic\nscenes with super-resolution. Speckle Flow SIM uses fixed speckle illumination\nand relies on sample motion to capture a sequence of raw images. Then, the\nspatio-temporal relationship of the dynamic scene is modeled using a neural\nspace-time model with coordinate-based multi-layer perceptrons (MLPs), and the\nmotion dynamics and the super-resolved scene are jointly recovered. We\nvalidated Speckle Flow SIM in simulation and built a simple, inexpensive\nexperimental setup with off-the-shelf components. We demonstrated that Speckle\nFlow SIM can reconstruct a dynamic scene with deformable motion and 1.88x the\ndiffraction-limited resolution in experiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Cao_R/0/1/0/all/0/1\">Ruiming Cao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_F/0/1/0/all/0/1\">Fanglin Linda Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yeh_L/0/1/0/all/0/1\">Li-Hao Yeh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Waller_L/0/1/0/all/0/1\">Laura Waller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaLR: Layer-wise Learning Rate based on Meta-Learning for Adaptively Fine-tuning Medical Pre-trained Models. (arXiv:2206.01408v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01408","description":"<p>When applying transfer learning for medical image analysis, downstream tasks\noften have significant gaps with the pre-training tasks. Previous methods\nmainly focus on improving the transferabilities of the pre-trained models to\nbridge the gaps. In fact, model fine-tuning can also play a very important role\nin tackling this problem. A conventional fine-tuning method is updating all\ndeep neural networks (DNNs) layers by a single learning rate (LR), which\nignores the unique transferabilities of different layers. In this work, we\nexplore the behaviors of different layers in the fine-tuning stage. More\nprecisely, we first hypothesize that lower-level layers are more\ndomain-specific while higher-level layers are more task-specific, which is\nverified by a simple bi-directional fine-tuning scheme. It is harder for the\npre-trained specific layers to transfer to new tasks than general layers. On\nthis basis, to make different layers better co-adapt to the downstream tasks\naccording to their transferabilities, a meta-learning-based LR learner, namely\nMetaLR, is proposed to assign LRs for each layer automatically. Extensive\nexperiments on various medical applications (i.e., POCUS, BUSI, Chest X-ray,\nand LiTS) well confirm our hypothesis and show the superior performance of the\nproposed methods to previous state-of-the-art fine-tuning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingxian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hua Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chris Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning an Adaptation Function to Assess Image Visual Similarities. (arXiv:2206.01417v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01417","description":"<p>Human perception is routinely assessing the similarity between images, both\nfor decision making and creative thinking. But the underlying cognitive process\nis not really well understood yet, hence difficult to be mimicked by computer\nvision systems. State-of-the-art approaches using deep architectures are often\nbased on the comparison of images described as feature vectors learned for\nimage categorization task. As a consequence, such features are powerful to\ncompare semantically related images but not really efficient to compare images\nvisually similar but semantically unrelated. Inspired by previous works on\nneural features adaptation to psycho-cognitive representations, we focus here\non the specific task of learning visual image similarities when analogy\nmatters. We propose to compare different supervised, semi-supervised and\nself-supervised networks, pre-trained on distinct scales and contents datasets\n(such as ImageNet-21k, ImageNet-1K or VGGFace2) to conclude which model may be\nthe best to approximate the visual cortex and learn only an adaptation function\ncorresponding to the approximation of the the primate IT cortex through the\nmetric learning framework. Our experiments conducted on the Totally Looks Like\nimage dataset highlight the interest of our method, by increasing the retrieval\nscores of the best model @1 by 2.25x. This research work was recently accepted\nfor publication at the ICIP 2021 international conference [1]. In this new\narticle, we expand on this previous work by using and comparing new pre-trained\nfeature extractors on other datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Risser_Maroix_O/0/1/0/all/0/1\">Olivier Risser-Maroix</a> (LIPADE), <a href=\"http://arxiv.org/find/cs/1/au:+Marzouki_A/0/1/0/all/0/1\">Amine Marzouki</a> (LIPADE), <a href=\"http://arxiv.org/find/cs/1/au:+Djeghim_H/0/1/0/all/0/1\">Hala Djeghim</a> (LIPADE), <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_C/0/1/0/all/0/1\">Camille Kurtz</a> (LIPADE), <a href=\"http://arxiv.org/find/cs/1/au:+Lomenie_N/0/1/0/all/0/1\">Nicolas Lomenie</a> (LIPADE)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning rich optical embeddings for privacy-preserving lensless image classification. (arXiv:2206.01429v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01429","description":"<p>By replacing the lens with a thin optical element, lensless imaging enables\nnew applications and solutions beyond those supported by traditional camera\ndesign and post-processing, e.g. compact and lightweight form factors and\nvisual privacy. The latter arises from the highly multiplexed measurements of\nlensless cameras, which require knowledge of the imaging system to recover a\nrecognizable image. In this work, we exploit this unique multiplexing property:\ncasting the optics as an encoder that produces learned embeddings directly at\nthe camera sensor. We do so in the context of image classification, where we\njointly optimize the encoder's parameters and those of an image classifier in\nan end-to-end fashion. Our experiments show that jointly learning the lensless\noptical encoder and the digital processing allows for lower resolution\nembeddings at the sensor, and hence better privacy as it is much harder to\nrecover meaningful images from these measurements. Additional experiments show\nthat such an optimization allows for lensless measurements that are more robust\nto typical real-world image transformations. While this work focuses on\nclassification, the proposed programmable lensless camera and end-to-end\noptimization can be applied to other computational imaging tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bezzam_E/0/1/0/all/0/1\">Eric Bezzam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetterli_M/0/1/0/all/0/1\">Martin Vetterli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simeoni_M/0/1/0/all/0/1\">Matthieu Simeoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LenslessPiCam: A Hardware and Software Platform for Lensless Computational Imaging with a Raspberry Pi. (arXiv:2206.01430v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01430","description":"<p>Lensless imaging seeks to replace/remove the lens in a conventional imaging\nsystem. The earliest cameras were in fact lensless, relying on long exposure\ntimes to form images on the other end of a small aperture in a darkened\nroom/container (camera obscura). The introduction of a lens allowed for more\nlight throughput and therefore shorter exposure times, while retaining sharp\nfocus. The incorporation of digital sensors readily enabled the use of\ncomputational imaging techniques to post-process and enhance raw images (e.g.\nvia deblurring, inpainting, denoising, sharpening). Recently, imaging\nscientists have started leveraging computational imaging as an integral part of\nlensless imaging systems, allowing them to form viewable images from the highly\nmultiplexed raw measurements of lensless cameras (see [5] and references\ntherein for a comprehensive treatment of lensless imaging). This represents a\nreal paradigm shift in camera system design as there is more flexibility to\ncater the hardware to the application at hand (e.g. lightweight or flat\ndesigns). This increased flexibility comes however at the price of a more\ndemanding post-processing of the raw digital recordings and a tighter\nintegration of sensing and computation, often difficult to achieve in practice\ndue to inefficient interactions between the various communities of scientists\ninvolved. With LenslessPiCam, we provide an easily accessible hardware and\nsoftware framework to enable researchers, hobbyists, and students to implement\nand explore practical and computational aspects of lensless imaging. We also\nprovide detailed guides and exercises so that LenslessPiCam can be used as an\neducational resource, and point to results from our graduate-level signal\nprocessing course.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bezzam_E/0/1/0/all/0/1\">Eric Bezzam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kashani_S/0/1/0/all/0/1\">Sepand Kashani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vetterli_M/0/1/0/all/0/1\">Martin Vetterli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simeoni_M/0/1/0/all/0/1\">Matthieu Simeoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Transformers for Behavioural Biometrics: A Case Study in Gait Recognition. (arXiv:2206.01441v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01441","description":"<p>Biometrics on mobile devices has attracted a lot of attention in recent years\nas it is considered a user-friendly authentication method. This interest has\nalso been motivated by the success of Deep Learning (DL). Architectures based\non Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)\nhave been established to be convenient for the task, improving the performance\nand robustness in comparison to traditional machine learning techniques.\nHowever, some aspects must still be revisited and improved. To the best of our\nknowledge, this is the first article that intends to explore and propose novel\ngait biometric recognition systems based on Transformers, which currently\nobtain state-of-the-art performance in many applications. Several\nstate-of-the-art architectures (Vanilla, Informer, Autoformer, Block-Recurrent\nTransformer, and THAT) are considered in the experimental framework. In\naddition, new configurations of the Transformers are proposed to further\nincrease the performance. Experiments are carried out using the two popular\npublic databases whuGAIT and OU-ISIR. The results achieved prove the high\nability of the proposed Transformer, outperforming state-of-the-art CNN and RNN\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delgado_Santos_P/0/1/0/all/0/1\">Paula Delgado-Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guest_R/0/1/0/all/0/1\">Richard Guest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deravi_F/0/1/0/all/0/1\">Farzin Deravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Bird Species Recognition by Learning from Field Guides. (arXiv:2206.01466v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01466","description":"<p>We exploit field guides to learn bird species recognition, in particular\nzero-shot recognition of unseen species. The illustrations contained in field\nguides deliberately focus on discriminative properties of a species, and can\nserve as side information to transfer knowledge from seen to unseen classes. We\nstudy two approaches: (1) a contrastive encoding of illustrations that can be\nfed into zero-shot learning schemes; and (2) a novel method that leverages the\nfact that illustrations are also images and as such structurally more similar\nto photographs than other kinds of side information. Our results show that\nillustrations from field guides, which are readily available for a wide range\nof species, are indeed a competitive source of side information. On the\niNaturalist2021 subset, we obtain a harmonic mean from 749 seen and 739 unseen\nclasses greater than $45\\%$ (@top-10) and $15\\%$ (@top-1). Which shows that\nfield guides are a valuable option for challenging real-world scenarios with\nmany species.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Andr&#xe9;s C. Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daudt_R/0/1/0/all/0/1\">Rodrigo Caye Daudt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan D. Wegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transfer-based Targeted Adversarial Perturbations against Real-World Computer Vision Systems based on Human Judgments. (arXiv:2206.01467v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01467","description":"<p>Computer vision systems are remarkably vulnerable to adversarial\nperturbations. Transfer-based adversarial images are generated on one (source)\nsystem and used to attack another (target) system. In this paper, we take the\nfirst step to investigate transfer-based targeted adversarial images in a\nrealistic scenario where the target system is trained on some private data with\nits inventory of semantic labels not publicly available. Our main contributions\ninclude an extensive human-judgment-based evaluation of attack success on the\nGoogle Cloud Vision API and additional analysis of the different behaviors of\nGoogle Cloud Vision in face of original images vs. adversarial images.\nResources are publicly available at\n\\url{https://github.com/ZhengyuZhao/Targeted-Tansfer/blob/main/google_results.zip}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhengyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_N/0/1/0/all/0/1\">Nga Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_M/0/1/0/all/0/1\">Martha Larson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributional loss for convolutional neural network regression and application to GNSS multi-path estimation. (arXiv:2206.01473v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01473","description":"<p>Convolutional Neural Network (CNN) have been widely used in image\nclassification. Over the years, they have also benefited from various\nenhancements and they are now considered as state of the art techniques for\nimage like data. However, when they are used for regression to estimate some\nfunction value from images, fewer recommendations are available. In this study,\na novel CNN regression model is proposed. It combines convolutional neural\nlayers to extract high level features representations from images with a soft\nlabelling technique. More specifically, as the deep regression task is\nchallenging, the idea is to account for some uncertainty in the targets that\nare seen as distributions around their mean. The estimations are carried out by\nthe model in the form of distributions. Building from earlier work, a specific\nhistogram loss function based on the Kullback-Leibler (KL) divergence is\napplied during training. The model takes advantage of the CNN feature\nrepresentation and is able to carry out estimation from multi-channel input\nimages. To assess and illustrate the technique, the model is applied to Global\nNavigation Satellite System (GNSS) multi-path estimation where multi-path\nsignal parameters have to be estimated from correlator output images from the I\nand Q channels. The multi-path signal delay, magnitude, Doppler shift frequency\nand phase parameters are estimated from synthetically generated datasets of\nsatellite signals. Experiments are conducted under various receiving conditions\nand various input images resolutions to test the estimation performances\nquality and robustness. The results show that the proposed soft labelling CNN\ntechnique using distributional loss outperforms classical CNN regression under\nall conditions. Furthermore, the extra learning performance achieved by the\nmodel allows the reduction of input image resolution from 80x80 down to 40x40\nor sometimes 20x20.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_T/0/1/0/all/0/1\">Thomas Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blais_A/0/1/0/all/0/1\">Antoine Blais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couellan_N/0/1/0/all/0/1\">Nicolas Cou&#xeb;llan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_C/0/1/0/all/0/1\">Christian Ruiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOv5s-GTB: light-weighted and improved YOLOv5s for bridge crack detection. (arXiv:2206.01498v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01498","description":"<p>In response to the situation that the conventional bridge crack manual\ndetection method has a large amount of human and material resources wasted,\nthis study is aimed to propose a light-weighted, high-precision, deep\nlearning-based bridge apparent crack recognition model that can be deployed in\nmobile devices' scenarios. In order to enhance the performance of YOLOv5,\nfirstly, the data augmentation methods are supplemented, and then the YOLOv5\nseries algorithm is trained to select a suitable basic framework. The YOLOv5s\nis identified as the basic framework for the light-weighted crack detection\nmodel through experiments for comparison and validation.By replacing the\ntraditional DarkNet backbone network of YOLOv5s with GhostNet backbone network,\nintroducing Transformer multi-headed self-attention mechanism and\nbi-directional feature pyramid network (BiFPN) to replace the commonly used\nfeature pyramid network, the improved model not only has 42% fewer parameters\nand faster inference response, but also significantly outperforms the original\nmodel in terms of accuracy and mAP (8.5% and 1.1% improvement, respectively).\nLuckily each improved part has a positive impact on the result. This paper\nprovides a feasible idea to establish a digital operation management system in\nthe field of highway and bridge in the future and to implement the whole life\ncycle structure health monitoring of civil infrastructure in China.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiqiang_X/0/1/0/all/0/1\">Xiao Ruiqiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly detection in surveillance videos using transformer based attention model. (arXiv:2206.01524v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01524","description":"<p>Surveillance footage can catch a wide range of realistic anomalies. This\nresearch suggests using a weakly supervised strategy to avoid annotating\nanomalous segments in training videos, which is time consuming. In this\napproach only video level labels are used to obtain frame level anomaly scores.\nWeakly supervised video anomaly detection (WSVAD) suffers from the wrong\nidentification of abnormal and normal instances during the training process.\nTherefore it is important to extract better quality features from the available\nvideos. WIth this motivation, the present paper uses better quality\ntransformer-based features named Videoswin Features followed by the attention\nlayer based on dilated convolution and self attention to capture long and short\nrange dependencies in temporal domain. This gives us a better understanding of\navailable videos. The proposed framework is validated on real-world dataset\ni.e. ShanghaiTech Campus dataset which results in competitive performance than\ncurrent state-of-the-art methods. The model and the code are available at\nhttps://github.com/kapildeshpande/Anomaly-Detection-in-Surveillance-Videos\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_K/0/1/0/all/0/1\">Kapil Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01612","description":"<p>We introduce OmniXAI, an open-source Python library of eXplainable AI (XAI),\nwhich offers omni-way explainable AI capabilities and various interpretable\nmachine learning techniques to address the pain points of understanding and\ninterpreting the decisions made by machine learning (ML) in practice. OmniXAI\naims to be a one-stop comprehensive library that makes explainable AI easy for\ndata scientists, ML researchers and practitioners who need explanation for\nvarious types of data, models and explanation methods at different stages of ML\nprocess (data exploration, feature engineering, model development, evaluation,\nand decision-making, etc). In particular, our library includes a rich family of\nexplanation methods integrated in a unified interface, which supports multiple\ndata types (tabular data, images, texts, time-series), multiple types of ML\nmodels (traditional ML in Scikit-learn and deep learning models in\nPyTorch/TensorFlow), and a range of diverse explanation methods including\n\"model-specific\" and \"model-agnostic\" ones (such as feature-attribution\nexplanation, counterfactual explanation, gradient-based explanation, etc). For\npractitioners, the library provides an easy-to-use unified interface to\ngenerate the explanations for their applications by only writing a few lines of\ncodes, and also a GUI dashboard for visualization of different explanations for\nmore insights about decisions. In this technical report, we present OmniXAI's\ndesign principles, system architectures, and major functionalities, and also\ndemonstrate several example use cases across different types of data, tasks,\nand models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenzhuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning for Interpretable, Feature-Preserving Circuits in CNNs. (arXiv:2206.01627v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01627","description":"<p>Deep convolutional neural networks are a powerful model class for a range of\ncomputer vision problems, but it is difficult to interpret the image filtering\nprocess they implement, given their sheer size. In this work, we introduce a\nmethod for extracting 'feature-preserving circuits' from deep CNNs, leveraging\nmethods from saliency-based neural network pruning. These circuits are modular\nsub-functions, embedded within the network, containing only a subset of\nconvolutional kernels relevant to a target feature. We compare the efficacy of\n3 saliency-criteria for extracting these sparse circuits. Further, we show how\n'sub-feature' circuits can be extracted, that preserve a feature's responses to\nparticular images, dividing the feature into even sparser filtering processes.\nWe also develop a tool for visualizing 'circuit diagrams', which render the\nentire image filtering process implemented by circuits in a parsable format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamblin_C/0/1/0/all/0/1\">Chris Hamblin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konkle_T/0/1/0/all/0/1\">Talia Konkle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_G/0/1/0/all/0/1\">George Alvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning with Neural Radiance Fields. (arXiv:2206.01634v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01634","description":"<p>It is a long-standing problem to find effective representations for training\nreinforcement learning (RL) agents. This paper demonstrates that learning state\nrepresentations with supervision from Neural Radiance Fields (NeRFs) can\nimprove the performance of RL compared to other learned representations or even\nlow-dimensional, hand-engineered state information. Specifically, we propose to\ntrain an encoder that maps multiple image observations to a latent space\ndescribing the objects in the scene. The decoder built from a\nlatent-conditioned NeRF serves as the supervision signal to learn the latent\nspace. An RL algorithm then operates on the learned latent space as its state\nrepresentation. We call this NeRF-RL. Our experiments indicate that NeRF as\nsupervision leads to a latent space better suited for the downstream RL tasks\ninvolving robotic object manipulations like hanging mugs on hooks, pushing\nobjects, or opening doors. Video: https://dannydriess.github.io/nerf-rl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1\">Danny Driess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_I/0/1/0/all/0/1\">Ingmar Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1\">Marc Toussaint</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mirror modular cloning and fast quantum associative retrieval. (arXiv:2206.01644v1 [quant-ph])","link":"http://arxiv.org/abs/2206.01644","description":"<p>We show that a quantum state can be perfectly cloned up to global mirroring\nwith a unitary transformation that depends on one single parameter. We then\nshow that this is equivalent to \"perfect\" cloning for quantum associative\nmemories which, as a consequence efficiently hold exponentially more\ninformation than their classical counterparts. Finally, we present a quantum\nassociative retrieval algorithm which can correct corrupted inputs and is\nexponentially faster than the Grover algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Diamantini_M/0/1/0/all/0/1\">M. C. Diamantini</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Trugenberger_C/0/1/0/all/0/1\">C. A. Trugenberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Positive Sampling for Contrastive Learning with Kernel. (arXiv:2206.01646v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01646","description":"<p>Data augmentation is a crucial component in unsupervised contrastive learning\n(CL). It determines how positive samples are defined and, ultimately, the\nquality of the representation. While efficient augmentations have been found\nfor standard vision datasets, such as ImageNet, it is still an open problem in\nother applications, such as medical imaging, or in datasets with easy-to-learn\nbut irrelevant imaging features. In this work, we propose a new way to define\npositive samples using kernel theory along with a novel loss called decoupled\nuniformity. We propose to integrate prior information, learnt from generative\nmodels or given as auxiliary attributes, into contrastive learning, to make it\nless dependent on data augmentation. We draw a connection between contrastive\nlearning and the conditional mean embedding theory to derive tight bounds on\nthe downstream classification loss. In an unsupervised setting, we empirically\ndemonstrate that CL benefits from generative models, such as VAE and GAN, to\nless rely on data augmentations. We validate our framework on vision datasets\nincluding CIFAR10, CIFAR100, STL10 and ImageNet100 and a brain MRI dataset. In\nthe weakly supervised setting, we demonstrate that our formulation provides\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dufumier_B/0/1/0/all/0/1\">Benoit Dufumier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbano_C/0/1/0/all/0/1\">Carlo Alberto Barbano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louiset_R/0/1/0/all/0/1\">Robin Louiset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchesnay_E/0/1/0/all/0/1\">Edouard Duchesnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D'ARTAGNAN: Counterfactual Video Generation. (arXiv:2206.01651v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01651","description":"<p>Causally-enabled machine learning frameworks could help clinicians to\nidentify the best course of treatments by answering counterfactual questions.\nWe explore this path for the case of echocardiograms by looking into the\nvariation of the Left Ventricle Ejection Fraction, the most essential clinical\nmetric gained from these examinations. We combine deep neural networks, twin\ncausal networks and generative adversarial methods for the first time to build\nD'ARTAGNAN (Deep ARtificial Twin-Architecture GeNerAtive Networks), a novel\ncausal generative model. We demonstrate the soundness of our approach on a\nsynthetic dataset before applying it to cardiac ultrasound videos by answering\nthe question: \"What would this echocardiogram look like if the patient had a\ndifferent ejection fraction?\". To do so, we generate new ultrasound videos,\nretaining the video style and anatomy of the original patient, with variations\nof the Ejection Fraction conditioned on a given input. We achieve an SSIM score\nof 0.79 and an R2 score of 0.51 on the counterfactual videos. Code and models\nare available at https://github.com/HReynaud/dartagnan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reynaud_H/0/1/0/all/0/1\">Hadrien Reynaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dombrowski_M/0/1/0/all/0/1\">Mischa Dombrowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Ciar&#xe1;n Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeson_P/0/1/0/all/0/1\">Paul Leeson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metrics reloaded: Pitfalls and recommendations for image analysis validation. (arXiv:2206.01653v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01653","description":"<p>The field of automatic biomedical image analysis crucially depends on robust\nand meaningful performance metrics for algorithm validation. Current metric\nusage, however, is often ill-informed and does not reflect the underlying\ndomain interest. Here, we present a comprehensive framework that guides\nresearchers towards choosing performance metrics in a problem-aware manner.\nSpecifically, we focus on biomedical image analysis problems that can be\ninterpreted as a classification task at image, object or pixel level. The\nframework first compiles domain interest-, target structure-, data set- and\nalgorithm output-related properties of a given problem into a problem\nfingerprint, while also mapping it to the appropriate problem category, namely\nimage-level classification, semantic segmentation, instance segmentation, or\nobject detection. It then guides users through the process of selecting and\napplying a set of appropriate validation metrics while making them aware of\npotential pitfalls related to individual choices. In this paper, we describe\nthe current status of the Metrics Reloaded recommendation framework, with the\ngoal of obtaining constructive feedback from the image analysis community. The\ncurrent version has been developed within an international consortium of more\nthan 60 image analysis experts and will be made openly available as a\nuser-friendly toolkit after community-driven optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulou_E/0/1/0/all/0/1\">Evangelia Christodoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozubek_M/0/1/0/all/0/1\">Michal Kozubek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1\">Mauricio Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1\">Manuel Wiesenfarth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_M/0/1/0/all/0/1\">Michael Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavur_A/0/1/0/all/0/1\">A. Emre Kavur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radsch_T/0/1/0/all/0/1\">Tim R&#xe4;dsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acion_L/0/1/0/all/0/1\">Laura Acion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonelli_M/0/1/0/all/0/1\">Michela Antonelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bankhead_P/0/1/0/all/0/1\">Peter Bankhead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benis_A/0/1/0/all/0/1\">Arriel Benis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimini_B/0/1/0/all/0/1\">Beth Cimini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_G/0/1/0/all/0/1\">Gary S. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1\">Michael M. Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huisman_M/0/1/0/all/0/1\">Merel Huisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannin_P/0/1/0/all/0/1\">Pierre Jannin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1\">Charles E. Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karargyris_A/0/1/0/all/0/1\">Alexandros Karargyris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes Kenngott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1\">Annette Kopp-Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreshuk_A/0/1/0/all/0/1\">Anna Kreshuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litjens_G/0/1/0/all/0/1\">Geert Litjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus Maier-Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattson_P/0/1/0/all/0/1\">Peter Mattson</a>, et al. (21 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification via Retinal Vessels Combining LBP and HOG. (arXiv:2206.01658v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01658","description":"<p>With development of information technology and necessity for high security,\nusing different identification methods has become very important. Each\nbiometric feature has its own advantages and disadvantages and choosing each of\nthem depends on our usage. Retinal scanning is a bio scale method for\nidentification. The retina is composed of vessels and optical disk. The vessels\ndistribution pattern is one the remarkable retinal identification methods. In\nthis paper, a new approach is presented for identification via retinal images\nusing LBP and hog methods. In the proposed method, it will be tried to separate\nthe retinal vessels accurately via machine vision techniques which will have\ngood sustainability in rotation and size change. HOG-based or LBP-based methods\nor their combination can be used for separation and also HSV color space can be\nused too. Having extracted the features, the similarity criteria can be used\nfor identification. The implementation of proposed method and its comparison\nwith one of the newly-presented methods in this area shows better performance\nof the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noori_A/0/1/0/all/0/1\">Ali Noori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-Content Disentanglement in Language-Image Pretraining Representations for Zero-Shot Sketch-to-Image Synthesis. (arXiv:2206.01661v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01661","description":"<p>In this work, we propose and validate a framework to leverage language-image\npretraining representations for training-free zero-shot sketch-to-image\nsynthesis. We show that disentangled content and style representations can be\nutilized to guide image generators to employ them as sketch-to-image generators\nwithout (re-)training any parameters. Our approach for disentangling style and\ncontent entails a simple method consisting of elementary arithmetic assuming\ncompositionality of information in representations of input sketches. Our\nresults demonstrate that this approach is competitive with state-of-the-art\ninstance-level open-domain sketch-to-image models, while only depending on\npretrained off-the-shelf models and a fraction of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuiderveld_J/0/1/0/all/0/1\">Jan Zuiderveld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric Video-Language Pretraining. (arXiv:2206.01670v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01670","description":"<p>Video-Language Pretraining (VLP), aiming to learn transferable representation\nto advance a wide range of video-text downstream tasks, has recently received\nincreasing attention. Dominant works that achieve strong performance rely on\nlarge-scale, 3rd-person video-text datasets, such as HowTo100M. In this work,\nwe exploit the recently released Ego4D dataset to pioneer Egocentric VLP along\nthree directions. (i) We create EgoClip, a 1st-person video-text pretraining\ndataset comprising 3.8M clip-text pairs well-chosen from Ego4D, covering a\nlarge variety of human daily activities. (ii) We propose a novel pretraining\nobjective, dubbed as EgoNCE, which adapts video-text contrastive learning to\negocentric domain by mining egocentric-aware positive and negative samples.\n(iii) We introduce EgoMCQ, a development benchmark that is close to EgoClip and\nhence can support effective validation and fast exploration of our design\ndecisions regarding EgoClip and EgoNCE. Furthermore, we demonstrate strong\nperformance on five egocentric downstream tasks across three datasets:\nvideo-text retrieval on EPIC-KITCHENS-100; action recognition on Charades-Ego;\nand natural language query, moment query, and object state change\nclassification on Ego4D challenge benchmarks. The dataset and code will be\navailable at https://github.com/showlab/EgoVLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Qinghong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldan_M/0/1/0/all/0/1\">Mattia Soldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_M/0/1/0/all/0/1\">Michael Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_E/0/1/0/all/0/1\">Eric Zhongcong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Rongcheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weijie Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Kernel Selection for Improved Generalization and Memory Efficiency in Meta-learning. (arXiv:2206.01690v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01690","description":"<p>Gradient based meta-learning methods are prone to overfit on the\nmeta-training set, and this behaviour is more prominent with large and complex\nnetworks. Moreover, large networks restrict the application of meta-learning\nmodels on low-power edge devices. While choosing smaller networks avoid these\nissues to a certain extent, it affects the overall generalization leading to\nreduced performance. Clearly, there is an approximately optimal choice of\nnetwork architecture that is best suited for every meta-learning problem,\nhowever, identifying it beforehand is not straightforward. In this paper, we\npresent MetaDOCK, a task-specific dynamic kernel selection strategy for\ndesigning compressed CNN models that generalize well on unseen tasks in\nmeta-learning. Our method is based on the hypothesis that for a given set of\nsimilar tasks, not all kernels of the network are needed by each individual\ntask. Rather, each task uses only a fraction of the kernels, and the selection\nof the kernels per task can be learnt dynamically as a part of the inner update\nsteps. MetaDOCK compresses the meta-model as well as the task-specific inner\nmodels, thus providing significant reduction in model size for each task, and\nthrough constraining the number of active kernels for every task, it implicitly\nmitigates the issue of meta-overfitting. We show that for the same inference\nbudget, pruned versions of large CNN models obtained using our approach\nconsistently outperform the conventional choices of CNN models. MetaDOCK\ncouples well with popular meta-learning approaches such as iMAML. The efficacy\nof our method is validated on CIFAR-fs and mini-ImageNet datasets, and we have\nobserved that our approach can provide improvements in model accuracy of up to\n2% on standard meta-learning benchmark, while reducing the model size by more\nthan 75%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1\">Arnav Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_R/0/1/0/all/0/1\">Rishabh Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamba_U/0/1/0/all/0/1\">Udbhav Bamba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak K. Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Obfuscation Checklist Test Gives a False Sense of Security. (arXiv:2206.01705v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01705","description":"<p>One popular group of defense techniques against adversarial attacks is based\non injecting stochastic noise into the network. The main source of robustness\nof such stochastic defenses however is often due to the obfuscation of the\ngradients, offering a false sense of security. Since most of the popular\nadversarial attacks are optimization-based, obfuscated gradients reduce their\nattacking ability, while the model is still susceptible to stronger or\nspecifically tailored adversarial attacks. Recently, five characteristics have\nbeen identified, which are commonly observed when the improvement in robustness\nis mainly caused by gradient obfuscation. It has since become a trend to use\nthese five characteristics as a sufficient test, to determine whether or not\ngradient obfuscation is the main source of robustness. However, these\ncharacteristics do not perfectly characterize all existing cases of gradient\nobfuscation, and therefore can not serve as a basis for a conclusive test. In\nthis work, we present a counterexample, showing this test is not sufficient for\nconcluding that gradient obfuscation is not the main cause of improvements in\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nikola Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01714","description":"<p>Large text-guided diffusion models, such as DALLE-2, are able to generate\nstunning photorealistic images given natural language descriptions. While such\nmodels are highly flexible, they struggle to understand the composition of\ncertain concepts, such as confusing the attributes of different objects or\nrelations between objects. In this paper, we propose an alternative structured\napproach for compositional generation using diffusion models. An image is\ngenerated by composing a set of diffusion models, with each of them modeling a\ncertain component of the image. To do this, we interpret diffusion models as\nenergy-based models in which the data distributions defined by the energy\nfunctions may be explicitly combined. The proposed method can generate scenes\nat test time that are substantially more complex than those seen in training,\ncomposing sentence descriptions, object relations, human facial attributes, and\neven generalizing to new combinations that are rarely seen in the real world.\nWe further illustrate how our approach may be used to compose pre-trained\ntext-guided diffusion models and generate photorealistic images containing all\nthe details described in the input descriptions, including the binding of\ncertain object attributes that have been shown difficult for DALLE-2. These\nresults point to the effectiveness of the proposed method in promoting\nstructured generalization for visual generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge. (arXiv:2206.01718v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01718","description":"<p>The Visual Question Answering (VQA) task aspires to provide a meaningful\ntestbed for the development of AI models that can jointly reason over visual\nand natural language inputs. Despite a proliferation of VQA datasets, this goal\nis hindered by a set of common limitations. These include a reliance on\nrelatively simplistic questions that are repetitive in both concepts and\nlinguistic structure, little world knowledge needed outside of the paired\nimage, and limited reasoning required to arrive at the correct answer. We\nintroduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about\n25K questions requiring a broad base of commonsense and world knowledge to\nanswer. In contrast to the existing knowledge-based VQA datasets, the questions\ngenerally cannot be answered by simply querying a knowledge base, and instead\nrequire some form of commonsense reasoning about the scene depicted in the\nimage. We demonstrate the potential of this new dataset through a detailed\nanalysis of its contents and baseline performance measurements over a variety\nof state-of-the-art vision-language models. Project page:\n<a href=\"http://a-okvqa.allenai.org/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_D/0/1/0/all/0/1\">Dustin Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marino_K/0/1/0/all/0/1\">Kenneth Marino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the \"Video\" in Video-Language Understanding. (arXiv:2206.01720v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01720","description":"<p>What makes a video task uniquely suited for videos, beyond what can be\nunderstood from a single image? Building on recent progress in self-supervised\nimage-language models, we revisit this question in the context of video and\nlanguage tasks. We propose the atemporal probe (ATP), a new model for\nvideo-language analysis which provides a stronger bound on the baseline\naccuracy of multimodal models constrained by image-level understanding. By\napplying this model to standard discriminative video and language tasks, such\nas video question answering and text-to-video retrieval, we characterize the\nlimitations and potential of current video-language benchmarks. We find that\nunderstanding of event temporality is often not necessary to achieve strong or\nstate-of-the-art performance, even compared with recent large-scale\nvideo-language models and in contexts intended to benchmark deeper video-level\nunderstanding. We also demonstrate how ATP can improve both video-language\ndataset and model design. We describe a technique for leveraging ATP to better\ndisentangle dataset subsets with a higher concentration of temporally\nchallenging data, improving benchmarking efficacy for causal and temporal\nunderstanding. Further, we show that effectively integrating ATP into full\nvideo-level temporal models can improve efficiency and state-of-the-art\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyzaguirre_C/0/1/0/all/0/1\">Crist&#xf3;bal Eyzaguirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1\">Juan Carlos Niebles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNAKE: Shape-aware Neural 3D Keypoint Field. (arXiv:2206.01724v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01724","description":"<p>Detecting 3D keypoints from point clouds is important for shape\nreconstruction, while this work investigates the dual question: can shape\nreconstruction benefit 3D keypoint detection? Existing methods either seek\nsalient features according to statistics of different orders or learn to\npredict keypoints that are invariant to transformation. Nevertheless, the idea\nof incorporating shape reconstruction into 3D keypoint detection is\nunder-explored. We argue that this is restricted by former problem\nformulations. To this end, a novel unsupervised paradigm named SNAKE is\nproposed, which is short for shape-aware neural 3D keypoint field. Similar to\nrecent coordinate-based radiance or distance field, our network takes 3D\ncoordinates as inputs and predicts implicit shape indicators and keypoint\nsaliency simultaneously, thus naturally entangling 3D keypoint detection and\nshape reconstruction. We achieve superior performance on various public\nbenchmarks, including standalone object datasets ModelNet40, KeypointNet, SMPL\nmeshes and scene-level datasets 3DMatch and Redwood. Intrinsic shape awareness\nbrings several advantages as follows. (1) SNAKE generates 3D keypoints\nconsistent with human semantic annotation, even without such supervision. (2)\nSNAKE outperforms counterparts in terms of repeatability, especially when the\ninput point clouds are down-sampled. (3) the generated keypoints allow accurate\ngeometric registration, notably in a zero-shot setting. Codes are available at\nhttps://github.com/zhongcl-thu/SNAKE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Chengliang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_P/0/1/0/all/0/1\">Peixing You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoxue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1\">Xiaodong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GASP, a generalized framework for agglomerative clustering of signed graphs and its application to Instance Segmentation. (arXiv:1906.11713v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1906.11713","description":"<p>We propose a theoretical framework that generalizes simple and fast\nalgorithms for hierarchical agglomerative clustering to weighted graphs with\nboth attractive and repulsive interactions between the nodes. This framework\ndefines GASP, a Generalized Algorithm for Signed graph Partitioning, and allows\nus to explore many combinations of different linkage criteria and cannot-link\nconstraints. We prove the equivalence of existing clustering methods to some of\nthose combinations and introduce new algorithms for combinations that have not\nbeen studied before. We study both theoretical and empirical properties of\nthese combinations and prove that some of these define an ultrametric on the\ngraph. We conduct a systematic comparison of various instantiations of GASP on\na large variety of both synthetic and existing signed clustering problems, in\nterms of accuracy but also efficiency and robustness to noise. Lastly, we show\nthat some of the algorithms included in our framework, when combined with the\npredictions from a CNN model, result in a simple bottom-up instance\nsegmentation pipeline. Going all the way from pixels to final segments with a\nsimple procedure, we achieve state-of-the-art accuracy on the CREMI 2016 EM\nsegmentation benchmark without requiring domain-specific superpixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bailoni_A/0/1/0/all/0/1\">Alberto Bailoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pape_C/0/1/0/all/0/1\">Constantin Pape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutsch_N/0/1/0/all/0/1\">Nathan H&#xfc;tsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_S/0/1/0/all/0/1\">Steffen Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beier_T/0/1/0/all/0/1\">Thorsten Beier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreshuk_A/0/1/0/all/0/1\">Anna Kreshuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamprecht_F/0/1/0/all/0/1\">Fred A. Hamprecht</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmented Equivariant Attention Networks for Microscopy Image Reconstruction. (arXiv:2011.03633v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.03633","description":"<p>It is time-consuming and expensive to take high-quality or high-resolution\nelectron microscopy (EM) and fluorescence microscopy (FM) images. Taking these\nimages could be even invasive to samples and may damage certain subtleties in\nthe samples after long or intense exposures, often necessary for achieving\nhigh-quality or high resolution in the first place. Advances in deep learning\nenable us to perform image-to-image transformation tasks for various types of\nmicroscopy image reconstruction, computationally producing high-quality images\nfrom the physically acquired low-quality ones. When training image-to-image\ntransformation models on pairs of experimentally acquired microscopy images,\nprior models suffer from performance loss due to their inability to capture\ninter-image dependencies and common features shared among images. Existing\nmethods that take advantage of shared features in image classification tasks\ncannot be properly applied to image reconstruction tasks because they fail to\npreserve the equivariance property under spatial permutations, something\nessential in image-to-image transformation. To address these limitations, we\npropose the augmented equivariant attention networks (AEANets) with better\ncapability to capture inter-image dependencies, while preserving the\nequivariance property. The proposed AEANets captures inter-image dependencies\nand shared features via two augmentations on the attention mechanism, which are\nthe shared references and the batch-aware attention during training. We\ntheoretically derive the equivariance property of the proposed augmented\nattention model and experimentally demonstrate its consistent superiority in\nboth quantitative and visual results over the baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yaochen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images. (arXiv:2104.12137v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12137","description":"<p>The fully convolutional network (FCN) with an encoder-decoder architecture\nhas been the standard paradigm for semantic segmentation. The encoder-decoder\narchitecture utilizes an encoder to capture multilevel feature maps, which are\nincorporated into the final prediction by a decoder. As the context is crucial\nfor precise segmentation, tremendous effort has been made to extract such\ninformation in an intelligent fashion, including employing dilated/atrous\nconvolutions or inserting attention modules. However, these endeavors are all\nbased on the FCN architecture with ResNet or other backbones, which cannot\nfully exploit the context from the theoretical concept. By contrast, we\nintroduce the Swin Transformer as the backbone to extract the context\ninformation and design a novel decoder of densely connected feature aggregation\nmodule (DCFAM) to restore the resolution and produce the segmentation map. The\nexperimental results on two remotely sensed semantic segmentation datasets\ndemonstrate the effectiveness of the proposed scheme.Code is available at\nhttps://github.com/WangLibo1995/GeoSeg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shenghui Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Volumetric Image Segmentation with Deformed Templates. (arXiv:2106.03987v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03987","description":"<p>There are many approaches to weakly-supervised training of networks to\nsegment 2D images. By contrast, existing approaches to segmenting volumetric\nimages rely on full-supervision of a subset of 2D slices of the 3D volume. We\npropose an approach to volume segmentation that is truly weakly-supervised in\nthe sense that we only need to provide a sparse set of 3D points on the surface\nof target objects instead of detailed 2D masks. We use the 3D points to deform\na 3D template so that it roughly matches the target object outlines and we\nintroduce an architecture that exploits the supervision it provides to train a\nnetwork to find accurate boundaries. We evaluate our approach on Computed\nTomography (CT), Magnetic Resonance Imagery (MRI) and Electron Microscopy (EM)\nimage datasets and show that it substantially reduces the required amount of\neffort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1\">Udaranga Wickramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_P/0/1/0/all/0/1\">Patrick M. Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mian Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source Data-Free Cross-Domain Semantic Segmentation: Align, Teach and Propagate. (arXiv:2106.11653v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11653","description":"<p>Benefiting from considerable pixel-level annotations collected from a\nspecific situation (source), the trained semantic segmentation model performs\nquite well but fails in a new situation (target). To mitigate the domain gap,\nprevious cross-domain semantic segmentation methods always assume the\nco-existence of source data and target data during domain alignment. However,\naccessing source data in the real scenario may raise privacy concerns and\nviolate intellectual property. To tackle this problem, we focus on an\ninteresting and challenging cross-domain semantic segmentation task where only\nthe trained source model is provided to the target domain. Specifically, we\npropose a unified framework called \\textbf{ATP}, which consists of three\nschemes, i.e., feature \\textbf{A}lignment, bidirectional \\textbf{T}eaching, and\ninformation \\textbf{P}ropagation. First, considering explicit alignment is\ninfeasible due to no source data, we devise a curriculum-style entropy\nminimization objective to implicitly align the target features with unseen\nsource features via the provided source model. Second, besides positive pseudo\nlabels in vanilla self-training, we introduce negative pseudo labels to this\nfield and develop a bidirectional self-training strategy to enhance the\nrepresentation learning in the target domain. It is the first work to use\nnegative pseudo labels during self-training for domain adaptation. Finally, the\ninformation propagation scheme is employed to further reduce the intra-domain\ndiscrepancy within the target domain via pseudo-semi-supervised learning, which\nis the first step by providing a simple and effective post-process for the\ndomain adaptation field. Furthermore, we also extend the proposed to the more\nchallenging black-box source-model scenario where only the source model's\nprediction is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiband VAE: Latent Space Alignment for Knowledge Consolidation in Continual Learning. (arXiv:2106.12196v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.12196","description":"<p>We propose a new method for unsupervised generative continual learning\nthrough realignment of Variational Autoencoder's latent space. Deep generative\nmodels suffer from catastrophic forgetting in the same way as other neural\nstructures. Recent generative continual learning works approach this problem\nand try to learn from new data without forgetting previous knowledge. However,\nthose methods usually focus on artificial scenarios where examples share almost\nno similarity between subsequent portions of data - an assumption not realistic\nin the real-life applications of continual learning. In this work, we identify\nthis limitation and posit the goal of generative continual learning as a\nknowledge accumulation task. We solve it by continuously aligning latent\nrepresentations of new data that we call bands in additional latent space where\nexamples are encoded independently of their source task. In addition, we\nintroduce a method for controlled forgetting of past data that simplifies this\nprocess. On top of the standard continual learning benchmarks, we propose a\nnovel challenging knowledge consolidation scenario and show that the proposed\napproach outperforms state-of-the-art by up to twofold across all experiments\nand the additional real-life evaluation. To our knowledge, Multiband VAE is the\nfirst method to show forward and backward knowledge transfer in generative\ncontinual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deja_K/0/1/0/all/0/1\">Kamil Deja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wawrzynski_P/0/1/0/all/0/1\">Pawe&#x142; Wawrzy&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masarczyk_W/0/1/0/all/0/1\">Wojciech Masarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marczak_D/0/1/0/all/0/1\">Daniel Marczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Meets Convolution: A Bilateral Awareness Network for Semantic Segmentation of Very Fine Resolution Urban Scene Images. (arXiv:2106.12413v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12413","description":"<p>Semantic segmentation from very fine resolution (VFR) urban scene images\nplays a significant role in several application scenarios including autonomous\ndriving, land cover classification, and urban planning, etc. However, the\ntremendous details contained in the VFR image, especially the considerable\nvariations in scale and appearance of objects, severely limit the potential of\nthe existing deep learning approaches. Addressing such issues represents a\npromising research field in the remote sensing community, which paves the way\nfor scene-level landscape pattern analysis and decision making. In this paper,\nwe propose a Bilateral Awareness Network which contains a dependency path and a\ntexture path to fully capture the long-range relationships and fine-grained\ndetails in VFR images. Specifically, the dependency path is conducted based on\nthe ResT, a novel Transformer backbone with memory-efficient multi-head\nself-attention, while the texture path is built on the stacked convolution\noperation. Besides, using the linear attention mechanism, a feature aggregation\nmodule is designed to effectively fuse the dependency features and texture\nfeatures. Extensive experiments conducted on the three large-scale urban scene\nimage segmentation datasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam\ndataset, and UAVid dataset, demonstrate the effectiveness of our BANet.\nSpecifically, a 64.6% mIoU is achieved on the UAVid dataset. Code is available\nat https://github.com/WangLibo1995/GeoSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Cervical Whole Slide Image Analysis Framework Based on Multi-scale Semantic and Location Deep Features. (arXiv:2106.15113v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15113","description":"<p>Digital gigapixel whole slide image (WSI) is widely used in clinical\ndiagnosis, and automated WSI analysis is key for computer-aided diagnosis.\nCurrently, analyzing the integrated descriptor of probabilities or feature maps\nfrom massive local patches encoded by ResNet classifier is the main manner for\nWSI-level prediction. Feature representations of the sparse and tiny lesion\ncells in cervical slides, however, are still challenging, while the unused\nlocation representations are available to supply the semantics classification.\nThis study designs a novel and efficient framework with a new module InCNet\nconstructed lightweight model YOLCO (You Only Look Cytology Once). It directly\nextracts feature inside the single cell (cluster) instead of the traditional\nway that from image tile with a fixed size. The InCNet (Inline Connection\nNetwork) enriches the multi-scale connectivity without efficiency loss. The\nproposal allows the input size enlarged to megapixel that can stitch the WSI by\nthe average repeats decreased from $10^3\\sim10^4$ to $10^1\\sim10^2$ for\ncollecting features and predictions at two scales. Based on Transformer for\nclassifying the integrated multi-scale multi-task WSI features, the\nexperimental results appear $0.872$ AUC score better than the best conventional\nmodel on our dataset ($n$=2,019) from four scanners. The code is available at\nhttps://github.com/Chrisa142857/You-Only-Look-Cytopathology-Once , where the\ndeployment version has the speed $\\sim$70 s/WSI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shenghua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shaoqun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NanoBatch Privacy: Enabling fast Differentially Private learning on the IPU. (arXiv:2109.12191v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.12191","description":"<p>Differentially private SGD (DPSGD) has recently shown promise in deep\nlearning. However, compared to non-private SGD, the DPSGD algorithm places\ncomputational overheads that can undo the benefit of batching in GPUs.\nMicro-batching is a common method to alleviate this and is fully supported in\nthe TensorFlow Privacy library (TFDP). However, it degrades accuracy. We\npropose NanoBatch Privacy, a lightweight add-on to TFDP to be used on Graphcore\nIPUs by leveraging batch size of 1 (without microbatching) and gradient\naccumulation. This allows us to achieve large total batch sizes with minimal\nimpacts to throughput. Second, we illustrate using Cifar-10 how larger batch\nsizes are not necessarily optimal from a privacy versus utility perspective. On\nImageNet, we achieve more than 15x speedup over TFDP versus 8x A100s and\nsignificant speedups even across libraries such as Opacus. We also provide two\nextensions: 1) DPSGD for pipelined models and 2) per-layer clipping that is 15x\nfaster than the Opacus implementation on 8x A100s. Finally as an application\ncase study, we apply NanoBatch training for use on private Covid-19 chest CT\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Edward H. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krell_M/0/1/0/all/0/1\">Mario Michael Krell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsyplikhin_A/0/1/0/all/0/1\">Alexander Tsyplikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rege_V/0/1/0/all/0/1\">Victoria Rege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colak_E/0/1/0/all/0/1\">Errol Colak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeom_K/0/1/0/all/0/1\">Kristen W. Yeom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D. (arXiv:2109.13410v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.13410","description":"<p>For the last few decades, several major subfields of artificial intelligence\nincluding computer vision, graphics, and robotics have progressed largely\nindependently from each other. Recently, however, the community has realized\nthat progress towards robust intelligent systems such as self-driving cars\nrequires a concerted effort across the different fields. This motivated us to\ndevelop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a\nsuburban driving dataset which comprises richer input modalities, comprehensive\nsemantic instance annotations and accurate localization to facilitate research\nat the intersection of vision, graphics and robotics. For efficient annotation,\nwe created a tool to label 3D scenes with bounding primitives and developed a\nmodel that transfers this information into the 2D image domain, resulting in\nover 150k images and 1B 3D points with coherent semantic instance annotations\nacross 2D and 3D. Moreover, we established benchmarks and baselines for several\ntasks relevant to mobile perception, encompassing problems from computer\nvision, graphics, and robotics on the same dataset, e.g., semantic scene\nunderstanding, novel view synthesis and semantic SLAM. KITTI-360 will enable\nprogress at the intersection of these research areas and thus contribute\ntowards solving one of today's grand challenges: the development of fully\nautonomous self-driving systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction. (arXiv:2111.11215v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11215","description":"<p>We present a super-fast convergence approach to reconstructing the per-scene\nradiance field from a set of images that capture the scene with known poses.\nThis task, which is often applied to novel view synthesis, is recently\nrevolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality\nand flexibility. However, NeRF and its variants require a lengthy training time\nranging from hours to days for a single scene. In contrast, our approach\nachieves NeRF-comparable quality and converges rapidly from scratch in less\nthan 15 minutes with a single GPU. We adopt a representation consisting of a\ndensity voxel grid for scene geometry and a feature voxel grid with a shallow\nnetwork for complex view-dependent appearance. Modeling with explicit and\ndiscretized volume representations is not new, but we propose two simple yet\nnon-trivial techniques that contribute to fast convergence speed and\nhigh-quality output. First, we introduce the post-activation interpolation on\nvoxel density, which is capable of producing sharp surfaces in lower grid\nresolution. Second, direct voxel density optimization is prone to suboptimal\ngeometry solutions, so we robustify the optimization process by imposing\nseveral priors. Finally, evaluation on five inward-facing benchmarks shows that\nour method matches, if not surpasses, NeRF's quality, yet it only takes about\n15 minutes to train from scratch for a new scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection. (arXiv:2111.13336v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13336","description":"<p>In object detection, the detection backbone consumes more than half of the\noverall inference cost. Recent researches attempt to reduce this cost by\noptimizing the backbone architecture with the help of Neural Architecture\nSearch (NAS). However, existing NAS methods for object detection require\nhundreds to thousands of GPU hours of searching, making them impractical in\nfast-paced research and development. In this work, we propose a novel zero-shot\nNAS method to address this issue. The proposed method, named MAE-DET,\nautomatically designs efficient detection backbones via the Maximum Entropy\nPrinciple without training network parameters, reducing the architecture design\ncost to nearly zero yet delivering the state-of-the-art (SOTA) performance.\nUnder the hood, MAE-DET maximizes the differential entropy of detection\nbackbones, leading to a better feature extractor for object detection under the\nsame computational budgets. After merely one GPU day of fully automatic design,\nMAE-DET innovates SOTA detection backbones on multiple detection benchmark\ndatasets with little human intervention. Comparing to ResNet-50 backbone,\nMAE-DET is $+2.0\\%$ better in mAP when using the same amount of\nFLOPs/parameters, and is $1.54$ times faster on NVIDIA V100 at the same mAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTCT: Patches with 3D-Temporal Convolutional Transformer Network for Precipitation Nowcasting. (arXiv:2112.01085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01085","description":"<p>Precipitation nowcasting is to predict the future rainfall intensity over a\nshort period of time, which mainly relies on the prediction of radar echo\nsequences. Though convolutional neural network (CNN) and recurrent neural\nnetwork (RNN) are widely used to generate radar echo frames, they suffer from\ninductive bias (i.e., translation invariance and locality) and seriality,\nrespectively. Recently, Transformer-based methods also gain much attention due\nto the great potential of Transformer structure, whereas short-term\ndependencies and autoregressive characteristic are ignored. In this paper, we\npropose a variant of Transformer named patches with 3D-temporal convolutional\nTransformer network (PTCT), where original frames are split into multiple\npatches to remove the constraint of inductive bias and 3D-temporal convolution\nis employed to capture short-term dependencies efficiently. After training, the\ninference of PTCT is performed in an autoregressive way to ensure the quality\nof generated radar echo frames. To validate our algorithm, we conduct\nexperiments on two radar echo dataset: Radar Echo Guangzhou and HKO-7. The\nexperimental results show that PTCT achieves state-of-the-art (SOTA)\nperformance compared with existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiangrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qifeng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods. (arXiv:2112.04417v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04417","description":"<p>A multitude of explainability methods and associated fidelity performance\nmetrics have been proposed to help better understand how modern AI systems make\ndecisions. However, much of the current work has remained theoretical --\nwithout much consideration for the human end-user. In particular, it is not yet\nknown (1) how useful current explainability methods are in practice for more\nreal-world scenarios and (2) how well associated performance metrics accurately\npredict how much knowledge individual explanations contribute to a human\nend-user trying to understand the inner-workings of the system. To fill this\ngap, we conducted psychophysics experiments at scale to evaluate the ability of\nhuman participants to leverage representative attribution methods for\nunderstanding the behavior of different image classifiers representing three\nreal-world scenarios: identifying bias in an AI system, characterizing the\nvisual strategy it uses for tasks that are too difficult for an untrained\nnon-expert human observer as well as understanding its failure cases. Our\nresults demonstrate that the degree to which individual attribution methods\nhelp human participants better understand an AI system varied widely across\nthese scenarios. This suggests a critical need for the field to move past\nquantitative improvements of current attribution methods towards the\ndevelopment of complementary approaches that provide qualitatively different\nsources of information to human end-users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colin_J/0/1/0/all/0/1\">Julien Colin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1\">Thomas Fel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1\">Remi Cadene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation Detection. (arXiv:2112.08935v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08935","description":"<p>The emergence of powerful image editing software has substantially\nfacilitated digital image tampering, leading to many security issues. Hence, it\nis urgent to identify tampered images and localize tampered regions. Although\nmuch attention has been devoted to image tampering localization in recent\nyears, it is still challenging to perform tampering localization in practical\nforensic applications. The reasons include the difficulty of learning\ndiscriminative representations of tampering traces and the lack of realistic\ntampered images for training. Since Photoshop is widely used for image\ntampering in practice, this paper attempts to address the issue of tampering\nlocalization by focusing on the detection of commonly used editing tools and\noperations in Photoshop. In order to well capture tampering traces, a fully\nconvolutional encoder-decoder architecture is designed, where dense connections\nand dilated convolutions are adopted for achieving better localization\nperformance. In order to effectively train a model in the case of insufficient\ntampered images, we design a training data generation strategy by resorting to\nPhotoshop scripting, which can imitate human manipulations and generate\nlarge-scale training samples. Extensive experimental results show that the\nproposed approach outperforms state-of-the-art competitors when the model is\ntrained with only generated images or fine-tuned with a small amount of\nrealistic tampered images. The proposed method also has good robustness against\nsome common post-processing operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengbo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruohan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Framework to Jointly Compress and Index Remote Sensing Images for Efficient Content-Based Retrieval. (arXiv:2201.06459v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06459","description":"<p>Remote sensing (RS) images are usually stored in compressed format to reduce\nthe storage size of the archives. Thus, existing content-based image retrieval\n(CBIR) systems in RS require decoding images before applying CBIR (which is\ncomputationally demanding in the case of large-scale CBIR problems). To address\nthis problem, in this paper, we present a joint framework that simultaneously\nlearns RS image compression and indexing. Thus, it eliminates the need for\ndecoding RS images before applying CBIR. The proposed framework is made up of\ntwo modules. The first module compresses RS images based on an auto-encoder\narchitecture. The second module produces hash codes with a high discrimination\ncapability by employing soft pairwise, bit-balancing and classification loss\nfunctions. We also introduce a two stage learning strategy with gradient\nmanipulation techniques to obtain image representations that are compatible\nwith both RS image indexing and compression. Experimental results show the\nefficacy of the proposed framework when compared to widely used approaches in\nRS. The code of the proposed framework is available at\nhttps://git.tu-berlin.de/rsim/RS-JCIF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1\">Gencer Sumbul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jun Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madam_N/0/1/0/all/0/1\">Nimisha Thekke Madam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Deep Contrastive Learning via Coordinate-wise Optimization. (arXiv:2201.12680v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12680","description":"<p>We show that Contrastive Learning (CL) under a broad family of loss functions\n(including InfoNCE) has a unified formulation of coordinate-wise optimization\non the network parameter $\\boldsymbol{\\theta}$ and pairwise importance\n$\\alpha$, where the \\emph{max player} $\\boldsymbol{\\theta}$ learns\nrepresentation for contrastiveness, and the \\emph{min player} $\\alpha$ puts\nmore weights on pairs of distinct samples that share similar representations.\nThe resulting formulation, called $\\alpha$-CL, unifies not only various\nexisting contrastive losses, which differ by how sample-pair importance\n$\\alpha$ is constructed, but also is able to extrapolate to give novel\ncontrastive losses beyond popular ones, opening a new avenue of contrastive\nloss design. These novel losses yield comparable (or better) performance on\nCIFAR10 and STL-10 than classic InfoNCE. Furthermore, we also analyze the max\nplayer in detail: we prove that with fixed $\\alpha$, max player is equivalent\nto Principal Component Analysis (PCA) for deep linear network, and almost all\nlocal minima are global and rank-1, recovering optimal PCA solutions. Finally,\nwe extend our analysis on max player to 2-layer ReLU networks, showing that its\nfixed points can have higher ranks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Scene Representation Learning via Reconstruction: A Survey. (arXiv:2202.07135v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.07135","description":"<p>Visual scene representation learning is an important research problem in the\nfield of computer vision. The performance of artificial intelligence systems on\nvision tasks could be improved if more suitable representations are learned for\nvisual scenes. Complex visual scenes are composed of relatively simple visual\nconcepts, and have the property of combinatorial explosion. Compared with\ndirectly representing the entire visual scene, extracting compositional scene\nrepresentations can better cope with the diverse combinations of background and\nobjects. Because compositional scene representations abstract the concept of\nobjects, performing visual scene analysis and understanding based on these\nrepresentations could be easier and more interpretable. Moreover, learning via\nreconstruction can greatly reduce the need for training data annotations.\nTherefore, reconstruction-based compositional scene representation learning has\nimportant research significance. In this survey, we first outline the current\nprogress on this research topic, including development history and\ncategorizations of existing methods from the perspectives of modeling of visual\nscenes and inference of scene representations; then provide benchmarks,\nincluding an open source toolbox to reproduce the benchmark experiments, of\nrepresentative methods that consider the most extensively studied problem\nsetting and form the foundation for other methods; and finally discuss the\nfuture directions of this research topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jinyang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tonglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography. (arXiv:2202.10847v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.10847","description":"<p>Implicit neural representations (INRs) have achieved impressive results for\nscene reconstruction and computer graphics, where their performance has\nprimarily been assessed on reconstruction accuracy. As INRs make their way into\nother domains, where model predictions inform high-stakes decision-making,\nuncertainty quantification of INR inference is becoming critical. To that end,\nwe study a Bayesian reformulation of INRs, UncertaINR, in the context of\ncomputed tomography, and evaluate several Bayesian deep learning\nimplementations in terms of accuracy and calibration. We find that they achieve\nwell-calibrated uncertainty, while retaining accuracy competitive with other\nclassical, INR-based, and CNN-based reconstruction techniques. In contrast to\nthe best-performing prior approaches, UncertaINR does not require a large\ntraining dataset, but only a handful of validation images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisca Vasconcelos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1\">Bobby He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_N/0/1/0/all/0/1\">Nalini Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge Augmentation for Large-Scale Sketch Recognition without Sketches. (arXiv:2202.13164v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13164","description":"<p>This work addresses scaling up the sketch classification task into a large\nnumber of categories. Collecting sketches for training is a slow and tedious\nprocess that has so far precluded any attempts to large-scale sketch\nrecognition. We overcome the lack of training sketch data by exploiting labeled\ncollections of natural images that are easier to obtain. To bridge the domain\ngap we present a novel augmentation technique that is tailored to the task of\nlearning sketch recognition from a training set of natural images.\nRandomization is introduced in the parameters of edge detection and edge\nselection. Natural images are translated to a pseudo-novel domain called\n\"randomized Binary Thin Edges\" (rBTE), which is used as a training domain\ninstead of natural images. The ability to scale up is demonstrated by training\nCNN-based sketch recognition of more than 2.5 times larger number of categories\nthan used previously. For this purpose, a dataset of natural images from 874\ncategories is constructed by combining a number of popular computer vision\ndatasets. The categories are selected to be suitable for sketch recognition. To\nestimate the performance, a subset of 393 categories with sketches is also\ncollected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Efthymiadis_N/0/1/0/all/0/1\">Nikos Efthymiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ondrej Chum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-SIMS: Semi-Parametric Image and Depth Synthesis. (arXiv:2203.03405v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03405","description":"<p>In this paper we present a compositing image synthesis method that generates\nRGB canvases with well aligned segmentation maps and sparse depth maps, coupled\nwith an in-painting network that transforms the RGB canvases into high quality\nRGB images and the sparse depth maps into pixel-wise dense depth maps. We\nbenchmark our method in terms of structural alignment and image quality,\nshowing an increase in mIoU over SOTA by 3.7 percentage points and a highly\ncompetitive FID. Furthermore, we analyse the quality of the generated data as\ntraining data for semantic segmentation and depth completion, and show that our\napproach is more suited for this purpose than other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Musat_V/0/1/0/all/0/1\">Valentina Musat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1\">Daniele De Martini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadd_M/0/1/0/all/0/1\">Matthew Gadd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_P/0/1/0/all/0/1\">Paul Newman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-End Unified Scene Text Detection and Layout Analysis. (arXiv:2203.15143v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15143","description":"<p>Scene text detection and document layout analysis have long been treated as\ntwo separate tasks in different image domains. In this paper, we bring them\ntogether and introduce the task of unified scene text detection and layout\nanalysis. The first hierarchical scene text dataset is introduced to enable\nthis novel research task. We also propose a novel method that is able to\nsimultaneously detect scene text and form text clusters in a unified way.\nComprehensive experiments show that our unified model achieves better\nperformance than multiple well-designed baseline methods. Additionally, this\nmodel achieves state-of-the-art results on multiple scene text detection\ndatasets without the need of complex post-processing. Dataset and code:\nhttps://github.com/google-research-datasets/hiertext and\nhttps://github.com/tensorflow/models/tree/master/official/projects/unified_detector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Shangbang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Siyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panteleev_D/0/1/0/all/0/1\">Dmitry Panteleev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissacco_A/0/1/0/all/0/1\">Alessandro Bissacco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raptis_M/0/1/0/all/0/1\">Michalis Raptis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Document Recognition and Understanding with Dessurt. (arXiv:2203.16618v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16618","description":"<p>We introduce Dessurt, a relatively simple document understanding transformer\ncapable of being fine-tuned on a greater variety of document tasks than prior\nmethods. It receives a document image and task string as input and generates\narbitrary text autoregressively as output. Because Dessurt is an end-to-end\narchitecture that performs text recognition in addition to the document\nunderstanding, it does not require an external recognition model as prior\nmethods do. Dessurt is a more flexible model than prior methods and is able to\nhandle a variety of document domains and tasks. We show that this model is\neffective at 9 different dataset-task combinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_B/0/1/0/all/0/1\">Brian Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morse_B/0/1/0/all/0/1\">Bryan Morse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Bryan Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tensmeyer_C/0/1/0/all/0/1\">Chris Tensmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wigington_C/0/1/0/all/0/1\">Curtis Wigington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1\">Vlad Morariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Visual Prompts for Adapting Large-Scale Models. (arXiv:2203.17274v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17274","description":"<p>We investigate the efficacy of visual prompting to adapt large-scale models\nin vision. Following the recent approach from prompt tuning and adversarial\nreprogramming, we learn a single image perturbation such that a frozen model\nprompted with this perturbation performs a new task. Through comprehensive\nexperiments, we demonstrate that visual prompting is particularly effective for\nCLIP and robust to distribution shift, achieving performance competitive with\nstandard linear probes. We further analyze properties of the downstream\ndataset, prompt design, and output transformation in regard to adaptation\nperformance. The surprising effectiveness of visual prompting provides a new\nperspective on adapting pre-trained models in vision. Code is available at\n<a href=\"http://hjbahng.github.io/visual_prompting\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahng_H/0/1/0/all/0/1\">Hyojin Bahng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanian_A/0/1/0/all/0/1\">Ali Jahanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_S/0/1/0/all/0/1\">Swami Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Powering Finetuning in Few-Shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling. (arXiv:2204.03749v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03749","description":"<p>In recent works, utilizing a deep network trained on meta-training set serves\nas a strong baseline in few-shot learning. In this paper, we move forward to\nrefine novel-class features by finetuning a trained deep network. Finetuning is\ndesigned to focus on reducing biases in novel-class feature distributions,\nwhich we define as two aspects: class-agnostic and class-specific biases.\nClass-agnostic bias is defined as the distribution shifting introduced by\ndomain difference, which we propose Distribution Calibration Module(DCM) to\nreduce. DCM owes good property of eliminating domain difference and fast\nfeature adaptation during optimization. Class-specific bias is defined as the\nbiased estimation using a few samples in novel classes, which we propose\nSelected Sampling(SS) to reduce. Without inferring the actual class\ndistribution, SS is designed by running sampling using proposal distributions\naround support-set samples. By powering finetuning with DCM and SS, we achieve\nstate-of-the-art results on Meta-Dataset with consistent performance boosts\nover ten datasets from different domains. We believe our simple yet effective\nmethod demonstrates its possibility to be applied on practical few-shot\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yutong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding. (arXiv:2204.08129v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08129","description":"<p>Understanding animals' behaviors is significant for a wide range of\napplications. However, existing animal behavior datasets have limitations in\nmultiple aspects, including limited numbers of animal classes, data samples and\nprovided tasks, and also limited variations in environmental conditions and\nviewpoints. To address these limitations, we create a large and diverse\ndataset, Animal Kingdom, that provides multiple annotated tasks to enable a\nmore thorough understanding of natural animal behaviors. The wild animal\nfootages used in our dataset record different times of the day in extensive\nrange of environments containing variations in backgrounds, viewpoints,\nillumination and weather conditions. More specifically, our dataset contains 50\nhours of annotated videos to localize relevant animal behavior segments in long\nvideos for the video grounding task, 30K video sequences for the fine-grained\nmulti-label action recognition task, and 33K frames for the pose estimation\ntask, which correspond to a diverse range of animals with 850 species across 6\nmajor animal classes. Such a challenging and comprehensive dataset shall be\nable to facilitate the community to develop, adapt, and evaluate various types\nof advanced methods for animal behavior analysis. Moreover, we propose a\nCollaborative Action Recognition (CARe) model that learns general and specific\nfeatures for action recognition with unseen new animals. This method achieves\npromising performance in our experiments. Our dataset can be found at\nhttps://sutdcv.github.io/Animal-Kingdom.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_X/0/1/0/all/0/1\">Xun Long Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1\">Kian Eng Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qichen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_S/0/1/0/all/0/1\">Si Yong Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HierAttn: Effectively Learn Representations from Stage Attention and Branch Attention for Skin Lesions Diagnosis. (arXiv:2205.04326v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.04326","description":"<p>Accurate and unbiased examinations of skin lesions are critical for the early\ndiagnosis and treatment of skin conditions and disorders. Visual features of\nskin lesions vary significantly because the images are collected from patients\nwith different lesion colours and morphologies by using dissimilar imaging\nequipment. Recent studies have reported ensembled convolutional neural networks\n(CNNs) to classify the images for early diagnosis of skin disorders. However,\nthe practical use of these ensembled CNNs is limited because they are\nheavyweight and inadequate for using contextual information. Although\nlightweight networks (e.g., MobileNetV3 and EfficientNet) were developed to\nachieve parameters reduction for implementing deep neural networks on mobile\ndevices, insufficient depth of feature representation restricts the\nperformance. To address the existing limitations, we introduce a new lite and\neffective neural network, namely HierAttn. The HierAttn applies a novel\nstrategy to learn the local and global features by using multi-stage and\nmulti-branch attention mechanisms. The efficacy of HierAttn was evaluated by\nusing the dermoscopy images dataset ISIC2019 and smartphone photos dataset\nPAD-UFES-20 (PAD20). The experimental results show that HierAttn achieves the\nbest accuracy and AUC among the state-of-the-art lightweight networks. The code\nis available at https://github.com/anthonyweidai/HierAttn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender and Racial Bias in Visual Question Answering Datasets. (arXiv:2205.08148v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08148","description":"<p>Vision-and-language tasks have increasingly drawn more attention as a means\nto evaluate human-like reasoning in machine learning models. A popular task in\nthe field is visual question answering (VQA), which aims to answer questions\nabout images. However, VQA models have been shown to exploit language bias by\nlearning the statistical correlations between questions and answers without\nlooking into the image content: e.g., questions about the color of a banana are\nanswered with yellow, even if the banana in the image is green. If societal\nbias (e.g., sexism, racism, ableism, etc.) is present in the training data,\nthis problem may be causing VQA models to learn harmful stereotypes. For this\nreason, we investigate gender and racial bias in five VQA datasets. In our\nanalysis, we find that the distribution of answers is highly different between\nquestions about women and men, as well as the existence of detrimental\ngender-stereotypical samples. Likewise, we identify that specific race-related\nattributes are underrepresented, whereas potentially discriminatory samples\nappear in the analyzed datasets. Our findings suggest that there are dangers\nassociated to using VQA datasets without considering and dealing with the\npotentially harmful stereotypes. We conclude the paper by proposing solutions\nto alleviate the problem before, during, and after the dataset collection\nprocess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1\">Yusuke Hirota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12693","description":"<p>Self-supervised learning has achieved a great success in the representation\nlearning of visual and textual data. However, the current methods are mainly\nvalidated on the well-curated datasets, which do not exhibit the real-world\nlong-tailed distribution. Recent attempts to consider self-supervised\nlong-tailed learning are made by rebalancing in the loss perspective or the\nmodel perspective, resembling the paradigms in the supervised long-tailed\nlearning. Nevertheless, without the aid of labels, these explorations have not\nshown the expected significant promise due to the limitation in tail sample\ndiscovery or the heuristic structure design. Different from previous works, we\nexplore this direction from an alternative perspective, i.e., the data\nperspective, and propose a novel Boosted Contrastive Learning (BCL) method.\nSpecifically, BCL leverages the memorization effect of deep neural networks to\nautomatically drive the information discrepancy of the sample views in\ncontrastive learning, which is more efficient to enhance the long-tailed\nlearning in the label-unaware context. Extensive experiments on a range of\nbenchmark datasets demonstrate the effectiveness of BCL over several\nstate-of-the-art methods. Our code is available at\nhttps://github.com/MediaBrain-SJTU/BCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning. (arXiv:2206.01088v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.01088","description":"<p>Cancer is a fatal disease caused by a combination of genetic diseases and a\nvariety of biochemical abnormalities. Lung and colon cancer have emerged as two\nof the leading causes of death and disability in humans. The histopathological\ndetection of such malignancies is usually the most important component in\ndetermining the best course of action. Early detection of the ailment on either\nfront considerably decreases the likelihood of mortality. Machine learning and\ndeep learning techniques can be utilized to speed up such cancer detection,\nallowing researchers to study a large number of patients in a much shorter\namount of time and at a lower cost. In this research work, we introduced a\nhybrid ensemble feature extraction model to efficiently identify lung and colon\ncancer. It integrates deep feature extraction and ensemble learning with\nhigh-performance filtering for cancer image datasets. The model is evaluated on\nhistopathological (LC25000) lung and colon datasets. According to the study\nfindings, our hybrid model can detect lung, colon, and (lung and colon) cancer\nwith accuracy rates of 99.05%, 100%, and 99.30%, respectively. The study's\nfindings show that our proposed strategy outperforms existing models\nsignificantly. Thus, these models could be applicable in clinics to support the\ndoctor in the diagnosis of cancers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Talukder_M/0/1/0/all/0/1\">Md. Alamin Talukder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1\">Md. Manowarul Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uddin_M/0/1/0/all/0/1\">Md Ashraf Uddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akhter_A/0/1/0/all/0/1\">Arnisha Akhter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_K/0/1/0/all/0/1\">Khondokar Fida Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moni_M/0/1/0/all/0/1\">Mohammad Ali Moni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives. (arXiv:2206.01136v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01136","description":"<p>Transformer, the latest technological advance of deep learning, has gained\nprevalence in natural language processing or computer vision. Since medical\nimaging bear some resemblance to computer vision, it is natural to inquire\nabout the status quo of Transformers in medical imaging and ask the question:\ncan the Transformer models transform medical imaging? In this paper, we attempt\nto make a response to the inquiry. After a brief introduction of the\nfundamentals of Transformers, especially in comparison with convolutional\nneural networks (CNNs), and highlighting key defining properties that\ncharacterize the Transformers, we offer a comprehensive review of the\nstate-of-the-art Transformer-based approaches for medical imaging and exhibit\ncurrent research progresses made in the areas of medical image segmentation,\nrecognition, detection, registration, reconstruction, enhancement, etc. In\nparticular, what distinguishes our review lies in its organization based on the\nTransformer's key defining properties, which are mostly derived from comparing\nthe Transformer and CNN, and its type of architecture, which specifies the\nmanner in which the Transformer and CNN are combined, all helping the readers\nto best understand the rationale behind the reviewed approaches. We conclude\nwith discussions of future perspectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Ce Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}