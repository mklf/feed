{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Event Coreference Resolution for Contentious Politics Events. (arXiv:2203.10123v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10123","description":"<p>We propose a dataset for event coreference resolution, which is based on\nrandom samples drawn from multiple sources, languages, and countries. Early\nscholarship on event information collection has not quantified the contribution\nof event coreference resolution. We prepared and analyzed a representative\nmultilingual corpus and measured the performance and contribution of the\nstate-of-the-art event coreference resolution approaches. We found that almost\nhalf of the event mentions in documents co-occur with other event mentions and\nthis makes it inevitable to obtain erroneous or partial event information. We\nshowed that event coreference resolution could help improving this situation.\nOur contribution sheds light on a challenge that has been overlooked or hard to\nstudy to date. Future event information collection studies can be designed\nbased on the results we present in this report. The repository for this study\nis on https://github.com/emerging-welfare/ECR4-Contentious-Politics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hurriyetoglu_A/0/1/0/all/0/1\">Ali H&#xfc;rriyeto&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1\">Osman Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyhan_F/0/1/0/all/0/1\">Fatih Beyhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durusan_F/0/1/0/all/0/1\">F&#x131;rat Duru&#x15f;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safaya_A/0/1/0/all/0/1\">Ali Safaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeniterzi_R/0/1/0/all/0/1\">Reyyan Yeniterzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoruk_E/0/1/0/all/0/1\">Erdem Y&#xf6;r&#xfc;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Factually Grounded Content Transfer with Factual Ablation. (arXiv:2203.10133v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10133","description":"<p>Despite recent success, large neural models often generate factually\nincorrect text. Compounding this is the lack of a standard automatic evaluation\nfor factuality--it cannot be meaningfully improved if it cannot be measured.\nGrounded generation promises a path to solving both of these problems: models\ndraw on a reliable external document (grounding) for factual information,\nsimplifying the challenge of factuality. Measuring factuality is also\nsimplified--to factual consistency, testing whether the generation agrees with\nthe grounding, rather than all facts. Yet, without a standard automatic metric\nfor factual consistency, factually grounded generation remains an open problem.\n</p>\n<p>We study this problem for content transfer, in which generations extend a\nprompt, using information from factual grounding. Particularly, this domain\nallows us to introduce the notion of factual ablation for automatically\nmeasuring factual consistency: this captures the intuition that the model\nshould be less likely to produce an output given a less relevant grounding\ndocument. In practice, we measure this by presenting a model with two grounding\ndocuments, and the model should prefer to use the more factually relevant one.\nWe contribute two evaluation sets to measure this. Applying our new evaluation,\nwe propose multiple novel methods improving over strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quirk_C/0/1/0/all/0/1\">Chris Quirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine. (arXiv:2203.10232v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10232","description":"<p>In this paper, we present DuReader_retrieval, a large-scale Chinese dataset\nfor passage retrieval. DuReader_retrieval contains more than 90K queries and\nover 8M unique passages from Baidu search. To ensure the quality of our\nbenchmark and address the shortcomings in other existing datasets, we (1)\nreduce the false negatives in development and testing sets by pooling the\nresults from multiple retrievers with human annotations, (2) and remove the\nsemantically similar questions between training with development and testing\nsets. We further introduce two extra out-of-domain testing sets for\nbenchmarking the domain generalization capability. Our experiment results\ndemonstrate that DuReader_retrieval is challenging and there is still plenty of\nroom for the community to improve, e.g. the generalization across domains,\nsalient phrase and syntax mismatch between query and paragraph and robustness.\nDuReader_retrieval will be publicly available at\nhttps://github.com/baidu/DuReader/tree/master/DuReader-Retrieval\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yifu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning. (arXiv:2203.10244v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10244","description":"<p>Charts are very popular for analyzing data. When exploring charts, people\noften ask a variety of complex reasoning questions that involve several logical\nand arithmetic operations. They also commonly refer to visual features of a\nchart in their questions. However, most existing datasets do not focus on such\ncomplex reasoning questions as their questions are template-based and answers\ncome from a fixed-vocabulary. In this work, we present a large-scale benchmark\ncovering 9.6K human-written questions as well as 23.1K questions generated from\nhuman-written chart summaries. To address the unique challenges in our\nbenchmark involving visual and logical reasoning over charts, we present two\ntransformer-based models that combine visual features and the data table of the\nchart in a unified way to answer questions. While our models achieve the\nstate-of-the-art results on the previous datasets as well as on our benchmark,\nthe evaluation also reveals several challenges in answering complex reasoning\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">Ahmed Masry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_D/0/1/0/all/0/1\">Do Xuan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jia Qing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension. (arXiv:2203.10249v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10249","description":"<p>Comprehending a dialogue requires a model to capture diverse kinds of key\ninformation in the utterances, which are either scattered around or implicitly\nimplied in different turns of conversations. Therefore, dialogue comprehension\nrequires diverse capabilities such as paraphrasing, summarizing, and\ncommonsense reasoning. Towards the objective of pre-training a zero-shot\ndialogue comprehension model, we develop a novel narrative-guided pre-training\nstrategy that learns by narrating the key information from a dialogue input.\nHowever, the dialogue-narrative parallel corpus for such a pre-training\nstrategy is currently unavailable. For this reason, we first construct a\ndialogue-narrative parallel corpus by automatically aligning movie subtitles\nand their synopses. We then pre-train a BART model on the data and evaluate its\nperformance on four dialogue-based tasks that require comprehension.\nExperimental results show that our model not only achieves superior zero-shot\nperformance but also exhibits stronger fine-grained dialogue comprehension\ncapabilities. The data and code are available at\nhttps://github.com/zhaochaocs/Diana\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-X$_{NLG}$: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation. (arXiv:2203.10250v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10250","description":"<p>Recently, the NLP community has witnessed a rapid advancement in multilingual\nand cross-lingual transfer research where the supervision is transferred from\nhigh-resource languages (HRLs) to low-resource languages (LRLs). However, the\ncross-lingual transfer is not uniform across languages, particularly in the\nzero-shot setting. Towards this goal, one promising research direction is to\nlearn shareable structures across multiple tasks with limited annotated data.\nThe downstream multilingual applications may benefit from such a learning setup\nas most of the languages across the globe are low-resource and share some\nstructures with other languages. In this paper, we propose a novel\nmeta-learning framework (called Meta-X$_{NLG}$) to learn shareable structures\nfrom typologically diverse languages based on meta-learning and language\nclustering. This is a step towards uniform cross-lingual transfer for unseen\nlanguages. We first cluster the languages based on language representations and\nidentify the centroid language of each cluster. Then, a meta-learning algorithm\nis trained with all centroid languages and evaluated on the other languages in\nthe zero-shot setting. We demonstrate the effectiveness of this modeling on two\nNLG tasks (Abstractive Text Summarization and Question Generation), 5 popular\ndatasets and 30 typologically diverse languages. Consistent improvements over\nstrong baselines demonstrate the efficacy of the proposed framework. The\ncareful design of the model makes this end-to-end NLG setup less vulnerable to\nthe accidental translation problem, which is a prominent concern in zero-shot\ncross-lingual NLG tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maurya_K/0/1/0/all/0/1\">Kaushal Kumar Maurya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1\">Maunendra Sankar Desarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity and Content-based Phonetic Self Attention for Speech Recognition. (arXiv:2203.10252v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10252","description":"<p>Transformer-based speech recognition models have achieved great success due\nto the self-attention (SA) mechanism that utilizes every frame in the feature\nextraction process. Especially, SA heads in lower layers capture various\nphonetic characteristics by the query-key dot product, which is designed to\ncompute the pairwise relationship between frames. In this paper, we propose a\nvariant of SA to extract more representative phonetic features. The proposed\nphonetic self-attention (phSA) is composed of two different types of phonetic\nattention; one is similarity-based and the other is content-based. In short,\nsimilarity-based attention utilizes the correlation between frames while\ncontent-based attention only considers each frame without being affected by\nothers. We identify which parts of the original dot product are related to two\ndifferent attention patterns and improve each part by simple modifications. Our\nexperiments on phoneme classification and speech recognition show that\nreplacing SA with phSA for lower layers improves the recognition performance\nwithout increasing the latency and the parameter size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1\">Kyuhong Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Read Top News First: A Document Reordering Approach for Multi-Document News Summarization. (arXiv:2203.10254v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10254","description":"<p>A common method for extractive multi-document news summarization is to\nre-formulate it as a single-document summarization problem by concatenating all\ndocuments as a single meta-document. However, this method neglects the relative\nimportance of documents. We propose a simple approach to reorder the documents\naccording to their relative importance before concatenating and summarizing\nthem. The reordering makes the salient content easier to learn by the\nsummarization model. Experiments show that our approach outperforms previous\nstate-of-the-art methods with more complex architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tenghao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_M/0/1/0/all/0/1\">Muthu Kumar Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency-based Mixture Language Models. (arXiv:2203.10256v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10256","description":"<p>Various models have been proposed to incorporate knowledge of syntactic\nstructures into neural language models. However, previous works have relied\nheavily on elaborate components for a specific language model, usually\nrecurrent neural network (RNN), which makes themselves unwieldy in practice to\nfit into other neural language models, such as Transformer and GPT-2. In this\npaper, we introduce the Dependency-based Mixture Language Models. In detail, we\nfirst train neural language models with a novel dependency modeling objective\nto learn the probability distribution of future dependent tokens given context.\nWe then formulate the next-token probability by mixing the previous dependency\nmodeling probability distributions with self-attention. Extensive experiments\nand human evaluations show that our method can be easily and effectively\napplied to different neural language models while improving neural text\ngeneration on various tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhixian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaiRR: Faithful and Robust Deductive Reasoning over Natural Language. (arXiv:2203.10261v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10261","description":"<p>Transformers have been shown to be able to perform deductive reasoning on a\nlogical rulebase containing rules and statements written in natural language.\nRecent works show that such models can also produce the reasoning steps (i.e.,\nthe proof graph) that emulate the model's logical reasoning process. Currently,\nthese black-box models generate both the proof graph and intermediate\ninferences within the same model and thus may be unfaithful. In this work, we\nframe the deductive logical reasoning task by defining three modular\ncomponents: rule selection, fact selection, and knowledge composition. The rule\nand fact selection steps select the candidate rule and facts to be used and\nthen the knowledge composition combines them to generate new inferences. This\nensures model faithfulness by assured causal relation from the proof step to\nthe inference reasoning. To test our framework, we propose FaiRR (Faithful and\nRobust Reasoner) where the above three components are independently modeled by\ntransformers. We observe that FaiRR is robust to novel language perturbations,\nand is faster at inference than previous works on existing reasoning datasets.\nAdditionally, in contrast to black-box generative models, the errors made by\nFaiRR are more interpretable due to the modular approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Harman Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clickbait Spoiling via Question Answering and Passage Retrieval. (arXiv:2203.10282v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10282","description":"<p>We introduce and study the task of clickbait spoiling: generating a short\ntext that satisfies the curiosity induced by a clickbait post. Clickbait links\nto a web page and advertises its contents by arousing curiosity instead of\nproviding an informative summary. Our contributions are approaches to classify\nthe type of spoiler needed (i.e., a phrase or a passage), and to generate\nappropriate spoilers. A large-scale evaluation and error analysis on a new\ncorpus of 5,000 manually spoiled clickbait posts -- the Webis Clickbait\nSpoiling Corpus 2022 -- shows that our spoiler type classifier achieves an\naccuracy of 80%, while the question answering model DeBERTa-large outperforms\nall others in generating spoilers for both types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1\">Matthias Hagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1\">Maik Fr&#xf6;be</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurk_A/0/1/0/all/0/1\">Artur Jurk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-channel CNN to classify nepali covid-19 related tweets using hybrid features. (arXiv:2203.10286v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10286","description":"<p>Because of the current COVID-19 pandemic with its increasing fears among\npeople, it has triggered several health complications such as depression and\nanxiety. Such complications have not only affected the developed countries but\nalso developing countries such as Nepal. These complications can be understood\nfrom peoples' tweets/comments posted online after their proper analysis and\nsentiment classification. Nevertheless, owing to the limited number of\ntokens/words in each tweet, it is always crucial to capture multiple\ninformation associated with them for their better understanding. In this study,\nwe, first, represent each tweet by combining both syntactic and semantic\ninformation, called hybrid features. The syntactic information is generated\nfrom the bag of words method, whereas the semantic information is generated\nfrom the combination of the fastText-based (ft) and domain-specific (ds)\nmethods. Second, we design a novel multi-channel convolutional neural network\n(MCNN), which ensembles the multiple CNNs, to capture multi-scale information\nfor better classification. Last, we evaluate the efficacy of both the proposed\nfeature extraction method and the MCNN model classifying tweets into three\nsentiment classes (positive, neutral and negative) on NepCOV19Tweets dataset,\nwhich is the only public COVID-19 tweets dataset in Nepali language. The\nevaluation results show that the proposed hybrid features outperform individual\nfeature extraction methods with the highest classification accuracy of 69.7%\nand the MCNN model outperforms the existing methods with the highest\nclassification accuracy of 71.3% during classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitaula_C/0/1/0/all/0/1\">Chiranjibi Sitaula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_T/0/1/0/all/0/1\">Tej Bahadur Shahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From meaning to perception -- exploring the space between word and odor perception embeddings. (arXiv:2203.10294v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10294","description":"<p>In this paper we propose the use of the Word2vec algorithm in order to obtain\nodor perception embeddings (or smell embeddings), only using publicly available\nperfume descriptions. Besides showing meaningful similarity relationships among\neach other, these embeddings also demonstrate to possess some shared\ninformation with their respective word embeddings. The meaningfulness of these\nembeddings suggests that aesthetics might provide enough constraints for using\nalgorithms motivated by distributional semantics on non-randomly combined data.\nFurthermore, they provide possibilities for new ways of classifying odors and\nanalyzing perfumes. We have also employed the embeddings in an attempt to\nunderstand the aesthetic nature of perfumes, based on the difference between\nreal and randomly generated perfumes. In an additional tentative experiment we\nexplore the possibility of a mapping between the word embedding space and the\nodor perception embedding space by fitting a regressor on the shared vocabulary\nand then predict the odor perception embeddings of words without an a priori\nassociated smell, such as night or sky.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amann_J/0/1/0/all/0/1\">Janek Amann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirrezabal_M/0/1/0/all/0/1\">Manex Agirrezabal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Machine Translation with Phrase-Level Universal Visual Representations. (arXiv:2203.10299v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10299","description":"<p>Multimodal machine translation (MMT) aims to improve neural machine\ntranslation (NMT) with additional visual information, but most existing MMT\nmethods require paired input of source sentence and image, which makes them\nsuffer from shortage of sentence-image pairs. In this paper, we propose a\nphrase-level retrieval-based method for MMT to get visual information for the\nsource input from existing sentence-image data sets so that MMT can break the\nlimitation of paired sentence-image input. Our method performs retrieval at the\nphrase level and hence learns visual information from pairs of source phrase\nand grounded region, which can mitigate data sparsity. Furthermore, our method\nemploys the conditional variational auto-encoder to learn visual\nrepresentations which can filter redundant visual information and only retain\nvisual information related to the phrase. Experiments show that the proposed\nmethod significantly outperforms strong baselines on multiple MMT datasets,\nespecially when the textual context is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qingkai Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging. (arXiv:2203.10315v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10315","description":"<p>In recent years, large-scale pre-trained language models (PLMs) have made\nextraordinary progress in most NLP tasks. But, in the unsupervised POS tagging\ntask, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA)\nperformance. The recent SOTA performance is yielded by a Guassian HMM variant\nproposed by He et al. (2018). However, as a generative model, HMM makes very\nstrong independence assumptions, making it very challenging to incorporate\ncontexualized word representations from PLMs. In this work, we for the first\ntime propose a neural conditional random field autoencoder (CRF-AE) model for\nunsupervised POS tagging. The discriminative encoder of CRF-AE can\nstraightforwardly incorporate ELMo word representations. Moreover, inspired by\nfeature-rich HMM, we reintroduce hand-crafted features into the decoder of\nCRF-AE. Finally, experiments clearly show that our model outperforms previous\nstate-of-the-art models by a large margin on Penn Treebank and multilingual\nUniversal Dependencies treebank v2.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Houquan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction. (arXiv:2203.10316v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10316","description":"<p>Solving math word problems requires deductive reasoning over the quantities\nin the text. Various recent research efforts mostly relied on\nsequence-to-sequence or sequence-to-tree models to generate mathematical\nexpressions without explicitly performing relational reasoning between\nquantities in the given context. While empirically effective, such approaches\ntypically do not provide explanations for the generated expressions. In this\nwork, we view the task as a complex relation extraction problem, proposing a\nnovel approach that presents explainable deductive reasoning steps to\niteratively construct target expressions, where each step involves a primitive\noperation over two quantities defining their relation. Through extensive\nexperiments on four benchmark datasets, we show that the proposed model\nsignificantly outperforms existing strong baselines. We further demonstrate\nthat the deductive procedure not only presents more explainable steps but also\nenables us to make more accurate predictions on questions that require more\ncomplex reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jierui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Knowledge Graph Completion and Question Answering. (arXiv:2203.10321v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10321","description":"<p>Knowledge graph embedding (KGE) models represent each entity and relation of\na knowledge graph (KG) with low-dimensional embedding vectors. These methods\nhave recently been applied to KG link prediction and question answering over\nincomplete KGs (KGQA). KGEs typically create an embedding for each entity in\nthe graph, which results in large model sizes on real-world graphs with\nmillions of entities. For downstream tasks these atomic entity representations\noften need to be integrated into a multi stage pipeline, limiting their\nutility. We show that an off-the-shelf encoder-decoder Transformer model can\nserve as a scalable and versatile KGE model obtaining state-of-the-art results\nfor KG link prediction and incomplete KG question answering. We achieve this by\nposing KG link prediction as a sequence-to-sequence task and exchange the\ntriple scoring approach taken by prior KGE methods with autoregressive\ndecoding. Such a simple but powerful method reduces the model size up to 98%\ncompared to conventional KGE models while keeping inference time tractable.\nAfter finetuning this model on the task of KGQA over incomplete KGs, our\napproach outperforms baselines on multiple large-scale datasets without\nextensive hyperparameter tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1\">Apoorv Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochsiek_A/0/1/0/all/0/1\">Adrian Kochsiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemulla_R/0/1/0/all/0/1\">Rainer Gemulla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining with Synthetic Language: Studying Transferable Knowledge in Language Models. (arXiv:2203.10326v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10326","description":"<p>We investigate what kind of structural knowledge learned in neural network\nencoders is transferable to processing natural language. We design synthetic\nlanguages with structural properties that mimic natural language, pretrain\nencoders on the data, and see how much performance the encoder exhibits on\ndownstream tasks in natural language. Our experimental results show that\npretraining with a synthetic language with a nesting dependency structure\nprovides some knowledge transferable to natural language. A follow-up probing\nanalysis indicates that its success in the transfer is related to the amount of\nencoded contextual information and what is transferred is the knowledge of\nposition-aware context dependence of language. Our results provide insights\ninto how neural network encoders process human languages and the source of\ncross-lingual transferability of recent multilingual language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding COVID-19 News Coverage using Medical NLP. (arXiv:2203.10338v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10338","description":"<p>Being a global pandemic, the COVID-19 outbreak received global media\nattention. In this study, we analyze news publications from CNN and The\nGuardian - two of the world's most influential media organizations. The dataset\nincludes more than 36,000 articles, analyzed using the clinical and biomedical\nNatural Language Processing (NLP) models from the Spark NLP for Healthcare\nlibrary, which enables a deeper analysis of medical concepts than previously\nachieved. The analysis covers key entities and phrases, observed biases, and\nchange over time in news coverage by correlating mined medical symptoms,\nprocedures, drugs, and guidance with commonly mentioned demographic and\noccupational groups. Another analysis is of extracted Adverse Drug Events about\ndrug and vaccine manufacturers, which when reported by major news outlets has\nan impact on vaccine hesitancy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varol_A/0/1/0/all/0/1\">Ali Emre Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocaman_V/0/1/0/all/0/1\">Veysel Kocaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haq_H/0/1/0/all/0/1\">Hasham Ul Haq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talby_D/0/1/0/all/0/1\">David Talby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection of Entity-Manipulated Text using Factual Knowledge. (arXiv:2203.10343v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10343","description":"<p>In this work, we focus on the problem of distinguishing a human written news\narticle from a news article that is created by manipulating entities in a human\nwritten news article (e.g., replacing entities with factually incorrect\nentities). Such manipulated articles can mislead the reader by posing as a\nhuman written news article. We propose a neural network based detector that\ndetects manipulated news articles by reasoning about the facts mentioned in the\narticle. Our proposed detector exploits factual knowledge via graph\nconvolutional neural network along with the textual information in the news\narticle. We also create challenging datasets for this task by considering\nvarious strategies to generate the new replacement entity (e.g., entity\ngeneration from GPT-2). In all the settings, our proposed model either matches\nor outperforms the state-of-the-art detector in terms of accuracy. Our code and\ndata are available at https://github.com/UBC-NLP/manipulated_entity_detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V. S. Lakshmanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense. (arXiv:2203.10346v1 [cs.LG])","link":"http://arxiv.org/abs/2203.10346","description":"<p>We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K\nhuman-written text perturbations in the wild and leverages them for realistic\nadversarial attack. Unlike existing character-based attacks which often\ndeductively hypothesize a set of manipulation strategies, our work is grounded\non actual observations from real-world texts. We find that adversarial texts\ngenerated by ANTHRO achieve the best trade-off between (1) attack success rate,\n(2) semantic preservation of the original text, and (3) stealthiness--i.e.\nindistinguishable from human writings hence harder to be flagged as suspicious.\nSpecifically, our attacks accomplished around 83% and 91% attack success rates\non BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger\nbaseline with an increase of 50% and 40% in terms of semantic preservation and\nstealthiness when evaluated by both layperson and professional human workers.\nANTHRO can further enhance a BERT classifier's performance in understanding\ndifferent variations of human-written toxic texts via adversarial training when\ncompared to the Perspective API.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jooyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Representative Keywords Selection: A Probabilistic Approach. (arXiv:2203.10365v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10365","description":"<p>We propose a probabilistic approach to select a subset of a \\textit{target\ndomain representative keywords} from a candidate set, contrasting with a\ncontext domain. Such a task is crucial for many downstream tasks in natural\nlanguage processing. To contrast the target domain and the context domain, we\nadapt the \\textit{two-component mixture model} concept to generate a\ndistribution of candidate keywords. It provides more importance to the\n\\textit{distinctive} keywords of the target domain than common keywords\ncontrasting with the context domain. To support the \\textit{representativeness}\nof the selected keywords towards the target domain, we introduce an\n\\textit{optimization algorithm} for selecting the subset from the generated\ncandidate distribution. We have shown that the optimization algorithm can be\nefficiently implemented with a near-optimal approximation guarantee. Finally,\nextensive experiments on multiple domains demonstrate the superiority of our\napproach over other baselines for the tasks of keyword summary generation and\ntrending keywords selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akash_P/0/1/0/all/0/1\">Pritom Saha Akash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1\">Lucian Popa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Online Behaviour of the Algerian Abusers in Social Media Networks. (arXiv:2203.10369v1 [cs.SI])","link":"http://arxiv.org/abs/2203.10369","description":"<p>Connecting to social media networks becomes a daily task for the majority of\npeople around the world, and the amount of shared information is growing\nexponentially. Thus, controlling the way in which people communicate is\nnecessary, in order to protect them from disorientation, conflicts,\naggressions, etc. In this paper, we conduct a statistical study on the\ncyber-bullying and the abusive content in social media (i.e. Facebook), where\nwe try to spot the online behaviour of the abusers in the Algerian community.\nMore specifically, we have involved 200 Facebook users from different regions\namong 600 to carry out this study. The aim of this investigation is to aid\nautomatic systems of abuse detection to take decision by incorporating the\nonline activity. Abuse detection systems require a large amount of data to\nperform better on such kind of texts (i.e. unstructured and informal texts),\nand this is due to the lack of standard orthography, where there are various\nAlgerian dialects and languages spoken.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abainia_K/0/1/0/all/0/1\">Kheireddine Abainia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robust Prefix-Tuning for Text Classification. (arXiv:2203.10378v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10378","description":"<p>Recently, prefix-tuning has gained increasing attention as a\nparameter-efficient finetuning method for large-scale pretrained language\nmodels. The method keeps the pretrained models fixed and only updates the\nprefix token parameters for each downstream task. Despite being lightweight and\nmodular, prefix-tuning still lacks robustness to textual adversarial attacks.\nHowever, most currently developed defense techniques necessitate auxiliary\nmodel update and storage, which inevitably hamper the modularity and low\nstorage of prefix-tuning. In this work, we propose a robust prefix-tuning\nframework that preserves the efficiency and modularity of prefix-tuning. The\ncore idea of our framework is leveraging the layerwise activations of the\nlanguage model by correctly-classified training data as the standard for\nadditional prefix finetuning. During the test phase, an extra batch-level\nprefix is tuned for each batch and added to the original prefix for robustness\nenhancement. Extensive experiments on three text classification benchmarks show\nthat our framework substantially improves robustness over several strong\nbaselines against five textual attacks of different types while maintaining\ncomparable accuracy on clean texts. We also interpret our robust prefix-tuning\nframework from the optimal control perspective and pose several directions for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zonghan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does the pre-training objective affect what large language models learn about linguistic properties?. (arXiv:2203.10415v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10415","description":"<p>Several pre-training objectives, such as masked language modeling (MLM), have\nbeen proposed to pre-train language models (e.g. BERT) with the aim of learning\nbetter language representations. However, to the best of our knowledge, no\nprevious work so far has investigated how different pre-training objectives\naffect what BERT learns about linguistics properties. We hypothesize that\nlinguistically motivated objectives such as MLM should help BERT to acquire\nbetter linguistic knowledge compared to other non-linguistically motivated\nobjectives that are not intuitive or hard for humans to guess the association\nbetween the input and the label to be predicted. To this end, we pre-train BERT\nwith two linguistically motivated objectives and three non-linguistically\nmotivated ones. We then probe for linguistic characteristics encoded in the\nrepresentation of the resulting models. We find strong evidence that there are\nonly small differences in probing performance between the representations\nlearned by the two different types of objectives. These surprising results\nquestion the dominant narrative of linguistically informed pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alajrami_A/0/1/0/all/0/1\">Ahmed Alajrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaOnce: A Metaverse Framework Based on Multi-scene Relations and Entity-relation-event Game. (arXiv:2203.10424v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10424","description":"<p>Existing metaverse systems lack rich relation types between entities and\nevents. The challenge is that there is no portable framework to introduce rich\nconcepts, relations, events into the metaverse. This paper introduces a new\nmetaverse framework, MetaOnce. This framework proposes to build multi-scene\ngraphs. This framework not only describes rich relations in a single scene but\nalso combines multiple scene graphs into a complete graph for more\ncomprehensive analysis and inference. Prior social network systems mainly\ndescribe friend relations. They ignore the effect of entity-relation-event\ngames on the metaverse system and existing rule constraints. We propose a rule\ncontroller and impose constraints on the relations that allow the framework to\nbehave in a compliant manner. We build a metaverse system to test the features\nof the framework, and experimental results show that our framework can build a\nmulti-scene metaverse with memory and rule constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation. (arXiv:2203.10426v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10426","description":"<p>How to learn a better speech representation for end-to-end speech-to-text\ntranslation (ST) with limited labeled data? Existing techniques often attempt\nto transfer powerful machine translation (MT) capabilities to ST, but neglect\nthe representation discrepancy across modalities. In this paper, we propose the\nSpeech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy.\nSpecifically, we mix up the representation sequences of different modalities,\nand take both unimodal speech sequences and multimodal mixed sequences as input\nto the translation model in parallel, and regularize their output predictions\nwith a self-learning framework. Experiments on MuST-C speech translation\nbenchmark and further analysis show that our method effectively alleviates the\ncross-modal representation discrepancy, and achieves significant improvements\nover a strong baseline on eight translation directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qingkai Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin. (arXiv:2203.10430v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10430","description":"<p>Polyphone disambiguation is the most crucial task in Mandarin\ngrapheme-to-phoneme (g2p) conversion. Previous studies have benefited from this\nproblem because of pre-trained language models, restricted output, and extra\ninformation from Part-Of-Speech (POS) tagging. Inspired by the strategies, we\nproposed a novel approach, called g2pW, which adapts learnable softmax-weights\nto condition the outputs of BERT with the polyphonic character of interest and\nits POS tagging. Rather than using the hard mask as in previous works, our\nexperiments showed that learning a soft-weighting function for the candidate\nphonemes benefits performance. Besides, our g2pW does not require extra\npre-trained POS tagging models while using POS tags as auxiliary features since\nwe train the POS tagging model simultaneously with the unified encoder. The\nexperiments show that our g2pW outperforms existing methods on the public\ndataset. All codes, model weights, and a user-friendly package are publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ren Yeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretability of Fine-grained Classification of Sadness and Depression. (arXiv:2203.10432v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10432","description":"<p>While sadness is a human emotion that people experience at certain times\nthroughout their lives, inflicting them with emotional disappointment and pain,\ndepression is a longer term mental illness which impairs social, occupational,\nand other vital regions of functioning making it a much more serious issue and\nneeds to be catered to at the earliest. NLP techniques can be utilized for the\ndetection and subsequent diagnosis of these emotions. Most of the open sourced\ndata on the web deal with sadness as a part of depression, as an emotion even\nthough the difference in severity of both is huge. Thus, we create our own\nnovel dataset illustrating the difference between the two. In this paper, we\naim to highlight the difference between the two and highlight how interpretable\nour models are to distinctly label sadness and depression. Due to the sensitive\nnature of such information, privacy measures need to be taken for handling and\ntraining of such data. Hence, we also explore the effect of Federated Learning\n(FL) on contextualised language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Structuring Real-World Data at Scale: Deep Learning for Extracting Key Oncology Information from Clinical Text with Patient-Level Supervision. (arXiv:2203.10442v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10442","description":"<p>Objective: The majority of detailed patient information in real-world data\n(RWD) is only consistently available in free-text clinical documents. Manual\ncuration is expensive and time-consuming. Developing natural language\nprocessing (NLP) methods for structuring RWD is thus essential for scaling\nreal-world evidence generation.\n</p>\n<p>Materials and Methods: Traditional rule-based systems are vulnerable to the\nprevalent linguistic variations and ambiguities in clinical text, and prior\napplications of machine-learning methods typically require sentence-level or\nreport-level labeled examples that are hard to produce at scale. We propose\nleveraging patient-level supervision from medical registries, which are often\nreadily available and capture key patient information, for general RWD\napplications. To combat the lack of sentence-level or report-level annotations,\nwe explore advanced deep-learning methods by combining domain-specific\npretraining, recurrent neural networks, and hierarchical attention.\n</p>\n<p>Results: We conduct an extensive study on 135,107 patients from the cancer\nregistry of a large integrated delivery network (IDN) comprising healthcare\nsystems in five western US states. Our deep learning methods attain test AUROC\nof 94-99% for key tumor attributes and comparable performance on held-out data\nfrom separate health systems and states.\n</p>\n<p>Discussion and Conclusion: Ablation results demonstrate clear superiority of\nthese advanced deep-learning methods over prior approaches. Error analysis\nshows that our NLP system sometimes even corrects errors in registrar labels.\nWe also conduct a preliminary investigation in accelerating registry curation\nand general RWD structuring via assisted curation for over 1.2 million cancer\npatients in this healthcare network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Preston_S/0/1/0/all/0/1\">Sam Preston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rajesh Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Michael Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerasinghe_R/0/1/0/all/0/1\">Roshanthi Weerasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soohee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piening_B/0/1/0/all/0/1\">Brian Piening</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tittel_P/0/1/0/all/0/1\">Paul Tittel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valluri_N/0/1/0/all/0/1\">Naveen Valluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bifulco_C/0/1/0/all/0/1\">Carlo Bifulco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEIM: An effective deep encoding and interaction model for sentence matching. (arXiv:2203.10482v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10482","description":"<p>Natural language sentence matching is the task of comparing two sentences and\nidentifying the relationship between them.It has a wide range of applications\nin natural language processing tasks such as reading comprehension, question\nand answer systems. The main approach is to compute the interaction between\ntext representations and sentence pairs through an attention mechanism, which\ncan extract the semantic information between sentence pairs well. However,this\nkind of method can not gain satisfactory results when dealing with complex\nsemantic features. To solve this problem, we propose a sentence matching method\nbased on deep encoding and interaction to extract deep semantic information. In\nthe encoder layer,we refer to the information of another sentence in the\nprocess of encoding a single sentence, and later use a heuristic algorithm to\nfuse the information. In the interaction layer, we use a bidirectional\nattention mechanism and a self-attention mechanism to obtain deep semantic\ninformation.Finally, we perform a pooling operation and input it to the MLP for\nclassification. we evaluate our model on three tasks: recognizing textual\nentailment, paraphrase recognition, and answer selection. We conducted\nexperiments on the SNLI and SciTail datasets for the recognizing textual\nentailment task, the Quora dataset for the paraphrase recognition task, and the\nWikiQA dataset for the answer selection task. The experimental results show\nthat the proposed algorithm can effectively extract deep semantic features that\nverify the effectiveness of the algorithm on sentence matching tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kexin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yahui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Rongyi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenguo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entailment Relation Aware Paraphrase Generation. (arXiv:2203.10483v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10483","description":"<p>We introduce a new task of entailment relation aware paraphrase generation\nwhich aims at generating a paraphrase conforming to a given entailment relation\n(e.g. equivalent, forward entailing, or reverse entailing) with respect to a\ngiven input. We propose a reinforcement learning-based weakly-supervised\nparaphrasing system, ERAP, that can be trained using existing paraphrase and\nnatural language inference (NLI) corpora without an explicit task-specific\ncorpus. A combination of automated and human evaluations show that ERAP\ngenerates paraphrases conforming to the specified entailment relation and are\nof good quality as compared to the baselines and uncontrolled paraphrasing\nsystems. Using ERAP for augmenting training data for downstream textual\nentailment task improves performance over an uncontrolled paraphrasing system,\nand introduces fewer training artifacts, indicating the benefit of explicit\ncontrol during paraphrasing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sancheti_A/0/1/0/all/0/1\">Abhilasha Sancheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1\">Balaji Vasan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Inductive Transfer for Continual Dialogue Learning. (arXiv:2203.10484v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10484","description":"<p>Pre-trained models have achieved excellent performance on the dialogue task.\nHowever, for the continual increase of online chit-chat scenarios, directly\nfine-tuning these models for each of the new tasks not only explodes the\ncapacity of the dialogue system on the embedded devices but also causes\nknowledge forgetting on pre-trained models and knowledge interference among\ndiverse dialogue tasks. In this work, we propose a hierarchical inductive\ntransfer framework to learn and deploy the dialogue skills continually and\nefficiently. First, we introduce the adapter module into pre-trained models for\nlearning new dialogue tasks. As the only trainable module, it is beneficial for\nthe dialogue system on the embedded devices to acquire new dialogue skills with\nnegligible additional parameters. Then, for alleviating knowledge interference\nbetween tasks yet benefiting the regularization between them, we further design\nhierarchical inductive transfer that enables new tasks to use general knowledge\nin the base adapter without being misled by diverse knowledge in task-specific\nadapters. Empirical evaluation and analysis indicate that our framework obtains\ncomparable performance under deployment-friendly model capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shaoxiong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Instance Query Network for Named Entity Recognition. (arXiv:2203.10545v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10545","description":"<p>Named entity recognition (NER) is a fundamental task in natural language\nprocessing. Recent works treat named entity recognition as a reading\ncomprehension task, constructing type-specific queries manually to extract\nentities. This paradigm suffers from three issues. First, type-specific queries\ncan only extract one type of entities per inference, which is inefficient.\nSecond, the extraction for different types of entities is isolated, ignoring\nthe dependencies between them. Third, query construction relies on external\nknowledge and is difficult to apply to realistic scenarios with hundreds of\nentity types. To deal with them, we propose Parallel Instance Query Network\n(PIQN), which sets up global and learnable instance queries to extract entities\nfrom a sentence in a parallel manner. Each instance query predicts one entity,\nand by feeding all instance queries simultaneously, we can query all entities\nin parallel. Instead of being constructed from external knowledge, instance\nqueries can learn their different query semantics during training. For training\nthe model, we treat label assignment as a one-to-many Linear Assignment Problem\n(LAP) and dynamically assign gold entities to instance queries with minimal\nassignment cost. Experiments on both nested and flat NER datasets demonstrate\nthat our proposed method outperforms previous state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural-Symbolic Approach to Natural Language Understanding. (arXiv:2203.10557v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10557","description":"<p>Deep neural networks, empowered by pre-trained language models, have achieved\nremarkable results in natural language understanding (NLU) tasks. However,\ntheir performances can deteriorate drastically when logical reasoning is needed\nin the process. This is because, ideally, NLU needs to depend on not only\nanalogical reasoning, which deep neural networks are good at, but also logical\nreasoning. According to the dual-process theory, analogical reasoning and\nlogical reasoning are respectively carried out by System 1 and System 2 in the\nhuman brain. Inspired by the theory, we present a novel framework for NLU\ncalled Neural-Symbolic Processor (NSP), which performs analogical reasoning\nbased on neural processing and performs logical reasoning based on both neural\nand symbolic processing. As a case study, we conduct experiments on two NLU\ntasks, question answering (QA) and natural language inference (NLI), when\nnumerical reasoning (a type of logical reasoning) is necessary. The\nexperimental results show that our method significantly outperforms\nstate-of-the-art methods in both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who will share Fake-News on Twitter? Psycholinguistic cues in online post histories discriminate Between actors in the misinformation ecosystem. (arXiv:2203.10560v1 [cs.CY])","link":"http://arxiv.org/abs/2203.10560","description":"<p>The spread of misinformation or fake-news is a global concern that undermines\nprogress on issues such as protecting democracy and public health. Past\nresearch aiming to combat its spread has largely focused on identifying its\nsemantic content and media outlets publishing such news. In contrast, we aim to\nidentify individuals who are more likely to share fake-news by studying the\nlanguage of actors in the fake-news ecosystem (such as fake-news sharers,\nfact-check sharers and random twitter users), and creating a linguistic profile\nof them. Fake-news sharers and fact-check sharers use significantly more\nhigh-arousal negative emotions in their language, but fake-news sharers express\nmore existentially-based needs than other actors. Incorporating\npsycholinguistic cues as inferred from their tweets into a model of\nsocio-demographic predictors considerably improves classification accuracy of\nfake-news sharers. The finding that fake-news sharers differ in important ways\nfrom other actors in the fake-news ecosystem (such as in their existential\nneeds), but are also similar to them in other ways (such as in their anger\nlevels), highlights the importance of studying the entire fake-news ecosystem\nto increase accuracy in identification and prediction. Our approach can help\nmitigate fake-news sharing by enabling platforms to pre-emptively screen\npotential fake-news sharers' posts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schoenmueller_V/0/1/0/all/0/1\">Verena Schoenmueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanchard_S/0/1/0/all/0/1\">Simon J. Blanchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johar_G/0/1/0/all/0/1\">Gita V. Johar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small Batch Sizes Improve Training of Low-Resource Neural MT. (arXiv:2203.10579v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10579","description":"<p>We study the role of an essential hyper-parameter that governs the training\nof Transformers for neural machine translation in a low-resource setting: the\nbatch size. Using theoretical insights and experimental evidence, we argue\nagainst the widespread belief that batch size should be set as large as allowed\nby the memory of the GPUs. We show that in a low-resource setting, a smaller\nbatch size leads to higher scores in a shorter training time, and argue that\nthis is due to better regularization of the gradients during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atrio_A/0/1/0/all/0/1\">&#xc0;lex R. Atrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_Belis_A/0/1/0/all/0/1\">Andrei Popescu-Belis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster & Tune: Boost Cold Start Performance in Text Classification. (arXiv:2203.10581v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10581","description":"<p>In real-world scenarios, a text classification task often begins with a cold\nstart, when labeled data is scarce. In such cases, the common practice of\nfine-tuning pre-trained models, such as BERT, for a target classification task,\nis prone to produce poor performance. We suggest a method to boost the\nperformance of such models by adding an intermediate unsupervised\nclassification task, between the pre-training and fine-tuning phases. As such\nan intermediate task, we perform clustering and train the pre-trained model on\npredicting the cluster labels. We test this hypothesis on various data sets,\nand show that this additional classification phase can significantly improve\nperformance, mainly for topical classification tasks, when the number of\nlabeled instances available for fine-tuning is only a couple of dozen to a few\nhundred.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shnarch_E/0/1/0/all/0/1\">Eyal Shnarch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gera_A/0/1/0/all/0/1\">Ariel Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halfon_A/0/1/0/all/0/1\">Alon Halfon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankin_L/0/1/0/all/0/1\">Lena Dankin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Five Psycholinguistic Characteristics for Better Interaction with Users. (arXiv:2012.09692v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.09692","description":"<p>When two people pay attention to each other and are interested in what the\nother has to say or write, they almost instantly adapt their writing/speaking\nstyle to match the other. For a successful interaction with a user, chatbots\nand dialogue systems should be able to do the same. We propose a framework\nconsisting of five psycholinguistic textual characteristics for better\nhuman-computer interaction. We describe the annotation processes used for\ncollecting the data, and benchmark five binary classification tasks,\nexperimenting with different training sizes and model architectures. The best\narchitectures noticeably outperform several baselines and achieve\nmacro-averaged F$_1$-scores between 72\\% and 96\\% depending on the language and\nthe task. The proposed framework proved to be fairly easy to model for various\nlanguages even with small amount of manually annotated data if right\narchitectures are used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stajner_S/0/1/0/all/0/1\">Sanja &#x160;tajner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yenikent_S/0/1/0/all/0/1\">Seren Yenikent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum Learning: A Survey. (arXiv:2101.10382v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.10382","description":"<p>Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soviany_P/0/1/0/all/0/1\">Petru Soviany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1\">Paolo Rota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Multi-Task Learning for Sequential Sentence Classification in Research Papers. (arXiv:2102.06008v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.06008","description":"<p>Sequential sentence classification deals with the categorisation of sentences\nbased on their content and context. Applied to scientific texts, it enables the\nautomatic structuring of research papers and the improvement of academic search\nengines. However, previous work has not investigated the potential of transfer\nlearning for sentence classification across different scientific domains and\nthe issue of different text structure of full papers and abstracts. In this\npaper, we derive seven related research questions and present several\ncontributions to address them: First, we suggest a novel uniform deep learning\narchitecture and multi-task learning for cross-domain sequential sentence\nclassification in scientific texts. Second, we tailor two common transfer\nlearning methods, sequential transfer learning and multi-task learning, to deal\nwith the challenges of the given task. Semantic relatedness of tasks is a\nprerequisite for successful transfer learning of neural models. Consequently,\nour third contribution is an approach to semi-automatically identify\nsemantically related classes from different annotation schemes and we present\nan analysis of four annotation schemes. Comprehensive experimental results\nindicate that models, which are trained on datasets from different scientific\ndomains, benefit from one another when using the proposed multi-task learning\narchitecture. We also report comparisons with several state-of-the-art\napproaches. Our approach outperforms the state of the art on full paper\ndatasets significantly while being on par for datasets consisting of abstracts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brack_A/0/1/0/all/0/1\">Arthur Brack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoppe_A/0/1/0/all/0/1\">Anett Hoppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschermohle_P/0/1/0/all/0/1\">Pascal Buscherm&#xf6;hle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning. (arXiv:2104.08793v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08793","description":"<p>Augmenting pre-trained language models with knowledge graphs (KGs) has\nachieved success on various commonsense reasoning tasks. However, for a given\ntask instance, the KG, or certain parts of the KG, may not be useful. Although\nKG-augmented models often use attention to focus on specific KG components, the\nKG is still always used, and the attention mechanism is never explicitly taught\nwhich KG components should be used. Meanwhile, saliency methods can measure how\nmuch a KG feature (e.g., graph, node, path) influences the model to make the\ncorrect prediction, thus explaining which KG features are useful. This paper\nexplores how saliency explanations can be used to improve KG-augmented models'\nperformance. First, we propose to create coarse (Is the KG useful?) and fine\n(Which nodes/paths in the KG are useful?) saliency explanations. Second, to\nmotivate saliency-based supervision, we analyze oracle KG-augmented models\nwhich directly use saliency explanations as extra inputs for guiding their\nattention. Third, we propose SalKG, a framework for KG-augmented models to\nlearn from coarse and/or fine saliency explanations. Given saliency\nexplanations created from a task's training set, SalKG jointly trains the model\nto predict the explanations, then solve the task by attending to KG features\nhighlighted by the predicted explanations. On three commonsense QA benchmarks\n(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can\nyield considerable performance gains -- up to 2.76% absolute improvement on\nCSQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Boyuan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BertGCN: Transductive Text Classification by Combining GCN and BERT. (arXiv:2105.05727v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05727","description":"<p>In this work, we propose BertGCN, a model that combines large scale\npretraining and transductive learning for text classification. BertGCN\nconstructs a heterogeneous graph over the dataset and represents documents as\nnodes using BERT representations. By jointly training the BERT and GCN modules\nwithin BertGCN, the proposed model is able to leverage the advantages of both\nworlds: large-scale pretraining which takes the advantage of the massive amount\nof raw data and transductive learning which jointly learns representations for\nboth training data and unlabeled test data by propagating label influence\nthrough graph convolution. Experiments show that BertGCN achieves SOTA\nperformances on a wide range of text classification datasets. Code is available\nat https://github.com/ZeroRin/BertGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuxiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qinghong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Automatic Speech Recognition: A Review. (arXiv:2106.04897v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04897","description":"<p>Automatic Speech Recognition (ASR) systems can be trained to achieve\nremarkable performance given large amounts of manually transcribed speech, but\nlarge labeled data sets can be difficult or expensive to acquire for all\nlanguages of interest. In this paper, we review the research literature to\nidentify models and ideas that could lead to fully unsupervised ASR, including\nunsupervised segmentation of the speech signal, unsupervised mapping from\nspeech segments to text, and semi-supervised models with nominal amounts of\nlabeled examples. The objective of the study is to identify the limitations of\nwhat can be learned from speech data alone and to understand the minimum\nrequirements for speech recognition. Identifying these limitations would help\noptimize the resources and efforts in ASR development for low-resource\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1\">Hanan Aldarmaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullah_A/0/1/0/all/0/1\">Asad Ullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_N/0/1/0/all/0/1\">Nazar Zaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automated Quality Evaluation Framework of Psychotherapy Conversations with Local Quality Estimates. (arXiv:2106.07922v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07922","description":"<p>Text-based computational approaches for assessing the quality of\npsychotherapy are being developed to support quality assurance and clinical\ntraining. However, due to the long durations of typical conversation based\ntherapy sessions, and due to limited annotated modeling resources,\ncomputational methods largely rely on frequency-based lexical features or\ndialogue acts to assess the overall session level characteristics. In this\nwork, we propose a hierarchical framework to automatically evaluate the quality\nof transcribed Cognitive Behavioral Therapy (CBT) interactions. Given the\nrichly dynamic nature of the spoken dialog within a talk therapy session, to\nevaluate the overall session level quality, we propose to consider modeling it\nas a function of local variations across the interaction. To implement that\nempirically, we divide each psychotherapy session into conversation segments\nand initialize the segment-level qualities with the session-level scores.\nFirst, we produce segment embeddings by fine-tuning a BERT-based model, and\npredict segment-level (local) quality scores. These embeddings are used as the\nlower-level input to a Bidirectional LSTM-based neural network to predict the\nsession-level (global) quality estimates. In particular, we model the global\nquality as a linear function of the local quality scores, which allows us to\nupdate the segment-level quality estimates based on the session-level quality\nprediction. These newly estimated segment-level scores benefit the BERT\nfine-tuning process, which in turn results in better segment embeddings. We\nevaluate the proposed framework on automatically derived transcriptions from\nreal-world CBT clinical recordings to predict session-level behavior codes. The\nresults indicate that our approach leads to improved evaluation accuracy for\nmost codes when used for both regression and classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuohao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flemotomos_N/0/1/0/all/0/1\">Nikolaos Flemotomos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_K/0/1/0/all/0/1\">Karan Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creed_T/0/1/0/all/0/1\">Torrey A. Creed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkins_D/0/1/0/all/0/1\">David C. Atkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.10199","description":"<p>We introduce BitFit, a sparse-finetuning method where only the bias-terms of\nthe model (or a subset of them) are being modified. We show that with\nsmall-to-medium training data, applying BitFit on pre-trained BERT models is\ncompetitive with (and sometimes better than) fine-tuning the entire model. For\nlarger data, the method is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1\">Elad Ben Zaken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics Sheets for AI Tasks. (arXiv:2107.01183v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2107.01183","description":"<p>Several high-profile events, such as the mass testing of emotion recognition\nsystems on vulnerable sub-populations and using question answering systems to\nmake moral judgments, have highlighted how technology will often lead to more\nadverse outcomes for those that are already marginalized. At issue here are not\njust individual systems and datasets, but also the AI tasks themselves. In this\nposition paper, I make a case for thinking about ethical considerations not\njust at the level of individual models and datasets, but also at the level of\nAI tasks. I will present a new form of such an effort, Ethics Sheets for AI\nTasks, dedicated to fleshing out the assumptions and ethical considerations\nhidden in how a task is commonly framed and in the choices we make regarding\nthe data, method, and evaluation. I will also present a template for ethics\nsheets with 50 ethical considerations, using the task of emotion recognition as\na running example. Ethics sheets are a mechanism to engage with and document\nethical considerations before building datasets and systems. Similar to survey\narticles, a small number of ethics sheets can serve numerous researchers and\ndevelopers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers. (arXiv:2107.05687v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05687","description":"<p>Active learning is the iterative construction of a classification model\nthrough targeted labeling, enabling significant labeling cost savings. As most\nresearch on active learning has been carried out before transformer-based\nlanguage models (\"transformers\") became popular, despite its practical\nimportance, comparably few papers have investigated how transformers can be\ncombined with active learning to date. This can be attributed to the fact that\nusing state-of-the-art query strategies for transformers induces a prohibitive\nruntime overhead, which effectively nullifies, or even outweighs the desired\ncost savings. For this reason, we revisit uncertainty-based query strategies,\nwhich had been largely outperformed before, but are particularly suited in the\ncontext of fine-tuning transformers. In an extensive evaluation, we connect\ntransformers to experiments from previous research, assessing their performance\non five widely used text classification benchmarks. For active learning with\ntransformers, several other uncertainty-based approaches outperform the\nwell-known prediction entropy query strategy, thereby challenging its status as\nmost popular uncertainty baseline in active learning for text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance. (arXiv:2108.05682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05682","description":"<p>In the domain of Morphology, Inflection is a fundamental and important task\nthat gained a lot of traction in recent years, mostly via SIGMORPHON's\nshared-tasks. With average accuracy above 0.9 over the scores of all languages,\nthe task is considered mostly solved using relatively generic neural seq2seq\nmodels, even with little data provided. In this work, we propose to re-evaluate\nmorphological inflection models by employing harder train-test splits that will\nchallenge the generalization capacity of the models. In particular, as opposed\nto the na{\\\"i}ve split-by-form, we propose a split-by-lemma method to challenge\nthe performance on existing benchmarks. Our experiments with the three\ntop-ranked systems on the SIGMORPHON's 2020 shared-task show that the\nlemma-split presents an average drop of 30 percentage points in macro-average\nfor the 90 languages included. The effect is most significant for low-resourced\nlanguages with a drop as high as 95 points, but even high-resourced languages\nlose about 10 points on average. Our results clearly show that generalizing\ninflection to unseen lemmas is far from being solved, presenting a simple yet\neffective means to promote more sophisticated models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guriel_D/0/1/0/all/0/1\">David Guriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It is AI's Turn to Ask Humans a Question: Question-Answer Pair Generation for Children's Story Books. (arXiv:2109.03423v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03423","description":"<p>Existing question answering (QA) techniques are created mainly to answer\nquestions asked by humans. But in educational applications, teachers often need\nto decide what questions they should ask, in order to help students to improve\ntheir narrative understanding capabilities. We design an automated\nquestion-answer generation (QAG) system for this education scenario: given a\nstory book at the kindergarten to eighth-grade level as input, our system can\nautomatically generate QA pairs that are capable of testing a variety of\ndimensions of a student's comprehension skills. Our proposed QAG model\narchitecture is demonstrated using a new expert-annotated FairytaleQA dataset,\nwhich has 278 child-friendly storybooks with 10,580 QA pairs. Automatic and\nhuman evaluations show that our model outperforms state-of-the-art QAG baseline\nsystems. On top of our QAG system, we also start to build an interactive\nstory-telling application for the future real-world deployment in this\neducational scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder. (arXiv:2109.04500v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04500","description":"<p>We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for\nsemantic parsing in task-oriented dialog. Our model consists of an encoder\nnetwork that incrementally builds the semantic parse tree by predicting the\nnon-terminal label and its positions in the linearized tree. At the generation\ntime, the model constructs the semantic parse tree by recursively inserting the\npredicted non-terminal labels at the predicted positions until termination.\nRINE achieves state-of-the-art exact match accuracy on low- and high-resource\nversions of the conversational semantic parsing benchmark TOP (Gupta et al.,\n2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and\ntransition-based parsers. We also show that our model design is applicable to\nnested named entity recognition task, where it performs on par with\nstate-of-the-art approach designed for that task. Finally, we demonstrate that\nour approach is 2-3.5 times faster than the sequence-to-sequence model at\ninference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems. (arXiv:2109.04645v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04645","description":"<p>As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema (definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5) is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05238","description":"<p>Simultaneous machine translation (SiMT) generates translation before reading\nthe entire source sentence and hence it has to trade off between translation\nquality and latency. To fulfill the requirements of different translation\nquality and latency in practical applications, the previous methods usually\nneed to train multiple SiMT models for different latency levels, resulting in\nlarge computational costs. In this paper, we propose a universal SiMT model\nwith Mixture-of-Experts Wait-k Policy to achieve the best translation quality\nunder arbitrary latency with only one trained model. Specifically, our method\nemploys multi-head attention to accomplish the mixture of experts where each\nhead is treated as a wait-k expert with its own waiting words number, and given\na test latency and source inputs, the weights of the experts are accordingly\nadjusted to produce the best translation. Experiments on three datasets show\nthat our method outperforms all the strong baselines under different latency,\nincluding the state-of-the-art adaptive policy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning When to Translate for Streaming Speech. (arXiv:2109.07368v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07368","description":"<p>How to find proper moments to generate partial sentence translation given a\nstreaming speech input? Existing approaches waiting-and-translating for a fixed\nduration often break the acoustic units in speech, since the boundaries between\nacoustic units in speech are not even. In this paper, we propose MoSST, a\nsimple yet effective method for translating streaming speech content. Given a\nusually long speech sequence, we develop an efficient monotonic segmentation\nmodule inside an encoder-decoder model to accumulate acoustic information\nincrementally and detect proper speech unit boundaries for the input in speech\ntranslation task. Experiments on multiple translation directions of the MuST-C\ndataset show that MoSST outperforms existing methods and achieves the best\ntrade-off between translation quality (BLEU) and latency. Our code is available\nat https://github.com/dqqcasia/mosst.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08256","description":"<p>The importance and pervasiveness of emotions in our lives makes affective\ncomputing a tremendously important and vibrant line of work. Systems for\nautomatic emotion recognition (AER) and sentiment analysis can be facilitators\nof enormous progress (e.g., in improving public health and commerce) but also\nenablers of great harm (e.g., for suppressing dissidents and manipulating\nvoters). Thus, it is imperative that the affective computing community actively\nengage with the ethical ramifications of their creations. In this paper, I have\nsynthesized and organized information from AI Ethics and Emotion Recognition\nliterature to present fifty ethical considerations relevant to AER. Notably,\nthe sheet fleshes out assumptions hidden in how AER is commonly framed, and in\nthe choices often made regarding the data, method, and evaluation. Special\nattention is paid to the implications of AER on privacy and social groups.\nAlong the way, key recommendations are made for responsible AER. The objective\nof the sheet is to facilitate and encourage more thoughtfulness on why to\nautomate, how to automate, and how to judge success well before the building of\nAER systems. Additionally, the sheet acts as a useful introductory document on\nemotion recognition (complementing survey articles).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering. (arXiv:2109.08678v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08678","description":"<p>Existing KBQA approaches, despite achieving strong performance on i.i.d. test\ndata, often struggle in generalizing to questions involving unseen KB schema\nitems. Prior ranking-based approaches have shown some success in\ngeneralization, but suffer from the coverage issue. We present RnG-KBQA, a\nRank-and-Generate approach for KBQA, which remedies the coverage issue with a\ngeneration model while preserving a strong generalization capability. Our\napproach first uses a contrastive ranker to rank a set of candidate logical\nforms obtained by searching over the knowledge graph. It then introduces a\ntailored generation model conditioned on the question and the top-ranked\ncandidates to compose the final logical form. We achieve new state-of-the-art\nresults on GrailQA and WebQSP datasets. In particular, our method surpasses the\nprior state-of-the-art by a large margin on the GrailQA leaderboard. In\naddition, RnG-KBQA outperforms all prior approaches on the popular WebQSP\nbenchmark, even including the ones that use the oracle entity linking. The\nexperimental results demonstrate the effectiveness of the interplay between\nranking and generation, which leads to the superior performance of our proposed\napproach across all settings with especially strong improvements in zero-shot\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Voice Activated Framework using Self-supervised Learning. (arXiv:2110.01077v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.01077","description":"<p>Self-supervised learning methods such as wav2vec 2.0 have shown promising\nresults in learning speech representations from unlabelled and untranscribed\nspeech data that are useful for speech recognition. Since these representations\nare learned without any task-specific supervision, they can also be useful for\nother voice-activated tasks like speaker verification, keyword spotting,\nemotion classification etc. In our work, we propose a general purpose framework\nfor adapting a pre-trained wav2vec 2.0 model for different voice-activated\ntasks. We develop downstream network architectures that operate on the\ncontextualized speech representations of wav2vec 2.0 to adapt the\nrepresentations for solving a given task. Finally, we extend our framework to\nperform multi-task learning by jointly optimizing the network parameters on\nmultiple voice activated tasks using a shared transformer backbone. Both of our\nsingle and multi-task frameworks achieve state-of-the-art results in speaker\nverification and keyword spotting benchmarks. Our best performing models\nachieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on VoxCeleb2 and\nVoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0\nkeyword spotting dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1\">Shehzeen Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visser_E/0/1/0/all/0/1\">Erik Visser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-training an Unsupervised Constituency Parser with Weak Supervision. (arXiv:2110.02283v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02283","description":"<p>We introduce a method for unsupervised parsing that relies on bootstrapping\nclassifiers to identify if a node dominates a specific span in a sentence.\nThere are two types of classifiers, an inside classifier that acts on a span,\nand an outside classifier that acts on everything outside of a given span.\nThrough self-training and co-training with the two classifiers, we show that\nthe interplay between them helps improve the accuracy of both, and as a result,\neffectively parse. A seed bootstrapping technique prepares the data to train\nthese classifiers. Our analyses further validate that such an approach in\nconjunction with weak supervision using prior branching knowledge of a known\nlanguage (left/right-branching) and minimal heuristics injects strong inductive\nbias into the parser, achieving 63.1 F$_1$ on the English (PTB) test set. In\naddition, we show the effectiveness of our architecture by evaluating on\ntreebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art\nresults. Our code and pre-trained models are available at\nhttps://github.com/Nickil21/weakly-supervised-parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maveli_N/0/1/0/all/0/1\">Nickil Maveli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design. (arXiv:2110.04541v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04541","description":"<p>Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pretraining example\nyields improved sentence representations and open domain question answering\nabilities. This theoretically motivated degree of freedom for pretraining\nexample design indicates new training schemes for self-improving\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannai_D/0/1/0/all/0/1\">Daniel Jannai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_D/0/1/0/all/0/1\">Dan Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition. (arXiv:2110.05354v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05354","description":"<p>Text-only adaptation of an end-to-end (E2E) model remains a challenging task\nfor automatic speech recognition (ASR). Language model (LM) fusion-based\napproaches require an additional external LM during inference, significantly\nincreasing the computation cost. To overcome this, we propose an internal LM\nadaptation (ILMA) of the E2E model using text-only data. Trained with\naudio-transcript pairs, an E2E model implicitly learns an internal LM that\ncharacterizes the token sequence probability which is approximated by the E2E\nmodel output after zeroing out the encoder contribution. During ILMA, we\nfine-tune the internal LM, i.e., the E2E components excluding the encoder, to\nminimize a cross-entropy loss. To make ILMA effective, it is essential to train\nthe E2E model with an internal LM loss besides the standard E2E loss.\nFurthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler\ndivergence between the output distributions of the adapted and unadapted\ninternal LMs. ILMA is the most effective when we update only the last linear\nlayer of the joint network. ILMA enables a fast text-only adaptation of the E2E\nmodel without increasing the run-time computational cost. Experimented with\n30K-hour trained transformer transducer models, ILMA achieves up to 34.9%\nrelative word error rate reduction from the unadapted baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dict-BERT: Enhancing Language Model Pre-training with Dictionary. (arXiv:2110.06490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06490","description":"<p>Pre-trained language models (PLMs) aim to learn universal language\nrepresentations by conducting self-supervised training tasks on large-scale\ncorpora. Since PLMs capture word semantics in different contexts, the quality\nof word representations highly depends on word frequency, which usually follows\na heavy-tailed distributions in the pre-training corpus. Therefore, the\nembeddings of rare words on the tail are usually poorly optimized. In this\nwork, we focus on enhancing language model pre-training by leveraging\ndefinitions of the rare words in dictionaries (e.g., Wiktionary). To\nincorporate a rare word definition as a part of input, we fetch its definition\nfrom the dictionary and append it to the end of the input text sequence. In\naddition to training with the masked language modeling objective, we propose\ntwo novel self-supervised pre-training tasks on word and sentence-level\nalignment between input text sequence and rare word definitions to enhance\nlanguage modeling representation with dictionary. We evaluate the proposed\nDict-BERT model on the language understanding benchmark GLUE and eight\nspecialized domain benchmark datasets. Extensive experiments demonstrate that\nDict-BERT can significantly improve the understanding of rare words and boost\nmodel performance on various NLP downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Donghan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphosyntactic Tagging with Pre-trained Language Models for Arabic and its Dialects. (arXiv:2110.06852v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06852","description":"<p>We present state-of-the-art results on morphosyntactic tagging across\ndifferent varieties of Arabic using fine-tuned pre-trained transformer language\nmodels. Our models consistently outperform existing systems in Modern Standard\nArabic and all the Arabic dialects we study, achieving 2.6% absolute\nimprovement over the previous state-of-the-art in Modern Standard Arabic, 2.8%\nin Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training\nsetups for fine-tuning pre-trained transformer language models, including\ntraining data size, the use of external linguistic resources, and the use of\nannotated data from other dialects in a low-resource scenario. Our results show\nthat strategic fine-tuning using datasets from other high-resource dialects is\nbeneficial for a low-resource dialect. Additionally, we show that high-quality\nmorphological analyzers as external linguistic resources are beneficial\nespecially in low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1\">Go Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_S/0/1/0/all/0/1\">Salam Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling. (arXiv:2110.07198v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07198","description":"<p>Given the claims of improved text generation quality across various\npre-trained neural models, we consider the coherence evaluation of machine\ngenerated text to be one of the principal applications of coherence models that\nneeds to be investigated. Prior work in neural coherence modeling has primarily\nfocused on devising new architectures for solving the permuted document task.\nWe instead use a basic model architecture and show significant improvements\nover state of the art within the same training regime. We then design a harder\nself-supervision objective by increasing the ratio of negative samples within a\ncontrastive learning setup, and enhance the model further through automatic\nhard negative mining coupled with a large global negative queue encoded by a\nmomentum encoder. We show empirically that increasing the density of negative\nsamples improves the basic model, and using a global negative queue further\nimproves and stabilizes the model while training with hard negative samples. We\nevaluate the coherence model on task-independent test sets that resemble\nreal-world applications and show significant improvements in coherence\nevaluations of downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jwalapuram_P/0/1/0/all/0/1\">Prathyusha Jwalapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. (arXiv:2110.07602v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07602","description":"<p>Prompt tuning, which only tunes continuous prompts with a frozen language\nmodel, substantially reduces per-task storage and memory usage at training.\nHowever, in the context of NLU, prior work reveals that prompt tuning does not\nperform well for normal-sized pretrained models. We also find that existing\nmethods of prompt tuning cannot handle hard sequence labeling tasks, indicating\na lack of universality. We present a novel empirical finding that properly\noptimized prompt tuning can be universally effective across a wide range of\nmodel scales and NLU tasks. It matches the performance of finetuning while\nhaving only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an\nimplementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning}\noptimized and adapted for NLU. Given the universality and simplicity of\nP-Tuning v2, we believe it can serve as an alternative to finetuning and a\nstrong baseline for future research.Our code and data are released at\nhttps://github.com/THUDM/P-tuning-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1\">Kaixuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yicheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_W/0/1/0/all/0/1\">Weng Lam Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhengxiao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Scale Substitution-based Word Sense Induction. (arXiv:2110.07681v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07681","description":"<p>We present a word-sense induction method based on pre-trained masked language\nmodels (MLMs), which can cheaply scale to large vocabularies and large corpora.\nThe result is a corpus which is sense-tagged according to a corpus-derived\nsense inventory and where each sense is associated with indicative words.\nEvaluation on English Wikipedia that was sense-tagged using our method shows\nthat both the induced senses, and the per-instance sense assignment, are of\nhigh quality even compared to WSD methods, such as Babelfy. Furthermore, by\ntraining a static word embeddings algorithm on the sense-tagged corpus, we\nobtain high-quality static senseful embeddings. These outperform existing\nsenseful embeddings methods on the WiC dataset and on a new outlier detection\ndataset we developed. The data driven nature of the algorithm allows to induce\ncorpora-specific senses, which may not appear in standard sense inventories, as\nwe demonstrate using a case study on the scientific domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eyal_M/0/1/0/all/0/1\">Matan Eyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadde_S/0/1/0/all/0/1\">Shoval Sadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taub_Tabib_H/0/1/0/all/0/1\">Hillel Taub-Tabib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models. (arXiv:2110.08151v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08151","description":"<p>Recent studies have shown that multilingual pretrained language models can be\neffectively improved with cross-lingual alignment information from Wikipedia\nentities. However, existing methods only exploit entity information in\npretraining and do not explicitly use entities in downstream tasks. In this\nstudy, we explore the effectiveness of leveraging entity representations for\ndownstream cross-lingual tasks. We train a multilingual language model with 24\nlanguages with entity representations and show the model consistently\noutperforms word-based pretrained models in various cross-lingual transfer\ntasks. We also analyze the model and the key insight is that incorporating\nentity representations into the input allows us to extract more\nlanguage-agnostic features. We also evaluate the model with a multilingual\ncloze prompt task with the mLAMA dataset. We show that entity-based prompt\nelicits correct factual knowledge more likely than using only word\nrepresentations. Our source code and pretrained models are available at\nhttps://github.com/studio-ousia/luke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Domain Question Answering with A Unified Knowledge Interface. (arXiv:2110.08417v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08417","description":"<p>The retriever-reader framework is popular for open-domain question answering\n(ODQA) due to its ability to use explicit knowledge. Although prior work has\nsought to increase the knowledge coverage by incorporating structured knowledge\nbeyond text, accessing heterogeneous knowledge sources through a unified\ninterface remains an open question. While data-to-text generation has the\npotential to serve as a universal interface for data and text, its feasibility\nfor downstream tasks remains largely unknown. In this work, we bridge this gap\nand use the data-to-text method as a means for encoding structured knowledge\nfor ODQA. Specifically, we propose a verbalizer-retriever-reader framework for\nODQA over data and text where verbalized tables from Wikipedia and graphs from\nWikidata are used as augmented knowledge sources. We show that our Unified Data\nand Text QA, UDT-QA, can effectively benefit from the expanded knowledge index,\nleading to large gains over text-only baselines. Notably, our approach sets the\nsingle-model state-of-the-art on Natural Questions. Furthermore, our analyses\nindicate that verbalized knowledge is preferred for answer reasoning for both\nadapted and hot-swap settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08501","description":"<p>Implicit knowledge, such as common sense, is key to fluid human\nconversations. Current neural response generation (RG) models are trained to\ngenerate responses directly, omitting unstated implicit knowledge. In this\npaper, we present Think-Before-Speaking (TBS), a generative approach to first\nexternalize implicit commonsense knowledge (think) and use this knowledge to\ngenerate responses (speak). We expect that externalizing implicit knowledge\nallows more efficient learning, produces more informative responses, and\nenables more explainable models. We analyze different choices to collect\nknowledge-aligned dialogues, represent implicit knowledge, and transition\nbetween knowledge and dialogues. Empirical results show TBS models outperform\nend-to-end and knowledge-augmented RG baselines on most automatic metrics and\ngenerate more informative, specific, and commonsense-following responses, as\nevaluated by human annotators. TBS also generates knowledge that makes sense\nand is relevant to the dialogue around 85\\% of the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented Structural Neural Encoders. (arXiv:2111.10545v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10545","description":"<p>Considering a collection of RDF triples, the RDF-to-text generation task aims\nto generate a text description. Most previous methods solve this task using a\nsequence-to-sequence model or using a graph-based model to encode RDF triples\nand to generate a text sequence. Nevertheless, these approaches fail to clearly\nmodel the local and global structural information between and within RDF\ntriples. Moreover, the previous methods also face the non-negligible problem of\nlow faithfulness of the generated text, which seriously affects the overall\nperformance of these models. To solve these problems, we propose a model\ncombining two new graph-augmented structural neural encoders to jointly learn\nboth local and global structural information in the input RDF triples. To\nfurther improve text faithfulness, we innovatively introduce a reinforcement\nlearning (RL) reward based on information extraction (IE). We first extract\ntriples from the generated text using a pretrained IE model and regard the\ncorrect number of the extracted triples as the additional RL reward.\nExperimental results on two benchmark datasets demonstrate that our proposed\nmodel outperforms the state-of-the-art baselines, and the additional\nreinforcement learning reward does help to improve the faithfulness of the\ngenerated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hanning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Po Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources. (arXiv:2112.00061v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00061","description":"<p>Misinformation is now a major problem due to its potential high risks to our\ncore democratic and societal values and orders. Out-of-context misinformation\nis one of the easiest and effective ways used by adversaries to spread viral\nfalse stories. In this threat, a real image is re-purposed to support other\nnarratives by misrepresenting its context and/or elements. The internet is\nbeing used as the go-to way to verify information using different sources and\nmodalities. Our goal is an inspectable method that automates this\ntime-consuming and reasoning-intensive process by fact-checking the\nimage-caption pairing using Web evidence. To integrate evidence and cues from\nboth modalities, we introduce the concept of 'multi-modal cycle-consistency\ncheck'; starting from the image/caption, we gather textual/visual evidence,\nwhich will be compared against the other paired caption/image, respectively.\nMoreover, we propose a novel architecture, Consistency-Checking Network (CCN),\nthat mimics the layered human reasoning across the same and different\nmodalities: the caption vs. textual evidence, the image vs. visual evidence,\nand the image vs. caption. Our work offers the first step and benchmark for\nopen-domain, content-based, multi-modal fact-checking, and significantly\noutperforms previous baselines that did not leverage external evidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelnabi_S/0/1/0/all/0/1\">Sahar Abdelnabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_R/0/1/0/all/0/1\">Rakibul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00374","description":"<p>Existing neural style transfer methods require reference style images to\ntransfer texture information of style images to content images. However, in\nmany practical situations, users may not have reference style images but still\nbe interested in transferring styles by just imagining them. In order to deal\nwith such applications, we propose a new framework that enables a style\ntransfer `without' a style image, but only with a text description of the\ndesired style. Using the pre-trained text-image embedding model of CLIP, we\ndemonstrate the modulation of the style of content images only with a single\ntext condition. Specifically, we propose a patch-wise text-image matching loss\nwith multiview augmentations for realistic texture transfer. Extensive\nexperimental results confirmed the successful image style transfer with\nrealistic textures that reflect semantic query texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gihyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Linguistic Information For Logical Inference In Pre-trained Language Models. (arXiv:2112.01753v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01753","description":"<p>Progress in pre-trained language models has led to a surge of impressive\nresults on downstream tasks for natural language understanding. Recent work on\nprobing pre-trained language models uncovered a wide range of linguistic\nproperties encoded in their contextualized representations. However, it is\nunclear whether they encode semantic knowledge that is crucial to symbolic\ninference methods. We propose a methodology for probing linguistic information\nfor logical inference in pre-trained language model representations. Our\nprobing datasets cover a list of linguistic phenomena required by major\nsymbolic inference systems. We find that (i) pre-trained language models do\nencode several types of linguistic information for inference, but there are\nalso some types of information that are weakly encoded, (ii) language models\ncan effectively learn missing linguistic information through fine-tuning.\nOverall, our findings provide insights into which aspects of linguistic\ninformation for logical inference do language models and their pre-training\nprocedures capture. Moreover, we have demonstrated language models' potential\nas semantic and background knowledge bases for supporting symbolic inference\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiyue Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Structure Learning via Graph Neural Networks for Inductive Document Classification. (arXiv:2112.06386v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06386","description":"<p>Recently, graph neural networks (GNNs) have been widely used for document\nclassification. However, most existing methods are based on static word\nco-occurrence graphs without sentence-level information, which poses three\nchallenges:(1) word ambiguity, (2) word synonymity, and (3) dynamic contextual\ndependency. To address these challenges, we propose a novel GNN-based sparse\nstructure learning model for inductive document classification. Specifically, a\ndocument-level graph is initially generated by a disjoint union of\nsentence-level word co-occurrence graphs. Our model collects a set of trainable\nedges connecting disjoint words between sentences and employs structure\nlearning to sparsely select edges with dynamic contextual dependencies. Graphs\nwith sparse structures can jointly exploit local and global contextual\ninformation in documents through GNNs. For inductive learning, the refined\ndocument graph is further fed into a general readout function for graph-level\nclassification and optimization in an end-to-end manner. Extensive experiments\non several real-world datasets demonstrate that the proposed model outperforms\nmost state-of-the-art results, and reveal the necessity to learn sparse\nstructures for each document.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piao_Y/0/1/0/all/0/1\">Yinhua Piao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangseon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dohoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging between Cognitive Processing Signals and Linguistic Features via a Unified Attentional Network. (arXiv:2112.08831v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08831","description":"<p>Cognitive processing signals can be used to improve natural language\nprocessing (NLP) tasks. However, it is not clear how these signals correlate\nwith linguistic information. Bridging between human language processing and\nlinguistic features has been widely studied in neurolinguistics, usually via\nsingle-variable controlled experiments with highly-controlled stimuli. Such\nmethods not only compromises the authenticity of natural reading, but also are\ntime-consuming and expensive. In this paper, we propose a data-driven method to\ninvestigate the relationship between cognitive processing signals and\nlinguistic features. Specifically, we present a unified attentional framework\nthat is composed of embedding, attention, encoding and predicting layers to\nselectively map cognitive processing signals to linguistic features. We define\nthe mapping procedure as a bridging task and develop 12 bridging tasks for\nlexical, syntactic and semantic features. The proposed framework only requires\ncognitive processing signals recorded under natural reading as inputs, and can\nbe used to detect a wide range of linguistic features with a single cognitive\ndataset. Observations from experiment results resonate with previous\nneuroscience findings. In addition to this, our experiments also reveal a\nnumber of interesting findings, such as the correlation between contextual\neye-tracking features and tense of sentence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers. (arXiv:2201.11870v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11870","description":"<p>We present a novel multiple-source unsupervised model for text classification\nunder domain shift. Our model exploits the update rates in document\nrepresentations to dynamically integrate domain encoders. It also employs a\nprobabilistic heuristic to infer the error rate in the target domain in order\nto pair source classifiers. Our heuristic exploits data transformation cost and\nthe classifier accuracy in the target feature space. We have used real world\nscenarios of Domain Adaptation to evaluate the efficacy of our algorithm. We\nalso used pretrained multi-layer transformers as the document encoder in the\nexperiments to demonstrate whether the improvement achieved by domain\nadaptation models can be delivered by out-of-the-box language model\npretraining. The experiments testify that our model is the top performing\napproach in this setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1\">Payam Karisani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locating and Editing Factual Knowledge in GPT. (arXiv:2202.05262v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.05262","description":"<p>We investigate the mechanisms underlying factual knowledge recall in\nautoregressive transformer language models. First, we develop a causal\nintervention for identifying neuron activations capable of altering a model's\nfactual predictions. Within large GPT-style models, this reveals two distinct\nsets of neurons that we hypothesize correspond to knowing an abstract fact and\nsaying a concrete word, respectively. This insight inspires the development of\nROME, a novel method for editing facts stored in model weights. For evaluation,\nwe assemble CounterFact, a dataset of over twenty thousand counterfactuals and\ntools to facilitate sensitive measurements of knowledge editing. Using\nCounterFact, we confirm the distinction between saying and knowing neurons, and\nwe find that ROME achieves state-of-the-art performance in knowledge editing\ncompared to other methods. An interactive demo notebook, full code\nimplementation, and the dataset are available at https://rome.baulab.info/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_K/0/1/0/all/0/1\">Kevin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation. (arXiv:2202.07959v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07959","description":"<p>We propose EdgeFormer -- a parameter-efficient Transformer of the\nencoder-decoder architecture for on-device seq2seq generation, which is\ncustomized under strict computation and memory constraints. EdgeFormer proposes\ntwo novel principles for cost-effective parameterization and further enhance\nthe model with efficient layer adaptation. We conduct extensive experiments on\ntwo practical on-device seq2seq tasks: Machine Translation and Grammatical\nError Correction, and show that EdgeFormer can effectively outperform previous\nparameter-efficient Transformer baselines and achieve very competitive results\nwith knowledge distillation under both the computation and memory constraints.\nMoreover, we release the pretrained EdgeFormer -- the first publicly available\npretrained model that can be easily fine-tuned for English seq2seq tasks with\nstrong results, largely facilitating on-device seq2seq generation in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking and Refining the Distinct Metric. (arXiv:2202.13587v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13587","description":"<p>Distinct-$n$ score\\cite{Li2016} is a widely used automatic metric for\nevaluating diversity in language generation tasks. However, we observed that\nthe original approach for calculating distinct scores has evident biases that\ntend to assign higher penalties to longer sequences. We refine the calculation\nof distinct scores by scaling the number of distinct tokens based on their\nexpectations. We provide both empirical and theoretical evidence to show that\nour method effectively removes the biases existing in the original distinct\nscore. Our experiments show that our proposed metric,\n\\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human\njudgment in evaluating response diversity. To foster future research, we\nprovide an example implementation at\n\\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sahand Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation. (arXiv:2202.13590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13590","description":"<p>In this study, we propose a simple and effective preprocessing method for\nsubword segmentation based on a data compression algorithm. Compression-based\nsubword segmentation has recently attracted significant attention as a\npreprocessing method for training data in Neural Machine Translation. Among\nthem, BPE/BPE-dropout is one of the fastest and most effective method compared\nto conventional approaches. However, compression-based approach has a drawback\nin that generating multiple segmentations is difficult due to the determinism.\nTo overcome this difficulty, we focus on a probabilistic string algorithm,\ncalled locally-consistent parsing (LCP), that has been applied to achieve\noptimum compression. Employing the probabilistic mechanism of LCP, we propose\nLCP-dropout for multiple subword segmentation that improves BPE/BPE-dropout,\nand show that it outperforms various baselines in learning from especially\nsmall training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nonaka_K/0/1/0/all/0/1\">Keita Nonaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamanouchi_K/0/1/0/all/0/1\">Kazutaka Yamanouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+I_T/0/1/0/all/0/1\">Tomohiro I</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okita_T/0/1/0/all/0/1\">Tsuyoshi Okita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimada_K/0/1/0/all/0/1\">Kazutaka Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakamoto_H/0/1/0/all/0/1\">Hiroshi Sakamoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRILLsson: Distilled Universal Paralinguistic Speech Representations. (arXiv:2203.00236v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.00236","description":"<p>Recent advances in self-supervision have dramatically improved the quality of\nspeech representations. However, deployment of state-of-the-art embedding\nmodels on devices has been restricted due to their limited public availability\nand large resource footprint. Our work addresses these issues by publicly\nreleasing a collection of paralinguistic speech models that are small and near\nstate-of-the-art performance. Our approach is based on knowledge distillation,\nand our models are distilled on public data only. We explore different\narchitectures and thoroughly evaluate our models on the Non-Semantic Speech\n(NOSS) benchmark. Our largest distilled model is less than 15% the size of the\noriginal model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7\ntasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB)\nand achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the\nopen source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model\noutperforms the open source Wav2Vec 2.0 on both emotion recognition tasks\ndespite being 7% the size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Conformer Based Acoustic Model for Robust Automatic Speech Recognition. (arXiv:2203.00725v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.00725","description":"<p>This study addresses robust automatic speech recognition (ASR) by introducing\na Conformer-based acoustic model. The proposed model builds on a\nstate-of-the-art recognition system using a bi-directional long short-term\nmemory (BLSTM) model with utterance-wise dropout and iterative speaker\nadaptation, but employs a Conformer encoder instead of the BLSTM network. The\nConformer encoder uses a convolution-augmented attention mechanism for acoustic\nmodeling. The proposed system is evaluated on the monaural ASR task of the\nCHiME-4 corpus. Coupled with utterance-wise normalization and speaker\nadaptation, our model achieves $6.25\\%$ word error rate, which outperforms the\nprevious best system by $8.4\\%$ relatively. In addition, the proposed\nConformer-based model is $18.3\\%$ smaller in model size and reduces total\ntraining time by $79.6\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">DeLiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Rank: Rethinking Evaluation with Word and Sentence Similarities. (arXiv:2203.02679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02679","description":"<p>Word and sentence embeddings are useful feature representations in natural\nlanguage processing. However, intrinsic evaluation for embeddings lags far\nbehind, and there has been no significant update since the past decade. Word\nand sentence similarity tasks have become the de facto evaluation method. It\nleads models to overfit to such evaluations, negatively impacting embedding\nmodels' development. This paper first points out the problems using semantic\nsimilarity as the gold standard for word and sentence embedding evaluations.\nFurther, we propose a new intrinsic evaluation method called EvalRank, which\nshows a much stronger correlation with downstream tasks. Extensive experiments\nare conducted based on 60+ models and popular datasets to certify our\njudgments. Finally, the practical evaluation toolkit is released for future\nbenchmarking purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Sketch Induction for Paraphrase Generation. (arXiv:2203.03463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03463","description":"<p>We propose a generative model of paraphrase generation, that encourages\nsyntactic diversity by conditioning on an explicit syntactic sketch. We\nintroduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE),\na method for learning decompositions of dense encodings as a sequence of\ndiscrete latent variables that make iterative refinements of increasing\ngranularity. This hierarchy of codes is learned through end-to-end training,\nand represents fine-to-coarse grained information about the input. We use\nHRQ-VAE to encode the syntactic form of an input sentence as a path through the\nhierarchy, allowing us to more easily predict syntactic sketches at test time.\nExtensive experiments, including a human evaluation, confirm that HRQ-VAE\nlearns a hierarchical representation of the input space, and generates\nparaphrases of higher quality than previous systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosking_T/0/1/0/all/0/1\">Tom Hosking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05598","description":"<p>The study of the applicability of the BERTScore metric was conducted to\ntranslation quality assessment at the sentence level for English -&gt; Russian\ndirection. Experiments were performed with a pre-trained Multilingual BERT as\nwell as with a pair of Monolingual BERT models. To align monolingual\nembeddings, an orthogonal transformation based on anchor tokens was used. It\nwas demonstrated that such transformation helps to prevent mismatching issue\nand shown that this approach gives better results than using embeddings of the\nMultilingual model. To improve the token matching process it is proposed to\ncombine all incomplete WorkPiece tokens into meaningful words and use simple\naveraging of corresponding vectors and to calculate BERTScore based on anchor\ntokens only. Such modifications allowed us to achieve a better correlation of\nthe model predictions with human judgments. In addition to evaluating machine\ntranslation, several versions of human translation were evaluated as well, the\nproblems of this approach were listed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_A/0/1/0/all/0/1\">A.A. Vetrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorn_E/0/1/0/all/0/1\">E.A. Gorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice. (arXiv:2203.06462v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.06462","description":"<p>Classifiers in natural language processing (NLP) often have a large number of\noutput classes. For example, neural language models (LMs) and machine\ntranslation (MT) models both predict tokens from a vocabulary of thousands. The\nSoftmax output layer of these models typically receives as input a dense\nfeature representation, which has much lower dimensionality than the output. In\ntheory, the result is some words may be impossible to be predicted via argmax,\nirrespective of input features, and empirically, there is evidence this happens\nin small language models. In this paper we ask whether it can happen in\npractical large language models and translation models. To do so, we develop\nalgorithms to detect such \\emph{unargmaxable} tokens in public models. We find\nthat 13 out of 150 models do indeed have such tokens; however, they are very\ninfrequent and unlikely to impact model quality. We release our code so that\nothers can inspect their models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grivas_A/0/1/0/all/0/1\">Andreas Grivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Adam Lopez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chart-to-Text: A Large-Scale Benchmark for Chart Summarization. (arXiv:2203.06486v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06486","description":"<p>Charts are commonly used for exploring data and communicating insights.\nGenerating natural language summaries from charts can be very helpful for\npeople in inferring key insights that would otherwise require a lot of\ncognitive and perceptual efforts. We present Chart-to-text, a large-scale\nbenchmark with two datasets and a total of 44,096 charts covering a wide range\nof topics and chart types. We explain the dataset construction process and\nanalyze the datasets. We also introduce a number of state-of-the-art neural\nmodels as baselines that utilize image captioning and data-to-text generation\ntechniques to tackle two problem variations: one assumes the underlying data\ntable of the chart is available while the other needs to extract data from\nchart images. Our analysis with automatic and human evaluation shows that while\nour best models usually generate fluent summaries and yield reasonable BLEU\nscores, they also suffer from hallucinations and factual errors as well as\ndifficulties in correctly explaining complex patterns and trends in charts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanthara_S/0/1/0/all/0/1\">Shankar Kanthara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_R/0/1/0/all/0/1\">Rixie Tiffany Ko Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">Ahmed Masry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Megh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the answering frame timeline. Extensive experiments on the medical\ninstructional dataset, namely MedVidQA, show that the proposed VPTSL\noutperforms other state-of-the-art (SOTA) methods by 28.36 in mIOU score with a\nlarge margin, which demonstrates the effectiveness of visual prompt and the\ntext span predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost. (arXiv:2203.07860v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07860","description":"<p>State-of-the-art NLP systems represent inputs with word embeddings, but these\nare brittle when faced with Out-of-Vocabulary (OOV) words. To address this\nissue, we follow the principle of mimick-like models to generate vectors for\nunseen words, by learning the behavior of pre-trained embeddings using only the\nsurface form of words. We present a simple contrastive learning framework,\nLOVE, which extends the word representation of an existing pre-trained language\nmodel (such as BERT), and makes it robust to OOV with few additional\nparameters. Extensive evaluations demonstrate that our lightweight model\nachieves similar or even better performances than prior competitors, both on\noriginal datasets and on corrupted variants. Moreover, it can be used in a\nplug-and-play fashion with FastText and BERT, where it significantly improves\ntheir robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lihu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1\">Ga&#xeb;l Varoquaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1\">Fabian M. Suchanek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation. (arXiv:2203.08394v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08394","description":"<p>Back-translation is a critical component of Unsupervised Neural Machine\nTranslation (UNMT), which generates pseudo parallel data from target\nmonolingual data. A UNMT model is trained on the pseudo parallel data with\ntranslated source, and translates natural source sentences in inference. The\nsource discrepancy between training and inference hinders the translation\nperformance of UNMT models. By carefully designing experiments, we identify two\nrepresentative characteristics of the data gap in source: (1) style gap (i.e.,\ntranslated vs. natural text style) that leads to poor generalization\ncapability; (2) content gap that induces the model to produce hallucination\ncontent biased towards the target language. To narrow the data gap, we propose\nan online self-training approach, which simultaneously uses the pseudo parallel\ndata {natural source, translated target} to mimic the inference scenario.\nExperimental results on several widely-used language pairs show that our\napproach outperforms two strong baselines (XLM and MASS) by remedying the style\nand content gaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConTinTin: Continual Learning from Task Instructions. (arXiv:2203.08512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08512","description":"<p>The mainstream machine learning paradigms for NLP often work with two\nunderlying presumptions. First, the target task is predefined and static; a\nsystem merely needs to learn to solve it exclusively. Second, the supervision\nof a task mainly comes from a set of labeled examples. A question arises: how\nto build a system that can keep learning new tasks from their instructions?\nThis work defines a new learning paradigm ConTinTin (Continual Learning from\nTask Instructions), in which a system should learn a sequence of new tasks one\nby one, each task is explained by a piece of textual instruction. The system is\nrequired to (i) generate the expected outputs of a new task by learning from\nits instruction, (ii) transfer the knowledge acquired from upstream tasks to\nhelp solve downstream tasks (i.e., forward-transfer), and (iii) retain or even\nimprove the performance on earlier tasks after learning new tasks (i.e.,\nbackward-transfer). This new problem is studied on a stream of more than 60\ntasks, each equipped with an instruction. Technically, our method\nInstructionSpeak contains two strategies that make full use of task\ninstructions to improve forward-transfer and backward-transfer: one is to learn\nfrom negative outputs, the other is to re-visit instructions of previous tasks.\nTo our knowledge, this is the first time to study ConTinTin in NLP. In addition\nto the problem formulation and our promising approach, this work also\ncontributes to providing rich analyses for the community to better understand\nthis novel learning problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study. (arXiv:2203.08527v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08527","description":"<p>In recent years, a flurry of morphological datasets had emerged, most notably\nUniMorph, a multi-lingual repository of inflection tables. However, the flat\nstructure of the current morphological annotation schema makes the treatment of\nsome languages quirky, if not impossible, specifically in cases of polypersonal\nagreement, where verbs agree with multiple arguments using true affixes. In\nthis paper, we propose to address this phenomenon by expanding the UniMorph\nannotation schema to a hierarchical feature structure that naturally\naccommodates complex argument marking. We apply this extended schema to one\nsuch language, Georgian, and provide a human-verified, accurate and balanced\nmorphological dataset for Georgian verbs. The dataset has 4 times more tables\nand 6 times more verb forms compared to the existing UniMorph dataset, covering\nall possible variants of argument marking, demonstrating the adequacy of our\nproposed scheme. Experiments with a standard reinflection model show that\ngeneralization is easy when the data is split at the form level, but extremely\nhard when splitting along lemma lines. Expanding the other languages in\nUniMorph to this schema is expected to improve both the coverage, consistency\nand interpretability of this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guriel_D/0/1/0/all/0/1\">David Guriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Autonomy: Self-Initiation, Adaptation and Continual Learning. (arXiv:2203.08994v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.08994","description":"<p>As more and more AI agents are used in practice, it is time to think about\nhow to make these agents fully autonomous so that they can (1) learn by\nthemselves continually in a self-motivated and self-initiated manner rather\nthan being retrained offline periodically on the initiation of human engineers\nand (2) accommodate or adapt to unexpected or novel circumstances. As the\nreal-world is an open environment that is full of unknowns or novelties,\ndetecting novelties, characterizing them, accommodating or adapting to them,\nand gathering ground-truth training data and incrementally learning the\nunknowns/novelties are critical to making the AI agent more and more\nknowledgeable and powerful over time. The key challenge is how to automate the\nprocess so that it is carried out continually on the agent's own initiative and\nthrough its own interactions with humans, other agents and the environment just\nlike human on-the-job learning. This paper proposes a framework (called SOLA)\nfor this learning paradigm to promote the research of building autonomous and\ncontinual learning enabled AI agents. To show feasibility, an implemented agent\nis also described.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1\">Sahisnu Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1\">Eric Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigsby_S/0/1/0/all/0/1\">Scott Grigsby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Conditional Masked Language Pre-training for Neural Machine Translation. (arXiv:2203.09210v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09210","description":"<p>Pre-trained sequence-to-sequence models have significantly improved Neural\nMachine Translation (NMT). Different from prior works where pre-trained models\nusually adopt an unidirectional decoder, this paper demonstrates that\npre-training a sequence-to-sequence model but with a bidirectional decoder can\nproduce notable performance gains for both Autoregressive and\nNon-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked\nlanguage model pre-trained on large-scale bilingual and monolingual corpora in\nmany languages. We also introduce two simple but effective methods to enhance\nthe CeMAT, aligned code-switching &amp; masking and dynamic dual-masking. We\nconduct extensive experiments and show that our CeMAT can achieve significant\nperformance improvement for all scenarios from low- to extremely high-resource\nlanguages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on\naverage for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it\ncan also produce consistent performance gains, i.e., up to +5.3 BLEU. To the\nbest of our knowledge, this is the first work to pre-train a unified model for\nfine-tuning on both NMT tasks. Code, data, and pre-trained models are available\nat https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09590","description":"<p>With the emerging research effort to integrate structured and unstructured\nknowledge, many approaches incorporate factual knowledge into pre-trained\nlanguage models (PLMs) and apply the knowledge-enhanced PLMs on downstream NLP\ntasks. However, (1) they only consider static factual knowledge, but knowledge\ngraphs (KGs) also contain temporal facts or events indicating evolutionary\nrelationships among entities at different timestamps. (2) PLMs cannot be\ndirectly applied to many KG tasks, such as temporal KG completion.\n</p>\n<p>In this paper, we focus on \\textbf{e}nhancing temporal knowledge embeddings\nwith \\textbf{co}ntextualized \\textbf{la}nguage representations (ECOLA). We\nalign structured knowledge contained in temporal knowledge graphs with their\ntextual descriptions extracted from news articles and propose a novel\nknowledge-text prediction task to inject the abundant information from\ndescriptions into temporal knowledge embeddings. ECOLA jointly optimizes the\nknowledge-text prediction objective and the temporal knowledge embeddings,\nwhich can simultaneously take full advantage of textual and knowledge\ninformation. For training ECOLA, we introduce three temporal KG datasets with\naligned textual descriptions. Experimental results on the temporal knowledge\ngraph completion task show that ECOLA outperforms state-of-the-art temporal KG\nmodels by a large margin. The proposed datasets can serve as new temporal KG\nbenchmarks and facilitate future research on structured and unstructured\nknowledge integration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruotong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Beiyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zifeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppl_H/0/1/0/all/0/1\">Heinz K&#xf6;ppl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"FaceMap: Towards Unsupervised Face Clustering via Map Equation. (arXiv:2203.10090v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10090","description":"<p>Face clustering is an essential task in computer vision due to the explosion\nof related applications such as augmented reality or photo album management.\nThe main challenge of this task lies in the imperfectness of similarities among\nimage feature representations. Given an existing feature extraction model, it\nis still an unresolved problem that how can the inherent characteristics of\nsimilarities of unlabelled images be leveraged to improve the clustering\nperformance. Motivated by answering the question, we develop an effective\nunsupervised method, named as FaceMap, by formulating face clustering as a\nprocess of non-overlapping community detection, and minimizing the entropy of\ninformation flows on a network of images. The entropy is denoted by the map\nequation and its minimum represents the least description of paths among images\nin expectation. Inspired by observations on the ranked transition probabilities\nin the affinity graph constructed from facial images, we develop an outlier\ndetection strategy to adaptively adjust transition probabilities among images.\nExperiments with ablation studies demonstrate that FaceMap significantly\noutperforms existing methods and achieves new state-of-the-arts on three\npopular large-scale datasets for face clustering, e.g., an absolute improvement\nof more than $10\\%$ and $4\\%$ comparing with prior unsupervised and supervised\nmethods respectively in terms of average of Pairwise F-score. Our code is\npublicly available on github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaotian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Aibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Ling Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1\">Hanling Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label conditioned segmentation. (arXiv:2203.10091v1 [eess.IV])","link":"http://arxiv.org/abs/2203.10091","description":"<p>Semantic segmentation is an important task in computer vision that is often\ntackled with convolutional neural networks (CNNs). A CNN learns to produce\npixel-level predictions through training on pairs of images and their\ncorresponding ground-truth segmentation labels. For segmentation tasks with\nmultiple classes, the standard approach is to use a network that computes a\nmulti-channel probabilistic segmentation map, with each channel representing\none class. In applications where the image grid size (e.g., when it is a 3D\nvolume) and/or the number of labels is relatively large, the standard\n(baseline) approach can become prohibitively expensive for our computational\nresources. In this paper, we propose a simple yet effective method to address\nthis challenge. In our approach, the segmentation network produces a\nsingle-channel output, while being conditioned on a single class label, which\ndetermines the output class of the network. Our method, called label\nconditioned segmentation (LCS), can be used to segment images with a very large\nnumber of classes, which might be infeasible for the baseline approach. We also\ndemonstrate in the experiments that label conditioning can improve the accuracy\nof a given backbone architecture, likely, thanks to its parameter efficiency.\nFinally, as we show in our results, an LCS model can produce previously unseen\nfine-grained labels during inference time, when only coarse labels were\navailable during training. We provide all of our code here:\nhttps://github.com/tym002/Label-conditioned-segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_T/0/1/0/all/0/1\">Tianyu Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin C. Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selection of entropy based features for the analysis of the Archimedes' spiral applied to essential tremor. (arXiv:2203.10094v1 [q-bio.QM])","link":"http://arxiv.org/abs/2203.10094","description":"<p>Biomedical systems are regulated by interacting mechanisms that operate\nacross multiple spatial and temporal scales and produce biosignals with linear\nand non-linear information inside. In this sense entropy could provide a useful\nmeasure about disorder in the system, lack of information in time-series and/or\nirregularity of the signals. Essential tremor (ET) is the most common movement\ndisorder, being 20 times more common than Parkinson's disease, and 50-70% of\nthis disease cases are estimated to be genetic in origin. Archimedes spiral\ndrawing is one of the most used standard tests for clinical diagnosis. This\nwork, on selection of nonlinear biomarkers from drawings and handwriting, is\npart of a wide-ranging cross study for the diagnosis of essential tremor in\nBioDonostia Health Institute. Several entropy algorithms are used to generate\nnonlinear feayures. The automatic analysis system consists of several Machine\nLearning paradigms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Lopez_De_Ipina_K/0/1/0/all/0/1\">Karmele L&#xf3;pez-De-Ipi&#xf1;a</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bergareche_A/0/1/0/all/0/1\">Alberto Bergareche</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Riva_P/0/1/0/all/0/1\">Patricia De La Riva</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sole_Casals_J/0/1/0/all/0/1\">Jordi Sole-Casals</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marti_Masso_J/0/1/0/all/0/1\">Jose Felix Marti-Masso</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Iturrate_M/0/1/0/all/0/1\">Mikel Iturrate</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Beitia_B/0/1/0/all/0/1\">Blanca Beitia</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Calvo_P/0/1/0/all/0/1\">Pilar Calvo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sesa_Nogueras_E/0/1/0/all/0/1\">Enric Sesa-Nogueras</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Roure_J/0/1/0/all/0/1\">Josep Roure</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gurrutxaga_I/0/1/0/all/0/1\">Itziar Gurrutxaga</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Garcia_Melero_J/0/1/0/all/0/1\">Joseba Garcia-Melero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlignTransformer: Hierarchical Alignment of Visual Regions and Disease Tags for Medical Report Generation. (arXiv:2203.10095v1 [eess.IV])","link":"http://arxiv.org/abs/2203.10095","description":"<p>Recently, medical report generation, which aims to automatically generate a\nlong and coherent descriptive paragraph of a given medical image, has received\ngrowing research interests. Different from the general image captioning tasks,\nmedical report generation is more challenging for data-driven neural models.\nThis is mainly due to 1) the serious data bias: the normal visual regions\ndominate the dataset over the abnormal visual regions, and 2) the very long\nsequence. To alleviate above two problems, we propose an AlignTransformer\nframework, which includes the Align Hierarchical Attention (AHA) and the\nMulti-Grained Transformer (MGT) modules: 1) AHA module first predicts the\ndisease tags from the input image and then learns the multi-grained visual\nfeatures by hierarchically aligning the visual regions and disease tags. The\nacquired disease-grounded visual features can better represent the abnormal\nregions of the input image, which could alleviate data bias problem; 2) MGT\nmodule effectively uses the multi-grained features and Transformer framework to\ngenerate the long medical report. The experiments on the public IU-Xray and\nMIMIC-CXR datasets show that the AlignTransformer can achieve results\ncompetitive with state-of-the-art methods on the two datasets. Moreover, the\nhuman evaluation conducted by professional radiologists further proves the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+You_D/0/1/0/all/0/1\">Di You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1\">Xiaoxia Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Perceptual Model for Estimating the Quality of Visual Speech. (arXiv:2203.10117v1 [cs.SD])","link":"http://arxiv.org/abs/2203.10117","description":"<p>Generating realistic lip motions to simulate speech production is key for\ndriving natural character animations from audio. Previous research has shown\nthat traditional metrics used to optimize and assess models for generating lip\nmotions from speech are not a good indicator of subjective opinion of animation\nquality. Yet, running repetitive subjective studies for assessing the quality\nof animations can be time-consuming and difficult to replicate. In this work,\nwe seek to understand the relationship between perturbed lip motion and\nsubjective opinion of lip motion quality. Specifically, we adjust the degree of\narticulation for lip motion sequences and run a user-study to examine how this\nadjustment impacts the perceived quality of lip motion. We then train a model\nusing the scores collected from our user-study to automatically predict the\nsubjective quality of an animated sequence. Our results show that (1) users\nscore lip motions with slight over-articulation the highest in terms of\nperceptual quality; (2) under-articulation had a more detrimental effect on\nperceived quality of lip motion compared to the effect of over-articulation;\nand (3) we can automatically estimate the subjective perceptual score for a\ngiven lip motion sequences with low error rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aldeneh_Z/0/1/0/all/0/1\">Zakaria Aldeneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedzechkina_M/0/1/0/all/0/1\">Masha Fedzechkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seto_S/0/1/0/all/0/1\">Skyler Seto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metcalf_K/0/1/0/all/0/1\">Katherine Metcalf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarabia_M/0/1/0/all/0/1\">Miguel Sarabia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobald_B/0/1/0/all/0/1\">Barry-John Theobald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI system for fetal ultrasound in low-resource settings. (arXiv:2203.10139v1 [cs.LG])","link":"http://arxiv.org/abs/2203.10139","description":"<p>Despite considerable progress in maternal healthcare, maternal and perinatal\ndeaths remain high in low-to-middle income countries. Fetal ultrasound is an\nimportant component of antenatal care, but shortage of adequately trained\nhealthcare workers has limited its adoption. We developed and validated an\nartificial intelligence (AI) system that uses novice-acquired \"blind sweep\"\nultrasound videos to estimate gestational age (GA) and fetal malpresentation.\nWe further addressed obstacles that may be encountered in low-resourced\nsettings. Using a simplified sweep protocol with real-time AI feedback on sweep\nquality, we have demonstrated the generalization of model performance to\nminimally trained novice ultrasound operators using low cost ultrasound devices\nwith on-device AI integration. The GA model was non-inferior to standard fetal\nbiometry estimates with as few as two sweeps, and the fetal malpresentation\nmodel had high AUC-ROCs across operators and devices. Our AI models have the\npotential to assist in upleveling the capabilities of lightly trained\nultrasound operators in low resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomes_R/0/1/0/all/0/1\">Ryan G. Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vwalika_B/0/1/0/all/0/1\">Bellington Vwalika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chace Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_A/0/1/0/all/0/1\">Angelica Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sieniek_M/0/1/0/all/0/1\">Marcin Sieniek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_J/0/1/0/all/0/1\">Joan T. Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Christina Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaro_M/0/1/0/all/0/1\">Margaret P. Kasaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1\">James A. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stringer_E/0/1/0/all/0/1\">Elizabeth M. Stringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinney_S/0/1/0/all/0/1\">Scott Mayer McKinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindano_N/0/1/0/all/0/1\">Ntazana Sindano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahl_G/0/1/0/all/0/1\">George E. Dahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodnight_W/0/1/0/all/0/1\">William Goodnight III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1\">Justin Gilmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1\">Benjamin H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1\">Charles Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spitz_T/0/1/0/all/0/1\">Terry Spitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saensuksopa_T/0/1/0/all/0/1\">T Saensuksopa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kris Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jonny Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilgrim_R/0/1/0/all/0/1\">Rory Pilgrim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uddin_A/0/1/0/all/0/1\">Akib Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1\">Greg Corrado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lily Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_K/0/1/0/all/0/1\">Katherine Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tse_D/0/1/0/all/0/1\">Daniel Tse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stringer_J/0/1/0/all/0/1\">Jeffrey S. A. Stringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1\">Shravya Shetty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Closing the Generalization Gap of Cross-silo Federated Medical Image Segmentation. (arXiv:2203.10144v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10144","description":"<p>Cross-silo federated learning (FL) has attracted much attention in medical\nimaging analysis with deep learning in recent years as it can resolve the\ncritical issues of insufficient data, data privacy, and training efficiency.\nHowever, there can be a generalization gap between the model trained from FL\nand the one from centralized training. This important issue comes from the\nnon-iid data distribution of the local data in the participating clients and is\nwell-known as client drift. In this work, we propose a novel training framework\nFedSM to avoid the client drift issue and successfully close the generalization\ngap compared with the centralized training for medical image segmentation tasks\nfor the first time. We also propose a novel personalized FL objective\nformulation and a new method SoftPull to solve it in our proposed framework\nFedSM. We conduct rigorous theoretical analysis to guarantee its convergence\nfor optimizing the non-convex smooth objective function. Real-world medical\nimage segmentation experiments using deep FL validate the motivations and\neffectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">An Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Can Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10157","description":"<p>Novel view synthesis is a long-standing problem. In this work, we consider a\nvariant of the problem where we are given only a few context views sparsely\ncovering a scene or an object. The goal is to predict novel viewpoints in the\nscene, which requires learning priors. The current state of the art is based on\nNeural Radiance Fields (NeRFs), and while achieving impressive results, the\nmethods suffer from long training times as they require evaluating thousands of\n3D point samples via a deep neural network for each image. We propose a 2D-only\nmethod that maps multiple context views and a query pose to a new image in a\nsingle pass of a neural network. Our model uses a two-stage architecture\nconsisting of a codebook and a transformer model. The codebook is used to embed\nindividual images into a smaller latent space, and the transformer solves the\nview synthesis task in this more compact space. To train our model efficiently,\nwe introduce a novel branching attention mechanism that allows us to use the\nsame model not only for neural rendering but also for camera pose estimation.\nExperimental results on real-world scenes show that our approach is competitive\ncompared to NeRF-based methods while not reasoning in 3D, and it is faster to\ntrain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulhanek_J/0/1/0/all/0/1\">Jon&#xe1;&#x161; Kulh&#xe1;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derner_E/0/1/0/all/0/1\">Erik Derner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1\">Robert Babu&#x161;ka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Objects that Can Move. (arXiv:2203.10159v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10159","description":"<p>This paper studies the problem of object discovery -- separating objects from\nthe background without manual labels. Existing approaches utilize appearance\ncues, such as color, texture, and location, to group pixels into object-like\nregions. However, by relying on appearance alone, these methods fail to\nseparate objects from the background in cluttered scenes. This is a fundamental\nlimitation since the definition of an object is inherently ambiguous and\ncontext-dependent. To resolve this ambiguity, we choose to focus on dynamic\nobjects -- entities that can move independently in the world. We then scale the\nrecent auto-encoder based frameworks for unsupervised object discovery from toy\nsynthetic images to complex real-world scenes. To this end, we simplify their\narchitecture, and augment the resulting model with a weak learning signal from\ngeneral motion segmentation algorithms. Our experiments demonstrate that,\ndespite only capturing a small subset of the objects that move, this signal is\nenough to generalize to segment both moving and static instances of dynamic\nobjects. We show that our model scales to a newly collected, photo-realistic\nsynthetic dataset with street driving scenarios. Additionally, we leverage\nground truth segmentation and flow annotations in this dataset for thorough\nablation and evaluation. Finally, our experiments on the real-world KITTI\nbenchmark demonstrate that the proposed approach outperforms both heuristic-\nand learning-based methods by capitalizing on motion cues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhipeng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokmakov_P/0/1/0/all/0/1\">Pavel Tokmakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabri_A/0/1/0/all/0/1\">Allan Jabri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1\">Martial Hebert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Knowledge Distillation with Features, Logits, and Gradients. (arXiv:2203.10163v1 [cs.LG])","link":"http://arxiv.org/abs/2203.10163","description":"<p>Knowledge distillation (KD) is a substantial strategy for transferring\nlearned knowledge from one neural network model to another. A vast number of\nmethods have been developed for this strategy. While most method designs a more\nefficient way to facilitate knowledge transfer, less attention has been put on\ncomparing the effect of knowledge sources such as features, logits, and\ngradients. This work provides a new perspective to motivate a set of knowledge\ndistillation strategies by approximating the classical KL-divergence criteria\nwith different knowledge sources, making a systematic comparison possible in\nmodel compression and incremental learning. Our analysis indicates that logits\nare generally a more efficient knowledge source and suggests that having\nsufficient feature dimensions is crucial for the model design, providing a\npractical guideline for effective KD-based transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept-based Adversarial Attacks: Tricking Humans and Classifiers Alike. (arXiv:2203.10166v1 [cs.LG])","link":"http://arxiv.org/abs/2203.10166","description":"<p>We propose to generate adversarial samples by modifying activations of upper\nlayers encoding semantically meaningful concepts. The original sample is\nshifted towards a target sample, yielding an adversarial sample, by using the\nmodified activations to reconstruct the original sample. A human might (and\npossibly should) notice differences between the original and the adversarial\nsample. Depending on the attacker-provided constraints, an adversarial sample\ncan exhibit subtle differences or appear like a \"forged\" sample from another\nclass. Our approach and goal are in stark contrast to common attacks involving\nperturbations of single pixels that are not recognizable by humans. Our\napproach is relevant in, e.g., multi-stage processing of inputs, where both\nhumans and machines are involved in decision-making because invisible\nperturbations will not fool a human. Our evaluation focuses on deep neural\nnetworks. We also show the transferability of our adversarial examples among\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Johannes Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apruzzese_G/0/1/0/all/0/1\">Giovanni Apruzzese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of April Tag and WhyCode Fiducial Systems for Autonomous Precision Drone Landing with a Gimbal-Mounted Camera. (arXiv:2203.10180v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10180","description":"<p>Fiducial markers provide a computationally cheap way for drones to determine\ntheir location with respect to a landing pad and execute precision landings.\nHowever, most existing work in this field uses a fixed, downward facing camera\nthat does not leverage the common gimbal-mounted camera setup found on many\ndrones. Such rigid systems cannot easily track detected markers, and may lose\nsight of the markers in non-ideal conditions (e.g. wind gusts). This paper\nevaluates April Tag and WhyCode fiducial systems for drone landing with a\ngimbal-mounted, monocular camera, with the advantage that the drone system can\ntrack the marker over time. However, since the orientation of the camera\nchanges, we must know the orientation of the marker, which is unreliable in\nmonocular fiducial systems. Additionally, the system must be fast. We propose 2\nmethods for mitigating the orientation ambiguity of WhyCode, and 1 method for\nincreasing the runtime detection rate of April Tag. We evaluate our 3 systems\nagainst 2 default systems in terms of marker orientation ambiguity, and\ndetection rate. We test rates of marker detection in a ROS framework on a\nRaspberry Pi 4, and we rank the systems in terms of their performance. Our\nfirst WhyCode variant significantly reduces orientation ambiguity with an\ninsignificant reduction in detection rate. Our second WhyCode variant does not\nshow significantly different orientation ambiguity from the default WhyCode\nsystem, but does provide additional functionality in terms of multi-marker\nWhyCode bundle arrangements. Our April Tag variant does not show performance\nimprovements on a Raspberry Pi 4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1\">Joshua Springer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyas_M/0/1/0/all/0/1\">Marcel Kyas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks on Deep Learning-based Video Compression and Classification Systems. (arXiv:2203.10183v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10183","description":"<p>Video compression plays a crucial role in enabling video streaming and\nclassification systems and maximizing the end-user quality of experience (QoE)\nat a given bandwidth budget. In this paper, we conduct the first systematic\nstudy for adversarial attacks on deep learning based video compression and\ndownstream classification systems. We propose an adaptive adversarial attack\nthat can manipulate the Rate-Distortion (R-D) relationship of a video\ncompression model to achieve two adversarial goals: (1) increasing the network\nbandwidth or (2) degrading the video quality for end-users. We further devise\nnovel objectives for targeted and untargeted attacks to a downstream video\nclassification service. Finally, we design an input-invariant perturbation that\nuniversally disrupts video compression and classification systems in real time.\nUnlike previously proposed attacks on video classification, our adversarial\nperturbations are the first to withstand compression. We empirically show the\nresilience of our attacks against various defenses, i.e., adversarial training,\nvideo denoising, and JPEG compression. Our extensive experimental results on\nvarious video datasets demonstrate the effectiveness of our attacks. Our video\nquality and bandwidth attacks deteriorate peak signal-to-noise ratio by up to\n5.4dB and the bit-rate by up to 2.4 times on the standard video compression\ndatasets while achieving over 90% attack success rate on a downstream\nclassifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jung-Woo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javaheripi_M/0/1/0/all/0/1\">Mojan Javaheripi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidano_S/0/1/0/all/0/1\">Seira Hidano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional-Flow NeRF: Accurate 3D Modelling with Reliable Uncertainty Quantification. (arXiv:2203.10192v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10192","description":"<p>A critical limitation of current methods based on Neural Radiance Fields\n(NeRF) is that they are unable to quantify the uncertainty associated with the\nlearned appearance and geometry of the scene. This information is paramount in\nreal applications such as medical diagnosis or autonomous driving where, to\nreduce potentially catastrophic failures, the confidence on the model outputs\nmust be included into the decision-making process. In this context, we\nintroduce Conditional-Flow NeRF (CF-NeRF), a novel probabilistic framework to\nincorporate uncertainty quantification into NeRF-based approaches. For this\npurpose, our method learns a distribution over all possible radiance fields\nmodelling which is used to quantify the uncertainty associated with the\nmodelled scene. In contrast to previous approaches enforcing strong constraints\nover the radiance field distribution, CF-NeRF learns it in a flexible and fully\ndata-driven manner by coupling Latent Variable Modelling and Conditional\nNormalizing Flows. This strategy allows to obtain reliable uncertainty\nestimation while preserving model expressivity. Compared to previous\nstate-of-the-art methods proposed for uncertainty quantification in NeRF, our\nexperiments show that the proposed method achieves significantly lower\nprediction errors and more reliable uncertainty values for synthetic novel view\nand depth-map estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianxiong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1\">Adria Ruiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis and Adaptation of YOLOv4 for Object Detection in Aerial Images. (arXiv:2203.10194v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10194","description":"<p>The recent and rapid growth in Unmanned Aerial Vehicles (UAVs) deployment for\nvarious computer vision tasks has paved the path for numerous opportunities to\nmake them more effective and valuable. Object detection in aerial images is\nchallenging due to variations in appearance, pose, and scale. Autonomous aerial\nflight systems with their inherited limited memory and computational power\ndemand accurate and computationally efficient detection algorithms for\nreal-time applications. Our work shows the adaptation of the popular YOLOv4\nframework for predicting the objects and their locations in aerial images with\nhigh accuracy and inference speed. We utilized transfer learning for faster\nconvergence of the model on the VisDrone DET aerial object detection dataset.\nThe trained model resulted in a mean average precision (mAP) of 45.64% with an\ninference speed reaching 8.7 FPS on the Tesla K80 GPU and was highly accurate\nin detecting truncated and occluded objects. We experimentally evaluated the\nimpact of varying network resolution sizes and training epochs on the\nperformance. A comparative study with several contemporary aerial object\ndetectors proved that YOLOv4 performed better, implying a more suitable\ndetection algorithm to incorporate on aerial platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samyal_A/0/1/0/all/0/1\">Aryaman Singh Samyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_A/0/1/0/all/0/1\">Akshatha K R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hans_S/0/1/0/all/0/1\">Soham Hans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karunakar A K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+B_S/0/1/0/all/0/1\">Satish Shenoy B</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10196","description":"<p>We propose MisMatch, a novel consistency-driven semi-supervised segmentation\nframework which produces predictions that are invariant to learnt feature\nperturbations. MisMatch consists of an encoder and a two-head decoders. One\ndecoder learns positive attention to the foreground regions of interest (RoI)\non unlabelled images thereby generating dilated features. The other decoder\nlearns negative attention to the foreground on the same unlabelled images\nthereby generating eroded features. We then apply a consistency regularisation\non the paired predictions. MisMatch outperforms state-of-the-art\nsemi-supervised methods on a CT-based pulmonary vessel segmentation task and a\nMRI-based brain tumour segmentation task. In addition, we show that the\neffectiveness of MisMatch comes from better model calibration than its\nsupervised learning counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mou-Cheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu-Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumberg_S/0/1/0/all/0/1\">Stefano B Blumberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_F/0/1/0/all/0/1\">Frederick J Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+deGroot_M/0/1/0/all/0/1\">Marius deGroot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oxtoby_N/0/1/0/all/0/1\">Neil P. Oxtoby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1\">Joseph Jacob</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relationformer: A Unified Framework for Image-to-Graph Generation. (arXiv:2203.10202v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10202","description":"<p>A comprehensive representation of an image requires understanding objects and\ntheir mutual relationship, especially in image-to-graph generation, e.g., road\nnetwork extraction, blood-vessel network extraction, or scene graph generation.\nTraditionally, image-to-graph generation is addressed with a two-stage approach\nconsisting of object detection followed by a separate relation prediction,\nwhich prevents simultaneous object-relation interaction. This work proposes a\nunified one-stage transformer-based framework, namely Relationformer, that\njointly predicts objects and their relations. We leverage direct set-based\nobject prediction and incorporate the interaction among the objects to learn an\nobject-relation representation jointly. In addition to existing [obj]-tokens,\nwe propose a novel learnable token, namely [rln]-token. Together with\n[obj]-tokens, [rln]-token exploits local and global semantic reasoning in an\nimage through a series of mutual associations. In combination with the\npair-wise [obj]-token, the [rln]-token contributes to a computationally\nefficient relation prediction. We achieve state-of-the-art performance on\nmultiple, diverse and multi-domain datasets that demonstrate our approach's\neffectiveness and generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1\">Rajat Koner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wittmann_B/0/1/0/all/0/1\">Bastian Wittmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paetzold_J/0/1/0/all/0/1\">Johannes Paetzold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiazhen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifzadeh_S/0/1/0/all/0/1\">Sahand Sharifzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring topological transitions in pattern-forming processes with self-supervised learning. (arXiv:2203.10204v1 [cond-mat.mtrl-sci])","link":"http://arxiv.org/abs/2203.10204","description":"<p>The identification and classification of transitions in topological and\nmicrostructural regimes in pattern-forming processes is critical for\nunderstanding and fabricating microstructurally precise novel materials in many\napplication domains. Unfortunately, relevant microstructure transitions may\ndepend on process parameters in subtle and complex ways that are not captured\nby the classic theory of phase transition. While supervised machine learning\nmethods may be useful for identifying transition regimes, they need labels\nwhich require prior knowledge of order parameters or relevant structures.\nMotivated by the universality principle for dynamical systems, we instead use a\nself-supervised approach to solve the inverse problem of predicting process\nparameters from observed microstructures using neural networks. This approach\ndoes not require labeled data about the target task of predicting\nmicrostructure transitions. We show that the difficulty of performing this\nprediction task is related to the goal of discovering microstructure regimes,\nbecause qualitative changes in microstructural patterns correspond to changes\nin uncertainty for our self-supervised prediction problem. We demonstrate the\nvalue of our approach by automatically discovering transitions in\nmicrostructural regimes in two distinct pattern-forming processes: the spinodal\ndecomposition of a two-phase mixture and the formation of concentration\nmodulations of binary alloys during physical vapor deposition of thin films.\nThis approach opens a promising path forward for discovering and understanding\nunseen or hard-to-detect transition regimes, and ultimately for controlling\ncomplex pattern-forming processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Abram_M/0/1/0/all/0/1\">Marcin Abram</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Burghardt_K/0/1/0/all/0/1\">Keith Burghardt</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Dingreville_R/0/1/0/all/0/1\">Remi Dingreville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwinTextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition. (arXiv:2203.10209v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10209","description":"<p>End-to-end scene text spotting has attracted great attention in recent years\ndue to the success of excavating the intrinsic synergy of the scene text\ndetection and recognition. However, recent state-of-the-art methods usually\nincorporate detection and recognition simply by sharing the backbone, which\ndoes not directly take advantage of the feature interaction between the two\ntasks. In this paper, we propose a new end-to-end scene text spotting framework\ntermed SwinTextSpotter. Using a transformer encoder with dynamic head as the\ndetector, we unify the two tasks with a novel Recognition Conversion mechanism\nto explicitly guide text localization through recognition loss. The\nstraightforward design results in a concise framework that requires neither\nadditional rectification module nor character-level annotation for the\narbitrarily-shaped text. Qualitative and quantitative experiments on\nmulti-oriented datasets RoIC13 and ICDAR 2015, arbitrarily-shaped datasets\nTotal-Text and CTW1500, and multi-lingual datasets ReCTS (Chinese) and VinText\n(Vietnamese) demonstrate SwinTextSpotter significantly outperforms existing\nmethods. Code is available at https://github.com/mxin262/SwinTextSpotter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhenghao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chongyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kai Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction. (arXiv:2203.10212v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10212","description":"<p>Semantic 3D keypoints are category-level semantic consistent points on 3D\nobjects. Detecting 3D semantic keypoints is a foundation for a number of 3D\nvision tasks but remains challenging, due to the ambiguity of semantic\ninformation, especially when the objects are represented by unordered 3D point\nclouds. Existing unsupervised methods tend to generate category-level keypoints\nin implicit manners, making it difficult to extract high-level information,\nsuch as semantic labels and topology. From a novel mutual reconstruction\nperspective, we present an unsupervised method to generate consistent semantic\nkeypoints from point clouds explicitly. To achieve this, the proposed model\npredicts keypoints that not only reconstruct the object itself but also\nreconstruct other instances in the same category. To the best of our knowledge,\nthe proposed method is the first to mine 3D semantic consistent keypoints from\na mutual reconstruction view. Experiments under various evaluation metrics as\nwell as comparisons with the state-of-the-arts demonstrate the efficacy of our\nnew solution to mining semantic consistent keypoints with mutual\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haocheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shichao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiaxi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaqi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Volkit: A Performance-Portable Computer Vision Library for 3D Volumetric Data. (arXiv:2203.10213v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10213","description":"<p>We present volkit, an open source library with high performance\nimplementations of image manipulation and computer vision algorithms that focus\non 3D volumetric representations. Volkit implements a cross-platform,\nperformance-portable API targeting both CPUs and GPUs that defers data and\nresource movement and hides them from the application developer using a managed\nAPI. We use volkit to process medical and simulation data that is rendered in\nVR and consequently integrated the library into the C++ virtual reality\nsoftware CalVR. The paper presents case studies and performance results and by\nthat demonstrates the library's effectiveness and the efficiency of this\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellmann_S/0/1/0/all/0/1\">Stefan Zellmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguirre_G/0/1/0/all/0/1\">Giovanni Aguirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulze_J/0/1/0/all/0/1\">J&#xfc;rgen P. Schulze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition. (arXiv:2203.10233v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10233","description":"<p>Human action recognition has recently become one of the popular research\ntopics in the computer vision community. Various 3D-CNN based methods have been\npresented to tackle both the spatial and temporal dimensions in the task of\nvideo action recognition with competitive results. However, these methods have\nsuffered some fundamental limitations such as lack of robustness and\ngeneralization, e.g., how does the temporal ordering of video frames affect the\nrecognition results? This work presents a novel end-to-end Transformer-based\nDirected Attention (DirecFormer) framework for robust action recognition. The\nmethod takes a simple but novel perspective of Transformer-based approach to\nunderstand the right order of sequence actions. Therefore, the contributions of\nthis work are three-fold. Firstly, we introduce the problem of ordered temporal\nlearning issues to the action recognition problem. Secondly, a new Directed\nAttention mechanism is introduced to understand and provide attentions to human\nactions in the right order. Thirdly, we introduce the conditional dependency in\naction sequence modeling that includes orders and classes. The proposed\napproach consistently achieves the state-of-the-art (SOTA) results compared\nwith the recent action recognition methods, on three standard large-scale\nbenchmarks, i.e. Jester, Kinetics-400 and Something-Something-V2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quoc-Huy Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_H/0/1/0/all/0/1\">Han-Seok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_S/0/1/0/all/0/1\">Son Lam Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIPA: Hierarchical Patch Transformer for Single Image Super Resolution. (arXiv:2203.10247v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10247","description":"<p>Transformer-based architectures start to emerge in single image super\nresolution (SISR) and have achieved promising performance. Most existing Vision\nTransformers divide images into the same number of patches with a fixed size,\nwhich may not be optimal for restoring patches with different levels of texture\nrichness. This paper presents HIPA, a novel Transformer architecture that\nprogressively recovers the high resolution image using a hierarchical patch\npartition. Specifically, we build a cascaded model that processes an input\nimage in multiple stages, where we start with tokens with small patch sizes and\ngradually merge to the full resolution. Such a hierarchical patch mechanism not\nonly explicitly enables feature aggregation at multiple resolutions but also\nadaptively learns patch-aware features for different image regions, e.g., using\na smaller patch for areas with fine details and a larger patch for textureless\nregions. Meanwhile, a new attention-based position encoding scheme for\nTransformer is proposed to let the network focus on which tokens should be paid\nmore attention by assigning different weights to different tokens, which is the\nfirst time to our best knowledge. Furthermore, we also propose a new\nmulti-reception field attention module to enlarge the convolution reception\nfield from different branches. The experimental results on several public\ndatasets demonstrate the superior performance of the proposed HIPA over\nprevious methods quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qing Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yee-Hong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation-Agnostic Shape Fields. (arXiv:2203.10259v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10259","description":"<p>3D shape analysis has been widely explored in the era of deep learning.\nNumerous models have been developed for various 3D data representation formats,\ne.g., MeshCNN for meshes, PointNet for point clouds and VoxNet for voxels. In\nthis study, we present Representation-Agnostic Shape Fields (RASF), a\ngeneralizable and computation-efficient shape embedding module for 3D deep\nlearning. RASF is implemented with a learnable 3D grid with multiple channels\nto store local geometry. Based on RASF, shape embeddings for various 3D shape\nrepresentations (point clouds, meshes and voxels) are retrieved by coordinate\nindexing. While there are multiple ways to optimize the learnable parameters of\nRASF, we provide two effective schemes among all in this paper for RASF\npre-training: shape reconstruction and normal estimation. Once trained, RASF\nbecomes a plug-and-play performance booster with negligible cost. Extensive\nexperiments on diverse 3D representation formats, networks and applications,\nvalidate the universal effectiveness of the proposed RASF. Code and pre-trained\nmodels are publicly available https://github.com/seanywang0408/RASF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Teng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Self-Supervised Low-Rank Network for Single-Stage Weakly and Semi-Supervised Semantic Segmentation. (arXiv:2203.10278v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10278","description":"<p>Semantic segmentation with limited annotations, such as weakly supervised\nsemantic segmentation (WSSS) and semi-supervised semantic segmentation (SSSS),\nis a challenging task that has attracted much attention recently. Most leading\nWSSS methods employ a sophisticated multi-stage training strategy to estimate\npseudo-labels as precise as possible, but they suffer from high model\ncomplexity. In contrast, there exists another research line that trains a\nsingle network with image-level labels in one training cycle. However, such a\nsingle-stage strategy often performs poorly because of the compounding effect\ncaused by inaccurate pseudo-label estimation. To address this issue, this paper\npresents a Self-supervised Low-Rank Network (SLRNet) for single-stage WSSS and\nSSSS. The SLRNet uses cross-view self-supervision, that is, it simultaneously\npredicts several complementary attentive LR representations from different\nviews of an image to learn precise pseudo-labels. Specifically, we reformulate\nthe LR representation learning as a collective matrix factorization problem and\noptimize it jointly with the network learning in an end-to-end manner. The\nresulting LR representation deprecates noisy information while capturing stable\nsemantics across different views, making it robust to the input variations,\nthereby reducing overfitting to self-supervision errors. The SLRNet can provide\na unified single-stage framework for various label-efficient semantic\nsegmentation settings: 1) WSSS with image-level labeled data, 2) SSSS with a\nfew pixel-level labeled data, and 3) SSSS with a few pixel-level labeled data\nand many image-level labeled data. Extensive experiments on the Pascal VOC\n2012, COCO, and L2ID datasets demonstrate that our SLRNet outperforms both\nstate-of-the-art WSSS and SSSS methods with a variety of different settings,\nproving its good generalizability and efficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junwen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation. (arXiv:2203.10291v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10291","description":"<p>For video frame interpolation (VFI), existing deep-learning-based approaches\nstrongly rely on the ground-truth (GT) intermediate frames, which sometimes\nignore the non-unique nature of motion judging from the given adjacent frames.\nAs a result, these methods tend to produce averaged solutions that are not\nclear enough. To alleviate this issue, we propose to relax the requirement of\nreconstructing an intermediate frame as close to the GT as possible. Towards\nthis end, we develop a texture consistency loss (TCL) upon the assumption that\nthe interpolated content should maintain similar structures with their\ncounterparts in the given frames. Predictions satisfying this constraint are\nencouraged, though they may differ from the pre-defined GT. Without the bells\nand whistles, our plug-and-play TCL is capable of improving the performance of\nexisting VFI frameworks. On the other hand, previous methods usually adopt the\ncost volume or correlation map to achieve more accurate image/feature warping.\nHowever, the O(N^2) ({N refers to the pixel count}) computational complexity\nmakes it infeasible for high-resolution cases. In this work, we design a\nsimple, efficient (O(N)) yet powerful cross-scale pyramid alignment (CSPA)\nmodule, where multi-scale information is highly exploited. Extensive\nexperiments justify the efficiency and effectiveness of the proposed strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangbo Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Few-Shot Learning via Implanting and Compressing. (arXiv:2203.10297v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10297","description":"<p>This work focuses on tackling the challenging but realistic visual task of\nIncremental Few-Shot Learning (IFSL), which requires a model to continually\nlearn novel classes from only a few examples while not forgetting the base\nclasses on which it was pre-trained. Our study reveals that the challenges of\nIFSL lie in both inter-class separation and novel-class representation. Dur to\nintra-class variation, a novel class may implicitly leverage the knowledge from\nmultiple base classes to construct its feature representation. Hence, simply\nreusing the pre-trained embedding space could lead to a scattered feature\ndistribution and result in category confusion. To address such issues, we\npropose a two-step learning strategy referred to as \\textbf{Im}planting and\n\\textbf{Co}mpressing (\\textbf{IMCO}), which optimizes both feature space\npartition and novel class reconstruction in a systematic manner. Specifically,\nin the \\textbf{Implanting} step, we propose to mimic the data distribution of\nnovel classes with the assistance of data-abundant base set, so that a model\ncould learn semantically-rich features that are beneficial for discriminating\nbetween the base and other unseen classes. In the \\textbf{Compressing} step, we\nadapt the feature extractor to precisely represent each novel class for\nenhancing intra-class compactness, together with a regularized parameter\nupdating rule for preventing aggressive model updating. Finally, we demonstrate\nthat IMCO outperforms competing baselines with a significant margin, both in\nimage classification task and more challenging object detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haiyue Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xijia Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zilong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Cheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadakkepat_P/0/1/0/all/0/1\">Prahlad Vadakkepat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tong Heng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling nonlinear dependencies in the latent space of inverse scattering. (arXiv:2203.10307v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10307","description":"<p>The problem of inverse scattering proposed by Angles and Mallat in 2018,\nconcerns training a deep neural network to invert the scattering transform\napplied to an image. After such a network is trained, it can be used as a\ngenerative model given that we can sample from the distribution of principal\ncomponents of scattering coefficients. For this purpose, Angles and Mallat\nsimply use samples from independent Gaussians. However, as shown in this paper,\nthe distribution of interest can actually be very far from normal and\nnon-negligible dependencies might exist between different coefficients. This\nmotivates using models for this distribution that allow for non-linear\ndependencies between variables. Within this paper, two such models are\nexplored, namely a Variational AutoEncoder and a Generative Adversarial\nNetwork. We demonstrate the results obtained can be extremely realistic on some\ndatasets and look better than those produced by Angles and Mallat. The\nconducted meta-analysis also shows a clear practical advantage of such\nconstructed generative models in terms of the efficiency of their training\nprocess compared to existing generative models for images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziomek_J/0/1/0/all/0/1\">Juliusz Ziomek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrahi_K/0/1/0/all/0/1\">Katayoun Farrahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds. (arXiv:2203.10314v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10314","description":"<p>Transformer has demonstrated promising performance in many 2D vision tasks.\nHowever, it is cumbersome to compute the self-attention on large-scale point\ncloud data because point cloud is a long sequence and unevenly distributed in\n3D space. To solve this issue, existing methods usually compute self-attention\nlocally by grouping the points into clusters of the same size, or perform\nconvolutional self-attention on a discretized representation. However, the\nformer results in stochastic point dropout, while the latter typically has\nnarrow attention fields. In this paper, we propose a novel voxel-based\narchitecture, namely Voxel Set Transformer (VoxSeT), to detect 3D objects from\npoint clouds by means of set-to-set translation. VoxSeT is built upon a\nvoxel-based set attention (VSA) module, which reduces the self-attention in\neach voxel by two cross-attentions and models features in a hidden space\ninduced by a group of latent codes. With the VSA module, VoxSeT can manage\nvoxelized point clusters with arbitrary size in a wide range, and process them\nin parallel with linear complexity. The proposed VoxSeT integrates the high\nperformance of transformer with the efficiency of voxel-based model, which can\nbe used as a good alternative to the convolutional and point-based backbones.\nVoxSeT reports competitive results on the KITTI and Waymo detection benchmarks.\nThe source codes can be found at \\url{https://github.com/skyhehe123/VoxSeT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation Meets Zero-Shot Learning: An Annotation-Efficient Approach to Multi-Modality Medical Image Segmentation. (arXiv:2203.10332v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10332","description":"<p>Due to the lack of properly annotated medical data, exploring the\ngeneralization capability of the deep model is becoming a public concern.\nZero-shot learning (ZSL) has emerged in recent years to equip the deep model\nwith the ability to recognize unseen classes. However, existing studies mainly\nfocus on natural images, which utilize linguistic models to extract auxiliary\ninformation for ZSL. It is impractical to apply the natural image ZSL solutions\ndirectly to medical images, since the medical terminology is very\ndomain-specific, and it is not easy to acquire linguistic models for the\nmedical terminology. In this work, we propose a new paradigm of ZSL\nspecifically for medical images utilizing cross-modality information. We make\nthree main contributions with the proposed paradigm. First, we extract the\nprior knowledge about the segmentation targets, called relation prototypes,\nfrom the prior model and then propose a cross-modality adaptation module to\ninherit the prototypes to the zero-shot model. Second, we propose a relation\nprototype awareness module to make the zero-shot model aware of information\ncontained in the prototypes. Last but not least, we develop an inheritance\nattention module to recalibrate the relation prototypes to enhance the\ninheritance process. The proposed framework is evaluated on two public\ncross-modality datasets including a cardiac dataset and an abdominal dataset.\nExtensive experiments show that the proposed framework significantly\noutperforms the state of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenglang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shuang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TO-FLOW: Efficient Continuous Normalizing Flows with Temporal Optimization adjoint with Moving Speed. (arXiv:2203.10335v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10335","description":"<p>Continuous normalizing flows (CNFs) construct invertible mappings between an\narbitrary complex distribution and an isotropic Gaussian distribution using\nNeural Ordinary Differential Equations (neural ODEs). It has not been tractable\non large datasets due to the incremental complexity of the neural ODE training.\nOptimal Transport theory has been applied to regularize the dynamics of the ODE\nto speed up training in recent works. In this paper, a temporal optimization is\nproposed by optimizing the evolutionary time for forward propagation of the\nneural ODE training. In this appoach, we optimize the network weights of the\nCNF alternately with evolutionary time by coordinate descent. Further with\ntemporal regularization, stability of the evolution is ensured. This approach\ncan be used in conjunction with the original regularization approach. We have\nexperimentally demonstrated that the proposed approach can significantly\naccelerate training without sacrifying performance over baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shian Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yihong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Delu Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occlusion-Aware Self-Supervised Monocular 6D Object Pose Estimation. (arXiv:2203.10339v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10339","description":"<p>6D object pose estimation is a fundamental yet challenging problem in\ncomputer vision. Convolutional Neural Networks (CNNs) have recently proven to\nbe capable of predicting reliable 6D pose estimates even under monocular\nsettings. Nonetheless, CNNs are identified as being extremely data-driven, and\nacquiring adequate annotations is oftentimes very time-consuming and labor\nintensive. To overcome this limitation, we propose a novel monocular 6D pose\nestimation approach by means of self-supervised learning, removing the need for\nreal annotations. After training our proposed network fully supervised with\nsynthetic RGB data, we leverage current trends in noisy student training and\ndifferentiable rendering to further self-supervise the model on these\nunsupervised real RGB(-D) samples, seeking for a visually and geometrically\noptimal alignment. Moreover, employing both visible and amodal mask\ninformation, our self-supervision becomes very robust towards challenging\nscenarios such as occlusion. Extensive evaluations demonstrate that our\nproposed self-supervision outperforms all other methods relying on synthetic\ndata or employing elaborate techniques from the domain adaptation realm.\nNoteworthy, our self-supervised approach consistently improves over its\nsynthetically trained baseline and often almost closes the gap towards its\nfully supervised counterpart. The code and models are publicly available at\nhttps://github.com/THU-DA-6D-Pose-Group/self6dpp.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1\">Fabian Manhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Shifted Augmentations (NSA): compact distributions for robust self-supervised Anomaly Detection. (arXiv:2203.10344v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10344","description":"<p>Unsupervised Anomaly detection (AD) requires building a notion of normalcy,\ndistinguishing in-distribution (ID) and out-of-distribution (OOD) data, using\nonly available ID samples. Recently, large gains were made on this task for the\ndomain of natural images using self-supervised contrastive feature learning as\na first step followed by kNN or traditional one-class classifiers for feature\nscoring. Learned representations that are non-uniformly distributed on the unit\nhypersphere have been shown to be beneficial for this task. We go a step\nfurther and investigate how the \\emph {geometrical compactness} of the ID\nfeature distribution makes isolating and detecting outliers easier, especially\nin the realistic situation when ID training data is polluted (i.e. ID data\ncontains some OOD data that is used for learning the feature extractor\nparameters). We propose novel architectural modifications to the\nself-supervised feature learning step, that enable such compact distributions\nfor ID data to be learned. We show that the proposed modifications can be\neffectively applied to most existing self-supervised objectives, with large\ngains in performance. Furthermore, this improved OOD performance is obtained\nwithout resorting to tricks such as using strongly augmented ID images (e.g. by\n90 degree rotations) as proxies for the unseen OOD data, as these impose overly\nprescriptive assumptions about ID data and its invariances. We perform\nextensive studies on benchmark datasets for one-class OOD detection and show\nstate-of-the-art performance in the presence of pollution in the ID data, and\ncomparable performance otherwise. We also propose and extensively evaluate a\nnovel feature scoring technique based on the angular Mahalanobis distance, and\npropose a simple and novel technique for feature ensembling during evaluation\nthat enables a big boost in performance at nearly zero run-time cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousef_M/0/1/0/all/0/1\">Mohamed Yousef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ackermann_M/0/1/0/all/0/1\">Marcel Ackermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1\">Unmesh Kurup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bishop_T/0/1/0/all/0/1\">Tom Bishop</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Font Generation with Missing Impression Labels. (arXiv:2203.10348v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10348","description":"<p>Our goal is to generate fonts with specific impressions, by training a\ngenerative adversarial network with a font dataset with impression labels. The\nmain difficulty is that font impression is ambiguous and the absence of an\nimpression label does not always mean that the font does not have the\nimpression. This paper proposes a font generation model that is robust against\nmissing impression labels. The key ideas of the proposed method are (1)a\nco-occurrence-based missing label estimator and (2)an impression label space\ncompressor. The first is to interpolate missing impression labels based on the\nco-occurrence of labels in the dataset and use them for training the model as\ncompleted label conditions. The second is an encoder-decoder module to compress\nthe high-dimensional impression space into low-dimensional. We proved that the\nproposed model generates high-quality font images using multi-label data with\nmissing labels through qualitative and quantitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_S/0/1/0/all/0/1\">Seiya Matsuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLRNet: Cross Layer Refinement Network for Lane Detection. (arXiv:2203.10350v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10350","description":"<p>Lane is critical in the vision navigation system of the intelligent vehicle.\nNaturally, lane is a traffic sign with high-level semantics, whereas it owns\nthe specific local pattern which needs detailed low-level features to localize\naccurately. Using different feature levels is of great importance for accurate\nlane detection, but it is still under-explored. In this work, we present Cross\nLayer Refinement Network (CLRNet) aiming at fully utilizing both high-level and\nlow-level features in lane detection. In particular, it first detects lanes\nwith high-level semantic features then performs refinement based on low-level\nfeatures. In this way, we can exploit more contextual information to detect\nlanes while leveraging local detailed lane features to improve localization\naccuracy. We present ROIGather to gather global context, which further enhances\nthe feature representation of lanes. In addition to our novel network design,\nwe introduce Line IoU loss which regresses the lane line as a whole unit to\nimprove the localization accuracy. Experiments demonstrate that the proposed\nmethod greatly outperforms the state-of-the-art lane detection approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wenjian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Domain Multi-Definition Landmark Localization for Small Datasets. (arXiv:2203.10358v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10358","description":"<p>We present a novel method for multi image domain and multi-landmark\ndefinition learning for small dataset facial localization. Training a small\ndataset alongside a large(r) dataset helps with robust learning for the former,\nand provides a universal mechanism for facial landmark localization for new\nand/or smaller standard datasets. To this end, we propose a Vision Transformer\nencoder with a novel decoder with a definition agnostic shared landmark\nsemantic group structured prior, that is learnt, as we train on more than one\ndataset concurrently. Due to our novel definition agnostic group prior the\ndatasets may vary in landmark definitions and domains. During the decoder stage\nwe use cross- and self-attention, whose output is later fed into\ndomain/definition specific heads that minimize a Laplacian-log-likelihood loss.\nWe achieve state-of-the-art performance on standard landmark localization\ndatasets such as COFW and WFLW, when trained with a bigger dataset. We also\nshow state-of-the-art performance on several varied image domain small datasets\nfor animals, caricatures, and facial portrait paintings. Further, we contribute\na small dataset (150 images) of pareidolias to show efficacy of our method.\nFinally, we provide several analysis and ablation studies to justify our\nclaims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferman_D/0/1/0/all/0/1\">David Ferman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharaj_G/0/1/0/all/0/1\">Gaurav Bharaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALAP-AE: As-Lite-as-Possible Auto-Encoder. (arXiv:2203.10363v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10363","description":"<p>We present a novel algorithm to reduce tensor compute required by a\nconditional image generation autoencoder and make it as-lite-as-possible,\nwithout sacrificing quality of photo-realistic image generation. Our method is\ndevice agnostic, and can optimize an autoencoder for a given CPU-only, GPU\ncompute device(s) in about normal time it takes to train an autoencoder on a\ngeneric workstation. We achieve this via a two-stage novel strategy where,\nfirst, we condense the channel weights, such that, as few as possible channels\nare used. Then, we prune the nearly zeroed out weight activations, and\nfine-tune this lite autoencoder. To maintain image quality, fine-tuning is done\nvia student-teacher training, where we reuse the condensed autoencoder as the\nteacher. We show performance gains for various conditional image generation\ntasks: segmentation mask to face images, face images to cartoonization, and\nfinally CycleGAN-based model on horse to zebra dataset over multiple compute\ndevices. We perform various ablation studies to justify the claims and design\nchoices, and achieve real-time versions of various autoencoders on CPU-only\ndevices while maintaining image quality, thus enabling at-scale deployment of\nsuch autoencoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg A. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharaj_G/0/1/0/all/0/1\">Gaurav Bharaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A naive method to discover directions in the StyleGAN2 latent space. (arXiv:2203.10373v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10373","description":"<p>Several research groups have shown that Generative Adversarial Networks\n(GANs) can generate photo-realistic images in recent years. Using the GANs, a\nmap is created between a latent code and a photo-realistic image. This process\ncan also be reversed: given a photo as input, it is possible to obtain the\ncorresponding latent code. In this paper, we will show how the inversion\nprocess can be easily exploited to interpret the latent space and control the\noutput of StyleGAN2, a GAN architecture capable of generating photo-realistic\nfaces. From a biological perspective, facial features such as nose size depend\non important genetic factors, and we explore the latent spaces that correspond\nto such biological features, including masculinity and eye colour. We show the\nresults obtained by applying the proposed method to a set of photos extracted\nfrom the CelebA-HQ database. We quantify some of these measures by utilizing\ntwo landmarking protocols, and evaluate their robustness through statistical\nanalysis. Finally we correlate these measures with the input parameters used to\nperturb the latent spaces along those interpretable directions. Our results\ncontribute towards building the groundwork of using such GAN architecture in\nforensics to generate photo-realistic faces that satisfy certain biological\nattributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giardina_A/0/1/0/all/0/1\">Andrea Giardina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paria_S/0/1/0/all/0/1\">Soumya Subhra Paria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaustubh_A/0/1/0/all/0/1\">Adhikari Kaustubh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PressureVision: Estimating Hand Pressure from a Single RGB Image. (arXiv:2203.10385v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10385","description":"<p>People often interact with their surroundings by applying pressure with their\nhands. Machine perception of hand pressure has been limited by the challenges\nof placing sensors between the hand and the contact surface. We explore the\npossibility of using a conventional RGB camera to infer hand pressure. The\ncentral insight is that the application of pressure by a hand results in\ninformative appearance changes. Hands share biomechanical properties that\nresult in similar observable phenomena, such as soft-tissue deformation, blood\ndistribution, hand pose, and cast shadows. We collected videos of 36\nparticipants with diverse skin tone applying pressure to an instrumented planar\nsurface. We then trained a deep model (PressureVisionNet) to infer a pressure\nimage from a single RGB image. Our model infers pressure for participants\noutside of the training data and outperforms baselines. We also show that the\noutput of our model depends on the appearance of the hand and cast shadows near\ncontact regions. Overall, our results suggest the appearance of a previously\nunobserved human hand can be used to accurately infer applied pressure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grady_P/0/1/0/all/0/1\">Patrick Grady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengcheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahmbhatt_S/0/1/0/all/0/1\">Samarth Brahmbhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twigg_C/0/1/0/all/0/1\">Christopher D. Twigg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Chengde Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1\">James Hays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1\">Charles C. Kemp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Semantic Segmentation of Accident Scenes via Multi-Source Mixed Sampling and Meta-Learning. (arXiv:2203.10395v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10395","description":"<p>Autonomous vehicles utilize urban scene segmentation to understand the real\nworld like a human and react accordingly. Semantic segmentation of normal\nscenes has experienced a remarkable rise in accuracy on conventional\nbenchmarks. However, a significant portion of real-life accidents features\nabnormal scenes, such as those with object deformations, overturns, and\nunexpected traffic behaviors. Since even small mis-segmentation of driving\nscenes can lead to serious threats to human lives, the robustness of such\nmodels in accident scenarios is an extremely important factor in ensuring\nsafety of intelligent transportation systems.\n</p>\n<p>In this paper, we propose a Multi-source Meta-learning Unsupervised Domain\nAdaptation (MMUDA) framework, to improve the generalization of segmentation\ntransformers to extreme accident scenes. In MMUDA, we make use of Multi-Domain\nMixed Sampling to augment the images of multiple-source domains (normal scenes)\nwith the target data appearances (abnormal scenes). To train our model, we\nintertwine and study a meta-learning strategy in the multi-source setting for\nrobustifying the segmentation results. We further enhance the segmentation\nbackbone (SegFormer) with a HybridASPP decoder design, featuring large window\nattention spatial pyramid pooling and strip pooling, to efficiently aggregate\nlong-range contextual dependencies. Our approach achieves a mIoU score of\n46.97% on the DADA-seg benchmark, surpassing the previous state-of-the-art\nmodel by more than 7.50%. Code will be made publicly available at\nhttps://github.com/xinyu-laura/MMUDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xinyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attri-VAE: attribute-based, disentangled and interpretable representations of medical images with variational autoencoders. (arXiv:2203.10417v1 [eess.IV])","link":"http://arxiv.org/abs/2203.10417","description":"<p>Deep learning (DL) methods where interpretability is intrinsically considered\nas part of the model are required to better understand the relationship of\nclinical and imaging-based attributes with DL outcomes, thus facilitating their\nuse in reasoning medical decisions. Latent space representations built with\nvariational autoencoders (VAE) do not ensure individual control of data\nattributes. Attribute-based methods enforcing attribute disentanglement have\nbeen proposed in the literature for classical computer vision tasks in\nbenchmark data. In this paper, we propose a VAE approach, the Attri-VAE, that\nincludes an attribute regularization term to associate clinical and medical\nimaging attributes with different regularized dimensions in the generated\nlatent space, enabling a better disentangled interpretation of the attributes.\nFurthermore, the generated attention maps explained the attribute encoding in\nthe regularized latent space dimensions. The Attri-VAE approach analyzed\nhealthy and myocardial infarction patients with clinical, cardiac morphology,\nand radiomics attributes. The proposed model provided an excellent trade-off\nbetween reconstruction fidelity, disentanglement, and interpretability,\noutperforming state-of-the-art VAE approaches according to several quantitative\nmetrics. The resulting latent space allowed the generation of realistic\nsynthetic data in the trajectory between two distinct input samples or along a\nspecific attribute dimension to better interpret changes between different\ncardiac conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cetin_I/0/1/0/all/0/1\">Irem Cetin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Camara_O/0/1/0/all/0/1\">Oscar Camara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballester_M/0/1/0/all/0/1\">Miguel Angel Gonzalez Ballester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration. (arXiv:2203.10421v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10421","description":"<p>Households across the world contain arbitrary objects: from mate gourds and\ncoffee mugs to sitars and guitars. Considering this diversity, robot perception\nmust handle a large variety of semantic objects without additional fine-tuning\nto be broadly applicable in homes. Recently, zero-shot models have demonstrated\nimpressive performance in image classification of arbitrary objects (i.e.,\nclassifying images at inference with categories not explicitly seen during\ntraining). In this paper, we translate the success of zero-shot vision models\n(e.g., CLIP) to the popular embodied AI task of object navigation. In our\nsetting, an agent must find an arbitrary goal object, specified via text, in\nunseen environments coming from different datasets. Our key insight is to\nmodularize the task into zero-shot object localization and exploration.\nEmploying this philosophy, we design CLIP on Wheels (CoW) baselines for the\ntask and evaluate each zero-shot model in both Habitat and RoboTHOR simulators.\nWe find that a straightforward CoW, with CLIP-based object localization plus\nclassical exploration, and no additional training, often outperforms learnable\napproaches in terms of success, efficiency, and robustness to dataset\ndistribution shift. This CoW achieves 6.3% SPL in Habitat and 10.0% SPL in\nRoboTHOR, when tested zero-shot on all categories. On a subset of four RoboTHOR\ncategories considered in prior work, the same CoW shows a 16.1 percentage point\nimprovement in Success over the learnable state-of-the-art baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Human-Gaze-Target Detection with Transformers. (arXiv:2203.10433v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10433","description":"<p>In this paper, we propose an effective and efficient method for\nHuman-Gaze-Target (HGT) detection, i.e., gaze following. Current approaches\ndecouple the HGT detection task into separate branches of salient object\ndetection and human gaze prediction, employing a two-stage framework where\nhuman head locations must first be detected and then be fed into the next gaze\ntarget prediction sub-network. In contrast, we redefine the HGT detection task\nas detecting human head locations and their gaze targets, simultaneously. By\nthis way, our method, named Human-Gaze-Target detection TRansformer or HGTTR,\nstreamlines the HGT detection pipeline by eliminating all other additional\ncomponents. HGTTR reasons about the relations of salient objects and human gaze\nfrom the global image context. Moreover, unlike existing two-stage methods that\nrequire human head locations as input and can predict only one human's gaze\ntarget at a time, HGTTR can directly predict the locations of all people and\ntheir gaze targets at one time in an end-to-end manner. The effectiveness and\nrobustness of our proposed method are verified with extensive experiments on\nthe two standard benchmark datasets, GazeFollowing and VideoAttentionTarget.\nWithout bells and whistles, HGTTR outperforms existing state-of-the-art methods\nby large margins (6.4 mAP gain on GazeFollowing and 10.3 mAP gain on\nVideoAttentionTarget) with a much simpler architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Danyang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Huiyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer with Convolutions Architecture Search. (arXiv:2203.10435v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10435","description":"<p>Transformers exhibit great advantages in handling computer vision tasks. They\nmodel image classification tasks by utilizing a multi-head attention mechanism\nto process a series of patches consisting of split images. However, for complex\ntasks, Transformer in computer vision not only requires inheriting a bit of\ndynamic attention and global context, but also needs to introduce features\nconcerning noise reduction, shifting, and scaling invariance of objects.\nTherefore, here we take a step forward to study the structural characteristics\nof Transformer and convolution and propose an architecture search method-Vision\nTransformer with Convolutions Architecture Search (VTCAS). The high-performance\nbackbone network searched by VTCAS introduces the desirable features of\nconvolutional neural networks into the Transformer architecture while\nmaintaining the benefits of the multi-head attention mechanism. The searched\nblock-based backbone network can extract feature maps at different scales.\nThese features are compatible with a wider range of visual tasks, such as image\nclassification (32 M parameters, 82.0% Top-1 accuracy on ImageNet-1K) and\nobject detection (50.4% mAP on COCO2017). The proposed topology based on the\nmulti-head attention mechanism and CNN adaptively associates relational\nfeatures of pixels with multi-scale features of objects. It enhances the\nrobustness of the neural network for object recognition, especially in the low\nillumination indoor scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_K/0/1/0/all/0/1\">Kuangrong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1\">Witold Pedrycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuesong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Bing Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning. (arXiv:2203.10444v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10444","description":"<p>Human-annotated attributes serve as powerful semantic embeddings in zero-shot\nlearning. However, their annotation process is labor-intensive and needs expert\nsupervision. Current unsupervised semantic embeddings, i.e., word embeddings,\nenable knowledge transfer between classes. However, word embeddings do not\nalways reflect visual similarities and result in inferior zero-shot\nperformance. We propose to discover semantic embeddings containing\ndiscriminative visual properties for zero-shot learning, without requiring any\nhuman annotation. Our model visually divides a set of images from seen classes\ninto clusters of local image regions according to their visual similarity, and\nfurther imposes their class discrimination and semantic relatedness. To\nassociate these clusters with previously unseen classes, we use external\nknowledge, e.g., word embeddings and propose a novel class relation discovery\nmodule. Through quantitative and qualitative evaluation, we demonstrate that\nour model discovers semantic embeddings that model the visual properties of\nboth seen and unseen classes. Furthermore, we demonstrate on three benchmarks\nthat our visually-grounded semantic embeddings further improve performance over\nword embeddings across various ZSL models by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yongqin Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partitioning Image Representation in Contrastive Learning. (arXiv:2203.10454v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10454","description":"<p>In contrastive learning in the image domain, the anchor and positive samples\nare forced to have as close representations as possible. However, forcing the\ntwo samples to have the same representation could be misleading because the\ndata augmentation techniques make the two samples different. In this paper, we\nintroduce a new representation, partitioned representation, which can learn\nboth common and unique features of the anchor and positive samples in\ncontrastive learning. The partitioned representation consists of two parts: the\ncontent part and the style part. The content part represents common features of\nthe class, and the style part represents the own features of each sample, which\ncan lead to the representation of the data augmentation method. We can achieve\nthe partitioned representation simply by decomposing a loss function of\ncontrastive learning into two terms on the two separate representations,\nrespectively. To evaluate our representation with two parts, we take two\nframework models: Variational AutoEncoder (VAE) and BootstrapYour Own\nLatent(BYOL) to show the separability of content and style, and to confirm the\ngeneralization ability in classification, respectively. Based on the\nexperiments, we show that our approach can separate two types of information in\nthe VAE framework and outperforms the conventional BYOL in linear separability\nand a few-shot learning task as downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunsub Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Mutual Leakage Network for Cell Image Segmentation. (arXiv:2203.10455v1 [eess.IV])","link":"http://arxiv.org/abs/2203.10455","description":"<p>We propose three segmentation methods using GAN and information leakage\nbetween generator and discriminator. First, we propose an Adversarial Training\nAttention Module (ATA-Module) that uses an attention mechanism from the\ndiscriminator to the generator to enhance and leak important information in the\ndiscriminator. ATA-Module transmits important information to the generator from\nthe discriminator. Second, we propose a Top-Down Pixel-wise Difficulty\nAttention Module (Top-Down PDA-Module) that leaks an attention map based on\npixel-wise difficulty in the generator to the discriminator. The generator\ntrains to focus on pixel-wise difficulty, and the discriminator uses the\ndifficulty information leaked from the generator for classification. Finally,\nwe propose an Adversarial Mutual Leakage Network (AML-Net) that mutually leaks\nthe information each other between the generator and the discriminator. By\nusing the information of the other network, it is able to train more\nefficiently than ordinary segmentation models. Three proposed methods have been\nevaluated on two datasets for cell image segmentation. The experimental results\nshow that the segmentation accuracy of AML-Net was much improved in comparison\nwith conventional methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tsuda_H/0/1/0/all/0/1\">Hiroki Tsuda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hotta_K/0/1/0/all/0/1\">Kazuhiro Hotta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"simCrossTrans: A Simple Cross-Modality Transfer Learning for Object Detection with ConvNets or Vision Transformers. (arXiv:2203.10456v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10456","description":"<p>Transfer learning is widely used in computer vision (CV), natural language\nprocessing (NLP) and achieves great success. Most transfer learning systems are\nbased on the same modality (e.g. RGB image in CV and text in NLP). However, the\ncross-modality transfer learning (CMTL) systems are scarce. In this work, we\nstudy CMTL from 2D to 3D sensor to explore the upper bound performance of 3D\nsensor only systems, which play critical roles in robotic navigation and\nperform well in low light scenarios. While most CMTL pipelines from 2D to 3D\nvision are complicated and based on Convolutional Neural Networks (ConvNets),\nours is easy to implement, expand and based on both ConvNets and Vision\ntransformers(ViTs): 1) By converting point clouds to pseudo-images, we can use\nan almost identical network from pre-trained models based on 2D images. This\nmakes our system easy to implement and expand. 2) Recently ViTs have been\nshowing good performance and robustness to occlusions, one of the key reasons\nfor poor performance of 3D vision systems. We explored both ViT and ConvNet\nwith similar model sizes to investigate the performance difference. We name our\napproach simCrossTrans: simple cross-modality transfer learning with ConvNets\nor ViTs. Experiments on SUN RGB-D dataset show: with simCrossTrans we achieve\n$13.2\\%$ and $16.1\\%$ absolute performance gain based on ConvNets and ViTs\nseparately. We also observed the ViTs based performs $9.7\\%$ better than the\nConvNets one, showing the power of simCrossTrans with ViT. simCrossTrans with\nViTs surpasses the previous state-of-the-art (SOTA) by a large margin of\n$+15.4\\%$ mAP50. Compared with the previous 2D detection SOTA based RGB images,\nour depth image only system only has a $1\\%$ gap. The code, training/inference\nlogs and models are publicly available at\nhttps://github.com/liketheflower/simCrossTrans\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoke Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamos_I/0/1/0/all/0/1\">Ioannis Stamos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Flow for Video Super-Resolution: A Survey. (arXiv:2203.10462v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10462","description":"<p>Video super-resolution is currently one of the most active research topics in\ncomputer vision as it plays an important role in many visual applications.\nGenerally, video super-resolution contains a significant component, i.e.,\nmotion compensation, which is used to estimate the displacement between\nsuccessive video frames for temporal alignment. Optical flow, which can supply\ndense and sub-pixel motion between consecutive frames, is among the most common\nways for this task. To obtain a good understanding of the effect that optical\nflow acts in video super-resolution, in this work, we conduct a comprehensive\nreview on this subject for the first time. This investigation covers the\nfollowing major topics: the function of super-resolution (i.e., why we require\nsuper-resolution); the concept of video super-resolution (i.e., what is video\nsuper-resolution); the description of evaluation metrics (i.e., how (video)\nsuperresolution performs); the introduction of optical flow based video\nsuper-resolution; the investigation of using optical flow to capture temporal\ndependency for video super-resolution. Prominently, we give an in-depth study\nof the deep learning based video super-resolution method, where some\nrepresentative algorithms are analyzed and compared. Additionally, we highlight\nsome promising research directions and open issues that should be further\naddressed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"{Unidirectional Thin Adapter for Efficient Adaptation of Deep Neural Networks. (arXiv:2203.10463v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10463","description":"<p>In this paper, we propose a new adapter network for adapting a pre-trained\ndeep neural network to a target domain with minimal computation. The proposed\nmodel, unidirectional thin adapter (UDTA), helps the classifier adapt to new\ndata by providing auxiliary features that complement the backbone network. UDTA\ntakes outputs from multiple layers of the backbone as input features but does\nnot transmit any feature to the backbone. As a result, UDTA can learn without\ncomputing the gradient of the backbone, which saves computation for training\nsignificantly. In addition, since UDTA learns the target task without modifying\nthe backbone, a single backbone can adapt to multiple tasks by learning only\nUDTAs separately. In experiments on five fine-grained classification datasets\nconsisting of a small number of samples, UDTA significantly reduced computation\nand training time required for backpropagation while showing comparable or even\nimproved accuracy compared with conventional adapter models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Han Gyel Sun</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1\">Hyunjae Ahn</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">HyunGyu Lee</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Injung Kim</a> (1) ((1) Handong Global University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Portrait Eyeglasses and Shadow Removal by Leveraging 3D Synthetic Data. (arXiv:2203.10474v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10474","description":"<p>In portraits, eyeglasses may occlude facial regions and generate cast shadows\non faces, which degrades the performance of many techniques like face\nverification and expression recognition. Portrait eyeglasses removal is\ncritical in handling these problems. However, completely removing the\neyeglasses is challenging because the lighting effects (e.g., cast shadows)\ncaused by them are often complex. In this paper, we propose a novel framework\nto remove eyeglasses as well as their cast shadows from face images. The method\nworks in a detect-then-remove manner, in which eyeglasses and cast shadows are\nboth detected and then removed from images. Due to the lack of paired data for\nsupervised training, we present a new synthetic portrait dataset with both\nintermediate and final supervisions for both the detection and removal tasks.\nFurthermore, we apply a cross-domain technique to fill the gap between the\nsynthetic and real data. To the best of our knowledge, the proposed technique\nis the first to remove eyeglasses and their cast shadows simultaneously. The\ncode and synthetic dataset are available at\nhttps://github.com/StoryMY/take-off-eyeglasses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Junfeng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Camera Placements for Overlapped Coverage with 3D Camera Projections. (arXiv:2203.10479v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10479","description":"<p>This paper proposes a method to compute camera 6Dof poses to achieve a user\ndefined coverage. The camera placement problem is modeled as a combinatorial\noptimization where given the maximum number of cameras, a camera set is\nselected from a larger pool of possible camera poses. We propose to minimize\nthe squared error between the desired and the achieved coverage, and formulate\nthe non-linear cost function as a mixed integer linear programming problem. A\ncamera lens model is utilized to project the cameras view on a 3D voxel map to\ncompute a coverage score which makes the optimization problem in real\nenvironments tractable. Experimental results in two real retail store\nenvironments demonstrate the better performance of the proposed formulation in\nterms of coverage and overlap for triangulation compared to existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malhotra_A/0/1/0/all/0/1\">Akshay Malhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Dhananjay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadlani_T/0/1/0/all/0/1\">Tushar Dadlani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_L/0/1/0/all/0/1\">Luis Yoichi Morales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring Articulated Rigid Body Dynamics from RGBD Video. (arXiv:2203.10488v1 [cs.RO])","link":"http://arxiv.org/abs/2203.10488","description":"<p>Being able to reproduce physical phenomena ranging from light interaction to\ncontact mechanics, simulators are becoming increasingly useful in more and more\napplication domains where real-world interaction or labeled data are difficult\nto obtain. Despite recent progress, significant human effort is needed to\nconfigure simulators to accurately reproduce real-world behavior. We introduce\na pipeline that combines inverse rendering with differentiable simulation to\ncreate digital twins of real-world articulated mechanisms from depth or RGB\nvideos. Our approach automatically discovers joint types and estimates their\nkinematic parameters, while the dynamic properties of the overall mechanism are\ntuned to attain physically accurate simulations. Control policies optimized in\nour derived simulation transfer successfully back to the original system, as we\ndemonstrate on a simulated system. Further, our approach accurately\nreconstructs the kinematic tree of an articulated mechanism being manipulated\nby a robot, and highly nonlinear dynamics of a real-world coupled pendulum\nmechanism.\n</p>\n<p>Website: https://eric-heiden.github.io/video2sim\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heiden_E/0/1/0/all/0/1\">Eric Heiden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coumans_E/0/1/0/all/0/1\">Erwin Coumans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1\">Gaurav S. Sukhatme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TVConv: Efficient Translation Variant Convolution for Layout-aware Visual Processing. (arXiv:2203.10489v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10489","description":"<p>As convolution has empowered many smart applications, dynamic convolution\nfurther equips it with the ability to adapt to diverse inputs. However, the\nstatic and dynamic convolutions are either layout-agnostic or\ncomputation-heavy, making it inappropriate for layout-specific applications,\ne.g., face recognition and medical image segmentation. We observe that these\napplications naturally exhibit the characteristics of large intra-image\n(spatial) variance and small cross-image variance. This observation motivates\nour efficient translation variant convolution (TVConv) for layout-aware visual\nprocessing. Technically, TVConv is composed of affinity maps and a\nweight-generating block. While affinity maps depict pixel-paired relationships\ngracefully, the weight-generating block can be explicitly overparameterized for\nbetter training while maintaining efficient inference. Although conceptually\nsimple, TVConv significantly improves the efficiency of the convolution and can\nbe readily plugged into various network architectures. Extensive experiments on\nface recognition show that TVConv reduces the computational cost by up to 3.1x\nand improves the corresponding throughput by 2.3x while maintaining a high\naccuracy compared to the depthwise convolution. Moreover, for the same\ncomputation cost, we boost the mean accuracy by up to 4.21%. We also conduct\nexperiments on the optic disc/cup segmentation task and obtain better\ngeneralization performance, which helps mitigate the critical data scarcity\nissue. Code is available at https://github.com/JierunChen/TVConv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jierun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianlang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Weipeng Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Li Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Sangtae Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">S.-H. Gary Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization. (arXiv:2203.10492v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10492","description":"<p>Recently self-supervised representation learning has drawn considerable\nattention from the scene text recognition community. Different from previous\nstudies using contrastive learning, we tackle the issue from an alternative\nperspective, i.e., by formulating the representation learning scheme in a\ngenerative manner. Typically, the neighboring image patches among one text line\ntend to have similar styles, including the strokes, textures, colors, etc.\nMotivated by this common sense, we augment one image patch and use its\nneighboring patch as guidance to recover itself. Specifically, we propose a\nSimilarity-Aware Normalization (SimAN) module to identify the different\npatterns and align the corresponding styles from the guiding patch. In this\nway, the network gains representation capability for distinguishing complex\npatterns such as messy strokes and cluttered backgrounds. Experiments show that\nthe proposed SimAN significantly improves the representation quality and\nachieves promising performance. Moreover, we surprisingly find that our\nself-supervised generative network has impressive potential for data synthesis,\ntext image editing, and font interpolation, which suggests that the proposed\nSimAN has a wide range of practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Estimation by Combining Binocular Stereo and Monocular Structured-Light. (arXiv:2203.10493v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10493","description":"<p>It is well known that the passive stereo system cannot adapt well to weak\ntexture objects, e.g., white walls. However, these weak texture targets are\nvery common in indoor environments. In this paper, we present a novel stereo\nsystem, which consists of two cameras (an RGB camera and an IR camera) and an\nIR speckle projector. The RGB camera is used both for depth estimation and\ntexture acquisition. The IR camera and the speckle projector can form a\nmonocular structured-light (MSL) subsystem, while the two cameras can form a\nbinocular stereo subsystem. The depth map generated by the MSL subsystem can\nprovide external guidance for the stereo matching networks, which can improve\nthe matching accuracy significantly. In order to verify the effectiveness of\nthe proposed system, we build a prototype and collect a test dataset in indoor\nscenes. The evaluation results show that the Bad 2.0 error of the proposed\nsystem is 28.2% of the passive stereo system when the network RAFT is used. The\ndataset and trained models are available at\nhttps://github.com/YuhuaXu/MonoStereoFusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoli Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yushan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zhaobi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-image Human-body Reshaping with Deep Neural Networks. (arXiv:2203.10496v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10496","description":"<p>In this paper, we present NeuralReshaper, a novel method for semantic\nreshaping of human bodies in single images using deep generative networks. To\nachieve globally coherent reshaping effects, our approach follows a\nfit-then-reshape pipeline, which first fits a parametric 3D human model to a\nsource human image and then reshapes the fitted 3D model with respect to\nuser-specified semantic attributes. Previous methods rely on image warping to\ntransfer 3D reshaping effects to the entire image domain and thus often cause\ndistortions in both foreground and background. Instead, to achieve more\nrealistic reshaping results, we resort to generative adversarial nets\nconditioned on the source image and a 2D warping field induced by the reshaped\n3D model. Specifically, we separately encode the foreground and background\ninformation in the source image using a two-headed U-net-like generator and\nguide the information flow from the foreground branch to the background branch\nvia feature space warping. Furthermore, to deal with the lack-of-data problem\nthat no paired data exist (i.e., the same human bodies in varying shapes), we\nintroduce a novel weakly-supervised strategy to train our network. Besides,\nunlike previous methods that often require manual efforts to correct\nundesirable artifacts caused by incorrect body-to-image fitting, our method is\nfully automatic. Extensive experiments on both indoor and outdoor datasets\ndemonstrate the superiority of our method over previous approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beijia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft-CP: A Credible and Effective Data Augmentation for Semantic Segmentation of Medical Lesions. (arXiv:2203.10507v1 [eess.IV])","link":"http://arxiv.org/abs/2203.10507","description":"<p>The medical datasets are usually faced with the problem of scarcity and data\nimbalance. Moreover, annotating large datasets for semantic segmentation of\nmedical lesions is domain-knowledge and time-consuming. In this paper, we\npropose a new object-blend method(short in soft-CP) that combines the\nCopy-Paste augmentation method for semantic segmentation of medical lesions\noffline, ensuring the correct edge information around the lession to solve the\nissue above-mentioned. We proved the method's validity with several datasets in\ndifferent imaging modalities. In our experiments on the KiTS19[2] dataset,\nSoft-CP outperforms existing medical lesions synthesis approaches. The Soft-CP\naugementation provides gains of +26.5% DSC in the low data regime(10% of data)\nand +10.2% DSC in the high data regime(all of data), In offline training data,\nthe ratio of real images to synthetic images is 3:1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_P/0/1/0/all/0/1\">Pingping Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_L/0/1/0/all/0/1\">Licong Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Ruihan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Haiming Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Whole Heart Mesh Generation From Patient Images For Computational Simulations. (arXiv:2203.10517v1 [eess.IV])","link":"http://arxiv.org/abs/2203.10517","description":"<p>Patient-specific cardiac modeling combines geometries of the heart derived\nfrom medical images and biophysical simulations to predict various aspects of\ncardiac function. However, generating simulation-suitable models of the heart\nfrom patient image data often requires complicated procedures and significant\nhuman effort. We present a fast and automated deep-learning method to construct\nsimulation-suitable models of the heart from medical images. The approach\nconstructs meshes from 3D patient images by learning to deform a small set of\ndeformation handles on a whole heart template. For both 3D CT and MR data, this\nmethod achieves promising accuracy for whole heart reconstruction, consistently\noutperforming prior methods in constructing simulation-suitable meshes of the\nheart. When evaluated on time-series CT data, this method produced more\nanatomically and temporally consistent geometries than prior methods, and was\nable to produce geometries that better satisfy modeling requirements for\ncardiac flow simulations. Our source code will be available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kong_F/0/1/0/all/0/1\">Fanwei Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shadden_S/0/1/0/all/0/1\">Shawn Shadden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Video Prediction with Structure and Motion. (arXiv:2203.10528v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10528","description":"<p>While stochastic video prediction models enable future prediction under\nuncertainty, they mostly fail to model the complex dynamics of real-world\nscenes. For example, they cannot provide reliable predictions for scenes with a\nmoving camera and independently moving foreground objects in driving scenarios.\nThe existing methods fail to fully capture the dynamics of the structured world\nby only focusing on changes in pixels. In this paper, we assume that there is\nan underlying process creating observations in a video and propose to factorize\nit into static and dynamic components. We model the static part based on the\nscene structure and the ego-motion of the vehicle, and the dynamic part based\non the remaining motion of the dynamic objects. By learning separate\ndistributions of changes in foreground and background, we can decompose the\nscene into static and dynamic parts and separately model the change in each.\nOur experiments demonstrate that disentangling structure and motion helps\nstochastic video prediction, leading to better future predictions in complex\ndriving scenarios on two real-world driving datasets, KITTI and Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1\">Adil Kaan Akan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safadoust_S/0/1/0/all/0/1\">Sadra Safadoust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1\">Fatma G&#xfc;ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iwin: Human-Object Interaction Detection via Transformer with Irregular Windows. (arXiv:2203.10537v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10537","description":"<p>This paper presents a new vision Transformer, named Iwin Transformer, which\nis specifically designed for human-object interaction (HOI) detection, a\ndetailed scene understanding task involving a sequential process of\nhuman/object detection and interaction recognition. Iwin Transformer is a\nhierarchical Transformer which progressively performs token representation\nlearning and token agglomeration within irregular windows. The irregular\nwindows, achieved by augmenting regular grid locations with learned offsets, 1)\neliminate redundancy in token representation learning, which leads to efficient\nhuman/object detection, and 2) enable the agglomerated tokens to align with\nhumans/objects with different shapes, which facilitates the acquisition of\nhighly-abstracted visual semantics for interaction recognition. The\neffectiveness and efficiency of Iwin Transformer are verified on the two\nstandard HOI detection benchmark datasets, HICO-DET and V-COCO. Results show\nour method outperforms existing Transformers-based methods by large margins\n(3.7 mAP gain on HICO-DET and 2.0 mAP gain on V-COCO) with fewer training\nepochs ($0.5 \\times$).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Danyang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Huiyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Video Text Spotting with Transformer. (arXiv:2203.10539v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10539","description":"<p>Recent video text spotting methods usually require the three-staged pipeline,\ni.e., detecting text in individual images, recognizing localized text, tracking\ntext streams with post-processing to generate final results. These methods\ntypically follow the tracking-by-match paradigm and develop sophisticated\npipelines. In this paper, rooted in Transformer sequence modeling, we propose a\nsimple, but effective end-to-end video text DEtection, Tracking, and\nRecognition framework (TransDETR). TransDETR mainly includes two advantages: 1)\nDifferent from the explicit match paradigm in the adjacent frame, TransDETR\ntracks and recognizes each text implicitly by the different query termed text\nquery over long-range temporal sequence (more than 7 frames). 2) TransDETR is\nthe first end-to-end trainable video text spotting framework, which\nsimultaneously addresses the three sub-tasks (e.g., text detection, tracking,\nrecognition). Extensive experiments in four video text datasets (i.e.,ICDAR2013\nVideo, ICDAR2015 Video, Minetto, and YouTube Video Text) are conducted to\ndemonstrate that TransDETR achieves state-of-the-art performance with up to\naround 8.0% improvements on video text spotting tasks. The code of TransDETR\ncan be found at https://github.com/weijiawu/TransDETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanqiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Nighttime Aerial Tracking. (arXiv:2203.10541v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10541","description":"<p>Previous advances in object tracking mostly reported on favorable\nillumination circumstances while neglecting performance at nighttime, which\nsignificantly impeded the development of related aerial robot applications.\nThis work instead develops a novel unsupervised domain adaptation framework for\nnighttime aerial tracking (named UDAT). Specifically, a unique object discovery\napproach is provided to generate training patches from raw nighttime tracking\nvideos. To tackle the domain discrepancy, we employ a Transformer-based\nbridging layer post to the feature extractor to align image features from both\ndomains. With a Transformer day/night feature discriminator, the daytime\ntracking model is adversarially trained to track at night. Moreover, we\nconstruct a pioneering benchmark namely NAT2021 for unsupervised domain\nadaptive nighttime tracking, which comprises a test set of 180 manually\nannotated tracking sequences and a train set of over 276k unlabelled nighttime\ntracking frames. Exhaustive experiments demonstrate the robustness and domain\nadaptability of the proposed framework in nighttime aerial tracking. The code\nand benchmark are available at https://github.com/vision4robotics/UDAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guangze Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Dewarping with Control Points. (arXiv:2203.10543v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10543","description":"<p>Document images are now widely captured by handheld devices such as mobile\nphones. The OCR performance on these images are largely affected due to\ngeometric distortion of the document paper, diverse camera positions and\ncomplex backgrounds. In this paper, we propose a simple yet effective approach\nto rectify distorted document image by estimating control points and reference\npoints. After that, we use interpolation method between control points and\nreference points to convert sparse mappings to backward mapping, and remap the\noriginal distorted document image to the rectified image. Furthermore, control\npoints are controllable to facilitate interaction or subsequent adjustment. We\ncan flexibly select post-processing methods and the number of vertices\naccording to different application scenarios. Experiments show that our\napproach can rectify document images with various distortion types, and yield\nstate-of-the-art performance on real-world dataset. This paper also provides a\ntraining dataset based on control points for document dewarping. Both the code\nand the dataset are released at\nhttps://github.com/gwxie/Document-Dewarping-with-Control-Points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guo-Wang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu-Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng-Lin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards 3D Scene Understanding by Referring Synthetic Models. (arXiv:2203.10546v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10546","description":"<p>Promising performance has been achieved for visual perception on the point\ncloud. However, the current methods typically rely on labour-extensive\nannotations on the scene scans. In this paper, we explore how synthetic models\nalleviate the real scene annotation burden, i.e., taking the labelled 3D\nsynthetic models as reference for supervision, the neural network aims to\nrecognize specific categories of objects on a real scene scan (without scene\nannotation for supervision). The problem studies how to transfer knowledge from\nsynthetic 3D models to real 3D scenes and is named Referring Transfer Learning\n(RTL). The main challenge is solving the model-to-scene (from a single model to\nthe scene) and synthetic-to-real (from synthetic model to real scene's object)\ngap between the synthetic model and the real scene. To this end, we propose a\nsimple yet effective framework to perform two alignment operations. First,\nphysical data alignment aims to make the synthetic models cover the diversity\nof the scene's objects with data processing techniques. Then a novel\n\\textbf{convex-hull regularized feature alignment} introduces learnable\nprototypes to project the point features of both synthetic models and real\nscenes to a unified feature space, which alleviates the domain gap. These\noperations ease the model-to-scene and synthetic-to-real difficulty for a\nnetwork to recognize the target objects on a real unseen scene. Experiments\nshow that our method achieves the average mAP of 46.08\\% and 55.49\\% on the\nScanNet and S3DIS datasets by learning the synthetic models from the ModelNet\ndataset. Code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nenglun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dawei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Human Pose Estimation Using M\\\"obius Graph Convolutional Networks. (arXiv:2203.10554v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10554","description":"<p>3D human pose estimation is fundamental to understanding human behavior.\nRecently, promising results have been achieved by graph convolutional networks\n(GCNs), which achieve state-of-the-art performance and provide rather\nlight-weight architectures. However, a major limitation of GCNs is their\ninability to encode all the transformations between joints explicitly. To\naddress this issue, we propose a novel spectral GCN using the M\\\"obius\ntransformation (M\\\"obiusGCN). In particular, this allows us to directly and\nexplicitly encode the transformation between joints, resulting in a\nsignificantly more compact representation. Compared to even the lightest\narchitectures so far, our novel approach requires 90-98% fewer parameters, i.e.\nour lightest M\\\"obiusGCN uses only 0.042M trainable parameters. Besides the\ndrastic parameter reduction, explicitly encoding the transformation of joints\nalso enables us to achieve state-of-the-art results. We evaluate our approach\non the two challenging pose estimation benchmarks, Human3.6M and MPI-INF-3DHP,\ndemonstrating both state-of-the-art results and the generalization capabilities\nof M\\\"obiusGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_N/0/1/0/all/0/1\">Niloofar Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Possegger_H/0/1/0/all/0/1\">Horst Possegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10562","description":"<p>Image signal processors (ISPs) are historically grown legacy software systems\nfor reconstructing color images from noisy raw sensor measurements. They are\nusually composited of many heuristic blocks for denoising, demosaicking, and\ncolor restoration. Color reproduction in this context is of particular\nimportance, since the raw colors are often severely distorted, and each smart\nphone manufacturer has developed their own characteristic heuristics for\nimproving the color rendition, for example of skin tones and other visually\nimportant colors.\n</p>\n<p>In recent years there has been strong interest in replacing the historically\ngrown ISP systems with deep learned pipelines. Much progress has been made in\napproximating legacy ISPs with such learned models. However, so far the focus\nof these efforts has been on reproducing the structural features of the images,\nwith less attention paid to color rendition.\n</p>\n<p>Here we present CRISPnet, the first learned ISP model to specifically target\ncolor rendition accuracy relative to a complex, legacy smart phone ISP. We\nachieve this by utilizing both image metadata (like a legacy ISP would), as\nwell as by learning simple global semantics based on image classification --\nsimilar to what a legacy ISP does to determine the scene type. We also\ncontribute a new ISP image dataset consisting of both high dynamic range\nmonitor data, as well as real-world data, both captured with an actual cell\nphone ISP pipeline under a variety of lighting conditions, exposure times, and\ngain settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1\">Matheus Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Integrated Task and Motion Planning with Neural Feasibility Checking. (arXiv:2203.10568v1 [cs.RO])","link":"http://arxiv.org/abs/2203.10568","description":"<p>As robots play an increasingly important role in the industrial, the\nexpectations about their applications for everyday living tasks are getting\nhigher. Robots need to perform long-horizon tasks that consist of several\nsub-tasks that need to be accomplished. Task and Motion Planning (TAMP)\nprovides a hierarchical framework to handle the sequential nature of\nmanipulation tasks by interleaving a symbolic task planner that generates a\npossible action sequence, with a motion planner that checks the kinematic\nfeasibility in the geometric world, generating robot trajectories if several\nconstraints are satisfied, e.g., a collision-free trajectory from one state to\nanother. Hence, the reasoning about the task plan's geometric grounding is\ntaken over by the motion planner. However, motion planning is computationally\nintense and is usability as feasibility checker casts TAMP methods inapplicable\nto real-world scenarios. In this paper, we introduce neural feasibility\nclassifier (NFC), a simple yet effective visual heuristic for classifying the\nfeasibility of proposed actions in TAMP. Namely, NFC will identify infeasible\nactions of the task planner without the need for costly motion planning, hence\nreducing planning time in multi-step manipulation tasks. NFC encodes the image\nof the robot's workspace into a feature map thanks to convolutional neural\nnetwork (CNN). We train NFC using simulated data from TAMP problems and label\nthe instances based on IK feasibility checking. Our empirical results in\ndifferent simulated manipulation tasks show that our NFC generalizes to the\nentire robot workspace and has high prediction accuracy even in scenes with\nmultiple obstructions. When combined with state-of-the-art integrated TAMP, our\nNFC enhances its performance while reducing its planning time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tianyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalvatzaki_G/0/1/0/all/0/1\">Georgia Chalvatzaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jan Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Point Cloud Completion on Real Traffic Scenes via Scene-concerned Bottom-up Mechanism. (arXiv:2203.10569v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10569","description":"<p>Real scans always miss partial geometries of objects due to the\nself-occlusions, external-occlusions, and limited sensor resolutions. Point\ncloud completion aims to refer the complete shapes for incomplete 3D scans of\nobjects. Current deep learning-based approaches rely on large-scale complete\nshapes in the training process, which are usually obtained from synthetic\ndatasets. It is not applicable for real-world scans due to the domain gap. In\nthis paper, we propose a self-supervised point cloud completion method (TraPCC)\nfor vehicles in real traffic scenes without any complete data. Based on the\nsymmetry and similarity of vehicles, we make use of consecutive point cloud\nframes to construct vehicle memory bank as reference. We design a bottom-up\nmechanism to focus on both local geometry details and global shape features of\ninputs. In addition, we design a scene-graph in the network to pay attention to\nthe missing parts by the aid of neighboring vehicles. Experiments show that\nTraPCC achieve good performance for real-scan completion on KITTI and nuScenes\ntraffic datasets even without any complete data in training. We also show a\ndownstream application of 3D detection, which benefits from our completion\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yiming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_P/0/1/0/all/0/1\">Peishan Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point3D: tracking actions as moving points with 3D CNNs. (arXiv:2203.10584v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10584","description":"<p>Spatio-temporal action recognition has been a challenging task that involves\ndetecting where and when actions occur. Current state-of-the-art action\ndetectors are mostly anchor-based, requiring sensitive anchor designs and huge\ncomputations due to calculating large numbers of anchor boxes. Motivated by\nnascent anchor-free approaches, we propose Point3D, a flexible and\ncomputationally efficient network with high precision for spatio-temporal\naction recognition. Our Point3D consists of a Point Head for action\nlocalization and a 3D Head for action classification. Firstly, Point Head is\nused to track center points and knot key points of humans to localize the\nbounding box of an action. These location features are then piped into a\ntime-wise attention to learn long-range dependencies across frames. The 3D Head\nis later deployed for the final action classification. Our Point3D achieves\nstate-of-the-art performance on the JHMDB, UCF101-24, and AVA benchmarks in\nterms of frame-mAP and video-mAP. Comprehensive ablation studies also\ndemonstrate the effectiveness of each module proposed in our Point3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jingfei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiaoqing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation. (arXiv:2203.10593v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10593","description":"<p>Open-vocabulary object detection aims to detect novel object categories\nbeyond the training set.\n</p>\n<p>The advanced open-vocabulary two-stage detectors employ instance-level\nvisual-to-visual knowledge distillation to align the visual space of the\ndetector with the semantic space of the Pre-trained Visual-Language Model\n(PVLM).\n</p>\n<p>However, in the more efficient one-stage detector, the absence of\nclass-agnostic object proposals hinders the knowledge distillation on unseen\nobjects, leading to severe performance degradation.\n</p>\n<p>In this paper, we propose a hierarchical visual-language knowledge\ndistillation method, i.e., HierKD, for open-vocabulary one-stage detection.\n</p>\n<p>Specifically, a global-level knowledge distillation is explored to transfer\nthe knowledge of unseen categories from the PVLM to the detector.\n</p>\n<p>Moreover, we combine the proposed global-level knowledge distillation and the\ncommon instance-level knowledge distillation to learn the knowledge of seen and\nunseen categories simultaneously.\n</p>\n<p>Extensive experiments on MS-COCO show that our method significantly surpasses\nthe previous best one-stage detector with 11.9\\% and 6.7\\% $AP_{50}$ gains\nunder the zero-shot detection and generalized zero-shot detection settings, and\nreduces the $AP_{50}$ performance gap from 14\\% to 7.3\\% compared to the best\ntwo-stage detector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaoru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Congxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Clinical Practice: Design and Implementation of Convolutional Neural Network-Based Assistive Diagnosis System for COVID-19 Case Detection from Chest X-Ray Images. (arXiv:2203.10596v1 [eess.IV])","link":"http://arxiv.org/abs/2203.10596","description":"<p>One of the critical tools for early detection and subsequent evaluation of\nthe incidence of lung diseases is chest radiography. This study presents a\nreal-world implementation of a convolutional neural network (CNN) based Carebot\nCovid app to detect COVID-19 from chest X-ray (CXR) images. Our proposed model\ntakes the form of a simple and intuitive application. Used CNN can be deployed\nas a STOW-RS prediction endpoint for direct implementation into DICOM viewers.\nThe results of this study show that the deep learning model based on DenseNet\nand ResNet architecture can detect SARS-CoV-2 from CXR images with precision of\n0.981, recall of 0.962 and AP of 0.993.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kvak_D/0/1/0/all/0/1\">Daniel Kvak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bendik_M/0/1/0/all/0/1\">Marian Bendik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chromcova_A/0/1/0/all/0/1\">Anna Chromcova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks. (arXiv:1804.06039v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1804.06039","description":"<p>Rotation-invariant face detection, i.e. detecting faces with arbitrary\nrotation-in-plane (RIP) angles, is widely required in unconstrained\napplications but still remains as a challenging task, due to the large\nvariations of face appearances. Most existing methods compromise with speed or\naccuracy to handle the large RIP variations. To address this problem more\nefficiently, we propose Progressive Calibration Networks (PCN) to perform\nrotation-invariant face detection in a coarse-to-fine manner. PCN consists of\nthree stages, each of which not only distinguishes the faces from non-faces,\nbut also calibrates the RIP orientation of each face candidate to upright\nprogressively. By dividing the calibration process into several progressive\nsteps and only predicting coarse orientations in early stages, PCN can achieve\nprecise and fast calibration. By performing binary classification of face vs.\nnon-face with gradually decreasing RIP ranges, PCN can accurately detect faces\nwith full $360^{\\circ}$ RIP angles. Such designs lead to a real-time\nrotation-invariant face detector. The experiments on multi-oriented FDDB and a\nchallenging subset of WIDER FACE containing rotated faces in the wild show that\nour PCN achieves quite promising performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xuepeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Meina Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Semantic Segmentation via Spatial-detail Guided Context Propagation. (arXiv:2005.11034v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.11034","description":"<p>Nowadays, vision-based computing tasks play an important role in various\nreal-world applications. However, many vision computing tasks, e.g. semantic\nsegmentation, are usually computationally expensive, posing a challenge to the\ncomputing systems that are resource-constrained but require fast response\nspeed. Therefore, it is valuable to develop accurate and real-time vision\nprocessing models that only require limited computational resources. To this\nend, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet)\nfor achieving real-time semantic segmentation. In SGCPNet, we propose the\nstrategy of spatial-detail guided context propagation. It uses the spatial\ndetails of shallow layers to guide the propagation of the low-resolution global\ncontexts, in which the lost spatial information can be effectively\nreconstructed. In this way, the need for maintaining high-resolution features\nalong the network is freed, therefore largely improving the model efficiency.\nOn the other hand, due to the effective reconstruction of spatial details, the\nsegmentation accuracy can be still preserved. In the experiments, we validate\nthe effectiveness and efficiency of the proposed SGCPNet model. On the\nCitysacpes dataset, for example, our SGCPNet achieves 69.5% mIoU segmentation\naccuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX\n1080 Ti GPU card. In addition, SGCPNet is very lightweight and only contains\n0.61 M parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shijie Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanrong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Few-Shot Classification by Few-Iteration Meta-Learning. (arXiv:2010.00511v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.00511","description":"<p>Autonomous agents interacting with the real world need to learn new concepts\nefficiently and reliably. This requires learning in a low-data regime, which is\na highly challenging problem. We address this task by introducing a fast\noptimization-based meta-learning method for few-shot classification. It\nconsists of an embedding network, providing a general representation of the\nimage, and a base learner module. The latter learns a linear classifier during\nthe inference through an unrolled optimization procedure. We design an inner\nlearning objective composed of (i) a robust classification loss on the support\nset and (ii) an entropy loss, allowing transductive learning from unlabeled\nquery samples. By employing an efficient initialization module and a Steepest\nDescent based optimization algorithm, our base learner predicts a powerful\nclassifier within only a few iterations. Further, our strategy enables\nimportant aspects of the base learner objective to be learned during\nmeta-training. To the best of our knowledge, this work is the first to\nintegrate both induction and transduction into the base learner in an\noptimization-based meta-learning framework. We perform a comprehensive\nexperimental analysis, demonstrating the speed and effectiveness of our\napproach on four few-shot classification datasets. The Code is available at\n\\href{https://github.com/4rdhendu/FIML}{\\textcolor{blue}{https://github.com/4rdhendu/FIML}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1\">Ardhendu Shekhar Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum Learning: A Survey. (arXiv:2101.10382v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.10382","description":"<p>Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soviany_P/0/1/0/all/0/1\">Petru Soviany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1\">Paolo Rota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents. (arXiv:2101.11796v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11796","description":"<p>Creating presentation materials requires complex multimodal reasoning skills\nto summarize key concepts and arrange them in a logical and visually pleasing\nmanner. Can machines learn to emulate this laborious process? We present a\nnovel task and approach for document-to-slide generation. Solving this involves\ndocument summarization, image and text retrieval, slide structure and layout\nprediction to arrange key elements in a form suitable for presentation. We\npropose a hierarchical sequence-to-sequence approach to tackle our task in an\nend-to-end manner. Our approach exploits the inherent structures within\ndocuments and slides and incorporates paraphrasing and layout prediction\nmodules to generate slides. To help accelerate research in this domain, we\nrelease a dataset about 6K paired documents and slide decks used in our\nexperiments. We show that our approach outperforms strong baselines and\nproduces slides with rich content and aligned imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Atlas Generative Models and Geodesic Interpolation. (arXiv:2102.00264v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.00264","description":"<p>Generative neural networks have a well recognized ability to estimate\nunderlying manifold structure of high dimensional data. However, if a single\nlatent space is used, it is not possible to faithfully represent a manifold\nwith topology different from Euclidean space. In this work we define the\ngeneral class of Atlas Generative Models (AGMs), models with hybrid\ndiscrete-continuous latent space that estimate an atlas on the underlying data\nmanifold together with a partition of unity on the data space. We identify\nexisting examples of models from various popular generative paradigms that fit\ninto this class. Due to the atlas interpretation, ideas from non-linear latent\nspace analysis and statistics, e.g. geodesic interpolation, which has\npreviously only been investigated for models with simply connected latent\nspaces, may be extended to the entire class of AGMs in a natural way. We\nexemplify this by generalizing an algorithm for graph based geodesic\ninterpolation to the setting of AGMs, and verify its performance\nexperimentally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stolberg_Larsen_J/0/1/0/all/0/1\">Jakob Stolberg-Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommer_S/0/1/0/all/0/1\">Stefan Sommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selfie Periocular Verification using an Efficient Super-Resolution Approach. (arXiv:2102.08449v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.08449","description":"<p>Selfie-based biometrics has great potential for a wide range of applications\nsince, e.g. periocular verification is contactless and is safe to use in\npandemics such as COVID-19, when a major portion of a face is covered by a\nfacial mask. Despite its advantages, selfie-based biometrics presents\nchallenges since there is limited control over data acquisition at different\ndistances. Therefore, Super-Resolution (SR) has to be used to increase the\nquality of the eye images and to keep or improve the recognition performance.\nWe propose an Efficient Single Image Super-Resolution algorithm, which takes\ninto account a trade-off between the efficiency and the size of its filters. To\nthat end, the method implements a loss function based on the Sharpness metric\nused to evaluate iris images quality. Our method drastically reduces the number\nof parameters compared to the state-of-the-art: from 2,170,142 to 28,654. Our\nbest results on remote verification systems with no redimensioning reached an\nEER of 8.89\\% for FaceNet, 12.14% for VGGFace, and 12.81% for ArcFace. Then,\nembedding vectors were extracted from SR images, the FaceNet-based system\nyielded an EER of 8.92% for a resizing of x2, 8.85% for x3, and 9.32% for x4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1\">Juan Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenzuela_A/0/1/0/all/0/1\">Andres Valenzuela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lara_R/0/1/0/all/0/1\">Rodrigo Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Barrero_M/0/1/0/all/0/1\">Marta Gomez-Barrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Semantic Transitive Consistency for Cross-Modal Learning. (arXiv:2103.14103v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14103","description":"<p>Cross-modal retrieval is generally performed by projecting and aligning the\ndata from two different modalities onto a shared representation space. This\nshared space often also acts as a bridge for translating the modalities. We\naddress the problem of learning such representation space by proposing and\nexploiting the property of Discriminative Semantic Transitive Consistency --\nensuring that the data points are correctly classified even after being\ntransferred to the other modality. Along with semantic transitive consistency,\nwe also enforce the traditional distance minimizing constraint which makes the\nprojections of the corresponding data points from both the modalities to come\ncloser in the representation space. We analyze and compare the contribution of\nboth the loss terms and their interaction, for the task. In addition, we\nincorporate semantic cycle-consistency for each of the modality. We empirically\ndemonstrate better performance owing to the different components with clear\nablation studies. We also provide qualitative results to support the proposals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parida_K/0/1/0/all/0/1\">Kranti Kumar Parida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gaurav Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text to Image Generation with Semantic-Spatial Aware GAN. (arXiv:2104.00567v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00567","description":"<p>Text-to-image synthesis (T2I) aims to generate photo-realistic images which\nare semantically consistent with the text descriptions. Existing methods are\nusually built upon conditional generative adversarial networks (GANs) and\ninitialize an image from noise with sentence embedding, and then refine the\nfeatures with fine-grained word embedding iteratively. A close inspection of\ntheir generated images reveals a major limitation: even though the generated\nimage holistically matches the description, individual image regions or parts\nof somethings are often not recognizable or consistent with words in the\nsentence, e.g. \"a white crown\". To address this problem, we propose a novel\nframework Semantic-Spatial Aware GAN for synthesizing images from input text.\nConcretely, we introduce a simple and effective Semantic-Spatial Aware block,\nwhich (1) learns semantic-adaptive transformation conditioned on text to\neffectively fuse text features and image features, and (2) learns a semantic\nmask in a weakly-supervised way that depends on the current text-image fusion\nprocess in order to guide the transformation spatially. Experiments on the\nchallenging COCO and CUB bird datasets demonstrate the advantage of our method\nover the recent state-of-the-art approaches, regarding both visual fidelity and\nalignment with input text description. Code available at\nhttps://github.com/wtliao/text2image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformers. (arXiv:2104.01122v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01122","description":"<p>Video editing tools are widely used nowadays for digital design. Although the\ndemand for these tools is high, the prior knowledge required makes it difficult\nfor novices to get started. Systems that could follow natural language\ninstructions to perform automatic editing would significantly improve\naccessibility. This paper introduces the language-based video editing (LBVE)\ntask, which allows the model to edit, guided by text instruction, a source\nvideo into a target video. LBVE contains two features: 1) the scenario of the\nsource video is preserved instead of generating a completely different video;\n2) the semantic is presented differently in the target video, and all changes\nare controlled by the given instruction. We propose a Multi-Modal Multi-Level\nTransformer (M$^3$L) to carry out LBVE. M$^3$L dynamically learns the\ncorrespondence between video perception and language semantic at different\nlevels, which benefits both the video understanding and video frame synthesis.\nWe build three new datasets for evaluation, including two diagnostic and one\nfrom natural videos with human-labeled text. Extensive experimental results\nshow that M$^3$L is effective for video editing and that LBVE can lead to a new\nfield toward vision-and-language research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grafton_S/0/1/0/all/0/1\">Scott T. Grafton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel P. Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Does Self-Supervised Pre-Training Perform with Streaming Data?. (arXiv:2104.12081v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.12081","description":"<p>Prior works on self-supervised pre-training focus on the joint training\nscenario, where massive unlabeled data are assumed to be given as input all at\nonce, and only then is a learner trained. Unfortunately, such a problem setting\nis often impractical if not infeasible since many real-world tasks rely on\nsequential learning, e.g., data are decentralized or collected in a streaming\nfashion. In this paper, we conduct the first thorough and dedicated\ninvestigation on self-supervised pre-training with streaming data, aiming to\nshed light on the model behavior under this overlooked setup. Specifically, we\npre-train over 500 models on four categories of pre-training streaming data\nfrom ImageNet and DomainNet and evaluate them on three types of downstream\ntasks and 12 different downstream datasets. Our studies show that, somehow\nbeyond our expectation, with simple data replay or parameter regularization,\nsequential self-supervised pre-training turns out to be an efficient\nalternative for joint pre-training, as the performances of the former are\nmostly on par with those of the latter. Moreover, catastrophic forgetting, a\ncommon issue in sequential supervised learning, is much alleviated in\nsequential self-supervised learning (SSL), which is well justified through our\ncomprehensive empirical analysis on representations and the sharpness of minima\nin the loss landscape. Our findings, therefore, suggest that, in practice, for\nSSL, the cumbersome joint training can be replaced mainly by sequential\nlearning, which in turn enables a much broader spectrum of potential\napplication scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dapeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shipeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qizhengqiu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hailin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving Towards Centers: Re-ranking with Attention and Memory for Re-identification. (arXiv:2105.01447v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01447","description":"<p>Re-ranking utilizes contextual information to optimize the initial ranking\nlist of person or vehicle re-identification (re-ID), which boosts the retrieval\nperformance at post-processing steps. This paper proposes a re-ranking network\nto predict the correlations between the probe and top-ranked neighbor samples.\nSpecifically, all the feature embeddings of query and gallery images are\nexpanded and enhanced by a linear combination of their neighbors, with the\ncorrelation prediction serving as discriminative combination weights. The\ncombination process is equivalent to moving independent embeddings toward the\nidentity centers, improving cluster compactness. For correlation prediction, we\nfirst aggregate the contextual information for probe's k-nearest neighbors via\nthe Transformer encoder. Then, we distill and refine the probe-related features\ninto the Contextual Memory cell via attention mechanism. Like humans that\nretrieve images by not only considering probe images but also memorizing the\nretrieved ones, the Contextual Memory produces multi-view descriptions for each\ninstance. Finally, the neighbors are reconstructed with features fetched from\nthe Contextual Memory, and a binary classifier predicts their correlations with\nthe probe. Experiments on six widely-used person and vehicle re-ID benchmarks\ndemonstrate the effectiveness of the proposed method. Especially, our method\nsurpasses the state-of-the-art re-ranking approaches on large-scale datasets by\na significant margin, i.e., with an average 4.83% CMC@1 and 14.83% mAP\nimprovements on VERI-Wild, MSMT17, and VehicleID datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1\">Lap-Pui Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Shot Face Swapping on Megapixels. (arXiv:2105.04932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04932","description":"<p>Face swapping has both positive applications such as entertainment,\nhuman-computer interaction, etc., and negative applications such as DeepFake\nthreats to politics, economics, etc. Nevertheless, it is necessary to\nunderstand the scheme of advanced methods for high-quality face swapping and\ngenerate enough and representative face swapping images to train DeepFake\ndetection algorithms. This paper proposes the first Megapixel level method for\none shot Face Swapping (or MegaFS for short). Firstly, MegaFS organizes face\nrepresentation hierarchically by the proposed Hierarchical Representation Face\nEncoder (HieRFE) in an extended latent space to maintain more facial details,\nrather than compressed representation in previous face swapping methods.\nSecondly, a carefully designed Face Transfer Module (FTM) is proposed to\ntransfer the identity from a source image to the target by a non-linear\ntrajectory without explicit feature disentanglement. Finally, the swapped faces\ncan be synthesized by StyleGAN2 with the benefits of its training stability and\npowerful generative capability. Each part of MegaFS can be trained separately\nso the requirement of our model for GPU memory can be satisfied for megapixel\nface swapping. In summary, complete face representation, stable training, and\nlimited memory usage are the three novel contributions to the success of our\nmethod. Extensive experiments demonstrate the superiority of MegaFS and the\nfirst megapixel level face swapping database is released for research on\nDeepFake detection and face image editing in the public domain. The dataset is\nat this link.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Driven Image Style Transfer. (arXiv:2106.00178v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00178","description":"<p>Despite having promising results, style transfer, which requires preparing\nstyle images in advance, may result in lack of creativity and accessibility.\nFollowing human instruction, on the other hand, is the most natural way to\nperform artistic style transfer that can significantly improve controllability\nfor visual effect applications. We introduce a new task, language-driven\nartistic style transfer (LDAST), to manipulate the style of a content image,\nguided by a text. We propose contrastive language visual artist (CLVA) that\nlearns to extract visual semantics from style instructions and accomplish LDAST\nby the patch-wise style discriminator. The discriminator considers the\ncorrelation between language and patches of style images or transferred results\nto jointly embed style instructions. CLVA further compares contrastive pairs of\ncontent images and style instructions to improve the mutual relativeness. The\nresults from the same content image can preserve consistent content structures.\nBesides, they should present analogous style patterns from style instructions\nthat contain similar visual semantics. The experiments show that our CLVA is\neffective and achieves superb transferred results on LDAST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Than Meets the Eye: Self-Supervised Depth Reconstruction from Brain Activity. (arXiv:2106.05113v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05113","description":"<p>In the past few years, significant advancements were made in reconstruction\nof observed natural images from fMRI brain recordings using deep-learning\ntools. Here, for the first time, we show that dense 3D depth maps of observed\n2D natural images can also be recovered directly from fMRI brain recordings. We\nuse an off-the-shelf method to estimate the unknown depth maps of natural\nimages. This is applied to both: (i) the small number of images presented to\nsubjects in an fMRI scanner (images for which we have fMRI recordings -\nreferred to as \"paired\" data), and (ii) a very large number of natural images\nwith no fMRI recordings (\"unpaired data\"). The estimated depth maps are then\nused as an auxiliary reconstruction criterion to train for depth reconstruction\ndirectly from fMRI. We propose two main approaches: Depth-only recovery and\njoint image-depth RGBD recovery. Because the number of available \"paired\"\ntraining data (images with fMRI) is small, we enrich the training data via\nself-supervised cycle-consistent training on many \"unpaired\" data (natural\nimages &amp; depth maps without fMRI). This is achieved using our newly defined and\ntrained Depth-based Perceptual Similarity metric as a reconstruction criterion.\nWe show that predicting the depth map directly from fMRI outperforms its\nindirect sequential recovery from the reconstructed images. We further show\nthat activations from early cortical visual areas dominate our depth\nreconstruction results, and propose means to characterize fMRI voxels by their\ndegree of depth-information tuning. This work adds an important layer of\ndecoded information, extending the current envelope of visual brain decoding\ncapabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaziv_G/0/1/0/all/0/1\">Guy Gaziv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05680","description":"<p>Generative Adversarial Networks (GANs) are commonly used for modeling complex\ndistributions of data. Both the generators and discriminators of GANs are often\nmodeled by neural networks, posing a non-transparent optimization problem which\nis non-convex and non-concave over the generator and discriminator,\nrespectively. Such networks are often heuristically optimized with gradient\ndescent-ascent (GDA), but it is unclear whether the optimization problem\ncontains any saddle points, or whether heuristic methods can find them in\npractice. In this work, we analyze the training of Wasserstein GANs with\ntwo-layer neural network discriminators through the lens of convex duality, and\nfor a variety of generators expose the conditions under which Wasserstein GANs\ncan be solved exactly with convex optimization approaches, or can be\nrepresented as convex-concave games. Using this convex duality interpretation,\nwe further demonstrate the impact of different activation functions of the\ndiscriminator. Our observations are verified with numerical results\ndemonstrating the power of the convex interpretation, with applications in\nprogressive training of convex architectures corresponding to linear generators\nand quadratic-activation discriminators for CelebA image generation. The code\nfor our experiments is available at https://github.com/ardasahiner/ProCoGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahiner_A/0/1/0/all/0/1\">Arda Sahiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1\">Tolga Ergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1\">Batu Ozturkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartan_B/0/1/0/all/0/1\">Burak Bartan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1\">Morteza Mardani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03823","description":"<p>Deep learning-based computer-aided diagnosis is gradually deployed to review\nand analyze medical images. However, this paradigm is restricted in real-world\nclinical applications due to the poor robustness and generalization. The issue\nis more sinister with a lack of training data. In this paper, we address the\nchallenge from the transfer learning point of view. Different from the common\nsetting that transferring knowledge from the natural image domain to the\nmedical image domain, we find the knowledge from the same domain further boosts\nthe model robustness and generalization. Therefore, we propose a novel\ntwo-stage framework for robust generalized medical image segmentation. Firstly,\nan unsupervised tile-wise autoencoder pretraining architecture is proposed to\nlearn local and global knowledge. Secondly, the downstream segmentation model\ncoupled with an auxiliary reconstruction network is designed. The\nreconstruction branch encourages the model to capture more general semantic\nfeatures. Experiments of lung segmentation on multi chest X-ray datasets are\nconducted. Comprehensive results demonstrate the superior robustness of the\nproposed framework to corruption and high generalization performance on unseen\ndatasets, especially under the scenario of the limited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06227","description":"<p>Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, most existing\nlearning-based approaches usually suffer from limited manually annotated\nmedical data, which poses a major practical problem for accurate and robust\nmedical image segmentation. In addition, most existing semi-supervised\napproaches are usually not robust compared with the supervised counterparts,\nand also lack explicit modeling of geometric structure and semantic\ninformation, both of which limit the segmentation accuracy. In this work, we\npresent SimCVD, a simple contrastive distillation framework that significantly\nadvances state-of-the-art voxel-wise representation learning. We first describe\nan unsupervised training strategy, which takes two views of an input volume and\npredicts their signed distance maps of object boundaries in a contrastive\nobjective, with only two independent dropout as mask. This simple approach\nworks surprisingly well, performing on the same level as previous fully\nsupervised methods with much less labeled data. We hypothesize that dropout can\nbe viewed as a minimal form of data augmentation and makes the network robust\nto representation collapse. Then, we propose to perform structural distillation\nby distilling pair-wise similarities. We evaluate SimCVD on two popular\ndatasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\ndataset. The results on the LA dataset demonstrate that, in two types of\nlabeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\nprevious best results. Our method can be trained in an end-to-end fashion,\nshowing the promise of utilizing SimCVD as a general framework for downstream\ntasks, such as medical image synthesis, enhancement, and registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation. (arXiv:2109.06165v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06165","description":"<p>Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to a different unlabeled target domain. Most existing\nUDA methods focus on learning domain-invariant feature representation, either\nfrom the domain level or category level, using convolution neural networks\n(CNNs)-based frameworks. One fundamental problem for the category level based\nUDA is the production of pseudo labels for samples in target domain, which are\nusually too noisy for accurate domain alignment, inevitably compromising the\nUDA performance. With the success of Transformer in various tasks, we find that\nthe cross-attention in Transformer is robust to the noisy input pairs for\nbetter feature alignment, thus in this paper Transformer is adopted for the\nchallenging UDA task. Specifically, to generate accurate input pairs, we design\na two-way center-aware labeling algorithm to produce pseudo labels for target\nsamples. Along with the pseudo labels, a weight-sharing triple-branch\ntransformer framework is proposed to apply self-attention and cross-attention\nfor source/target feature learning and source-target domain alignment,\nrespectively. Such design explicitly enforces the framework to learn\ndiscriminative domain-specific and domain-invariant representations\nsimultaneously. The proposed method is dubbed CDTrans (cross-domain\ntransformer), and it provides one of the first attempts to solve UDA tasks with\na pure transformer solution. Experiments show that our proposed method achieves\nthe best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet.\nCode and models are available at https://github.com/CDTrans/CDTrans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tongkun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ElasticFace: Elastic Margin Loss for Deep Face Recognition. (arXiv:2109.09416v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09416","description":"<p>Learning discriminative face features plays a major role in building\nhigh-performing face recognition models. The recent state-of-the-art face\nrecognition solutions proposed to incorporate a fixed penalty margin on\ncommonly used classification loss function, softmax loss, in the normalized\nhypersphere to increase the discriminative power of face recognition models, by\nminimizing the intra-class variation and maximizing the inter-class variation.\nMarginal penalty softmax losses, such as ArcFace and CosFace, assume that the\ngeodesic distance between and within the different identities can be equally\nlearned using a fixed penalty margin. However, such a learning objective is not\nrealistic for real data with inconsistent inter-and intra-class variation,\nwhich might limit the discriminative and generalizability of the face\nrecognition model. In this paper, we relax the fixed penalty margin constrain\nby proposing elastic penalty margin loss (ElasticFace) that allows flexibility\nin the push for class separability. The main idea is to utilize random margin\nvalues drawn from a normal distribution in each training iteration. This aims\nat giving the decision boundary chances to extract and retract to allow space\nfor flexible class separability learning. We demonstrate the superiority of our\nElasticFace loss over ArcFace and CosFace losses, using the same geometric\ntransformation, on a large set of mainstream benchmarks. From a wider\nperspective, our ElasticFace has advanced the state-of-the-art face recognition\nperformance on seven out of nine mainstream benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Autoencoder Training Performance for Hyperspectral Unmixing with Network Reinitialisation. (arXiv:2109.13748v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.13748","description":"<p>Neural networks, in particular autoencoders, are one of the most promising\nsolutions for unmixing hyperspectral data, i.e. reconstructing the spectra of\nobserved substances (endmembers) and their relative mixing fractions\n(abundances), which is needed for effective hyperspectral analysis and\nclassification. However, as we show in this paper, the training of autoencoders\nfor unmixing is highly dependent on weights initialisation; some sets of\nweights lead to degenerate or low-performance solutions, introducing negative\nbias in the expected performance. In this work, we experimentally investigate\nautoencoders stability as well as network reinitialisation methods based on\ncoefficients of neurons' dead activations. We demonstrate that the proposed\ntechniques have a positive effect on autoencoder training in terms of\nreconstruction, abundances and endmembers errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ksiazek_K/0/1/0/all/0/1\">Kamil Ksi&#x105;&#x17c;ek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glomb_P/0/1/0/all/0/1\">Przemys&#x142;aw G&#x142;omb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Romaszewski_M/0/1/0/all/0/1\">Micha&#x142; Romaszewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cholewa_M/0/1/0/all/0/1\">Micha&#x142; Cholewa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grabowski_B/0/1/0/all/0/1\">Bartosz Grabowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buza_K/0/1/0/all/0/1\">Kriszti&#xe1;n B&#xfa;za</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Memory-Guided Semantic Reasoning Model for Image Inpainting. (arXiv:2110.00261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00261","description":"<p>Most existing methods for image inpainting focus on learning the intra-image\npriors from the known regions of the current input image to infer the content\nof the corrupted regions in the same image. While such methods perform well on\nimages with small corrupted regions, it is challenging for these methods to\ndeal with images with large corrupted area due to two potential limitations: 1)\nsuch methods tend to overfit each single training pair of images relying solely\non the intra-image prior knowledge learned from the limited known area; 2) the\ninter-image prior knowledge about the general distribution patterns of visual\nsemantics, which can be transferred across images sharing similar semantics, is\nnot exploited. In this paper, we propose the Generative Memory-Guided Semantic\nReasoning Model (GM-SRM), which not only learns the intra-image priors from the\nknown regions, but also distills the inter-image reasoning priors to infer the\ncontent of the corrupted regions. In particular, the proposed GM-SRM first\npre-learns a generative memory from the whole training data to capture the\nsemantic distribution patterns in a global view. Then the learned memory are\nleveraged to retrieve the matching inter-image priors for the current corrupted\nimage to perform semantic reasoning during image inpainting. While the\nintra-image priors are used for guaranteeing the pixel-level content\nconsistency, the inter-image priors are favorable for performing high-level\nsemantic reasoning, which is particularly effective for inferring semantic\ncontent for large corrupted area. Extensive experiments on Paris Street View,\nCelebA-HQ, and Places2 benchmarks demonstrate that our GM-SRM outperforms the\nstate-of-the-art methods for image inpainting in terms of both the visual\nquality and quantitative metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical-Flow-Reuse-Based Bidirectional Recurrent Network for Space-Time Video Super-Resolution. (arXiv:2110.06786v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06786","description":"<p>In this paper, we consider the task of space-time video super-resolution\n(ST-VSR), which simultaneously increases the spatial resolution and frame rate\nfor a given video. However, existing methods typically suffer from difficulties\nin how to efficiently leverage information from a large range of neighboring\nframes or avoiding the speed degradation in the inference using deformable\nConvLSTM strategies for alignment. % Some recent LSTM-based ST-VSR methods have\nachieved promising results. To solve the above problem of the existing methods,\nwe propose a coarse-to-fine bidirectional recurrent neural network instead of\nusing ConvLSTM to leverage knowledge between adjacent frames. Specifically, we\nfirst use bi-directional optical flow to update the hidden state and then\nemploy a Feature Refinement Module (FRM) to refine the result. Since we could\nfully utilize a large range of neighboring frames, our method leverages local\nand global information more effectively. In addition, we propose an optical\nflow-reuse strategy that can reuse the intermediate flow of adjacent frames,\nwhich considerably reduces the computation burden of frame alignment compared\nwith existing LSTM-based designs. Extensive experiments demonstrate that our\noptical-flow-reuse-based bidirectional recurrent network(OFR-BRN) is superior\nto the state-of-the-art methods both in terms of accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuantong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huairui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference. (arXiv:2110.10031v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.10031","description":"<p>Despite rapid advances in continual learning, a large body of research is\ndevoted to improving performance in the existing setups. While a handful of\nwork do propose new continual learning setups, they still lack practicality in\ncertain aspects. For better practicality, we first propose a novel continual\nlearning setup that is online, task-free, class-incremental, of blurry task\nboundaries and subject to inference queries at any moment. We additionally\npropose a new metric to better measure the performance of the continual\nlearning methods subject to inference queries at any moment. To address the\nchallenging setup and evaluation protocol, we propose an effective method that\nemploys a new memory management scheme and novel learning techniques. Our\nempirical validation demonstrates that the proposed method outperforms prior\narts by large margins. Code and data splits are available at\nhttps://github.com/naver-ai/i-Blurry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1\">Hyunseo Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConAM: Confidence Attention Module for Convolutional Neural Networks. (arXiv:2110.14369v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14369","description":"<p>The so-called \"attention\" is an efficient mechanism to improve the\nperformance of convolutional neural networks. It uses contextual information to\nrecalibrate the input to strengthen the propagation of informative features.\nHowever, the majority of the attention mechanisms only consider either local or\nglobal contextual information, which is singular to extract features. Moreover,\nmany existing mechanisms directly use the contextual information to recalibrate\nthe input, which unilaterally enhances the propagation of the informative\nfeatures, but does not suppress the useless ones. This paper proposes a new\nattention mechanism module based on the correlation between local and global\ncontextual information and we name this correlation as confidence. The novel\nattention mechanism extracts the local and global contextual information\nsimultaneously, and calculates the confidence between them, then uses this\nconfidence to recalibrate the input pixels. The extraction of local and global\ncontextual information increases the diversity of features. The recalibration\nwith confidence suppresses useless information while enhancing the informative\none with fewer parameters. We use CIFAR-10 and CIFAR-100 in our experiments and\nexplore the performance of our method's components by sufficient ablation\nstudies. Finally, we compare our method with a various state-of-the-art\nconvolutional neural networks and the results show that our method completely\nsurpasses these models. We implement ConAM with the Python library, Pytorch,\nand the code and models will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Ziming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neri_F/0/1/0/all/0/1\">Ferrante Neri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Part Discovery from Contrastive Reconstruction. (arXiv:2111.06349v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06349","description":"<p>The goal of self-supervised visual representation learning is to learn\nstrong, transferable image representations, with the majority of research\nfocusing on object or scene level. On the other hand, representation learning\nat part level has received significantly less attention. In this paper, we\npropose an unsupervised approach to object part discovery and segmentation and\nmake three contributions. First, we construct a proxy task through a set of\nobjectives that encourages the model to learn a meaningful decomposition of the\nimage into its parts. Secondly, prior work argues for reconstructing or\nclustering pre-computed features as a proxy to parts; we show empirically that\nthis alone is unlikely to find meaningful parts; mainly because of their low\nresolution and the tendency of classification networks to spatially smear out\ninformation. We suggest that image reconstruction at the level of pixels can\nalleviate this problem, acting as a complementary cue. Lastly, we show that the\nstandard evaluation based on keypoint regression does not correlate well with\nsegmentation quality and thus introduce different metrics, NMI and ARI, that\nbetter characterize the decomposition of objects into parts. Our method yields\nsemantic parts which are consistent across fine-grained but visually distinct\ncategories, outperforming the state of the art on three benchmark datasets.\nCode is available at the project page:\nhttps://www.robots.ox.ac.uk/~vgg/research/unsup-parts/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Subhabrata Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laina_I/0/1/0/all/0/1\">Iro Laina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1\">Christian Rupprecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Color Mapping Functions For HDR Panorama Imaging: Weighted Histogram Averaging. (arXiv:2111.07283v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07283","description":"<p>It is challenging to stitch multiple images with different exposures due to\npossible color distortion and loss of details in the brightest and darkest\nregions of input images. In this paper, a novel color mapping algorithm is\nfirst proposed by introducing a new concept of weighted histogram averaging\n(WHA). The proposed WHA algorithm leverages the correspondence between the\nhistogram bins of two images which are built up by using the non-decreasing\nproperty of the color mapping functions (CMFs). The WHA algorithm is then\nadopted to synthesize a set of differently exposed panorama images. The\nintermediate panorama images are finally fused via a state-of-the-art\nmulti-scale exposure fusion (MEF) algorithm to produce the final panorama\nimage. Extensive experiments indicate that the proposed WHA algorithm\nsignificantly surpasses the related state-of-the-art color mapping methods. The\nproposed high dynamic range (HDR) stitching algorithm based on MEF also\npreserves details in the brightest and darkest regions of the input images\nwell. The related materials will be publicly accessible at\nhttps://github.com/yilun-xu/WHA for reproducible research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yilun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Changyun Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction. (arXiv:2111.07910v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.07910","description":"<p>Hyperspectral image (HSI) reconstruction aims to recover the 3D\nspatial-spectral signal from a 2D measurement in the coded aperture snapshot\nspectral imaging (CASSI) system. The HSI representations are highly similar and\ncorrelated across the spectral dimension. Modeling the inter-spectra\ninteractions is beneficial for HSI reconstruction. However, existing CNN-based\nmethods show limitations in capturing spectral-wise similarity and long-range\ndependencies. Besides, the HSI information is modulated by a coded aperture\n(physical mask) in CASSI. Nonetheless, current algorithms have not fully\nexplored the guidance effect of the mask for HSI restoration. In this paper, we\npropose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI\nreconstruction. Specifically, we present a Spectral-wise Multi-head\nSelf-Attention (S-MSA) that treats each spectral feature as a token and\ncalculates self-attention along the spectral dimension. In addition, we\ncustomize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to\nspatial regions with high-fidelity spectral representations. Extensive\nexperiments show that our MST significantly outperforms state-of-the-art (SOTA)\nmethods on simulation and real HSI datasets while requiring dramatically\ncheaper computational and memory costs. Code and pre-trained models are\navailable at https://github.com/caiyuanhao1998/MST/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Segment-level Semantics for Online Phase Recognition from Surgical Videos. (arXiv:2111.11044v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11044","description":"<p>Automatic surgical phase recognition plays an important role in\nrobot-assisted surgeries. Existing methods ignored a pivotal problem that\nsurgical phases should be classified by learning segment-level semantics\ninstead of solely relying on frame-wise information. In this paper, we present\na segment-attentive hierarchical consistency network (SAHC) for surgical phase\nrecognition from videos. The key idea is to extract hierarchical high-level\nsemantic-consistent segments and use them to refine the erroneous predictions\ncaused by ambiguous frames. To achieve it, we design a temporal hierarchical\nnetwork to generate hierarchical high-level segments. Then, we introduce a\nhierarchical segment-frame attention (SFA) module to capture relations between\nthe low-level frames and high-level segments. By regularizing the predictions\nof frames and their corresponding segments via a consistency loss, the network\ncan generate semantic-consistent segments and then rectify the misclassified\npredictions caused by ambiguous low-level frames. We validate SAHC on two\npublic surgical video datasets, i.e., the M2CAI16 challenge dataset and the\nCholec80 dataset. Experimental results show that our method outperforms\nprevious state-of-the-arts by a large margin, notably reaches 3.8% improvements\non M2CAI16.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinpeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoNeRF: Generalizing NeRF with Geometry Priors. (arXiv:2111.13539v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13539","description":"<p>We present GeoNeRF, a generalizable photorealistic novel view synthesis\nmethod based on neural radiance fields. Our approach consists of two main\nstages: a geometry reasoner and a renderer. To render a novel view, the\ngeometry reasoner first constructs cascaded cost volumes for each nearby source\nview. Then, using a Transformer-based attention mechanism and the cascaded cost\nvolumes, the renderer infers geometry and appearance, and renders detailed\nimages via classical volume rendering techniques. This architecture, in\nparticular, allows sophisticated occlusion reasoning, gathering information\nfrom consistent source views. Moreover, our method can easily be fine-tuned on\na single scene, and renders competitive results with per-scene optimized neural\nrendering methods with a fraction of computational cost. Experiments show that\nGeoNeRF outperforms state-of-the-art generalizable neural rendering models on\nvarious synthetic and real datasets. Lastly, with a slight modification to the\ngeometry reasoner, we also propose an alternative model that adapts to RGBD\nimages. This model directly exploits the depth information often available\nthanks to depth sensors. The implementation code is available at\nhttps://www.idiap.ch/paper/geonerf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Johari_M/0/1/0/all/0/1\">Mohammad Mahdi Johari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepoittevin_Y/0/1/0/all/0/1\">Yann Lepoittevin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Fran&#xe7;ois Fleuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and Accurate Superquadric Recovery: a Probabilistic Approach. (arXiv:2111.14517v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14517","description":"<p>Interpreting objects with basic geometric primitives has long been studied in\ncomputer vision. Among geometric primitives, superquadrics are well known for\ntheir ability to represent a wide range of shapes with few parameters. However,\nas the first and foremost step, recovering superquadrics accurately and\nrobustly from 3D data still remains challenging. The existing methods are\nsubject to local optima and sensitive to noise and outliers in real-world\nscenarios, resulting in frequent failure in capturing geometric shapes. In this\npaper, we propose the first probabilistic method to recover superquadrics from\npoint clouds. Our method builds a Gaussian-uniform mixture model (GUM) on the\nparametric surface of a superquadric, which explicitly models the generation of\noutliers and noise. The superquadric recovery is formulated as a Maximum\nLikelihood Estimation (MLE) problem. We propose an algorithm, Expectation,\nMaximization, and Switching (EMS), to solve this problem, where: (1) outliers\nare predicted from the posterior perspective; (2) the superquadric parameter is\noptimized by the trust-region reflective algorithm; and (3) local optima are\navoided by globally searching and switching among parameters encoding similar\nsuperquadrics. We show that our method can be extended to the\nmulti-superquadrics recovery for complex objects. The proposed method\noutperforms the state-of-the-art in terms of accuracy, efficiency, and\nrobustness on both synthetic and real-world datasets. The code is at\n<a href=\"http://github.com/bmlklwx/EMS-superquadric_fitting.git.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weixiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Sipu Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirikjian_G/0/1/0/all/0/1\">Gregory S. Chirikjian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FENeRF: Face Editing in Neural Radiance Fields. (arXiv:2111.15490v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15490","description":"<p>Previous portrait image generation methods roughly fall into two categories:\n2D GANs and 3D-aware GANs. 2D GANs can generate high fidelity portraits but\nwith low view consistency. 3D-aware GAN methods can maintain view consistency\nbut their generated images are not locally editable. To overcome these\nlimitations, we propose FENeRF, a 3D-aware generator that can produce\nview-consistent and locally-editable portrait images. Our method uses two\ndecoupled latent codes to generate corresponding facial semantics and texture\nin a spatial aligned 3D volume with shared geometry. Benefiting from such\nunderlying 3D representation, FENeRF can jointly render the boundary-aligned\nimage and semantic mask and use the semantic mask to edit the 3D volume via GAN\ninversion. We further show such 3D representation can be learned from widely\navailable monocular image and semantic mask pairs. Moreover, we reveal that\njoint learning semantics and texture helps to generate finer geometry. Our\nexperiments demonstrate that FENeRF outperforms state-of-the-art methods in\nvarious face editing tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources. (arXiv:2112.00061v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00061","description":"<p>Misinformation is now a major problem due to its potential high risks to our\ncore democratic and societal values and orders. Out-of-context misinformation\nis one of the easiest and effective ways used by adversaries to spread viral\nfalse stories. In this threat, a real image is re-purposed to support other\nnarratives by misrepresenting its context and/or elements. The internet is\nbeing used as the go-to way to verify information using different sources and\nmodalities. Our goal is an inspectable method that automates this\ntime-consuming and reasoning-intensive process by fact-checking the\nimage-caption pairing using Web evidence. To integrate evidence and cues from\nboth modalities, we introduce the concept of 'multi-modal cycle-consistency\ncheck'; starting from the image/caption, we gather textual/visual evidence,\nwhich will be compared against the other paired caption/image, respectively.\nMoreover, we propose a novel architecture, Consistency-Checking Network (CCN),\nthat mimics the layered human reasoning across the same and different\nmodalities: the caption vs. textual evidence, the image vs. visual evidence,\nand the image vs. caption. Our work offers the first step and benchmark for\nopen-domain, content-based, multi-modal fact-checking, and significantly\noutperforms previous baselines that did not leverage external evidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelnabi_S/0/1/0/all/0/1\">Sahar Abdelnabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_R/0/1/0/all/0/1\">Rakibul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions. (arXiv:2112.00246v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00246","description":"<p>Perceiving and interacting with 3D articulated objects, such as cabinets,\ndoors, and faucets, pose particular challenges for future home-assistant robots\nperforming daily tasks in human environments. Besides parsing the articulated\nparts and joint parameters, researchers recently advocate learning manipulation\naffordance over the input shape geometry which is more task-aware and\ngeometrically fine-grained. However, taking only passive observations as\ninputs, these methods ignore many hidden but important kinematic constraints\n(e.g., joint location and limits) and dynamic factors (e.g., joint friction and\nrestitution), therefore losing significant accuracy for test cases with such\nuncertainties. In this paper, we propose a novel framework, named AdaAfford,\nthat learns to perform very few test-time interactions for quickly adapting the\naffordance priors to more accurate instance-specific posteriors. We conduct\nlarge-scale experiments using the PartNet-Mobility dataset and prove that our\nsystem performs better than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1\">Jiaqi Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Propagation Cluster: Unleash Full Potential of Object Detectors. (arXiv:2112.00342v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00342","description":"<p>It has been a long history that most object detection methods obtain objects\nby using the non-maximum suppression (NMS) and its improved versions like\nSoft-NMS to remove redundant bounding boxes. We challenge those NMS-based\nmethods from three aspects: 1) The bounding box with highest confidence value\nmay not be the true positive having the biggest overlap with the ground-truth\nbox. 2) Not only suppression is required for redundant boxes, but also\nconfidence enhancement is needed for those true positives. 3) Sorting candidate\nboxes by confidence values is not necessary so that full parallelism is\nachievable.\n</p>\n<p>In this paper, inspired by belief propagation (BP), we propose the Confidence\nPropagation Cluster (CP-Cluster) to replace NMS-based methods, which is fully\nparallelizable as well as better in accuracy. In CP-Cluster, we borrow the\nmessage passing mechanism from BP to penalize redundant boxes and enhance true\npositives simultaneously in an iterative way until convergence. We verified the\neffectiveness of CP-Cluster by applying it to various mainstream detectors such\nas FasterRCNN, SSD, FCOS, YOLOv3, YOLOv5, Centernet etc. Experiments on MS COCO\nshow that our plug and play method, without retraining detectors, is able to\nsteadily improve average mAP of all those state-of-the-art models with a clear\nmargin from 0.3 to 1.9 respectively when compared with NMS-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yichun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wanli Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rundong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Junghyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00374","description":"<p>Existing neural style transfer methods require reference style images to\ntransfer texture information of style images to content images. However, in\nmany practical situations, users may not have reference style images but still\nbe interested in transferring styles by just imagining them. In order to deal\nwith such applications, we propose a new framework that enables a style\ntransfer `without' a style image, but only with a text description of the\ndesired style. Using the pre-trained text-image embedding model of CLIP, we\ndemonstrate the modulation of the style of content images only with a single\ntext condition. Specifically, we propose a patch-wise text-image matching loss\nwith multiview augmentations for realistic texture transfer. Extensive\nexperimental results confirmed the successful image style transfer with\nrealistic textures that reflect semantic query texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gihyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Video Transformer. (arXiv:2112.01514v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01514","description":"<p>In this paper, we propose self-supervised training for video transformers\nusing unlabeled video data. From a given video, we create local and global\nspatiotemporal views with varying spatial sizes and frame rates. Our\nself-supervised objective seeks to match the features of these different views\nrepresenting the same video, to be invariant to spatiotemporal variations in\nactions. To the best of our knowledge, the proposed approach is the first to\nalleviate the dependency on negative samples or dedicated memory banks in\nSelf-supervised Video Transformer (SVT). Further, owing to the flexibility of\nTransformer models, SVT supports slow-fast video processing within a single\narchitecture using dynamically adjusted positional encoding and supports\nlong-term relationship modeling along spatiotemporal dimensions. Our approach\nperforms well on four action recognition benchmarks (Kinetics-400, UCF-101,\nHMDB-51, and SSv2) and converges faster with small batch sizes. Code:\nhttps://git.io/J1juJ\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1\">Kanchana Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael Ryoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting. (arXiv:2112.01518v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01518","description":"<p>Recent progress has shown that large-scale pre-training using contrastive\nimage-text pairs can be a promising alternative for high-quality visual\nrepresentation learning from natural language supervision. Benefiting from a\nbroader source of supervision, this new paradigm exhibits impressive\ntransferability to downstream classification tasks and datasets. However, the\nproblem of transferring the knowledge learned from image-text pairs to more\ncomplex dense prediction tasks has barely been visited. In this work, we\npresent a new framework for dense prediction by implicitly and explicitly\nleveraging the pre-trained knowledge from CLIP. Specifically, we convert the\noriginal image-text matching problem in CLIP to a pixel-text matching problem\nand use the pixel-text score maps to guide the learning of dense prediction\nmodels. By further using the contextual information from the image to prompt\nthe language model, we are able to facilitate our model to better exploit the\npre-trained knowledge. Our method is model-agnostic, which can be applied to\narbitrary dense prediction systems and various pre-trained visual backbones\nincluding both CLIP models and ImageNet pre-trained models. Extensive\nexperiments demonstrate the superior performance of our methods on semantic\nsegmentation, object detection, and instance segmentation tasks. Code is\navailable at https://github.com/raoyongming/DenseCLIP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Input-level Inductive Biases for 3D Reconstruction. (arXiv:2112.03243v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03243","description":"<p>Much of the recent progress in 3D vision has been driven by the development\nof specialized architectures that incorporate geometrical inductive biases. In\nthis paper we tackle 3D reconstruction using a domain agnostic architecture and\nstudy how instead to inject the same type of inductive biases directly as extra\ninputs to the model. This approach makes it possible to apply existing general\nmodels, such as Perceivers, on this rich domain, without the need for\narchitectural changes, while simultaneously maintaining data efficiency of\nbespoke models. In particular we study how to encode cameras, projective ray\nincidence and epipolar geometry as model inputs, and demonstrate competitive\nmulti-view depth estimation performance on multiple benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1\">Wang Yifan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1\">Carl Doersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_R/0/1/0/all/0/1\">Relja Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion. (arXiv:2112.03530v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03530","description":"<p>3D point cloud is an important 3D representation for capturing real world 3D\nobjects. However, real-scanned 3D point clouds are often incomplete, and it is\nimportant to recover complete point clouds for downstream applications. Most\nexisting point cloud completion methods use Chamfer Distance (CD) loss for\ntraining. The CD loss estimates correspondences between two point clouds by\nsearching nearest neighbors, which does not capture the overall point density\ndistribution on the generated shape, and therefore likely leads to non-uniform\npoint cloud generation. To tackle this problem, we propose a novel Point\nDiffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of\na Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The\nCGNet uses a conditional generative model called the denoising diffusion\nprobabilistic model (DDPM) to generate a coarse completion conditioned on the\npartial observation. DDPM establishes a one-to-one pointwise mapping between\nthe generated point cloud and the uniform ground truth, and then optimizes the\nmean squared error loss to realize uniform generation. The RFNet refines the\ncoarse output of the CGNet and further improves quality of the completed point\ncloud. Furthermore, we develop a novel dual-path architecture for both\nnetworks. The architecture can (1) effectively and efficiently extract\nmulti-level features from partially observed point clouds to guide completion,\nand (2) accurately manipulate spatial locations of 3D points to obtain smooth\nsurfaces and sharp details. Extensive experimental results on various benchmark\ndatasets show that our PDR paradigm outperforms previous state-of-the-art\nmethods for point cloud completion. Remarkably, with the help of the RFNet, we\ncan accelerate the iterative generation process of the DDPM by up to 50 times\nwithout much performance drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhifeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xudong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GaTector: A Unified Framework for Gaze Object Prediction. (arXiv:2112.03549v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03549","description":"<p>Gaze object prediction is a newly proposed task that aims to discover the\nobjects being stared at by humans. It is of great application significance but\nstill lacks a unified solution framework. An intuitive solution is to\nincorporate an object detection branch into an existing gaze prediction method.\nHowever, previous gaze prediction methods usually use two different networks to\nextract features from scene image and head image, which would lead to heavy\nnetwork architecture and prevent each branch from joint optimization. In this\npaper, we build a novel framework named GaTector to tackle the gaze object\nprediction problem in a unified way. Particularly, a specific-general-specific\n(SGS) feature extractor is firstly proposed to utilize a shared backbone to\nextract general features for both scene and head images. To better consider the\nspecificity of inputs and tasks, SGS introduces two input-specific blocks\nbefore the shared backbone and three task-specific blocks after the shared\nbackbone. Specifically, a novel Defocus layer is designed to generate\nobject-specific features for the object detection task without losing\ninformation or requiring extra computations. Moreover, the energy aggregation\nloss is introduced to guide the gaze heatmap to concentrate on the stared box.\nIn the end, we propose a novel wUoC metric that can reveal the difference\nbetween boxes even when they share no overlapping area. Extensive experiments\non the GOO dataset verify the superiority of our method in all three tracks,\ni.e. object detection, gaze estimation, and gaze object prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binglu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhijie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction. (arXiv:2112.05146v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.05146","description":"<p>Diffusion models have recently attained significant interest within the\ncommunity owing to their strong performance as generative models. Furthermore,\nits application to inverse problems have demonstrated state-of-the-art\nperformance. Unfortunately, diffusion models have a critical downside - they\nare inherently slow to sample from, needing few thousand steps of iteration to\ngenerate images from pure Gaussian noise. In this work, we show that starting\nfrom Gaussian noise is unnecessary. Instead, starting from a single forward\ndiffusion with better initialization significantly reduces the number of\nsampling steps in the reverse conditional diffusion. This phenomenon is\nformally explained by the contraction theory of the stochastic difference\nequations like our conditional diffusion strategy - the alternating\napplications of reverse diffusion followed by a non-expansive data consistency\nstep. The new sampling strategy, dubbed Come-Closer-Diffuse-Faster (CCDF), also\nreveals a new insight on how the existing feed-forward neural network\napproaches for inverse problems can be synergistically combined with the\ndiffusion models. Experimental results with super-resolution, image inpainting,\nand compressed sensing MRI demonstrate that our method can achieve\nstate-of-the-art reconstruction performance at significantly reduced sampling\nsteps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_B/0/1/0/all/0/1\">Byeongsu Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality. (arXiv:2112.05892v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05892","description":"<p>Group Activity Recognition detects the activity collectively performed by a\ngroup of actors, which requires compositional reasoning of actors and objects.\nWe approach the task by modeling the video as tokens that represent the\nmulti-scale semantic concepts in the video. We propose COMPOSER, a Multiscale\nTransformer based architecture that performs attention-based reasoning over\ntokens at each scale and learns group activity compositionally. In addition,\nprior works suffer from scene biases with privacy and ethical concerns. We only\nuse the keypoint modality which reduces scene biases and prevents acquiring\ndetailed visual data that may contain private or biased information of users.\nWe improve the multiscale representations in COMPOSER by clustering the\nintermediate scale representations, while maintaining consistent cluster\nassignments between scales. Finally, we use techniques such as auxiliary\nprediction and data augmentations tailored to the keypoint signals to aid model\ntraining. We demonstrate the model's strength and interpretability on two\nwidely-used datasets (Volleyball and Collective Activity). COMPOSER achieves up\nto +5.4% improvement with just the keypoint modality. Our code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Honglu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsian_A/0/1/0/all/0/1\">Aviv Shamsian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Farley Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1\">Mubbasir Kapadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graf_H/0/1/0/all/0/1\">Hans Peter Graf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06116","description":"<p>We study the effect of adversarial perturbations of images on deep stereo\nmatching networks for the disparity estimation task. We present a method to\ncraft a single set of perturbations that, when added to any stereo image pair\nin a dataset, can fool a stereo network to significantly alter the perceived\nscene geometry. Our perturbation images are \"universal\" in that they not only\ncorrupt estimates of the network on the dataset they are optimized for, but\nalso generalize to different architectures trained on different datasets. We\nevaluate our approach on multiple benchmark datasets where our perturbations\ncan increase the D1-error (akin to fooling rate) of state-of-the-art stereo\nnetworks from 1% to as much as 87%. We investigate the effect of perturbations\non the estimated scene geometry and identify object classes that are most\nvulnerable. Our analysis on the activations of registered points between left\nand right images led us to find architectural components that can increase\nrobustness against adversaries. By simply designing networks with such\ncomponents, one can reduce the effect of adversaries by up to 60.5%, which\nrivals the robustness of networks fine-tuned with costly adversarial data\naugmentation. Our design principle also improves their robustness against\ncommon image corruptions by an average of 70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berger_Z/0/1/0/all/0/1\">Zachary Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Parth Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards General and Efficient Active Learning. (arXiv:2112.07963v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07963","description":"<p>Active learning selects the most informative samples to exploit limited\nannotation budgets. Existing work follows a cumbersome pipeline that repeats\nthe time-consuming model training and batch data selection multiple times. In\nthis paper, we challenge this status quo by proposing a novel general and\nefficient active learning (GEAL) method following our designed new pipeline.\nUtilizing a publicly available pretrained model, our method selects data from\ndifferent datasets with a single-pass inference of the same model without extra\ntraining or supervision. To capture subtle local information, we propose\nknowledge clusters extracted from intermediate features. Free from the\ntroublesome batch selection strategy, all data samples are selected in one-shot\nthrough a distance-based sampling in the fine-grained knowledge cluster level.\nThis whole process is faster than prior arts by hundreds of times. Extensive\nexperiments verify the effectiveness of our method on object detection, image\nclassification, and semantic segmentation. Our code is publicly available in\nhttps://github.com/yichen928/GEAL_active_learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yichen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09081","description":"<p>We present a visual localization system that learns to estimate camera poses\nin the real world with the help of synthetic data. Despite significant progress\nin recent years, most learning-based approaches to visual localization target\nat a single domain and require a dense database of geo-tagged images to\nfunction well. To mitigate the data scarcity issue and improve the scalability\nof the neural localization models, we introduce TOPO-DataGen, a versatile\nsynthetic data generation tool that traverses smoothly between the real and\nvirtual world, hinged on the geographic camera viewpoint. New large-scale\nsim-to-real benchmark datasets are proposed to showcase and evaluate the\nutility of the said synthetic data. Our experiments reveal that synthetic data\ngenerically enhances the neural network performance on real data. Furthermore,\nwe introduce CrossLoc, a cross-modal visual representation learning approach to\npose estimation that makes full use of the scene coordinate ground truth via\nself-supervision. Without any extra data, CrossLoc significantly outperforms\nthe state-of-the-art methods and achieves substantially higher real-data sample\nefficiency. Our code and datasets are all available at\nhttps://crossloc.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reding_S/0/1/0/all/0/1\">Simon Reding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doytchinov_I/0/1/0/all/0/1\">Iordan Doytchinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Strong Scaling Through Burst Parallel Training. (arXiv:2112.10065v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2112.10065","description":"<p>As emerging deep neural network (DNN) models continue to grow in size, using\nlarge GPU clusters to train DNNs is becoming an essential requirement to\nachieving acceptable training times. In this paper, we consider the case where\nfuture increases in cluster size will cause the global batch size that can be\nused to train models to reach a fundamental limit: beyond a certain point,\nlarger global batch sizes cause sample efficiency to degrade, increasing\noverall time to accuracy. As a result, to achieve further improvements in\ntraining performance, we must instead consider \"strong scaling\" strategies that\nhold the global batch size constant and allocate smaller batches to each GPU.\nUnfortunately, this makes it significantly more difficult to use cluster\nresources efficiently. We present DeepPool, a system that addresses this\nefficiency challenge through two key ideas. First, burst parallelism allocates\nlarge numbers of GPUs to foreground jobs in bursts to exploit the unevenness in\nparallelism across layers. Second, GPU multiplexing prioritizes throughput for\nforeground training jobs, while packing in background training jobs to reclaim\nunderutilized GPU resources, thereby improving cluster-wide utilization.\nTogether, these two ideas enable DeepPool to deliver a 1.2 - 2.3x improvement\nin total cluster throughput over standard data parallelism with a single task\nwhen the cluster scale is large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seo Jin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_J/0/1/0/all/0/1\">Joshua Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1\">Mohammad Alizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belay_A/0/1/0/all/0/1\">Adam Belay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Efficient Transformer-Based Image Pre-training for Low-Level Vision. (arXiv:2112.10175v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10175","description":"<p>Pre-training has marked numerous state of the arts in high-level computer\nvision, while few attempts have ever been made to investigate how pre-training\nacts in image processing systems. In this paper, we tailor transformer-based\npre-training regimes that boost various low-level tasks. To comprehensively\ndiagnose the influence of pre-training, we design a whole set of principled\nevaluation tools that uncover its effects on internal representations. The\nobservations demonstrate that pre-training plays strikingly different roles in\nlow-level tasks. For example, pre-training introduces more local information to\nhigher layers in super-resolution (SR), yielding significant performance gains,\nwhile pre-training hardly affects internal feature representations in\ndenoising, resulting in limited gains. Further, we explore different methods of\npre-training, revealing that multi-related-task pre-training is more effective\nand data-efficient than other alternatives. Finally, we extend our study to\nvarying data scales and model sizes, as well as comparisons between\ntransformers and CNNs-based architectures. Based on the study, we successfully\ndevelop state-of-the-art models for multiple low-level tasks. Code is released\nat https://github.com/fenglinglwb/EDT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shengju Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangbo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projected Sliced Wasserstein Autoencoder-based Hyperspectral Images Anomaly Detection. (arXiv:2112.11243v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11243","description":"<p>Anomaly detection (AD) has been an active research area in various domains.\nYet, the increasing data scale, complexity, and dimension turn the traditional\nmethods into challenging. Recently, the deep generative model, such as the\nvariational autoencoder (VAE), has sparked a renewed interest in the AD\nproblem. However, the probability distribution divergence used as the\nregularization is too strong, which causes the model cannot capture the\nmanifold of the true data. In this paper, we propose the Projected Sliced\nWasserstein (PSW) autoencoder-based anomaly detection method. Rooted in the\noptimal transportation, the PSW distance is a weaker distribution measure\ncompared with $f$-divergence. In particular, the computation-friendly\neigen-decomposition method is leveraged to find the principal component for\nslicing the high-dimensional data. In this case, the Wasserstein distance can\nbe calculated with the closed-form, even the prior distribution is not\nGaussian. Comprehensive experiments conducted on various real-world\nhyperspectral anomaly detection benchmarks demonstrate the superior performance\nof the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Q. M. Jonathan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yimin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition. (arXiv:2112.12496v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12496","description":"<p>Current state-of-the-art deep learning based face recognition (FR) models\nrequire a large number of face identities for central training. However, due to\nthe growing privacy awareness, it is prohibited to access the face images on\nuser devices to continually improve face recognition models. Federated Learning\n(FL) is a technique to address the privacy issue, which can collaboratively\noptimize the model without sharing the data between clients. In this work, we\npropose a FL based framework called FedFR to improve the generic face\nrepresentation in a privacy-aware manner. Besides, the framework jointly\noptimizes personalized models for the corresponding clients via the proposed\nDecoupled Feature Customization module. The client-specific personalized model\ncan serve the need of optimized face recognition experience for registered\nidentities at the local device. To the best of our knowledge, we are the first\nto explore the personalized face recognition in FL setup. The proposed\nframework is validated to be superior to previous approaches on several generic\nand personalized face recognition benchmarks with diverse FL scenarios. The\nsource codes and our proposed personalized FR benchmark under FL setup are\navailable at https://github.com/jackie840129/FedFR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chih-Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chien-Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shang-Hong Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Variational State-Space Filtering. (arXiv:2201.01353v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.01353","description":"<p>We introduce Variational State-Space Filters (VSSF), a new method for\nunsupervised learning, identification, and filtering of latent Markov state\nspace models from raw pixels. We present a theoretically sound framework for\nlatent state space inference under heterogeneous sensor configurations. The\nresulting model can integrate an arbitrary subset of the sensor measurements\nused during training, enabling the learning of semi-supervised state\nrepresentations, thus enforcing that certain components of the learned latent\nstate space to agree with interpretable measurements. From this framework we\nderive L-VSSF, an explicit instantiation of this model with linear latent\ndynamics and Gaussian distribution parameterizations. We experimentally\ndemonstrate L-VSSF's ability to filter in latent space beyond the sequence\nlength of the training dataset across several different test environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfrommer_D/0/1/0/all/0/1\">Daniel Pfrommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1\">Nikolai Matni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04019","description":"<p>The recently proposed MaskFormer gives a refreshed perspective on the task of\nsemantic segmentation: it shifts from the popular pixel-level classification\nparadigm to a mask-level classification method. In essence, it generates paired\nprobabilities and masks corresponding to category segments and combines them\nduring inference for the segmentation maps. In our study, we find that per-mask\nclassification decoder on top of a single-scale feature is not effective enough\nto extract reliable probability or mask. To mine for rich semantic information\nacross the feature pyramid, we propose a transformer-based Pyramid Fusion\nTransformer (PFT) for per-mask approach semantic segmentation with multi-scale\nfeatures. The proposed transformer decoder performs cross-attention between the\nlearnable queries and each spatial feature from the feature pyramid in parallel\nand uses cross-scale inter-query attention to exchange complimentary\ninformation. We achieve competitive performance on three widely used semantic\nsegmentation datasets. In particular, on ADE20K validation set, our result with\nSwin-B backbone surpasses that of MaskFormer's with a much larger Swin-L\nbackbone in both single-scale and multi-scale inference, achieving 54.1 mIoU\nand 55.7 mIoU respectively. Using a Swin-L backbone, we achieve single-scale\n56.1 mIoU and multi-scale 57.4 mIoU, obtaining state-of-the-art performance on\nthe dataset. Extensive experiments on three widely used semantic segmentation\ndatasets verify the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zipeng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Maoqing Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip Connections and Hybrid Learning. (arXiv:2201.05585v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05585","description":"<p>In this paper we address the challenging problem of domain adaptation in\nLiDAR semantic segmentation. We consider the setting where we have a\nfully-labeled data set from source domain and a target domain with a few\nlabeled and many unlabeled examples. We propose a domain adaption framework\nthat mitigates the issue of domain shift and produces appealing performance on\nthe target domain. To this end, we develop a GAN-based image-to-image\ntranslation engine that has generators with alternating connections, and couple\nit with a state-of-the-art LiDAR semantic segmentation network. Our framework\nis hybrid in nature in the sense that our model learning is composed of\nself-supervision, semi-supervision and unsupervised learning. Extensive\nexperiments on benchmark LiDAR semantic segmentation data sets demonstrate that\nour method achieves superior performance in comparison to strong baselines and\nprior arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corral_Soto_E/0/1/0/all/0/1\">Eduardo R. Corral-Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rochan_M/0/1/0/all/0/1\">Mrigank Rochan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yannis Y. He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aich_S/0/1/0/all/0/1\">Shubhra Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingbing_L/0/1/0/all/0/1\">Liu Bingbing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable Augmented Network for Invertible MR Coil Compression. (arXiv:2201.07428v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07428","description":"<p>A large number of coils are able to provide enhanced signal-to-noise ratio\nand improve imaging performance in parallel imaging. Nevertheless, the\nincreasing growth of coil number simultaneously aggravates the drawbacks of\ndata storage and reconstruction speed, especially in some iterative\nreconstructions. Coil compression addresses these issues by generating fewer\nvirtual coils. In this work, a novel variable augmentation network for\ninvertible coil compression termed VAN-ICC is presented. It utilizes inherent\nreversibility of normalizing flow-based models for high-precision compression\nand invertible recovery. By employing the variable augmentation technology to\nimage/k-space variables from multi-coils, VAN-ICC trains invertible networks by\nfinding an invertible and bijective function, which can map the original data\nto the compressed counterpart and vice versa. Experiments conducted on both\nfully-sampled and under-sampled data verified the effectiveness and flexibility\nof VAN-ICC. Quantitative and qualitative comparisons with traditional non-deep\nlearning-based approaches demonstrated that VAN-ICC can carry much higher\ncompression effects. Additionally, its performance is not susceptible to\ndifferent number of virtual coils.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xianghao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lanlan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Coil Augmentation Technology for MR Coil Extrapolation via Deep Learning. (arXiv:2201.07540v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07540","description":"<p>Magnetic resonance imaging (MRI) is a widely used medical imaging modality.\nHowever, due to the limitations in hardware, scan time, and throughput, it is\noften clinically challenging to obtain high-quality MR images. In this article,\nwe propose a method of using artificial intelligence to expand the channel to\nachieve the goal of generating the virtual coils. The main characteristic of\nour work is utilizing dummy variable technology to expand/extrapolate the\nreceive coils in both image and k-space domains. The high-dimensional\ninformation formed by channel expansion is used as the prior information to\nimprove the reconstruction effect of parallel imaging. Two main components are\nincorporated into the network design, namely variable augmentation technology\nand sum of squares (SOS) objective function. Variable augmentation provides the\nnetwork with more high-dimensional prior information, which is helpful for the\nnetwork to extract the deep feature information of the data. The SOS objective\nfunction is employed to solve the deficiency of k-space data training while\nspeeding up convergence. Experimental results demonstrated its great potentials\nin super-resolution of MR images and accelerated parallel imaging\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cailian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xianghao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08845","description":"<p>Volumetric neural rendering methods like NeRF generate high-quality view\nsynthesis results but are optimized per-scene leading to prohibitive\nreconstruction time. On the other hand, deep multi-view stereo methods can\nquickly reconstruct scene geometry via direct network inference. Point-NeRF\ncombines the advantages of these two approaches by using neural 3D point\nclouds, with associated neural features, to model a radiance field. Point-NeRF\ncan be rendered efficiently by aggregating neural point features near scene\nsurfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can\nbe initialized via direct inference of a pre-trained deep network to produce a\nneural point cloud; this point cloud can be finetuned to surpass the visual\nquality of NeRF with 30X faster training time. Point-NeRF can be combined with\nother 3D reconstruction methods and handles the errors and outliers in such\nmethods via a novel pruning and growing mechanism. The experiments on the DTU,\nthe NeRF Synthetics , the ScanNet and the Tanks and Temples datasets\ndemonstrate Point-NeRF can surpass the existing methods and achieve the\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiangeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Object Counting with Similarity-Aware Feature Enhancement. (arXiv:2201.08959v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08959","description":"<p>This work studies the problem of few-shot object counting, which counts the\nnumber of exemplar objects (i.e., described by one or several support images)\noccurring in the query image. The major challenge lies in that the target\nobjects can be densely packed in the query image, making it hard to recognize\nevery single one. To tackle the obstacle, we propose a novel learning block,\nequipped with a similarity comparison module (SCM) and a feature enhancement\nmodule (FEM). Concretely, given a support image and a query image, we first\nderive a score map by comparing their projected features at every spatial\nposition. The score maps regarding all support images are collected together\nand normalized across both the exemplar dimension and the spatial dimensions,\nproducing a reliable similarity map. We then enhance the query feature with the\nsupport features by employing the developed point-wise similarities as the\nweighting coefficients. Such a design encourages the model to inspect the query\nimage by focusing more on the regions akin to the support images, leading to\nmuch clearer boundaries between different objects. Extensive experiments on\nvarious benchmarks and training setups suggest that our method surpasses the\nstate-of-the-art approaches by a sufficiently large margin. For instance, on\nthe very recent large-scale FSC-147 dataset, we beat the second competitor by\nimproving the mean absolute counting error from 22.08 to 14.32 (35%\n$\\uparrow$).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhiyuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_X/0/1/0/all/0/1\">Xinyi Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infrastructure-Based Object Detection and Tracking for Cooperative Driving Automation: A Survey. (arXiv:2201.11871v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11871","description":"<p>Object detection plays a fundamental role in enabling Cooperative Driving\nAutomation (CDA), which is regarded as the revolutionary solution to addressing\nsafety, mobility, and sustainability issues of contemporary transportation\nsystems. Although current computer vision technologies could provide\nsatisfactory object detection results in occlusion-free scenarios, the\nperception performance of onboard sensors could be inevitably limited by the\nrange and occlusion. Owing to flexible position and pose for sensor\ninstallation, infrastructure-based detection and tracking systems can enhance\nthe perception capability for connected vehicles and thus quickly become one of\nthe most popular research topics. In this paper, we review the research\nprogress for infrastructure-based object detection and tracking systems.\nArchitectures of roadside perception systems based on different types of\nsensors are reviewed to show a high-level description of the workflows for\ninfrastructure-based perception systems. Roadside sensors and different\nperception methodologies are reviewed and analyzed with detailed literature to\nprovide a low-level explanation for specific methods followed by Datasets and\nSimulators to draw an overall landscape of infrastructure-based object\ndetection and tracking methods. Discussions are conducted to point out current\nopportunities, open problems, and anticipated future trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhengwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xuewei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_M/0/1/0/all/0/1\">Matthew J. Barth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Built Environment Features for Planning Research with Computer Vision: A Review and Discussion of State-of-the-Art Approaches. (arXiv:2201.12693v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12693","description":"<p>This is an extended abstract for a presentation at The 17th International\nConference on CUPUM - Computational Urban Planning and Urban Management in June\n2021. This study presents an interdisciplinary synthesis of the\nstate-of-the-art approaches in computer vision technologies to extract built\nenvironment features that could improve the robustness of empirical research in\nplanning. We discussed the findings from the review of studies in both planning\nand computer science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meiqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1\">Hao Sheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Features with Parameter-Free Layers. (arXiv:2202.02777v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02777","description":"<p>Trainable layers such as convolutional building blocks are the standard\nnetwork design choices by learning parameters to capture the global context\nthrough successive spatial operations. When designing an efficient network,\ntrainable layers such as the depthwise convolution is the source of efficiency\nin the number of parameters and FLOPs, but there was little improvement to the\nmodel speed in practice. This paper argues that simple built-in parameter-free\noperations can be a favorable alternative to the efficient trainable layers\nreplacing spatial operations in a network architecture. We aim to break the\nstereotype of organizing the spatial operations of building blocks into\ntrainable layers. Extensive experimental analyses based on layer-level studies\nwith fully-trained models and neural architecture searches are provided to\ninvestigate whether parameter-free operations such as the max-pool are\nfunctional. The studies eventually give us a simple yet effective idea for\nredesigning network architectures, where the parameter-free operations are\nheavily used as the main building block without sacrificing the model accuracy\nas much. Experimental results on the ImageNet dataset demonstrate that the\nnetwork architectures with parameter-free operations could enjoy the advantages\nof further efficiency in terms of model speed, the number of the parameters,\nand FLOPs. Code and ImageNet pretrained models are available at\nhttps://github.com/naver-ai/PfLayer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">YoungJoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDAM: Heuristic Difference Attention Module for Convolutional Neural Networks. (arXiv:2202.09556v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09556","description":"<p>The attention mechanism is one of the most important priori knowledge to\nenhance convolutional neural networks. Most attention mechanisms are bound to\nthe convolutional layer and use local or global contextual information to\nrecalibrate the input. This is a popular attention strategy design method.\nGlobal contextual information helps the network to consider the overall\ndistribution, while local contextual information is more general. The\ncontextual information makes the network pay attention to the mean or maximum\nvalue of a particular receptive field. Different from the most attention\nmechanism, this article proposes a novel attention mechanism with the heuristic\ndifference attention module, HDAM. HDAM's input recalibration is based on the\ndifference between the local and global contextual information instead of the\nmean and maximum values. At the same time, to make different layers have a more\nsuitable local receptive field size and increase the exibility of the local\nreceptive field design, we use genetic algorithm to heuristically produce local\nreceptive fields. First, HDAM extracts the mean value of the global and local\nreceptive fields as the corresponding contextual information. Then the\ndifference between the global and local contextual information is calculated.\nFinally HDAM uses this difference to recalibrate the input. In addition, we use\nthe heuristic ability of genetic algorithm to search for the local receptive\nfield size of each layer. Our experiments on CIFAR-10 and CIFAR-100 show that\nHDAM can use fewer parameters than other attention mechanisms to achieve higher\naccuracy. We implement HDAM with the Python library, Pytorch, and the code and\nmodels will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Ziming Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tripartite: Tackle Noisy Labels by a More Precise Partition. (arXiv:2202.09579v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09579","description":"<p>Samples in large-scale datasets may be mislabeled due to various reasons, and\nDeep Neural Networks can easily over-fit to the noisy label data. To tackle\nthis problem, the key point is to alleviate the harm of these noisy labels.\nMany existing methods try to divide training data into clean and noisy subsets\nin terms of loss values, and then process the noisy label data varied. One of\nthe reasons hindering a better performance is the hard samples. As hard samples\nalways have relatively large losses whether their labels are clean or noisy,\nthese methods could not divide them precisely. Instead, we propose a Tripartite\nsolution to partition training data more precisely into three subsets: hard,\nnoisy, and clean. The partition criteria are based on the inconsistent\npredictions of two networks, and the inconsistency between the prediction of a\nnetwork and the given label. To minimize the harm of noisy labels but maximize\nthe value of noisy label data, we apply a low-weight learning on hard data and\na self-supervised learning on noisy label data without using the given labels.\nExtensive experiments demonstrate that Tripartite can filter out noisy label\ndata more precisely, and outperforms most state-of-the-art methods on five\nbenchmark datasets, especially on real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xuefeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Longshan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Ying Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Smoothing and Thresholding Image Segmentation Framework with Weighted Anisotropic-Isotropic Total Variation. (arXiv:2202.10115v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10115","description":"<p>In this paper, we propose a multi-stage image segmentation framework that\nincorporates a weighted difference of anisotropic and isotropic total variation\n(AITV). The segmentation framework generally consists of two stages: smoothing\nand thresholding, thus referred to as SaT. In the first stage, a smoothed image\nis obtained by an AITV-regularized Mumford-Shah (MS) model, which can be solved\nefficiently by the alternating direction method of multipliers (ADMM) with a\nclosed-form solution of a proximal operator of the $\\ell_1 -\\alpha \\ell_2$\nregularizer. Convergence of the ADMM algorithm is analyzed. In the second\nstage, we threshold the smoothed image by $k$-means clustering to obtain the\nfinal segmentation result. Numerical experiments demonstrate that the proposed\nsegmentation framework is versatile for both grayscale and color images,\nefficient in producing high-quality segmentation results within a few seconds,\nand robust to input images that are corrupted with noise, blur, or both. We\ncompare the AITV method with its original convex and nonconvex TV$^p (0&lt;p&lt;1)$\ncounterparts, showcasing the qualitative and quantitative advantages of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1\">Kevin Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yifei Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Fredrick Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RIConv++: Effective Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. (arXiv:2202.13094v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13094","description":"<p>3D point clouds deep learning is a promising field of research that allows a\nneural network to learn features of point clouds directly, making it a robust\ntool for solving 3D scene understanding tasks. While recent works show that\npoint cloud convolutions can be invariant to translation and point permutation,\ninvestigations of the rotation invariance property for point cloud convolution\nhas been so far scarce. Some existing methods perform point cloud convolutions\nwith rotation-invariant features, existing methods generally do not perform as\nwell as translation-invariant only counterpart. In this work, we argue that a\nkey reason is that compared to point coordinates, rotation-invariant features\nconsumed by point cloud convolution are not as distinctive. To address this\nproblem, we propose a simple yet effective convolution operator that enhances\nfeature distinction by designing powerful rotation invariant features from the\nlocal regions. We consider the relationship between the point of interest and\nits neighbors as well as the internal relationship of the neighbors to largely\nimprove the feature descriptiveness. Our network architecture can capture both\nlocal and global context by simply tuning the neighborhood size in each\nconvolution layer. We conduct several experiments on synthetic and real-world\npoint cloud classifications, part segmentation, and shape retrieval to evaluate\nour method, which achieves the state-of-the-art accuracy under challenging\nrotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai-Kit Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Class-agnostic Tracking Using Feature Decorrelation in Point Clouds. (arXiv:2202.13524v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13524","description":"<p>Single object tracking in point clouds has been attracting more and more\nattention owing to the presence of LiDAR sensors in 3D vision. However, the\nexisting methods based on deep neural networks focus mainly on training\ndifferent models for different categories, which makes them unable to perform\nwell in real-world applications when encountering classes unseen during the\ntraining phase. In this work, we investigate a more challenging task in the\nLiDAR point clouds, class-agnostic tracking, where a general model is supposed\nto be learned for any specified targets of both observed and unseen categories.\nIn particular, we first investigate the class-agnostic performances of the\nstate-of-the-art trackers via exposing the unseen categories to them during\ntesting, finding that a key factor for class-agnostic tracking is how to\nconstrain fused features between the template and search region to maintain\ngeneralization when the distribution is shifted from observed to unseen\nclasses. Therefore, we propose a feature decorrelation method to address this\nproblem, which eliminates the spurious correlations of the fused features\nthrough a set of learned weights and further makes the search region consistent\namong foreground points and distinctive between foreground and background\npoints. Experiments on the KITTI and NuScenes demonstrate that the proposed\nmethod can achieve considerable improvements by benchmarking against the\nadvanced trackers P2B and BAT, especially when tracking unseen objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengjing Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuping Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Standardized Pipeline for Colon Nuclei Identification and Counting Challenge. (arXiv:2203.00171v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.00171","description":"<p>Nuclear segmentation and classification is an essential step for\ncomputational pathology. TIA lab from Warwick University organized a nuclear\nsegmentation and classification challenge (CoNIC) for H&amp;E stained\nhistopathology images in colorectal cancer with two highly correlated tasks,\nnuclei segmentation and classification task and cellular composition task.\nThere are a few obstacles we have to address in this challenge, 1) limited\ntraining samples, 2) color variation, 3) imbalanced annotations, 4) similar\nmorphological appearance among classes. To deal with these challenges, we\nproposed a standardized pipeline for nuclear segmentation and classification by\nintegrating several pluggable components. First, we built a GAN-based model to\nautomatically generate pseudo images for data augmentation. Then we trained a\nself-supervised stain normalization model to solve the color variation problem.\nNext we constructed a baseline model HoVer-Net with cost-sensitive loss to\nencourage the model pay more attention on the minority classes. According to\nthe results of the leaderboard, our proposed pipeline achieves 0.40665 mPQ+\n(Rank 49th) and 0.62199 r2 (Rank 10th) in the preliminary test phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jijun Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_F/0/1/0/all/0/1\">Feihu Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jiatai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenbing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zaiyi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Chu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with guttae. (arXiv:2203.01882v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01882","description":"<p>To estimate the corneal endothelial parameters from specular microscopy\nimages depicting cornea guttata (Fuchs dystrophy), we propose a new deep\nlearning methodology that includes a novel attention mechanism named feedback\nnon-local attention (fNLA). Our approach first infers the cell edges, then\nselects the cells that are well detected, and finally applies a postprocessing\nmethod to correct mistakes and provide the binary segmentation from which the\ncorneal parameters are estimated (cell density [ECD], coefficient of variation\n[CV], and hexagonality [HEX]). In this study, we analyzed 1203 images acquired\nwith a Topcon SP-1P microscope, 500 of which contained guttae. Manual\nsegmentation was performed in all images. We compared the results of different\nnetworks (UNet, ResUNeXt, DenseUNets, UNet++) and found that DenseUNets with\nfNLA provided the best performance, with a mean absolute error of 23.16\n[cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%] in HEX, which was 3-6\ntimes smaller than the error obtained by Topcon's built-in software. Our\napproach handled the cells affected by guttae remarkably well, detecting cell\nedges occluded by small guttae while discarding areas covered by large guttae.\nOverall, the proposed method obtained accurate estimations in extremely\nchallenging specular images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rooij_J/0/1/0/all/0/1\">Jeroen van Rooij</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dooren_B/0/1/0/all/0/1\">Bart T.H. van Dooren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemij_H/0/1/0/all/0/1\">Hans G. Lemij</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islamaj_E/0/1/0/all/0/1\">Esma Islamaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vliet_L/0/1/0/all/0/1\">Lucas J. van Vliet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vermeer_K/0/1/0/all/0/1\">Koenraad A. Vermeer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACVNet: Attention Concatenation Volume for Accurate and Efficient Stereo Matching. (arXiv:2203.02146v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02146","description":"<p>Stereo matching is a fundamental building block for many vision and robotics\napplications. An informative and concise cost volume representation is vital\nfor stereo matching of high accuracy and efficiency. In this paper, we present\na novel cost volume construction method which generates attention weights from\ncorrelation clues to suppress redundant information and enhance\nmatching-related information in the concatenation volume. To generate reliable\nattention weights, we propose multi-level adaptive patch matching to improve\nthe distinctiveness of the matching cost at different disparities even for\ntextureless regions. The proposed cost volume is named attention concatenation\nvolume (ACV) which can be seamlessly embedded into most stereo matching\nnetworks, the resulting networks can use a more lightweight aggregation network\nand meanwhile achieve higher accuracy, e.g. using only 1/25 parameters of the\naggregation network can achieve higher accuracy for GwcNet. Furthermore, we\ndesign a highly accurate network (ACVNet) based on our ACV, which achieves\nstate-of-the-art performance on several benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Junda Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Peng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation. (arXiv:2203.02533v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.02533","description":"<p>In this paper, we propose a novel semi-supervised learning (SSL) framework\nnamed BoostMIS that combines adaptive pseudo labeling and informative active\nannotation to unleash the potential of medical image SSL models: (1) BoostMIS\ncan adaptively leverage the cluster assumption and consistency regularization\nof the unlabeled data according to the current learning status. This strategy\ncan adaptively generate one-hot \"hard\" labels converted from task model\npredictions for better task model training. (2) For the unselected unlabeled\nimages with low confidence, we introduce an Active learning (AL) algorithm to\nfind the informative samples as the annotation candidates by exploiting virtual\nadversarial perturbation and model's density-aware entropy. These informative\ncandidates are subsequently fed into the next training cycle for better SSL\nlabel propagation. Notably, the adaptive pseudo-labeling and informative active\nannotation form a learning closed-loop that are mutually collaborative to boost\nmedical image SSL. To verify the effectiveness of the proposed method, we\ncollected a metastatic epidural spinal cord compression (MESCC) dataset that\naims to optimize MESCC diagnosis and classification for improved specialist\nreferral and treatment. We conducted an extensive experimental study of\nBoostMIS on MESCC and another public dataset COVIDx. The experimental results\nverify our framework's effectiveness and generalisability for different medical\nimage datasets with a significant improvement over various state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hallinan_J/0/1/0/all/0/1\">James Hallinan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makmur_A/0/1/0/all/0/1\">Andrew Makmur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Q/0/1/0/all/0/1\">Qingpeng Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ooi_B/0/1/0/all/0/1\">Beng Chin Ooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation. (arXiv:2203.02557v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02557","description":"<p>Image-to-image translation has broad applications in art, design, and\nscientific simulations. The original CycleGAN model emphasizes one-to-one\nmapping via a cycle-consistent loss, while more recent works promote\none-to-many mapping to boost the diversity of the translated images. With\nscientific simulation and one-to-one needs in mind, this work examines if\nequipping CycleGAN with a vision transformer (ViT) and employing advanced\ngenerative adversarial network (GAN) training techniques can achieve better\nperformance. The resulting UNet ViT Cycle-consistent GAN (UVCGAN) model is\ncompared with previous best-performing models on open benchmark image-to-image\ntranslation datasets, Selfie2Anime and CelebA. UVCGAN performs better and\nretains a strong correlation between the original and translated images. An\naccompanying ablation study shows that the gradient penalty and BERT-like\npre-training also contribute to the improvement.~To promote reproducibility and\nopen science, the source code, hyperparameter configurations, and pre-trained\nmodel will be made available at: https://github.com/LS4GAN/uvcgan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Torbunov_D/0/1/0/all/0/1\">Dmitrii Torbunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiwang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Shinjae Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Meifeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viren_B/0/1/0/all/0/1\">Brett Viren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yihui Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Applications in Diagnosis, Treatment and Prognosis of Lung Cancer. (arXiv:2203.02794v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.02794","description":"<p>The recent development of imaging and sequencing technologies enables\nsystematic advances in the clinical study of lung cancer. Meanwhile, the human\nmind is limited in effectively handling and fully utilizing the accumulation of\nsuch enormous amounts of data. Machine learning-based approaches play a\ncritical role in integrating and analyzing these large and complex datasets,\nwhich have extensively characterized lung cancer through the use of different\nperspectives from these accrued data. In this article, we provide an overview\nof machine learning-based approaches that strengthen the varying aspects of\nlung cancer diagnosis and therapy, including early detection, auxiliary\ndiagnosis, prognosis prediction and immunotherapy practice. Moreover, we\nhighlight the challenges and opportunities for future applications of machine\nlearning in lung cancer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Federated Learning with Local Regularization and Sparsification. (arXiv:2203.03106v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.03106","description":"<p>User-level differential privacy (DP) provides certifiable privacy guarantees\nto the information that is specific to any user's data in federated learning.\nExisting methods that ensure user-level DP come at the cost of severe accuracy\ndecrease. In this paper, we study the cause of model performance degradation in\nfederated learning under user-level DP guarantee. We find the key to solving\nthis issue is to naturally restrict the norm of local updates before executing\noperations that guarantee DP. To this end, we propose two techniques, Bounded\nLocal Update Regularization and Local Update Sparsification, to increase model\nquality without sacrificing privacy. We provide theoretical analysis on the\nconvergence of our framework and give rigorous privacy guarantees. Extensive\nexperiments show that our framework significantly improves the privacy-utility\ntrade-off over the state-of-the-arts for federated learning with user-level DP\nguarantee.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">Anda Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peisong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Sheryl Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Rectangling for Image Stitching: A Learning Baseline. (arXiv:2203.03831v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03831","description":"<p>Stitched images provide a wide field-of-view (FoV) but suffer from unpleasant\nirregular boundaries. To deal with this problem, existing image rectangling\nmethods devote to searching an initial mesh and optimizing a target mesh to\nform the mesh deformation in two stages. Then rectangular images can be\ngenerated by warping stitched images. However, these solutions only work for\nimages with rich linear structures, leading to noticeable distortions for\nportraits and landscapes with non-linear objects. In this paper, we address\nthese issues by proposing the first deep learning solution to image\nrectangling. Concretely, we predefine a rigid target mesh and only estimate an\ninitial mesh to form the mesh deformation, contributing to a compact one-stage\nsolution. The initial mesh is predicted using a fully convolutional network\nwith a residual progressive regression strategy. To obtain results with high\ncontent fidelity, a comprehensive objective function is proposed to\nsimultaneously encourage the boundary rectangular, mesh shape-preserving, and\ncontent perceptually natural. Besides, we build the first image stitching\nrectangling dataset with a large diversity in irregular boundaries and scenes.\nExperiments demonstrate our superiority over traditional methods both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04132","description":"<p>Autonomous systems and humans are increasingly sharing the same space. Robots\nwork side by side or even hand in hand with humans to balance each other's\nlimitations. Such cooperative interactions are ever more sophisticated. Thus,\nthe ability to reason not just about a human's center of gravity position, but\nalso its granular motion is an important prerequisite for human-robot\ninteraction. Though, many algorithms ignore the multimodal nature of humans or\nneglect uncertainty in their motion forecasts. We present Motron, a multimodal,\nprobabilistic, graph-structured model, that captures human's multimodality\nusing probabilistic methods while being able to output deterministic\nmaximum-likelihood motions and corresponding confidence values for each mode.\nOur model aims to be tightly integrated with the robotic\nplanning-control-interaction loop; outputting physically feasible human motions\nand being computationally efficient. We demonstrate the performance of our\nmodel on several challenging real-world motion forecasting datasets,\noutperforming a wide array of generative/variational methods while providing\nstate-of-the-art single-output motions if required. Both using significantly\nless computational power than state-of-the art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_T/0/1/0/all/0/1\">Tim Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryll_M/0/1/0/all/0/1\">Markus Ryll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Image Classification. (arXiv:2203.04771v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04771","description":"<p>Hyperspectral images (HSI) not only have a broad macroscopic field of view\nbut also contain rich spectral information, and the types of surface objects\ncan be identified through spectral information, which is one of the main\napplications in hyperspectral image related research.In recent years, more and\nmore deep learning methods have been proposed, among which convolutional neural\nnetworks (CNN) are the most influential. However, CNN-based methods are\ndifficult to capture long-range dependencies, and also require a large amount\nof labeled data for model training.Besides, most of the self-supervised\ntraining methods in the field of HSI classification are based on the\nreconstruction of input samples, and it is difficult to achieve effective use\nof unlabeled samples. To address the shortcomings of CNN networks, we propose a\nnoval multi-scale convolutional embedding module for HSI to realize effective\nextraction of spatial-spectral information, which can be better combined with\nTransformer network.In order to make more efficient use of unlabeled data, we\npropose a new self-supervised pretask. Similar to Mask autoencoder, but our\npre-training method only masks the corresponding token of the central pixel in\nthe encoder, and inputs the remaining token into the decoder to reconstruct the\nspectral information of the central pixel.Such a pretask can better model the\nrelationship between the central feature and the domain feature, and obtain\nmore stable training results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Map Learning for Vision and Language Navigation. (arXiv:2203.05137v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05137","description":"<p>We consider the problem of Vision-and-Language Navigation (VLN). The majority\nof current methods for VLN are trained end-to-end using either unstructured\nmemory such as LSTM, or using cross-modal attention over the egocentric\nobservations of the agent. In contrast to other works, our key insight is that\nthe association between language and vision is stronger when it occurs in\nexplicit spatial representations. In this work, we propose a cross-modal map\nlearning model for vision-and-language navigation that first learns to predict\nthe top-down semantics on an egocentric map for both observed and unobserved\nregions, and then predicts a path towards the goal as a set of waypoints. In\nboth cases, the prediction is informed by the language through cross-modal\nattention mechanisms. We experimentally test the basic hypothesis that\nlanguage-driven navigation can be solved given a map, and then show competitive\nresults on the full VLN-CE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1\">Georgios Georgakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1\">Karl Schmeckpeper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanchoo_K/0/1/0/all/0/1\">Karan Wanchoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1\">Soham Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miltsakaki_E/0/1/0/all/0/1\">Eleni Miltsakaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05151","description":"<p>Current adversarial attack research reveals the vulnerability of\nlearning-based classifiers against carefully crafted perturbations. However,\nmost existing attack methods have inherent limitations in cross-dataset\ngeneralization as they rely on a classification layer with a closed set of\ncategories. Furthermore, the perturbations generated by these methods may\nappear in regions easily perceptible to the human visual system (HVS). To\ncircumvent the former problem, we propose a novel algorithm that attacks\nsemantic similarity on feature representations. In this way, we are able to\nfool classifiers without limiting attacks to a specific dataset. For\nimperceptibility, we introduce the low-frequency constraint to limit\nperturbations within high-frequency components, ensuring perceptual similarity\nbetween adversarial examples and originals. Extensive experiments on three\ndatasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online\nplatforms indicate that our attack can yield misleading and transferable\nadversarial examples across architectures and datasets. Additionally,\nvisualization results and quantitative performance (in terms of four different\nmetrics) show that the proposed algorithm generates more imperceptible\nperturbations than the state-of-the-art methods. Code is made available at.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinliang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weicheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bizhu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PillarGrid: Deep Learning-based Cooperative Perception for 3D Object Detection from Onboard-Roadside LiDAR. (arXiv:2203.06319v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06319","description":"<p>3D object detection plays a fundamental role in enabling autonomous driving,\nwhich is regarded as the significant key to unlocking the bottleneck of\ncontemporary transportation systems from the perspectives of safety, mobility,\nand sustainability. Most of the state-of-the-art (SOTA) object detection\nmethods from point clouds are developed based on a single onboard LiDAR, whose\nperformance will be inevitably limited by the range and occlusion, especially\nin dense traffic scenarios. In this paper, we propose \\textit{PillarGrid}, a\nnovel cooperative perception method fusing information from multiple 3D LiDARs\n(both on-board and roadside), to enhance the situation awareness for connected\nand automated vehicles (CAVs). PillarGrid consists of four main phases: 1)\ncooperative preprocessing of point clouds, 2) pillar-wise voxelization and\nfeature extraction, 3) grid-wise deep fusion of features from multiple sensors,\nand 4) convolutional neural network (CNN)-based augmented 3D object detection.\nA novel cooperative perception platform is developed for model training and\ntesting. Extensive experimentation shows that PillarGrid outperforms the SOTA\nsingle-LiDAR-based 3D object detection methods with respect to both accuracy\nand range by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhengwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_M/0/1/0/all/0/1\">Matthew J. Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisbot_E/0/1/0/all/0/1\">Emrah Akin Sisbot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the answering frame timeline. Extensive experiments on the medical\ninstructional dataset, namely MedVidQA, show that the proposed VPTSL\noutperforms other state-of-the-art (SOTA) methods by 28.36 in mIOU score with a\nlarge margin, which demonstrates the effectiveness of visual prompt and the\ntext span predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention based Memory video portrait matting. (arXiv:2203.06890v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06890","description":"<p>We proposed a novel trimap free video matting method based on the attention\nmechanism. By the nature of the problem, most existing approaches use either\nmultiple computational expansive modules or complex algorithms to exploit\ntemporal information fully. We designed a temporal aggregation module to\ncompute the temporal coherence between the current frame and its two previous\nframes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shufeng Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Memory Learning for Fine-Grained Scene Graph Generation. (arXiv:2203.06907v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06907","description":"<p>As far as Scene Graph Generation (SGG), coarse and fine predicates mix in the\ndataset due to the crowd-sourced labeling, and the long-tail problem is also\npronounced. Given this tricky situation, many existing SGG methods treat the\npredicates equally and learn the model under the supervision of\nmixed-granularity predicates in one stage, leading to relatively coarse\npredictions. In order to alleviate the negative impact of the suboptimum\nmixed-granularity annotation and long-tail effect problems, this paper proposes\na novel Hierarchical Memory Learning (HML) framework to learn the model from\nsimple to complex, which is similar to the human beings' hierarchical memory\nlearning process. After the autonomous partition of coarse and fine predicates,\nthe model is first trained on the coarse predicates and then learns the fine\npredicates. In order to realize this hierarchical learning pattern, this paper,\nfor the first time, formulates the HML framework using the new Concept\nReconstruction (CR) and Model Reconstruction (MR) constraints. It is worth\nnoticing that the HML framework can be taken as one general optimization\nstrategy to improve various SGG models, and significant improvement can be\nachieved on the SGG benchmark (i.e., Visual Genome).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Youming Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yansheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive End-to-End Object Detection in Crowded Scenes. (arXiv:2203.07669v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07669","description":"<p>In this paper, we propose a new query-based detection framework for crowd\ndetection. Previous query-based detectors suffer from two drawbacks: first,\nmultiple predictions will be inferred for a single object, typically in crowded\nscenes; second, the performance saturates as the depth of the decoding stage\nincreases. Benefiting from the nature of the one-to-one label assignment rule,\nwe propose a progressive predicting method to address the above issues.\nSpecifically, we first select accepted queries prone to generate true positive\npredictions, then refine the rest noisy queries according to the previously\naccepted predictions. Experiments show that our method can significantly boost\nthe performance of query-based detectors in crowded scenes. Equipped with our\napproach, Sparse RCNN achieves 92.0\\% $\\text{AP}$, 41.4\\% $\\text{MR}^{-2}$ and\n83.2\\% $\\text{JI}$ on the challenging CrowdHuman \\cite{shao2018crowdhuman}\ndataset, outperforming the box-based method MIP \\cite{chu2020detection} that\nspecifies in handling crowded scenarios. Moreover, the proposed method, robust\nto crowdedness, can still obtain consistent improvements on moderately and\nslightly crowded datasets like CityPersons \\cite{zhang2017citypersons} and COCO\n\\cite{lin2014microsoft}. Code will be made publicly available at\nhttps://github.com/megvii-model/Iter-E2EDET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1\">Anlin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revitalize Region Feature for Democratizing Video-Language Pre-training. (arXiv:2203.07720v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07720","description":"<p>Recent dominant methods for video-language pre-training (VLP) learn\ntransferable representations from the raw pixels in an end-to-end manner to\nachieve advanced performance on downstream video-language tasks. Despite the\nimpressive results, VLP research becomes extremely expensive with the need for\nmassive data and a long training time, preventing further explorations. In this\nwork, we revitalize region features of sparsely sampled video clips to\nsignificantly reduce both spatial and temporal visual redundancy towards\ndemocratizing VLP research at the same time achieving state-of-the-art results.\nSpecifically, to fully explore the potential of region features, we introduce a\nnovel bidirectional region-word alignment regularization that properly\noptimizes the fine-grained relations between regions and certain words in\nsentences, eliminating the domain/modality disconnections between pre-extracted\nregion features and text. Extensive results of downstream text-to-video\nretrieval and video question answering tasks on seven datasets demonstrate the\nsuperiority of our method on both effectiveness and efficiency, e.g., our\nmethod achieves competing results with 80\\% fewer data and 85\\% less\npre-training time compared to the most efficient VLP method so far. The code\nwill be available at \\url{https://github.com/showlab/DemoVLP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianping Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Autofocusing using Tiny Networks for Digital Holographic Microscopy. (arXiv:2203.07772v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.07772","description":"<p>The numerical wavefront backpropagation principle of digital holography\nconfers unique extended focus capabilities, without mechanical displacements\nalong z-axis. However, the determination of the correct focusing distance is a\nnon-trivial and time consuming issue. A deep learning (DL) solution is proposed\nto cast the autofocusing as a regression problem and tested over both\nexperimental and simulated holograms. Single wavelength digital holograms were\nrecorded by a Digital Holographic Microscope (DHM) with a 10$\\mathrm{x}$\nmicroscope objective from a patterned target moving in 3D over an axial range\nof 92 $\\mu$m. Tiny DL models are proposed and compared such as a tiny Vision\nTransformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The\nexperiments show that the predicted focusing distance $Z_R^{\\mathrm{Pred}}$ is\naccurately inferred with an accuracy of 1.2 $\\mu$m in average in comparison\nwith the DHM depth of field of 15 $\\mu$m. Numerical simulations show that all\ntiny models give the $Z_R^{\\mathrm{Pred}}$ with an error below 0.3 $\\mu$m. Such\na prospect would significantly improve the current capabilities of computer\nvision position sensing in applications such as 3D microscopy for life sciences\nor micro-robotics. Moreover, all models reach state of the art inference time\non CPU, less than 25 ms per inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cuenat_S/0/1/0/all/0/1\">St&#xe9;phane Cuenat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andreoli_L/0/1/0/all/0/1\">Louis Andr&#xe9;oli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andre_A/0/1/0/all/0/1\">Antoine N. Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandoz_P/0/1/0/all/0/1\">Patrick Sandoz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laurent_G/0/1/0/all/0/1\">Guillaume J. Laurent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jacquot_M/0/1/0/all/0/1\">Maxime Jacquot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels. (arXiv:2203.07788v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.07788","description":"<p>Noisy training set usually leads to the degradation of generalization and\nrobustness of neural networks. In this paper, we propose using a theoretically\nguaranteed noisy label detection framework to detect and remove noisy data for\nLearning with Noisy Labels (LNL). Specifically, we design a penalized\nregression to model the linear relation between network features and one-hot\nlabels, where the noisy data are identified by the non-zero mean shift\nparameters solved in the regression model. To make the framework scalable to\ndatasets that contain a large number of categories and training data, we\npropose a split algorithm to divide the whole training set into small pieces\nthat can be solved by the penalized regression in parallel, leading to the\nScalable Penalized Regression (SPR) framework. We provide the non-asymptotic\nprobabilistic condition for SPR to correctly identify the noisy data. While SPR\ncan be regarded as a sample selection module for standard supervised training\npipeline, we further combine it with semi-supervised algorithm to further\nexploit the support of noisy data as unlabeled data. Experimental results on\nseveral benchmark datasets and real-world noisy datasets show the effectiveness\nof our framework. Our code and pretrained models are released at\nhttps://github.com/Yikai-Wang/SPR-LNL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xinwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training. (arXiv:2203.08959v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.08959","description":"<p>In this paper, we introduce a novel neural network training framework that\nincreases model's adversarial robustness to adversarial attacks while\nmaintaining high clean accuracy by combining contrastive learning (CL) with\nadversarial training (AT). We propose to improve model robustness to\nadversarial attacks by learning feature representations that are consistent\nunder both data augmentations and adversarial perturbations. We leverage\ncontrastive learning to improve adversarial robustness by considering an\nadversarial example as another positive example, and aim to maximize the\nsimilarity between random augmentations of data samples and their adversarial\nexample, while constantly updating the classification head in order to avoid a\ncognitive dissociation between the classification head and the embedding space.\nThis dissociation is caused by the fact that CL updates the network up to the\nembedding space, while freezing the classification head which is used to\ngenerate new positive adversarial examples. We validate our method, Contrastive\nLearning with Adversarial Features(CLAF), on the CIFAR-10 dataset on which it\noutperforms both robust accuracy and clean accuracy over alternative supervised\nand self-supervised adversarial learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahamim_A/0/1/0/all/0/1\">Adir Rahamim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeh_I/0/1/0/all/0/1\">Itay Naeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-aligned Fusion Transformer for One-shot Object Detection. (arXiv:2203.09093v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09093","description":"<p>One-shot object detection aims at detecting novel objects according to merely\none given instance. With extreme data scarcity, current approaches explore\nvarious feature fusions to obtain directly transferable meta-knowledge. Yet,\ntheir performances are often unsatisfactory. In this paper, we attribute this\nto inappropriate correlation methods that misalign query-support semantics by\noverlooking spatial structures and scale variances. Upon analysis, we leverage\nthe attention mechanism and propose a simple but effective architecture named\nSemantic-aligned Fusion Transformer (SaFT) to resolve these issues.\nSpecifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale\nsemantic enhancement and a horizontal fusion module (HFM) for cross-sample\nfeature fusion. Together, they broaden the vision for each feature point from\nthe support to a whole augmented feature pyramid from the query, facilitating\nsemantic-aligned associations. Extensive experiments on multiple benchmarks\ndemonstrate the superiority of our framework. Without fine-tuning on novel\nclasses, it brings significant performance gains to one-stage baselines,\nlifting state-of-the-art results to a higher level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes. (arXiv:2203.09440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09440","description":"<p>Many basic indoor activities such as eating or writing are always conducted\nupon different tabletops (e.g., coffee tables, writing desks). It is\nindispensable to understanding tabletop scenes in 3D indoor scene parsing\napplications. Unfortunately, it is hard to meet this demand by directly\ndeploying data-driven algorithms, since 3D tabletop scenes are rarely available\nin current datasets. To remedy this defect, we introduce TO-Scene, a\nlarge-scale dataset focusing on tabletop scenes, which contains 20,740 scenes\nwith three variants. To acquire the data, we design an efficient and scalable\nframework, where a crowdsourcing UI is developed to transfer CAD objects onto\ntables from ScanNet, then the output tabletop scenes are simulated into real\nscans and annotated automatically.\n</p>\n<p>Further, a tabletop-aware learning strategy is proposed for better perceiving\nthe small-sized tabletop instances. Notably, we also provide a real scanned\ntest set TO-Real to verify the practical value of TO-Scene. Experiments show\nthat the algorithms trained on TO-Scene indeed work on the realistic test data,\nand our proposed tabletop-aware learning strategy greatly improves the\nstate-of-the-art results on both 3D semantic segmentation and object detection\ntasks. TO-Scene and TO-Real, plus Web UI, will all be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mutian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haolin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos. (arXiv:2203.09463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09463","description":"<p>Current benchmarks for facial expression recognition (FER) mainly focus on\nstatic images, while there are limited datasets for FER in videos. It is still\nambiguous to evaluate whether performances of existing methods remain\nsatisfactory in real-world application-oriented scenes. For example, the\n\"Happy\" expression with high intensity in Talk-Show is more discriminating than\nthe same expression with low intensity in Official-Event. To fill this gap, we\nbuild a large-scale multi-scene dataset, coined as FERV39k. We analyze the\nimportant ingredients of constructing such a novel dataset in three aspects:\n(1) multi-scene hierarchy and expression class, (2) generation of candidate\nvideo clips, (3) trusted manual labelling process. Based on these guidelines,\nwe select 4 scenarios subdivided into 22 scenes, annotate 86k samples\nautomatically obtained from 4k videos based on the well-designed workflow, and\nfinally build 38,935 video clips labeled with 7 classic expressions. Experiment\nbenchmarks on four kinds of baseline frameworks were also provided and further\nanalysis on their performance across different scenes and some challenges for\nfuture research were given. Besides, we systematically investigate key\ncomponents of DFER by ablation studies. The baseline framework and our project\nwill be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yixuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Data-Efficient Detection Transformers. (arXiv:2203.09507v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09507","description":"<p>Detection Transformers have achieved competitive performance on the\nsample-rich COCO dataset. However, we show most of them suffer from significant\nperformance drops on small-size datasets, like Cityscapes. In other words, the\ndetection transformers are generally data-hungry. To tackle this problem, we\nempirically analyze the factors that affect data efficiency, through a\nstep-by-step transition from a data-efficient RCNN variant to the\nrepresentative DETR. The empirical results suggest that sparse feature sampling\nfrom local image areas holds the key. Based on this observation, we alleviate\nthe data-hungry issue of existing detection transformers by simply alternating\nhow key and value sequences are constructed in the cross-attention layer, with\nminimum modifications to the original models. Besides, we introduce a simple\nyet effective label augmentation method to provide richer supervision and\nimprove data efficiency. Experiments show that our method can be readily\napplied to different detection transformers and improve their performance on\nboth small-size and sample-rich datasets. Code will be made publicly available\nat \\url{https://github.com/encounter1997/DE-DETRs}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2203.09215","description":"<p>We propose Human-centered 4D Scene Capture (HSC4D) to accurately and\nefficiently create a dynamic digital world, containing large-scale\nindoor-outdoor scenes, diverse human motions, and rich interactions between\nhumans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is\nspace-free without any external devices' constraints and map-free without\npre-built maps. Considering that IMUs can capture human poses but always drift\nfor long-period use, while LiDAR is stable for global localization but rough\nfor local positions and orientations, HSC4D makes both sensors complement each\nother by a joint optimization and achieves promising results for long-term\ncapture. Relationships between humans and environments are also explored to\nmake their interaction more realistic. To facilitate many down-stream tasks,\nlike AR, VR, robots, autonomous driving, etc., we propose a dataset containing\nthree large scenes (1k-5k $m^2$) with accurate dynamic human motions and\nlocations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)\nand challenging human activities (exercising, walking up/down stairs, climbing,\netc.) demonstrate the effectiveness and the generalization ability of HSC4D.\nThe dataset and code is available at https://github.com/climbingdaily/HSC4D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yudi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yitai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chenglu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}