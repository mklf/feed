{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.6","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-11-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ASMDD: Arabic Speech Mispronunciation Detection Dataset. (arXiv:2111.01136v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01136","description":"<p>The largest dataset of Arabic speech mispronunciation detections in Egyptian\ndialogues is introduced. The dataset is composed of annotated audio files\nrepresenting the top 100 words that are most frequently used in the Arabic\nlanguage, pronounced by 100 Egyptian children (aged between 2 and 8 years old).\nThe dataset is collected and annotated on segmental pronunciation error\ndetections by expert listeners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aly_S/0/1/0/all/0/1\">Salah A. Aly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salah_A/0/1/0/all/0/1\">Abdelrahman Salah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1\">Hesham M. Eraqi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating robustness of You Only Hear Once(YOHO) Algorithm on noisy audios in the VOICe Dataset. (arXiv:2111.01205v1 [cs.SD])","link":"http://arxiv.org/abs/2111.01205","description":"<p>Sound event detection (SED) in machine listening entails identifying the\ndifferent sounds in an audio file and identifying the start and end time of a\nparticular sound event in the audio. SED finds use in various applications such\nas audio surveillance, speech recognition, and context-based indexing and\nretrieval of data in a multimedia database. However, in real-life scenarios,\nthe audios from various sources are seldom devoid of any interfering noise or\ndisturbance. In this paper, we test the performance of the You Only Hear Once\n(YOHO) algorithm on noisy audio data. Inspired by the You Only Look Once (YOLO)\nalgorithm in computer vision, the YOHO algorithm can match the performance of\nthe various state-of-the-art algorithms on datasets such as Music Speech\nDetection Dataset, TUT Sound Event, and Urban-SED datasets but at lower\ninference times. In this paper, we explore the performance of the YOHO\nalgorithm on the VOICe dataset containing audio files with noise at different\nsound-to-noise ratios (SNR). YOHO could outperform or at least match the best\nperforming SED algorithms reported in the VOICe dataset paper and make\ninferences in less time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1\">Soham Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kshitiz Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulimani_M/0/1/0/all/0/1\">Manjunath Mulimani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying causal associations in tweets using deep learning: Use case on diabetes-related tweets from 2017-2021. (arXiv:2111.01225v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01225","description":"<p>Objective: Leveraging machine learning methods, we aim to extract both\nexplicit and implicit cause-effect associations in patient-reported,\ndiabetes-related tweets and provide a tool to better understand opinion,\nfeelings and observations shared within the diabetes online community from a\ncausality perspective. Materials and Methods: More than 30 million\ndiabetes-related tweets in English were collected between April 2017 and\nJanuary 2021. Deep learning and natural language processing methods were\napplied to focus on tweets with personal and emotional content. A\ncause-effect-tweet dataset was manually labeled and used to train 1) a\nfine-tuned Bertweet model to detect causal sentences containing a causal\nassociation 2) a CRF model with BERT based features to extract possible\ncause-effect associations. Causes and effects were clustered in a\nsemi-supervised approach and visualised in an interactive cause-effect-network.\nResults: Causal sentences were detected with a recall of 68% in an imbalanced\ndataset. A CRF model with BERT based features outperformed a fine-tuned BERT\nmodel for cause-effect detection with a macro recall of 68%. This led to 96,676\nsentences with cause-effect associations. \"Diabetes\" was identified as the\ncentral cluster followed by \"Death\" and \"Insulin\". Insulin pricing related\ncauses were frequently associated with \"Death\". Conclusions: A novel\nmethodology was developed to detect causal sentences and identify both explicit\nand implicit, single and multi-word cause and corresponding effect as expressed\nin diabetes-related tweets leveraging BERT-based architectures and visualised\nas cause-effect-network. Extracting causal associations on real-life, patient\nreported outcomes in social media data provides a useful complementary source\nof information in diabetes research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahne_A/0/1/0/all/0/1\">Adrian Ahne</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Tannier_X/0/1/0/all/0/1\">Xavier Tannier</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Rizvi_M/0/1/0/all/0/1\">Md Imbessat Hassan Rizvi</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Czernichow_T/0/1/0/all/0/1\">Thomas Czernichow</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Orchard_F/0/1/0/all/0/1\">Francisco Orchard</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Bour_C/0/1/0/all/0/1\">Charline Bour</a> (6), <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Fagherazzi_G/0/1/0/all/0/1\">Guy Fagherazzi</a> (6) ((1) Paris-Saclay University, UVSQ, Inserm, Gustave Roussy, Exposome and Heredity team, CESP, F-94805, Villejuif, France, (2) Epiconcept, Paris, France, (3) Accenture Labs, San Francisco, USA, (4) Sorbonne University, Inserm, University Sorbonne Paris Nord, Laboratoire d&#x27;Informatique Medicale et d&#x27;Ingenierie des Connaissances pour la e-Sante, LIMICS, Paris, France, (5) Indian Institute of Science, Bengaluru, India, (6) Deep Digital Phenotyping Research Unit, Department of Precision Health, Luxembourg Institute of Health, Strassen, Luxembourg)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching. (arXiv:2111.01231v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01231","description":"<p>Code-switching (CS), a ubiquitous phenomenon due to the ease of communication\nit offers in multilingual communities still remains an understudied problem in\nlanguage processing. The primary reasons behind this are: (1) minimal efforts\nin leveraging large pretrained multilingual models, and (2) the lack of\nannotated data. The distinguishing case of low performance of multilingual\nmodels in CS is the intra-sentence mixing of languages leading to switch\npoints. We first benchmark two sequence labeling tasks -- POS and NER on 4\ndifferent language pairs with a suite of pretrained models to identify the\nproblems and select the best performing model, char-BERT, among them\n(addressing (1)). We then propose a self training method to repurpose the\nexisting pretrained models using a switch-point bias by leveraging unannotated\ndata (addressing (2)). We finally demonstrate that our approach performs well\non both tasks by reducing the gap between the switch point performance while\nretaining the overall performance on two distinct language pairs in both the\ntasks. Our code is available here:\nhttps://github.com/PC09/EMNLP2021-Switch-Point-biased-Self-Training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_P/0/1/0/all/0/1\">Parul Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">Sai Krishna Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Raghavi Chandu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Cost Algorithmic Recourse for Users With Uncertain Cost Functions. (arXiv:2111.01235v1 [cs.LG])","link":"http://arxiv.org/abs/2111.01235","description":"<p>The problem of identifying algorithmic recourse for people affected by\nmachine learning model decisions has received much attention recently. Some\nrecent works model user-incurred cost, which is directly linked to user\nsatisfaction. But they assume a single global cost function that is shared\nacross all users. This is an unrealistic assumption when users have dissimilar\npreferences about their willingness to act upon a feature and different costs\nassociated with changing that feature. In this work, we formalize the notion of\nuser-specific cost functions and introduce a new method for identifying\nactionable recourses for users. By default, we assume that users' cost\nfunctions are hidden from the recourse method, though our framework allows\nusers to partially or completely specify their preferences or cost function. We\npropose an objective function, Expected Minimum Cost (EMC), based on two key\nideas: (1) when presenting a set of options to a user, it is vital that there\nis at least one low-cost solution the user could adopt; (2) when we do not know\nthe user's true cost function, we can approximately optimize for user\nsatisfaction by first sampling plausible cost functions, then finding a set\nthat achieves a good cost for the user in expectation. We optimize EMC with a\nnovel discrete optimization algorithm, Cost-Optimized Local Search (COLS),\nwhich is guaranteed to improve the recourse set quality over iterations.\nExperimental evaluation on popular real-world datasets with simulated user\ncosts demonstrates that our method satisfies up to 25.89 percentage points more\nusers compared to strong baseline methods. Using standard fairness metrics, we\nalso show that our method can provide more fair solutions across demographic\ngroups than comparable methods, and we verify that our method is robust to\nmisspecification of the cost function distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Prateek Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey. (arXiv:2111.01243v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01243","description":"<p>Large, pre-trained transformer-based language models such as BERT have\ndrastically changed the Natural Language Processing (NLP) field. We present a\nsurvey of recent work that uses these large language models to solve NLP tasks\nvia pre-training then fine-tuning, prompting, or text generation approaches. We\nalso present approaches that use pre-trained language models to generate data\nfor training augmentation or other purposes. We conclude with discussions on\nlimitations and suggested directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1\">Bonan Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_H/0/1/0/all/0/1\">Hayley Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulem_E/0/1/0/all/0/1\">Elior Sulem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinz_I/0/1/0/all/0/1\">Ilana Heinz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Transduction with Graph-based Supervision. (arXiv:2111.01272v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01272","description":"<p>The recurrent neural network transducer (RNN-T) objective plays a major role\nin building today's best automatic speech recognition (ASR) systems for\nproduction. Similarly to the connectionist temporal classification (CTC)\nobjective, the RNN-T loss uses specific rules that define how a set of\nalignments is generated to form a lattice for the full-sum training. However,\nit is yet largely unknown if these rules are optimal and do lead to the best\npossible ASR results. In this work, we present a new transducer objective\nfunction that generalizes the RNN-T loss to accept a graph representation of\nthe labels, thus providing a flexible and efficient framework to manipulate\ntraining lattices, for example for restricting alignments or studying different\ntransition rules. We demonstrate that transducer-based ASR with CTC-like\nlattice achieves better results compared to standard RNN-T, while also ensuring\na strictly monotonic alignment, which will allow better optimization of the\ndecoding procedure. For example, the proposed CTC-like transducer system\nachieves a word error rate of 5.9% for the test-other condition of LibriSpeech,\ncorresponding to an improvement of 4.8% relative to an equivalent RNN-T based\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moritz_N/0/1/0/all/0/1\">Niko Moritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP. (arXiv:2111.01322v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01322","description":"<p>Meta-learning considers the problem of learning an efficient learning process\nthat can leverage its past experience to accurately solve new tasks. However,\nthe efficacy of meta-learning crucially depends on the distribution of tasks\navailable for training, and this is often assumed to be known a priori or\nconstructed from limited supervised datasets. In this work, we aim to provide\ntask distributions for meta-learning by considering self-supervised tasks\nautomatically proposed from unlabeled text, to enable large-scale meta-learning\nin NLP. We design multiple distributions of self-supervised tasks by\nconsidering important aspects of task diversity, difficulty, type, domain, and\ncurriculum, and investigate how they affect meta-learning performance. Our\nanalysis shows that all these factors meaningfully alter the task distribution,\nsome inducing significant improvements in downstream few-shot accuracy of the\nmeta-learned models. Empirically, results on 20 downstream tasks show\nsignificant improvements in few-shot learning -- adding up to +4.2% absolute\naccuracy (on average) to the previous unsupervised meta-learning method, and\nperform comparably to supervised methods on the FewRel 2.0 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_T/0/1/0/all/0/1\">Trapit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekaran_K/0/1/0/all/0/1\">Karthick Gunasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munkhdalai_T/0/1/0/all/0/1\">Tsendsuren Munkhdalai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer for Speech Processing using Acoustic Language Similarity. (arXiv:2111.01326v1 [eess.AS])","link":"http://arxiv.org/abs/2111.01326","description":"<p>Speech processing systems currently do not support the vast majority of\nlanguages, in part due to the lack of data in low-resource languages.\nCross-lingual transfer offers a compelling way to help bridge this digital\ndivide by incorporating high-resource data into low-resource systems. Current\ncross-lingual algorithms have shown success in text-based tasks and\nspeech-related tasks over some low-resource languages. However, scaling up\nspeech systems to support hundreds of low-resource languages remains unsolved.\nTo help bridge this gap, we propose a language similarity approach that can\nefficiently identify acoustic cross-lingual transfer pairs across hundreds of\nlanguages. We demonstrate the effectiveness of our approach in language family\nclassification, speech recognition, and speech synthesis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1\">Yifan Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting to the Long Tail: A Meta-Analysis of Transfer Learning Research for Language Understanding Tasks. (arXiv:2111.01340v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01340","description":"<p>Natural language understanding (NLU) has made massive progress driven by\nlarge benchmarks, paired with research on transfer learning to broaden its\nimpact. Benchmarks are dominated by a small set of frequent phenomena, leaving\na long tail of infrequent phenomena underrepresented. In this work, we reflect\non the question: have transfer learning methods sufficiently addressed\nperformance of benchmark-trained models on the long tail? Since benchmarks do\nnot list included/excluded phenomena, we conceptualize the long tail using\nmacro-level dimensions such as underrepresented genres, topics, etc. We assess\ntrends in transfer learning research through a qualitative meta-analysis of 100\nrepresentative papers on transfer learning for NLU. Our analysis asks three\nquestions: (i) Which long tail dimensions do transfer learning studies target?\n(ii) Which properties help adaptation methods improve performance on the long\ntail? (iii) Which methodological gaps have greatest negative impact on long\ntail performance? Our answers to these questions highlight major avenues for\nfuture research in transfer learning for the long tail. Lastly, we present a\ncase study comparing the performance of various adaptation methods on clinical\nnarratives to show how systematically conducted meta-experiments can provide\ninsights that enable us to make progress along these future avenues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Aakanksha Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1\">Jill Lehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Carolyn Rose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Pretrained Language Model for Dialogue Policy Learning. (arXiv:2111.01398v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01398","description":"<p>Reinforcement Learning (RL) has been witnessed its potential for training a\ndialogue policy agent towards maximizing the accumulated rewards given from\nusers. However, the reward can be very sparse for it is usually only provided\nat the end of a dialog session, which causes unaffordable interaction\nrequirements for an acceptable dialog agent. Distinguished from many efforts\ndedicated to optimizing the policy and recovering the reward alternatively\nwhich suffers from easily getting stuck in local optima and model collapse, we\ndecompose the adversarial training into two steps: 1) we integrate a\npre-trained language model as a discriminator to judge whether the current\nsystem action is good enough for the last user action (i.e., \\textit{next\naction prediction}); 2) the discriminator gives and extra local dense reward to\nguide the agent's exploration. The experimental result demonstrates that our\nmethod significantly improves the complete rate (~4.4\\%) and success rate\n(~8.0\\%) of the dialogue system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Dialogue Systems: From Trained Monkeys to Stochastic Parrots. (arXiv:2111.01414v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01414","description":"<p>In spoken dialogue systems, we aim to deploy artificial intelligence to build\nautomated dialogue agents that can converse with humans. Dialogue systems are\nincreasingly being designed to move beyond just imitating conversation and also\nimprove from such interactions over time. In this survey, we present a broad\noverview of methods developed to build dialogue systems over the years.\nDifferent use cases for dialogue systems ranging from task-based systems to\nopen domain chatbots motivate and necessitate specific systems. Starting from\nsimple rule-based systems, research has progressed towards increasingly complex\narchitectures trained on a massive corpus of datasets, like deep learning\nsystems. Motivated with the intuition of resembling human dialogues, progress\nhas been made towards incorporating emotions into the natural language\ngenerator, using reinforcement learning. While we see a trend of highly\nmarginal improvement on some metrics, we find that limited justification exists\nfor the metrics, and evaluation practices are not uniform. To conclude, we flag\nthese concerns and highlight possible research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patlan_A/0/1/0/all/0/1\">Atharv Singh Patlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Shiven Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korde_S/0/1/0/all/0/1\">Shubham Korde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"System Combination for Grammatical Error Correction Based on Integer Programming. (arXiv:2111.01465v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01465","description":"<p>In this paper, we propose a system combination method for grammatical error\ncorrection (GEC), based on nonlinear integer programming (IP). Our method\noptimizes a novel F score objective based on error types, and combines multiple\nend-to-end GEC systems. The proposed IP approach optimizes the selection of a\nsingle best system for each grammatical error type present in the data.\nExperiments of the IP approach on combining state-of-the-art standalone GEC\nsystems show that the combined system outperforms all standalone systems. It\nimproves F0.5 score by 3.61% when combining the two best participating systems\nin the BEA 2019 shared task, and achieves F0.5 score of 73.08%. We also perform\nexperiments to compare our IP approach with another state-of-the-art system\ncombination method for GEC, demonstrating IP's competitive combination\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Ruixi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Translation using Diffusion Models. (arXiv:2111.01471v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01471","description":"<p>In this work, we show a novel method for neural machine translation (NMT),\nusing a denoising diffusion probabilistic model (DDPM), adjusted for textual\ndata, following recent advances in the field. We show that it's possible to\ntranslate sentences non-autoregressively using a diffusion model conditioned on\nthe source sentence. We also show that our model is able to translate between\npairs of languages unseen during training (zero-shot learning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dovrat_S/0/1/0/all/0/1\">Shaked Dovrat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Hate Speech using BERT and Hate Speech Word Embedding with Deep Model. (arXiv:2111.01515v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01515","description":"<p>The enormous amount of data being generated on the web and social media has\nincreased the demand for detecting online hate speech. Detecting hate speech\nwill reduce their negative impact and influence on others. A lot of effort in\nthe Natural Language Processing (NLP) domain aimed to detect hate speech in\ngeneral or detect specific hate speech such as religion, race, gender, or\nsexual orientation. Hate communities tend to use abbreviations, intentional\nspelling mistakes, and coded words in their communication to evade detection,\nadding more challenges to hate speech detection tasks. Thus, word\nrepresentation will play an increasingly pivotal role in detecting hate speech.\nThis paper investigates the feasibility of leveraging domain-specific word\nembedding in Bidirectional LSTM based deep model to automatically\ndetect/classify hate speech. Furthermore, we investigate the use of the\ntransfer learning language model (BERT) on hate speech problem as a binary\nclassification task. The experiments showed that domainspecific word embedding\nwith the Bidirectional LSTM based deep model achieved a 93% f1-score while BERT\nachieved up to 96% f1-score on a combined balanced dataset from available hate\nspeech datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_H/0/1/0/all/0/1\">Hind Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhothali_A/0/1/0/all/0/1\">Areej Alhothali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moria_K/0/1/0/all/0/1\">Kawthar Moria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HydraText: Multi-objective Optimization for Adversarial Textual Attack. (arXiv:2111.01528v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01528","description":"<p>The field of adversarial textual attack has significantly grown over the last\nyears, where the commonly considered objective is to craft adversarial examples\nthat can successfully fool the target models. However, the imperceptibility of\nattacks, which is also an essential objective, is often left out by previous\nstudies. In this work, we advocate considering both objectives at the same\ntime, and propose a novel multi-optimization approach (dubbed HydraText) with\nprovable performance guarantee to achieve successful attacks with high\nimperceptibility. We demonstrate the efficacy of HydraText through extensive\nexperiments under both score-based and decision-based settings, involving five\nmodern NLP models across five benchmark datasets. In comparison to existing\nstate-of-the-art attacks, HydraText consistently achieves simultaneously higher\nsuccess rates, lower modification rates, and higher semantic similarity to the\noriginal texts. A human evaluation study shows that the adversarial examples\ncrafted by HydraText maintain validity and naturality well. Finally, these\nexamples also exhibit good transferability and can bring notable robustness\nimprovement to the target models by adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UQuAD1.0: Development of an Urdu Question Answering Training Data for Machine Reading Comprehension. (arXiv:2111.01543v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01543","description":"<p>In recent years, low-resource Machine Reading Comprehension (MRC) has made\nsignificant progress, with models getting remarkable performance on various\nlanguage datasets. However, none of these models have been customized for the\nUrdu language. This work explores the semi-automated creation of the Urdu\nQuestion Answering Dataset (UQuAD1.0) by combining machine-translated SQuAD\nwith human-generated samples derived from Wikipedia articles and Urdu RC\nworksheets from Cambridge O-level books. UQuAD1.0 is a large-scale Urdu dataset\nintended for extractive machine reading comprehension tasks consisting of 49k\nquestion Answers pairs in question, passage, and answer format. In UQuAD1.0,\n45000 pairs of QA were generated by machine translation of the original\nSQuAD1.0 and approximately 4000 pairs via crowdsourcing. In this study, we used\ntwo types of MRC models: rule-based baseline and advanced Transformer-based\nmodels. However, we have discovered that the latter outperforms the others;\nthus, we have decided to concentrate solely on Transformer-based architectures.\nUsing XLMRoBERTa and multi-lingual BERT, we acquire an F1 score of 0.66 and\n0.63, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazi_S/0/1/0/all/0/1\">Samreen Kazi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Khoja_S/0/1/0/all/0/1\">Shakeel Khoja</a> (1) ((1) School of Mathematics &amp; Computer Science, Institute of Business Administration, Karachi Pakistan)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMdiff: A Visual Diff Tool to Compare Language Models. (arXiv:2111.01582v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01582","description":"<p>While different language models are ubiquitous in NLP, it is hard to contrast\ntheir outputs and identify which contexts one can handle better than the other.\nTo address this question, we introduce LMdiff, a tool that visually compares\nprobability distributions of two models that differ, e.g., through finetuning,\ndistillation, or simply training with different parameter sizes. LMdiff allows\nthe generation of hypotheses about model behavior by investigating text\ninstances token by token and further assists in choosing these interesting text\ninstances by identifying the most interesting phrases from large corpora. We\nshowcase the applicability of LMdiff for hypothesis generation across multiple\ncase studies. A demo is available at <a href=\"http://lmdiff.net\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1\">Arvind Satyanarayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards text-based phishing detection. (arXiv:2111.01676v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01676","description":"<p>This paper reports on an experiment into text-based phishing detection using\nreadily available resources and without the use of semantics. The developed\nalgorithm is a modified version of previously published work that works with\nthe same tools. The results obtained in recognizing phishing emails are\nconsiderably better than the previously reported work; but the rate of text\nfalsely identified as phishing is slightly worse. It is expected that adding\nsemantic component will reduce the false positive rate while preserving the\ndetection accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gilchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1\">Julia M. Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Classifier Training Efficiency for Automatic Cyberbullying Detection with Feature Density. (arXiv:2111.01689v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01689","description":"<p>We study the effectiveness of Feature Density (FD) using different\nlinguistically-backed feature preprocessing methods in order to estimate\ndataset complexity, which in turn is used to comparatively estimate the\npotential performance of machine learning (ML) classifiers prior to any\ntraining. We hypothesise that estimating dataset complexity allows for the\nreduction of the number of required experiments iterations. This way we can\noptimize the resource-intensive training of ML models which is becoming a\nserious issue due to the increases in available dataset sizes and the ever\nrising popularity of models based on Deep Neural Networks (DNN). The problem of\nconstantly increasing needs for more powerful computational resources is also\naffecting the environment due to alarmingly-growing amount of CO2 emissions\ncaused by training of large-scale ML models. The research was conducted on\nmultiple datasets, including popular datasets, such as Yelp business review\ndataset used for training typical sentiment analysis models, as well as more\nrecent datasets trying to tackle the problem of cyberbullying, which, being a\nserious social problem, is also a much more sophisticated problem form the\npoint of view of linguistic representation. We use cyberbullying datasets\ncollected for multiple languages, namely English, Japanese and Polish. The\ndifference in linguistic complexity of datasets allows us to additionally\ndiscuss the efficacy of linguistically-backed word preprocessing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eronen_J/0/1/0/all/0/1\">Juuso Eronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_A/0/1/0/all/0/1\">Aleksander Pohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leliwa_G/0/1/0/all/0/1\">Gniewosz Leliwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroczynski_M/0/1/0/all/0/1\">Michal Wroczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in End-to-End Automatic Speech Recognition. (arXiv:2111.01690v1 [eess.AS])","link":"http://arxiv.org/abs/2111.01690","description":"<p>Recently, the speech community is seeing a significant trend of moving from\ndeep neural network based hybrid modeling to end-to-end (E2E) modeling for\nautomatic speech recognition (ASR). While E2E models achieve the\nstate-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid\nmodels are still used in a large proportion of commercial ASR systems at the\ncurrent time. There are lots of practical factors that affect the production\nmodel deployment decision. Traditional hybrid models, being optimized for\nproduction for decades, are usually good at these factors. Without providing\nexcellent solutions to all these factors, it is hard for E2E models to be\nwidely commercialized. In this paper, we will overview the recent advances in\nE2E models, focusing on technologies addressing those challenges from the\nindustry's perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Effectiveness of Using Internal Signals for Check-Worthy Claim Identification in Unlabeled Data for Automated Fact-Checking. (arXiv:2111.01706v1 [cs.CL])","link":"http://arxiv.org/abs/2111.01706","description":"<p>While recent work on automated fact-checking has focused mainly on verifying\nand explaining claims, for which the list of claims is readily available,\nidentifying check-worthy claim sentences from a text remains challenging.\nCurrent claim identification models rely on manual annotations for each\nsentence in the text, which is an expensive task and challenging to conduct on\na frequent basis across multiple domains. This paper explores methodology to\nidentify check-worthy claim sentences from fake news articles, irrespective of\ndomain, without explicit sentence-level annotations. We leverage two internal\nsupervisory signals - headline and the abstractive summary - to rank the\nsentences based on semantic similarity. We hypothesize that this ranking\ndirectly correlates to the check-worthiness of the sentences. To assess the\neffectiveness of this hypothesis, we build pipelines that leverage the ranking\nof sentences based on either the headline or the abstractive summary. The\ntop-ranked sentences are used for the downstream fact-checking tasks of\nevidence retrieval and the article's veracity prediction by the pipeline. Our\nfindings suggest that the top 3 ranked sentences contain enough information for\nevidence-based fact-checking of a fake news article. We also show that while\nthe headline has more gisting similarity with how a fact-checking website\nwrites a claim, the summary-based pipeline is the most promising for an\nend-to-end fact-checking system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1\">Archita Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1\">Rohini K. Srihari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized One-Shot Lipreading for an ALS Patient. (arXiv:2111.01740v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01740","description":"<p>Lipreading or visually recognizing speech from the mouth movements of a\nspeaker is a challenging and mentally taxing task. Unfortunately, multiple\nmedical conditions force people to depend on this skill in their day-to-day\nlives for essential communication. Patients suffering from Amyotrophic Lateral\nSclerosis (ALS) often lose muscle control, consequently their ability to\ngenerate speech and communicate via lip movements. Existing large datasets do\nnot focus on medical patients or curate personalized vocabulary relevant to an\nindividual. Collecting a large-scale dataset of a patient, needed to train\nmod-ern data-hungry deep learning models is, however, extremely challenging. In\nthis work, we propose a personalized network to lipread an ALS patient using\nonly one-shot examples. We depend on synthetically generated lip movements to\naugment the one-shot scenario. A Variational Encoder based domain adaptation\ntechnique is used to bridge the real-synthetic domain gap. Our approach\nsignificantly improves and achieves high top-5accuracy with 83.2% accuracy\ncompared to 62.6% achieved by comparable methods for the patient. Apart from\nevaluating our approach on the ALS patient, we also extend it to people with\nhearing impairment relying extensively on lip movements to communicate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_B/0/1/0/all/0/1\">Bipasha Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aditya Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_R/0/1/0/all/0/1\">Rudrabha Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language. (arXiv:2103.01242v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01242","description":"<p>Current NLP datasets targeting ambiguity can be solved by a native speaker\nwith relative ease. We present Cryptonite, a large-scale dataset based on\ncryptic crosswords, which is both linguistically complex and naturally sourced.\nEach example in Cryptonite is a cryptic clue, a short phrase or sentence with a\nmisleading surface reading, whose solving requires disambiguating semantic,\nsyntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues\npose a challenge even for experienced solvers, though top-tier experts can\nsolve them with almost 100% accuracy. Cryptonite is a challenging task for\ncurrent models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6%\naccuracy, on par with the accuracy of a rule-based clue solver (8.6%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilman_D/0/1/0/all/0/1\">Dan Kilman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of Intent Classification and Slot Labeling in Goal-oriented Dialog Systems to Real-world Noise. (arXiv:2104.07149v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07149","description":"<p>Intent Classification (IC) and Slot Labeling (SL) models, which form the\nbasis of dialogue systems, often encounter noisy data in real-word\nenvironments. In this work, we investigate how robust IC/SL models are to noisy\ndata. We collect and publicly release a test-suite for seven common noise types\nfound in production human-to-bot conversations (abbreviations, casing,\nmisspellings, morphological variants, paraphrases, punctuation and synonyms).\nOn this test-suite, we show that common noise types substantially degrade the\nIC accuracy and SL F1 performance of state-of-the-art BERT-based IC/SL models.\nBy leveraging cross-noise robustness transfer -- training on one noise type to\nimprove robustness on another noise type -- we design aggregate\ndata-augmentation approaches that increase the model performance across all\nseven noise types by +10.8% for IC accuracy and +15 points for SL F1 on\naverage. To the best of our knowledge, this is the first work to present a\nsingle IC/SL model that is robust to a wide range of noise phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sailik Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krone_J/0/1/0/all/0/1\">Jason Krone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KLUE: Korean Language Understanding Evaluation. (arXiv:2105.09680v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.09680","description":"<p>We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE\nis a collection of 8 Korean natural language understanding (NLU) tasks,\nincluding Topic Classification, SemanticTextual Similarity, Natural Language\nInference, Named Entity Recognition, Relation Extraction, Dependency Parsing,\nMachine Reading Comprehension, and Dialogue State Tracking. We build all of the\ntasks from scratch from diverse source corpora while respecting copyrights, to\nensure accessibility for anyone without any restrictions. With ethical\nconsiderations in mind, we carefully design annotation protocols. Along with\nthe benchmark tasks and data, we provide suitable evaluation metrics and\nfine-tuning recipes for pretrained language models for each task. We\nfurthermore release the pretrained language models (PLM), KLUE-BERT and\nKLUE-RoBERTa, to help reproducing baseline models on KLUE and thereby\nfacilitate future research. We make a few interesting observations from the\npreliminary experiments using the proposed KLUE benchmark suite, already\ndemonstrating the usefulness of this new benchmark suite. First, we find\nKLUE-RoBERTa-large outperforms other baselines, including multilingual PLMs and\nexisting open-source Korean PLMs. Second, we see minimal degradation in\nperformance even when we replace personally identifiable information from the\npretraining corpus, suggesting that privacy and NLU capability are not at odds\nwith each other. Lastly, we find that using BPE tokenization in combination\nwith morpheme-level pre-tokenization is effective in tasks involving\nmorpheme-level tagging, detection and generation. In addition to accelerating\nKorean NLP research, our comprehensive documentation on creating KLUE will\nfacilitate creating similar resources for other languages in the future. KLUE\nis available at https://klue-benchmark.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jihyung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jangwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chisung Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junseong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yongsook Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Taehwan Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joohong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Juhyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Sungwon Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Younghoon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Inkwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Sangwoo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Myeonghwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Seongbo Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_S/0/1/0/all/0/1\">Seungwon Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunkyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jamin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Lucy Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01135","description":"<p>Sequence-to-sequence learning with neural networks has become the de facto\nstandard for sequence prediction tasks. This approach typically models the\nlocal distribution over the next word with a powerful neural network that can\ncondition on arbitrary context. While flexible and performant, these models\noften require large datasets for training and can fail spectacularly on\nbenchmarks designed to test for compositional generalization. This work\nexplores an alternative, hierarchical approach to sequence-to-sequence learning\nwith quasi-synchronous grammars, where each node in the target tree is\ntransduced by a node in the source tree. Both the source and target trees are\ntreated as latent and induced during training. We develop a neural\nparameterization of the grammar which enables parameter sharing over the\ncombinatorial space of derivation rules without the need for manual feature\nengineering. We apply this latent neural grammar to various domains -- a\ndiagnostic language navigation task designed to test for compositional\ngeneralization (SCAN), style transfer, and small-scale machine translation --\nand find that it performs respectably compared to standard baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Latent Tree Induction with Distant Supervision via Span Constraints. (arXiv:2109.05112v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05112","description":"<p>For over thirty years, researchers have developed and analyzed methods for\nlatent tree induction as an approach for unsupervised syntactic parsing.\nNonetheless, modern systems still do not perform well enough compared to their\nsupervised counterparts to have any practical use as structural annotation of\ntext. In this work, we present a technique that uses distant supervision in the\nform of span constraints (i.e. phrase bracketing) to improve performance in\nunsupervised constituency parsing. Using a relatively small number of span\nconstraints we can substantially improve the output from DIORA, an already\ncompetitive unsupervised parsing system. Compared with full parse tree\nannotation, span constraints can be acquired with minimal effort, such as with\na lexicon derived from Wikipedia, to find exact text matches. Our experiments\nshow span constraints based on entities improves constituency parsing on\nEnglish WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to\nany domain where span constraints are easily attainable, and as a case study we\ndemonstrate its effectiveness by parsing biomedical text from the CRAFT\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdov_A/0/1/0/all/0/1\">Andrew Drozdov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jay Yoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rongali_S/0/1/0/all/0/1\">Subendhu Rongali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkbeiner_D/0/1/0/all/0/1\">Dylan Finkbeiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1\">Shilpa Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection. (arXiv:2109.08113v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08113","description":"<p>Much of natural language processing is focused on leveraging large capacity\nlanguage models, typically trained over single messages with a task of\npredicting one or more tokens. However, modeling human language at\nhigher-levels of context (i.e., sequences of messages) is under-explored. In\nstance detection and other social media tasks where the goal is to predict an\nattribute of a message, we have contextual data that is loosely semantically\nconnected by authorship. Here, we introduce Message-Level Transformer (MeLT) --\na hierarchical message-encoder pre-trained over Twitter and applied to the task\nof stance prediction. We focus on stance prediction as a task benefiting from\nknowing the context of the message (i.e., the sequence of previous messages).\nThe model is trained using a variant of masked-language modeling; where instead\nof predicting tokens, it seeks to generate an entire masked (aggregated)\nmessage vector via reconstruction loss. We find that applying this pre-trained\nmasked message-level transformer to the downstream task of stance detection\nachieves F1 performance of 67%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matero_M/0/1/0/all/0/1\">Matthew Matero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_N/0/1/0/all/0/1\">Nikita Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting semantic lexicons using word embeddings and transfer learning. (arXiv:2109.09010v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09010","description":"<p>Sentiment-aware intelligent systems are essential to a wide array of\napplications. These systems are driven by language models which broadly fall\ninto two paradigms: Lexicon-based and contextual. Although recent contextual\nmodels are increasingly dominant, we still see demand for lexicon-based models\nbecause of their interpretability and ease of use. For example, lexicon-based\nmodels allow researchers to readily determine which words and phrases\ncontribute most to a change in measured sentiment. A challenge for any\nlexicon-based approach is that the lexicon needs to be routinely expanded with\nnew words and expressions. Here, we propose two models for automatic lexicon\nexpansion. Our first model establishes a baseline employing a simple and\nshallow neural network initialized with pre-trained word embeddings using a\nnon-contextual approach. Our second model improves upon our baseline, featuring\na deep Transformer-based network that brings to bear word definitions to\nestimate their lexical polarity. Our evaluation shows that both models are able\nto score new words with a similar accuracy to reviewers from Amazon Mechanical\nTurk, but at a fraction of the cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oort_C/0/1/0/all/0/1\">Colin M. Van Oort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">Mikaela Irene Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Impact of Pre-trained Language Models on Dialog Evaluation. (arXiv:2110.01895v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01895","description":"<p>Recently, there is a surge of interest in applying pre-trained language\nmodels (Pr-LM) in automatic open-domain dialog evaluation. Pr-LMs offer a\npromising direction for addressing the multi-domain evaluation challenge. Yet,\nthe impact of different Pr-LMs on the performance of automatic metrics is not\nwell-understood. This paper examines 8 different Pr-LMs and studies their\nimpact on three typical automatic dialog evaluation metrics across three\ndifferent dialog evaluation benchmarks. Specifically, we analyze how the choice\nof Pr-LMs affects the performance of automatic metrics. Extensive correlation\nanalyses on each of the metrics are performed to assess the effects of\ndifferent Pr-LMs along various axes, including pre-training objectives, dialog\nevaluation criteria, model size, and cross-dataset robustness. This study\nserves as the first comprehensive assessment of the effects of different Pr-LMs\non automatic dialog evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrichs_T/0/1/0/all/0/1\">Thomas Friedrichs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Concept Map Generation through Task-Guided Graph Translation. (arXiv:2110.15720v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15720","description":"<p>Recent years have witnessed the rapid development of concept map generation\ntechniques due to their advantages in providing well-structured summarization\nof knowledge from free texts. Traditional unsupervised methods do not generate\ntask-oriented concept maps, whereas deep generative models require large\namounts of training data. In this work, we present GT-D2G (Graph Translation\nbased Document-To-Graph), an automatic concept map generation framework that\nleverages generalized NLP pipelines to derive semantic-rich initial graphs, and\ntranslates them into more concise structures under the weak supervision of\ndocument labels. The quality and interpretability of such concept maps are\nvalidated through human evaluation on three real-world corpora, and their\nutility in the downstream task is further demonstrated in the controlled\nexperiments with scarce document labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangjue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-11-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Comparing Bayesian Models for Organ Contouring in Headand Neck Radiotherapy. (arXiv:2111.01134v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01134","description":"<p>Deep learning models for organ contouring in radiotherapy are poised for\nclinical usage, but currently, there exist few tools for automated quality\nassessment (QA) of the predicted contours. Using Bayesian models and their\nassociated uncertainty, one can potentially automate the process of detecting\ninaccurate predictions. We investigate two Bayesian models for auto-contouring,\nDropOut and FlipOut, using a quantitative measure - expected calibration error\n(ECE) and a qualitative measure - region-based accuracy-vs-uncertainty (R-AvU)\ngraphs. It is well understood that a model should have low ECE to be considered\ntrustworthy. However, in a QA context, a model should also have high\nuncertainty in inaccurate regions and low uncertainty in accurate regions. Such\nbehaviour could direct visual attention of expert users to potentially\ninaccurate regions, leading to a speed up in the QA process. Using R-AvU\ngraphs, we qualitatively compare the behaviour of different models in accurate\nand inaccurate regions. Experiments are conducted on the MICCAI2015 Head and\nNeck Segmentation Challenge and on the DeepMindTCIA CT dataset using three\nmodels: DropOut-DICE, Dropout-CE (Cross Entropy) and FlipOut-CE. Quantitative\nresults show that DropOut-DICE has the highest ECE, while Dropout-CE and\nFlipOut-CE have the lowest ECE. To better understand the difference between\nDropOut-CE and FlipOut-CE, we use the R-AvU graph which shows that FlipOut-CE\nhas better uncertainty coverage in inaccurate regions than DropOut-CE. Such a\ncombination of quantitative and qualitative metrics explores a new approach\nthat helps to select which model can be deployed as a QA tool in clinical\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mody_P/0/1/0/all/0/1\">Prerak Mody</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaves_de_Plaza_N/0/1/0/all/0/1\">Nicolas Chaves-de-Plaza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hildebrandt_K/0/1/0/all/0/1\">Klaus Hildebrandt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egmond_R/0/1/0/all/0/1\">Rene van Egmond</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ridder_H/0/1/0/all/0/1\">Huib de Ridder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Staring_M/0/1/0/all/0/1\">Marius Staring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arch-Net: Model Distillation for Architecture Agnostic Model Deployment. (arXiv:2111.01135v1 [cs.LG])","link":"http://arxiv.org/abs/2111.01135","description":"<p>Vast requirement of computation power of Deep Neural Networks is a major\nhurdle to their real world applications. Many recent Application Specific\nIntegrated Circuit (ASIC) chips feature dedicated hardware support for Neural\nNetwork Acceleration. However, as ASICs take multiple years to develop, they\nare inevitably out-paced by the latest development in Neural Architecture\nResearch. For example, Transformer Networks do not have native support on many\npopular chips, and hence are difficult to deploy. In this paper, we propose\nArch-Net, a family of Neural Networks made up of only operators efficiently\nsupported across most architectures of ASICs. When a Arch-Net is produced, less\ncommon network constructs, like Layer Normalization and Embedding Layers, are\neliminated in a progressive manner through label-free Blockwise Model\nDistillation, while performing sub-eight bit quantization at the same time to\nmaximize performance. Empirical results on machine translation and image\nclassification tasks confirm that we can transform latest developed Neural\nArchitectures into fast running and as-accurate Arch-Net, ready for deployment\non multiple mass-produced ASIC chips. The code will be available at\nhttps://github.com/megvii-research/Arch-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weixin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zipeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shuangkang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Song Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Frequency Modulation for Visually Explaining Video Understanding Models. (arXiv:2111.01215v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01215","description":"<p>In many applications, it is essential to understand why a machine learning\nmodel makes the decisions it does, but this is inhibited by the black-box\nnature of state-of-the-art neural networks. Because of this, increasing\nattention has been paid to explainability in deep learning, including in the\narea of video understanding. Due to the temporal dimension of video data, the\nmain challenge of explaining a video action recognition model is to produce\nspatiotemporally consistent visual explanations, which has been ignored in the\nexisting literature. In this paper, we propose Frequency-based Extremal\nPerturbation (F-EP) to explain a video understanding model's decisions. Because\nthe explanations given by perturbation methods are noisy and non-smooth both\nspatially and temporally, we propose to modulate the frequencies of gradient\nmaps from the neural network model with a Discrete Cosine Transform (DCT). We\nshow in a range of experiments that F-EP provides more spatiotemporally\nconsistent explanations that more faithfully represent the model's decisions\ncompared to the existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xinmiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wentao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_M/0/1/0/all/0/1\">Matthew Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRViT: Multi-Scale High-Resolution Vision Transformer. (arXiv:2111.01236v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01236","description":"<p>Vision transformers (ViTs) have attracted much attention for their superior\nperformance on computer vision tasks. To address their limitations of\nsingle-scale low-resolution representations, prior work adapts ViTs to\nhigh-resolution dense prediction tasks with hierarchical architectures to\ngenerate pyramid features. However, multi-scale representation learning is\nstill under-explored on ViTs, given their classification-like sequential\ntopology. To enhance ViTs with more capability to learn semantically-rich and\nspatially-precise multi-scale representations, in this work, we present an\nefficient integration of high-resolution multi-branch architectures with vision\ntransformers, dubbed HRViT, pushing the Pareto front of dense prediction tasks\nto a new level. We explore heterogeneous branch design, reduce the redundancy\nin linear layers, and augment the model nonlinearity to balance the model\nperformance and hardware efficiency. The proposed HRViT achieves 50.20% mIoU on\nADE20K and 83.16% mIoU on Cityscapes for semantic segmentation tasks,\nsurpassing state-of-the-art MiT and CSWin with an average of +1.78 mIoU\nimprovement, 28% parameter reduction, and 21% FLOPs reduction, demonstrating\nthe potential of HRViT as strong vision backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Hyoukjun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Hsin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Liangzhen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Eye-in-Hand Camera Calibration from a Single Image. (arXiv:2111.01245v1 [cs.RO])","link":"http://arxiv.org/abs/2111.01245","description":"<p>Eye-in-hand camera calibration is a fundamental and long-studied problem in\nrobotics. We present a study on using learning-based methods for solving this\nproblem online from a single RGB image, whilst training our models with\nentirely synthetic data. We study three main approaches: one direct regression\nmodel that directly predicts the extrinsic matrix from an image, one sparse\ncorrespondence model that regresses 2D keypoints and then uses PnP, and one\ndense correspondence model that uses regressed depth and segmentation maps to\nenable ICP pose estimation. In our experiments, we benchmark these methods\nagainst each other and against well-established classical methods, to find the\nsurprising result that direct regression outperforms other approaches, and we\nperform noise-sensitivity analysis to gain further insights into these results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valassakis_E/0/1/0/all/0/1\">Eugene Valassakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreczkowski_K/0/1/0/all/0/1\">Kamil Dreczkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Scene Flow Prior. (arXiv:2111.01253v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01253","description":"<p>Before the deep learning revolution, many perception algorithms were based on\nruntime optimization in conjunction with a strong prior/regularization penalty.\nA prime example of this in computer vision is optical and scene flow.\nSupervised learning has largely displaced the need for explicit regularization.\nInstead, they rely on large amounts of labeled data to capture prior\nstatistics, which are not always readily available for many problems. Although\noptimization is employed to learn the neural network, the weights of this\nnetwork are frozen at runtime. As a result, these learning solutions are\ndomain-specific and do not generalize well to other statistically different\nscenarios. This paper revisits the scene flow problem that relies predominantly\non runtime optimization and strong regularization. A central innovation here is\nthe inclusion of a neural scene flow prior, which uses the architecture of\nneural networks as a new type of implicit regularizer. Unlike learning-based\nscene flow methods, optimization occurs at runtime, and our approach needs no\noffline datasets -- making it ideal for deployment in new environments such as\nautonomous driving. We show that an architecture based exclusively on\nmultilayer perceptrons (MLPs) can be used as a scene flow prior. Our method\nattains competitive -- if not better -- results on scene flow benchmarks. Also,\nour neural prior's implicit and continuous scene flow representation allows us\nto estimate dense long-term correspondences across a sequence of point clouds.\nThe dense motion information is represented by scene flow fields where points\ncan be propagated through time by integrating motion vectors. We demonstrate\nsuch a capability by accumulating a sequence of lidar point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xueqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pontes_J/0/1/0/all/0/1\">Jhony Kaesemodel Pontes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Detection of Motion Boundaries and Occlusions. (arXiv:2111.01261v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01261","description":"<p>We propose MONet, a convolutional neural network that jointly detects motion\nboundaries (MBs) and occlusion regions (Occs) in video both forward and\nbackward in time. Detection is difficult because optical flow is discontinuous\nalong MBs and undefined in Occs, while many flow estimators assume smoothness\nand a flow defined everywhere. To reason in the two time directions\nsimultaneously, we direct-warp the estimated maps between the two frames. Since\nappearance mismatches between frames often signal vicinity to MBs or Occs, we\nconstruct a cost block that for each feature in one frame records the lowest\ndiscrepancy with matching features in a search range. This cost block is\ntwo-dimensional, and much less expensive than the four-dimensional cost volumes\nused in flow analysis. Cost-block features are computed by an encoder, and MB\nand Occ estimates are computed by a decoder. We found that arranging decoder\nlayers fine-to-coarse, rather than coarse-to-fine, improves performance. MONet\noutperforms the prior state of the art for both tasks on the Sintel and\nFlyingChairsOcc benchmarks without any fine-tuning on them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hannah Halin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shuzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasi_C/0/1/0/all/0/1\">Carlo Tomasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masking Modalities for Cross-modal Video Retrieval. (arXiv:2111.01300v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01300","description":"<p>Pre-training on large scale unlabelled datasets has shown impressive\nperformance improvements in the fields of computer vision and natural language\nprocessing. Given the advent of large-scale instructional video datasets, a\ncommon strategy for pre-training video encoders is to use the accompanying\nspeech as weak supervision. However, as speech is used to supervise the\npre-training, it is never seen by the video encoder, which does not learn to\nprocess that modality. We address this drawback of current pre-training\nmethods, which fail to exploit the rich cues in spoken language. Our proposal\nis to pre-train a video encoder using all the available video modalities as\nsupervision, namely, appearance, sound, and transcribed speech. We mask an\nentire modality in the input and predict it using the other two modalities.\nThis encourages each modality to collaborate with the others, and our video\nencoder learns to process appearance and audio as well as speech. We show the\nsuperior performance of our \"modality masking\" pre-training approach for video\nretrieval on the How2R, YouCook2 and Condensed Movies datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabeur_V/0/1/0/all/0/1\">Valentin Gabeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Semi-supervised Video Object Segmentation Problem from a Cyclic Perspective. (arXiv:2111.01323v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01323","description":"<p>Modern video object segmentation (VOS) algorithms have achieved remarkably\nhigh performance in a sequential processing order, while most of currently\nprevailing pipelines still show some obvious inadequacy like accumulative\nerror, unknown robustness or lack of proper interpretation tools. In this\npaper, we place the semi-supervised video object segmentation problem into a\ncyclic workflow and find the defects above can be collectively addressed via\nthe inherent cyclic property of semi-supervised VOS systems. Firstly, a cyclic\nmechanism incorporated to the standard sequential flow can produce more\nconsistent representations for pixel-wise correspondance. Relying on the\naccurate reference mask in the starting frame, we show that the error\npropagation problem can be mitigated. Next, a simple gradient correction\nmodule, which naturally extends the offline cyclic pipeline to an online\nmanner, can highlight the high-frequent and detailed part of results to further\nimprove the segmentation quality while keeping feasible computation cost.\nMeanwhile such correction can protect the network from severe performance\ndegration resulted from interference signals. Finally we develop cycle\neffective receptive field (cycle-ERF) based on gradient correction process to\nprovide a new perspective into analyzing object-specific regions of interests.\nWe conduct comprehensive comparison and detailed analysis on challenging\nbenchmarks of DAVIS16, DAVIS17 and Youtube-VOS, demonstrating that the cyclic\nmechanism is helpful to enhance segmentation quality, improve the robustness of\nVOS systems, and further provide qualitative comparison and interpretation on\nhow different VOS algorithms work. The code of this project can be found at\nhttps://github.com/lyxok1/STM-Training\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenjie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute-Based Deep Periocular Recognition: Leveraging Soft Biometrics to Improve Periocular Recognition. (arXiv:2111.01325v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01325","description":"<p>In recent years, periocular recognition has been developed as a valuable\nbiometric identification approach, especially in wild environments (for\nexample, masked faces due to COVID-19 pandemic) where facial recognition may\nnot be applicable. This paper presents a new deep periocular recognition\nframework called attribute-based deep periocular recognition (ADPR), which\npredicts soft biometrics and incorporates the prediction into a periocular\nrecognition algorithm to determine identity from periocular images with high\naccuracy. We propose an end-to-end framework, which uses several shared\nconvolutional neural network (CNN)layers (a common network) whose output feeds\ntwo separate dedicated branches (modality dedicated layers); the first branch\nclassifies periocular images while the second branch predicts softn biometrics.\nNext, the features from these two branches are fused together for a final\nperiocular recognition. The proposed method is different from existing methods\nas it not only uses a shared CNN feature space to train these two tasks\njointly, but it also fuses predicted soft biometric features with the\nperiocular features in the training step to improve the overall periocular\nrecognition performance. Our proposed model is extensively evaluated using four\ndifferent publicly available datasets. Experimental results indicate that our\nsoft biometric based periocular recognition approach outperforms other\nstate-of-the-art methods for periocular recognition in wild environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talreja_V/0/1/0/all/0/1\">Veeru Talreja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenti_M/0/1/0/all/0/1\">Matthew C. Valenti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Split Vision Transformer for COVID-19CXR Diagnosis using Task-Agnostic Training. (arXiv:2111.01338v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01338","description":"<p>Federated learning, which shares the weights of the neural network across\nclients, is gaining attention in the healthcare sector as it enables training\non a large corpus of decentralized data while maintaining data privacy. For\nexample, this enables neural network training for COVID-19 diagnosis on chest\nX-ray (CXR) images without collecting patient CXR data across multiple\nhospitals. Unfortunately, the exchange of the weights quickly consumes the\nnetwork bandwidth if highly expressive network architecture is employed.\nSo-called split learning partially solves this problem by dividing a neural\nnetwork into a client and a server part, so that the client part of the network\ntakes up less extensive computation resources and bandwidth. However, it is not\nclear how to find the optimal split without sacrificing the overall network\nperformance. To amalgamate these methods and thereby maximize their distinct\nstrengths, here we show that the Vision Transformer, a recently developed deep\nlearning architecture with straightforward decomposable configuration, is\nideally suitable for split learning without sacrificing performance. Even under\nthe non-independent and identically distributed data distribution which\nemulates a real collaboration between hospitals using CXR datasets from\nmultiple sources, the proposed framework was able to attain performance\ncomparable to data-centralized training. In addition, the proposed framework\nalong with heterogeneous multi-task clients also improves individual task\nperformances including the diagnosis of COVID-19, eliminating the need for\nsharing large weights with innumerable parameters. Our results affirm the\nsuitability of Transformer for collaborative learning in medical imaging and\npave the way forward for future real-world implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sangjoon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1\">Gwanghyun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jeongsol Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_B/0/1/0/all/0/1\">Boah Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing High-Order Signed Distance Maps from Computed Tomography Data with Application to Bone Morphometry. (arXiv:2111.01350v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01350","description":"<p>An algorithm is presented for constructing high-order signed distance fields\nfor two phase materials imaged with computed tomography. The signed distance\nfield is high-order in that it is free of the quantization artifact associated\nwith the distance transform of sampled signals. The narrowband is solved using\na closest point algorithm extended for implicit embeddings that are not a\nsigned distance field. The high-order fast sweeping algorithm is used to extend\nthe narrowband to the remainder of the domain. The order of accuracy of the\nnarrowband and extension methods are verified on ideal implicit surfaces. The\nmethod is applied to ten excised cubes of bovine trabecular bone. Localization\nof the surface, estimation of phase densities, and local morphometry is\nvalidated with these subjects. Since the embedding is high-order, gradients and\nthus curvatures can be accurately estimated locally in the image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Besler_B/0/1/0/all/0/1\">Bryce A. Besler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kemp_T/0/1/0/all/0/1\">Tannis D. Kemp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Forkert_N/0/1/0/all/0/1\">Nils D. Forkert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyd_S/0/1/0/all/0/1\">Steven K. Boyd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Vision Transformers Perform Convolution?. (arXiv:2111.01353v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01353","description":"<p>Several recent studies have demonstrated that attention-based networks, such\nas Vision Transformer (ViT), can outperform Convolutional Neural Networks\n(CNNs) on several computer vision tasks without using convolutional layers.\nThis naturally leads to the following questions: Can a self-attention layer of\nViT express any convolution operation? In this work, we prove that a single ViT\nlayer with image patches as the input can perform any convolution operation\nconstructively, where the multi-head attention mechanism and the relative\npositional encoding play essential roles. We further provide a lower bound on\nthe number of heads for Vision Transformers to express CNNs. Corresponding with\nour analysis, experimental results show that the construction in our proof can\nhelp inject convolutional bias into Transformers and significantly improve the\nperformance of ViT in low data regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary Distribution Estimation to Precise Object Detection. (arXiv:2111.01396v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01396","description":"<p>In principal modern detectors, the task of object localization is implemented\nby the box subnet which concentrates on bounding box regression. The box subnet\ncustomarily predicts the position of the object by regressing box center\nposition and scaling factors. Although this approach is frequently adopted, we\nobserve that the result of localization remains defective, which makes the\nperformance of the detector unsatisfactory. In this paper, we prove the flaws\nin the previous method through theoretical analysis and experimental\nverification and propose a novel solution to detect objects precisely. Rather\nthan plainly focusing on center and size, our approach refines the edges of the\nbounding box on previous localization results by estimating the distribution at\nthe boundary of the object. Experimental results have shown the potentiality\nand generalization of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingguo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pixel-Level Meta-Learner for Weakly Supervised Few-Shot Semantic Segmentation. (arXiv:2111.01418v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01418","description":"<p>Few-shot semantic segmentation addresses the learning task in which only few\nimages with ground truth pixel-level labels are available for the novel classes\nof interest. One is typically required to collect a large mount of data (i.e.,\nbase classes) with such ground truth information, followed by meta-learning\nstrategies to address the above learning task. When only image-level semantic\nlabels can be observed during both training and testing, it is considered as an\neven more challenging task of weakly supervised few-shot semantic segmentation.\nTo address this problem, we propose a novel meta-learning framework, which\npredicts pseudo pixel-level segmentation masks from a limited amount of data\nand their semantic labels. More importantly, our learning scheme further\nexploits the produced pixel-level information for query image inputs with\nsegmentation guarantees. Thus, our proposed learning model can be viewed as a\npixel-level meta-learner. Through extensive experiments on benchmark datasets,\nwe show that our model achieves satisfactory performances under fully\nsupervised settings, yet performs favorably against state-of-the-art methods\nunder weakly supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yuan-Hao Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fu-En Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Chiang Frank Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HHP-Net: A light Heteroscedastic neural network for Head Pose estimation with uncertainty. (arXiv:2111.01440v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01440","description":"<p>In this paper we introduce a novel method to estimate the head pose of people\nin single images starting from a small set of head keypoints. To this purpose,\nwe propose a regression model that exploits keypoints computed automatically by\n2D pose estimation algorithms and outputs the head pose represented by yaw,\npitch, and roll. Our model is simple to implement and more efficient with\nrespect to the state of the art -- faster in inference and smaller in terms of\nmemory occupancy -- with comparable accuracy. Our method also provides a\nmeasure of the heteroscedastic uncertainties associated with the three angles,\nthrough an appropriately designed loss function; we show there is a correlation\nbetween error and uncertainty values, thus this extra source of information may\nbe used in subsequent computational steps. As an example application, we\naddress social interaction analysis in images: we propose an algorithm for a\nquantitative estimation of the level of interaction between people, starting\nfrom their head poses and reasoning on their mutual positions. The code is\navailable at https://github.com/cantarinigiorgio/HHP-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cantarini_G/0/1/0/all/0/1\">Giorgio Cantarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomenotti_F/0/1/0/all/0/1\">Federico Figari Tomenotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noceti_N/0/1/0/all/0/1\">Nicoletta Noceti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odone_F/0/1/0/all/0/1\">Francesca Odone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out of distribution detection for skin and malaria images. (arXiv:2111.01505v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01505","description":"<p>Deep neural networks have shown promising results in disease detection and\nclassification using medical image data. However, they still suffer from the\nchallenges of handling real-world scenarios especially reliably detecting\nout-of-distribution (OoD) samples. We propose an approach to robustly classify\nOoD samples in skin and malaria images without the need to access labeled OoD\nsamples during training. Specifically, we use metric learning along with\nlogistic regression to force the deep networks to learn much rich class\nrepresentative features. To guide the learning process against the OoD\nexamples, we generate ID similar-looking examples by either removing\nclass-specific salient regions in the image or permuting image parts and\ndistancing them away from in-distribution samples. During inference time, the\nK-reciprocal nearest neighbor is employed to detect out-of-distribution\nsamples. For skin cancer OoD detection, we employ two standard benchmark skin\ncancer ISIC datasets as ID, and six different datasets with varying difficulty\nlevels were taken as out of distribution. For malaria OoD detection, we use the\nBBBC041 malaria dataset as ID and five different challenging datasets as out of\ndistribution. We achieved state-of-the-art results, improving 5% and 4% in\nTNR@TPR95% over the previous state-of-the-art for skin cancer and malaria OoD\ndetection respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zaida_M/0/1/0/all/0/1\">Muhammad Zaida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Shafaqat Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussein_S/0/1/0/all/0/1\">Sarfaraz Hussein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saadia_A/0/1/0/all/0/1\">Asma Saadia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sultani_W/0/1/0/all/0/1\">Waqas Sultani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISP-Agnostic Image Reconstruction for Under-Display Cameras. (arXiv:2111.01511v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01511","description":"<p>Under-display cameras have been proposed in recent years as a way to reduce\nthe form factor of mobile devices while maximizing the screen area.\nUnfortunately, placing the camera behind the screen results in significant\nimage distortions, including loss of contrast, blur, noise, color shift,\nscattering artifacts, and reduced light sensitivity. In this paper, we propose\nan image-restoration pipeline that is ISP-agnostic, i.e. it can be combined\nwith any legacy ISP to produce a final image that matches the appearance of\nregular cameras using the same ISP. This is achieved with a deep learning\napproach that performs a RAW-to-RAW image restoration. To obtain large\nquantities of real under-display camera training data with sufficient contrast\nand scene diversity, we furthermore develop a data capture method utilizing an\nHDR monitor, as well as a data augmentation method to generate suitable HDR\ncontent. The monitor data is supplemented with real-world data that has less\nscene diversity but allows us to achieve fine detail recovery without being\nlimited by the monitor resolution. Together, this approach successfully\nrestores color and contrast as well as image detail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qi_M/0/1/0/all/0/1\">Miao Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive and Clinically Accurate Head and Neck Organs at Risk Delineation via Stratified Deep Learning: A Large-scale Multi-Institutional Study. (arXiv:2111.01544v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01544","description":"<p>Accurate organ at risk (OAR) segmentation is critical to reduce the\nradiotherapy post-treatment complications. Consensus guidelines recommend a set\nof more than 40 OARs in the head and neck (H&amp;N) region, however, due to the\npredictable prohibitive labor-cost of this task, most institutions choose a\nsubstantially simplified protocol by delineating a smaller subset of OARs and\nneglecting the dose distributions associated with other OARs. In this work we\npropose a novel, automated and highly effective stratified OAR segmentation\n(SOARS) system using deep learning to precisely delineate a comprehensive set\nof 42 H&amp;N OARs. SOARS stratifies 42 OARs into anchor, mid-level, and small &amp;\nhard subcategories, with specifically derived neural network architectures for\neach category by neural architecture search (NAS) principles. We built SOARS\nmodels using 176 training patients in an internal institution and independently\nevaluated on 1327 external patients across six different institutions. It\nconsistently outperformed other state-of-the-art methods by at least 3-5% in\nDice score for each institutional evaluation (up to 36% relative error\nreduction in other metrics). More importantly, extensive multi-user studies\nevidently demonstrated that 98% of the SOARS predictions need only very minor\nor no revisions for direct clinical acceptance (saving 90% radiation\noncologists workload), and their segmentation and dosimetric accuracy are\nwithin or smaller than the inter-user variation. These findings confirmed the\nstrong clinical applicability of SOARS for the OAR delineation process in H&amp;N\ncancer radiotherapy workflows, with improved efficiency, comprehensiveness, and\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1\">Dazhou Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_J/0/1/0/all/0/1\">Jia Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_X/0/1/0/all/0/1\">Xianghua Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_S/0/1/0/all/0/1\">Senxiang Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xin_Y/0/1/0/all/0/1\">Yi Xin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yuchen Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_B/0/1/0/all/0/1\">Bing-shen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hung_T/0/1/0/all/0/1\">Tsung-Min Hung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhuotun Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Ling Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yanping Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Gong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_M/0/1/0/all/0/1\">Mengyuan Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohua Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zhongjie Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenxiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Lingyun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harrison_A/0/1/0/all/0/1\">Adam P. Harrison</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1\">Chien-Yu Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_D/0/1/0/all/0/1\">Dakai Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_T/0/1/0/all/0/1\">Tsung-Ying Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima. (arXiv:2111.01549v1 [cs.LG])","link":"http://arxiv.org/abs/2111.01549","description":"<p>This paper considers incremental few-shot learning, which requires a model to\ncontinually recognize new categories with only a few examples provided. Our\nstudy shows that existing methods severely suffer from catastrophic forgetting,\na well-known problem in incremental learning, which is aggravated due to data\nscarcity and imbalance in the few-shot setting. Our analysis further suggests\nthat to prevent catastrophic forgetting, actions need to be taken in the\nprimitive stage -- the training of base classes instead of later few-shot\nlearning sessions. Therefore, we propose to search for flat local minima of the\nbase training objective function and then fine-tune the model parameters within\nthe flat region on new tasks. In this way, the model can efficiently learn new\nclasses while preserving the old ones. Comprehensive experimental results\ndemonstrate that our approach outperforms all prior state-of-the-art methods\nand is very close to the approximate upper bound. The source code is available\nat https://github.com/moukamisama/F2M.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guangyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Li-Ming Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accounting for Dependencies in Deep Learning Based Multiple Instance Learning for Whole Slide Imaging. (arXiv:2111.01556v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01556","description":"<p>Multiple instance learning (MIL) is a key algorithm for classification of\nwhole slide images (WSI). Histology WSIs can have billions of pixels, which\ncreate enormous computational and annotation challenges. Typically, such images\nare divided into a set of patches (a bag of instances), where only bag-level\nclass labels are provided. Deep learning based MIL methods calculate instance\nfeatures using convolutional neural network (CNN). Our proposed approach is\nalso deep learning based, with the following two contributions: Firstly, we\npropose to explicitly account for dependencies between instances during\ntraining by embedding self-attention Transformer blocks to capture dependencies\nbetween instances. For example, a tumor grade may depend on the presence of\nseveral particular patterns at different locations in WSI, which requires to\naccount for dependencies between patches. Secondly, we propose an instance-wise\nloss function based on instance pseudo-labels. We compare the proposed\nalgorithm to multiple baseline methods, evaluate it on the PANDA challenge\ndataset, the largest publicly available WSI dataset with over 11K images, and\ndemonstrate state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1\">Andriy Myronenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointNu-Net: Simultaneous Multi-tissue Histology Nuclei Segmentation and Classification in the Clinical Wild. (arXiv:2111.01557v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01557","description":"<p>Automatic nuclei segmentation and classification plays a vital role in\ndigital pathology. However, previous works are mostly built on data with\nlimited diversity and small sizes, making the results questionable or\nmisleading in actual downstream tasks. In this paper, we aim to build a\nreliable and robust method capable of dealing with data from the 'the clinical\nwild'. Specifically, we study and design a new method to simultaneously detect,\nsegment, and classify nuclei from Haematoxylin and Eosin (H&amp;E) stained\nhistopathology data, and evaluate our approach using the recent largest\ndataset: PanNuke. We address the detection and classification of each nuclei as\na novel semantic keypoint estimation problem to determine the center point of\neach nuclei. Next, the corresponding class-agnostic masks for nuclei center\npoints are obtained using dynamic instance segmentation. By decoupling two\nsimultaneous challenging tasks, our method can benefit from class-aware\ndetection and class-agnostic segmentation, thus leading to a significant\nperformance boost. We demonstrate the superior performance of our proposed\napproach for nuclei segmentation and classification across 19 different tissue\ntypes, delivering new benchmark results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jie Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_A/0/1/0/all/0/1\">Amir Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jude_C/0/1/0/all/0/1\">Curran Jude</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-cortical structure segmentation database for young population. (arXiv:2111.01561v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01561","description":"<p>Segmentation of sub-cortical structures from MRI scans is of interest in many\nneurological diagnosis. Since this is a laborious task machine learning and\nspecifically deep learning (DL) methods have become explored. The structural\ncomplexity of the brain demands a large, high quality segmentation dataset to\ndevelop good DL-based solutions for sub-cortical structure segmentation.\nTowards this, we are releasing a set of 114, 1.5 Tesla, T1 MRI scans with\nmanual delineations for 14 sub-cortical structures. The scans in the dataset\nwere acquired from healthy young (21-30 years) subjects ( 58 male and 56\nfemale) and all the structures are manually delineated by experienced radiology\nexperts. Segmentation experiments have been conducted with this dataset and\nresults demonstrate that accurate results can be obtained with deep-learning\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sivaswamy_J/0/1/0/all/0/1\">Jayanthi Sivaswamy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thottupattu_A/0/1/0/all/0/1\">Alphin J Thottupattu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+V_M/0/1/0/all/0/1\">Mythri V</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1\">Raghav Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheelakumari_R/0/1/0/all/0/1\">R Sheelakumari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kesavadas_C/0/1/0/all/0/1\">Chandrasekharan Kesavadas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fitness Landscape Footprint: A Framework to Compare Neural Architecture Search Problems. (arXiv:2111.01584v1 [cs.LG])","link":"http://arxiv.org/abs/2111.01584","description":"<p>Neural architecture search is a promising area of research dedicated to\nautomating the design of neural network models. This field is rapidly growing,\nwith a surge of methodologies ranging from Bayesian optimization,neuroevoltion,\nto differentiable search, and applications in various contexts. However,\ndespite all great advances, few studies have presented insights on the\ndifficulty of the problem itself, thus the success (or fail) of these\nmethodologies remains unexplained. In this sense, the field of optimization has\ndeveloped methods that highlight key aspects to describe optimization problems.\nThe fitness landscape analysis stands out when it comes to characterize\nreliably and quantitatively search algorithms. In this paper, we propose to use\nfitness landscape analysis to study a neural architecture search problem.\nParticularly, we introduce the fitness landscape footprint, an aggregation of\neight (8)general-purpose metrics to synthesize the landscape of an architecture\nsearch problem. We studied two problems, the classical image classification\nbenchmark CIFAR-10, and the Remote-Sensing problem So2Sat LCZ42. The results\npresent a quantitative appraisal of the problems, allowing to characterize the\nrelative difficulty and other characteristics, such as the ruggedness or the\npersistence, that helps to tailor a search strategy to the problem. Also, the\nfootprint is a tool that enables the comparison of multiple problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Traore_K/0/1/0/all/0/1\">Kalifou Ren&#xe9; Traor&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camero_A/0/1/0/all/0/1\">Andr&#xe9;s Camero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect-and-Segment: a Deep Learning Approach to Automate Wound Image Segmentation. (arXiv:2111.01590v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01590","description":"<p>Chronic wounds significantly impact quality of life. If not properly managed,\nthey can severely deteriorate. Image-based wound analysis could aid in\nobjectively assessing the wound status by quantifying important features that\nare related to healing. However, the high heterogeneity of the wound types,\nimage background composition, and capturing conditions challenge the robust\nsegmentation of wound images. We present Detect-and-Segment (DS), a deep\nlearning approach to produce wound segmentation maps with high generalization\ncapabilities. In our approach, dedicated deep neural networks detected the\nwound position, isolated the wound from the uninformative background, and\ncomputed the wound segmentation map. We evaluated this approach using one data\nset with images of diabetic foot ulcers. For further testing, 4 supplemental\nindependent data sets with larger variety of wound types from different body\nlocations were used. The Matthews' correlation coefficient (MCC) improved from\n0.29 when computing the segmentation on the full image to 0.85 when combining\ndetection and segmentation in the same approach. When tested on the wound\nimages drawn from the supplemental data sets, the DS approach increased the\nmean MCC from 0.17 to 0.85. Furthermore, the DS approach enabled the training\nof segmentation models with up to 90% less training data while maintaining the\nsegmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scebba_G/0/1/0/all/0/1\">Gaetano Scebba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_S/0/1/0/all/0/1\">Sabrina Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihai_C/0/1/0/all/0/1\">Carina Mihai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Distler_O/0/1/0/all/0/1\">Oliver Distler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berli_M/0/1/0/all/0/1\">Martin Berli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlen_W/0/1/0/all/0/1\">Walter Karlen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating 3D Motion and Forces of Human-Object Interactions from Internet Videos. (arXiv:2111.01591v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01591","description":"<p>In this paper, we introduce a method to automatically reconstruct the 3D\nmotion of a person interacting with an object from a single RGB video. Our\nmethod estimates the 3D poses of the person together with the object pose, the\ncontact positions and the contact forces exerted on the human body. The main\ncontributions of this work are three-fold. First, we introduce an approach to\njointly estimate the motion and the actuation forces of the person on the\nmanipulated object by modeling contacts and the dynamics of the interactions.\nThis is cast as a large-scale trajectory optimization problem. Second, we\ndevelop a method to automatically recognize from the input video the 2D\nposition and timing of contacts between the person and the object or the\nground, thereby significantly simplifying the complexity of the optimization.\nThird, we validate our approach on a recent video+MoCap dataset capturing\ntypical parkour actions, and demonstrate its performance on a new dataset of\nInternet videos showing people manipulating a variety of tools in unconstrained\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongmian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1\">Jiri Sedlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpentier_J/0/1/0/all/0/1\">Justin Carpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansard_N/0/1/0/all/0/1\">Nicolas Mansard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory Prediction with Graph-based Dual-scale Context Fusion. (arXiv:2111.01592v1 [cs.RO])","link":"http://arxiv.org/abs/2111.01592","description":"<p>Motion prediction for traffic participants is essential for a safe and robust\nautomated driving system, especially in cluttered urban environments. However,\nit is highly challenging due to the complex road topology as well as the\nuncertain intentions of the other agents. In this paper, we present a\ngraph-based trajectory prediction network named the Dual Scale Predictor (DSP),\nwhich encodes both the static and dynamical driving context in a hierarchical\nmanner. Different from methods based on a rasterized map or sparse lane graph,\nwe consider the driving context as a graph with two layers, focusing on both\ngeometrical and topological features. Graph neural networks (GNNs) are applied\nto extract features with different levels of granularity, and features are\nsubsequently aggregated with attention-based inter-layer networks, realizing\nbetter local-global feature fusion. Following the recent goal-driven trajectory\nprediction pipeline, goal candidates with high likelihood for the target agent\nare extracted, and predicted trajectories are generated conditioned on these\ngoals. Thanks to the proposed dual-scale context fusion network, our DSP is\nable to generate accurate and human-like multi-modal trajectories. We evaluate\nthe proposed method on the large-scale Argoverse motion forecasting benchmark,\nand it achieves promising results, outperforming the recent state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peiliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shaojie Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Critical Study on the Recent Deep Learning Based Semi-Supervised Video Anomaly Detection Methods. (arXiv:2111.01604v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01604","description":"<p>Video anomaly detection is one of the hot research topics in computer vision\nnowadays, as abnormal events contain a high amount of information. Anomalies\nare one of the main detection targets in surveillance systems, usually needing\nreal-time actions. Regarding the availability of labeled data for training\n(i.e., there is not enough labeled data for abnormalities), semi-supervised\nanomaly detection approaches have gained interest recently. This paper\nintroduces the researchers of the field to a new perspective and reviews the\nrecent deep-learning based semi-supervised video anomaly detection approaches,\nbased on a common strategy they use for anomaly detection. Our goal is to help\nresearchers develop more effective video anomaly detection methods. As the\nselection of a right Deep Neural Network plays an important role for several\nparts of this task, a quick comparative review on DNNs is prepared first.\nUnlike previous surveys, DNNs are reviewed from a spatiotemporal feature\nextraction viewpoint, customized for video anomaly detection. This part of the\nreview can help researchers in this field select suitable networks for\ndifferent parts of their methods. Moreover, some of the state-of-the-art\nanomaly detection methods, based on their detection strategy, are critically\nsurveyed. The review provides a novel and deep look at existing methods and\nresults in stating the shortcomings of these approaches, which can be a hint\nfor future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_M/0/1/0/all/0/1\">Mohammad Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergevin_R/0/1/0/all/0/1\">Robert Bergevin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyTrack: Tracking with Bounding Polygons. (arXiv:2111.01606v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01606","description":"<p>In this paper, we present a novel method called PolyTrack for fast\nmulti-object tracking and segmentation using bounding polygons. Polytrack\ndetects objects by producing heatmaps of their center keypoint. For each of\nthem, a rough segmentation is done by computing a bounding polygon over each\ninstance instead of the traditional bounding box. Tracking is done by taking\ntwo consecutive frames as input and computing a center offset for each object\ndetected in the first frame to predict its location in the second frame. A\nKalman filter is also applied to reduce the number of ID switches. Since our\ntarget application is automated driving systems, we apply our method on urban\nenvironment videos. We trained and evaluated PolyTrack on the MOTS and\nKITTIMOTS datasets. Results show that tracking polygons can be a good\nalternative to bounding box and mask tracking. The code of PolyTrack is\navailable at https://github.com/gafaua/PolyTrack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faure_G/0/1/0/all/0/1\">Gaspar Faure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perreault_H/0/1/0/all/0/1\">Hughes Perreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN. (arXiv:2111.01619v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01619","description":"<p>Recently, StyleGAN has enabled various image manipulation and editing tasks\nthanks to the high-quality generation and the disentangled latent space.\nHowever, additional architectures or task-specific training paradigms are\nusually required for different tasks. In this work, we take a deeper look at\nthe spatial properties of StyleGAN. We show that with a pretrained StyleGAN\nalong with some operations, without any additional architecture, we can perform\ncomparably to the state-of-the-art methods on various tasks, including image\nblending, panorama generation, generation from a single image, controllable and\nlocal multimodal image to image translation, and attributes transfer. The\nproposed method is simple, effective, efficient, and applicable to any existing\npretrained StyleGAN model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Tri-attention Fusion Guided Multi-modal Segmentation Network. (arXiv:2111.01623v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01623","description":"<p>In the field of multimodal segmentation, the correlation between different\nmodalities can be considered for improving the segmentation results.\nConsidering the correlation between different MR modalities, in this paper, we\npropose a multi-modality segmentation network guided by a novel tri-attention\nfusion. Our network includes N model-independent encoding paths with N image\nsources, a tri-attention fusion block, a dual-attention fusion block, and a\ndecoding path. The model independent encoding paths can capture\nmodality-specific features from the N modalities. Considering that not all the\nfeatures extracted from the encoders are useful for segmentation, we propose to\nuse dual attention based fusion to re-weight the features along the modality\nand space paths, which can suppress less informative features and emphasize the\nuseful ones for each modality at different positions. Since there exists a\nstrong correlation between different modalities, based on the dual attention\nfusion block, we propose a correlation attention module to form the\ntri-attention fusion block. In the correlation attention module, a correlation\ndescription block is first used to learn the correlation between modalities and\nthen a constraint based on the correlation is used to guide the network to\nlearn the latent correlated features which are more relevant for segmentation.\nFinally, the obtained fused feature representation is projected by the decoder\nto obtain the segmentation results. Our experiment results tested on BraTS 2018\ndataset for brain tumor segmentation demonstrate the effectiveness of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tongxue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1\">Pierre Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canu_S/0/1/0/all/0/1\">St&#xe9;phane Canu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Attention in Fine-grained Classification. (arXiv:2111.01628v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01628","description":"<p>The way humans attend to, process and classify a given image has the\npotential to vastly benefit the performance of deep learning models. Exploiting\nwhere humans are focusing can rectify models when they are deviating from\nessential features for correct decisions. To validate that human attention\ncontains valuable information for decision-making processes such as\nfine-grained classification, we compare human attention and model explanations\nin discovering important features. Towards this goal, we collect human gaze\ndata for the fine-grained classification dataset CUB and build a dataset named\nCUB-GHA (Gaze-based Human Attention). Furthermore, we propose the Gaze\nAugmentation Training (GAT) and Knowledge Fusion Network (KFN) to integrate\nhuman gaze knowledge into classification models. We implement our proposals in\nCUB-GHA and the recently released medical dataset CXR-Eye of chest X-ray\nimages, which includes gaze data collected from a radiologist. Our result\nreveals that integrating human attention knowledge benefits classification\neffectively, e.g. improving the baseline by 4.38% on CXR. Hence, our work\nprovides not only valuable insights into understanding human attention in\nfine-grained classification, but also contributes to future research in\nintegrating human gaze with computer vision tasks. CUB-GHA and code are\navailable at https://github.com/yaorong0921/CUB-GHA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yao Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Medical Image Segmentation via Generative Adversarial Networks and Layer-wise Relevance Propagation. (arXiv:2111.01665v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01665","description":"<p>This paper contributes to automating medical image segmentation by proposing\ngenerative adversarial network-based models to segment both polyps and\ninstruments in endoscopy images. A major contribution of this work is to\nprovide explanations for the predictions using a layer-wise relevance\npropagation approach designating which input image pixels are relevant to the\npredictions and to what extent. On the polyp segmentation task, the models\nachieved 0.84 of accuracy and 0.46 on Jaccard index. On the instrument\nsegmentation task, the models achieved 0.96 of accuracy and 0.70 on Jaccard\nindex. The code is available at https://github.com/Awadelrahman/MedAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_A/0/1/0/all/0/1\">Awadelrahman M. A. Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_L/0/1/0/all/0/1\">Leen A. M. Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Self-Attention: What's Missing in Attention for Video Understanding. (arXiv:2111.01673v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01673","description":"<p>Convolution has been arguably the most important feature transform for modern\nneural networks, leading to the advance of deep learning. Recent emergence of\nTransformer networks, which replace convolution layers with self-attention\nblocks, has revealed the limitation of stationary convolution kernels and\nopened the door to the era of dynamic feature transforms. The existing dynamic\ntransforms, including self-attention, however, are all limited for video\nunderstanding where correspondence relations in space and time, i.e., motion\ninformation, are crucial for effective representation. In this work, we\nintroduce a relational feature transform, dubbed the relational self-attention\n(RSA), that leverages rich structures of spatio-temporal relations in videos by\ndynamically generating relational kernels and aggregating relational contexts.\nOur experiments and ablation studies show that the RSA network substantially\noutperforms convolution and self-attention counterparts, achieving the state of\nthe art on the standard motion-centric benchmarks for video action recognition,\nsuch as Something-Something-V1 &amp; V2, Diving48, and FineGym.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Manjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heeseung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots. (arXiv:2111.01674v1 [cs.RO])","link":"http://arxiv.org/abs/2111.01674","description":"<p>Legged locomotion is commonly studied and expressed as a discrete set of gait\npatterns, like walk, trot, gallop, which are usually treated as given and\npre-programmed in legged robots for efficient locomotion at different speeds.\nHowever, fixing a set of pre-programmed gaits limits the generality of\nlocomotion. Recent animal motor studies show that these conventional gaits are\nonly prevalent in ideal flat terrain conditions while real-world locomotion is\nunstructured and more like bouts of intermittent steps. What principles could\nlead to both structured and unstructured patterns across mammals and how to\nsynthesize them in robots? In this work, we take an analysis-by-synthesis\napproach and learn to move by minimizing mechanical energy. We demonstrate that\nlearning to minimize energy consumption plays a key role in the emergence of\nnatural locomotion gaits at different speeds in real quadruped robots. The\nemergent gaits are structured in ideal terrains and look similar to that of\nhorses and sheep. The same approach leads to unstructured gaits in rough\nterrains which is consistent with the findings in animal motor control. We\nvalidate our hypothesis in both simulation and real hardware across natural\nterrains. Videos at https://energy-locomotion.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zipeng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Top1 Solution of QQ Browser 2021 Ai Algorithm Competition Track 1 : Multimodal Video Similarity. (arXiv:2111.01677v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01677","description":"<p>In this paper, we describe the solution to the QQ Browser 2021 Ai Algorithm\nCompetition (AIAC) Track 1. We use the multi-modal transformer model for the\nvideo embedding extraction. In the pretrain phase, we train the model with\nthree tasks, (1) Video Tag Classification (VTC), (2) Mask Language Modeling\n(MLM) and (3) Mask Frame Modeling (MFM). In the finetune phase, we train the\nmodel with video similarity based on rank normalized human labels. Our full\npipeline, after ensembling several models, scores 0.852 on the leaderboard,\nwhich we achieved the 1st place in the competition. The source codes have been\nreleased at Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhuoran Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_M/0/1/0/all/0/1\">Majing Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xuan Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency detection with moving camera via background model completion. (arXiv:2111.01681v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01681","description":"<p>To detect saliency in video is a fundamental step in many computer vision\nsystems. Saliency is the significant target(s) in the video. The object of\ninterest is further analyzed for high-level applications. The segregation of\nsaliency and the background can be made if they exhibit different visual cues.\nTherefore, saliency detection is often formulated as background subtraction.\nHowever, saliency detection is challenging. For instance, dynamic background\ncan result in false positive errors. In another scenario, camouflage will lead\nto false negative errors. With moving camera, the captured scenes are even more\ncomplicated to handle. We propose a new framework, called saliency detection\nvia background model completion (SD-BMC), that comprises of a background\nmodeler and the deep learning background/foreground segmentation network. The\nbackground modeler generates an initial clean background image from a short\nimage sequence. Based on the idea of video completion, a good background frame\ncan be synthesized with the co-existence of changing background and moving\nobjects. We adopt the background/foreground segmenter, although pre-trained\nwith a specific video dataset, can also detect saliency in unseen videos. The\nbackground modeler can adjust the background image dynamically when the\nbackground/foreground segmenter output deteriorates during processing of a long\nvideo. To the best of our knowledge, our framework is the first one to adopt\nvideo completion for background modeling and saliency detection in videos\ncaptured by moving camera. The results, obtained from the PTZ videos, show that\nour proposed framework outperforms some deep learning-based background\nsubtraction models by 11% or more. With more challenging videos, our framework\nalso outperforms many high ranking background subtraction methods by more than\n3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yupei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kwok-Leung Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive observation of Covid-19 vaccination effects on skin-cellular structures by use of Intelligent Laser Speckle Classification (ILSC). (arXiv:2111.01682v1 [eess.IV])","link":"http://arxiv.org/abs/2111.01682","description":"<p>We have made a progressive observation of Covid-19 Astra Zeneca Vaccination\neffect on Skin cellular network and properties by use of well established\nIntelligent Laser Speckle Classification (ILSC) image based technique and\nmanaged to distinguish between three different subjects groups via their laser\nspeckle skin image samplings such as early-vaccinated, late-vaccinated and\nnon-vaccinated individuals. The results have proven that the ILSC technique in\nassociation with the optimised Bayesian network is capable of classifying skin\nchanges of vaccinated and non-vaccinated individuals and also of detecting\nprogressive development made on skin cellular properties for a month period.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Orun_A/0/1/0/all/0/1\">Ahmet Orun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurugollu_F/0/1/0/all/0/1\">Fatih Kurugollu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Synthetic Images To Uncover Population Biases In Facial Landmarks Detection. (arXiv:2111.01683v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01683","description":"<p>In order to analyze a trained model performance and identify its weak spots,\none has to set aside a portion of the data for testing. The test set has to be\nlarge enough to detect statistically significant biases with respect to all the\nrelevant sub-groups in the target population. This requirement may be difficult\nto satisfy, especially in data-hungry applications. We propose to overcome this\ndifficulty by generating synthetic test set. We use the face landmarks\ndetection task to validate our proposal by showing that all the biases observed\non real datasets are also seen on a carefully designed synthetic dataset. This\nshows that synthetic test sets can efficiently detect a model's weak spots and\novercome limitations of real test set in terms of quantity and/or diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shadmi_R/0/1/0/all/0/1\">Ran Shadmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laserson_J/0/1/0/all/0/1\">Jonathan Laserson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbaz_G/0/1/0/all/0/1\">Gil Elbaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Knowledge Distillation From the Perspective of Model Calibration. (arXiv:2111.01684v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01684","description":"<p>Recent years have witnessed dramatically improvements in the knowledge\ndistillation, which can generate a compact student model for better efficiency\nwhile retaining the model effectiveness of the teacher model. Previous studies\nfind that: more accurate teachers do not necessary make for better teachers due\nto the mismatch of abilities. In this paper, we aim to analysis the phenomenon\nfrom the perspective of model calibration. We found that the larger teacher\nmodel may be too over-confident, thus the student model cannot effectively\nimitate. While, after the simple model calibration of the teacher model, the\nsize of the teacher model has a positive correlation with the performance of\nthe student model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jincen Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks. (arXiv:2111.01714v1 [cs.LG])","link":"http://arxiv.org/abs/2111.01714","description":"<p>Adversarial attacks based on randomized search schemes have obtained\nstate-of-the-art results in black-box robustness evaluation recently. However,\nas we demonstrate in this work, their efficiency in different query budget\nregimes depends on manual design and heuristic tuning of the underlying\nproposal distributions. We study how this issue can be addressed by adapting\nthe proposal distribution online based on the information obtained during the\nattack. We consider Square Attack, which is a state-of-the-art score-based\nblack-box attack, and demonstrate how its performance can be improved by a\nlearned controller that adjusts the parameters of the proposal distribution\nonline during the attack. We train the controller using gradient-based\nend-to-end training on a CIFAR10 model with white box access. We demonstrate\nthat plugging the learned controller into the attack consistently improves its\nblack-box robustness estimate in different query regimes by up to 20% for a\nwide range of different models with black-box access. We further show that the\nlearned adaptation principle transfers well to the other data distributions\nsuch as CIFAR100 or ImageNet and to the targeted attack setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yatsura_M/0/1/0/all/0/1\">Maksym Yatsura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Absolute distance prediction based on deep learning object detection and monocular depth estimation models. (arXiv:2111.01715v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01715","description":"<p>Determining the distance between the objects in a scene and the camera sensor\nfrom 2D images is feasible by estimating depth images using stereo cameras or\n3D cameras. The outcome of depth estimation is relative distances that can be\nused to calculate absolute distances to be applicable in reality. However,\ndistance estimation is very challenging using 2D monocular cameras. This paper\npresents a deep learning framework that consists of two deep networks for depth\nestimation and object detection using a single image. Firstly, objects in the\nscene are detected and localized using the You Only Look Once (YOLOv5) network.\nIn parallel, the estimated depth image is computed using a deep autoencoder\nnetwork to detect the relative distances. The proposed object detection based\nYOLO was trained using a supervised learning technique, in turn, the network of\ndepth estimation was self-supervised training. The presented distance\nestimation framework was evaluated on real images of outdoor scenes. The\nachieved results show that the proposed framework is promising and it yields an\naccuracy of 96% with RMSE of 0.203 of the correct absolute distance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masoumian_A/0/1/0/all/0/1\">Armin Masoumian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marei_D/0/1/0/all/0/1\">David G. F. Marei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulwahab_S/0/1/0/all/0/1\">Saddam Abdulwahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristiano_J/0/1/0/all/0/1\">Julian Cristiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puig_D/0/1/0/all/0/1\">Domenec Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashwan_H/0/1/0/all/0/1\">Hatem A. Rashwan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixFace: Improving Face Verification Focusing on Fine-grained Conditions. (arXiv:2111.01717v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01717","description":"<p>The performance of face recognition has become saturated for public benchmark\ndatasets such as LFW, CFP-FP, and AgeDB, owing to the rapid advances in CNNs.\nHowever, the effects of faces with various fine-grained conditions on FR models\nhave not been investigated because of the absence of such datasets. This paper\nanalyzes their effects in terms of different conditions and loss functions\nusing K-FACE, a recently introduced FR dataset with fine-grained conditions. We\npropose a novel loss function, MixFace, that combines classification and metric\nlosses. The superiority of MixFace in terms of effectiveness and robustness is\ndemonstrated experimentally on various benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Junuk Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sungbin Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joochan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Yongjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seonhoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Heung-Seon Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPSeg: Cluster-free Panoptic Segmentation of 3D LiDAR Point Clouds. (arXiv:2111.01723v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01723","description":"<p>A fast and accurate panoptic segmentation system for LiDAR point clouds is\ncrucial for autonomous driving vehicles to understand the surrounding objects\nand scenes. Existing approaches usually rely on proposals or clustering to\nsegment foreground instances. As a result, they struggle to achieve real-time\nperformance. In this paper, we propose a novel real-time end-to-end panoptic\nsegmentation network for LiDAR point clouds, called CPSeg. In particular, CPSeg\ncomprises a shared encoder, a dual decoder, a task-aware attention module (TAM)\nand a cluster-free instance segmentation head. TAM is designed to enforce these\ntwo decoders to learn rich task-aware features for semantic and instance\nembedding. Moreover, CPSeg incorporates a new cluster-free instance\nsegmentation head to dynamically pillarize foreground points according to the\nlearned embedding. Then, it acquires instance labels by finding connected\npillars with a pairwise embedding comparison. Thus, the conventional\nproposal-based or clustering-based instance segmentation is transformed into a\nbinary segmentation problem on the pairwise embedding comparison matrix. To\nhelp the network regress instance embedding, a fast and deterministic depth\ncompletion algorithm is proposed to calculate surface normal of each point\ncloud in real-time. The proposed method is benchmarked on two large-scale\nautonomous driving datasets, namely, SemanticKITTI and nuScenes. Notably,\nextensive experimental results show that CPSeg achieves the state-of-the-art\nresults among real-time approaches on both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Enxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razani_R/0/1/0/all/0/1\">Ryan Razani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yixuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingbing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized One-Shot Lipreading for an ALS Patient. (arXiv:2111.01740v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01740","description":"<p>Lipreading or visually recognizing speech from the mouth movements of a\nspeaker is a challenging and mentally taxing task. Unfortunately, multiple\nmedical conditions force people to depend on this skill in their day-to-day\nlives for essential communication. Patients suffering from Amyotrophic Lateral\nSclerosis (ALS) often lose muscle control, consequently their ability to\ngenerate speech and communicate via lip movements. Existing large datasets do\nnot focus on medical patients or curate personalized vocabulary relevant to an\nindividual. Collecting a large-scale dataset of a patient, needed to train\nmod-ern data-hungry deep learning models is, however, extremely challenging. In\nthis work, we propose a personalized network to lipread an ALS patient using\nonly one-shot examples. We depend on synthetically generated lip movements to\naugment the one-shot scenario. A Variational Encoder based domain adaptation\ntechnique is used to bridge the real-synthetic domain gap. Our approach\nsignificantly improves and achieves high top-5accuracy with 83.2% accuracy\ncompared to 62.6% achieved by comparable methods for the patient. Apart from\nevaluating our approach on the ALS patient, we also extend it to people with\nhearing impairment relying extensively on lip movements to communicate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_B/0/1/0/all/0/1\">Bipasha Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aditya Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_R/0/1/0/all/0/1\">Rudrabha Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LogAvgExp Provides a Principled and Performant Global Pooling Operator. (arXiv:2111.01742v1 [cs.LG])","link":"http://arxiv.org/abs/2111.01742","description":"<p>We seek to improve the pooling operation in neural networks, by applying a\nmore theoretically justified operator. We demonstrate that LogSumExp provides a\nnatural OR operator for logits. When one corrects for the number of elements\ninside the pooling operator, this becomes $\\text{LogAvgExp} :=\n\\log(\\text{mean}(\\exp(x)))$. By introducing a single temperature parameter,\nLogAvgExp smoothly transitions from the max of its operands to the mean (found\nat the limiting cases $t \\to 0^+$ and $t \\to +\\infty$). We experimentally\ntested LogAvgExp, both with and without a learnable temperature parameter, in a\nvariety of deep neural network architectures for computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Scott C. Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trappenberg_T/0/1/0/all/0/1\">Thomas Trappenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oore_S/0/1/0/all/0/1\">Sageev Oore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing Liquid State Machine Performance with Edge-of-Chaos Dynamics Organized by Astrocyte-modulated Plasticity. (arXiv:2111.01760v1 [cs.NE])","link":"http://arxiv.org/abs/2111.01760","description":"<p>The liquid state machine (LSM) combines low training complexity and\nbiological plausibility, which has made it an attractive machine learning\nframework for edge and neuromorphic computing paradigms. Originally proposed as\na model of brain computation, the LSM tunes its internal weights without\nbackpropagation of gradients, which results in lower performance compared to\nmulti-layer neural networks. Recent findings in neuroscience suggest that\nastrocytes, a long-neglected non-neuronal brain cell, modulate synaptic\nplasticity and brain dynamics, tuning brain networks to the vicinity of the\ncomputationally optimal critical phase transition between order and chaos.\nInspired by this disruptive understanding of how brain networks self-tune, we\npropose the neuron-astrocyte liquid state machine (NALSM) that addresses\nunder-performance through self-organized near-critical dynamics. Similar to its\nbiological counterpart, the astrocyte model integrates neuronal activity and\nprovides global feedback to spike-timing-dependent plasticity (STDP), which\nself-organizes NALSM dynamics around a critical branching factor that is\nassociated with the edge-of-chaos. We demonstrate that NALSM achieves\nstate-of-the-art accuracy versus comparable LSM methods, without the need for\ndata-specific hand-tuning. With a top accuracy of 97.61% on MNIST, 97.51% on\nN-MNIST, and 85.84% on Fashion-MNIST, NALSM achieved comparable performance to\ncurrent fully-connected multi-layer spiking neural networks trained via\nbackpropagation. Our findings suggest that the further development of\nbrain-inspired machine learning methods has the potential to reach the\nperformance of deep learning, with the added benefits of supporting robust and\nenergy-efficient neuromorphic computing on the edge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_V/0/1/0/all/0/1\">Vladimir A. Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michmizos_K/0/1/0/all/0/1\">Konstantinos P. Michmizos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatchGame: Learning to Signal Mid-level Patches in Referential Games. (arXiv:2111.01785v1 [cs.CV])","link":"http://arxiv.org/abs/2111.01785","description":"<p>We study a referential game (a type of signaling game) where two agents\ncommunicate with each other via a discrete bottleneck to achieve a common goal.\nIn our referential game, the goal of the speaker is to compose a message or a\nsymbolic representation of \"important\" image patches, while the task for the\nlistener is to match the speaker's message to a different view of the same\nimage. We show that it is indeed possible for the two agents to develop a\ncommunication protocol without explicit or implicit supervision. We further\ninvestigate the developed protocol and show the applications in speeding up\nrecent Vision Transformers by using only important patches, and as pre-training\nfor downstream recognition tasks (e.g., classification). Code available at\nhttps://github.com/kampta/PatchGame.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kamal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somepalli_G/0/1/0/all/0/1\">Gowthami Somepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anubhav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasundara_V/0/1/0/all/0/1\">Vinoj Jayasundara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not all Failure Modes are Created Equal: Training Deep Neural Networks for Explicable (Mis)Classification. (arXiv:2006.14841v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.14841","description":"<p>Deep Neural Networks are often brittle on image classification tasks and\nknown to misclassify inputs. While these misclassifications may be inevitable,\nall failure modes cannot be considered equal. Certain misclassifications (eg.\nclassifying the image of a dog to an airplane) can perplex humans and result in\nthe loss of human trust in the system. Even worse, these errors (eg. a person\nmisclassified as a primate) can have odious societal impacts. Thus, in this\nwork, we aim to reduce inexplicable errors. To address this challenge, we first\ndiscuss methods to obtain the class-level semantics that capture the human's\nexpectation ($M^h$) regarding which classes are semantically close {\\em vs.}\nones that are far away. We show that for popular image benchmarks (like\nCIFAR-10, CIFAR-100, ImageNet), class-level semantics can be readily obtained\nby leveraging either human subject studies or publicly available human-curated\nknowledge bases. Second, we propose the use of Weighted Loss Functions (WLFs)\nto penalize misclassifications by the weight of their inexplicability. Finally,\nwe show that training (or fine-tuning) existing classifiers with the proposed\nmethods lead to Deep Neural Networks that have (1) comparable top-1 accuracy,\n(2) more explicable failure modes on both in-distribution and\nout-of-distribution (OOD) test data, and (3) incur significantly less cost in\nthe gathering of additional human labels compared to existing works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1\">Alberto Olmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sailik Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1\">Subbarao Kambhampati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised 3D Human Pose Representation with Viewpoint and Pose Disentanglement. (arXiv:2007.07053v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.07053","description":"<p>Learning a good 3D human pose representation is important for human pose\nrelated tasks, e.g. human 3D pose estimation and action recognition. Within all\nthese problems, preserving the intrinsic pose information and adapting to view\nvariations are two critical issues. In this work, we propose a novel Siamese\ndenoising autoencoder to learn a 3D pose representation by disentangling the\npose-dependent and view-dependent feature from the human skeleton data, in a\nfully unsupervised manner. These two disentangled features are utilized\ntogether as the representation of the 3D pose. To consider both the kinematic\nand geometric dependencies, a sequential bidirectional recursive network\n(SeBiReNet) is further proposed to model the human skeleton data. Extensive\nexperiments demonstrate that the learned representation 1) preserves the\nintrinsic information of human pose, 2) shows good transferability across\ndatasets and tasks. Notably, our approach achieves state-of-the-art performance\non two inherently different tasks: pose denoising and unsupervised action\nrecognition. Code and models are available at:\n\\url{https://github.com/NIEQiang001/unsupervised-human-pose.git}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Q/0/1/0/all/0/1\">Qiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunhui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Learning of Multi-Object 3D Scene Decompositions Using Deep Shape Priors. (arXiv:2010.04030v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.04030","description":"<p>Representing scenes at the granularity of objects is a prerequisite for scene\nunderstanding and decision making. We propose PriSMONet, a novel approach based\non Prior Shape knowledge for learning Multi-Object 3D scene decomposition and\nrepresentations from single images. Our approach learns to decompose images of\nsynthetic scenes with multiple objects on a planar surface into its constituent\nscene objects and to infer their 3D properties from a single view. A recurrent\nencoder regresses a latent representation of 3D shape, pose and texture of each\nobject from an input RGB image. By differentiable rendering, we train our model\nto decompose scenes from RGB-D images in a self-supervised way. The 3D shapes\nare represented continuously in function-space as signed distance functions\nwhich we pre-train from example shapes in a supervised way. These shape priors\nprovide weak supervision signals to better condition the challenging overall\nlearning task. We evaluate the accuracy of our model in inferring 3D scene\nlayout, demonstrate its generative capabilities, assess its generalization to\nreal images, and point out benefits of the learned representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elich_C/0/1/0/all/0/1\">Cathrin Elich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1\">Joerg Stueckler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular Action Concept Grounding in Semantic Video Prediction. (arXiv:2011.11201v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11201","description":"<p>Recent works in video prediction have mainly focused on passive forecasting\nand low-level action-conditional prediction, which sidesteps the learning of\ninteraction between agents and objects. We introduce the task of semantic\naction-conditional video prediction, which uses semantic action labels to\ndescribe those interactions and can be regarded as an inverse problem of action\nrecognition. The challenge of this new task primarily lies in how to\neffectively inform the model of semantic action information. Inspired by the\nidea of Mixture of Experts, we embody each abstract label by a structured\ncombination of various visual concept learners and propose a novel video\nprediction model, Modular Action Concept Network (MAC). Our method is evaluated\non two newly designed synthetic datasets, CLEVR-Building-Blocks and\nSapien-Kitchen, and one real-world dataset called Tower-Creation. Extensive\nexperiments demonstrate that MAC can correctly condition on given instructions\nand generate corresponding future frames without need of bounding boxes. We\nfurther show that the trained model can make out-of-distribution\ngeneralization, be quickly adapted to new object categories and exploit its\nlearnt features for object detection, showing the progression towards\nhigher-level cognitive abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Songhenh Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Easterbrook_S/0/1/0/all/0/1\">Steve Easterbrook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer. (arXiv:2012.00517v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.00517","description":"<p>Computer vision and machine learning can be used to automate various tasks in\ncancer diagnostic and detection. If an attacker can manipulate the automated\nprocessing, the results can be devastating and in the worst case lead to wrong\ndiagnosis and treatment. In this research, the goal is to demonstrate the use\nof one-pixel attacks in a real-life scenario with a real pathology dataset,\nTUPAC16, which consists of digitized whole-slide images. We attack against the\nIBM CODAIT's MAX breast cancer detector using adversarial images. These\nadversarial examples are found using differential evolution to perform the\none-pixel modification to the images in the dataset. The results indicate that\na minor one-pixel modification of a whole slide image under analysis can affect\nthe diagnosis by reversing the automatic diagnosis result. The attack poses a\nthreat from the cyber security perspective: the one-pixel method can be used as\nan attack vector by a motivated attacker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1\">Joni Korpihalkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1\">Tuomo Sipola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puuska_S/0/1/0/all/0/1\">Samir Puuska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1\">Tero Kokkonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition. (arXiv:2102.07092v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.07092","description":"<p>Spatio-temporal convolution often fails to learn motion dynamics in videos\nand thus an effective motion representation is required for video understanding\nin the wild. In this paper, we propose a rich and robust motion representation\nbased on spatio-temporal self-similarity (STSS). Given a sequence of frames,\nSTSS represents each local region as similarities to its neighbors in space and\ntime. By converting appearance features into relational values, it enables the\nlearner to better recognize structural patterns in space and time. We leverage\nthe whole volume of STSS and let our model learn to extract an effective motion\nrepresentation from it. The proposed neural block, dubbed SELFY, can be easily\ninserted into neural architectures and trained end-to-end without additional\nsupervision. With a sufficient volume of the neighborhood in space and time, it\neffectively captures long-term interaction and fast motion in the video,\nleading to robust action recognition. Our experimental analysis demonstrates\nits superiority over previous methods for motion modeling as well as its\ncomplementarity to spatio-temporal features from direct convolution. On the\nstandard action recognition benchmarks, Something-Something-V1 &amp; V2, Diving-48,\nand FineGym, the proposed method achieves the state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heeseung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Manjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Anytime Prediction with Parallel Cascaded Networks and a Temporal-Difference Loss. (arXiv:2102.09808v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.09808","description":"<p>Although deep feedforward neural networks share some characteristics with the\nprimate visual system, a key distinction is their dynamics. Deep nets typically\noperate in serial stages wherein each layer completes its computation before\nprocessing begins in subsequent layers. In contrast, biological systems have\ncascaded dynamics: information propagates from neurons at all layers in\nparallel but transmission occurs gradually over time, leading to speed-accuracy\ntrade offs even in feedforward architectures. We explore the consequences of\nbiologically inspired parallel hardware by constructing cascaded ResNets in\nwhich each residual block has propagation delays but all blocks update in\nparallel in a stateful manner. Because information transmitted through skip\nconnections avoids delays, the functional depth of the architecture increases\nover time, yielding anytime predictions that improve with internal-processing\ntime. We introduce a temporal-difference training loss that achieves a strictly\nsuperior speed-accuracy profile over standard losses and enables the cascaded\narchitecture to outperform state-of-the-art anytime-prediction methods. The\ncascaded architecture has intriguing properties, including: it classifies\ntypical instances more rapidly than atypical instances; it is more robust to\nboth persistent and transient noise than is a conventional ResNet; and its\ntime-varying output trace provides a signal that can be exploited to improve\ninformation processing and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iuzzolino_M/0/1/0/all/0/1\">Michael L. Iuzzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1\">Samy Bengio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Dataset Collaborative Learning for Semantic Segmentation in Autonomous Driving. (arXiv:2103.11351v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11351","description":"<p>Semantic segmentation is an important task for scene understanding in\nself-driving cars and robotics, which aims to assign dense labels for all\npixels in the image. Existing work typically improves semantic segmentation\nperformance by exploring different network architectures on a target dataset.\nLittle attention has been paid to build a unified system by simultaneously\nlearning from multiple datasets due to the inherent distribution shift across\ndifferent datasets. In this paper, we propose a simple, flexible, and general\nmethod for semantic segmentation, termed Cross-Dataset Collaborative Learning\n(CDCL). Our goal is to train a unified model for improving the performance in\neach dataset by leveraging information from all the datasets. Specifically, we\nfirst introduce a family of Dataset-Aware Blocks (DAB) as the fundamental\ncomputing units of the network, which help capture homogeneous convolutional\nrepresentations and heterogeneous statistics across different datasets. Second,\nwe present a Dataset Alternation Training (DAT) mechanism to facilitate the\ncollaborative optimization procedure. We conduct extensive evaluations on\ndiverse semantic segmentation datasets for autonomous driving. Experiments\ndemonstrate that our method consistently achieves notable improvements over\nprior single-dataset and cross-dataset training methods without introducing\nextra FLOPs. Particularly, with the same architecture of PSPNet (ResNet-18),\nour method outperforms the single-dataset baseline by 5.65\\%, 6.57\\%, and\n5.79\\% mIoU on the validation sets of Cityscapes, BDD100K, CamVid,\nrespectively. We also apply CDCL for point cloud 3D semantic segmentation and\nachieve improved performance, which further validates the superiority and\ngenerality of our method. Code and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinzhang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shared Latent Space of Font Shapes and Their Noisy Impressions. (arXiv:2103.12347v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12347","description":"<p>Styles of typefaces or fonts are often associated with specific impressions,\nsuch as heavy, contemporary, or elegant. This indicates that there are certain\ncorrelations between font shapes and their impressions. To understand the\ncorrelations, this paper realizes a shared latent space where a font and its\nimpressions are embedded nearby. The difficulty is that the impression words\nattached to a font are often very noisy. This is because impression words are\nvery subjective and diverse. More importantly, some impression words have no\ndirect relevance to the font shapes and will disturb the realization of the\nshared latent space. We, therefore, use DeepSets for enhancing shape-relevant\nwords and suppressing shape irrelevant words automatically while training the\nshared latent space. Quantitative and qualitative experimental results with a\nlarge-scale font-impression dataset demonstrate that the shared latent space by\nthe proposed method describes the correlation appropriately, especially for the\nshape-relevant impression words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jihun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_D/0/1/0/all/0/1\">Daichi Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_S/0/1/0/all/0/1\">Seiya Matsuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novelty Detection and Analysis of Traffic Scenario Infrastructures in the Latent Space of a Vision Transformer-Based Triplet Autoencoder. (arXiv:2105.01924v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01924","description":"<p>Detecting unknown and untested scenarios is crucial for scenario-based\ntesting. Scenario-based testing is considered to be a possible approach to\nvalidate autonomous vehicles. A traffic scenario consists of multiple\ncomponents, with infrastructure being one of it. In this work, a method to\ndetect novel traffic scenarios based on their infrastructure images is\npresented. An autoencoder triplet network provides latent representations for\ninfrastructure images which are used for outlier detection. The triplet\ntraining of the network is based on the connectivity graphs of the\ninfrastructure. By using the proposed architecture, expert-knowledge is used to\nshape the latent space such that it incorporates a pre-defined similarity in\nthe neighborhood relationships of an autoencoder. An ablation study on the\narchitecture is highlighting the importance of the triplet autoencoder\ncombination. The best performing architecture is based on vision transformers,\na convolution-free attention-based network. The presented method outperforms\nother state-of-the-art outlier detection approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wurst_J/0/1/0/all/0/1\">Jonas Wurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_L/0/1/0/all/0/1\">Lakshman Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1\">Michael Botsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utschick_W/0/1/0/all/0/1\">Wolfgang Utschick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GasHisSDB: A New Gastric Histopathology Image Dataset for Computer Aided Diagnosis of Gastric Cancer. (arXiv:2106.02473v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02473","description":"<p>Background and Objective: Gastric cancer has turned out to be the fifth most\ncommon cancer globally, and early detection of gastric cancer is essential to\nsave lives. Histopathological examination of gastric cancer is the gold\nstandard for the diagnosis of gastric cancer. However, computer-aided\ndiagnostic techniques are challenging to evaluate due to the scarcity of\npublicly available gastric histopathology image datasets. Methods: In this\npaper, a noble publicly available Gastric Histopathology Sub-size Image\nDatabase (GasHisSDB) is published to identify classifiers' performance.\nSpecifically, two types of data are included: normal and abnormal, with a total\nof 245,196 tissue case images. In order to prove that the methods of different\nperiods in the field of image classification have discrepancies on GasHisSDB,\nwe select a variety of classifiers for evaluation. Seven classical machine\nlearning classifiers, three Convolutional Neural Network classifiers, and a\nnovel transformer-based classifier are selected for testing on image\nclassification tasks. Results: This study performed extensive experiments using\ntraditional machine learning and deep learning methods to prove that the\nmethods of different periods have discrepancies on GasHisSDB. Traditional\nmachine learning achieved the best accuracy rate of 86.08% and a minimum of\njust 41.12%. The best accuracy of deep learning reached 96.47% and the lowest\nwas 86.21%. Accuracy rates vary significantly across classifiers. Conclusions:\nTo the best of our knowledge, it is the first publicly available gastric cancer\nhistopathology dataset containing a large number of images for weakly\nsupervised learning. We believe that GasHisSDB can attract researchers to\nexplore new algorithms for the automated diagnosis of gastric cancer, which can\nhelp physicians and patients in the clinical setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiquan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Improving Adversarial Transferability of Vision Transformers. (arXiv:2106.04169v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04169","description":"<p>Vision transformers (ViTs) process input images as sequences of patches via\nself-attention; a radically different architecture than convolutional neural\nnetworks (CNNs). This makes it interesting to study the adversarial feature\nspace of ViT models and their transferability. In particular, we observe that\nadversarial patterns found via conventional adversarial attacks show very low\nblack-box transferability even for large ViT models. However, we show that this\nphenomenon is only due to the sub-optimal attack procedures that do not\nleverage the true representation potential of ViTs. A deep ViT is composed of\nmultiple blocks, with a consistent architecture comprising of self-attention\nand feed-forward layers, where each block is capable of independently producing\na class token. Formulating an attack using only the last class token\n(conventional approach) does not directly leverage the discriminative\ninformation stored in the earlier tokens, leading to poor adversarial\ntransferability of ViTs. Using the compositional nature of ViT models, we\nenhance the transferability of existing attacks by introducing two novel\nstrategies specific to the architecture of ViT models. (i) Self-Ensemble: We\npropose a method to find multiple discriminative pathways by dissecting a\nsingle ViT model into an ensemble of networks. This allows explicitly utilizing\nclass-specific information at each ViT block. (ii) Token Refinement: We then\npropose to refine the tokens to further enhance the discriminative capacity at\neach block of ViT. Our token refinement systematically combines the class\ntokens with structural information preserved within the patch tokens. An\nadversarial attack, when applied to such refined tokens within the ensemble of\nclassifiers found in a single vision transformer, has significantly higher\ntransferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1\">Kanchana Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Densely connected normalizing flows. (arXiv:2106.04627v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04627","description":"<p>Normalizing flows are bijective mappings between inputs and latent\nrepresentations with a fully factorized distribution. They are very attractive\ndue to exact likelihood valuation and efficient sampling. However, their\neffective capacity is often insufficient since the bijectivity constraint\nlimits the model width. We address this issue by incrementally padding\nintermediate representations with noise. We precondition the noise in\naccordance with previous invertible units, which we describe as cross-unit\ncoupling. Our invertible glow-like modules increase the model expressivity by\nfusing a densely connected block with Nystrom self-attention. We refer to our\narchitecture as DenseFlow since both cross-unit and intra-module couplings rely\non dense connectivity. Experiments show significant improvements due to the\nproposed contributions and reveal state-of-the-art density estimation under\nmoderate computing budgets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grcic_M/0/1/0/all/0/1\">Matej Grci&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1\">Ivan Grubi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey: Image Mixing and Deleting for Data Augmentation. (arXiv:2106.07085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07085","description":"<p>Data augmentation has been widely used to improve deep nerual networks\nperformance. Numerous approaches are suggested, for example, dropout,\nregularization and image augmentation, to avoid over-ftting and enhancing\ngeneralization of neural networks. One of the sub-area within data augmentation\nis image mixing and deleting. This specific type of augmentation either mixes\ntwo images or delete image regions to hide or make certain characteristics of\nimages confusing for the network to force it to emphasize on overall structure\nof object in image. The model trained with this approach has shown to perform\nand generalize well as compared to one trained without imgage mixing or\ndeleting. Additional benefit achieved with this method of training is\nrobustness against image corruptions. Due to its low compute cost and success\nin recent past, many techniques of image mixing and deleting are proposed. This\npaper provides detailed review on these devised approaches, dividing\naugmentation strategies in three main categories cut and delete, cut and mix\nand mixup. The second part of paper emprically evaluates these approaches for\nimage classification, finegrained image recognition and object detection where\nit is shown that this category of data augmentation improves the overall\nperformance for deep neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naveed_H/0/1/0/all/0/1\">Humza Naveed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v4 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2106.08208","description":"<p>Adaptive gradient methods have shown excellent performances for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using some specific adaptive learning rates.\nIt is desired to design a universal framework for practical algorithms of\nadaptive gradients with theoretical guarantee to solve general problems. To\nfill this gap, we propose a faster and universal framework of adaptive\ngradients (i.e., SUPER-ADAM) by introducing a universal adaptive matrix that\nincludes most existing adaptive gradient forms. Moreover, our framework can\nflexibly integrate the momentum and variance reduced techniques. In particular,\nour novel framework provides the convergence analysis support for adaptive\ngradient methods under the nonconvex setting. In theoretical analysis, we prove\nthat our SUPER-ADAM algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms. Code is available at\nhttps://github.com/LIJUNYI95/SuperAdam\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity. (arXiv:2106.14568v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14568","description":"<p>Recent works on sparse neural networks have demonstrated the possibility to\ntrain a sparse subnetwork independently from scratch, to match the performance\nof its corresponding dense network. However, identifying such sparse\nsubnetworks (winning tickets) either involves a costly iterative\ntrain-prune-retrain process (e.g., Lottery Ticket Hypothesis) or an\nover-extended training time (e.g., Dynamic Sparse Training). In this work, we\ndraw a unique connection between sparse neural network training and the deep\nensembling technique, yielding a novel ensemble learning framework called\nFreeTickets. Instead of starting from a dense network, FreeTickets randomly\ninitializes a sparse subnetwork and then trains the subnetwork while\ndynamically adjusting its sparse mask, resulting in many diverse sparse\nsubnetworks throughout the training process. FreeTickets is defined as the\nensemble of these sparse subnetworks freely obtained during this one-pass,\nsparse-to-sparse training, which uses only a fraction of the computational\nresources required by the vanilla dense training. Moreover, despite being an\nensemble of models, FreeTickets has even fewer parameters and training FLOPs\ncompared to a single dense model: this seemingly counter-intuitive outcome is\ndue to the high sparsity of each subnetwork. FreeTickets is observed to\ndemonstrate a significant all-round improvement compared to standard dense\nbaselines, in prediction accuracy, uncertainty estimation, robustness, and\nefficiency. FreeTickets easily outperforms the naive deep ensemble with\nResNet50 on ImageNet using only a quarter of the training FLOPs required by the\nlatter. Our results provide insights into the strength of sparse neural\nnetworks and suggest that the benefits of sparsity go way beyond the usually\nexpected inference efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1\">Zahra Atashgahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokar_G/0/1/0/all/0/1\">Ghada Sokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_E/0/1/0/all/0/1\">Elena Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Mesh Prior: Unsupervised Mesh Restoration using Graph Convolutional Networks. (arXiv:2107.02909v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02909","description":"<p>This paper addresses mesh restoration problems, i.e., denoising and\ncompletion, by learning self-similarity in an unsupervised manner. For this\npurpose, the proposed method, which we refer to as Deep Mesh Prior, uses a\ngraph convolutional network on meshes to learn the self-similarity. The network\ntakes a single incomplete mesh as input data and directly outputs the\nreconstructed mesh without being trained using large-scale datasets. Our method\ndoes not use any intermediate representations such as an implicit field because\nthe whole process works on a mesh. We demonstrate that our unsupervised method\nperforms equally well or even better than the state-of-the-art methods using\nlarge-scale datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hattori_S/0/1/0/all/0/1\">Shota Hattori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatagawa_T/0/1/0/all/0/1\">Tatsuya Yatagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohtake_Y/0/1/0/all/0/1\">Yutaka Ohtake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hiromasa Suzuki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Machine Learning based Segmentation Models on Jet Fire Radiation Zones. (arXiv:2107.03461v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03461","description":"<p>Risk assessment is relevant in any workplace, however there is a degree of\nunpredictability when dealing with flammable or hazardous materials so that\ndetection of fire accidents by itself may not be enough. An example of this is\nthe impingement of jet fires, where the heat fluxes of the flame could reach\nnearby equipment and dramatically increase the probability of a domino effect\nwith catastrophic results. Because of this, the characterization of such fire\naccidents is important from a risk management point of view. One such\ncharacterization would be the segmentation of different radiation zones within\nthe flame, so this paper presents an exploratory research regarding several\ntraditional computer vision and Deep Learning segmentation approaches to solve\nthis specific problem. A data set of propane jet fires is used to train and\nevaluate the different approaches and given the difference in the distribution\nof the zones and background of the images, different loss functions, that seek\nto alleviate data imbalance, are also explored. Additionally, different metrics\nare correlated to a manual ranking performed by experts to make an evaluation\nthat closely resembles the expert's criteria. The Hausdorff Distance and\nAdjusted Random Index were the metrics with the highest correlation and the\nbest results were obtained from the UNet architecture with a Weighted\nCross-Entropy Loss. These results can be used in future research to extract\nmore geometric information from the segmentation masks or could even be\nimplemented on other types of fire accidents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Guerrero_C/0/1/0/all/0/1\">Carmina P&#xe9;rez-Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_A/0/1/0/all/0/1\">Adriana Palacios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_C/0/1/0/all/0/1\">Christian Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1\">Miguel Gonzalez-Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_Morales_L/0/1/0/all/0/1\">Luis Eduardo Falc&#xf3;n-Morales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Frozen Section to FFPE Translation. (arXiv:2107.11786v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.11786","description":"<p>Frozen sectioning (FS) is the preparation method of choice for microscopic\nevaluation of tissues during surgical operations. The high speed of the\nprocedure allows pathologists to rapidly assess the key microscopic features,\nsuch as tumour margins and malignant status to guide surgical decision-making\nand minimise disruptions to the course of the operation. However, FS is prone\nto introducing many misleading artificial structures (histological artefacts),\nsuch as nuclear ice crystals, compression, and cutting artefacts, hindering\ntimely and accurate diagnostic judgement of the pathologist. Additional\ntraining and prolonged experience is often required to make highly effective\nand time-critical diagnosis on frozen sections. On the other hand, the gold\nstandard tissue preparation technique of formalin-fixation and\nparaffin-embedding (FFPE) provides significantly superior image quality, but is\na very time-consuming process (12-48 hours), making it unsuitable for\nintra-operative use. In this paper, we propose an artificial intelligence (AI)\nmethod that improves FS image quality by computationally transforming\nfrozen-sectioned whole-slide images (FS-WSIs) into whole-slide FFPE-style\nimages in minutes. AI-FFPE rectifies FS artefacts with the guidance of an\nattention mechanism that puts a particular emphasis on artefacts while\nutilising a self-regularization mechanism established between FS input image\nand synthesized FFPE-style image that preserves clinically relevant features.\nAs a result, AI-FFPE method successfully generates FFPE-style images without\nsignificantly extending tissue processing time and consequently improves\ndiagnostic accuracy. We demonstrate the efficacy of AI-FFPE on lung and brain\nfrozen sections using a variety of different qualitative and quantitative\nmetrics including visual Turing tests from 20 board certified pathologists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ozyoruk_K/0/1/0/all/0/1\">Kutsev Bengisu Ozyoruk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Can_S/0/1/0/all/0/1\">Sermet Can</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gokceler_G/0/1/0/all/0/1\">Guliz Irem Gokceler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basak_K/0/1/0/all/0/1\">Kayhan Basak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demir_D/0/1/0/all/0/1\">Derya Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Serin_G/0/1/0/all/0/1\">Gurdeniz Serin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hacisalihoglu_U/0/1/0/all/0/1\">Uguray Payam Hacisalihoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurtulus_E/0/1/0/all/0/1\">Emirhan Kurtulu&#x15f;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Darbaz_B/0/1/0/all/0/1\">Berkan Darbaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_F/0/1/0/all/0/1\">Funda Yilmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turan_M/0/1/0/all/0/1\">Mehmet Turan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction. (arXiv:2108.00238v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.00238","description":"<p>Understanding complex social interactions among agents is a key challenge for\ntrajectory prediction. Most existing methods consider the interactions between\npairwise traffic agents or in a local area, while the nature of interactions is\nunlimited, involving an uncertain number of agents and non-local areas\nsimultaneously. Besides, they treat heterogeneous traffic agents the same,\nnamely those among agents of different categories, while neglecting people's\ndiverse reaction patterns toward traffic agents in ifferent categories. To\naddress these problems, we propose a simple yet effective Unlimited\nNeighborhood Interaction Network (UNIN), which predicts trajectories of\nheterogeneous agents in multiple categories. Specifically, the proposed\nunlimited neighborhood interaction module generates the fused-features of all\nagents involved in an interaction simultaneously, which is adaptive to any\nnumber of agents and any range of interaction area. Meanwhile, a hierarchical\ngraph attention module is proposed to obtain category-to-category interaction\nand agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model\nare estimated for generating the future trajectories. Extensive experimental\nresults on benchmark datasets demonstrate a significant performance improvement\nof our method over the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.03272","description":"<p>Recent research in embodied AI has been boosted by the use of simulation\nenvironments to develop and train robot learning approaches. However, the use\nof simulation has skewed the attention to tasks that only require what robotics\nsimulators can simulate: motion and physical contact. We present iGibson 2.0,\nan open-source simulation environment that supports the simulation of a more\ndiverse set of household tasks through three key innovations. First, iGibson\n2.0 supports object states, including temperature, wetness level, cleanliness\nlevel, and toggled and sliced states, necessary to cover a wider range of\ntasks. Second, iGibson 2.0 implements a set of predicate logic functions that\nmap the simulator states to logic states like Cooked or Soaked. Additionally,\ngiven a logic state, iGibson 2.0 can sample valid physical states that satisfy\nit. This functionality can generate potentially infinite instances of tasks\nwith minimal effort from the users. The sampling mechanism allows our scenes to\nbe more densely populated with small objects in semantically meaningful\nlocations. Third, iGibson 2.0 includes a virtual reality (VR) interface to\nimmerse humans in its scenes to collect demonstrations. As a result, we can\ncollect demonstrations from humans on these new types of tasks, and use them\nfor imitation learning. We evaluate the new capabilities of iGibson 2.0 to\nenable robot learning of novel tasks, in the hope of demonstrating the\npotential of this new simulator to support new research in embodied AI. iGibson\n2.0 and its new dataset will be publicly available at\n<a href=\"http://svl.stanford.edu/igibson/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1\">Gokul Dharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization of Batch Whitening by Convolutional Unit Optimization. (arXiv:2108.10629v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10629","description":"<p>Batch Whitening is a technique that accelerates and stabilizes training by\ntransforming input features to have a zero mean (Centering) and a unit variance\n(Scaling), and by removing linear correlation between channels (Decorrelation).\nIn commonly used structures, which are empirically optimized with Batch\nNormalization, the normalization layer appears between convolution and\nactivation function. Following Batch Whitening studies have employed the same\nstructure without further analysis; even Batch Whitening was analyzed on the\npremise that the input of a linear layer is whitened. To bridge the gap, we\npropose a new Convolutional Unit that is in line with the theory, and our\nmethod generally improves the performance of Batch Whitening. Moreover, we show\nthe inefficacy of the original Convolutional Unit by investigating rank and\ncorrelation of features. As our method is employable off-the-shelf whitening\nmodules, we use Iterative Normalization (IterNorm), the state-of-the-art\nwhitening module, and obtain significantly improved performance on five image\nclassification datasets: CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, and\nImageNet. Notably, we verify that our method improves stability and performance\nof whitening when using large learning rate, group size, and iteration number.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Yooshin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hanbyel Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-domain semantic segmentation with overlapping labels. (arXiv:2108.11224v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11224","description":"<p>Deep supervised models have an unprecedented capacity to absorb large\nquantities of training data. Hence, training on many datasets becomes a method\nof choice towards graceful degradation in unusual scenes. Unfortunately,\ndifferent datasets often use incompatible labels. For instance, the Cityscapes\nroad class subsumes all driving surfaces, while Vistas defines separate classes\nfor road markings, manholes etc. We address this challenge by proposing a\nprincipled method for seamless learning on datasets with overlapping classes\nbased on partial labels and probabilistic loss. Our method achieves competitive\nwithin-dataset and cross-dataset generalization, as well as ability to learn\nvisual concepts which are not separately labeled in any of the training\ndatasets. Experiments reveal competitive or state-of-the-art performance on two\nmulti-domain dataset collections and on the WildDash 2 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevandic_P/0/1/0/all/0/1\">Petra Bevandi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1\">Marin Or&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1\">Ivan Grubi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saric_J/0/1/0/all/0/1\">Josip &#x160;ari&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Multi-level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection. (arXiv:2109.07950v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07950","description":"<p>With the increased deployment of face recognition systems in our daily lives,\nface presentation attack detection (PAD) is attracting much attention and\nplaying a key role in securing face recognition systems. Despite the great\nperformance achieved by the hand-crafted and deep-learning-based methods in\nintra-dataset evaluations, the performance drops when dealing with unseen\nscenarios. In this work, we propose a dual-stream convolution neural networks\n(CNNs) framework. One stream adapts four learnable frequency filters to learn\nfeatures in the frequency domain, which are less influenced by variations in\nsensors/illuminations. The other stream leverages the RGB images to complement\nthe features of the frequency domain. Moreover, we propose a hierarchical\nattention module integration to join the information from the two streams at\ndifferent stages by considering the nature of deep features in different layers\nof the CNN. The proposed method is evaluated in the intra-dataset and\ncross-dataset setups, and the results demonstrate that our proposed approach\nenhances the generalizability in most experimental setups in comparison to\nstate-of-the-art, including the methods designed explicitly for domain\nadaption/shift problems. We successfully prove the design of our proposed PAD\nsolution in a step-wise ablation study that involves our proposed learnable\nfrequency decomposition, our hierarchical attention module design, and the used\nloss function. Training codes and pre-trained models are publicly released\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oriented Object Detection in Aerial Images Based on Area Ratio of Parallelogram. (arXiv:2109.10187v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10187","description":"<p>Oriented object detection is a challenging task in aerial images since the\nobjects in aerial images are displayed in arbitrary directions and are\nfrequently densely packed. The mainstream detectors describe rotating objects\nusing a five-parament or eight-parament representations, which suffer from\nrepresentation ambiguity for orientated object definition. In this paper, we\npropose a novel representation method based on area ratio of parallelogram,\ncalled ARP. Specifically, ARP regresses the minimum bounding rectangle of the\noriented object and three area ratios. Three area ratios include the area ratio\nof a directed object to the smallest circumscribed rectangle and two\nparallelograms to the minimum circumscribed rectangle. It simplifies offset\nlearning and eliminates the issue of angular periodicity or label point\nsequences for oriented objects. To further remedy the confusion issue of nearly\nhorizontal objects, the area ratio between the object and its minimal\ncircumscribed rectangle is employed to guide the selection of horizontal or\noriented detection for each object. Moreover, the rotated efficient\nIntersection over Union (R-EIoU) loss with horizontal bounding box and three\narea ratios are designed to optimize the bounding box regression for rotating\nobjects. Experimental results on remote sensing datasets, including HRSC2016,\nDOTA, and UCAS-AOD, show that our method achieves superior detection\nperformance than many state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and Decomposable Average Precision for Image Retrieval. (arXiv:2110.01445v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.01445","description":"<p>In image retrieval, standard evaluation metrics rely on score ranking, e.g.\naverage precision (AP). In this paper, we introduce a method for robust and\ndecomposable average precision (ROADMAP) addressing two major challenges for\nend-to-end training of deep neural networks with AP: non-differentiability and\nnon-decomposability. Firstly, we propose a new differentiable approximation of\nthe rank function, which provides an upper bound of the AP loss and ensures\nrobust training. Secondly, we design a simple yet effective loss function to\nreduce the decomposability gap between the AP in the whole training set and its\naveraged batch approximation, for which we provide theoretical guarantees.\nExtensive experiments conducted on three image retrieval datasets show that\nROADMAP outperforms several recent AP approximation methods and highlight the\nimportance of our two contributions. Finally, using ROADMAP for training deep\nmodels yields very good performances, outperforming state-of-the-art results on\nthe three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramzi_E/0/1/0/all/0/1\">Elias Ramzi</a> (CNAM, CEDRIC - VERTIGO), <a href=\"http://arxiv.org/find/cs/1/au:+Thome_N/0/1/0/all/0/1\">Nicolas Thome</a> (CNAM, CEDRIC - VERTIGO), <a href=\"http://arxiv.org/find/cs/1/au:+Rambour_C/0/1/0/all/0/1\">Cl&#xe9;ment Rambour</a> (CNAM, CEDRIC - VERTIGO), <a href=\"http://arxiv.org/find/cs/1/au:+Audebert_N/0/1/0/all/0/1\">Nicolas Audebert</a> (CNAM, CEDRIC - VERTIGO), <a href=\"http://arxiv.org/find/cs/1/au:+Bitot_X/0/1/0/all/0/1\">Xavier Bitot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let there be a clock on the beach: Reducing Object Hallucination in Image Captioning. (arXiv:2110.01705v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01705","description":"<p>Explaining an image with missing or non-existent objects is known as object\nbias (hallucination) in image captioning. This behaviour is quite common in the\nstate-of-the-art captioning models which is not desirable by humans. To\ndecrease the object hallucination in captioning, we propose three simple yet\nefficient training augmentation method for sentences which requires no new\ntraining data or increase in the model size. By extensive analysis, we show\nthat the proposed methods can significantly diminish our models' object bias on\nhallucination metrics. Moreover, we experimentally demonstrate that our methods\ndecrease the dependency on the visual features. All of our code, configuration\nfiles and model weights will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-based Excavator Activity Analysis and Safety Monitoring System. (arXiv:2110.03083v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03083","description":"<p>In this paper, we propose an excavator activity analysis and safety\nmonitoring system, leveraging recent advancements in deep learning and computer\nvision. Our proposed system detects the surrounding environment and the\nexcavators while estimating the poses and actions of the excavators. Compared\nto previous systems, our method achieves higher accuracy in object detection,\npose estimation, and action recognition tasks. In addition, we build an\nexcavator dataset using the Autonomous Excavator System (AES) on the waste\ndisposal recycle scene to demonstrate the effectiveness of our system. We also\nevaluate our method on a benchmark construction dataset. The experimental\nresults show that the proposed action recognition approach outperforms the\nstate-of-the-art approaches on top-1 accuracy by about 5.18%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cartoon Explanations of Image Classifiers. (arXiv:2110.03485v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2110.03485","description":"<p>We present CartoonX (Cartoon Explanation), a novel model-agnostic explanation\nmethod tailored towards image classifiers and based on the rate-distortion\nexplanation (RDE) framework. Natural images are roughly piece-wise smooth\nsignals -- also called cartoon images -- and tend to be sparse in the wavelet\ndomain. CartoonX is the first explanation method to exploit this by requiring\nits explanations to be sparse in the wavelet domain, thus extracting the\n\\emph{relevant piece-wise smooth} part of an image instead of relevant\npixel-sparse regions. We demonstrate experimentally that CartoonX is not only\nhighly interpretable due to its piece-wise smooth nature but also particularly\napt at explaining misclassifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolek_S/0/1/0/all/0/1\">Stefan Kolek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1\">Ron Levie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1\">Gitta Kutyniok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Prototype Classifier for Few-shot Image Classification. (arXiv:2110.05076v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05076","description":"<p>The prototypical network is a prototype classifier based on meta-learning and\nis widely used for few-shot learning because it classifies unseen examples by\nconstructing class-specific prototypes without adjusting hyper-parameters\nduring meta-testing. Interestingly, recent research has attracted a lot of\nattention, showing that a linear classifier with fine-tuning, which does not\nuse a meta-learning algorithm, performs comparably with the prototypical\nnetwork. However, fine-tuning requires additional hyper-parameters when\nadapting a model to a new environment. In addition, although the purpose of\nfew-shot learning is to enable the model to quickly adapt to a new environment,\nfine-tuning needs to be applied every time a new class appears, making fast\nadaptation difficult. In this paper, we analyze how a prototype classifier\nworks equally well without fine-tuning and meta-learning. We experimentally\nfound that directly using the feature vector extracted using standard\npre-trained models to construct a prototype classifier in meta-testing does not\nperform as well as the prototypical network and linear classifiers with\nfine-tuning and feature vectors of pre-trained models. Thus, we derive a novel\ngeneralization bound for the prototypical network and show that focusing on the\nvariance of the norm of a feature vector can improve performance. We\nexperimentally investigated several normalization methods for minimizing the\nvariance of the norm and found that the same performance can be obtained by\nusing the L2 normalization and embedding space transformation without\nfine-tuning or meta-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1\">Mingcheng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Issei Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09113","description":"<p>Salt and pepper noise removal is a common inverse problem in image\nprocessing. Traditional denoising methods have two limitations. First, noise\ncharacteristics are often not described accurately. For example, the noise\nlocation information is often ignored and the sparsity of the salt and pepper\nnoise is often described by L1 norm, which cannot illustrate the sparse\nvariables clearly. Second, conventional methods separate the contaminated image\ninto a recovered image and a noise part, thus resulting in recovering an image\nwith unsatisfied smooth parts and detail parts. In this study, we introduce a\nnoise detection strategy to determine the position of the noise, and a\nnon-convex sparsity regularization depicted by Lp quasi-norm is employed to\ndescribe the sparsity of the noise, thereby addressing the first limitation.\nThe morphological component analysis framework with stationary Framelet\ntransform is adopted to decompose the processed image into cartoon, texture,\nand noise parts to resolve the second limitation. Then, the alternating\ndirection method of multipliers (ADMM) is employed to solve the proposed model.\nFinally, experiments are conducted to verify the proposed method and compare it\nwith some current state-of-the-art denoising methods. The experimental results\nshow that the proposed method can remove salt and pepper noise while preserving\nthe details of the processed image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yingpin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huiying Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jianhua Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Chaoqun Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanping Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-Based Detection, Classification and Prediction/Prognosis in Medical Imaging: Towards Radiophenomics. (arXiv:2110.10332v3 [physics.med-ph] UPDATED)","link":"http://arxiv.org/abs/2110.10332","description":"<p>Artificial intelligence (AI) techniques have significant potential to enable\neffective, robust and automated image phenotyping including identification of\nsubtle patterns. AI-based detection searches the image space to find the\nregions of interest based on patterns and features. There is a spectrum of\ntumor histologies from benign to malignant that can be identified by AI-based\nclassification approaches using image features. The extraction of minable\ninformation from images gives way to the field of radiomics and can be explored\nvia explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics\nanalysis has the potential to be utilized as a noninvasive technique for the\naccurate characterization of tumors to improve diagnosis and treatment\nmonitoring. This work reviews AI-based techniques, with a special focus on\noncological PET and PET/CT imaging, for different detection, classification,\nand prediction/prognosis tasks. We also discuss needed efforts to enable the\ntranslation of AI techniques to routine clinical workflows, and potential\nimprovements and complementary techniques such as the use of natural language\nprocessing on electronic health records and neuro-symbolic AI techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yousefirizi_F/0/1/0/all/0/1\">Fereshteh Yousefirizi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Decazes_P/0/1/0/all/0/1\">Pierre Decazes</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Amyar_A/0/1/0/all/0/1\">Amine Amyar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saboury_B/0/1/0/all/0/1\">Babak Saboury</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for HDR Imaging: State-of-the-Art and Future Trends. (arXiv:2110.10394v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.10394","description":"<p>High dynamic range (HDR) imaging is a technique that allows an extensive\ndynamic range of exposures, which is important in image processing, computer\ngraphics, and computer vision. In recent years, there has been a significant\nadvancement in HDR imaging using deep learning (DL). This study conducts a\ncomprehensive and insightful survey and analysis of recent developments in deep\nHDR imaging methodologies. We hierarchically and structurally group existing\ndeep HDR imaging methods into five categories based on (1) number/domain of\ninput exposures, (2) number of learning tasks, (3) novel sensor data, (4) novel\nlearning strategies, and (5) applications. Importantly, we provide a\nconstructive discussion on each category regarding its potential and\nchallenges. Moreover, we review some crucial aspects of deep HDR imaging, such\nas datasets and evaluation metrics. Finally, we highlight some open problems\nand point out future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Wearing a Face Mask on Face Image Quality. (arXiv:2110.11283v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11283","description":"<p>Due to the COVID-19 situation, face masks have become a main part of our\ndaily life. Wearing mouth-and-nose protection has been made a mandate in many\npublic places, to prevent the spread of the COVID-19 virus. However, face masks\naffect the performance of face recognition, since a large area of the face is\ncovered. The effect of wearing a face mask on the different components of the\nface recognition system in a collaborative environment is a problem that is\nstill to be fully studied. This work studies, for the first time, the effect of\nwearing a face mask on face image quality by utilising state-of-the-art face\nimage quality assessment methods of different natures. This aims at providing\nbetter understanding on the effect of face masks on the operation of face\nrecognition as a whole system. In addition, we further studied the effect of\nsimulated masks on face image utility in comparison to real face masks. We\ndiscuss the correlation between the mask effect on face image quality and that\non the face verification performance by automatic systems and human experts,\nindicating a consistent trend between both factors. The evaluation is conducted\non the database containing (1) no-masked faces, (2) real face masks, and (3)\nsimulated face masks, by synthetically generating digital facial masks on\nno-masked faces. Finally, a visual interpretation of the face areas\ncontributing to the quality score of a selected set of quality assessment\nmethods is provided to give a deeper insight into the difference of network\ndecisions in masked and non-masked faces, among other variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Biying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UVO Challenge on Video-based Open-World Segmentation 2021: 1st Place Solution. (arXiv:2110.11661v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11661","description":"<p>In this report, we introduce our (pretty straightforard) two-step\n\"detect-then-match\" video instance segmentation method. The first step performs\ninstance segmentation for each frame to get a large number of instance mask\nproposals. The second step is to do inter-frame instance mask matching with the\nhelp of optical flow. We demonstrate that with high quality mask proposals, a\nsimple matching mechanism is good enough for tracking. Our approach achieves\nthe first place in the UVO 2021 Video-based Open-World Segmentation Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parametric Variational Linear Units (PVLUs) in Deep Convolutional Networks. (arXiv:2110.12246v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12246","description":"<p>The Rectified Linear Unit is currently a state-of-the-art activation function\nin deep convolutional neural networks. To combat ReLU's dying neuron problem,\nwe propose the Parametric Variational Linear Unit (PVLU), which adds a\nsinusoidal function with trainable coefficients to ReLU. Along with introducing\nnonlinearity and non-zero gradients across the entire real domain, PVLU acts as\na mechanism of fine-tuning when implemented in the context of transfer\nlearning. On a simple, non-transfer sequential CNN, PVLU substitution allowed\nfor relative error decreases of 16.3% and 11.3% (without and with data\naugmentation) on CIFAR-100. PVLU is also tested on transfer learning models.\nThe VGG-16 and VGG-19 models experience relative error reductions of 9.5% and\n10.7% on CIFAR-10, respectively, after the substitution of ReLU with PVLU. When\ntraining on Gaussian-filtered CIFAR-10 images, similar improvements are noted\nfor the VGG models. Most notably, fine-tuning using PVLU allows for relative\nerror reductions up to and exceeding 10% for near state-of-the-art residual\nneural network architectures on the CIFAR datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aarush Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_S/0/1/0/all/0/1\">Shikhar Ahuja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion. (arXiv:2110.13746v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13746","description":"<p>We present neural radiance fields for rendering and temporal (4D)\nreconstruction of humans in motion (H-NeRF), as captured by a sparse set of\ncameras or even from a monocular video. Our approach combines ideas from neural\nscene representation, novel-view synthesis, and implicit statistical geometric\nhuman representations, coupled using novel loss functions. Instead of learning\na radiance field with a uniform occupancy prior, we constrain it by a\nstructured implicit human body model, represented using signed distance\nfunctions. This allows us to robustly fuse information from sparse views and\ngeneralize well beyond the poses or views observed in training. Moreover, we\napply geometric constraints to co-learn the structure of the observed subject\n-- including both body and clothing -- and to regularize the radiance field to\ngeometrically plausible solutions. Extensive experiments on multiple datasets\ndemonstrate the robustness and the accuracy of our approach, its generalization\ncapabilities significantly outside a small training set of poses and views, and\nstatistical extrapolation beyond the observed shape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alldieck_T/0/1/0/all/0/1\">Thiemo Alldieck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FocusFace: Multi-task Contrastive Learning for Masked Face Recognition. (arXiv:2110.14940v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14940","description":"<p>SARS-CoV-2 has presented direct and indirect challenges to the scientific\ncommunity. One of the most prominent indirect challenges advents from the\nmandatory use of face masks in a large number of countries. Face recognition\nmethods struggle to perform identity verification with similar accuracy on\nmasked and unmasked individuals. It has been shown that the performance of\nthese methods drops considerably in the presence of face masks, especially if\nthe reference image is unmasked. We propose FocusFace, a multi-task\narchitecture that uses contrastive learning to be able to accurately perform\nmasked face recognition. The proposed architecture is designed to be trained\nfrom scratch or to work on top of state-of-the-art face recognition methods\nwithout sacrificing the capabilities of a existing models in conventional face\nrecognition tasks. We also explore different approaches to design the\ncontrastive learning module. Results are presented in terms of masked-masked\n(M-M) and unmasked-masked (U-M) face verification performance. For both\nsettings, the results are on par with published methods, but for M-M\nspecifically, the proposed method was able to outperform all the solutions that\nit was compared to. We further show that when using our method on top of\nalready existing methods the training computational costs decrease\nsignificantly while retaining similar performances. The implementation and the\ntrained models are available at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1\">Pedro C. Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1\">Jo&#xe3;o Ribeiro Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sequeira_A/0/1/0/all/0/1\">Ana F. Sequeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">Jaime S. Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Image Restorer: Denoising and Luminance Adjustment for Low-photon-count Imaging. (arXiv:2110.15715v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.15715","description":"<p>Imaging under photon-scarce situations introduces challenges to many\napplications as the captured images are with low signal-to-noise ratio and poor\nluminance. In this paper, we investigate the raw image restoration under\nlow-photon-count conditions by simulating the imaging of quanta image sensor\n(QIS). We develop a lightweight framework, which consists of a multi-level\npyramid denoising network (MPDNet) and a luminance adjustment (LA) module to\nachieve separate denoising and luminance enhancement. The main component of our\nframework is the multi-skip attention residual block (MARB), which integrates\nmulti-scale feature fusion and attention mechanism for better feature\nrepresentation. Our MPDNet adopts the idea of Laplacian pyramid to learn the\nsmall-scale noise map and larger-scale high-frequency details at different\nlevels, and feature extractions are conducted on the multi-scale input images\nto encode richer contextual information. Our LA module enhances the luminance\nof the denoised image by estimating its illumination, which can better avoid\ncolor distortion. Extensive experimental results have demonstrated that our\nimage restorer can achieve superior performance on the degraded images with\nvarious photon levels by suppressing noise and recovering luminance and color\neffectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shansi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_E/0/1/0/all/0/1\">Edmund Y. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Explanations for Convolutional Neural Networks via Latent Traversal of Generative Adversarial Networks. (arXiv:2111.00116v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00116","description":"<p>Lack of explainability in artificial intelligence, specifically deep neural\nnetworks, remains a bottleneck for implementing models in practice. Popular\ntechniques such as Gradient-weighted Class Activation Mapping (Grad-CAM)\nprovide a coarse map of salient features in an image, which rarely tells the\nwhole story of what a convolutional neural network (CNN) learned. Using\nCOVID-19 chest X-rays, we present a method for interpreting what a CNN has\nlearned by utilizing Generative Adversarial Networks (GANs). Our GAN framework\ndisentangles lung structure from COVID-19 features. Using this GAN, we can\nvisualize the transition of a pair of COVID negative lungs in a chest\nradiograph to a COVID positive pair by interpolating in the latent space of the\nGAN, which provides fine-grained visualization of how the CNN responds to\nvarying features within the lungs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dravid_A/0/1/0/all/0/1\">Amil Dravid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K. Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing Families In the Wild (RFIW): The 5th Edition. (arXiv:2111.00598v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00598","description":"<p>Recognizing Families In the Wild (RFIW), held as a data challenge in\nconjunction with the 16th IEEE International Conference on Automatic Face and\nGesture Recognition (FG), is a large-scale, multi-track visual kinship\nrecognition evaluation. This is our fifth edition of RFIW, for which we\ncontinue the effort to attract scholars, bring together professionals, publish\nnew work, and discuss prospects. In this paper, we summarize submissions for\nthe three tasks of this year's RFIW: specifically, we review the results for\nkinship verification, tri-subject verification, and family member search and\nretrieval. We take a look at the RFIW problem, as well as share current efforts\nand make recommendations for promising future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joseph P. Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Can Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Ming Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turk_M/0/1/0/all/0/1\">Matthew A. Turk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Human and Machine Face Detection using a Novel Distinctive Human Appearance Dataset. (arXiv:2111.00660v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00660","description":"<p>Face detection is a long-standing challenge in the field of computer vision,\nwith the ultimate goal being to accurately localize human faces in an\nunconstrained environment. There are significant technical hurdles in making\nthese systems accurate due to confounding factors related to pose, image\nresolution, illumination, occlusion, and viewpoint [44]. That being said, with\nrecent developments in machine learning, face-detection systems have achieved\nextraordinary accuracy, largely built on data-driven deep-learning models [70].\nThough encouraging, a critical aspect that limits face-detection performance\nand social responsibility of deployed systems is the inherent diversity of\nhuman appearance. Every human appearance reflects something unique about a\nperson, including their heritage, identity, experiences, and visible\nmanifestations of self-expression. However, there are questions about how well\nface-detection systems perform when faced with varying face size and shape,\nskin color, body modification, and body ornamentation. Towards this goal, we\ncollected the Distinctive Human Appearance dataset, an image set that\nrepresents appearances with low frequency and that tend to be undersampled in\nface datasets. Then, we evaluated current state-of-the-art face-detection\nmodels in their ability to detect faces in these images. The evaluation results\nshow that face-detection algorithms do not generalize well to these diverse\nappearances. Evaluating and characterizing the state of current face-detection\nmodels will accelerate research and development towards creating fairer and\nmore accurate face-detection systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurkan_N/0/1/0/all/0/1\">Necdet Gurkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suchow_J/0/1/0/all/0/1\">Jordan W. Suchow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Object Detectors with Feature Richness. (arXiv:2111.00674v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00674","description":"<p>In recent years, large-scale deep models have achieved great success, but the\nhuge computational complexity and massive storage requirements make it a great\nchallenge to deploy them in resource-limited devices. As a model compression\nand acceleration method, knowledge distillation effectively improves the\nperformance of small models by transferring the dark knowledge from the teacher\ndetector. However, most of the existing distillation-based detection methods\nmainly imitating features near bounding boxes, which suffer from two\nlimitations. First, they ignore the beneficial features outside the bounding\nboxes. Second, these methods imitate some features which are mistakenly\nregarded as the background by the teacher detector. To address the above\nissues, we propose a novel Feature-Richness Score (FRS) method to choose\nimportant features that improve generalized detectability during distilling.\nThe proposed method effectively retrieves the important features outside the\nbounding boxes and removes the detrimental features within the bounding boxes.\nExtensive experiments show that our methods achieve excellent performance on\nboth anchor-based and anchor-free detectors. For example, RetinaNet with\nResNet-50 achieves 39.7% in mAP on the COCO2017 dataset, which even surpasses\nthe ResNet-101 based teacher detector 38.9% by 0.8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhixing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaPool: Exponential Adaptive Pooling for Information-Retaining Downsampling. (arXiv:2111.00772v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00772","description":"<p>Pooling layers are essential building blocks of Convolutional Neural Networks\n(CNNs) that reduce computational overhead and increase the receptive fields of\nproceeding convolutional operations. They aim to produce downsampled volumes\nthat closely resemble the input volume while, ideally, also being\ncomputationally and memory efficient. It is a challenge to meet both\nrequirements jointly. To this end, we propose an adaptive and exponentially\nweighted pooling method named adaPool. Our proposed method uses a parameterized\nfusion of two sets of pooling kernels that are based on the exponent of the\nDice-Sorensen coefficient and the exponential maximum, respectively. A key\nproperty of adaPool is its bidirectional nature. In contrast to common pooling\nmethods, weights can be used to upsample a downsampled activation map. We term\nthis method adaUnPool. We demonstrate how adaPool improves the preservation of\ndetail through a range of tasks including image and video classification and\nobject detection. We then evaluate adaUnPool on image and video frame\nsuper-resolution and frame interpolation tasks. For benchmarking, we introduce\nInter4K, a novel high-quality, high frame-rate video dataset. Our combined\nexperiments demonstrate that adaPool systematically achieves better results\nacross tasks and backbone architectures, while introducing a minor additional\ncomputational and memory overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stergiou_A/0/1/0/all/0/1\">Alexandros Stergiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poppe_R/0/1/0/all/0/1\">Ronald Poppe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Livestock Monitoring with Transformer. (arXiv:2111.00801v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00801","description":"<p>Tracking the behaviour of livestock enables early detection and thus\nprevention of contagious diseases in modern animal farms. Apart from economic\ngains, this would reduce the amount of antibiotics used in livestock farming\nwhich otherwise enters the human diet exasperating the epidemic of antibiotic\nresistance - a leading cause of death. We could use standard video cameras,\navailable in most modern farms, to monitor livestock. However, most computer\nvision algorithms perform poorly on this task, primarily because, (i) animals\nbred in farms look identical, lacking any obvious spatial signature, (ii) none\nof the existing trackers are robust for long duration, and (iii) real-world\nconditions such as changing illumination, frequent occlusion, varying camera\nangles, and sizes of the animals make it hard for models to generalize. Given\nthese challenges, we develop an end-to-end behaviour monitoring system for\ngroup-housed pigs to perform simultaneous instance level segmentation,\ntracking, action recognition and re-identification (STAR) tasks. We present\nstarformer, the first end-to-end multiple-object livestock monitoring framework\nthat learns instance-level embeddings for grouped pigs through the use of\ntransformer architecture. For benchmarking, we present Pigtrace, a carefully\ncurated dataset comprising video sequences with instance level bounding box,\nsegmentation, tracking and activity classification of pigs in real indoor\nfarming environment. Using simultaneous optimization on STAR tasks we show that\nstarformer outperforms popular baseline models trained for individual tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tangirala_B/0/1/0/all/0/1\">Bhavesh Tangirala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_I/0/1/0/all/0/1\">Ishan Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laszlo_D/0/1/0/all/0/1\">Daniel Laszlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak K. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_R/0/1/0/all/0/1\">Rajat M. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arya_D/0/1/0/all/0/1\">Devanshu Arya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Fashion: A Review of AI Applications in the Fashion & Apparel Industry. (arXiv:2111.00905v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00905","description":"<p>The fashion industry is on the verge of an unprecedented change. The\nimplementation of machine learning, computer vision, and artificial\nintelligence (AI) in fashion applications is opening lots of new opportunities\nfor this industry. This paper provides a comprehensive survey on this matter,\ncategorizing more than 580 related articles into 22 well-defined\nfashion-related tasks. Such structured task-based multi-label classification of\nfashion research articles provides researchers with explicit research\ndirections and facilitates their access to the related studies, improving the\nvisibility of studies simultaneously. For each task, a time chart is provided\nto analyze the progress through the years. Furthermore, we provide a list of 86\npublic fashion datasets accompanied by a list of suggested applications and\nadditional information for each.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1\">Seyed Omid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalhor_A/0/1/0/all/0/1\">Ahmad Kalhor</a> (University of Tehran, College of Engineering, School of Electrical and Computer Engineering, Tehran, Iran)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Multiple Instance Learning with Attention Mechanisms. (arXiv:2111.00947v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.00947","description":"<p>Multiple instance learning (MIL) is a type of weakly supervised learning\nwhere multiple instances of data with unknown labels are sorted into bags.\nSince knowledge about the individual instances is incomplete, labels are\nassigned to the bags containing the instances. While this method fits diverse\napplications were labelled data is scarce, it lacks depth for solving more\ncomplex scenarios where associations between sets of instances have to be made,\nlike finding relevant regions of interest in an image or detecting events in a\nset of time-series signals. Nested MIL considers labelled bags within bags,\nwhere only the outermost bag is labelled and inner-bags and instances are\nrepresented as latent labels. In addition, we propose using an attention\nmechanism to add interpretability, providing awareness into the impact of each\ninstance to the weak bag label. Experiments in classical image datasets show\nthat our proposed model provides high accuracy performance as well as spotting\nrelevant instances on image regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fuster_S/0/1/0/all/0/1\">Saul Fuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eftestol_T/0/1/0/all/0/1\">Trygve Eftest&#xf8;l</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engan_K/0/1/0/all/0/1\">Kjersti Engan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness of deep learning algorithms in astronomy -- galaxy morphology studies. (arXiv:2111.00961v2 [astro-ph.GA] UPDATED)","link":"http://arxiv.org/abs/2111.00961","description":"<p>Deep learning models are being increasingly adopted in wide array of\nscientific domains, especially to handle high-dimensionality and volume of the\nscientific data. However, these models tend to be brittle due to their\ncomplexity and overparametrization, especially to the inadvertent adversarial\nperturbations that can appear due to common image processing such as\ncompression or blurring that are often seen with real scientific data. It is\ncrucial to understand this brittleness and develop models robust to these\nadversarial perturbations. To this end, we study the effect of observational\nnoise from the exposure time, as well as the worst case scenario of a one-pixel\nattack as a proxy for compression or telescope errors on performance of\nResNet18 trained to distinguish between galaxies of different morphologies in\nLSST mock data. We also explore how domain adaptation techniques can help\nimprove model robustness in case of this type of naturally occurring attacks\nand help scientists build more trustworthy and stable models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Ciprijanovic_A/0/1/0/all/0/1\">A. &#x106;iprijanovi&#x107;</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kafkes_D/0/1/0/all/0/1\">D. Kafkes</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Perdue_G/0/1/0/all/0/1\">G. N. Perdue</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Pedro_K/0/1/0/all/0/1\">K. Pedro</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Snyder_G/0/1/0/all/0/1\">G. Snyder</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sanchez_F/0/1/0/all/0/1\">F. J. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Madireddy_S/0/1/0/all/0/1\">S. Madireddy</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wild_S/0/1/0/all/0/1\">S. M. Wild</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Nord_B/0/1/0/all/0/1\">B. Nord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign-to-Speech Model for Sign Language Understanding: A Case Study of Nigerian Sign Language. (arXiv:2111.00995v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00995","description":"<p>Through this paper, we seek to reduce the communication barrier between the\nhearing-impaired community and the larger society who are usually not familiar\nwith sign language in the sub-Saharan region of Africa with the largest\noccurrences of hearing disability cases, while using Nigeria as a case study.\nThe dataset is a pioneer dataset for the Nigerian Sign Language and was created\nin collaboration with relevant stakeholders. We pre-processed the data in\nreadiness for two different object detection models and a classification model\nand employed diverse evaluation metrics to gauge model performance on\nsign-language to text conversion tasks. Finally, we convert the predicted sign\ntexts to speech and deploy the best performing model in a lightweight\napplication that works in real-time and achieves impressive results converting\nsign words/phrases to text and subsequently, into speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolawole_S/0/1/0/all/0/1\">Steven Kolawole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osakuade_O/0/1/0/all/0/1\">Opeyemi Osakuade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_N/0/1/0/all/0/1\">Nayan Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olorisade_B/0/1/0/all/0/1\">Babatunde Kazeem Olorisade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-11-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}