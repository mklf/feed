{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"An Introduction to Automatic Differentiation forMachine Learning. (arXiv:2110.06209v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06209","description":"<p>Machine learning and neural network models in particular have been improving\nthe state of the art performance on many artificial intelligence related tasks.\nNeural network models are typically implemented using frameworks that perform\ngradient based optimization methods to fit a model to a dataset. These\nframeworks use a technique of calculating derivatives called automatic\ndifferentiation (AD) which removes the burden of performing derivative\ncalculations from the model designer. In this report we describe AD, its\nmotivations, and different implementation approaches. We briefly describe\ndataflow programming as it relates to AD. Lastly, we present example programs\nthat are implemented with Tensorflow and PyTorch, which are two commonly used\nAD frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">Davan Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Effect of Natural Language Explanations on Out-of-Distribution Generalization in Few-shot NLI. (arXiv:2110.06223v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06223","description":"<p>Although neural models have shown strong performance in datasets such as\nSNLI, they lack the ability to generalize out-of-distribution (OOD). In this\nwork, we formulate a few-shot learning setup and examine the effects of natural\nlanguage explanations on OOD generalization. We leverage the templates in the\nHANS dataset and construct templated natural language explanations for each\ntemplate. Although generated explanations show competitive BLEU scores against\ngroundtruth explanations, they fail to improve prediction performance. We\nfurther show that generated explanations often hallucinate information and miss\nkey elements that indicate the label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangqiaoyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Summarization using Restricted Self-Attention. (arXiv:2110.06263v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06263","description":"<p>Speech summarization is typically performed by using a cascade of speech\nrecognition and text summarization models. End-to-end modeling of speech\nsummarization models is challenging due to memory and compute constraints\narising from long input audio sequences. Recent work in document summarization\nhas inspired methods to reduce the complexity of self-attentions, which enables\ntransformer models to handle long sequences. In this work, we introduce a\nsingle model optimized end-to-end for speech summarization. We apply the\nrestricted self-attention technique from text-based models to speech models to\naddress the memory and compute constraints. We demonstrate that the proposed\nmodel learns to directly summarize speech for the How-2 corpus of instructional\nvideos. The proposed end-to-end model outperforms the previously proposed\ncascaded model by 3 points absolute on ROUGE. Further, we consider the spoken\nlanguage understanding task of predicting concepts from speech inputs and show\nthat the proposed end-to-end model outperforms the cascade model by 4 points\nabsolute F-1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Roshan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palaskar_S/0/1/0/all/0/1\">Shruti Palaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sm{\\aa}prat: DialoGPT for Natural Language Generation of Swedish Dialogue by Transfer Learning. (arXiv:2110.06273v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06273","description":"<p>Building open-domain conversational systems (or chatbots) that produce\nconvincing responses is a recognized challenge. Recent state-of-the-art (SoTA)\ntransformer-based models for the generation of natural language dialogue have\ndemonstrated impressive performance in simulating human-like, single-turn\nconversations in English. This work investigates, by an empirical study, the\npotential for transfer learning of such models to Swedish language. DialoGPT,\nan English language pre-trained model, is adapted by training on three\ndifferent Swedish language conversational datasets obtained from publicly\navailable sources. Perplexity score (an automated intrinsic language model\nmetric) and surveys by human evaluation were used to assess the performances of\nthe fine-tuned models, with results that indicate that the capacity for\ntransfer learning can be exploited with considerable success. Human evaluators\nasked to score the simulated dialogue judged over 57% of the chatbot responses\nto be human-like for the model trained on the largest (Swedish) dataset. We\nprovide the demos and model checkpoints of our English and Swedish chatbots on\nthe HuggingFace platform for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_N/0/1/0/all/0/1\">Nosheen Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pahlavan_M/0/1/0/all/0/1\">Maryam Pahlavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brannvall_R/0/1/0/all/0/1\">Rickard Br&#xe4;nnvall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabry_S/0/1/0/all/0/1\">Sana Sabah Sabry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiST: Lite Self-training Makes Efficient Few-shot Learners. (arXiv:2110.06274v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06274","description":"<p>We present a new method LiST for efficient fine-tuning of large pre-trained\nlanguage models (PLMs) in few-shot learning settings. LiST significantly\nimproves over recent methods that adopt prompt fine-tuning using two key\ntechniques. The first one is the use of self-training to leverage large amounts\nof unlabeled data for prompt-tuning to significantly boost the model\nperformance in few-shot settings. We use self-training in conjunction with\nmeta-learning for re-weighting noisy pseudo-prompt labels. However, traditional\nself-training is expensive as it requires updating all the model parameters\nrepetitively. Therefore, we use a second technique for light-weight fine-tuning\nwhere we introduce a small number of task-specific adapter parameters that are\nfine-tuned during self-training while keeping the PLM encoder frozen. This also\nsignificantly reduces the overall model footprint across several tasks that can\nnow share a common PLM encoder as backbone for inference. Combining the above\ntechniques, LiST not only improves the model performance for few-shot learning\non target domains but also reduces the model memory footprint. We present a\ncomprehensive study on six NLU tasks to validate the effectiveness of LiST. The\nresults show that LiST improves by 35% over classic fine-tuning methods and 6%\nover prompt-tuning with 96% reduction in number of trainable parameters when\nfine-tuned with no more than 30 labeled examples from each target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S3PRL-VC: Open-source Voice Conversion Framework with Self-supervised Speech Representations. (arXiv:2110.06280v1 [cs.SD])","link":"http://arxiv.org/abs/2110.06280","description":"<p>This paper introduces S3PRL-VC, an open-source voice conversion (VC)\nframework based on the S3PRL toolkit. In the context of recognition-synthesis\nVC, self-supervised speech representation (S3R) is valuable in its potential to\nreplace the expensive supervised representation adopted by state-of-the-art VC\nsystems. Moreover, we claim that VC is a good probing task for S3R analysis. In\nthis work, we provide a series of in-depth analyses by benchmarking on the two\ntasks in VCC2020, namely intra-/cross-lingual any-to-one (A2O) VC, as well as\nan any-to-any (A2A) setting. We also provide comparisons between not only\ndifferent S3Rs but also top systems in VCC2020 with supervised representations.\nSystematic objective and subjective evaluation were conducted, and we show that\nS3R is comparable with VCC2020 top systems in the A2O setting in terms of\nsimilarity, and achieves state-of-the-art in S3R-based A2A VC. We believe the\nextensive analysis, as well as the toolkit itself, contribute to not only the\nS3R community but also the VC community. The codebase is now open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decision-Theoretic Question Generation for Situated Reference Resolution: An Empirical Study and Computational Model. (arXiv:2110.06288v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06288","description":"<p>Dialogue agents that interact with humans in situated environments need to\nmanage referential ambiguity across multiple modalities and ask for help as\nneeded. However, it is not clear what kinds of questions such agents should ask\nnor how the answers to such questions can be used to resolve ambiguity. To\naddress this, we analyzed dialogue data from an interactive study in which\nparticipants controlled a virtual robot tasked with organizing a set of tools\nwhile engaging in dialogue with a live, remote experimenter. We discovered a\nnumber of novel results, including the distribution of question types used to\nresolve ambiguity and the influence of dialogue-level factors on the reference\nresolution process. Based on these empirical findings we: (1) developed a\ncomputational model for clarification requests using a decision network with an\nentropy-based utility assignment method that operates across modalities, (2)\nevaluated the model, showing that it outperforms a slot-filling baseline in\nenvironments of varying ambiguity, and (3) interpreted the results to offer\ninsight into the ways that agents can ask questions to facilitate situated\nreference resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gervits_F/0/1/0/all/0/1\">Felix Gervits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briggs_G/0/1/0/all/0/1\">Gordon Briggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roque_A/0/1/0/all/0/1\">Antonio Roque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadomatsu_G/0/1/0/all/0/1\">Genki A. Kadomatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thurston_D/0/1/0/all/0/1\">Dean Thurston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheutz_M/0/1/0/all/0/1\">Matthias Scheutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marge_M/0/1/0/all/0/1\">Matthew Marge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained style control in Transformer-based Text-to-speech Synthesis. (arXiv:2110.06306v1 [eess.AS])","link":"http://arxiv.org/abs/2110.06306","description":"<p>In this paper, we present a novel architecture to realize fine-grained style\ncontrol on the transformer-based text-to-speech synthesis (TransformerTTS).\nSpecifically, we model the speaking style by extracting a time sequence of\nlocal style tokens (LST) from the reference speech. The existing content\nencoder in TransformerTTS is then replaced by our designed cross-attention\nblocks for fusion and alignment between content and style. As the fusion is\nperformed along with the skip connection, our cross-attention block provides a\ngood inductive bias to gradually infuse the phoneme representation with a given\nstyle. Additionally, we prevent the style embedding from encoding linguistic\ncontent by randomly truncating LST during training and using wav2vec 2.0\nfeatures. Experiments show that with fine-grained style control, our system\nperforms better in terms of naturalness, intelligibility, and style\ntransferability. Our code and samples are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Wav2vec 2.0 fine-tuning for improved speech emotion recognition. (arXiv:2110.06309v1 [eess.AS])","link":"http://arxiv.org/abs/2110.06309","description":"<p>While wav2vec 2.0 has been proposed for speech recognition (ASR), it can also\nbe used for speech emotion recognition (SER); its performance can be\nsignificantly improved using different fine-tuning strategies. Two baseline\nmethods, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT) are\nfirst presented. We show that V-FT is able to outperform state-of-the-art\nmodels on the IEMOCAP dataset. TAPT, an existing NLP fine-tuning strategy,\nfurther improves the performance on SER. We also introduce a novel fine-tuning\nmethod termed P-TAPT, which modifies the TAPT objective to learn contextualized\nemotion representations. Experiments show that P-TAPT performs better than TAPT\nespecially under low-resource settings. Compared to prior works in this\nliterature, our top-line system achieved a 7.4% absolute improvement on\nunweighted accuracy (UA) over the state-of-the-art performance on IEMOCAP. Our\ncode is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Compact Metrics for MT. (arXiv:2110.06341v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06341","description":"<p>Recent developments in machine translation and multilingual text generation\nhave led researchers to adopt trained metrics such as COMET or BLEURT, which\ntreat evaluation as a regression problem and use representations from\nmultilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on\nrelated tasks suggest that these models are most efficient when they are large,\nwhich is costly and impractical for evaluation. We investigate the trade-off\nbetween multilinguality and model capacity with RemBERT, a state-of-the-art\nmultilingual language model, using data from the WMT Metrics Shared Task. We\npresent a series of experiments which show that model size is indeed a\nbottleneck for cross-lingual transfer, then demonstrate how distillation can\nhelp addressing this bottleneck, by leveraging synthetic data generation and\ntransferring knowledge from one teacher to multiple students trained on related\nlanguages. Our method yields up to 10.5% improvement over vanilla fine-tuning\nand reaches 92.6% of RemBERT's performance using only a third of its\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_A/0/1/0/all/0/1\">Amy Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell Me How to Survey: Literature Review Made Simple with Automatic Reading Path Generation. (arXiv:2110.06354v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06354","description":"<p>Recent years have witnessed the dramatic growth of paper volumes with plenty\nof new research papers published every day, especially in the area of computer\nscience. How to glean papers worth reading from the massive literature to do a\nquick survey or keep up with the latest advancement about a specific research\ntopic has become a challenging task. Existing academic search engines such as\nGoogle Scholar return relevant papers by individually calculating the relevance\nbetween each paper and query. However, such systems usually omit the\nprerequisite chains of a research topic and cannot form a meaningful reading\npath. In this paper, we introduce a new task named Reading Path Generation\n(RPG) which aims at automatically producing a path of papers to read for a\ngiven query. To serve as a research benchmark, we further propose SurveyBank, a\ndataset consisting of large quantities of survey papers in the field of\ncomputer science as well as their citation relationships. Each survey paper\ncontains key phrases extracted from its title and multi-level reading lists\ninferred from its references. Furthermore, we propose a\ngraph-optimization-based approach for reading path generation which takes the\nrelationship between papers into account. Extensive evaluations demonstrate\nthat our approach outperforms other baselines. A Real-time Reading Path\nGeneration System (RePaGer) has been also implemented with our designed model.\nTo the best of our knowledge, we are the first to target this important\nresearch problem. Our source code of RePaGer system and SurveyBank dataset can\nbe found on here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiayuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zijing Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangyang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Masking for Temporal Language Models. (arXiv:2110.06366v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06366","description":"<p>Our world is constantly evolving, and so is the content on the web.\nConsequently, our languages, often said to mirror the world, are dynamic in\nnature. However, most current contextual language models are static and cannot\nadapt to changes over time. In this work, we propose a temporal contextual\nlanguage model called TempoBERT, which uses time as an additional context of\ntexts. Our technique is based on modifying texts with temporal information and\nperforming time masking - specific masking for the supplementary time\ninformation. We leverage our approach for the tasks of semantic change\ndetection and sentence time prediction, experimenting on diverse datasets in\nterms of time, size, genre, and language. Our extensive evaluation shows that\nboth tasks benefit from exploiting time masking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_G/0/1/0/all/0/1\">Guy D. Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_I/0/1/0/all/0/1\">Ido Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALL Dolphins Are Intelligent and SOME Are Friendly: Probing BERT for Nouns' Semantic Properties and their Prototypicality. (arXiv:2110.06376v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06376","description":"<p>Large scale language models encode rich commonsense knowledge acquired\nthrough exposure to massive data during pre-training, but their understanding\nof entities and their semantic properties is unclear. We probe BERT (Devlin et\nal., 2019) for the properties of English nouns as expressed by adjectives that\ndo not restrict the reference scope of the noun they modify (as in \"red car\"),\nbut instead emphasise some inherent aspect (\"red strawberry\"). We base our\nstudy on psycholinguistics datasets that capture the association strength\nbetween nouns and their semantic features. We probe BERT using cloze tasks and\nin a classification setting, and show that the model has marginal knowledge of\nthese features and their prevalence as expressed in these datasets. We discuss\nfactors that make evaluation challenging and impede drawing general conclusions\nabout the models' knowledge of noun properties. Finally, we show that when\ntested in a fine-tuning setting addressing entailment, BERT successfully\nleverages the information needed for reasoning about the meaning of\nadjective-noun constructions outperforming previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soler_A/0/1/0/all/0/1\">Aina Gar&#xed; Soler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoNLU: Detecting, root-causing, and fixing NLU model errors. (arXiv:2110.06384v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06384","description":"<p>Improving the quality of Natural Language Understanding (NLU) models, and\nmore specifically, task-oriented semantic parsing models, in production is a\ncumbersome task. In this work, we present a system called AutoNLU, which we\ndesigned to scale the NLU quality improvement process. It adds automation to\nthree key steps: detection, attribution, and correction of model errors, i.e.,\nbugs. We detected four times more failed tasks than with random sampling,\nfinding that even a simple active learning sampling method on an uncalibrated\nmodel is surprisingly effective for this purpose. The AutoNLU tool empowered\nlinguists to fix ten times more semantic parsing bugs than with prior manual\nprocesses, auto-correcting 65% of all identified bugs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sethi_P/0/1/0/all/0/1\">Pooja Sethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savenkov_D/0/1/0/all/0/1\">Denis Savenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1\">Forough Arabshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goetz_J/0/1/0/all/0/1\">Jack Goetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolliver_M/0/1/0/all/0/1\">Micaela Tolliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_N/0/1/0/all/0/1\">Nicolas Scheffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabul_I/0/1/0/all/0/1\">Ilknur Kabul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1\">Ahmed Aly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization. (arXiv:2110.06388v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06388","description":"<p>To capture the semantic graph structure from raw text, most existing\nsummarization approaches are built on GNNs with a pre-trained model. However,\nthese methods suffer from cumbersome procedures and inefficient computations\nfor long-text documents. To mitigate these issues, this paper proposes\nHETFORMER, a Transformer-based pre-trained model with multi-granularity sparse\nattentions for long-text extractive summarization. Specifically, we model\ndifferent types of semantic nodes in raw text as a potential heterogeneous\ngraph and directly learn heterogeneous relationships (edges) among nodes by\nTransformer. Extensive experiments on both single- and multi-document\nsummarization tasks show that HETFORMER achieves state-of-the-art performance\nin Rouge F1 while using less memory and fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian-Guo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-guided Generative Models for Extractive Question Answering. (arXiv:2110.06393v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06393","description":"<p>We propose a novel method for applying Transformer models to extractive\nquestion answering (QA) tasks. Recently, pretrained generative\nsequence-to-sequence (seq2seq) models have achieved great success in question\nanswering. Contributing to the success of these models are internal attention\nmechanisms such as cross-attention. We propose a simple strategy to obtain an\nextractive answer span from the generative model by leveraging the decoder\ncross-attention patterns. Viewing cross-attention as an architectural prior, we\napply joint training to further improve QA performance. Empirical results show\nthat on open-domain question answering datasets like NaturalQuestions and\nTriviaQA, our method approaches state-of-the-art performance on both generative\nand extractive inference, all while using much fewer parameters. Furthermore,\nthis strategy allows us to perform hallucination-free inference while\nconferring significant improvements to the model's ability to rerank relevant\npassages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Davis Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Ontologies and Tool Support for COVID-19 Analytics. (arXiv:2110.06397v1 [cs.SE])","link":"http://arxiv.org/abs/2110.06397","description":"<p>The outbreak of the SARS-CoV-2 pandemic of the new COVID-19 disease (COVID-19\nfor short) demands empowering existing medical, economic, and social emergency\nbackend systems with data analytics capabilities. An impediment in taking\nadvantages of data analytics in these systems is the lack of a unified\nframework or reference model. Ontologies are highlighted as a promising\nsolution to bridge this gap by providing a formal representation of COVID-19\nconcepts such as symptoms, infections rate, contact tracing, and drug\nmodelling. Ontology-based solutions enable the integration of diverse data\nsources that leads to a better understanding of pandemic data, management of\nsmart lockdowns by identifying pandemic hotspots, and knowledge-driven\ninference, reasoning, and recommendations to tackle surrounding issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Aakash Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandara_M/0/1/0/all/0/1\">Madhushi Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahmideh_M/0/1/0/all/0/1\">Mahdi Fahmideh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proper_H/0/1/0/all/0/1\">Henderik A. Proper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizzardi_G/0/1/0/all/0/1\">Giancarlo Guizzardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soar_J/0/1/0/all/0/1\">Jeffrey Soar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Natural Language Generation for Personalized Dialogue System. (arXiv:2110.06419v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06419","description":"<p>Neural conversational models have long suffered from the problem of\ninconsistency and lacking coherent personality. To address the issue,\npersona-based models capturing individual characteristics have been proposed,\nbut they still face the dilemma of model adaption and data privacy. To break\nthis dilemma, we propose a novel Federated Natural Language Generation (FedNLG)\nframework, which learns personalized representations from various dataset on\ndistributed devices, and thus implements the personalized dialogue system\nefficiently and safely. FedNLG first pre-trains parameters of standard neural\nconversational model over a large dialogue corpus, and then fine-tune the model\nparameters and persona embeddings on specific datasets, in a federated manner.\nThus, the model could simultaneously learn the persona embeddings in local\nclients and learn shared model parameters by federated aggregation, which\nachieves accuracyprivacy balance. By conducting extensive experiments, we\ndemonstrate the effectiveness of our model by pre-training model over Cornell\nMovie-Dialogs Corpus and fine-tuning the model over two TV series dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Huanli Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yong Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Graph-based Sentence Ordering with Iteratively Predicted Pairwise Orderings. (arXiv:2110.06446v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06446","description":"<p>Dominant sentence ordering models can be classified into pairwise ordering\nmodels and set-to-sequence models. However, there is little attempt to combine\nthese two types of models, which inituitively possess complementary advantages.\nIn this paper, we propose a novel sentence ordering framework which introduces\ntwo classifiers to make better use of pairwise orderings for graph-based\nsentence ordering. Specially, given an initial sentence-entity graph, we first\nintroduce a graph-based classifier to predict pairwise orderings between linked\nsentences. Then, in an iterative manner, based on the graph updated by\npreviously predicted high-confident pairwise orderings, another classifier is\nused to predict the remaining uncertain pairwise orderings. At last, we adapt a\nGRN-based sentence ordering model on the basis of final graph. Experiments on\nfive commonly-used datasets demonstrate the effectiveness and generality of our\nmodel. Particularly, when equipped with BERT and FHDecoder, our model achieves\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shaopeng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ante Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiali Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Junfeng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Degen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake News Detection in Spanish Using Deep Learning Techniques. (arXiv:2110.06461v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06461","description":"<p>This paper addresses the problem of fake news detection in Spanish using\nMachine Learning techniques. It is fundamentally the same problem tackled for\nthe English language; however, there is not a significant amount of publicly\navailable and adequately labeled fake news in Spanish to effectively train a\nMachine Learning model, similarly to those proposed for the English language.\nTherefore, this work explores different training strategies and architectures\nto establish a baseline for further research in this area. Four datasets were\nused, two in English and two in Spanish, and four experimental schemes were\ntested, including a baseline with classical Machine Learning models, trained\nand validated using a small dataset in Spanish. The remaining schemes include\nstate-of-the-art Deep Learning models trained (or fine-tuned) and validated in\nEnglish, trained and validated in Spanish, and fitted in English and validated\nwith automatic translated Spanish sentences. The Deep Learning architectures\nwere built on top of different pre-trained Word Embedding representations,\nincluding GloVe, ELMo, BERT, and BETO (a BERT version trained on a large corpus\nin Spanish). According to the results, the best strategy was a combination of a\npre-trained BETO model and a Recurrent Neural Network based on LSTM layers,\nyielding an accuracy of up to 80%; nonetheless, a baseline model using a Random\nForest estimator obtained similar outcomes. Additionally, the translation\nstrategy did not yield acceptable results because of the propagation error;\nthere was also observed a significant difference in models performance when\ntrained in English or Spanish, mainly attributable to the number of samples\navailable for each language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gallego_K/0/1/0/all/0/1\">Kevin Mart&#xed;nez-Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Ortiz_A/0/1/0/all/0/1\">Andr&#xe9;s M. &#xc1;lvarez-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_Londono_J/0/1/0/all/0/1\">Juli&#xe1;n D. Arias-Londo&#xf1;o</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ActiveEA: Active Learning for Neural Entity Alignment. (arXiv:2110.06474v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06474","description":"<p>Entity Alignment (EA) aims to match equivalent entities across different\nKnowledge Graphs (KGs) and is an essential step of KG fusion. Current\nmainstream methods -- neural EA models -- rely on training with seed alignment,\ni.e., a set of pre-aligned entity pairs which are very costly to annotate. In\nthis paper, we devise a novel Active Learning (AL) framework for neural EA,\naiming to create highly informative seed alignment to obtain more effective EA\nmodels with less annotation cost. Our framework tackles two main challenges\nencountered when applying AL to EA: (1) How to exploit dependencies between\nentities within the AL strategy. Most AL strategies assume that the data\ninstances to sample are independent and identically distributed. However,\nentities in KGs are related. To address this challenge, we propose a\nstructure-aware uncertainty sampling strategy that can measure the uncertainty\nof each entity as well as its impact on its neighbour entities in the KG. (2)\nHow to recognise entities that appear in one KG but not in the other KG (i.e.,\nbachelors). Identifying bachelors would likely save annotation budget. To\naddress this challenge, we devise a bachelor recognizer paying attention to\nalleviate the effect of sampling bias. Empirical results show that our proposed\nAL strategy can significantly improve sampling quality with good generality\nacross different datasets, EA models and amount of bachelors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scells_H/0/1/0/all/0/1\">Harrisen Scells</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Genghong Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding of Emotion Perception from Art. (arXiv:2110.06486v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06486","description":"<p>Computational modeling of the emotions evoked by art in humans is a\nchallenging problem because of the subjective and nuanced nature of art and\naffective signals. In this paper, we consider the above-mentioned problem of\nunderstanding emotions evoked in viewers by artwork using both text and visual\nmodalities. Specifically, we analyze images and the accompanying text captions\nfrom the viewers expressing emotions as a multimodal classification task. Our\nresults show that single-stream multimodal transformer-based models like MMBT\nand VisualBERT perform better compared to both image-only models and\ndual-stream multimodal models having separate pathways for text and image\nmodalities. We also observe improvements in performance for extreme positive\nand negative emotion classes, when a single-stream model like MMBT is compared\nwith a text-only transformer model like BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_D/0/1/0/all/0/1\">Digbalay Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_R/0/1/0/all/0/1\">Rimita Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dict-BERT: Enhancing Language Model Pre-training with Dictionary. (arXiv:2110.06490v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06490","description":"<p>Pre-trained language models (PLMs) aim to learn universal language\nrepresentations by conducting self-supervised training tasks on large-scale\ncorpora. Since PLMs capture word semantics in different contexts, the quality\nof word representations highly depends on word frequency, which usually follows\na heavy-tailed distributions in the pre-training corpus. Therefore, the\nembeddings of rare words on the tail are usually poorly optimized. In this\nwork, we focus on enhancing language model pre-training by leveraging\ndefinitions of the rare words in dictionaries (e.g., Wiktionary). To\nincorporate a rare word definition as a part of input, we fetch its definition\nfrom the dictionary and append it to the end of the input text sequence. In\naddition to training with the masked language modeling objective, we propose\ntwo novel self-supervised pre-training tasks on word and sentence-level\nalignment between input text sequence and rare word definitions to enhance\nlanguage modeling representation with dictionary. We evaluate the proposed\nDict-BERT model on the language understanding benchmark GLUE and eight\nspecialized domain benchmark datasets. Extensive experiments demonstrate that\nDict-BERT can significantly improve the understanding of rare words and boost\nmodel performance on various NLP downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Donghan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual COVID-19 Fake News Detection. (arXiv:2110.06495v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06495","description":"<p>The COVID-19 pandemic poses a great threat to global public health.\nMeanwhile, there is massive misinformation associated with the pandemic which\nadvocates unfounded or unscientific claims. Even major social media and news\noutlets have made an extra effort in debunking COVID-19 misinformation, most of\nthe fact-checking information is in English, whereas some unmoderated COVID-19\nmisinformation is still circulating in other languages, threatening the health\nof less-informed people in immigrant communities and developing countries. In\nthis paper, we make the first attempt to detect COVID-19 misinformation in a\nlow-resource language (Chinese) only using the fact-checked news in a\nhigh-resource language (English). We start by curating a Chinese real&amp;fake news\ndataset according to existing fact-checking information. Then, we propose a\ndeep learning framework named CrossFake to jointly encode the cross-lingual\nnews body texts and capture the news content as much as possible. Empirical\nresults on our dataset demonstrate the effectiveness of CorssFake under the\ncross-lingual setting and it also outperforms several monolingual and\ncross-lingual fake news detectors. The dataset is available at\nhttps://github.com/YingtongDou/CrossFake.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiangshu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yingtong Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Limeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Fine-tuning of Language Models. (arXiv:2110.06500v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06500","description":"<p>We give simpler, sparser, and faster algorithms for differentially private\nfine-tuning of large-scale pre-trained language models, which achieve the\nstate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.\nWe propose a meta-framework for this problem, inspired by the recent success of\nhighly parameter-efficient methods for fine-tuning. Our experiments show that\ndifferentially private adaptations of these approaches outperform previous\nprivate algorithms in three important dimensions: utility, privacy, and the\ncomputational and memory cost of private training. On many commonly studied\ndatasets, the utility of private models approaches that of non-private models.\nFor example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using\nRoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of\n$\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large\nachieves an accuracy of $90.2\\%$. Our findings are similar for natural language\ngeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,\nGPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8\nrespectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas the\nnon-private baseline is $48.1$. All our experiments suggest that larger models\nare better suited for private fine-tuning: while they are well known to achieve\nsuperior accuracy non-privately, we find that they also better maintain their\naccuracy when privacy is introduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Da Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_S/0/1/0/all/0/1\">Saurabh Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1\">Sivakanth Gopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1\">Gautam Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1\">Janardhan Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoel_A/0/1/0/all/0/1\">Andre Manoel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1\">Sergey Yekhanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient domain adaptation of language models in ASR systems using Prompt-tuning. (arXiv:2110.06502v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06502","description":"<p>Automatic Speech Recognition (ASR) systems have found their use in numerous\nindustrial applications in very diverse domains. Since domain-specific systems\nperform better than their generic counterparts on in-domain evaluation, the\nneed for memory and compute-efficient domain adaptation is obvious.\nParticularly, adapting parameter-heavy transformer-based language models used\nfor rescoring ASR hypothesis is challenging. In this work, we overcome the\nproblem using prompt-tuning, a methodology that trains a small number of domain\ntoken embedding parameters to prime a transformer-based LM to a particular\ndomain. With just a handful of extra parameters per domain, we achieve much\nbetter perplexity scores over the baseline of using an unadapted LM. Despite\nbeing parameter-efficient, these improvements are comparable to those of\nfully-fine-tuned models with hundreds of millions of parameters. We replicate\nour findings in perplexity numbers to Word Error Rate in a domain-specific ASR\nsystem for one such domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dingliwal_S/0/1/0/all/0/1\">Saket Dingliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1\">Ashish Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Ravi Teja Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perception Point: Identifying Critical Learning Periods in Speech for Bilingual Networks. (arXiv:2110.06507v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06507","description":"<p>Recent studies in speech perception have been closely linked to fields of\ncognitive psychology, phonology, and phonetics in linguistics. During\nperceptual attunement, a critical and sensitive developmental trajectory has\nbeen examined in bilingual and monolingual infants where they can best\ndiscriminate common phonemes. In this paper, we compare and identify these\ncognitive aspects on deep neural-based visual lip-reading models. We conduct\nexperiments on the two most extensive public visual speech recognition datasets\nfor English and Mandarin. Through our experimental results, we observe a strong\ncorrelation between these theories in cognitive psychology and our unique\nmodeling. We inspect how these computational models develop similar phases in\nspeech perception and acquisitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saraswat_A/0/1/0/all/0/1\">Anuj Saraswat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_M/0/1/0/all/0/1\">Mehar Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dawn of Quantum Natural Language Processing. (arXiv:2110.06510v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06510","description":"<p>In this paper, we discuss the initial attempts at boosting understanding\nhuman language based on deep-learning models with quantum computing. We\nsuccessfully train a quantum-enhanced Long Short-Term Memory network to perform\nthe parts-of-speech tagging task via numerical simulations. Moreover, a\nquantum-enhanced Transformer is proposed to perform the sentiment analysis\nbased on the existing dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sipio_R/0/1/0/all/0/1\">Riccardo Di Sipio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Hong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Samuel Yen-Chi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangini_S/0/1/0/all/0/1\">Stefano Mangini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventBERT: A Pre-Trained Model for Event Correlation Reasoning. (arXiv:2110.06533v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06533","description":"<p>Event correlation reasoning infers whether a natural language paragraph\ncontaining multiple events conforms to human common sense. For example, \"Andrew\nwas very drowsy, so he took a long nap, and now he is very alert\" is sound and\nreasonable. In contrast, \"Andrew was very drowsy, so he stayed up a long time,\nnow he is very alert\" does not comply with human common sense. Such reasoning\ncapability is essential for many downstream tasks, such as script reasoning,\nabductive reasoning, narrative incoherence, story cloze test, etc. However,\nconducting event correlation reasoning is challenging due to a lack of large\namounts of diverse event-based knowledge and difficulty in capturing\ncorrelation among multiple events. In this paper, we propose EventBERT, a\npre-trained model to encapsulate eventuality knowledge from unlabeled text.\nSpecifically, we collect a large volume of training examples by identifying\nnatural language paragraphs that describe multiple correlated events and\nfurther extracting event spans in an unsupervised manner. We then propose three\nnovel event- and correlation-based learning objectives to pre-train an event\ncorrelation model on our created training corpus. Empirical results show\nEventBERT outperforms strong baselines on four downstream tasks, and achieves\nSoTA results on most of them. Besides, it outperforms existing pre-trained\nmodels by a large margin, e.g., 6.5~23%, in zero-shot learning of these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and the growth of margin. To counteract this deficiency, we\npropose to reward well-classified examples with additive bonuses to revive\ntheir contribution to learning. This counterexample theoretically addresses\nthese three issues. We empirically support this claim by directly verify the\ntheoretical results or through the significant performance improvement with our\ncounterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that\nbecause our idea can solve these three issues, we can deal with complex\nscenarios, such as imbalanced classification, OOD detection, and applications\nunder adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple or Complex? Complexity-Controllable Question Generation with Soft Templates and Deep Mixture of Experts Model. (arXiv:2110.06560v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06560","description":"<p>The ability to generate natural-language questions with controlled complexity\nlevels is highly desirable as it further expands the applicability of question\ngeneration. In this paper, we propose an end-to-end neural\ncomplexity-controllable question generation model, which incorporates a mixture\nof experts (MoE) as the selector of soft templates to improve the accuracy of\ncomplexity control and the quality of generated questions. The soft templates\ncapture question similarity while avoiding the expensive construction of actual\ntemplates. Our method introduces a novel, cross-domain complexity estimator to\nassess the complexity of a question, taking into account the passage, the\nquestion, the answer and their interactions. The experimental results on two\nbenchmark QA datasets demonstrate that our QG model is superior to\nstate-of-the-art methods in both automatic and manual evaluation. Moreover, our\ncomplexity estimator is significantly more accurate than the baselines in both\nin-domain and out-domain settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sheng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shirong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yinlin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators. (arXiv:2110.06609v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06609","description":"<p>Pre-trained language models have recently been shown to be able to perform\ntranslation without finetuning via prompting. Inspired by these findings, we\nstudy improving the performance of pre-trained language models on translation\ntasks, where training neural machine translation models is the current de facto\napproach. We present Multi-Stage Prompting, a simple and lightweight approach\nfor better adapting pre-trained language models to translation tasks. To make\npre-trained language models better translators, we divide the translation\nprocess via pre-trained language models into three separate stages: the\nencoding stage, the re-encoding stage, and the decoding stage. During each\nstage, we independently apply different continuous prompts for allowing\npre-trained language models better adapting to translation tasks. We conduct\nextensive experiments on low-, medium-, and high-resource translation tasks.\nExperiments show that our method can significantly improve the translation\nperformance of pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Dense Retrieval for Dialogue Response Selection. (arXiv:2110.06612v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06612","description":"<p>Recent research on dialogue response selection has been mainly focused on\nselecting a proper response from a pre-defined small set of candidates using\nsophisticated neural models. Due to their heavy computational overhead, they\nare unable to select responses from a large candidate pool. In this study, we\npresent a solution to directly select proper responses from a large corpus or\neven a nonparallel corpus that only consists of unpaired sentences, using a\ndense retrieval model. We extensively test our proposed approach under two\nexperiment settings: (i) re-rank experiment that aims to rank a small set of\npre-defined candidates; (ii) full-rank experiment where the target is to\ndirectly select proper responses from a full candidate pool that may contain\nmillions of candidates. For re-rank setting, the superiority is quite\nsurprising given its simplicity. For full-rank setting, we can emphasize that\nwe are the first to do such evaluation. Moreover, human evaluation results show\nthat increasing the size of nonparallel corpus leads to further improvement of\nour model performance\\footnote{All our source codes, models and other related\nresources are publically available at\n\\url{https://github.com/gmftbyGMFTBY/SimpleReDial-v1}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximizing Efficiency of Language Model Pre-training for Learning Representation. (arXiv:2110.06620v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06620","description":"<p>Pre-trained language models in the past years have shown exponential growth\nin model parameters and compute time. ELECTRA is a novel approach for improving\nthe compute efficiency of pre-trained language models (e.g. BERT) based on\nmasked language modeling (MLM) by addressing the sample inefficiency problem\nwith the replaced token detection (RTD) task. Our work proposes adaptive early\nexit strategy to maximize the efficiency of the pre-training process by\nrelieving the model's subsequent layers of the need to process latent features\nby leveraging earlier layer representations. Moreover, we evaluate an initial\napproach to the problem that has not succeeded in maintaining the accuracy of\nthe model while showing a promising compute efficiency by thoroughly\ninvestigating the necessity of the generator module of ELECTRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Junmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Suwon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeonghwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaeyoung Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myaeng_S/0/1/0/all/0/1\">Sung-Hyon Myaeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end translation of human neural activity to speech with a dual-dual generative adversarial network. (arXiv:2110.06634v1 [cs.SD])","link":"http://arxiv.org/abs/2110.06634","description":"<p>In a recent study of auditory evoked potential (AEP) based brain-computer\ninterface (BCI), it was shown that, with an encoder-decoder framework, it is\npossible to translate human neural activity to speech (T-CAS). However, current\nencoder-decoder-based methods achieve T-CAS often with a two-step method where\nthe information is passed between the encoder and decoder with a shared\ndimension reduction vector, which may result in a loss of information. A\npotential approach to this problem is to design an end-to-end method by using a\ndual generative adversarial network (DualGAN) without dimension reduction of\npassing information, but it cannot realize one-to-one signal-to-signal\ntranslation (see Fig.1 (a) and (b)). In this paper, we propose an end-to-end\nmodel to translate human neural activity to speech directly, create a new\nelectroencephalogram (EEG) datasets for participants with good attention by\ndesign a device to detect participants' attention, and introduce a dual-dual\ngenerative adversarial network (Dual-DualGAN) (see Fig. 1 (c) and (d)) to\naddress an end-to-end translation of human neural activity to speech (ET-CAS)\nproblem by group labelling EEG signals and speech signals, inserting a\ntransition domain to realize cross-domain mapping. In the transition domain,\nthe transition signals are cascaded by the corresponding EEG and speech signals\nin a certain proportion, which can build bridges for EEG and speech signals\nwithout corresponding features, and realize one-to-one cross-domain\nEEG-to-speech translation. The proposed method can translate word-length and\nsentence-length sequences of neural activity to speech. Experimental evaluation\nhas been conducted to show that the proposed method significantly outperforms\nstate-of-the-art methods on both words and sentences of auditory stimulus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yina Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhenying Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Anhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction. (arXiv:2110.06651v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06651","description":"<p>Keyphrases are phrases in a document providing a concise summary of core\ncontent, helping readers to understand what the article is talking about in a\nminute. However, existing unsupervised works are not robust enough to handle\nvarious types of documents owing to the mismatch of sequence length for\ncomparison. In this paper, we propose a novel unsupervised keyword extraction\nmethod by leveraging the BERT-based model to select and rank candidate\nkeyphrases with a MASK strategy. In addition, we further enhance the model,\ndenoted as Keyphrases Extraction BERT (KPEBERT), via designing a compatible\nself-supervised task and conducting a contrast learning. We conducted extensive\nexperimental evaluation to demonstrate the superiority and robustness of the\nproposed method as well as the effectiveness of KPEBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truthful AI: Developing and governing AI that does not lie. (arXiv:2110.06674v1 [cs.CY])","link":"http://arxiv.org/abs/2110.06674","description":"<p>In many contexts, lying -- the use of verbal falsehoods to deceive -- is\nharmful. While lying has traditionally been a human affair, AI systems that\nmake sophisticated verbal statements are becoming increasingly prevalent. This\nraises the question of how we should limit the harm caused by AI \"lies\" (i.e.\nfalsehoods that are actively selected for). Human truthfulness is governed by\nsocial norms and by laws (against defamation, perjury, and fraud). Differences\nbetween AI and humans present an opportunity to have more precise standards of\ntruthfulness for AI, and to have these standards rise over time. This could\nprovide significant benefits to public epistemics and the economy, and mitigate\nrisks of worst-case AI futures.\n</p>\n<p>Establishing norms or laws of AI truthfulness will require significant work\nto: (1) identify clear truthfulness standards; (2) create institutions that can\njudge adherence to those standards; and (3) develop AI systems that are\nrobustly truthful.\n</p>\n<p>Our initial proposals for these areas include: (1) a standard of avoiding\n\"negligent falsehoods\" (a generalisation of lies that is easier to assess); (2)\ninstitutions to evaluate AI systems before and after real-world deployment; and\n(3) explicitly training AI systems to be truthful via curated datasets and\nhuman interaction.\n</p>\n<p>A concerning possibility is that evaluation mechanisms for eventual\ntruthfulness standards could be captured by political interests, leading to\nharmful censorship and propaganda. Avoiding this might take careful attention.\nAnd since the scale of AI speech acts might grow dramatically over the coming\ndecades, early truthfulness standards might be particularly important because\nof the precedents they set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotton_Barratt_O/0/1/0/all/0/1\">Owen Cotton-Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finnveden_L/0/1/0/all/0/1\">Lukas Finnveden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bales_A/0/1/0/all/0/1\">Adam Bales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balwit_A/0/1/0/all/0/1\">Avital Balwit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wills_P/0/1/0/all/0/1\">Peter Wills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1\">Luca Righetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1\">William Saunders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese. (arXiv:2110.06696v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06696","description":"<p>Although pre-trained models (PLMs) have achieved remarkable improvements in a\nwide range of NLP tasks, they are expensive in terms of time and resources.\nThis calls for the study of training more efficient models with less\ncomputation but still ensures impressive performance. Instead of pursuing a\nlarger scale, we are committed to developing lightweight yet more powerful\nmodels trained with equal or less computation and friendly to rapid deployment.\nThis technical report releases our pre-trained model called Mengzi, which\nstands for a family of discriminative, generative, domain-specific, and\nmultimodal pre-trained model variants, capable of a wide range of language and\nvision tasks. Compared with public Chinese PLMs, Mengzi is simple but more\npowerful. Our lightweight model has achieved new state-of-the-art results on\nthe widely-used CLUE benchmark with our optimized pre-training and fine-tuning\ntechniques. Without modifying the model architecture, our model can be easily\nemployed as an alternative to existing PLMs. Our sources are available at\nhttps://github.com/Langboat/Mengzi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_J/0/1/0/all/0/1\">Jingyun Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematic Inequalities in Language Technology Performance across the World's Languages. (arXiv:2110.06733v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06733","description":"<p>Natural language processing (NLP) systems have become a central technology in\ncommunication, education, medicine, artificial intelligence, and many other\ndomains of research and development. While the performance of NLP methods has\ngrown enormously over the last decade, this progress has been restricted to a\nminuscule subset of the world's 6,500 languages. We introduce a framework for\nestimating the global utility of language technologies as revealed in a\ncomprehensive snapshot of recent publications in NLP. Our analyses involve the\nfield at large, but also more in-depth studies on both user-facing technologies\n(machine translation, language understanding, question answering,\ntext-to-speech synthesis) as well as more linguistic NLP tasks (dependency\nparsing, morphological inflection). In the process, we (1) quantify disparities\nin the current state of NLP research, (2) explore some of its associated\nsocietal and academic factors, and (3) produce tailored recommendations for\nevidence-based policy making aimed at promoting more global and equitable\nlanguage technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1\">Dami&#xe1;n Blasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masader: Metadata Sourcing for Arabic Text and Speech Data Resources. (arXiv:2110.06744v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06744","description":"<p>The NLP pipeline has evolved dramatically in the last few years. The first\nstep in the pipeline is to find suitable annotated datasets to evaluate the\ntasks we are trying to solve. Unfortunately, most of the published datasets\nlack metadata annotations that describe their attributes. Not to mention, the\nabsence of a public catalogue that indexes all the publicly available datasets\nrelated to specific regions or languages. When we consider low-resource\ndialectical languages, for example, this issue becomes more prominent. In this\npaper we create \\textit{Masader}, the largest public catalogue for Arabic NLP\ndatasets, which consists of 200 datasets annotated with 25 attributes.\nFurthermore, We develop a metadata annotation strategy that could be extended\nto other languages. We also make remarks and highlight some issues about the\ncurrent status of Arabic NLP datasets and suggest recommendations to address\nthem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoud_M/0/1/0/all/0/1\">Maraim Masoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaleb_M/0/1/0/all/0/1\">Mustafa Ghaleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_shaibani_M/0/1/0/all/0/1\">Maged S. Al-shaibani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Automated Unit Tests for Unsupervised Code Translation. (arXiv:2110.06773v1 [cs.SE])","link":"http://arxiv.org/abs/2110.06773","description":"<p>With little to no parallel data available for programming languages,\nunsupervised methods are well-suited to source code translation. However, the\nmajority of unsupervised machine translation approaches rely on\nback-translation, a method developed in the context of natural language\ntranslation and one that inherently involves training on noisy inputs.\nUnfortunately, source code is highly sensitive to small changes; a single token\ncan result in compilation failures or erroneous programs, unlike natural\nlanguages where small inaccuracies may not change the meaning of a sentence. To\naddress this issue, we propose to leverage an automated unit-testing system to\nfilter out invalid translations, thereby creating a fully tested parallel\ncorpus. We found that fine-tuning an unsupervised model with this filtered data\nset significantly reduces the noise in the translations so-generated,\ncomfortably outperforming the state-of-the-art for all language pairs studied.\nIn particular, for Java $\\to$ Python and Python $\\to$ C++ we outperform the\nbest previous methods by more than 16% and 24% respectively, reducing the error\nrate by more than 35%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Roziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie M. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_F/0/1/0/all/0/1\">Francois Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harman_M/0/1/0/all/0/1\">Mark Harman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lample_G/0/1/0/all/0/1\">Guillaume Lample</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems. (arXiv:2110.06800v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06800","description":"<p>Zero/few-shot transfer to unseen services is a critical challenge in\ntask-oriented dialogue research. The Schema-Guided Dialogue (SGD) dataset\nintroduced a paradigm for enabling models to support an unlimited number of\nservices without additional data collection or re-training through the use of\nschemas. Schemas describe service APIs in natural language, which models\nconsume to understand the services they need to support. However, the impact of\nthe choice of language in these schemas on model performance remains\nunexplored. We address this by releasing SGD-X, a benchmark for measuring the\nrobustness of dialogue systems to linguistic variations in schemas. SGD-X\nextends the SGD dataset with crowdsourced variants for every schema, where\nvariants are semantically similar yet stylistically diverse. We evaluate two\ndialogue state tracking models on SGD-X and observe that neither generalizes\nwell across schema variations, measured by joint goal accuracy and a novel\nmetric for measuring schema sensitivity. Furthermore, we present a simple\nmodel-agnostic data augmentation method to improve schema robustness and\nzero-shot generalization to unseen services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harrison Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raghav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging redundancy in attention with Reuse Transformers. (arXiv:2110.06821v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06821","description":"<p>Pairwise dot product-based attention allows Transformers to exchange\ninformation between tokens in an input-dependent way, and is key to their\nsuccess across diverse applications in language and vision. However, a typical\nTransformer model computes such pairwise attention scores repeatedly for the\nsame sequence, in multiple heads in multiple layers. We systematically analyze\nthe empirical similarity of these scores across heads and layers and find them\nto be considerably redundant, especially adjacent layers showing high\nsimilarity. Motivated by these findings, we propose a novel architecture that\nreuses attention scores computed in one layer in multiple subsequent layers.\nExperiments on a number of standard benchmarks show that reusing attention\ndelivers performance equivalent to or better than standard transformers, while\nreducing both compute and memory usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1\">Ayan Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1\">Himanshu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yin-Wen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Speaker-Aware Learning Framework for Improving Multi-turn Dialogue Coherence. (arXiv:2110.06823v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06823","description":"<p>This paper presents a novel open-domain dialogue generation framework\nemphasizing the differentiation of speakers in multi-turn conversations.\nDiffering from prior work that solely relies on the content of conversation\nhistory to generate a response, we argue that capturing relative social\nrelations among utterances (i.e., generated by either the same speaker or\ndifferent persons) benefits the machine capturing fine-grained context\ninformation from a conversation history to improve context coherence in the\ngenerated response. Given that, we propose a speaker-aware framework, named\nParallel Hierarchical Attentive Encoder-Decoder (PHAED), that aims to model\neach utterance with the awareness of its speaker and contextual associations\nwith the same speaker's previous messages. Specifically, in a conversation\ninvolving two speakers, we regard the utterances from one speaker as responses\nand those from the other as queries. After understanding queries via our\nencoder with inner-query and inter-query encodings, our decoder reuses the\nhidden states of previously generated responses to generate a new response. Our\nempirical results show that PHAED outperforms the state-of-the-art in both\nautomatic and human evaluations. Furthermore, our ablation study shows that\ndialogue models with speaker tokens can generally decrease the possibility of\ngenerating non-coherent responses regarding the conversation context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Ming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junli Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Language Model Integration for RNN Transducer based Speech Recognition. (arXiv:2110.06841v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06841","description":"<p>The mismatch between an external language model (LM) and the implicitly\nlearned internal LM (ILM) of RNN-Transducer (RNN-T) can limit the performance\nof LM integration such as simple shallow fusion. A Bayesian interpretation\nsuggests to remove this sequence prior as ILM correction. In this work, we\nstudy various ILM correction-based LM integration methods formulated in a\ncommon RNN-T framework. We provide a decoding interpretation on two major\nreasons for performance improvement with ILM correction, which is further\nexperimentally verified with detailed analysis. We also propose an exact-ILM\ntraining framework by extending the proof given in the hybrid autoregressive\ntransducer, which enables a theoretical justification for other ILM approaches.\nSystematic comparison is conducted for both in-domain and cross-domain\nevaluation on the Librispeech and TED-LIUM Release 2 corpora, respectively. Our\nproposed exact-ILM training can further improve the best ILM method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zuoyun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization in Dependency Parsing. (arXiv:2110.06843v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06843","description":"<p>Compositionality, or the ability to combine familiar units like words into\nnovel phrases and sentences, has been the focus of intense interest in\nartificial intelligence in recent years. To test compositional generalization\nin semantic parsing, Keysers et al. (2020) introduced Compositional Freebase\nQueries (CFQ). This dataset maximizes the similarity between the test and train\ndistributions over primitive units, like words, while maximizing the compound\ndivergence: the dissimilarity between test and train distributions over larger\nstructures, like phrases. Dependency parsing, however, lacks a compositional\ngeneralization benchmark. In this work, we introduce a gold-standard set of\ndependency parses for CFQ, and use this to analyze the behavior of a\nstate-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We\nfind that increasing compound divergence degrades dependency parsing\nperformance, although not as dramatically as semantic parsing performance.\nAdditionally, we find the performance of the dependency parser does not\nuniformly degrade relative to compound divergence, and the parser performs\ndifferently on different splits with the same compound divergence. We explore a\nnumber of hypotheses for what causes the non-uniform degradation in dependency\nparsing performance, and identify a number of syntactic structures that drive\nthe dependency parser's lower performance on the most challenging splits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_E/0/1/0/all/0/1\">Emily Goodwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ousiometrics and Telegnomics: The essence of meaning conforms to a two-dimensional powerful-weak and dangerous-safe framework with diverse corpora presenting a safety bias. (arXiv:2110.06847v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06847","description":"<p>We define `ousiometrics' to be the study of essential meaning in whatever\ncontext that meaningful signals are communicated, and `telegnomics' as the\nstudy of remotely sensed knowledge. From work emerging through the middle of\nthe 20th century, the essence of meaning has become generally accepted as being\nwell captured by the three orthogonal dimensions of evaluation, potency, and\nactivation (EPA). By re-examining first types and then tokens for the English\nlanguage, and through the use of automatically annotated histograms --\n`ousiograms' -- we find here that: 1. The essence of meaning conveyed by words\nis instead best described by a compass-like power-danger (PD) framework, and 2.\nAnalysis of a disparate collection of large-scale English language corpora --\nliterature, news, Wikipedia, talk radio, and social media -- shows that natural\nlanguage exhibits a systematic bias toward safe, low danger words -- a\nreinterpretation of the Pollyanna principle's positivity bias for written\nexpression. To help justify our choice of dimension names and to help address\nthe problems with representing observed ousiometric dimensions by bipolar\nadjective pairs, we introduce and explore `synousionyms' and `antousionyms' --\nousiometric counterparts of synonyms and antonyms. We further show that the PD\nframework revises the circumplex model of affect as a more general model of\nstate of mind. Finally, we use our findings to construct and test a prototype\n`ousiometer', a telegnomic instrument that measures ousiometric time series for\ntemporal corpora. We contend that our power-danger ousiometric framework\nprovides a complement for entropy-based measurements, and may be of value for\nthe study of a wide variety of communication across biological and artificial\nlife.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">P. S. Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">T. Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">M. I. Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmerman_J/0/1/0/all/0/1\">J. W. Zimmerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovato_J/0/1/0/all/0/1\">J. Lovato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1\">S. Beaulieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minot_J/0/1/0/all/0/1\">J. R. Minot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">M. V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagan_A/0/1/0/all/0/1\">A. J. Reagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">C. M. Danforth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphosyntactic Tagging with Pre-trained Language Models for Arabic and its Dialects. (arXiv:2110.06852v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06852","description":"<p>We present state-of-the-art results on morphosyntactic tagging across\ndifferent varieties of Arabic using fine-tuned pre-trained transformer language\nmodels. Our models consistently outperform existing systems in Modern Standard\nArabic and all the Arabic dialects we study, achieving 2.6% absolute\nimprovement over the previous state-of-the-art in Modern Standard Arabic, 2.8%\nin Gulf, 1.6% in Egyptian, and 7.0% in Levantine. We explore different training\nsetups for fine-tuning pre-trained transformer language models, including\ntraining data size, the use of external linguistic resources, and the use of\nannotated data from other dialects in a low-resource scenario. Our results show\nthat strategic fine-tuning using datasets from other high-resource dialects is\nbeneficial for a low-resource dialect. Additionally, we show that high-quality\nmorphological analyzers as external linguistic resources are beneficial\nespecially in low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1\">Go Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_S/0/1/0/all/0/1\">Salam Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Role Labeling as Dependency Parsing: Exploring Latent Tree Structures Inside Arguments. (arXiv:2110.06865v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06865","description":"<p>Semantic role labeling is a fundamental yet challenging task in the NLP\ncommunity. Recent works of SRL mainly fall into two lines:1) BIO-based and 2)\nspan-based. Despite effectiveness, they share some intrinsic drawbacks of not\nexplicitly considering internal argument structures, which may potentially\nhinder the model's expressiveness. To remedy this, we propose to reduce SRL to\na dependency parsing task and regard the flat argument spans as latent\nsubtrees. In particular, we equip our formulation with a novel span-constrained\nTreeCRF model to make tree structures span-aware, and further extend it to the\nsecond-order case. Experiments on CoNLL05 and CoNLL12 benchmarks reveal that\nthe results of our methods outperform all previous works and achieve the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qingrong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guohong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Essay Scoring Using Transformer Models. (arXiv:2110.06874v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06874","description":"<p>Automated essay scoring (AES) is gaining increasing attention in the\neducation sector as it significantly reduces the burden of manual scoring and\nallows ad hoc feedback for learners. Natural language processing based on\nmachine learning has been shown to be particularly suitable for text\nclassification and AES. While many machine-learning approaches for AES still\nrely on a bag-of-words (BOW) approach, we consider a transformer-based approach\nin this paper, compare its performance to a logistic regression model based on\nthe BOW approach and discuss their differences. The analysis is based on 2,088\nemail responses to a problem-solving task, that were manually labeled in terms\nof politeness. Both transformer models considered in that analysis outperformed\nwithout any hyper-parameter tuning the regression-based model. We argue that\nfor AES tasks such as politeness classification, the transformer-based approach\nhas significant advantages, while a BOW approach suffers from not taking word\norder into account and reducing the words to their stem. Further, we show how\nsuch models can help increase the accuracy of human raters, and we provide a\ndetailed instruction on how to implement transformer-based models for one's own\npurpose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ludwig_S/0/1/0/all/0/1\">Sabrina Ludwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_C/0/1/0/all/0/1\">Christian Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Christopher Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eilers_K/0/1/0/all/0/1\">Kerstin Eilers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_S/0/1/0/all/0/1\">Steffen Brandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers. (arXiv:2110.06884v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06884","description":"<p>We describe a Question Answering (QA) dataset that contains complex questions\nwith conditional answers, i.e. the answers are only applicable when certain\nconditions apply. We call this dataset ConditionalQA. In addition to\nconditional answers, the dataset also features: (1) long context documents with\ninformation that is related in logically complex ways; (2) multi-hop questions\nthat require compositional logical reasoning; (3) a combination of extractive\nquestions, yes/no questions, questions with multiple answers, and\nnot-answerable questions; (4) questions asked without knowing the answers. We\nshow that ConditionalQA is challenging for many of the existing QA models,\nespecially in selecting answer conditions. We believe that this dataset will\nmotivate further research in answering complex questions over long documents.\nData and leaderboard are publicly available at\n\\url{https://github.com/haitian-sun/ConditionalQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Scene-Aware Dialog and Reasoning using Audio-Visual Transformers with Joint Student-Teacher Learning. (arXiv:2110.06894v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06894","description":"<p>In previous work, we have proposed the Audio-Visual Scene-Aware Dialog (AVSD)\ntask, collected an AVSD dataset, developed AVSD technologies, and hosted an\nAVSD challenge track at both the 7th and 8th Dialog System Technology\nChallenges (DSTC7, DSTC8). In these challenges, the best-performing systems\nrelied heavily on human-generated descriptions of the video content, which were\navailable in the datasets but would be unavailable in real-world applications.\nTo promote further advancements for real-world applications, we proposed a\nthird AVSD challenge, at DSTC10, with two modifications: 1) the human-created\ndescription is unavailable at inference time, and 2) systems must demonstrate\ntemporal reasoning by finding evidence from the video to support each answer.\nThis paper introduces the new task that includes temporal reasoning and our new\nextension of the AVSD dataset for DSTC10, for which we collected\nhuman-generated temporal reasoning data. We also introduce a baseline system\nbuilt using an AV-transformer, which we released along with the new dataset.\nFinally, this paper introduces a new system that extends our baseline system\nwith attentional multimodal fusion, joint student-teacher learning (JSTL), and\nmodel combination techniques, achieving state-of-the-art performances on the\nAVSD datasets for DSTC7, DSTC8, and DSTC10. We also propose two temporal\nreasoning methods for AVSD: one attention-based, and one based on a time-domain\nregion proposal network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit P. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_T/0/1/0/all/0/1\">Tim K. Marks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1\">Chiori Hori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Models new APIs: Domain-Agnostic Simulators for Task Oriented Dialogue. (arXiv:2110.06905v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06905","description":"<p>We demonstrate that large language models are able to simulate Task Oriented\nDialogues in novel domains, provided only with an API implementation and a list\nof goals. We show these simulations can formulate online, automatic metrics\nthat correlate well with human evaluations. Furthermore, by checking for\nwhether the User's goals are met, we can use simulation to repeatedly generate\ntraining data and improve the quality of simulations themselves. With no human\nintervention or domain-specific training data, our simulations bootstrap\nend-to-end models which achieve a 37\\% error reduction in previously unseen\ndomains. By including as few as 32 domain-specific conversations, bootstrapped\nmodels can match the performance of a fully-supervised model with $10\\times$\nmore data. To our knowledge, this is the first time simulations have been shown\nto be effective at bootstrapping models without explicitly requiring any\ndomain-specific training data, rule-engineering, or humans-in-the-loop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Moya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crook_P/0/1/0/all/0/1\">Paul A. Crook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?. (arXiv:2110.06918v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06918","description":"<p>Despite their recent popularity and well known advantages, dense retrievers\nstill lag behind sparse methods such as BM25 in their ability to reliably match\nsalient phrases and rare entities in the query. It has been argued that this is\nan inherent limitation of dense models. We disprove this claim by introducing\nthe Salient Phrase Aware Retriever (SPAR), a dense retriever with the lexical\nmatching capacity of a sparse model. In particular, we show that a dense\nretriever {\\Lambda} can be trained to imitate a sparse one, and SPAR is built\nby augmenting a standard dense retriever with {\\Lambda}. When evaluated on five\nopen-domain question answering datasets and the MS MARCO passage retrieval\ntask, SPAR sets a new state of the art for dense and sparse retrievers and can\nmatch or exceed the performance of more complicated dense-sparse hybrid\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics-aware Attention Improves Neural Machine Translation. (arXiv:2110.06920v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06920","description":"<p>The integration of syntactic structures into Transformer machine translation\nhas shown positive results, but to our knowledge, no work has attempted to do\nso with semantic structures. In this work we propose two novel parameter-free\nmethods for injecting semantic information into Transformers, both rely on\nsemantics-aware masking of (some of) the attention heads. One such method\noperates on the encoder, through a Scene-Aware Self-Attention (SASA) head.\nAnother on the decoder, through a Scene-Aware Cross-Attention (SACrA) head. We\nshow a consistent improvement over the vanilla Transformer and syntax-aware\nmodels for four language pairs. We further show an additional gain when using\nboth semantic and syntactic structures in some language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slobodkin_A/0/1/0/all/0/1\">Aviv Slobodkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Representations for Modeling Variation in Speech. (arXiv:2011.12649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.12649","description":"<p>Variation in speech is often represented and investigated using phonetic\ntranscriptions, but transcribing speech is time-consuming and error prone. As\nan alternative representation, therefore, we investigate the extraction of\nacoustic embeddings from several self-supervised neural models. We use these\nrepresentations to compute word-based pronunciation differences between\nnon-native and native speakers of English, and between different dialect\npronunciations, and evaluate these differences by comparing them with available\nhuman native-likeness judgments. We show that Transformer-based speech\nrepresentations lead to significant performance gains over the use of phonetic\ntranscriptions, and find that feature-based use of Transformer models is most\neffective with one of the middle layers instead of the final layer. We also\ndemonstrate that these neural speech representations not only capture segmental\ndifferences, but also intonational and durational differences that cannot be\nrepresented by a set of discrete symbols used in phonetic transcriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_W/0/1/0/all/0/1\">Wietse de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanal_F/0/1/0/all/0/1\">Faraz Sanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_C/0/1/0/all/0/1\">Caitlin Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liberman_M/0/1/0/all/0/1\">Mark Liberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negation in Cognitive Reasoning. (arXiv:2012.12641v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.12641","description":"<p>Negation is both an operation in formal logic and in natural language by\nwhich a proposition is replaced by one stating the opposite, as by the addition\nof \"not\" or another negation cue. Treating negation in an adequate way is\nrequired for cognitive reasoning, which aims at modeling the human ability to\ndraw meaningful conclusions despite incomplete and inconsistent knowledge. One\ntask of cognitive reasoning is answering questions given by sentences in\nnatural language. There are tools based on discourse representation theory to\nconvert sentences automatically into a formal logic representation, and\nadditional knowledge can be added using the predicate names in the formula and\nknowledge databases. However, the knowledge in logic databases in practice\nalways is incomplete. Hence, forward reasoning of automated reasoning systems\nalone does not suffice to derive answers to questions because, instead of\ncomplete proofs, often only partial positive knowledge can be derived, while\nnegative knowledge is used only during the reasoning process. In consequence,\nwe aim at eliminating syntactic negation, strictly speaking, the negated event\nor property. In this paper, we describe an effective procedure to determine the\nnegated event or property in order to replace it by its inverse. This lays the\nbasis of cognitive reasoning, employing both logic and machine learning for\ngeneral question answering. We evaluate our procedure by several benchmarks and\ndemonstrate its practical usefulness in our cognitive reasoning system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schon_C/0/1/0/all/0/1\">Claudia Schon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_S/0/1/0/all/0/1\">Sophie Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolzenburg_F/0/1/0/all/0/1\">Frieder Stolzenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Specifying and Interpreting Reinforcement Learning Policies through Simulatable Machine Learning. (arXiv:2101.07140v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.07140","description":"<p>Human-AI collaborative policy synthesis is a procedure in which (1) a human\ninitializes an autonomous agent's behavior, (2) Reinforcement Learning improves\nthe human specified behavior, and (3) the agent can explain the final optimized\npolicy to the user. This paradigm leverages human expertise and facilitates a\ngreater insight into the learned behaviors of an agent. Existing approaches to\nenabling collaborative policy specification involve black box methods which are\nunintelligible and are not catered towards non-expert end-users. In this paper,\nwe develop a novel collaborative framework to enable humans to initialize and\ninterpret an autonomous agent's behavior, rooted in principles of\nhuman-centered design. Through our framework, we enable humans to specify an\ninitial behavior model in the form of unstructured, natural language, which we\nthen convert to lexical decision trees. Next, we are able to leverage these\nhuman-specified policies, to warm-start reinforcement learning and further\nallow the agent to optimize the policies through reinforcement learning.\nFinally, to close the loop on human-specification, we produce explanations of\nthe final learned policy, in multiple modalities, to provide the user a final\ndepiction about the learned policy of the agent. We validate our approach by\nshowing that our model can produce &gt;80% accuracy, and that human-initialized\npolicies are able to successfully warm-start RL. We then conduct a novel\nhuman-subjects study quantifying the relative subjective and objective benefits\nof varying XAI modalities(e.g., Tree, Language, and Program) for explaining\nlearned policies to end-users, in terms of usability and interpretability and\nidentify the circumstances that influence these measures. Our findings\nemphasize the need for personalized explainable systems that can facilitate\nuser-centric policy explanations for a variety of end-users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tambwekar_P/0/1/0/all/0/1\">Pradyumna Tambwekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Andrew Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_N/0/1/0/all/0/1\">Nakul Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1\">Matthew Gombolay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Proofs via Smart Contracts: Succinct and Informative Mathematical Derivations via Decentralized Markets. (arXiv:2102.03044v4 [cs.GT] UPDATED)","link":"http://arxiv.org/abs/2102.03044","description":"<p>Modern mathematics is built on the idea that proofs should be translatable\ninto formal proofs, whose validity is an objective question, decidable by a\ncomputer. Yet, in practice, proofs are informal and may omit many details. An\nagent considers a proof valid if they trust that it could be expanded into a\nmachine-verifiable proof. A proof's validity can thus become a subjective\nmatter and lead to a debate, which may be difficult to settle. Hence, while the\nconcept of valid proof is well-defined, the process to establish validity is\nitself a complex multi-agent problem.\n</p>\n<p>We introduce the SPRIG protocol. SPRIG allows agents to propose and verify\nsuccinct and informative proofs in a decentralized fashion; the trust is\nestablished by agents being able to request more details in the proof steps;\ndebates, if they arise, must isolate details of proofs and, if they persist, go\ndown to machine-level details, where they are automatically settled. A\nstructure of bounties and stakes is set to incentivize agents to act in good\nfaith.\n</p>\n<p>We propose a game-theoretic discussion of SPRIG, showing how agents with\nvarious types of information interact, leading to a proof tree with an\nappropriate level of detail and to the invalidation of wrong proofs, and we\ndiscuss resilience against various attacks. We then analyze a simplified model,\ncharacterize its equilibria and compute the agents' level of trust.\n</p>\n<p>SPRIG is designed to run as a smart contract on a blockchain platform. This\nallows anonymous agents to participate in the verification debate, and to\ncontribute with their information. The smart contract mediates the\ninteractions, settles debates, and guarantees that bounties and stakes are paid\nas specified.\n</p>\n<p>SPRIG enables new applications, such as the issuance of bounties for open\nproblems, and the creation of derivatives markets, allowing agents to inject\nmore information pertaining to proofs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carre_S/0/1/0/all/0/1\">Sylvain Carr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_F/0/1/0/all/0/1\">Franck Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongler_C/0/1/0/all/0/1\">Cl&#xe9;ment Hongler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacerda_G/0/1/0/all/0/1\">Gustavo Lacerda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capano_G/0/1/0/all/0/1\">Gloria Capano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advances in Multi-turn Dialogue Comprehension: A Survey. (arXiv:2103.03125v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.03125","description":"<p>Training machines to understand natural language and interact with humans is\nan elusive and essential task of artificial intelligence. A diversity of\ndialogue systems has been designed with the rapid development of deep learning\ntechniques, especially the recent pre-trained language models (PrLMs). Among\nthese studies, the fundamental yet challenging type of task is dialogue\ncomprehension whose role is to teach the machines to read and comprehend the\ndialogue context before responding. In this paper, we review the previous\nmethods from the technical perspective of dialogue modeling for the dialogue\ncomprehension task. We summarize the characteristics and challenges of dialogue\ncomprehension in contrast to plain-text reading comprehension. Then, we discuss\nthree typical patterns of dialogue modeling. In addition, we categorize\ndialogue-related pre-training techniques which are employed to enhance PrLMs in\ndialogue scenarios. Finally, we highlight the technical advances in recent\nyears and point out the lessons from the empirical analysis and the prospects\ntowards a new frontier of researches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema. (arXiv:2104.08161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08161","description":"<p>The Winograd Schema (WS) has been proposed as a test for measuring\ncommonsense capabilities of models. Recently, pre-trained language model-based\napproaches have boosted performance on some WS benchmarks but the source of\nimprovement is still not clear. This paper suggests that the apparent progress\non WS may not necessarily reflect progress in commonsense reasoning. To support\nthis claim, we first show that the current evaluation method of WS is\nsub-optimal and propose a modification that uses twin sentences for evaluation.\nWe also propose two new baselines that indicate the existence of artifacts in\nWS benchmarks. We then develop a method for evaluating WS-like sentences in a\nzero-shot setting to account for the commonsense reasoning abilities acquired\nduring the pretraining and observe that popular language models perform\nrandomly in this setting when using our more strict evaluation. We conclude\nthat the observed progress is mostly due to the use of supervision in training\nWS models, which is not likely to successfully support all the required\ncommonsense reasoning skills and knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGG: Learning to Select, Guide, and Generate for Keyphrase Generation. (arXiv:2105.02544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.02544","description":"<p>Keyphrases, that concisely summarize the high-level topics discussed in a\ndocument, can be categorized into present keyphrase which explicitly appears in\nthe source text, and absent keyphrase which does not match any contiguous\nsubsequence but is highly semantically related to the source. Most existing\nkeyphrase generation approaches synchronously generate present and absent\nkeyphrases without explicitly distinguishing these two categories. In this\npaper, a Select-Guide-Generate (SGG) approach is proposed to deal with present\nand absent keyphrase generation separately with different mechanisms.\nSpecifically, SGG is a hierarchical neural network which consists of a\npointing-based selector at low layer concentrated on present keyphrase\ngeneration, a selection-guided generator at high layer dedicated to absent\nkeyphrase generation, and a guider in the middle to transfer information from\nselector to generator. Experimental results on four keyphrase generation\nbenchmarks demonstrate the effectiveness of our model, which significantly\noutperforms the strong baselines for both present and absent keyphrases\ngeneration. Furthermore, we extend SGG to a title generation task which\nindicates its extensibility in natural language generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05541","description":"<p>Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahim Shahriar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1\">Mueeze Al Mushabbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Answer Similarity for Evaluating Question Answering Models. (arXiv:2108.06130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06130","description":"<p>The evaluation of question answering models compares ground-truth annotations\nwith model predictions. However, as of today, this comparison is mostly\nlexical-based and therefore misses out on answers that have no lexical overlap\nbut are still semantically similar, thus treating correct answers as false.\nThis underestimation of the true performance of models hinders user acceptance\nin applications and complicates a fair comparison of different models.\nTherefore, there is a need for an evaluation metric that is based on semantics\ninstead of pure string similarity. In this short paper, we present SAS, a\ncross-encoder-based metric for the estimation of semantic answer similarity,\nand compare it to seven existing metrics. To this end, we create an English and\na German three-way annotated evaluation dataset containing pairs of answers\nalong with human judgment of their semantic similarity, which we release along\nwith an implementation of the SAS metric and the experiments. We find that\nsemantic similarity metrics based on recent transformer models correlate much\nbetter with human judgment than traditional lexical similarity metrics on our\ntwo newly created datasets and one dataset from related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutsch_J/0/1/0/all/0/1\">Julian Gutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietsch_M/0/1/0/all/0/1\">Malte Pietsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06314","description":"<p>Time is an important dimension in our physical world. Lots of facts can\nevolve with respect to time. For example, the U.S. President might change every\nfour years. Therefore, it is important to consider the time dimension and\nempower the existing QA models to reason over time. However, the existing QA\ndatasets contain rather few time-sensitive questions, hence not suitable for\ndiagnosing or benchmarking the model's temporal reasoning capability. In order\nto promote research in this direction, we propose to construct a time-sensitive\nQA dataset. The dataset is constructed by 1) mining time-evolving facts from\nWikiData and align them to their corresponding Wikipedia page, 2) employing\ncrowd workers to verify and calibrate these noisy facts, 3) generating\nquestion-answer pairs based on the annotated time-sensitive facts. Our dataset\nposes challenges in the aspect of both temporal understanding and temporal\nreasoning. We evaluate different SoTA long-document QA systems like BigBird and\nFiD on our dataset. The best-performing model FiD can only achieve 46\\%\naccuracy, still far behind the human performance of 87\\%. We demonstrate that\nthese models are still lacking the ability to perform consistent temporal\nreasoning. Therefore, we believe that our dataset could serve as a benchmark to\ndevelop NLP models more sensitive to temporal shift. The dataset and code are\nreleased in~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Based Self-Critical Training For Question Generation. (arXiv:2108.12026v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12026","description":"<p>Question generation is a conditioned language generation task that consists\nin generating a context-aware question given a context and the targeted answer.\nTrain language modelling with a mere likelihood maximization has been widely\nused while suffering from exposure bias and the discordance between the\ntraining and the test metrics. In the way of addressing this issue, The\npresented work portrays a fully Transformer-based reinforcement learning\ngenerator-evaluation architecture for neural question generation. To edge the\nflexibility of the generation, a semantic-based reward score was externally\ninfused during the training to drive the training of the language model. The\nglobal architecture is laid out in a generator-evaluator fashion optimized\ndirectly to n-gram and semantic-based metrics. Evaluation metrics for language\nmodelling only based on n-gram overlapping do not consider semantic relations\nbetween reference and candidate sequences. To improve the evaluation step, a\ntwo-fold evaluation was carried out. On the one side, an n-gram overlapping\nevaluation using the BLEU score. On the other side, a semantic-based assessment\nusing BERTScore and NUBIA. The results were corroborated by a binary human\nevaluation of the semantic relatedness of the generated question and the ground\ntruth. The results obtained showed that use a semantic-based REINFORCE\nalgorithm for the question generation syntactically reshapes the generated\nquestions while preserving their underlying semantic meaning. Many downstream\napplications can be drawn from a successful question generation including the\nenlargement of question answering datasets, the improvement of conversational\nsystems, the enhancement of autonomous educational assessment systems, and so\nforth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo%5C%22ic/0/1/0/all/0/1\">Lo&#xef;c</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dassi_K/0/1/0/all/0/1\">Kwate Dassi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03137","description":"<p>Existing generative pre-trained language models (e.g., GPT) focus on modeling\nthe language structure and semantics of general texts. However, those models do\nnot consider the numerical properties of numbers and cannot perform robustly on\nnumerical reasoning tasks (e.g., math word problems and measurement\nestimation). In this paper, we propose NumGPT, a generative pre-trained model\nthat explicitly models the numerical properties of numbers in texts.\nSpecifically, it leverages a prototype-based numeral embedding to encode the\nmantissa of the number and an individual embedding to encode the exponent of\nthe number. A numeral-aware loss function is designed to integrate numerals\ninto the pre-training objective of NumGPT. We conduct extensive experiments on\nfour different datasets to evaluate the numeracy ability of NumGPT. The\nexperiment results show that NumGPT outperforms baseline models (e.g., GPT and\nGPT with DICE) on a range of numerical reasoning tasks such as measurement\nestimation, number comparison, math word problems, and magnitude\nclassification. Ablation studies are also conducted to evaluate the impact of\npre-training and model hyperparameters on the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings. (arXiv:2109.07833v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07833","description":"<p>Natural language inference (NLI) requires models to learn and apply\ncommonsense knowledge. These reasoning abilities are particularly important for\nexplainable NLI systems that generate a natural language explanation in\naddition to their label prediction. The integration of external knowledge has\nbeen shown to improve NLI systems, here we investigate whether it can also\nimprove their explanation capabilities. For this, we investigate different\nsources of external knowledge and evaluate the performance of our models on\nin-domain data as well as on special transfer datasets that are designed to\nassess fine-grained reasoning capabilities. We find that different sources of\nknowledge have a different effect on reasoning abilities, for example, implicit\nknowledge stored in language models can hinder reasoning on numbers and\nnegations. Finally, we conduct the largest and most fine-grained explainable\nNLI crowdsourcing study to date. It reveals that even large differences in\nautomatic performance scores do neither reflect in human ratings of label,\nexplanation, commonsense nor grammar correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsiu-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10255","description":"<p>The recognition of hate speech and offensive language (HOF) is commonly\nformulated as a classification task to decide if a text contains HOF. We\ninvestigate whether HOF detection can profit by taking into account the\nrelationships between HOF and similar concepts: (a) HOF is related to sentiment\nanalysis because hate speech is typically a negative statement and expresses a\nnegative opinion; (b) it is related to emotion analysis, as expressed hate\npoints to the author experiencing (or pretending to experience) anger while the\naddressees experience (or are intended to experience) fear. (c) Finally, one\nconstituting element of HOF is the mention of a targeted person or group. On\nthis basis, we hypothesize that HOF detection shows improvements when being\nmodeled jointly with these concepts, in a multi-task learning setup. We base\nour experiments on existing data sets for each of these concepts (sentiment,\nemotion, target of HOF) and evaluate our models as a participant (as team\nIMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection\nexperiments in which we consider multiple available resources and submissions\nto the shared task, we find that the combination of the CrowdFlower emotion\ncorpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target\ndetection data leads to an F1 =.79 in a multi-head multi-task learning model\nbased on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test\ndata, this result is more substantial with an increase by 2pp in F1 and a\nconsiderable increase in recall. Across both data sets (2019, 2021), the recall\nis particularly increased for the class of HOF (6pp for the 2019 data and 3pp\nfor the 2021 data), showing that MTL with emotion, sentiment, and target\nidentification is an appropriate approach for early warning systems that might\nbe deployed in social media platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plaza_del_Arco_F/0/1/0/all/0/1\">Flor Miriam Plaza-del-Arco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halat_S/0/1/0/all/0/1\">Sercan Halat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v3 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2109.13662","description":"<p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce\nan end-to-end trainable system that integrates reasoning and perception. PSL\nrepresents first-order logic in terms of a convex graphical model -- Hinge Loss\nMarkov random fields (HL-MRFs). PSL stands out among probabilistic logic\nframeworks due to its tractability having been applied to systems of more than\n1 billion ground rules. The key to our approach is to represent predicates in\nfirst-order logic using deep neural networks and then to approximately\nback-propagate through the HL-MRF and thus train every aspect of the\nfirst-order system being represented. We believe that this approach represents\nan interesting direction for the integration of deep learning and reasoning\ntechniques with applications to knowledge base learning, multi-task learning,\nand explainability. We evaluate DeepPSL on a zero shot learning problem in\nimage classification. State of the art results demonstrate the utility and\nflexibility of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel P. Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puranam_S/0/1/0/all/0/1\">Sai Akhil Puranam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dasaratha_S/0/1/0/all/0/1\">Sridhar Dasaratha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phogat_K/0/1/0/all/0/1\">Karmvir Singh Phogat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiyyagura_S/0/1/0/all/0/1\">Sunil Reddy Tiyyagura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. (arXiv:2110.00976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00976","description":"<p>Law, interpretations of law, legal arguments, agreements, etc. are typically\nexpressed in writing, leading to the production of vast corpora of legal text.\nTheir analysis, which is at the center of legal practice, becomes increasingly\nelaborate as these collections grow in size. Natural language understanding\n(NLU) technologies can be a valuable tool to support legal practitioners in\nthese endeavors. Their usefulness, however, largely depends on whether current\nstate-of-the-art models can generalize across various tasks in the legal\ndomain. To answer this currently open question, we introduce the Legal General\nLanguage Understanding Evaluation (LexGLUE) benchmark, a collection of datasets\nfor evaluating model performance across a diverse set of legal NLU tasks in a\nstandardized way. We also provide an evaluation and analysis of several generic\nand legal-oriented models demonstrating that the latter consistently offer\nperformance improvements across multiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_A/0/1/0/all/0/1\">Abhik Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_D/0/1/0/all/0/1\">Dirk Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1\">Michael Bommarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks. (arXiv:2110.03861v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2110.03861","description":"<p>The advent of noisy intermediate-scale quantum (NISQ) computers raises a\ncrucial challenge to design quantum neural networks for fully quantum learning\ntasks. To bridge the gap, this work proposes an end-to-end learning framework\nnamed QTN-VQC, by introducing a trainable quantum tensor network (QTN) for\nquantum embedding on a variational quantum circuit (VQC). The architecture of\nQTN is composed of a parametric tensor-train network for feature extraction and\na tensor product encoding for quantum encoding. We highlight the QTN for\nquantum embedding in terms of two perspectives: (1) we theoretically\ncharacterize QTN by analyzing its representation power of input features; (2)\nQTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the\ngeneration of quantum embedding to the output measurement. Our experiments on\nthe MNIST dataset demonstrate the advantages of QTN for quantum embedding over\nother quantum embedding approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Qi_J/0/1/0/all/0/1\">Jun Qi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. (arXiv:2110.03888v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03888","description":"<p>Recent expeditious developments in deep learning algorithms, distributed\ntraining, and even hardware design for large models have enabled training\nextreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of\nbillions or even trillions of parameters. However, under limited resources,\nextreme-scale model training that requires enormous amounts of computes and\nmemory footprint suffers from frustratingly low efficiency in model\nconvergence. In this paper, we propose a simple training strategy called\n\"Pseudo-to-Real\" for high-memory-footprint-required large models.\nPseudo-to-Real is compatible with large models with architecture of sequential\nlayers. We demonstrate a practice of pretraining unprecedented\n10-trillion-parameter model, an order of magnitude larger than the\nstate-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the\napplication of Pseudo-to-Real, we also provide a technique, Granular CPU\noffloading, to manage CPU memory for training large model and maintain high GPU\nutilities. Fast training of extreme-scale models on a decent amount of\nresources can bring much smaller carbon footprint and contribute to greener AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinze Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HydraSum -- Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models. (arXiv:2110.04400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04400","description":"<p>Existing abstractive summarization models lack explicit control mechanisms\nthat would allow users to influence the stylistic features of the model\noutputs. This results in generating generic summaries that do not cater to the\nusers needs or preferences. To address this issue we introduce HydraSum, a new\nsummarization architecture that extends the single decoder framework of current\nmodels, e.g. BART, to a mixture-of-experts version consisting of multiple\ndecoders. Our proposed model encourages each expert, i.e. decoder, to learn and\ngenerate stylistically-distinct summaries along dimensions such as\nabstractiveness, length, specificity, and others. At each time step, HydraSum\nemploys a gating mechanism that decides the contribution of each individual\ndecoder to the next token's output probability distribution. Through\nexperiments on three summarization datasets (CNN, Newsroom, XSum), we\ndemonstrate that this gating mechanism automatically learns to assign\ncontrasting summary styles to different HydraSum decoders under the standard\ntraining objective without the need for additional supervision. We further show\nthat a guided version of the training process can explicitly govern which\nsummary style is partitioned between decoders, e.g. high abstractiveness vs.\nlow abstractiveness or high specificity vs. low specificity, and also increase\nthe stylistic-difference between individual decoders. Finally, our experiments\ndemonstrate that our decoder framework is highly flexible: during inference, we\ncan sample from individual decoders or mixtures of different subsets of the\ndecoders to yield a diverse set of summaries and enforce single- and\nmulti-style control over summary generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEPP: Similarity Estimation of Predicted Probabilities for Defending and Detecting Adversarial Text. (arXiv:2110.05748v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05748","description":"<p>There are two cases describing how a classifier processes input text, namely,\nmisclassification and correct classification. In terms of misclassified texts,\na classifier handles the texts with both incorrect predictions and adversarial\ntexts, which are generated to fool the classifier, which is called a victim.\nBoth types are misunderstood by the victim, but they can still be recognized by\nother classifiers. This induces large gaps in predicted probabilities between\nthe victim and the other classifiers. In contrast, text correctly classified by\nthe victim is often successfully predicted by the others and induces small\ngaps. In this paper, we propose an ensemble model based on similarity\nestimation of predicted probabilities (SEPP) to exploit the large gaps in the\nmisclassified predictions in contrast to small gaps in the correct\nclassification. SEPP then corrects the incorrect predictions of the\nmisclassified texts. We demonstrate the resilience of SEPP in defending and\ndetecting adversarial texts through different types of victim classifiers,\nclassification tasks, and adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Son_H/0/1/0/all/0/1\">Hoang-Quoc Nguyen-Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidano_S/0/1/0/all/0/1\">Seira Hidano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukushima_K/0/1/0/all/0/1\">Kazuhide Fukushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyomoto_S/0/1/0/all/0/1\">Shinsaku Kiyomoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05896","description":"<p>Trained on the large corpus, pre-trained language models (PLMs) can capture\ndifferent levels of concepts in context and hence generate universal language\nrepresentations. They can benefit multiple downstream natural language\nprocessing (NLP) tasks. Although PTMs have been widely used in most NLP\napplications, especially for high-resource languages such as English, it is\nunder-represented in Lao NLP research. Previous work on Lao has been hampered\nby the lack of annotated datasets and the sparsity of language resources. In\nthis work, we construct a text classification dataset to alleviate the\nresource-scare situation of the Lao language. We additionally present the first\ntransformer-based PTMs for Lao with four versions: BERT-small, BERT-base,\nELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:\npart-of-speech tagging and text classification. Experiments demonstrate the\neffectiveness of our Lao models. We will release our models and datasets to the\ncommunity, hoping to facilitate the future development of Lao NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Real Image Inversion via Segments. (arXiv:2110.06269v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06269","description":"<p>In this short report, we present a simple, yet effective approach to editing\nreal images via generative adversarial networks (GAN). Unlike previous\ntechniques, that treat all editing tasks as an operation that affects pixel\nvalues in the entire image in our approach we cut up the image into a set of\nsmaller segments. For those segments corresponding latent codes of a generative\nnetwork can be estimated with greater accuracy due to the lower number of\nconstraints. When codes are altered by the user the content in the image is\nmanipulated locally while the rest of it remains unaffected. Thanks to this\nproperty the final edited image better retains the original structures and thus\nhelps to preserve natural look.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futschik_D/0/1/0/all/0/1\">David Futschik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukac_M/0/1/0/all/0/1\">Michal Luk&#xe1;&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sykora_D/0/1/0/all/0/1\">Daniel S&#xfd;kora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Learning from An Expert in Deep Recommendation Systems with Marginal Distance Probability Distribution. (arXiv:2110.06287v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06287","description":"<p>Recommendation systems play an important role in today's digital world. They\nhave found applications in various applications such as music platforms, e.g.,\nSpotify, and movie streaming services, e.g., Netflix. Less research effort has\nbeen devoted to physical exercise recommendation systems. Sedentary lifestyles\nhave become the major driver of several diseases as well as healthcare costs.\nIn this paper, we develop a recommendation system for daily exercise activities\nto users based on their history, profile and similar users. The developed\nrecommendation system uses a deep recurrent neural network with user-profile\nattention and temporal attention mechanisms.\n</p>\n<p>Moreover, exercise recommendation systems are significantly different from\nstreaming recommendation systems in that we are not able to collect click\nfeedback from the participants in exercise recommendation systems. Thus, we\npropose a real-time, expert-in-the-loop active learning procedure. The active\nlearners calculate the uncertainty of the recommender at each time step for\neach user and ask an expert for a recommendation when the certainty is low. In\nthis paper, we derive the probability distribution function of marginal\ndistance, and use it to determine when to ask experts for feedback. Our\nexperimental results on a mHealth dataset show improved accuracy after\nincorporating the real-time active learner with the recommendation system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahyari_A/0/1/0/all/0/1\">Arash Mahyari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirolli_P/0/1/0/all/0/1\">Peter Pirolli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeBlanc_J/0/1/0/all/0/1\">Jacqueline A. LeBlanc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Persistent Homologies for more Effective Deep Learning. (arXiv:2110.06295v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06295","description":"<p>Persistent Homologies have been successfully used to increase the performance\nof deep networks trained to detect curvilinear structures and to improve the\ntopological quality of the results. However, existing methods are very global\nand ignore the location of topological features. In this paper, we introduce an\napproach that relies on a new filtration function to account for location\nduring network training. We demonstrate experimentally on 2D images of roads\nand 3D image stacks of neuronal processes that networks trained in this manner\nare better at recovering the topology of the curvilinear structures they\nextract.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oner_D/0/1/0/all/0/1\">Doruk Oner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garin_A/0/1/0/all/0/1\">Ad&#xe9;lie Garin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozinski_M/0/1/0/all/0/1\">Mateusz Kozi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hess_K/0/1/0/all/0/1\">Kathryn Hess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Content Based Image Retrieval for Highly Imbalanced Melanoma Data using Style Transfer, Semantic Image Segmentation and Ensemble Learning. (arXiv:2110.06331v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06331","description":"<p>Lesion images are frequently taken in open-set settings. Because of this, the\nimage data generated is extremely varied in nature.It is difficult for a\nconvolutional neural network to find proper features and generalise well, as a\nresult content based image retrieval (CBIR) system for lesion images are\ndifficult to build. This paper explores this domain and proposes multiple\nsimilarity measures which uses Style Loss and Dice Coefficient via a novel\nsimilarity measure called I1-Score. Out of the CBIR similarity measures\nproposed, pure style loss approach achieves a remarkable accuracy increase over\ntraditional approaches like Euclidean Distance and Cosine Similarity. The\nI1-Scores using style loss performed better than traditional approaches by a\nsmall margin, whereas, I1-Scores with dice-coefficient faired very poorly. The\nmodel used is trained using ensemble learning for better generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_P/0/1/0/all/0/1\">Priyam Mehta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel framework based on deep learning and ANOVA feature selection method for diagnosis of COVID-19 cases from chest X-ray Images. (arXiv:2110.06340v1 [eess.IV])","link":"http://arxiv.org/abs/2110.06340","description":"<p>The new coronavirus (known as COVID-19) was first identified in Wuhan and\nquickly spread worldwide, wreaking havoc on the economy and people's everyday\nlives. Fever, cough, sore throat, headache, exhaustion, muscular aches, and\ndifficulty breathing are all typical symptoms of COVID-19. A reliable detection\ntechnique is needed to identify affected individuals and care for them in the\nearly stages of COVID-19 and reduce the virus's transmission. The most\naccessible method for COVID-19 identification is RT-PCR; however, due to its\ntime commitment and false-negative results, alternative options must be sought.\nIndeed, compared to RT-PCR, chest CT scans and chest X-ray images provide\nsuperior results. Because of the scarcity and high cost of CT scan equipment,\nX-ray images are preferable for screening. In this paper, a pre-trained\nnetwork, DenseNet169, was employed to extract features from X-ray images.\nFeatures were chosen by a feature selection method (ANOVA) to reduce\ncomputations and time complexity while overcoming the curse of dimensionality\nto improve predictive accuracy. Finally, selected features were classified by\nXGBoost. The ChestX-ray8 dataset, which was employed to train and evaluate the\nproposed method. This method reached 98.72% accuracy for two-class\nclassification (COVID-19, healthy) and 92% accuracy for three-class\nclassification (COVID-19, healthy, pneumonia).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nasiri_H/0/1/0/all/0/1\">Hamid Nasiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alavi_S/0/1/0/all/0/1\">Seyyed Ali Alavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice-assisted Image Labelling for Endoscopic Ultrasound Classification using Neural Networks. (arXiv:2110.06367v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06367","description":"<p>Ultrasound imaging is a commonly used technology for visualising patient\nanatomy in real-time during diagnostic and therapeutic procedures. High\noperator dependency and low reproducibility make ultrasound imaging and\ninterpretation challenging with a steep learning curve. Automatic image\nclassification using deep learning has the potential to overcome some of these\nchallenges by supporting ultrasound training in novices, as well as aiding\nultrasound image interpretation in patient with complex pathology for more\nexperienced practitioners. However, the use of deep learning methods requires a\nlarge amount of data in order to provide accurate results. Labelling large\nultrasound datasets is a challenging task because labels are retrospectively\nassigned to 2D images without the 3D spatial context available in vivo or that\nwould be inferred while visually tracking structures between frames during the\nprocedure. In this work, we propose a multi-modal convolutional neural network\n(CNN) architecture that labels endoscopic ultrasound (EUS) images from raw\nverbal comments provided by a clinician during the procedure. We use a CNN\ncomposed of two branches, one for voice data and another for image data, which\nare joined to predict image labels from the spoken names of anatomical\nlandmarks. The network was trained using recorded verbal comments from expert\noperators. Our results show a prediction accuracy of 76% at image level on a\ndataset with 5 different labels. We conclude that the addition of spoken\ncommentaries can increase the performance of ultrasound image classification,\nand eliminate the burden of manually labelling large EUS datasets necessary for\ndeep learning applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonmati_E/0/1/0/all/0/1\">Ester Bonmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimwood_A/0/1/0/all/0/1\">Alexander Grimwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_G/0/1/0/all/0/1\">Gavin J. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodchild_G/0/1/0/all/0/1\">George Goodchild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keane_M/0/1/0/all/0/1\">Margaret G. Keane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurusamy_K/0/1/0/all/0/1\">Kurinchi Gurusamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1\">Brian Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_S/0/1/0/all/0/1\">Stephen P. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Open Source User Activity Traces with Applications to User Mobility Characterization and Modeling. (arXiv:2110.06382v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06382","description":"<p>The current state-of-the-art in user mobility research has extensively relied\non open-source mobility traces captured from pedestrian and vehicular activity\nthrough a variety of communication technologies as users engage in a wide-range\nof applications, including connected healthcare, localization, social media,\ne-commerce, etc. Most of these traces are feature-rich and diverse, not only in\nthe information they provide, but also in how they can be used and leveraged.\nThis diversity poses two main challenges for researchers and practitioners who\nwish to make use of available mobility datasets. First, it is quite difficult\nto get a bird's eye view of the available traces without spending considerable\ntime looking them up. Second, once they have found the traces, they still need\nto figure out whether the traces are adequate to their needs.\n</p>\n<p>The purpose of this survey is three-fold. It proposes a taxonomy to classify\nopen-source mobility traces including their mobility mode, data source and\ncollection technology. It then uses the proposed taxonomy to classify existing\nopen-source mobility traces and finally, highlights three case studies using\npopular publicly available datasets to showcase how our taxonomy can tease out\nfeature sets in traces to help determine their applicability to specific\nuse-cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+King_S/0/1/0/all/0/1\">Sinjoni Mukhopadhyay King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawab_F/0/1/0/all/0/1\">Faisal Nawab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obraczka_K/0/1/0/all/0/1\">Katia Obraczka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CovXR: Automated Detection of COVID-19 Pneumonia in Chest X-Rays through Machine Learning. (arXiv:2110.06398v1 [eess.IV])","link":"http://arxiv.org/abs/2110.06398","description":"<p>Coronavirus disease 2019 (COVID-19) is the highly contagious illness caused\nby severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The standard\ndiagnostic testing procedure for COVID-19 is testing a nasopharyngeal swab for\nSARS-CoV-2 nucleic acid using a real-time polymerase chain reaction (PCR),\nwhich can take multiple days to provide a diagnosis. Another widespread form of\ntesting is rapid antigen testing, which has a low sensitivity compared to PCR,\nbut is favored for its quick diagnosis time of usually 15-30 minutes. Patients\nwho test positive for COVID-19 demonstrate diffuse alveolar damage in 87% of\ncases. Machine learning has proven to have advantages in image classification\nproblems with radiology. In this work, we introduce CovXR as a machine learning\nmodel designed to detect COVID-19 pneumonia in chest X-rays (CXR). CovXR is a\nconvolutional neural network (CNN) trained on over 4,300 chest X-rays. The\nperformance of the model is measured through accuracy, F1 score, sensitivity,\nand specificity. The model achieves an accuracy of 95.5% and an F1 score of\n0.954. The sensitivity is 93.5% and specificity is 97.5%. With accuracy above\n95% and F1 score above 0.95, CovXR is highly accurate in predicting COVID-19\npneumonia on CXRs. The model achieves better accuracy than prior work and uses\na unique approach to identify COVID-19 pneumonia. CovXR is highly accurate in\nidentifying COVID-19 on CXRs of patients with a PCR confirmed positive\ndiagnosis and provides much faster results than PCR tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shenoy_V/0/1/0/all/0/1\">Vishal Shenoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_S/0/1/0/all/0/1\">Sachin B. Malik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Inference with Neural Interpreters. (arXiv:2110.06399v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06399","description":"<p>Modern neural network architectures can leverage large amounts of data to\ngeneralize well within the training distribution. However, they are less\ncapable of systematic generalization to data drawn from unseen but related\ndistributions, a feat that is hypothesized to require compositional reasoning\nand reuse of knowledge. In this work, we present Neural Interpreters, an\narchitecture that factorizes inference in a self-attention network as a system\nof modules, which we call \\emph{functions}. Inputs to the model are routed\nthrough a sequence of functions in a way that is end-to-end learned. The\nproposed architecture can flexibly compose computation along width and depth,\nand lends itself well to capacity extension after training. To demonstrate the\nversatility of Neural Interpreters, we evaluate it in two distinct settings:\nimage classification and visual abstract reasoning on Raven Progressive\nMatrices. In the former, we show that Neural Interpreters perform on par with\nthe vision transformer using fewer parameters, while being transferrable to a\nnew task in a sample efficient manner. In the latter, we find that Neural\nInterpreters are competitive with respect to the state-of-the-art in terms of\nsystematic generalization\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_N/0/1/0/all/0/1\">Nasim Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gondal_M/0/1/0/all/0/1\">Muhammad Waleed Gondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shruti Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CyTran: Cycle-Consistent Transformers for Non-Contrast to Contrast CT Translation. (arXiv:2110.06400v1 [eess.IV])","link":"http://arxiv.org/abs/2110.06400","description":"<p>We propose a novel approach to translate unpaired contrast computed\ntomography (CT) scans to non-contrast CT scans and the other way around.\nSolving this task has two important applications: (i) to automatically generate\ncontrast CT scans for patients for whom injecting contrast substance is not an\noption, and (ii) to enhance alignment between contrast and non-contrast CT by\nreducing the differences induced by the contrast substance before registration.\nOur approach is based on cycle-consistent generative adversarial convolutional\ntransformers, for short, CyTran. Our neural model can be trained on unpaired\nimages, due to the integration of a cycle-consistency loss. To deal with\nhigh-resolution images, we design a hybrid architecture based on convolutional\nand multi-head attention layers. In addition, we introduce a novel data set,\nColtea-Lung-CT-100W, containing 3D triphasic lung CT scans (with a total of\n37,290 images) collected from 100 female patients. Each scan contains three\nphases (non-contrast, early portal venous, and late arterial), allowing us to\nperform experiments to compare our novel approach with state-of-the-art methods\nfor image style transfer. Our empirical results show that CyTran outperforms\nall competing methods. Moreover, we show that CyTran can be employed as a\npreliminary step to improve a state-of-the-art medical image alignment method.\nWe release our novel model and data set as open source at:\nhttps://github.com/ristea/cycle-transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miron_A/0/1/0/all/0/1\">Andreea-Iuliana Miron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Savencu_O/0/1/0/all/0/1\">Olivian Savencu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verga_N/0/1/0/all/0/1\">Nicolae Verga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMIU: Dataset for Visual Intent Understanding in Multimodal Assistants. (arXiv:2110.06416v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06416","description":"<p>In multimodal assistant, where vision is also one of the input modalities,\nthe identification of user intent becomes a challenging task as visual input\ncan influence the outcome. Current digital assistants take spoken input and try\nto determine the user intent from conversational or device context. So, a\ndataset, which includes visual input (i.e. images or videos for the\ncorresponding questions targeted for multimodal assistant use cases, is not\nreadily available. The research in visual question answering (VQA) and visual\nquestion generation (VQG) is a great step forward. However, they do not capture\nquestions that a visually-abled person would ask multimodal assistants.\nMoreover, many times questions do not seek information from external knowledge.\nIn this paper, we provide a new dataset, MMIU (MultiModal Intent\nUnderstanding), that contains questions and corresponding intents provided by\nhuman annotators while looking at images. We, then, use this dataset for intent\nclassification task in multimodal digital assistant. We also experiment with\nvarious approaches for combining vision and language features including the use\nof multimodal transformer for classification of image-question pairs into 14\nintents. We provide the benchmark results and discuss the role of visual and\ntext features for the intent classification task on our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Alkesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_R/0/1/0/all/0/1\">Roman Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzou_N/0/1/0/all/0/1\">Nick Tzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotek_H/0/1/0/all/0/1\">Hadas Kotek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renkens_V/0/1/0/all/0/1\">Vincent Renkens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Uncertainty Estimation. (arXiv:2110.06427v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06427","description":"<p>Deep neural networks can be roughly divided into deterministic neural\nnetworks and stochastic neural networks.The former is usually trained to\nachieve a mapping from input space to output space via maximum likelihood\nestimation for the weights, which leads to deterministic predictions during\ntesting. In this way, a specific weights set is estimated while ignoring any\nuncertainty that may occur in the proper weight space. The latter introduces\nrandomness into the framework, either by assuming a prior distribution over\nmodel parameters (i.e. Bayesian Neural Networks) or including latent variables\n(i.e. generative models) to explore the contribution of latent variables for\nmodel predictions, leading to stochastic predictions during testing. Different\nfrom the former that achieves point estimation, the latter aims to estimate the\nprediction distribution, making it possible to estimate uncertainty,\nrepresenting model ignorance about its predictions. We claim that conventional\ndeterministic neural network based dense prediction tasks are prone to\noverfitting, leading to over-confident predictions, which is undesirable for\ndecision making. In this paper, we investigate stochastic neural networks and\nuncertainty estimation techniques to achieve both accurate deterministic\nprediction and reliable uncertainty estimation. Specifically, we work on two\ntypes of uncertainty estimations solutions, namely ensemble based methods and\ngenerative model based methods, and explain their pros and cons while using\nthem in fully/semi/weakly-supervised framework. Due to the close connection\nbetween uncertainty estimation and model calibration, we also introduce how\nuncertainty estimation can be used for deep model calibration to achieve\nwell-calibrated models, namely dense model calibration. Code and data are\navailable at https://github.com/JingZhang617/UncertaintyEstimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_M/0/1/0/all/0/1\">Mochu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1\">Peyman Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-local Recurrent Regularization Networks for Multi-view Stereo. (arXiv:2110.06436v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06436","description":"<p>In deep multi-view stereo networks, cost regularization is crucial to achieve\naccurate depth estimation. Since 3D cost volume filtering is usually\nmemory-consuming, recurrent 2D cost map regularization has recently become\npopular and has shown great potential in reconstructing 3D models of different\nscales. However, existing recurrent methods only model the local dependencies\nin the depth domain, which greatly limits the capability of capturing the\nglobal scene context along the depth dimension. To tackle this limitation, we\npropose a novel non-local recurrent regularization network for multi-view\nstereo, named NR2-Net. Specifically, we design a depth attention module to\ncapture non-local depth interactions within a sliding depth block. Then, the\nglobal scene context between different blocks is modeled in a gated recurrent\nmanner. This way, the long-range dependencies along the depth dimension are\ncaptured to facilitate the cost regularization. Moreover, we design a dynamic\ndepth map fusion strategy to improve the algorithm robustness. Our method\nachieves state-of-the-art reconstruction results on both DTU and Tanks and\nTemples datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qingshan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1\">Wenbing Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing the Conditioning Sensorium for Improved Image Translation. (arXiv:2110.06443v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06443","description":"<p>Multi-modal domain translation typically refers to synthesizing a novel image\nthat inherits certain localized attributes from a 'content' image (e.g. layout,\nsemantics, or geometry), and inherits everything else (e.g. texture, lighting,\nsometimes even semantics) from a 'style' image. The dominant approach to this\ntask is attempting to learn disentangled 'content' and 'style' representations\nfrom scratch. However, this is not only challenging, but ill-posed, as what\nusers wish to preserve during translation varies depending on their goals.\nMotivated by this inherent ambiguity, we define 'content' based on conditioning\ninformation extracted by off-the-shelf pre-trained models. We then train our\nstyle extractor and image decoder with an easy to optimize set of\nreconstruction objectives. The wide variety of high-quality pre-trained models\navailable and simple training procedure makes our approach straightforward to\napply across numerous domains and definitions of 'content'. Additionally it\noffers intuitive control over which aspects of 'content' are preserved across\ndomains. We evaluate our method on traditional, well-aligned, datasets such as\nCelebA-HQ, and propose two novel datasets for evaluation on more complex\nscenes: ClassicTV and FFHQ-Wild. Our approach, Sensorium, enables higher\nquality domain translation for more complex scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nederhood_C/0/1/0/all/0/1\">Cooper Nederhood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolkin_N/0/1/0/all/0/1\">Nicholas Kolkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Deqing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salavon_J/0/1/0/all/0/1\">Jason Salavon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing the Covariate Shift by Mirror Samples in Cross Domain Alignment. (arXiv:2110.06448v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06448","description":"<p>Eliminating the covariate shift cross domains is one of the common methods to\ndeal with the issue of domain shift in visual unsupervised domain adaptation.\nHowever, current alignment methods, especially the prototype based or\nsample-level based methods neglect the structural properties of the underlying\ndistribution and even break the condition of covariate shift. To relieve the\nlimitations and conflicts, we introduce a novel concept named (virtual) mirror,\nwhich represents the equivalent sample in another domain. The equivalent sample\npairs, named mirror pairs reflect the natural correspondence of the empirical\ndistributions. Then a mirror loss, which aligns the mirror pairs cross domains,\nis constructed to enhance the alignment of the domains. The proposed method\ndoes not distort the internal structure of the underlying distribution. We also\nprovide theoretical proof that the mirror samples and mirror loss have better\nasymptotic properties in reducing the domain shift. By applying the virtual\nmirror and mirror loss to the generic unsupervised domain adaptation model, we\nachieved consistent superior performance on several mainstream benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Longjun Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Updating Street Maps using Changes Detected in Satellite Imagery. (arXiv:2110.06456v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06456","description":"<p>Accurately maintaining digital street maps is labor-intensive. To address\nthis challenge, much work has studied automatically processing geospatial data\nsources such as GPS trajectories and satellite images to reduce the cost of\nmaintaining digital maps. An end-to-end map update system would first process\ngeospatial data sources to extract insights, and second leverage those insights\nto update and improve the map. However, prior work largely focuses on the first\nstep of this pipeline: these map extraction methods infer road networks from\nscratch given geospatial data sources (in effect creating entirely new maps),\nbut do not address the second step of leveraging this extracted information to\nupdate the existing digital map data. In this paper, we first explain why\ncurrent map extraction techniques yield low accuracy when extended to update\nexisting maps. We then propose a novel method that leverages the progression of\nsatellite imagery over time to substantially improve accuracy. Our approach\nfirst compares satellite images captured at different times to identify\nportions of the physical road network that have visibly changed, and then\nupdates the existing map accordingly. We show that our change-based approach\nreduces map update error rates four-fold.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastani_F/0/1/0/all/0/1\">Favyen Bastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Songtao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagwani_S/0/1/0/all/0/1\">Satvat Jagwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1\">Mohammad Alizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_H/0/1/0/all/0/1\">Hari Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_S/0/1/0/all/0/1\">Sanjay Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madden_S/0/1/0/all/0/1\">Sam Madden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1\">Mohammad Amin Sadeghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking the Dilemma of Medical Image-to-image Translation. (arXiv:2110.06465v1 [eess.IV])","link":"http://arxiv.org/abs/2110.06465","description":"<p>Supervised Pix2Pix and unsupervised Cycle-consistency are two modes that\ndominate the field of medical image-to-image translation. However, neither\nmodes are ideal. The Pix2Pix mode has excellent performance. But it requires\npaired and well pixel-wise aligned images, which may not always be achievable\ndue to respiratory motion or anatomy change between times that paired images\nare acquired. The Cycle-consistency mode is less stringent with training data\nand works well on unpaired or misaligned images. But its performance may not be\noptimal. In order to break the dilemma of the existing modes, we propose a new\nunsupervised mode called RegGAN for medical image-to-image translation. It is\nbased on the theory of \"loss-correction\". In RegGAN, the misaligned target\nimages are considered as noisy labels and the generator is trained with an\nadditional registration network to fit the misaligned noise distribution\nadaptively. The goal is to search for the common optimal solution to both\nimage-to-image translation and registration tasks. We incorporated RegGAN into\na few state-of-the-art image-to-image translation methods and demonstrated that\nRegGAN could be easily combined with these methods to improve their\nperformances. Such as a simple CycleGAN in our mode surpasses latest NICEGAN\neven though using less network parameters. Based on our results, RegGAN\noutperformed both Pix2Pix on aligned data and Cycle-consistency on misaligned\nor unpaired data. RegGAN is insensitive to noises which makes it a better\nchoice for a wide range of scenarios, especially for medical image-to-image\ntranslation tasks in which well pixel-wise aligned data are not available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kong_L/0/1/0/all/0/1\">Lingke Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lian_C/0/1/0/all/0/1\">Chenyu Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_D/0/1/0/all/0/1\">Detian Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhenjiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yanle Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Q/0/1/0/all/0/1\">Qichao Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Winning the ICCV'2021 VALUE Challenge: Task-aware Ensemble and Transfer Learning with Visual Concepts. (arXiv:2110.06476v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06476","description":"<p>The VALUE (Video-And-Language Understanding Evaluation) benchmark is newly\nintroduced to evaluate and analyze multi-modal representation learning\nalgorithms on three video-and-language tasks: Retrieval, QA, and Captioning.\nThe main objective of the VALUE challenge is to train a task-agnostic model\nthat is simultaneously applicable for various tasks with different\ncharacteristics. This technical report describes our winning strategies for the\nVALUE challenge: 1) single model optimization, 2) transfer learning with visual\nconcepts, and 3) task-aware ensemble. The first and third strategies are\ndesigned to address heterogeneous characteristics of each task, and the second\none is to leverage rich and fine-grained visual information. We provide a\ndetailed and comprehensive analysis with extensive experimental results. Based\non our approach, we ranked first place on the VALUE and QA phases for the\ncompetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minchul Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1\">Jonghwan Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+On_K/0/1/0/all/0/1\">Kyoung-Woon On</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Woo-Young Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Gunsoo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eun-Sol Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Semantic Segmentation without Source Data. (arXiv:2110.06484v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06484","description":"<p>Domain adaptive semantic segmentation is recognized as a promising technique\nto alleviate the domain shift between the labeled source domain and the\nunlabeled target domain in many real-world applications, such as automatic\npilot. However, large amounts of source domain data often introduce significant\ncosts in storage and training, and sometimes the source data is inaccessible\ndue to privacy policies. To address these problems, we investigate domain\nadaptive semantic segmentation without source data, which assumes that the\nmodel is pre-trained on the source domain, and then adapting to the target\ndomain without accessing source data anymore. Since there is no supervision\nfrom the source domain data, many self-training methods tend to fall into the\n``winner-takes-all'' dilemma, where the {\\it majority} classes totally dominate\nthe segmentation networks and the networks fail to classify the {\\it minority}\nclasses. Consequently, we propose an effective framework for this challenging\nproblem with two components: positive learning and negative learning. In\npositive learning, we select the class-balanced pseudo-labeled pixels with\nintra-class threshold, while in negative learning, for each pixel, we\ninvestigate which category the pixel does not belong to with the proposed\nheuristic complementary label selection. Notably, our framework can be easily\nimplemented and incorporated with other methods to further enhance the\nperformance. Extensive experiments on two widely-used synthetic-to-real\nbenchmarks demonstrate our claims and the effectiveness of our framework, which\noutperforms the baseline with a large margin. Code is available at\n\\url{https://github.com/fumyou13/LDBE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_F/0/1/0/all/0/1\">Fuming You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding of Emotion Perception from Art. (arXiv:2110.06486v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06486","description":"<p>Computational modeling of the emotions evoked by art in humans is a\nchallenging problem because of the subjective and nuanced nature of art and\naffective signals. In this paper, we consider the above-mentioned problem of\nunderstanding emotions evoked in viewers by artwork using both text and visual\nmodalities. Specifically, we analyze images and the accompanying text captions\nfrom the viewers expressing emotions as a multimodal classification task. Our\nresults show that single-stream multimodal transformer-based models like MMBT\nand VisualBERT perform better compared to both image-only models and\ndual-stream multimodal models having separate pathways for text and image\nmodalities. We also observe improvements in performance for extreme positive\nand negative emotion classes, when a single-stream model like MMBT is compared\nwith a text-only transformer model like BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_D/0/1/0/all/0/1\">Digbalay Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_R/0/1/0/all/0/1\">Rimita Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dawn of Quantum Natural Language Processing. (arXiv:2110.06510v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06510","description":"<p>In this paper, we discuss the initial attempts at boosting understanding\nhuman language based on deep-learning models with quantum computing. We\nsuccessfully train a quantum-enhanced Long Short-Term Memory network to perform\nthe parts-of-speech tagging task via numerical simulations. Moreover, a\nquantum-enhanced Transformer is proposed to perform the sentiment analysis\nbased on the existing dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sipio_R/0/1/0/all/0/1\">Riccardo Di Sipio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Hong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Samuel Yen-Chi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangini_S/0/1/0/all/0/1\">Stefano Mangini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedNet: Pre-trained Convolutional Neural Network Model for the Medical Imaging Tasks. (arXiv:2110.06512v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06512","description":"<p>Deep Learning (DL) requires a large amount of training data to provide\nquality outcomes. However, the field of medical imaging suffers from the lack\nof sufficient data for properly training DL models because medical images\nrequire manual labelling carried out by clinical experts thus the process is\ntime-consuming, expensive, and error-prone. Recently, transfer learning (TL)\nwas introduced to reduce the need for the annotation procedure by means of\ntransferring the knowledge performed by a previous task and then fine-tuning\nthe result using a relatively small dataset. Nowadays, multiple classification\nmethods from medical imaging make use of TL from general-purpose pre-trained\nmodels, e.g., ImageNet, which has been proven to be ineffective due to the\nmismatch between the features learned from natural images (ImageNet) and those\nmore specific from medical images especially medical gray images such as\nX-rays. ImageNet does not have grayscale images such as MRI, CT, and X-ray. In\nthis paper, we propose a novel DL model to be used for addressing\nclassification tasks of medical imaging, called MedNet. To do so, we aim to\nissue two versions of MedNet. The first one is Gray-MedNet which will be\ntrained on 3M publicly available gray-scale medical images including MRI, CT,\nX-ray, ultrasound, and PET. The second version is Color-MedNet which will be\ntrained on 3M publicly available color medical images including histopathology,\ntaken images, and many others. To validate the effectiveness MedNet, both\nversions will be fine-tuned to train on the target tasks of a more reduced set\nof medical images. MedNet performs as the pre-trained model to tackle any\nreal-world application from medical imaging and achieve the level of\ngeneralization needed for dealing with medical imaging tasks, e.g.\nclassification. MedNet would serve the research community as a baseline for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alzubaidi_L/0/1/0/all/0/1\">Laith Alzubaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santamaria_J/0/1/0/all/0/1\">J. Santamar&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoufali_M/0/1/0/all/0/1\">Mohamed Manoufali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_B/0/1/0/all/0/1\">Beadaa Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadhel_M/0/1/0/all/0/1\">Mohammed A. Fadhel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinglan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Timemy_A/0/1/0/all/0/1\">Ali H.Al-Timemy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shamma_O/0/1/0/all/0/1\">Omran Al-Shamma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Ye Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking the Robustness of Spatial-Temporal Models Against Corruptions. (arXiv:2110.06513v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06513","description":"<p>The state-of-the-art deep neural networks are vulnerable to common\ncorruptions (e.g., input data degradations, distortions, and disturbances\ncaused by weather changes, system error, and processing). While much progress\nhas been made in analyzing and improving the robustness of models in image\nunderstanding, the robustness in video understanding is largely unexplored. In\nthis paper, we establish a corruption robustness benchmark, Mini Kinetics-C and\nMini SSV2-C, which considers temporal corruptions beyond spatial corruptions in\nimages. We make the first attempt to conduct an exhaustive study on the\ncorruption robustness of established CNN-based and Transformer-based\nspatial-temporal models. The study provides some guidance on robust model\ndesign and training: Transformer-based model performs better than CNN-based\nmodels on corruption robustness; the generalization ability of spatial-temporal\nmodels implies robustness against temporal corruptions; model corruption\nrobustness (especially robustness in the temporal domain) enhances with\ncomputational cost and model capacity, which may contradict the current trend\nof improving the computational efficiency of models. Moreover, we find the\nrobustness intervention for image-related tasks (e.g., training models with\nnoise) may not work for spatial-temporal models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_C/0/1/0/all/0/1\">Chenyu Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YANG_S/0/1/0/all/0/1\">SIYUAN YANG</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yap-peng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2D Multi-Class Model for Gray and White Matter Segmentation of the Cervical Spinal Cord at 7T. (arXiv:2110.06516v1 [eess.IV])","link":"http://arxiv.org/abs/2110.06516","description":"<p>The spinal cord (SC), which conveys information between the brain and the\nperipheral nervous system, plays a key role in various neurological disorders\nsuch as multiple sclerosis (MS) and amyotrophic lateral sclerosis (ALS), in\nwhich both gray matter (GM) and white matter (WM) may be impaired. While\nautomated methods for WM/GM segmentation are now largely available, these\ntechniques, developed for conventional systems (3T or lower) do not necessarily\nperform well on 7T MRI data, which feature finer details, contrasts, but also\ndifferent artifacts or signal dropout.\n</p>\n<p>The primary goal of this study is thus to propose a new deep learning model\nthat allows robust SC/GM multi-class segmentation based on ultra-high\nresolution 7T T2*-w MR images. The second objective is to highlight the\nrelevance of implementing a specific data augmentation (DA) strategy, in\nparticular to generate a generic model that could be used for multi-center\nstudies at 7T.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Medina_N/0/1/0/all/0/1\">Nilser J. Laines Medina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gros_C/0/1/0/all/0/1\">Charley Gros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Callot_V/0/1/0/all/0/1\">Virginie Callot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Troter_A/0/1/0/all/0/1\">Arnaud Le Troter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Information Bottleneck for Weakly Supervised Semantic Segmentation. (arXiv:2110.06530v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06530","description":"<p>Weakly supervised semantic segmentation produces pixel-level localization\nfrom class labels; however, a classifier trained on such labels is likely to\nfocus on a small discriminative region of the target object. We interpret this\nphenomenon using the information bottleneck principle: the final layer of a\ndeep neural network, activated by the sigmoid or softmax activation functions,\ncauses an information bottleneck, and as a result, only a subset of the\ntask-relevant information is passed on to the output. We first support this\nargument through a simulated toy experiment and then propose a method to reduce\nthe information bottleneck by removing the last activation function. In\naddition, we introduce a new pooling method that further encourages the\ntransmission of information from non-discriminative regions to the\nclassification. Our experimental evaluations demonstrate that this simple\nmodification significantly improves the quality of localization maps on both\nthe PASCAL VOC 2012 and MS COCO 2014 datasets, exhibiting a new\nstate-of-the-art performance for weakly supervised semantic segmentation. The\ncode is available at: https://github.com/jbeomlee93/RIB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungbeom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1\">Jisoo Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and the growth of margin. To counteract this deficiency, we\npropose to reward well-classified examples with additive bonuses to revive\ntheir contribution to learning. This counterexample theoretically addresses\nthese three issues. We empirically support this claim by directly verify the\ntheoretical results or through the significant performance improvement with our\ncounterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that\nbecause our idea can solve these three issues, we can deal with complex\nscenarios, such as imbalanced classification, OOD detection, and applications\nunder adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency Detection via Global Context Enhanced Feature Fusion and Edge Weighted Loss. (arXiv:2110.06550v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06550","description":"<p>UNet-based methods have shown outstanding performance in salient object\ndetection (SOD), but are problematic in two aspects. 1) Indiscriminately\nintegrating the encoder feature, which contains spatial information for\nmultiple objects, and the decoder feature, which contains global information of\nthe salient object, is likely to convey unnecessary details of non-salient\nobjects to the decoder, hindering saliency detection. 2) To deal with ambiguous\nobject boundaries and generate accurate saliency maps, the model needs\nadditional branches, such as edge reconstructions, which leads to increasing\ncomputational cost. To address the problems, we propose a context fusion\ndecoder network (CFDN) and near edge weighted loss (NEWLoss) function. The CFDN\ncreates an accurate saliency map by integrating global context information and\nthus suppressing the influence of the unnecessary spatial information. NEWLoss\naccelerates learning of obscure boundaries without additional modules by\ngenerating weight maps on object boundaries. Our method is evaluated on four\nbenchmarks and achieves state-of-the-art performance. We prove the\neffectiveness of the proposed method through comparative experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">MyeongAh Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization. (arXiv:2110.06554v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06554","description":"<p>Quantization is a widely used technique to compress and accelerate deep\nneural networks. However, conventional quantization methods use the same\nbit-width for all (or most of) the layers, which often suffer significant\naccuracy degradation in the ultra-low precision regime and ignore the fact that\nemergent hardware accelerators begin to support mixed-precision computation.\nConsequently, we present a novel and principled framework to solve the\nmixed-precision quantization problem in this paper. Briefly speaking, we first\nformulate the mixed-precision quantization as a discrete constrained\noptimization problem. Then, to make the optimization tractable, we approximate\nthe objective function with second-order Taylor expansion and propose an\nefficient approach to compute its Hessian matrix. Finally, based on the above\nsimplification, we show that the original problem can be reformulated as a\nMultiple-Choice Knapsack Problem (MCKP) and propose a greedy search algorithm\nto solve it efficiently. Compared with existing mixed-precision quantization\nworks, our method is derived in a principled way and much more computationally\nefficient. Moreover, extensive experiments conducted on the ImageNet dataset\nand various kinds of network architectures also demonstrate its superiority\nover existing uniform and mixed-precision quantization approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peisong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LENS: Localization enhanced by NeRF synthesis. (arXiv:2110.06558v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06558","description":"<p>Neural Radiance Fields (NeRF) have recently demonstrated photo-realistic\nresults for the task of novel view synthesis. In this paper, we propose to\napply novel view synthesis to the robot relocalization problem: we demonstrate\nimprovement of camera pose regression thanks to an additional synthetic dataset\nrendered by the NeRF class of algorithm. To avoid spawning novel views in\nirrelevant places we selected virtual camera locations from NeRF internal\nrepresentation of the 3D geometry of the scene. We further improved\nlocalization accuracy of pose regressors using synthesized realistic and\ngeometry consistent images as data augmentation during training. At the time of\npublication, our approach improved state of the art with a 60% lower error on\nCambridge Landmarks and 7-scenes datasets. Hence, the resulting accuracy\nbecomes comparable to structure-based methods, without any architecture\nmodification or domain adaptation constraints. Since our method allows almost\ninfinite generation of training data, we investigated limitations of camera\npose regression depending on size and distribution of data used for training on\npublic benchmarks. We concluded that pose regression accuracy is mostly bounded\nby relatively small and biased datasets rather than capacity of the pose\nregression model to solve the localization task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreau_A/0/1/0/all/0/1\">Arthur Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piasco_N/0/1/0/all/0/1\">Nathan Piasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortelle_A/0/1/0/all/0/1\">Arnaud de La Fortelle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Object Learning via Common Fate. (arXiv:2110.06562v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06562","description":"<p>Learning generative object models from unlabelled videos is a long standing\nproblem and required for causal scene modeling. We decompose this problem into\nthree easier subtasks, and provide candidate solutions for each of them.\nInspired by the Common Fate Principle of Gestalt Psychology, we first extract\n(noisy) masks of moving objects via unsupervised motion segmentation. Second,\ngenerative models are trained on the masks of the background and the moving\nobjects, respectively. Third, background and foreground models are combined in\na conditional \"dead leaves\" scene model to sample novel scene configurations\nwhere occlusions and depth layering arise naturally. To evaluate the individual\nstages, we introduce the Fishbowl dataset positioned between complex real-world\nscenes and common object-centric benchmarks of simplistic objects. We show that\nour approach allows learning generative models that generalize beyond the\nocclusions present in the input videos, and represent scenes in a modular\nfashion that allows sampling plausible scenes outside the training distribution\nby permitting, for instance, object numbers or densities not observed in the\ntraining set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tangemann_M/0/1/0/all/0/1\">Matthias Tangemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1\">Steffen Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerer_M/0/1/0/all/0/1\">Matthias K&#xfc;mmerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Superpixel-based Network for Blind Image Quality Assessment. (arXiv:2110.06564v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06564","description":"<p>The goal in a blind image quality assessment (BIQA) model is to simulate the\nprocess of evaluating images by human eyes and accurately assess the quality of\nthe image. Although many approaches effectively identify degradation, they do\nnot fully consider the semantic content in images resulting in distortion. In\norder to fill this gap, we propose a deep adaptive superpixel-based network,\nnamely DSN-IQA, to assess the quality of image based on multi-scale and\nsuperpixel segmentation. The DSN-IQA can adaptively accept arbitrary scale\nimages as input images, making the assessment process similar to human\nperception. The network uses two models to extract multi-scale semantic\nfeatures and generate a superpixel adjacency map. These two elements are united\ntogether via feature fusion to accurately predict image quality. Experimental\nresults on different benchmark databases demonstrate that our algorithm is\nhighly competitive with other approaches when assessing challenging authentic\nimage databases. Also, due to adaptive deep superpixel-based network, our model\naccurately assesses images with complicated distortion, much like the human\neye.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guangyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan%2E_Y/0/1/0/all/0/1\">Yang Zhan.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspectral 3D Mapping of Underwater Environments. (arXiv:2110.06571v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06571","description":"<p>Hyperspectral imaging has been increasingly used for underwater survey\napplications over the past years. As many hyperspectral cameras work as\npush-broom scanners, their use is usually limited to the creation of\nphoto-mosaics based on a flat surface approximation and by interpolating the\ncamera pose from dead-reckoning navigation. Yet, because of drift in the\nnavigation and the mostly wrong flat surface assumption, the quality of the\nobtained photo-mosaics is often too low to support adequate analysis.In this\npaper we present an initial method for creating hyperspectral 3D\nreconstructions of underwater environments. By fusing the data gathered by a\nclassical RGB camera, an inertial navigation system and a hyperspectral\npush-broom camera, we show that the proposed method creates highly accurate 3D\nreconstructions with hyperspectral textures. We propose to combine techniques\nfrom simultaneous localization and mapping, structure-from-motion and 3D\nreconstruction and advantageously use them to create 3D models with\nhyperspectral texture, allowing us to overcome the flat surface assumption and\nthe classical limitation of dead-reckoning navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrera_M/0/1/0/all/0/1\">Maxime Ferrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnaubec_A/0/1/0/all/0/1\">Aur&#xe9;lien Arnaubec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Istenic_K/0/1/0/all/0/1\">Klemen Istenic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gracias_N/0/1/0/all/0/1\">Nuno Gracias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajjouk_T/0/1/0/all/0/1\">Touria Bajjouk</a> (IFREMER)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Life is not black and white -- Combining Semi-Supervised Learning with fuzzy labels. (arXiv:2110.06592v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06592","description":"<p>The required amount of labeled data is one of the biggest issues in deep\nlearning. Semi-Supervised Learning can potentially solve this issue by using\nadditional unlabeled data. However, many datasets suffer from variability in\nthe annotations. The aggregated labels from these annotation are not consistent\nbetween different annotators and thus are considered fuzzy. These fuzzy labels\nare often not considered by Semi-Supervised Learning. This leads either to an\ninferior performance or to higher initial annotation costs in the complete\nmachine learning development cycle. We envision the incorporation of fuzzy\nlabels into Semi-Supervised Learning and give a proof-of-concept of the\npotential lower costs and higher consistency in the complete development cycle.\nAs part of our concept, we discuss current limitations, futures research\nopportunities and potential broad impacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling. (arXiv:2110.06607v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06607","description":"<p>In this paper, we propose THOMAS, a joint multi-agent trajectory prediction\nframework allowing for efficient and consistent prediction of multi-agent\nmulti-modal trajectories. We present a unified model architecture for fast and\nsimultaneous agent future heatmap estimation leveraging hierarchical and sparse\nimage generation. We demonstrate that heatmap output enables a higher level of\ncontrol on the predicted trajectories compared to vanilla multi-modal\ntrajectory regression, allowing to incorporate additional constraints for\ntighter sampling or collision-free predictions in a deterministic way. However,\nwe also highlight that generating scene-consistent predictions goes beyond the\nmere generation of collision-free trajectories. We therefore propose a\nlearnable trajectory recombination model that takes as input a set of predicted\ntrajectories for each agent and outputs its consistent reordered recombination.\nWe report our results on the Interaction multi-agent prediction challenge and\nrank $1^{st}$ on the online test leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP4Caption: CLIP for Video Caption. (arXiv:2110.06615v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06615","description":"<p>Video captioning is a challenging task since it requires generating sentences\ndescribing various diverse and complex videos. Existing video captioning models\nlack adequate visual representation due to the neglect of the existence of gaps\nbetween videos and texts. To bridge this gap, in this paper, we propose a\nCLIP4Caption framework that improves video captioning based on a CLIP-enhanced\nvideo-text matching network (VTM). This framework is taking full advantage of\nthe information from both vision and language and enforcing the model to learn\nstrongly text-correlated video features for text generation. Besides, unlike\nmost existing models using LSTM or GRU as the sentence decoder, we adopt a\nTransformer structured decoder network to effectively learn the long-range\nvisual and language dependency. Additionally, we introduce a novel ensemble\nstrategy for captioning tasks. Experimental results demonstrate the\neffectiveness of our method on two datasets: 1) on MSR-VTT dataset, our method\nachieved a new state-of-the-art result with a significant gain of up to 10% in\nCIDEr; 2) on the private test data, our method ranking 2nd place in the ACM MM\nmultimedia grand challenge 2021: Pre-training for Video Understanding\nChallenge. It is noted that our model is only trained on the MSR-VTT dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingkang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_F/0/1/0/all/0/1\">Fengyun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oriented Feature Alignment for Fine-grained Object Recognition in High-Resolution Satellite Imagery. (arXiv:2110.06628v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06628","description":"<p>Oriented object detection in remote sensing images has made great progress in\nrecent years. However, most of the current methods only focus on detecting\ntargets, and cannot distinguish fine-grained objects well in complex scenes. In\nthis technical report, we analyzed the key issues of fine-grained object\nrecognition, and use an oriented feature alignment network (OFA-Net) to achieve\nhigh-performance fine-grained oriented object recognition in optical remote\nsensing images. OFA-Net achieves accurate object localization through a rotated\nbounding boxes refinement module. On this basis, the boundary-constrained\nrotation feature alignment module is applied to achieve local feature\nextraction, which is beneficial to fine-grained object classification. The\nsingle model of our method achieved mAP of 46.51\\% in the GaoFen competition\nand won 3rd place in the ISPRS benchmark with the mAP of 43.73\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Junjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiqiang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fuzzy Overclustering: Semi-Supervised Classification of Fuzzy Labels with Overclustering and Inverse Cross-Entropy. (arXiv:2110.06630v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06630","description":"<p>Deep learning has been successfully applied to many classification problems\nincluding underwater challenges. However, a long-standing issue with deep\nlearning is the need for large and consistently labeled datasets. Although\ncurrent approaches in semi-supervised learning can decrease the required amount\nof annotated data by a factor of 10 or even more, this line of research still\nuses distinct classes. For underwater classification, and uncurated real-world\ndatasets in general, clean class boundaries can often not be given due to a\nlimited information content in the images and transitional stages of the\ndepicted objects. This leads to different experts having different opinions and\nthus producing fuzzy labels which could also be considered ambiguous or\ndivergent. We propose a novel framework for handling semi-supervised\nclassifications of such fuzzy labels. It is based on the idea of overclustering\nto detect substructures in these fuzzy labels. We propose a novel loss to\nimprove the overclustering capability of our framework and show the benefit of\noverclustering for fuzzy labels. We show that our framework is superior to\nprevious state-of-the-art semi-supervised methods when applied to real-world\nplankton data with fuzzy labels. Moreover, we acquire 5 to 10\\% more consistent\npredictions of substructures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunger_J/0/1/0/all/0/1\">Johannes Br&#xfc;nger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Simon-Martin Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiko_R/0/1/0/all/0/1\">Rainer Kiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Representation Learning for 3D Point Cloud Data. (arXiv:2110.06632v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06632","description":"<p>Though a number of point cloud learning methods have been proposed to handle\nunordered points, most of them are supervised and require labels for training.\nBy contrast, unsupervised learning of point cloud data has received much less\nattention to date. In this paper, we propose a simple yet effective approach\nfor unsupervised point cloud learning. In particular, we identify a very useful\ntransformation which generates a good contrastive version of an original point\ncloud. They make up a pair. After going through a shared encoder and a shared\nhead network, the consistency between the output representations are maximized\nwith introducing two variants of contrastive losses to respectively facilitate\ndownstream classification and segmentation. To demonstrate the efficacy of our\nmethod, we conduct experiments on three downstream tasks which are 3D object\nclassification (on ModelNet40 and ModelNet10), shape part segmentation (on\nShapeNet Part dataset) as well as scene segmentation (on S3DIS). Comprehensive\nresults show that our unsupervised contrastive representation learning enables\nimpressive outcomes in object classification and semantic segmentation. It\ngenerally outperforms current unsupervised methods, and even achieves\ncomparable performance to supervised methods. Our source codes will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jincen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meili Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADOP: Approximate Differentiable One-Pixel Point Rendering. (arXiv:2110.06635v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06635","description":"<p>We present a novel point-based, differentiable neural rendering pipeline for\nscene refinement and novel view synthesis. The input are an initial estimate of\nthe point cloud and the camera parameters. The output are synthesized images\nfrom arbitrary camera poses. The point cloud rendering is performed by a\ndifferentiable renderer using multi-resolution one-pixel point rasterization.\nSpatial gradients of the discrete rasterization are approximated by the novel\nconcept of ghost geometry. After rendering, the neural image pyramid is passed\nthrough a deep neural network for shading calculations and hole-filling. A\ndifferentiable, physically-based tonemapper then converts the intermediate\noutput to the target image. Since all stages of the pipeline are\ndifferentiable, we optimize all of the scene's parameters i.e. camera model,\ncamera pose, point position, point color, environment map, rendering network\nweights, vignetting, camera response function, per image exposure, and per\nimage white balance. We show that our system is able to synthesize sharper and\nmore consistent novel views than existing approaches because the initial\nreconstruction is refined during training. The efficient one-pixel point\nrasterization allows us to use arbitrary camera models and display scenes with\nwell over 100M points in real time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruckert_D/0/1/0/all/0/1\">Darius R&#xfc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_L/0/1/0/all/0/1\">Linus Franke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamminger_M/0/1/0/all/0/1\">Marc Stamminger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Slag Formations with Deep Convolutional Neural Networks. (arXiv:2110.06640v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06640","description":"<p>We investigate the ability to detect slag formations in images from inside a\nGrate-Kiln system furnace with two deep convolutional neural networks. The\nconditions inside the furnace cause occasional obstructions of the camera view.\nOur approach suggests dealing with this problem by introducing a convLSTM-layer\nin the deep convolutional neural network. The results show that it is possible\nto achieve sufficient performance to automate the decision of timely\ncountermeasures in the industrial operational setting. Furthermore, the\naddition of the convLSTM-layer results in fewer outlying predictions and a\nlower running variance of the fraction of detected slag in the image time\nseries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koch_C/0/1/0/all/0/1\">Christian von Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anzen_W/0/1/0/all/0/1\">William Anz&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Max Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainudiin_R/0/1/0/all/0/1\">Raazesh Sainudiin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EditVAE: Unsupervised Part-Aware Controllable 3D Point Cloud Shape Generation. (arXiv:2110.06679v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06679","description":"<p>This paper tackles the problem of parts-aware point cloud generation. Unlike\nexisting works which require the point cloud to be segmented into parts a\npriori, our parts-aware editing and generation is performed in an unsupervised\nmanner. We achieve this with a simple modification of the Variational\nAuto-Encoder which yields a joint model of the point cloud itself along with a\nschematic representation of it as a combination of shape primitives. In\nparticular, we introduce a latent representation of the point cloud which can\nbe decomposed into a disentangled representation for each part of the shape.\nThese parts are in turn disentangled into both a shape primitive and a point\ncloud representation, along with a standardising transformation to a canonical\ncoordinate system. The dependencies between our standardising transformations\npreserve the spatial dependencies between the parts in a manner which allows\nmeaningful parts-aware point cloud generation and shape editing. In addition to\nthe flexibility afforded by our disentangled representation, the inductive bias\nintroduced by our joint modelling approach yields the state-of-the-art\nexperimental results on the ShapeNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shidi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Color Counting for Fashion, Art, and Design. (arXiv:2110.06682v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06682","description":"<p>Color modelling and extraction is an important topic in fashion, art, and\ndesign. Recommender systems, color-based retrieval, decorating, and fashion\ndesign can benefit from color extraction tools. Research has shown that\nmodeling color so that it can be automatically analyzed and / or extracted is a\ndifficult task. Unlike machines, color perception, although very subjective, is\nmuch simpler for humans. That being said, the first step in color modeling is\nto estimate the number of colors in the item / object. This is because color\nmodels can take advantage of the number of colors as the seed for better\nmodelling, e.g., to make color extraction further deterministic. We aim in this\nwork to develop and test models that can count the number of colors of clothing\nand other items. We propose a novel color counting method based on cumulative\ncolor histogram, which stands out among other methods. We compare the method we\npropose with other methods that utilize exhaustive color search that uses\nGaussian Mixture Models (GMMs) and K-Means as bases for scoring the optimal\nnumber of colors, in addition to another method that relies on deep learning\nmodels. Unfortunately, the GMM, K-Means, and Deep Learning models all fail to\naccurately capture the number of colors. Our proposed method can provide the\ncolor baseline that can be used in AI-based fashion applications, and can also\nfind applications in other areas, for example, interior design. To the best of\nour knowledge, this work is the first of its kind that addresses the problem of\ncolor-counting machine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Rawi_M/0/1/0/all/0/1\">Mohammed Al-Rawi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plugging Self-Supervised Monocular Depth into Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2110.06685v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06685","description":"<p>Although recent semantic segmentation methods have made remarkable progress,\nthey still rely on large amounts of annotated training data, which are often\ninfeasible to collect in the autonomous driving scenario. Previous works\nusually tackle this issue with Unsupervised Domain Adaptation (UDA), which\nentails training a network on synthetic images and applying the model to real\nones while minimizing the discrepancy between the two domains. Yet, these\ntechniques do not consider additional information that may be obtained from\nother tasks. Differently, we propose to exploit self-supervised monocular depth\nestimation to improve UDA for semantic segmentation. On one hand, we deploy\ndepth to realize a plug-in component which can inject complementary geometric\ncues into any existing UDA method. We further rely on depth to generate a large\nand varied set of samples to Self-Train the final model. Our whole proposal\nallows for achieving state-of-the-art performance (58.8 mIoU) in the GTA5-&gt;CS\nbenchmark benchmark. Code is available at\nhttps://github.com/CVLAB-Unibo/d4-dbst.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cardace_A/0/1/0/all/0/1\">Adriano Cardace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luigi_L/0/1/0/all/0/1\">Luca De Luigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_P/0/1/0/all/0/1\">Pierluigi Zama Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salti_S/0/1/0/all/0/1\">Samuele Salti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefano_L/0/1/0/all/0/1\">Luigi Di Stefano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepVecFont: Synthesizing High-quality Vector Fonts via Dual-modality Learning. (arXiv:2110.06688v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06688","description":"<p>Automatic font generation based on deep learning has aroused a lot of\ninterest in the last decade. However, only a few recently-reported approaches\nare capable of directly generating vector glyphs and their results are still\nfar from satisfactory. In this paper, we propose a novel method, DeepVecFont,\nto effectively resolve this problem. Using our method, for the first time,\nvisually-pleasing vector glyphs whose quality and compactness are both\ncomparable to human-designed ones can be automatically generated. The key idea\nof our DeepVecFont is to adopt the techniques of image synthesis, sequence\nmodeling and differentiable rasterization to exhaustively exploit the\ndual-modality information (i.e., raster images and vector outlines) of vector\nfonts. The highlights of this paper are threefold. First, we design a\ndual-modality learning strategy which utilizes both image-aspect and\nsequence-aspect features of fonts to synthesize vector glyphs. Second, we\nprovide a new generative paradigm to handle unstructured data (e.g., vector\nglyphs) by randomly sampling plausible synthesis results to get the optimal one\nwhich is further refined under the guidance of generated structured data (e.g.,\nglyph images). Finally, qualitative and quantitative experiments conducted on a\npublicly-available dataset demonstrate that our method obtains high-quality\nsynthesis results in the applications of vector font generation and\ninterpolation, significantly outperforming the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhouhui Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Image Fusion. (arXiv:2110.06697v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06697","description":"<p>Image fusion methods and metrics for their evaluation have conventionally\nused pixel-based or low-level features. However, for many applications, the aim\nof image fusion is to effectively combine the semantic content of the input\nimages. This paper proposes a novel system for the semantic combination of\nvisual content using pre-trained CNN network architectures. Our proposed\nsemantic fusion is initiated through the fusion of the top layer feature map\noutputs (for each input image)through gradient updating of the fused image\ninput (so-called image optimisation). Simple \"choose maximum\" and \"local\nmajority\" filter based fusion rules are utilised for feature map fusion. This\nprovides a simple method to combine layer outputs and thus a unique framework\nto fuse single-channel and colour images within a decomposition pre-trained for\nclassification and therefore aligned with semantic fusion. Furthermore, class\nactivation mappings of each input image are used to combine semantic\ninformation at a higher level. The developed methods are able to give\nequivalent low-level fusion performance to state of the art methods while\nproviding a unique architecture to combine semantic information from multiple\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hill_P/0/1/0/all/0/1\">P.R. Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">D.R. Bull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Computerized Classification of Micro-Motions in the Hand using Waveforms from Mobile Phone. (arXiv:2110.06723v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06723","description":"<p>Our hands reveal important information such as the pulsing of our veins which\nhelp us determine the blood pressure, tremors indicative of motor control, or\nneurodegenerative disorders such as Essential Tremor or Parkinson's disease.\nThe Computerized Classification of Micro-Motions in the hand using waveforms\nfrom mobile phone videos is a novel method that uses Eulerian Video\nMagnification, Skeletonization, Heatmapping, and the kNN machine learning model\nto detect the micro-motions in the human hand, synthesize their waveforms, and\nclassify these. The pre-processing is achieved by using Eulerian Video\nMagnification, Skeletonization, and Heat-mapping to magnify the micro-motions,\nlandmark essential features of the hand, and determine the extent of motion,\nrespectively. Following pre-processing, the visible motions are manually\nlabeled by appropriately grouping pixels to represent a particular label\ncorrectly. These labeled motions of the pixels are converted into waveforms.\nFinally, these waveforms are classified into four categories - hand or finger\nmovements, vein movement, background motion, and movement of the rest of the\nbody due to respiration using the kNN model. The final accuracy obtained was\naround 92 percent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1\">Ranjani Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RelationRS: Relationship Representation Network for Object Detection in Aerial Images. (arXiv:2110.06730v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06730","description":"<p>Object detection is a basic and important task in the field of aerial image\nprocessing and has gained much attention in computer vision. However, previous\naerial image object detection approaches have insufficient use of scene\nsemantic information between different regions of large-scale aerial images. In\naddition, complex background and scale changes make it difficult to improve\ndetection accuracy. To address these issues, we propose a relationship\nrepresentation network for object detection in aerial images (RelationRS): 1)\nFirstly, multi-scale features are fused and enhanced by a dual relationship\nmodule (DRM) with conditional convolution. The dual relationship module learns\nthe potential relationship between features of different scales and learns the\nrelationship between different scenes from different patches in a same\niteration. In addition, the dual relationship module dynamically generates\nparameters to guide the fusion of multi-scale features. 2) Secondly, The\nbridging visual representations module (BVR) is introduced into the field of\naerial images to improve the object detection effect in images with complex\nbackgrounds. Experiments with a publicly available object detection dataset for\naerial images demonstrate that the proposed RelationRS achieves a\nstate-of-the-art detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chongyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weifeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1\">Haipeng Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiu_J/0/1/0/all/0/1\">Jihong Xiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Semantic Aggregation and Calibration for Separated Domain Generalization. (arXiv:2110.06736v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06736","description":"<p>Domain generalization (DG) aims to learn from multiple known source domains a\nmodel that can generalize well to unknown target domains. The existing DG\nmethods usually rely on shared multi-source data fusion for generalizable model\ntraining. However, tremendous data is distributed across lots of places\nnowadays that can not be shared due to privacy policies, especially in some\ncrucial areas like finance and medical care. A dilemma is thus raised between\nreal-world data privacy protection and simultaneous multi-source semantic\nlearning with the shared data. In this paper, we investigate a separated domain\ngeneralization task with separated source datasets that can only be used\nlocally, which is vital for real-world privacy protection. We propose a novel\nsolution called Collaborative Semantic Aggregation and Calibration (CSAC) to\nenable this challenging task. To fully absorb multi-source semantic information\nwhile avoiding unsafe data fusion, we first conduct data-free semantic\naggregation by fusing the models trained on the separated domains\nlayer-by-layer. To address semantic dislocation caused by domain shift, we\nfurther design cross-layer semantic calibration with an attention mechanism to\nalign each semantic level and enhance domain invariance. We unify multi-source\nsemantic learning and alignment in a collaborative way by repeating the\nsemantic aggregation and calibration alternately, keeping each dataset\nlocalized, and privacy is thus carefully protected. Extensive experiments show\nthe significant performance of our method in addressing this challenging task,\nwhich is even comparable to the previous DG methods with shared data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junkun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transform and Bitstream Domain Image Classification. (arXiv:2110.06740v1 [eess.IV])","link":"http://arxiv.org/abs/2110.06740","description":"<p>Classification of images within the compressed domain offers significant\nbenefits. These benefits include reduced memory and computational requirements\nof a classification system. This paper proposes two such methods as a proof of\nconcept: The first classifies within the JPEG image transform domain (i.e. DCT\ntransform data); the second classifies the JPEG compressed binary bitstream\ndirectly. These two methods are implemented using Residual Network CNNs and an\nadapted Vision Transformer. Top-1 accuracy of approximately 70% and 60% were\nachieved using these methods respectively when classifying the Caltech C101\ndatabase. Although these results are significantly behind the state of the art\nfor classification for this database (~95%), it illustrates the first time\ndirect bitstream image classification has been achieved. This work confirms\nthat direct bitstream image classification is possible and could be utilised in\na first pass database screening of a raw bitstream (within a wired or wireless\nnetwork) or where computational, memory and bandwidth requirements are severely\nrestricted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hill_P/0/1/0/all/0/1\">P.R. Hill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1\">D.R. Bull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Meta Pattern for Face Anti-Spoofing. (arXiv:2110.06753v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06753","description":"<p>Face Anti-Spoofing (FAS) is essential to secure face recognition systems and\nhas been extensively studied in recent years. Although deep neural networks\n(DNNs) for the FAS task have achieved promising results in intra-dataset\nexperiments with similar distributions of training and testing data, the DNNs'\ngeneralization ability is limited under the cross-domain scenarios with\ndifferent distributions of training and testing data. To improve the\ngeneralization ability, recent hybrid methods have been explored to extract\ntask-aware handcrafted features (e.g., Local Binary Pattern) as discriminative\ninformation for the input of DNNs. However, the handcrafted feature extraction\nrelies on experts' domain knowledge, and how to choose appropriate handcrafted\nfeatures is underexplored. To this end, we propose a learnable network to\nextract Meta Pattern (MP) in our learning-to-learn framework. By replacing\nhandcrafted features with the MP, the discriminative information from MP is\ncapable of learning a more generalized model. Moreover, we devise a two-stream\nnetwork to hierarchically fuse the input RGB image and the extracted MP by\nusing our proposed Hierarchical Fusion Module (HFM). We conduct comprehensive\nexperiments and show that our MP outperforms the compared handcrafted features.\nAlso, our proposed method with HFM and the MP can achieve state-of-the-art\nperformance on two different domain generalization evaluation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Rizhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Renjie Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yongjian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex Chichung Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical-Flow-Reuse-Based Bidirectional Recurrent Network for Space-Time Video Super-Resolution. (arXiv:2110.06786v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06786","description":"<p>In this paper, we consider the task of space-time video super-resolution\n(ST-VSR), which simultaneously increases the spatial resolution and frame rate\nfor a given video. However, existing methods typically suffer from difficulties\nin how to efficiently leverage information from a large range of neighboring\nframes or avoiding the speed degradation in the inference using deformable\nConvLSTM strategies for alignment. % Some recent LSTM-based ST-VSR methods have\nachieved promising results. To solve the above problem of the existing methods,\nwe propose a coarse-to-fine bidirectional recurrent neural network instead of\nusing ConvLSTM to leverage knowledge between adjacent frames. Specifically, we\nfirst use bi-directional optical flow to update the hidden state and then\nemploy a Feature Refinement Module (FRM) to refine the result. Since we could\nfully utilize a large range of neighboring frames, our method leverages local\nand global information more effectively. In addition, we propose an optical\nflow-reuse strategy that can reuse the intermediate flow of adjacent frames,\nwhich considerably reduces the computation burden of frame alignment compared\nwith existing LSTM-based designs. Extensive experiments demonstrate that our\noptical-flow-reuse-based bidirectional recurrent network(OFR-BRN) is superior\nto the state-of-the-art methods both in terms of accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuantong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huairui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Layout Generation Algorithm of Graphic Design Based on Transformer-CVAE. (arXiv:2110.06794v1 [cs.HC])","link":"http://arxiv.org/abs/2110.06794","description":"<p>Graphic design is ubiquitous in people's daily lives. For graphic design, the\nmost time-consuming task is laying out various components in the interface.\nRepetitive manual layout design will waste a lot of time for professional\ngraphic designers. Existing templates are usually rudimentary and not suitable\nfor most designs, reducing efficiency and limiting creativity. This paper\nimplemented the Transformer model and conditional variational autoencoder\n(CVAE) to the graphic design layout generation task. It proposed an end-to-end\ngraphic design layout generation model named LayoutT-CVAE. We also proposed\nelement disentanglement and feature-based disentanglement strategies and\nintroduce new graphic design principles and similarity metrics into the model,\nwhich significantly increased the controllability and interpretability of the\ndeep model. Compared with the existing state-of-art models, the layout\ngenerated by ours performs better on many metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mengxi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dangqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaodong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of Attack-Specific Signatures in Adversarial Examples. (arXiv:2110.06802v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06802","description":"<p>The adversarial attack literature contains a myriad of algorithms for\ncrafting perturbations which yield pathological behavior in neural networks. In\nmany cases, multiple algorithms target the same tasks and even enforce the same\nconstraints. In this work, we show that different attack algorithms produce\nadversarial examples which are distinct not only in their effectiveness but\nalso in how they qualitatively affect their victims. We begin by demonstrating\nthat one can determine the attack algorithm that crafted an adversarial\nexample. Then, we leverage recent advances in parameter-space saliency maps to\nshow, both visually and quantitatively, that adversarial attack algorithms\ndiffer in which parts of the network and image they target. Our findings\nsuggest that prospective adversarial attacks should be compared not only via\ntheir success rates at fooling models but also via deeper downstream effects\nthey have on victims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1\">Hossein Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorramshahi_P/0/1/0/all/0/1\">Pirazh Khorramshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1\">Chun Pong Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Ignore: Domain Adaptation for Multi-Site MRI Analysis. (arXiv:2110.06803v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06803","description":"<p>Limited availability of large image datasets is a major issue in the\ndevelopment of accurate and generalizable machine learning methods in medicine.\nThe limitations in the amount of data are mainly due to the use of different\nacquisition protocols, different hardware, and data privacy. At the same time,\ntraining a classification model on a small dataset leads to a poor\ngeneralization quality of the model. To overcome this issue, a combination of\nvarious image datasets of different provenance is often used, e.g., multi-site\nstudies. However, if an additional dataset does not include all classes of the\ntask, the learning of the classification model can be biased to the device or\nplace of acquisition.\n</p>\n<p>This is especially the case for Magnetic Resonance (MR) images, where\ndifferent MR scanners introduce a bias that limits the performance of the\nmodel. In this paper, we present a novel method that learns to ignore the\nscanner-related features present in the images, while learning features\nrelevant for the classification task. We focus on a real-world scenario, where\nonly a small dataset provides images of all classes. We exploit this\ncircumstance by introducing specific additional constraints on the latent\nspace, which lead the focus on disease-related rather than scanner-specific\nfeatures. Our method Learn to Ignore outperforms state-of-the-art domain\nadaptation methods on a multi-site MRI dataset on a classification task between\nMultiple Sclerosis patients and healthy subjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolleb_J/0/1/0/all/0/1\">Julia Wolleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandkuhler_R/0/1/0/all/0/1\">Robin Sandk&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barakovic_M/0/1/0/all/0/1\">Muhamed Barakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulou_A/0/1/0/all/0/1\">Athina Papadopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadjikhani_N/0/1/0/all/0/1\">Nouchine Hadjikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaldizli_O/0/1/0/all/0/1\">&#xd6;zg&#xfc;r Yaldizli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhle_J/0/1/0/all/0/1\">Jens Kuhle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granziera_C/0/1/0/all/0/1\">Cristina Granziera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattin_P/0/1/0/all/0/1\">Philippe C. Cattin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comprehensive review of Binary Neural Network. (arXiv:2110.06804v1 [cs.NE])","link":"http://arxiv.org/abs/2110.06804","description":"<p>Binary Neural Network (BNN) method is an extreme application of convolutional\nneural network (CNN) parameter quantization. As opposed to the original CNN\nmethods which employed floating-point computation with full-precision weights\nand activations, BBN uses 1-bit activations and weights. With BBNs, a\nsignificant amount of storage, network complexity and energy consumption can be\nreduced, and neural networks can be implemented more efficiently in embedded\napplications. Unfortunately, binarization causes severe information loss. A gap\nstill exists between full-precision CNN models and their binarized\ncounterparts. The recent developments in BNN have led to a lot of algorithms\nand solutions that have helped address this issue. This article provides a full\noverview of recent developments in BNN. The present paper focuses exclusively\non 1-bit activations and weights networks, as opposed to previous surveys in\nwhich low-bit works are mixed in. In this paper, we conduct a complete\ninvestigation of BNN's development from their predecessors to the latest BNN\nalgorithms and techniques, presenting a broad design pipeline, and discussing\neach module's variants. Along the way, this paper examines BNN (a) purpose:\ntheir early successes and challenges; (b) BNN optimization: selected\nrepresentative works that contain key optimization techniques; (c) deployment:\nopen-source frameworks for BNN modeling and development; (d) terminal:\nefficient computing architectures and devices for BNN and (e) applications:\ndiverse applications with BNN. Moreover, this paper discusses potential\ndirections and future research opportunities for the latest BNN algorithms and\ntechniques, presents a broad design pipeline, and discusses each module's\nvariants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agaian_S/0/1/0/all/0/1\">Sos S. Agaian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Verification of Wasserstein Adversarial Robustness. (arXiv:2110.06816v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06816","description":"<p>Machine learning image classifiers are susceptible to adversarial and\ncorruption perturbations. Adding imperceptible noise to images can lead to\nsevere misclassifications of the machine learning model. Using $L_p$-norms for\nmeasuring the size of the noise fails to capture human similarity perception,\nwhich is why optimal transport based distance measures like the Wasserstein\nmetric are increasingly being used in the field of adversarial robustness.\nVerifying the robustness of classifiers using the Wasserstein metric can be\nachieved by proving the absence of adversarial examples (certification) or\nproving their presence (attack). In this work we present a framework based on\nthe work by Levine and Feizi, which allows us to transfer existing\ncertification methods for convex polytopes or $L_1$-balls to the Wasserstein\nthreat model. The resulting certification can be complete or incomplete,\ndepending on whether convex polytopes or $L_1$-balls were chosen. Additionally,\nwe present a new Wasserstein adversarial attack that is projected gradient\ndescent based and which has a significantly reduced computational burden\ncompared to existing attack approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wegel_T/0/1/0/all/0/1\">Tobias Wegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assion_F/0/1/0/all/0/1\">Felix Assion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mickisch_D/0/1/0/all/0/1\">David Mickisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gressner_F/0/1/0/all/0/1\">Florens Gre&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Character Recognition of 19th Century Classical Commentaries: the Current State of Affairs. (arXiv:2110.06817v1 [cs.DL])","link":"http://arxiv.org/abs/2110.06817","description":"<p>Together with critical editions and translations, commentaries are one of the\nmain genres of publication in literary and textual scholarship, and have a\ncentury-long tradition. Yet, the exploitation of thousands of digitized\nhistorical commentaries was hitherto hindered by the poor quality of Optical\nCharacter Recognition (OCR), especially on commentaries to Greek texts. In this\npaper, we evaluate the performances of two pipelines suitable for the OCR of\nhistorical classical commentaries. Our results show that Kraken + Ciaconna\nreaches a substantially lower character error rate (CER) than Tesseract/OCR-D\non commentary sections with high density of polytonic Greek text (average CER\n7% vs. 13%), while Tesseract/OCR-D is slightly more accurate than Kraken +\nCiaconna on text sections written predominantly in Latin script (average CER\n8.2% vs. 8.4%). As part of this paper, we also release GT4HistComment, a small\ndataset with OCR ground truth for 19th classical commentaries and Pogretra, a\nlarge collection of training data and pre-trained models for a wide variety of\nancient Greek typefaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romanello_M/0/1/0/all/0/1\">Matteo Romanello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najem_Meyer_S/0/1/0/all/0/1\">Sven Najem-Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_B/0/1/0/all/0/1\">Bruce Robertson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging redundancy in attention with Reuse Transformers. (arXiv:2110.06821v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06821","description":"<p>Pairwise dot product-based attention allows Transformers to exchange\ninformation between tokens in an input-dependent way, and is key to their\nsuccess across diverse applications in language and vision. However, a typical\nTransformer model computes such pairwise attention scores repeatedly for the\nsame sequence, in multiple heads in multiple layers. We systematically analyze\nthe empirical similarity of these scores across heads and layers and find them\nto be considerably redundant, especially adjacent layers showing high\nsimilarity. Motivated by these findings, we propose a novel architecture that\nreuses attention scores computed in one layer in multiple subsequent layers.\nExperiments on a number of standard benchmarks show that reusing attention\ndelivers performance equivalent to or better than standard transformers, while\nreducing both compute and memory usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1\">Ayan Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1\">Himanshu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yin-Wen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoisyActions2M: A Multimedia Dataset for Video Understanding from Noisy Labels. (arXiv:2110.06827v1 [cs.MM])","link":"http://arxiv.org/abs/2110.06827","description":"<p>Deep learning has shown remarkable progress in a wide range of problems.\nHowever, efficient training of such models requires large-scale datasets, and\ngetting annotations for such datasets can be challenging and costly. In this\nwork, we explore the use of user-generated freely available labels from web\nvideos for video understanding. We create a benchmark dataset consisting of\naround 2 million videos with associated user-generated annotations and other\nmeta information. We utilize the collected dataset for action classification\nand demonstrate its usefulness with existing small-scale annotated datasets,\nUCF101 and HMDB51. We study different loss functions and two pretraining\nstrategies, simple and self-supervised learning. We also show how a network\npretrained on the proposed dataset can help against video corruption and label\nnoise in downstream datasets. We present this as a benchmark dataset in noisy\nlearning for video understanding. The dataset, code, and trained models will be\npublicly available for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mohit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_R/0/1/0/all/0/1\">Raj Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_H/0/1/0/all/0/1\">Harshal Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1\">Shruti Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONetV2: Efficient Auto-Channel Size Optimization for CNNs. (arXiv:2110.06830v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06830","description":"<p>Neural Architecture Search (NAS) has been pivotal in finding optimal network\nconfigurations for Convolution Neural Networks (CNNs). While many methods\nexplore NAS from a global search-space perspective, the employed optimization\nschemes typically require heavy computational resources. This work introduces a\nmethod that is efficient in computationally constrained environments by\nexamining the micro-search space of channel size. In tackling channel-size\noptimization, we design an automated algorithm to extract the dependencies\nwithin different connected layers of the network. In addition, we introduce the\nidea of knowledge distillation, which enables preservation of trained weights,\nadmist trials where the channel sizes are changing. Further, since the standard\nperformance indicators (accuracy, loss) fail to capture the performance of\nindividual network components (providing an overall network evaluation), we\nintroduce a novel metric that highly correlates with test accuracy and enables\nanalysis of individual network layers. Combining dependency extraction,\nmetrics, and knowledge distillation, we introduce an efficient searching\nalgorithm, with simulated annealing inspired stochasticity, and demonstrate its\neffectiveness in finding optimal architectures that outperform baselines by a\nlarge margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Ru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1\">Samir Khaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weihang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mahdi S. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting the Certified Robustness of L-infinity Distance Nets. (arXiv:2110.06850v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06850","description":"<p>Recently, Zhang et al. (2021) developed a new neural network architecture\nbased on $\\ell_\\infty$-distance functions, which naturally possesses certified\nrobustness by its construction. Despite the excellent theoretical properties,\nthe model so far can only achieve comparable performance to conventional\nnetworks. In this paper, we significantly boost the certified robustness of\n$\\ell_\\infty$-distance nets through a careful analysis of its training process.\nIn particular, we show the $\\ell_p$-relaxation, a crucial way to overcome the\nnon-smoothness of the model, leads to an unexpected large Lipschitz constant at\nthe early training stage. This makes the optimization insufficient using hinge\nloss and produces sub-optimal solutions. Given these findings, we propose a\nsimple approach to address the issues above by using a novel objective function\nthat combines a scaled cross-entropy loss with clipped hinge loss. Our\nexperiments show that using the proposed training strategy, the certified\naccuracy of $\\ell_\\infty$-distance net can be dramatically improved from 33.30%\nto 40.06% on CIFAR-10 ($\\epsilon=8/255$), meanwhile significantly outperforming\nother approaches in this area. Such a result clearly demonstrates the\neffectiveness and potential of $\\ell_\\infty$-distance net for certified\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Du Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive and Contrastive Learning for Joint Depth and Motion Field Estimation. (arXiv:2110.06853v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06853","description":"<p>Estimating the motion of the camera together with the 3D structure of the\nscene from a monocular vision system is a complex task that often relies on the\nso-called scene rigidity assumption. When observing a dynamic environment, this\nassumption is violated which leads to an ambiguity between the ego-motion of\nthe camera and the motion of the objects. To solve this problem, we present a\nself-supervised learning framework for 3D object motion field estimation from\nmonocular videos. Our contributions are two-fold. First, we propose a two-stage\nprojection pipeline to explicitly disentangle the camera ego-motion and the\nobject motions with dynamics attention module, called DAM. Specifically, we\ndesign an integrated motion model that estimates the motion of the camera and\nobject in the first and second warping stages, respectively, controlled by the\nattention module through a shared motion encoder. Second, we propose an object\nmotion field estimation through contrastive sample consensus, called CSAC,\ntaking advantage of weak semantic prior (bounding box from an object detector)\nand geometric constraints (each object respects the rigid body motion model).\nExperiments on KITTI, Cityscapes, and Waymo Open Dataset demonstrate the\nrelevance of our approach and show that our method outperforms state-of-the-art\nalgorithms for the tasks of self-supervised monocular depth estimation, object\nmotion segmentation, monocular scene flow estimation, and visual odometry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seokju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Users' Mental Model with Attention-directed Counterfactual Edits. (arXiv:2110.06863v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06863","description":"<p>In the domain of Visual Question Answering (VQA), studies have shown\nimprovement in users' mental model of the VQA system when they are exposed to\nexamples of how these systems answer certain Image-Question (IQ) pairs. In this\nwork, we show that showing controlled counterfactual image-question examples\nare more effective at improving the mental model of users as compared to simply\nshowing random examples. We compare a generative approach and a retrieval-based\napproach to show counterfactual examples. We use recent advances in generative\nadversarial networks (GANs) to generate counterfactual images by deleting and\ninpainting certain regions of interest in the image. We then expose users to\nchanges in the VQA system's answer on those altered images. To select the\nregion of interest for inpainting, we experiment with using both\nhuman-annotated attention maps and a fully automatic method that uses the VQA\nsystem's attention values. Finally, we test the user's mental model by asking\nthem to predict the model's performance on a test counterfactual image. We note\nan overall improvement in users' accuracy to predict answer change when shown\ncounterfactual explanations. While realistic retrieved counterfactuals\nobviously are the most effective at improving the mental model, we show that a\ngenerative approach can also be equally effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alipour_K/0/1/0/all/0/1\">Kamran Alipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Arijit Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1\">Michael Cogswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulze_J/0/1/0/all/0/1\">Jurgen P. Schulze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burachas_G/0/1/0/all/0/1\">Giedrius T. Burachas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ByteTrack: Multi-Object Tracking by Associating Every Detection Box. (arXiv:2110.06864v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06864","description":"<p>Multi-object tracking (MOT) aims at estimating bounding boxes and identities\nof objects in videos. Most methods obtain identities by associating detection\nboxes whose scores are higher than a threshold. The objects with low detection\nscores, e.g. occluded objects, are simply thrown away, which brings\nnon-negligible true object missing and fragmented trajectories. To solve this\nproblem, we present a simple, effective and generic association method, called\nBYTE, tracking BY associaTing Every detection box instead of only the high\nscore ones. For the low score detection boxes, we utilize their similarities\nwith tracklets to recover true objects and filter out the background\ndetections. We apply BYTE to 9 different state-of-the-art trackers and achieve\nconsistent improvement on IDF1 score ranging from 1 to 10 points. To put\nforwards the state-of-the-art performance of MOT, we design a simple and strong\ntracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1\nand 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single\nV100 GPU. The source code, pre-trained models with deploy versions and\ntutorials of applying to other trackers are released at\n\\url{https://github.com/ifzhang/ByteTrack}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review on Human Pose Estimation. (arXiv:2110.06877v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06877","description":"<p>The phenomenon of Human Pose Estimation (HPE) is a problem that has been\nexplored over the years, particularly in computer vision. But what exactly is\nit? To answer this, the concept of a pose must first be understood. Pose can be\ndefined as the arrangement of human joints in a specific manner. Therefore, we\ncan define the problem of Human Pose Estimation as the localization of human\njoints or predefined landmarks in images and videos. There are several types of\npose estimation, including body, face, and hand, as well as many aspects to it.\nThis paper will cover them, starting with the classical approaches to HPE to\nthe Deep Learning based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Josyula_R/0/1/0/all/0/1\">Rohit Josyula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Region Video Transformers. (arXiv:2110.06915v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06915","description":"<p>Evidence from cognitive psychology suggests that understanding\nspatio-temporal object interactions and dynamics can be essential for\nrecognizing actions in complex videos. Therefore, action recognition models are\nexpected to benefit from explicit modeling of objects, including their\nappearance, interaction, and dynamics. Recently, video transformers have shown\ngreat success in video understanding, exceeding CNN performance. Yet, existing\nvideo transformer models do not explicitly model objects. In this work, we\npresent Object-Region Video Transformers (ORViT), an \\emph{object-centric}\napproach that extends video transformer layers with a block that directly\nincorporates object representations. The key idea is to fuse object-centric\nspatio-temporal representations throughout multiple transformer layers. Our\nORViT block consists of two object-level streams: appearance and dynamics. In\nthe appearance stream, an ``Object-Region Attention'' element applies\nself-attention over the patches and \\emph{object regions}. In this way, visual\nobject regions interact with uniform patch tokens and enrich them with\ncontextualized object information. We further model object dynamics via a\nseparate ``Object-Dynamics Module'', which captures trajectory interactions,\nand show how to integrate the two streams. We evaluate our model on standard\nand compositional action recognition on Something-Something V2, standard action\nrecognition on Epic-Kitchen100 and Diving48, and spatio-temporal action\ndetection on AVA. We show strong improvement in performance across all tasks\nand datasets considered, demonstrating the value of a model that incorporates\nobject representations into a transformer architecture. For code and pretrained\nmodels, visit the project page at https://roeiherz.github.io/ORViT/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1\">Roei Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Avraham_E/0/1/0/all/0/1\">Elad Ben-Avraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1\">Amir Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries. (arXiv:2110.06922v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06922","description":"<p>We introduce a framework for multi-camera 3D object detection. In contrast to\nexisting works, which estimate 3D bounding boxes directly from monocular images\nor use depth prediction networks to generate input for 3D object detection from\n2D information, our method manipulates predictions directly in 3D space. Our\narchitecture extracts 2D features from multiple camera images and then uses a\nsparse set of 3D object queries to index into these 2D features, linking 3D\npositions to multi-view images using camera transformation matrices. Finally,\nour model makes a bounding box prediction per object query, using a set-to-set\nloss to measure the discrepancy between the ground-truth and the prediction.\nThis top-down approach outperforms its bottom-up counterpart in which object\nbounding box prediction follows per-pixel depth estimation, since it does not\nsuffer from the compounding error introduced by a depth prediction model.\nMoreover, our method does not require post-processing such as non-maximum\nsuppression, dramatically improving inference speed. We achieve\nstate-of-the-art performance on the nuScenes autonomous driving benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1\">Justin Solomon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object DGCNN: 3D Object Detection using Dynamic Graphs. (arXiv:2110.06923v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06923","description":"<p>3D object detection often involves complicated training and testing\npipelines, which require substantial domain knowledge about individual\ndatasets. Inspired by recent non-maximum suppression-free 2D object detection\nmodels, we propose a 3D object detection architecture on point clouds. Our\nmethod models 3D object detection as message passing on a dynamic graph,\ngeneralizing the DGCNN framework to predict a set of objects. In our\nconstruction, we remove the necessity of post-processing via object confidence\naggregation or non-maximum suppression. To facilitate object detection from\nsparse point clouds, we also propose a set-to-set distillation approach\ncustomized to 3D detection. This approach aligns the outputs of the teacher\nmodel and the student model in a permutation-invariant fashion, significantly\nsimplifying knowledge distillation for the 3D detection task. Our method\nachieves state-of-the-art performance on autonomous driving benchmarks. We also\nprovide abundant analysis of the detection model and distillation framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1\">Justin Solomon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Graph Data Learning via Latent Graph Convolutional Representation. (arXiv:1904.11883v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.11883","description":"<p>Graph Convolutional Representation (GCR) has achieved impressive performance\nfor graph data representation. However, existing GCR is generally defined on\nthe input fixed graph which may restrict the representation capacity and also\nbe vulnerable to the structural attacks and noises. To address this issue, we\npropose a novel Latent Graph Convolutional Representation (LatGCR) for robust\ngraph data representation and learning. Our LatGCR is derived based on\nreformulating graph convolutional representation from the aspect of graph\nneighborhood reconstruction. Given an input graph $\\textbf{A}$, LatGCR aims to\ngenerate a flexible latent graph $\\widetilde{\\textbf{A}}$ for graph\nconvolutional representation which obviously enhances the representation\ncapacity and also performs robustly w.r.t graph structural attacks and noises.\nMoreover, LatGCR is implemented in a self-supervised manner and thus provides a\nbasic block for both supervised and unsupervised graph learning tasks.\nExperiments on several datasets demonstrate the effectiveness and robustness of\nLatGCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Objectness-Aware Few-Shot Semantic Segmentation. (arXiv:2004.02945v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.02945","description":"<p>Few-shot semantic segmentation models aim to segment images after learning\nfrom only a few annotated examples. A key challenge for them is how to avoid\noverfitting because limited training data is available. While prior works\nusually limited the overall model capacity to alleviate overfitting, this\nhampers segmentation accuracy. We demonstrate how to increase overall model\ncapacity to achieve improved performance, by introducing objectness, which is\nclass-agnostic and so not prone to overfitting, for complementary use with\nclass-specific features. Extensive experiments demonstrate the versatility of\nour simple approach of introducing objectness for different base architectures\nthat rely on different data loaders and training schedules (DENet, PFENet) as\nwell as with different backbone models (ResNet-50, ResNet-101 and HRNetV2-W48).\nGiven only one annotated example of an unseen category, experiments show that\nour method outperforms state-of-art methods with respect to mIoU by at least\n4.7% and 1.5% on PASCAL-5i and COCO-20i respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yinan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Few-Shot Video Classification with Video Retrieval and Feature Generation. (arXiv:2007.04755v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.04755","description":"<p>Few-shot learning aims to recognize novel classes from a few examples.\nAlthough significant progress has been made in the image domain, few-shot video\nclassification is relatively unexplored. We argue that previous methods\nunderestimate the importance of video feature learning and propose to learn\nspatiotemporal features using a 3D CNN. Proposing a two-stage approach that\nlearns video features on base classes followed by fine-tuning the classifiers\non novel classes, we show that this simple baseline approach outperforms prior\nfew-shot video classification methods by over 20 points on existing benchmarks.\nTo circumvent the need of labeled examples, we present two novel approaches\nthat yield further improvement. First, we leverage tag-labeled videos from a\nlarge dataset using tag retrieval followed by selecting the best clips with\nvisual similarities. Second, we learn generative adversarial networks that\ngenerate video features of novel classes from their semantic embeddings.\nMoreover, we find existing benchmarks are limited because they only focus on 5\nnovel classes in each testing episode and introduce more realistic benchmarks\nby involving more novel classes, i.e. few-shot learning, as well as a mixture\nof novel and base classes, i.e. generalized few-shot learning. The experimental\nresults show that our retrieval and feature generation approach significantly\noutperform the baseline approach on the new benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yongqin Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbar_B/0/1/0/all/0/1\">Bruno Korbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues. (arXiv:2007.12131v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.12131","description":"<p>Recent progress in fine-grained gesture and action classification, and\nmachine translation, point to the possibility of automated sign language\nrecognition becoming a reality. A key stumbling block in making progress\ntowards this goal is a lack of appropriate training data, stemming from the\nhigh complexity of sign annotation and a limited supply of qualified\nannotators. In this work, we introduce a new scalable approach to data\ncollection for sign recognition in continuous videos. We make use of\nweakly-aligned subtitles for broadcast footage together with a keyword spotting\nmethod to automatically localise sign-instances for a vocabulary of 1,000 signs\nin 1,000 hours of video. We make the following contributions: (1) We show how\nto use mouthing cues from signers to obtain high-quality annotations from video\ndata - the result is the BSL-1K dataset, a collection of British Sign Language\n(BSL) signs of unprecedented scale; (2) We show that we can use BSL-1K to train\nstrong sign recognition models for co-articulated signs in BSL and that these\nmodels additionally form excellent pretraining for other sign languages and\nbenchmarks - we exceed the state of the art on both the MSASL and WLASL\nbenchmarks. Finally, (3) we propose new large-scale evaluation sets for the\ntasks of sign recognition and sign spotting and provide baselines which we hope\nwill serve to stimulate research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momeni_L/0/1/0/all/0/1\">Liliane Momeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1\">Triantafyllos Afouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_N/0/1/0/all/0/1\">Neil Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Self-supervised Monocular Depth Estimation. (arXiv:2009.07714v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.07714","description":"<p>In the recent years, many methods demonstrated the ability of neural networks\nto learn depth and pose changes in a sequence of images, using only\nself-supervision as the training signal. Whilst the networks achieve good\nperformance, the often over-looked detail is that due to the inherent ambiguity\nof monocular vision they predict depth up to an unknown scaling factor. The\nscaling factor is then typically obtained from the LiDAR ground truth at test\ntime, which severely limits practical applications of these methods. In this\npaper, we show that incorporating prior information about the camera\nconfiguration and the environment, we can remove the scale ambiguity and\npredict depth directly, still using the self-supervised formulation and not\nrelying on any additional sensors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McCraith_R/0/1/0/all/0/1\">Robert McCraith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_L/0/1/0/all/0/1\">Lukas Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arbitrary-Oriented Ship Detection through Center-Head Point Extraction. (arXiv:2101.11189v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11189","description":"<p>Ship detection in remote sensing images plays a crucial role in various\napplications and has drawn increasing attention in recent years. However,\nexisting arbitrary-oriented ship detection methods are generally developed on a\nset of predefined rotated anchor boxes. These predefined boxes not only lead to\ninaccurate angle predictions but also introduce extra hyper-parameters and high\ncomputational cost. Moreover, the prior knowledge of ship size has not been\nfully exploited by existing methods, which hinders the improvement of their\ndetection accuracy. Aiming at solving the above issues, in this paper, we\npropose a center-head point extraction based detector (named CHPDet) to achieve\narbitrary-oriented ship detection in remote sensing images. Our CHPDet\nformulates arbitrary-oriented ships as rotated boxes with head points which are\nused to determine the direction. And rotated Gaussian kernel is used to map the\nannotations into target heatmaps. Keypoint estimation is performed to find the\ncenter of ships. Then, the size and head point of the ships are regressed. The\norientation-invariant model (OIM) is also used to produce orientation-invariant\nfeature maps. Finally, we use the target size as prior to finetune the results.\nMoreover, we introduce a new dataset for multi-class arbitrary-oriented ship\ndetection in remote sensing images at a fixed ground sample distance (GSD)\nwhich is named FGSD2021. Experimental results on FGSD2021 and two other widely\nused data sets, i.e., HRSC2016, and UCAS-AOD demonstrate that our CHPDet\nachieves state-of-the-art performance and can well distinguish between bow and\nstern. Code and FGSD2021 dataset are available at\nhttps://github.com/zf020114/CHPDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yi Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Framing of Science Conspiracy Videos: Integrating Machine Learning with Communication Theories to Study the Use of Color and Brightness. (arXiv:2102.01163v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2102.01163","description":"<p>Recent years have witnessed an explosion of science conspiracy videos on the\nInternet, challenging science epistemology and public understanding of science.\nScholars have started to examine the persuasion techniques used in conspiracy\nmessages such as uncertainty and fear yet, little is understood about the\nvisual narratives, especially how visual narratives differ in videos that\ndebunk conspiracies versus those that propagate conspiracies. This paper\naddresses this gap in understanding visual framing in conspiracy videos through\nanalyzing millions of frames from conspiracy and counter-conspiracy YouTube\nvideos using computational methods. We found that conspiracy videos tended to\nuse lower color variance and brightness, especially in thumbnails and earlier\nparts of the videos. This paper also demonstrates how researchers can integrate\ntextual and visual features in machine learning models to study conspiracies on\nsocial media and discusses the implications of computational modeling for\nscholars interested in studying visual manipulation in the digital era. The\nanalysis of visual and textual features presented in this paper could be useful\nfor future studies focused on designing systems to identify conspiracy content\non the Internet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sang Jung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiantong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raschka_S/0/1/0/all/0/1\">Sebastian Raschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cycle Self-Training for Domain Adaptation. (arXiv:2103.03571v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.03571","description":"<p>Mainstream approaches for unsupervised domain adaptation (UDA) learn\ndomain-invariant representations to narrow the domain shift. Recently,\nself-training has been gaining momentum in UDA, which exploits unlabeled target\ndata by training with target pseudo-labels. However, as corroborated in this\nwork, under distributional shift in UDA, the pseudo-labels can be unreliable in\nterms of their large discrepancy from target ground truth. Thereby, we propose\nCycle Self-Training (CST), a principled self-training algorithm that explicitly\nenforces pseudo-labels to generalize across domains. CST cycles between a\nforward step and a reverse step until convergence. In the forward step, CST\ngenerates target pseudo-labels with a source-trained classifier. In the reverse\nstep, CST trains a target classifier using target pseudo-labels, and then\nupdates the shared representations to make the target classifier perform well\non the source data. We introduce the Tsallis entropy as a confidence-friendly\nregularization to improve the quality of target pseudo-labels. We analyze CST\ntheoretically under realistic assumptions, and provide hard cases where CST\nrecovers target ground truth, while both invariant feature learning and vanilla\nself-training fail. Empirical results indicate that CST significantly improves\nover the state-of-the-arts on visual recognition and sentiment analysis\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Busy-Quiet Video Disentangling for Video Classification. (arXiv:2103.15584v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15584","description":"<p>In video data, busy motion details from moving regions are conveyed within a\nspecific frequency bandwidth in the frequency domain. Meanwhile, the rest of\nthe frequencies of video data are encoded with quiet information with\nsubstantial redundancy, which causes low processing efficiency in existing\nvideo models that take as input raw RGB frames. In this paper, we consider\nallocating intenser computation for the processing of the important busy\ninformation and less computation for that of the quiet information. We design a\ntrainable Motion Band-Pass Module (MBPM) for separating busy information from\nquiet information in raw video data. By embedding the MBPM into a two-pathway\nCNN architecture, we define a Busy-Quiet Net (BQN). The efficiency of BQN is\ndetermined by avoiding redundancy in the feature space processed by the two\npathways: one operating on Quiet features of low-resolution, while the other\nprocesses Busy features. The proposed BQN outperforms many recent video\nprocessing models on Something-Something V1, Kinetics400, UCF101 and HMDB51\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoxi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"von Mises-Fisher Loss: An Exploration of Embedding Geometries for Supervised Learning. (arXiv:2103.15718v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.15718","description":"<p>Recent work has argued that classification losses utilizing softmax\ncross-entropy are superior not only for fixed-set classification tasks, but\nalso by outperforming losses developed specifically for open-set tasks\nincluding few-shot learning and retrieval. Softmax classifiers have been\nstudied using different embedding geometries -- Euclidean, hyperbolic, and\nspherical -- and claims have been made about the superiority of one or another,\nbut they have not been systematically compared with careful controls. We\nconduct an empirical investigation of embedding geometry on softmax losses for\na variety of fixed-set classification and image retrieval tasks. An interesting\nproperty observed for the spherical losses lead us to propose a probabilistic\nclassifier based on the von Mises-Fisher distribution, and we show that it is\ncompetitive with state-of-the-art methods while producing improved\nout-of-the-box calibration. We provide guidance regarding the trade-offs\nbetween losses and how to choose among them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scott_T/0/1/0/all/0/1\">Tyler R. Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_A/0/1/0/all/0/1\">Andrew C. Gallagher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Regression on Manifolds: A 3D Rotation Case Study. (arXiv:2103.16317v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16317","description":"<p>Many machine learning problems involve regressing variables on a\nnon-Euclidean manifold -- e.g. a discrete probability distribution, or the 6D\npose of an object. One way to tackle these problems through gradient-based\nlearning is to use a differentiable function that maps arbitrary inputs of a\nEuclidean space onto the manifold. In this paper, we establish a set of\ndesirable properties for such mapping, and in particular highlight the\nimportance of pre-images connectivity/convexity. We illustrate these properties\nwith a case study regarding 3D rotations. Through theoretical considerations\nand methodological experiments on a variety of tasks, we review various\ndifferentiable mappings on the 3D rotation space, and conjecture about the\nimportance of their local linearity. We show that a mapping based on Procrustes\northonormalization generally performs best among the mappings considered, but\nthat a rotation vector representation might also be suitable when restricted to\nsmall angles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bregier_R/0/1/0/all/0/1\">Romain Br&#xe9;gier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenGAN: Open-Set Recognition via Open Data Generation. (arXiv:2104.02939v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02939","description":"<p>Real-world machine learning systems need to analyze test data that may differ\nfrom training data. In K-way classification, this is crisply formulated as\nopen-set recognition, core to which is the ability to discriminate open-set\ndata outside the K closed-set classes. Two conceptually elegant ideas for\nopen-set discrimination are: 1) discriminatively learning an open-vs-closed\nbinary discriminator by exploiting some outlier data as the open-set, and 2)\nunsupervised learning the closed-set data distribution with a GAN, using its\ndiscriminator as the open-set likelihood function. However, the former\ngeneralizes poorly to diverse open test data due to overfitting to the training\noutliers, which are unlikely to exhaustively span the open-world. The latter\ndoes not work well, presumably due to the instable training of GANs. Motivated\nby the above, we propose OpenGAN, which addresses the limitation of each\napproach by combining them with several technical insights. First, we show that\na carefully selected GAN-discriminator on some real outlier data already\nachieves the state-of-the-art. Second, we augment the available set of real\nopen training examples with adversarially synthesized \"fake\" data. Third and\nmost importantly, we build the discriminator over the features computed by the\nclosed-world K-way networks. This allows OpenGAN to be implemented via a\nlightweight discriminator head built on top of an existing K-way network.\nExtensive experiments show that OpenGAN significantly outperforms prior\nopen-set methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Shu Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Pedestrian Crossing Intention with Feature Fusion and Spatio-Temporal Attention. (arXiv:2104.05485v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05485","description":"<p>Predicting vulnerable road user behavior is an essential prerequisite for\ndeploying Automated Driving Systems (ADS) in the real-world. Pedestrian\ncrossing intention should be recognized in real-time, especially for urban\ndriving. Recent works have shown the potential of using vision-based deep\nneural network models for this task. However, these models are not robust and\ncertain issues still need to be resolved. First, the global spatio-temproal\ncontext that accounts for the interaction between the target pedestrian and the\nscene has not been properly utilized. Second, the optimum strategy for fusing\ndifferent sensor data has not been thoroughly investigated. This work addresses\nthe above limitations by introducing a novel neural network architecture to\nfuse inherently different spatio-temporal features for pedestrian crossing\nintention prediction. We fuse different phenomena such as sequences of RGB\nimagery, semantic segmentation masks, and ego-vehicle speed in an optimum way\nusing attention mechanisms and a stack of recurrent neural networks. The\noptimum architecture was obtained through exhaustive ablation and comparison\nstudies. Extensive comparative experiments on the JAAD pedestrian action\nprediction benchmark demonstrate the effectiveness of the proposed method,\nwhere state-of-the-art performance was achieved. Our code is open-source and\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongfang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurtsever_E/0/1/0/all/0/1\">Ekim Yurtsever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redmill_K/0/1/0/all/0/1\">Keith Redmill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozguner_U/0/1/0/all/0/1\">&#xdc;mit &#xd6;zg&#xfc;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXplainable Neural-Symbolic Learning (X-NeSyL) methodology to fuse deep learning representations with expert knowledge graphs: the MonuMAI cultural heritage use case. (arXiv:2104.11914v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.11914","description":"<p>The latest Deep Learning (DL) models for detection and classification have\nachieved an unprecedented performance over classical machine learning\nalgorithms. However, DL models are black-box methods hard to debug, interpret,\nand certify. DL alone cannot provide explanations that can be validated by a\nnon technical audience. In contrast, symbolic AI systems that convert concepts\ninto rules or symbols -- such as knowledge graphs -- are easier to explain.\nHowever, they present lower generalisation and scaling capabilities. A very\nimportant challenge is to fuse DL representations with expert knowledge. One\nway to address this challenge, as well as the performance-explainability\ntrade-off is by leveraging the best of both streams without obviating domain\nexpert knowledge. We tackle such problem by considering the symbolic knowledge\nis expressed in form of a domain expert knowledge graph. We present the\neXplainable Neural-symbolic learning (X-NeSyL) methodology, designed to learn\nboth symbolic and deep representations, together with an explainability metric\nto assess the level of alignment of machine and human expert explanations. The\nultimate objective is to fuse DL representations with expert domain knowledge\nduring the learning process to serve as a sound basis for explainability.\nX-NeSyL methodology involves the concrete use of two notions of explanation at\ninference and training time respectively: 1) EXPLANet: Expert-aligned\neXplainable Part-based cLAssifier NETwork Architecture, a compositional CNN\nthat makes use of symbolic representations, and 2) SHAP-Backprop, an\nexplainable AI-informed training procedure that guides the DL process to align\nwith such symbolic representations in form of knowledge graphs. We showcase\nX-NeSyL methodology using MonuMAI dataset for monument facade image\nclassification, and demonstrate that our approach improves explainability and\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1\">Natalia D&#xed;az-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamas_A/0/1/0/all/0/1\">Alberto Lamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_J/0/1/0/all/0/1\">Jules Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donadello_I/0/1/0/all/0/1\">Ivan Donadello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabik_S/0/1/0/all/0/1\">Siham Tabik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1\">David Filliat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_P/0/1/0/all/0/1\">Policarpo Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montes_R/0/1/0/all/0/1\">Rosana Montes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1\">Francisco Herrera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation-Based Bounding Box Generation for Omnidirectional Pedestrian Detection. (arXiv:2104.13764v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13764","description":"<p>We propose a segmentation-based bounding box generation method for\nomnidirectional pedestrian detection that enables detectors to tightly fit\nbounding boxes to pedestrians without omnidirectional images for training. Due\nto the wide angle of view, omnidirectional cameras are more cost-effective than\nstandard cameras and hence suitable for large-scale monitoring. The problem of\nusing omnidirectional cameras for pedestrian detection is that the performance\nof standard pedestrian detectors is likely to be substantially degraded because\npedestrians' appearance in omnidirectional images may be rotated to any angle.\nExisting methods mitigate this issue by transforming images during inference.\nHowever, the transformation substantially degrades the detection accuracy and\nspeed. A recently proposed method obviates the transformation by training\ndetectors with omnidirectional images, which instead incurs huge annotation\ncosts. To obviate both the transformation and annotation works, we leverage an\nexisting large-scale object detection dataset. We train a detector with rotated\nimages and tightly fitted bounding box annotations generated from the\nsegmentation annotations in the dataset, resulting in detecting pedestrians in\nomnidirectional images with tightly fitted bounding boxes. We also develop\npseudo-fisheye distortion augmentation, which further enhances the performance.\nExtensive analysis shows that our detector successfully fits bounding boxes to\npedestrians and demonstrates substantial performance improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tamura_M/0/1/0/all/0/1\">Masato Tamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1\">Tomoaki Yoshinaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-vocabulary Object Detection via Vision and Language Knowledge Distillation. (arXiv:2104.13921v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13921","description":"<p>We aim at advancing open-vocabulary object detection, which detects objects\ndescribed by arbitrary text inputs. The fundamental challenge is the\navailability of training data. Existing object detection datasets only contain\nhundreds of categories, and it is costly to scale further. To overcome this\nchallenge, we propose ViLD, a training method via Vision and Language knowledge\nDistillation. Our method distills the knowledge from a pretrained\nopen-vocabulary image classification model (teacher) into a two-stage detector\n(student). Specifically, we use the teacher model to encode category texts and\nimage regions of object proposals. Then we train a student detector, whose\nregion embeddings of detected boxes are aligned with the text and image\nembeddings inferred by the teacher. We benchmark on LVIS by holding out all\nrare categories as novel categories not seen during training. ViLD obtains 16.1\nmask AP$_r$, even outperforming the supervised counterpart by 3.8 with a\nResNet-50 backbone. The model can directly transfer to other datasets without\nfinetuning, achieving 72.2 AP$_{50}$, 36.6 AP and 11.8 AP on PASCAL VOC, COCO\nand Objects365, respectively. On COCO, ViLD outperforms previous SOTA by 4.8 on\nnovel AP and 11.4 on overall AP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiuye Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust joint registration of multiple stains and MRI for multimodal 3D histology reconstruction: Application to the Allen human brain atlas. (arXiv:2104.14873v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.14873","description":"<p>Joint registration of a stack of 2D histological sections to recover 3D\nstructure (``3D histology reconstruction'') finds application in areas such as\natlas building and validation of \\emph{in vivo} imaging. Straightforward\npairwise registration of neighbouring sections yields smooth reconstructions\nbut has well-known problems such as ``banana effect'' (straightening of curved\nstructures) and ``z-shift'' (drift). While these problems can be alleviated\nwith an external, linearly aligned reference (e.g., Magnetic Resonance (MR)\nimages), registration is often inaccurate due to contrast differences and the\nstrong nonlinear distortion of the tissue, including artefacts such as folds\nand tears. In this paper, we present a probabilistic model of spatial\ndeformation that yields reconstructions for multiple histological stains that\nthat are jointly smooth, robust to outliers, and follow the reference shape.\nThe model relies on a spanning tree of latent transforms connecting all the\nsections and slices of the reference volume, and assumes that the registration\nbetween any pair of images can be see as a noisy version of the composition of\n(possibly inverted) latent transforms connecting the two images. Bayesian\ninference is used to compute the most likely latent transforms given a set of\npairwise registrations between image pairs within and across modalities. The\nframework is used for accurate 3D reconstruction of two stains (Nissl and\nparvalbumin) from the Allen human brain atlas, showing its benefits on real\ndata with severe distortions. Moreover, we also provide the registration of the\nreconstructed volume to MNI space, bridging the gaps between two of the most\nwidely used atlases in histology and MRI. The 3D reconstructed volumes and\natlas registration can be downloaded from\nhttps://openneuro.org/datasets/ds003590. The code is freely available at\nhttps://github.com/acasamitjana/3dhirest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Casamitjana_A/0/1/0/all/0/1\">Adri&#xe0; Casamitjana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferraris_S/0/1/0/all/0/1\">Sebastiano Ferraris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peter_L/0/1/0/all/0/1\">Loc Peter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Modat_M/0/1/0/all/0/1\">Marc Modat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stevens_A/0/1/0/all/0/1\">Allison Stevens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GridToPix: Training Embodied Agents with Minimal Supervision. (arXiv:2105.00931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00931","description":"<p>While deep reinforcement learning (RL) promises freedom from hand-labeled\ndata, great successes, especially for Embodied AI, require significant work to\ncreate supervision via carefully shaped rewards. Indeed, without shaped\nrewards, i.e., with only terminal rewards, present-day Embodied AI results\ndegrade significantly across Embodied AI problems from single-agent\nHabitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent\nAI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent\nGoogle Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1).\nAs training from shaped rewards doesn't scale to more realistic tasks, the\ncommunity needs to improve the success of training with terminal rewards. For\nthis we propose GridToPix: 1) train agents with terminal rewards in gridworlds\nthat generically mirror Embodied AI environments, i.e., they are independent of\nthe task; 2) distill the learned policy into agents that reside in complex\nvisual worlds. Despite learning from only terminal rewards with identical\nmodels and RL algorithms, GridToPix significantly improves results across\ntasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture\nMoving (success improves from 1% to 25%) to football gameplay (game score\nimproves from 0.1 to 0.6). GridToPix even helps to improve the results of\nshaped reward training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_U/0/1/0/all/0/1\">Unnat Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_I/0/1/0/all/0/1\">Iou-Jen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1\">Svetlana Lazebnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1\">Luca Weihs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Reusable Knowledge for Continual Learning via Metalearning. (arXiv:2106.05390v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05390","description":"<p>When learning tasks over time, artificial neural networks suffer from a\nproblem known as Catastrophic Forgetting (CF). This happens when the weights of\na network are overwritten during the training of a new task causing forgetting\nof old information. To address this issue, we propose MetA Reusable Knowledge\nor MARK, a new method that fosters weight reusability instead of overwriting\nwhen learning a new task. Specifically, MARK keeps a set of shared weights\namong tasks. We envision these shared weights as a common Knowledge Base (KB)\nthat is not only used to learn new tasks, but also enriched with new knowledge\nas the model learns new tasks. Key components behind MARK are two-fold. On the\none hand, a metalearning approach provides the key mechanism to incrementally\nenrich the KB with new knowledge and to foster weight reusability among tasks.\nOn the other hand, a set of trainable masks provides the key mechanism to\nselectively choose from the KB relevant weights to solve each task. By using\nMARK, we achieve state of the art results in several popular benchmarks,\nsurpassing the best performing methods in terms of average accuracy by over 10%\non the 20-Split-MiniImageNet dataset, while achieving almost zero forgetfulness\nusing 55% of the number of parameters. Furthermore, an ablation study provides\nevidence that, indeed, MARK is learning reusable knowledge that is selectively\nused by each task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Julio Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raymond_Saez_A/0/1/0/all/0/1\">Alain Raymond-Saez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.06946","description":"<p>Randomized Smoothing (RS) is a promising method for obtaining robustness\ncertificates by evaluating a base model under noise. In this work, we: (i)\ntheoretically motivate why ensembles are a particularly suitable choice as base\nmodels for RS, and (ii) empirically confirm this choice, obtaining\nstate-of-the-art results in multiple settings. The key insight of our work is\nthat the reduced variance of ensembles over the perturbations introduced in RS\nleads to significantly more consistent classifications for a given input. This,\nin turn, leads to substantially increased certifiable radii for samples close\nto the decision boundary. Additionally, we introduce key optimizations which\nenable an up to 55-fold decrease in sample complexity of RS, thus drastically\nreducing its computational overhead. Experimentally, we show that ensembles of\nonly 3 to 10 classifiers consistently improve on their strongest constituting\nmodel with respect to their average certified radius (ACR) by 5% to 21% on both\nCIFAR10 and ImageNet, achieving a new state-of-the-art ACR of 0.86 and 1.11,\nrespectively. We release all code and models required to reproduce our results\nupon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horvath_M/0/1/0/all/0/1\">Mikl&#xf3;s Z. Horv&#xe1;th</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Mark Niklas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Marc Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2C2 -- An orthogonal method for Semi-Supervised Learning on ambiguous labels. (arXiv:2106.16209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.16209","description":"<p>Semi-Supervised Learning (SSL) can decrease the required amount of labeled\nimage data and thus the cost for deep learning. Most SSL methods assume a clear\ndistinction between classes, but class boundaries are often ambiguous in\nreal-world datasets due to intra- or interobserver variability. This ambiguity\nof annotations must be addressed as it will otherwise limit the performance of\nSSL and deep learning in general due to inconsistent label information. We\npropose Semi-Supervised Classification &amp; Clustering (S2C2) which can extend\nmany deep SSL algorithms. S2C2 automatically estimates the ambiguity of an\nimage and applies the respective SSL algorithm as a classification to certainly\nlabeled data while partitioning the ambiguous data into clusters of visual\nsimilar images. We show that S2C2 results in a 7.6% better F1-score for\nclassifications and 7.9% lower inner distance of clusters on average across\nmultiple SSL algorithms and datasets. Moreover, the output of S2C2 can be used\nto decrease the ambiguity of labels with the help of human experts. Overall, a\ncombination of Semi-Supervised Learning with our method S2C2 leads to better\nhandling of ambiguous labels and thus real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Simon-Martin Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelenka_C/0/1/0/all/0/1\">Claudius Zelenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiko_R/0/1/0/all/0/1\">Rainer Kiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stracke_J/0/1/0/all/0/1\">Jenny Stracke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkmann_N/0/1/0/all/0/1\">Nina Volkmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A study of CNN capacity applied to Left Venticle Segmentation in Cardiac MRI. (arXiv:2107.01318v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.01318","description":"<p>CNN (Convolutional Neural Network) models have been successfully used for\nsegmentation of the left ventricle (LV) in cardiac MRI (Magnetic Resonance\nImaging), providing clinical measurements. In practice, two questions arise\nwith deployment of CNNs: 1) when is it better to use a shallow model instead of\na deeper one? 2) how the size of a dataset might change the network\nperformance? We propose a framework to answer them, by experimenting with deep\nand shallow versions of three U-Net families, trained from scratch in six\nsubsets varying from 100 to 10,000 images, different network sizes, learning\nrates and regularization values. 1620 models were evaluated using 5-fold\ncross-validation by loss and DICE. The results indicate that: sample size\naffects performance more than architecture or hyper-parameters; in small\nsamples the performance is more sensitive to hyper-parameters than\narchitecture; the performance difference between shallow and deeper networks is\nnot the same across families.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Toledo_M/0/1/0/all/0/1\">Marcelo Toledo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lima_D/0/1/0/all/0/1\">Daniel Lima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krieger_J/0/1/0/all/0/1\">Jos&#xe9; Krieger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_M/0/1/0/all/0/1\">Marco Gutierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Face Recognition System for Remote Employee Tracking. (arXiv:2107.07576v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07576","description":"<p>During the COVID-19 pandemic, most of the human-to-human interactions have\nbeen stopped. To mitigate the spread of deadly coronavirus, many offices took\nthe initiative so that the employees can work from home. But, tracking the\nemployees and finding out if they are really performing what they were supposed\nto turn out to be a serious challenge for all the companies and organizations\nwho are facilitating \"Work From Home\". To deal with the challenge effectively,\nwe came up with a solution to track the employees with face recognition. We\nhave been testing this system experimentally for our office. To train the face\nrecognition module, we used FaceNet with KNN using the Labeled Faces in the\nWild (LFW) dataset and achieved 97.8\\% accuracy. We integrated the trained\nmodel into our central system, where the employees log their time. In this\npaper, we discuss in brief the system we have been experimenting with and the\npros and cons of the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdous_R/0/1/0/all/0/1\">Refat E Ferdous</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining the Benefits of Two-stage and One-stage HOI Detection. (arXiv:2108.05077v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05077","description":"<p>Two-stage methods have dominated Human-Object Interaction (HOI) detection for\nseveral years. Recently, one-stage HOI detection methods have become popular.\nIn this paper, we aim to explore the essential pros and cons of two-stage and\none-stage methods. With this as the goal, we find that conventional two-stage\nmethods mainly suffer from positioning positive interactive human-object pairs,\nwhile one-stage methods are challenging to make an appropriate trade-off on\nmulti-task learning, i.e., object detection, and interaction classification.\nTherefore, a core problem is how to take the essence and discard the dregs from\nthe conventional two types of methods. To this end, we propose a novel\none-stage framework with disentangling human-object detection and interaction\nclassification in a cascade manner. In detail, we first design a human-object\npair generator based on a state-of-the-art one-stage HOI detector by removing\nthe interaction classification module or head and then design a relatively\nisolated interaction classifier to classify each human-object pair. Two cascade\ndecoders in our proposed framework can focus on one specific task, detection or\ninteraction classification. In terms of the specific implementation, we adopt a\ntransformer-based HOI detector as our base model. The newly introduced\ndisentangling paradigm outperforms existing methods by a large margin, with a\nsignificant relative mAP gain of 9.32% on HICO-Det. The source codes are\navailable at https://github.com/YueLiao/CDN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aixi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yue Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Miao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting socially interacting groups using f-formation: A survey of taxonomy, methods, datasets, applications, challenges, and future research directions. (arXiv:2108.06181v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.06181","description":"<p>Robots in our daily surroundings are increasing day by day. Their usability\nand acceptability largely depend on their explicit and implicit interaction\ncapability with fellow human beings. As a result, social behavior is one of the\nmost sought-after qualities that a robot can possess. However, there is no\nspecific aspect and/or feature that defines socially acceptable behavior and it\nlargely depends on the situation, application, and society. In this article, we\ninvestigate one such social behavior for collocated robots. Imagine a group of\npeople is interacting with each other and we want to join the group. We as\nhuman beings do it in a socially acceptable manner, i.e., within the group, we\ndo position ourselves in such a way that we can participate in the group\nactivity without disturbing/obstructing anybody. To possess such a quality,\nfirst, a robot needs to determine the formation of the group and then determine\na position for itself, which we humans do implicitly. The theory of f-formation\ncan be utilized for this purpose. As the types of formations can be very\ndiverse, detecting the social groups is not a trivial task. In this article, we\nprovide a comprehensive survey of the existing work on social interaction and\ngroup detection using f-formation for robotics and other applications. We also\nput forward a novel holistic survey framework combining all the possible\nconcerns and modules relevant to this problem. We define taxonomies based on\nmethods, camera views, datasets, detection capabilities and scale, evaluation\napproaches, and application areas. We discuss certain open challenges and\nlimitations in current literature along with possible future research\ndirections based on this framework. In particular, we discuss the existing\nmethods/techniques and their relative merits and demerits, applications, and\nprovide a set of unsolved but relevant problems in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1\">Hrishav Bakul Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mg_T/0/1/0/all/0/1\">Theint Haythi Mg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_P/0/1/0/all/0/1\">Pradip Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_C/0/1/0/all/0/1\">Chayan Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARCH++: Animation-Ready Clothed Human Reconstruction Revisited. (arXiv:2108.07845v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07845","description":"<p>We present ARCH++, an image-based method to reconstruct 3D avatars with\narbitrary clothing styles. Our reconstructed avatars are animation-ready and\nhighly realistic, in both the visible regions from input views and the unseen\nregions. While prior work shows great promise of reconstructing animatable\nclothed humans with various topologies, we observe that there exist fundamental\nlimitations resulting in sub-optimal reconstruction quality. In this paper, we\nrevisit the major steps of image-based avatar reconstruction and address the\nlimitations with ARCH++. First, we introduce an end-to-end point based geometry\nencoder to better describe the semantics of the underlying 3D human body, in\nreplacement of previous hand-crafted features. Second, in order to address the\noccupancy ambiguity caused by topological changes of clothed humans in the\ncanonical pose, we propose a co-supervising framework with cross-space\nconsistency to jointly estimate the occupancy in both the posed and canonical\nspaces. Last, we use image-to-image translation networks to further refine\ndetailed geometry and texture on the reconstructed surface, which improves the\nfidelity and consistency across arbitrary viewpoints. In the experiments, we\ndemonstrate improvements over the state of the art on both public benchmarks\nand user studies in reconstruction quality and realism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Image Coding for Machines: A Content-Adaptive Approach. (arXiv:2108.09992v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.09992","description":"<p>Today, according to the Cisco Annual Internet Report (2018-2023), the\nfastest-growing category of Internet traffic is machine-to-machine\ncommunication. In particular, machine-to-machine communication of images and\nvideos represents a new challenge and opens up new perspectives in the context\nof data compression. One possible solution approach consists of adapting\ncurrent human-targeted image and video coding standards to the use case of\nmachine consumption. Another approach consists of developing completely new\ncompression paradigms and architectures for machine-to-machine communications.\nIn this paper, we focus on image compression and present an inference-time\ncontent-adaptive finetuning scheme that optimizes the latent representation of\nan end-to-end learned image codec, aimed at improving the compression\nefficiency for machine-consumption. The conducted experiments show that our\nonline finetuning brings an average bitrate saving (BD-rate) of -3.66% with\nrespect to our pretrained image codec. In particular, at low bitrate points,\nour proposed method results in a significant bitrate saving of -9.85%. Overall,\nour pretrained-and-then-finetuned system achieves -30.54% BD-rate over the\nstate-of-the-art image/video codec Versatile Video Coding (VVC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1\">Nam Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Honglei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cricri_F/0/1/0/all/0/1\">Francesco Cricri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghaznavi_Youvalari_R/0/1/0/all/0/1\">Ramin Ghaznavi-Youvalari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tavakoli_H/0/1/0/all/0/1\">Hamed Rezazadegan Tavakoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Spatial Relationships by Transformers for Domain Generalization. (arXiv:2108.10046v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10046","description":"<p>Due to the rapid increase in the diversity of image data, the problem of\ndomain generalization has received increased attention recently. While domain\ngeneralization is a challenging problem, it has achieved great development\nthanks to the fast development of AI techniques in computer vision. Most of\nthese advanced algorithms are proposed with deep architectures based on\nconvolution neural nets (CNN). However, though CNNs have a strong ability to\nfind the discriminative features, they do a poor job of modeling the relations\nbetween different locations in the image due to the response to CNN filters are\nmostly local. Since these local and global spatial relationships are\ncharacterized to distinguish an object under consideration, they play a\ncritical role in improving the generalization ability against the domain gap.\nIn order to get the object parts relationships to gain better domain\ngeneralization, this work proposes to use the self attention model. However,\nthe attention models are proposed for sequence, which are not expert in\ndiscriminate feature extraction for 2D images. Considering this, we proposed a\nhybrid architecture to discover the spatial relationships between these local\nfeatures, and derive a composite representation that encodes both the\ndiscriminative features and their relationships to improve the domain\ngeneralization. Evaluation on three well-known benchmarks demonstrates the\nbenefits of modeling relationships between the features of an image using the\nproposed method and achieves state-of-the-art domain generalization\nperformance. More specifically, the proposed algorithm outperforms the\nstate-of-the-art by 2.2% and 3.4% on PACS and Office-Home databases,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1\">Cuicui Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1\">Karthik Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01745","description":"<p>The COVID-19 pandemic raises the problem of adapting face recognition systems\nto the new reality, where people may wear surgical masks to cover their noses\nand mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for\ntraining these systems were released before the pandemic, so they now seem\nunsuited due to the lack of examples of people wearing masks. We propose a\nmethod for enhancing data sets containing faces without masks by creating\nsynthetic masks and overlaying them on faces in the original images. Our method\nrelies on SparkAR Studio, a developer program made by Facebook that is used to\ncreate Instagram face filters. In our approach, we use 9 masks of different\ncolors, shapes and fabrics. We employ our method to generate a number of\n445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254\n(96.8%) masks for the CelebA data set, releasing the mask images at\nhttps://github.com/securifai/masked_faces. We show that our method produces\nsignificantly more realistic training examples of masks overlaid on faces by\nasking volunteers to qualitatively compare it to other methods or data sets\ndesigned for the same task. We also demonstrate the usefulness of our method by\nevaluating state-of-the-art face recognition systems (FaceNet, VGG-face,\nArcFace) trained on our enhanced data sets and showing that they outperform\nequivalent systems trained on original data sets (containing faces without\nmasks) or competing data sets (containing masks generated by related methods),\nwhen the test benchmarks contain masked faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mare_T/0/1/0/all/0/1\">Tudor Mare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duta_G/0/1/0/all/0/1\">Georgian Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1\">Bogdan Alexe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond. (arXiv:2109.02752v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.02752","description":"<p>Training deep neural networks may be challenging in real world data. Using\nmodels as black-boxes, even with transfer learning, can result in poor\ngeneralization or inconclusive results when it comes to small datasets or\nspecific applications. This tutorial covers the basic steps as well as more\nrecent options to improve models, in particular, but not restricted to,\nsupervised learning. It can be particularly useful in datasets that are not as\nwell-prepared as those in challenges, and also under scarce annotation and/or\nsmall data. We describe basic procedures: as data preparation, optimization and\ntransfer learning, but also recent architectural choices such as use of\ntransformer modules, alternative convolutional layers, activation functions,\nwide and deep networks, as well as training procedures including as curriculum,\ncontrastive and self-supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1\">Fernando Pereira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leo Sampaio Ferraz Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallari_G/0/1/0/all/0/1\">Gabriel Biscaro Cavallari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering. (arXiv:2109.08029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08029","description":"<p>Integrating outside knowledge for reasoning in visio-linguistic tasks such as\nvisual question answering (VQA) is an open problem. Given that pretrained\nlanguage models have been shown to include world knowledge, we propose to use a\nunimodal (text-only) train and inference procedure based on automatic\noff-the-shelf captioning of images and pretrained language models. Our results\non a visual question answering task which requires external knowledge (OK-VQA)\nshow that our text-only model outperforms pretrained multimodal (image-text)\nmodels of comparable number of parameters. In contrast, our model is less\neffective in a standard VQA task (VQA 2.0) confirming that our text-only method\nis specially effective for tasks requiring external knowledge. In addition, we\nshow that our unimodal model is complementary to multimodal models in both\nOK-VQA and VQA 2.0, and yield the best result to date in OK-VQA among systems\nnot using external knowledge graphs, and comparable to systems that do use\nthem. Our qualitative analysis on OK-VQA reveals that automatic captions often\nfail to capture relevant information in the images, which seems to be balanced\nby the better inference ability of the text-only language models. Our work\nopens up possibilities to further improve inference in visio-linguistic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salaberria_A/0/1/0/all/0/1\">Ander Salaberria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azkune_G/0/1/0/all/0/1\">Gorka Azkune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Hybrid Transformer: Learning Global-local Context for Urban Scene Segmentation. (arXiv:2109.08937v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08937","description":"<p>Semantic segmentation of fine-resolution urban scene images plays a vital\nrole in extensive practical applications, such as land cover mapping, urban\nchange detection, environmental protection and economic assessment. Driven by\nrapid developments in deep learning technologies, the convolutional neural\nnetwork (CNN) has dominated the semantic segmentation task for many years.\nConvolutional neural networks adopt hierarchical feature representation,\ndemonstrating strong local information extraction. However, the local property\nof the convolution layer limits the network from capturing global context that\nis crucial for precise segmentation. Recently, Transformer comprise a hot topic\nin the computer vision domain. Transformer demonstrates the great capability of\nglobal information modelling, boosting many vision tasks, such as image\nclassification, object detection and especially semantic segmentation. In this\npaper, we propose an efficient hybrid Transformer (EHT) for real-time urban\nscene segmentation. The EHT adopts a hybrid structure with and CNN-based\nencoder and a transformer-based decoder, learning global-local context with\nlower computation. Extensive experiments demonstrate that our EHT has faster\ninference speed with competitive accuracy compared with state-of-the-art\nlightweight models. Specifically, the proposed EHT achieves a 66.9% mIoU on the\nUAVid test set and outperforms other benchmark networks significantly. The code\nwill be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shenghui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student Helping Teacher: Teacher Evolution via Self-Knowledge Distillation. (arXiv:2110.00329v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00329","description":"<p>Knowledge distillation usually transfers the knowledge from a pre-trained\ncumbersome teacher network to a compact student network, which follows the\nclassical teacher-teaching-student paradigm. Based on this paradigm, previous\nmethods mostly focus on how to efficiently train a better student network for\ndeployment. Different from the existing practices, in this paper, we propose a\nnovel student-helping-teacher formula, Teacher Evolution via Self-Knowledge\nDistillation (TESKD), where the target teacher (for deployment) is learned with\nthe help of multiple hierarchical students by sharing the structural backbone.\nThe diverse feedback from multiple students allows the teacher to improve\nitself through the shared feature representations. The effectiveness of our\nproposed framework is demonstrated by extensive experiments with various\nnetwork settings on two standard benchmarks including CIFAR-100 and ImageNet.\nNotably, when trained together with our proposed method, ResNet-18 achieves\n79.15% and 71.14% accuracy on CIFAR-100 and ImageNet, outperforming the\nbaseline results by 4.74% and 1.43%, respectively. The code is available at:\nhttps://github.com/zhengli427/TESKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhigeng Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03374","description":"<p>Unsupervised domain adaptation aims to align a labeled source domain and an\nunlabeled target domain, but it requires to access the source data which often\nraises concerns in data privacy, data portability and data transmission\nefficiency. We study unsupervised model adaptation (UMA), or called\nUnsupervised Domain Adaptation without Source Data, an alternative setting that\naims to adapt source-trained models towards target distributions without\naccessing source data. To this end, we design an innovative historical\ncontrastive learning (HCL) technique that exploits historical source hypothesis\nto make up for the absence of source data in UMA. HCL addresses the UMA\nchallenge from two perspectives. First, it introduces historical contrastive\ninstance discrimination (HCID) that learns from target samples by contrasting\ntheir embeddings which are generated by the currently adapted model and the\nhistorical models. With the source-trained and earlier-epoch models as the\nhistorical models, HCID encourages UMA to learn instance-discriminative target\nrepresentations while preserving the source hypothesis. Second, it introduces\nhistorical contrastive category discrimination (HCCD) that pseudo-labels target\nsamples to learn category-discriminative target representations. Instead of\nglobally thresholding pseudo labels, HCCD re-weights pseudo labels according to\ntheir prediction consistency across the current and historical models.\nExtensive experiments show that HCL outperforms and complements\nstate-of-the-art methods consistently across a variety of visual tasks (e.g.,\nsegmentation, classification and detection) and setups (e.g., close-set,\nopen-set and partial adaptation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks. (arXiv:2110.03861v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2110.03861","description":"<p>The advent of noisy intermediate-scale quantum (NISQ) computers raises a\ncrucial challenge to design quantum neural networks for fully quantum learning\ntasks. To bridge the gap, this work proposes an end-to-end learning framework\nnamed QTN-VQC, by introducing a trainable quantum tensor network (QTN) for\nquantum embedding on a variational quantum circuit (VQC). The architecture of\nQTN is composed of a parametric tensor-train network for feature extraction and\na tensor product encoding for quantum encoding. We highlight the QTN for\nquantum embedding in terms of two perspectives: (1) we theoretically\ncharacterize QTN by analyzing its representation power of input features; (2)\nQTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the\ngeneration of quantum embedding to the output measurement. Our experiments on\nthe MNIST dataset demonstrate the advantages of QTN for quantum embedding over\nother quantum embedding approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Qi_J/0/1/0/all/0/1\">Jun Qi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP4Caption ++: Multi-CLIP for Video Caption. (arXiv:2110.05204v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05204","description":"<p>This report describes our solution to the VALUE Challenge 2021 in the\ncaptioning task. Our solution, named CLIP4Caption++, is built on\nX-Linear/X-Transformer, which is an advanced model with encoder-decoder\narchitecture. We make the following improvements on the proposed\nCLIP4Caption++: We employ an advanced encoder-decoder model architecture\nX-Transformer as our main framework and make the following improvements: 1) we\nutilize three strong pre-trained CLIP models to extract the text-related\nappearance visual features. 2) we adopt the TSN sampling strategy for data\nenhancement. 3) we involve the video subtitle information to provide richer\nsemantic information. 3) we introduce the subtitle information, which fuses\nwith the visual features as guidance. 4) we design word-level and\nsentence-level ensemble strategies. Our proposed method achieves 86.5, 148.4,\n64.5 CIDEr scores on VATEX, YC2C, and TVC datasets, respectively, which shows\nthe superior performance of our proposed CLIP4Caption++ on all three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingkang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhaoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_F/0/1/0/all/0/1\">Fengyun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Autoregressive Image Captioning. (arXiv:2110.05342v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05342","description":"<p>Current state-of-the-art approaches for image captioning typically adopt an\nautoregressive manner, i.e., generating descriptions word by word, which\nsuffers from slow decoding issue and becomes a bottleneck in real-time\napplications. Non-autoregressive image captioning with continuous iterative\nrefinement, which eliminates the sequential dependence in a sentence\ngeneration, can achieve comparable performance to the autoregressive\ncounterparts with a considerable acceleration. Nevertheless, based on a\nwell-designed experiment, we empirically proved that iteration times can be\neffectively reduced when providing sufficient prior knowledge for the language\ndecoder. Towards that end, we propose a novel two-stage framework, referred to\nas Semi-Autoregressive Image Captioning (SAIC), to make a better trade-off\nbetween performance and speed. The proposed SAIC model maintains autoregressive\nproperty in global but relieves it in local. Specifically, SAIC model first\njumpily generates an intermittent sequence in an autoregressive manner, that\nis, it predicts the first word in every word group in order. Then, with the\nhelp of the partially deterministic prior information and image features, SAIC\nmodel non-autoregressively fills all the skipped words with one iteration.\nExperimental results on the MS COCO benchmark demonstrate that our SAIC model\noutperforms the preceding non-autoregressive image captioning models while\nobtaining a competitive inference speedup. Code is available at\nhttps://github.com/feizc/SAIC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhengcong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rescoring Sequence-to-Sequence Models for Text Line Recognition with CTC-Prefixes. (arXiv:2110.05909v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05909","description":"<p>In contrast to Connectionist Temporal Classification (CTC) approaches,\nSequence-To-Sequence (S2S) models for Handwritten Text Recognition (HTR) suffer\nfrom errors such as skipped or repeated words which often occur at the end of a\nsequence. In this paper, to combine the best of both approaches, we propose to\nuse the CTC-Prefix-Score during S2S decoding. Hereby, during beam search, paths\nthat are invalid according to the CTC confidence matrix are penalised. Our\nnetwork architecture is composed of a Convolutional Neural Network (CNN) as\nvisual backbone, bidirectional Long-Short-Term-Memory-Cells (LSTMs) as encoder,\nand a decoder which is a Transformer with inserted mutual attention layers. The\nCTC confidences are computed on the encoder while the Transformer is only used\nfor character-wise S2S decoding. We evaluate this setup on three HTR data sets:\nIAM, Rimes, and StAZH. On IAM, we achieve a competitive Character Error Rate\n(CER) of 2.95% when pretraining our model on synthetic data and including a\ncharacter-based language model for contemporary English. Compared to other\nstate-of-the-art approaches, our model requires about 10-20 times less\nparameters. Access our shared implementations via this link to GitHub:\nhttps://github.com/Planet-AI-GmbH/tfaip-hybrid-ctc-s2s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wick_C/0/1/0/all/0/1\">Christoph Wick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">Jochen Z&#xf6;llner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruning_T/0/1/0/all/0/1\">Tobias Gr&#xfc;ning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}