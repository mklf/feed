{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v1 [cs.SE])","link":"http://arxiv.org/abs/2204.05999","description":"<p>Code is seldom written in a single left-to-right pass and is instead\nrepeatedly edited and refined. We introduce InCoder, a unified generative model\nthat can perform program synthesis (via left-to-right generation) as well as\nediting (via infilling). InCoder is trained to generate code files from a large\ncorpus of permissively licensed code, where regions of code have been randomly\nmasked and moved to the end of each file, allowing code infilling with\nbidirectional context. Our model is the first generative model that is able to\ndirectly perform zero-shot code infilling, which we evaluate on challenging\ntasks such as type inference, comment generation, and variable re-naming. We\nfind that the ability to condition on bidirectional context substantially\nimproves performance on these tasks, while still performing comparably on\nstandard program synthesis benchmarks in comparison to left-to-right only\nmodels pretrained at similar scale. The InCoder models and code are publicly\nreleased. https://sites.google.com/view/incoder-code-models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Freda Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022. (arXiv:2204.06028v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06028","description":"<p>In this paper, we describe our submission to the Simultaneous Speech\nTranslation at IWSLT 2022. We explore strategies to utilize an offline model in\na simultaneous setting without the need to modify the original model. In our\nexperiments, we show that our onlinization algorithm is almost on par with the\noffline setting while being 3x faster than offline in terms of latency on the\ntest set. We make our system publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1\">Peter Pol&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngoc_N/0/1/0/all/0/1\">Ngoc-Quan Ngoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Nam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullov_C/0/1/0/all/0/1\">Carlos Mullov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT models. (arXiv:2204.06029v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06029","description":"<p>Named Entity Recognition (NER) is a basic NLP task and finds major\napplications in conversational and search systems. It helps us identify key\nentities in a sentence used for the downstream application. NER or similar slot\nfilling systems for popular languages have been heavily used in commercial\napplications. In this work, we focus on Marathi, an Indian language, spoken\nprominently by the people of Maharashtra state. Marathi is a low resource\nlanguage and still lacks useful NER resources. We present L3Cube-MahaNER, the\nfirst major gold standard named entity recognition dataset in Marathi. We also\ndescribe the manual annotation guidelines followed during the process. In the\nend, we benchmark the dataset on different CNN, LSTM, and Transformer based\nmodels like mBERT, XLM-RoBERTa, IndicBERT, MahaBERT, etc. The MahaBERT provides\nthe best performance among all the models. The data and models are available at\nhttps://github.com/l3cube-pune/MarathiNLP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_P/0/1/0/all/0/1\">Parth Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranade_A/0/1/0/all/0/1\">Aparna Ranade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabane_M/0/1/0/all/0/1\">Maithili Sabane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1\">Onkar Litake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review on Language Models as Knowledge Bases. (arXiv:2204.06031v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06031","description":"<p>Recently, there has been a surge of interest in the NLP community on the use\nof pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have\nshown that LMs trained on a sufficiently large (web) corpus will encode a\nsignificant amount of knowledge implicitly in its parameters. The resulting LM\ncan be probed for different kinds of knowledge and thus acting as a KB. This\nhas a major advantage over traditional KBs in that this method requires no\nhuman supervision. In this paper, we present a set of aspects that we deem a LM\nshould have to fully act as a KB, and review the recent literature with respect\nto those aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AlKhamissi_B/0/1/0/all/0/1\">Badr AlKhamissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Millicent Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Trolls Under Bridges: Preliminary Work on a Motif Detector. (arXiv:2204.06085v1 [cs.AI])","link":"http://arxiv.org/abs/2204.06085","description":"<p>Motifs are distinctive recurring elements found in folklore that have\nsignificance as communicative devices in news, literature, press releases, and\npropaganda. Motifs concisely imply a large constellation of culturally-relevant\ninformation, and their broad usage suggests their cognitive importance as\ntouchstones of cultural knowledge, making their detection a worthy step toward\nculturally-aware natural language processing tasks. Until now, folklorists and\nothers interested in motifs have only extracted motifs from narratives\nmanually. We present a preliminary report on the development of a system for\nautomatically detecting motifs. We briefly describe an annotation effort to\nproduce data for training motif detection, which is on-going. We describe our\nin-progress architecture in detail, which aims to capture, in part, how people\ndetermine whether or not a motif candidate is being used in a motific way. This\ndescription includes a test of an off-the-shelf metaphor detector as a feature\nfor motif detection, which achieves a F1 of 0.35 on motifs and a macro-average\nF1 of 0.21 across four categories which we assign to motif candidates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yarlott_W/0/1/0/all/0/1\">W. Victor H. Yarlott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_A/0/1/0/all/0/1\">Armando Ochoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Anurag Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobrow_L/0/1/0/all/0/1\">Laurel Bobrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estrada_D/0/1/0/all/0/1\">Diego Castro Estrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_D/0/1/0/all/0/1\">Diana Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Joan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_D/0/1/0/all/0/1\">David McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Chris Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Mark A. Finlayson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASQA: Factoid Questions Meet Long-Form Answers. (arXiv:2204.06092v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06092","description":"<p>An abundance of datasets and availability of reliable evaluation metrics have\nresulted in strong progress in factoid question answering (QA). This progress,\nhowever, does not easily transfer to the task of long-form QA, where the goal\nis to answer questions that require in-depth explanations. The hurdles include\n(i) a lack of high-quality data, and (ii) the absence of a well-defined notion\nof the answer's quality. In this work, we address these problems by (i)\nreleasing a novel dataset and a task that we call ASQA (Answer Summaries for\nQuestions which are Ambiguous); and (ii) proposing a reliable metric for\nmeasuring performance on ASQA. Our task focuses on factoid questions that are\nambiguous, that is, have different correct answers depending on interpretation.\nAnswers to ambiguous questions should synthesize factual information from\nmultiple sources into a long-form summary that resolves the ambiguity. In\ncontrast to existing long-form QA tasks (such as ELI5), ASQA admits a clear\nnotion of correctness: a user faced with a good summary should be able to\nanswer different interpretations of the original ambiguous question. We use\nthis notion of correctness to define an automated metric of performance for\nASQA. Our analysis demonstrates an agreement between this metric and human\njudgments, and reveals a considerable gap between human performance and strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stelmakh_I/0/1/0/all/0/1\">Ivan Stelmakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impossible Triangle: What's Next for Pre-trained Language Models?. (arXiv:2204.06130v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06130","description":"<p>Recent development of large-scale pre-trained language models (PLM) have\nsignificantly improved the capability of models in various NLP tasks, in terms\nof performance after task-specific fine-tuning and zero-shot / few-shot\nlearning. However, many of such models come with a dauntingly huge size that\nfew institutions can afford to pre-train, fine-tune or even deploy, while\nmoderate-sized models usually lack strong generalized few-shot learning\ncapabilities. In this paper, we first elaborate the current obstacles of using\nPLM models in terms of the Impossible Triangle: 1) moderate model size, 2)\nstate-of-the-art few-shot learning capability, and 3) state-of-the-art\nfine-tuning capability. We argue that all existing PLM models lack one or more\nproperties from the Impossible Triangle. To remedy these missing properties of\nPLMs, various techniques have been proposed, such as knowledge distillation,\ndata augmentation and prompt learning, which inevitably brings additional work\nto the application of PLMs in real scenarios. We then offer insights into\nfuture research directions of PLMs to achieve the Impossible Triangle, and\nbreak down the task into several key phases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIT at SemEval-2022 Task 2: Pre-trained Language Model for Idioms Detection. (arXiv:2204.06145v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06145","description":"<p>The same multi-word expressions may have different meanings in different\nsentences. They can be mainly divided into two categories, which are literal\nmeaning and idiomatic meaning. Non-contextual-based methods perform poorly on\nthis problem, and we need contextual embedding to understand the idiomatic\nmeaning of multi-word expressions correctly. We use a pre-trained language\nmodel, which can provide a context-aware sentence embedding, to detect whether\nmulti-word expression in the sentence is idiomatic usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Cluster-Based k-Nearest-Neighbor Machine Translation. (arXiv:2204.06175v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06175","description":"<p>k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as\na non-parametric solution for domain adaptation in neural machine translation\n(NMT). It aims to alleviate the performance degradation of advanced MT systems\nin translating out-of-domain sentences by coordinating with an additional\ntoken-level feature-based retrieval module constructed from in-domain data.\nPrevious studies have already demonstrated that non-parametric NMT is even\nsuperior to models fine-tuned on out-of-domain data. In spite of this success,\nkNN retrieval is at the expense of high latency, in particular for large\ndatastores. To make it practical, in this paper, we explore a more efficient\nkNN-MT and propose to use clustering to improve the retrieval efficiency.\nConcretely, we first propose a cluster-based Compact Network for feature\nreduction in a contrastive learning manner to compress context features into\n90+% lower dimensional vectors. We then suggest a cluster-based pruning\nsolution to filter out 10%-40% redundant nodes in large datastores while\nretaining translation quality. Our proposed methods achieve better or\ncomparable performance while reducing up to 57% inference latency against the\nadvanced non-parametric MT model on several machine translation benchmarks.\nExperimental results indicate that the proposed methods maintain the most\nuseful information of the original datastore and the Compact Network shows good\ngeneralization on unseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Universality-Individuality Integration Model for Dialog Act Classification. (arXiv:2204.06185v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06185","description":"<p>Dialog Act (DA) reveals the general intent of the speaker utterance in a\nconversation. Accurately predicting DAs can greatly facilitate the development\nof dialog agents. Although researchers have done extensive research on dialog\nact classification, the feature information of classification has not been\nfully considered. This paper suggests that word cues, part-of-speech cues and\nstatistical cues can complement each other to improve the basis for\nrecognition. In addition, the different types of the three lead to the\ndiversity of their distribution forms, which hinders the mining of feature\ninformation. To solve this problem, we propose a novel model based on\nuniversality and individuality strategies, called Universality-Individuality\nIntegration Model (UIIM). UIIM not only deepens the connection between the\nclues by learning universality, but also utilizes the learning of individuality\nto capture the characteristics of the clues themselves. Experiments were made\nover two most popular benchmark data sets SwDA and MRDA for dialogue act\nclassification, and the results show that extracting the universalities and\nindividualities between cues can more fully excavate the hidden information in\nthe utterance, and improve the accuracy of automatic dialogue act recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pengfei_G/0/1/0/all/0/1\">Gao Pengfei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yinglong_M/0/1/0/all/0/1\">Ma Yinglong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing for Constituency Structure in Neural Language Models. (arXiv:2204.06201v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06201","description":"<p>In this paper, we investigate to which extent contextual neural language\nmodels (LMs) implicitly learn syntactic structure. More concretely, we focus on\nconstituent structure as represented in the Penn Treebank (PTB). Using standard\nprobing techniques based on diagnostic classifiers, we assess the accuracy of\nrepresenting constituents of different categories within the neuron activations\nof a LM such as RoBERTa. In order to make sure that our probe focuses on\nsyntactic knowledge and not on implicit semantic generalizations, we also\nexperiment on a PTB version that is obtained by randomly replacing constituents\nwith each other while keeping syntactic structure, i.e., a semantically\nill-formed but syntactically well-formed version of the PTB. We find that 4\npretrained transfomer LMs obtain high performance on our probing tasks even on\nmanipulated data, suggesting that semantic and syntactic knowledge in their\nrepresentations can be separated and that constituency information is in fact\nlearned by the LM. Moreover, we show that a complete constituency tree can be\nlinearly separated from LM representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arps_D/0/1/0/all/0/1\">David Arps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samih_Y/0/1/0/all/0/1\">Younes Samih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallmeyer_L/0/1/0/all/0/1\">Laura Kallmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Question Rewriting Help Conversational Question Answering?. (arXiv:2204.06239v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06239","description":"<p>Question rewriting (QR) is a subtask of conversational question answering\n(CQA) aiming to ease the challenges of understanding dependencies among\ndialogue history by reformulating questions in a self-contained form. Despite\nseeming plausible, little evidence is available to justify QR as a mitigation\nmethod for CQA. To verify the effectiveness of QR in CQA, we investigate a\nreinforcement learning approach that integrates QR and CQA tasks and does not\nrequire corresponding QR datasets for targeted CQA. We find, however, that the\nRL method is on par with the end-to-end baseline. We provide an analysis of the\nfailure and describe the difficulty of exploiting QR for CQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experimental Standards for Deep Learning Research: A Natural Language Processing Perspective. (arXiv:2204.06251v1 [cs.LG])","link":"http://arxiv.org/abs/2204.06251","description":"<p>The field of Deep Learning (DL) has undergone explosive growth during the\nlast decade, with a substantial impact on Natural Language Processing (NLP) as\nwell. Yet, as with other fields employing DL techniques, there has been a lack\nof common experimental standards compared to more established disciplines.\nStarting from fundamental scientific principles, we distill ongoing discussions\non experimental standards in DL into a single, widely-applicable methodology.\nFollowing these best practices is crucial to strengthening experimental\nevidence, improve reproducibility and enable scientific progress. These\nstandards are further collected in a public repository to help them\ntransparently adapt to future needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1\">Dennis Ulmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassignana_E/0/1/0/all/0/1\">Elisa Bassignana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1\">Max M&#xfc;ller-Eberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varab_D/0/1/0/all/0/1\">Daniel Varab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardmeier_C/0/1/0/all/0/1\">Christian Hardmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Matters in Language Conditioned Robotic Imitation Learning. (arXiv:2204.06252v1 [cs.RO])","link":"http://arxiv.org/abs/2204.06252","description":"<p>A long-standing goal in robotics is to build robots that can perform a wide\nrange of daily tasks from perceptions obtained with their onboard sensors and\nspecified only via natural language. While recently substantial advances have\nbeen achieved in language-driven robotics by leveraging end-to-end learning\nfrom pixels, there is no clear and well-understood process for making various\ndesign choices due to the underlying variation in setups. In this paper, we\nconduct an extensive study of the most critical challenges in learning language\nconditioned policies from offline free-form imitation datasets. We further\nidentify architectural and algorithmic techniques that improve performance,\nsuch as a hierarchical decomposition of the robot control learning, a\nmultimodal transformer encoder, discrete latent plans and a self-supervised\ncontrastive loss that aligns video and language representations. By combining\nthe results of our investigation with our improved model components, we are\nable to present a novel approach that significantly outperforms the state of\nthe art on the challenging language conditioned long-horizon robot manipulation\nCALVIN benchmark. We have open-sourced our implementation to facilitate future\nresearch in learning to perform many complex manipulation skills in a row\nspecified with natural language. Codebase and trained models available at\n<a href=\"http://hulc.cs.uni-freiburg.de\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_L/0/1/0/all/0/1\">Lukas Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-critical Sequence Training for Automatic Speech Recognition. (arXiv:2204.06260v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06260","description":"<p>Although automatic speech recognition (ASR) task has gained remarkable\nsuccess by sequence-to-sequence models, there are two main mismatches between\nits training and testing that might lead to performance degradation: 1) The\ntypically used cross-entropy criterion aims to maximize log-likelihood of the\ntraining data, while the performance is evaluated by word error rate (WER), not\nlog-likelihood; 2) The teacher-forcing method leads to the dependence on ground\ntruth during training, which means that model has never been exposed to its own\nprediction before testing. In this paper, we propose an optimization method\ncalled self-critical sequence training (SCST) to make the training procedure\nmuch closer to the testing phase. As a reinforcement learning (RL) based\nmethod, SCST utilizes a customized reward function to associate the training\ncriterion and WER. Furthermore, it removes the reliance on teacher-forcing and\nharmonizes the model with respect to its inference procedure. We conducted\nexperiments on both clean and noisy speech datasets, and the results show that\nthe proposed SCST respectively achieves 8.7% and 7.8% relative improvements\nover the baseline in terms of WER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_N/0/1/0/all/0/1\">Nana Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaofeng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Heqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TangoBERT: Reducing Inference Cost by using Cascaded Architecture. (arXiv:2204.06271v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06271","description":"<p>The remarkable success of large transformer-based models such as BERT,\nRoBERTa and XLNet in many NLP tasks comes with a large increase in monetary and\nenvironmental cost due to their high computational load and energy consumption.\nIn order to reduce this computational load in inference time, we present\nTangoBERT, a cascaded model architecture in which instances are first processed\nby an efficient but less accurate first tier model, and only part of those\ninstances are additionally processed by a less efficient but more accurate\nsecond tier model. The decision of whether to apply the second tier model is\nbased on a confidence score produced by the first tier model. Our simple method\nhas several appealing practical advantages compared to standard cascading\napproaches based on multi-layered transformer models. First, it enables higher\nspeedup gains (average lower latency). Second, it takes advantage of batch size\noptimization for cascading, which increases the relative inference cost\nreductions. We report TangoBERT inference CPU speedup on four text\nclassification GLUE tasks and on one reading comprehension task. Experimental\nresults show that TangoBERT outperforms efficient early exit baseline models;\non the the SST-2 task, it achieves an accuracy of 93.9% with a CPU speedup of\n8.2x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mamou_J/0/1/0/all/0/1\">Jonathan Mamou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereg_O/0/1/0/all/0/1\">Oren Pereg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1\">Moshe Wasserblat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding. (arXiv:2204.06283v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06283","description":"<p>In the age of large transformer language models, linguistic evaluation play\nan important role in diagnosing models' abilities and limitations on natural\nlanguage understanding. However, current evaluation methods show some\nsignificant shortcomings. In particular, they do not provide insight into how\nwell a language model captures distinct linguistic skills essential for\nlanguage understanding and reasoning. Thus they fail to effectively map out the\naspects of language understanding that remain challenging to existing models,\nwhich makes it hard to discover potential limitations in models and datasets.\nIn this paper, we introduce Curriculum as a new format of NLI benchmark for\nevaluation of broad-coverage linguistic phenomena. Curriculum contains a\ncollection of datasets that covers 36 types of major linguistic phenomena and\nan evaluation procedure for diagnosing how well a language model captures\nreasoning skills for distinct types of linguistic phenomena. We show that this\nlinguistic-phenomena-driven benchmark can serve as an effective tool for\ndiagnosing model behavior and verifying model learning quality. In addition,\nOur experiments provide insight into the limitation of existing benchmark\ndatasets and state-of-the-art models that may encourage future research on\nre-designing datasets, model architectures, and learning objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiyue Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIB-VA at SemEval-2022 Task 5: A Multimodal Architecture for the Detection and Classification of Misogynous Memes. (arXiv:2204.06299v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06299","description":"<p>The detection of offensive, hateful content on social media is a challenging\nproblem that affects many online users on a daily basis. Hateful content is\noften used to target a group of people based on ethnicity, gender, religion and\nother factors. The hate or contempt toward women has been increasing on social\nplatforms. Misogynous content detection is especially challenging when textual\nand visual modalities are combined to form a single context, e.g., an overlay\ntext embedded on top of an image, also known as meme. In this paper, we present\na multimodal architecture that combines textual and visual features in order to\ndetect misogynous meme content. The proposed architecture is evaluated in the\nSemEval-2022 Task 5: MAMI - Multimedia Automatic Misogyny Identification\nchallenge under the team name TIB-VA. Our solution obtained the best result in\nthe Task-B where the challenge is to classify whether a given document is\nmisogynous and further identify the main sub-classes of shaming, stereotype,\nobjectification, and violence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1\">Gullal S. Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification. (arXiv:2204.06305v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06305","description":"<p>Prompt-based learning (i.e., prompting) is an emerging paradigm for\nexploiting knowledge learned by a pretrained language model. In this paper, we\npropose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method\nto automatically select label mappings for few-shot text classification with\nprompting. Our method exploits one-to-many label mappings and a\nstatistics-based algorithm to select label mappings given a prompt template.\nOur experiments demonstrate that AMuLaP achieves competitive performance on the\nGLUE benchmark without human effort or external resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Call-sign recognition and understanding for noisy air-traffic transcripts using surveillance information. (arXiv:2204.06309v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06309","description":"<p>Air traffic control (ATC) relies on communication via speech between pilot\nand air-traffic controller (ATCO). The call-sign, as unique identifier for each\nflight, is used to address a specific pilot by the ATCO. Extracting the\ncall-sign from the communication is a challenge because of the noisy ATC voice\nchannel and the additional noise introduced by the receiver. A low\nsignal-to-noise ratio (SNR) in the speech leads to high word error rate (WER)\ntranscripts. We propose a new call-sign recognition and understanding (CRU)\nsystem that addresses this issue. The recognizer is trained to identify\ncall-signs in noisy ATC transcripts and convert them into the standard\nInternational Civil Aviation Organization (ICAO) format. By incorporating\nsurveillance information, we can multiply the call-sign accuracy (CSA) up to a\nfactor of four. The introduced data augmentation adds additional performance on\nhigh WER transcripts and allows the adaptation of the model to unseen\nairspaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blatt_A/0/1/0/all/0/1\">Alexander Blatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocour_M/0/1/0/all/0/1\">Martin Kocour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vesely_K/0/1/0/all/0/1\">Karel Vesel&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szoke_I/0/1/0/all/0/1\">Igor Sz&#xf6;ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Production federated keyword spotting via distillation, filtering, and joint federated-centralized training. (arXiv:2204.06322v1 [eess.AS])","link":"http://arxiv.org/abs/2204.06322","description":"<p>We trained a keyword spotting model using federated learning on real user\ndevices and observed significant improvements when the model was deployed for\ninference on phones. To compensate for data domains that are missing from\non-device training caches, we employed joint federated-centralized training.\nAnd to learn in the absence of curated labels on-device, we formulated a\nconfidence filtering strategy based on user-feedback signals for federated\ndistillation. These techniques created models that significantly improved\nquality metrics in offline evaluations and user-experience metrics in live A/B\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hard_A/0/1/0/all/0/1\">Andrew Hard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Partridge_K/0/1/0/all/0/1\">Kurt Partridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_N/0/1/0/all/0/1\">Neng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Augenstein_S/0/1/0/all/0/1\">Sean Augenstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1\">Aishanee Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_H/0/1/0/all/0/1\">Hyun Jin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_A/0/1/0/all/0/1\">Alex Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_S/0/1/0/all/0/1\">Sara Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_J/0/1/0/all/0/1\">Jessica Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition. (arXiv:2204.06328v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06328","description":"<p>Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT)\nand wav2vec 2.0, has brought significant improvements in automatic speech\nrecognition (ASR). However, these models usually require an expensive\ncomputational cost to achieve outstanding performance, slowing down the\ninference speed. To improve the model efficiency, we propose an early exit\nscheme for ASR, namely HuBERT-EE, that allows the model to stop the inference\ndynamically. In HuBERT-EE, multiple early exit branches are added at the\nintermediate layers, and each branch is used to decide whether a prediction can\nbe exited early. Experimental results on the LibriSpeech dataset show that\nHuBERT-EE can accelerate the inference of a large-scale HuBERT model while\nsimultaneously balancing the trade-off between the word error rate (WER)\nperformance and the latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Ji Won Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_B/0/1/0/all/0/1\">Beom Jun Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nam Soo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Approach to Train Diverse Types of Language Models for Health Mention Classification of Tweets. (arXiv:2204.06337v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06337","description":"<p>Health mention classification deals with the disease detection in a given\ntext containing disease words. However, non-health and figurative use of\ndisease words adds challenges to the task. Recently, adversarial training\nacting as a means of regularization has gained popularity in many NLP tasks. In\nthis paper, we propose a novel approach to train language models for health\nmention classification of tweets that involves adversarial training. We\ngenerate adversarial examples by adding perturbation to the representations of\ntransformer models for tweet examples at various levels using Gaussian noise.\nFurther, we employ contrastive loss as an additional objective function. We\nevaluate the proposed method on the PHM2017 dataset extended version. Results\nshow that our proposed approach improves the performance of classifier\nsignificantly over the baseline methods. Moreover, our analysis shows that\nadding noise at earlier layers improves models' performance whereas adding\nnoise at intermediate layers deteriorates models' performance. Finally, adding\nnoise towards the final layers performs better than the middle layers noise\naddition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_P/0/1/0/all/0/1\">Pervaiz Iqbal Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_I/0/1/0/all/0/1\">Imran Razzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sheraz Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiDiverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types. (arXiv:2204.06347v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06347","description":"<p>Multimodal Entity Linking (MEL) which aims at linking mentions with\nmultimodal contexts to the referent entities from a knowledge base (e.g.,\nWikipedia), is an essential task for many multimodal applications. Although\nmuch attention has been paid to MEL, the shortcomings of existing MEL datasets\nincluding limited contextual topics and entity types, simplified mention\nambiguity, and restricted availability, have caused great obstacles to the\nresearch and application of MEL. In this paper, we present WikiDiverse, a\nhigh-quality human-annotated MEL dataset with diversified contextual topics and\nentity types from Wikinews, which uses Wikipedia as the corresponding knowledge\nbase. A well-tailored annotation procedure is adopted to ensure the quality of\nthe dataset. Based on WikiDiverse, a sequence of well-designed MEL models with\nintra-modality and inter-modality attentions are implemented, which utilize the\nvisual information of images more adequately than existing MEL models do.\nExtensive experimental analyses are conducted to investigate the contributions\nof different modalities in terms of MEL, facilitating the future research on\nthis task. The dataset and baseline models are available at\nhttps://github.com/wangxw5/wikiDiverse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1\">Min Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRUSH: Contextually Regularized and User anchored Self-supervised Hate speech Detection. (arXiv:2204.06389v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06389","description":"<p>The last decade has witnessed a surge in the interaction of people through\nsocial networking platforms. While there are several positive aspects of these\nsocial platforms, the proliferation has led them to become the breeding ground\nfor cyber-bullying and hate speech. Recent advances in NLP have often been used\nto mitigate the spread of such hateful content. Since the task of hate speech\ndetection is usually applicable in the context of social networks, we introduce\nCRUSH, a framework for hate speech detection using user-anchored\nself-supervision and contextual regularization. Our proposed approach secures ~\n1-12% improvement in test set metrics over best performing previous approaches\non two types of tasks and multiple popular english social media datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_P/0/1/0/all/0/1\">Parag Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Souvic Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sumegh Roychowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Markovian Generative Architectures for Efficient Task-Oriented Dialog Systems. (arXiv:2204.06452v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06452","description":"<p>Recently, Transformer based pretrained language models (PLMs), such as GPT2\nand T5, have been leveraged to build generative task-oriented dialog (TOD)\nsystems. A drawback of existing PLM-based models is their non-Markovian\narchitectures across turns, i.e., the whole history is used as the conditioning\ninput at each turn, which brings inefficiencies in memory, computation and\nlearning. In this paper, we propose to revisit Markovian Generative\nArchitectures (MGA), which have been used in previous LSTM-based TOD systems,\nbut not studied for PLM-based systems. Experiments on MultiWOZ2.1 show the\nefficiency advantages of the proposed Markovian PLM-based systems over their\nnon-Markovian counterparts, in both supervised and semi-supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yucheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer. (arXiv:2204.06457v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06457","description":"<p>Large pre-trained multilingual models such as mBERT and XLM-R enabled\neffective cross-lingual zero-shot transfer in many NLP tasks. A cross-lingual\nadjustment of these models using a small parallel corpus can potentially\nfurther improve results. This is a more data efficient method compared to\ntraining a machine-translation system or a multi-lingual model from scratch\nusing only parallel data. In this study, we experiment with zero-shot transfer\nof English models to four typologically different languages (Spanish, Russian,\nVietnamese, and Hindi) and three NLP tasks (QA, NLI, and NER). We carry out a\ncross-lingual adjustment of an off-the-shelf mBERT model. We confirm prior\nfinding that this adjustment makes embeddings of semantically similar words\nfrom different languages closer to each other, while keeping unrelated words\napart. However, from the paired-differences histograms introduced in our work\nwe can see that the adjustment only modestly affects the relative distances\nbetween related and unrelated words. In contrast, fine-tuning of mBERT on\nEnglish data (for a specific task such as NER) draws embeddings of both related\nand unrelated words closer to each other. The cross-lingual adjustment of mBERT\nimproves NLI in four languages and NER in two languages, while QA performance\nnever improves and sometimes degrades. When we fine-tune a cross-lingual\nadjusted mBERT for a specific task (e.g., NLI), the cross-lingual adjustment of\nmBERT may still improve the separation between related and related words, but\nthis works consistently only for the XNLI task. Our study contributes to a\nbetter understanding of cross-lingual transfer capabilities of large\nmultilingual language models and of effectiveness of their cross-lingual\nadjustment in various NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Efimov_P/0/1/0/all/0/1\">Pavel Efimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1\">Leonid Boytsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arslanova_E/0/1/0/all/0/1\">Elena Arslanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Language Model Adaptive Fine-Tuning: A Study on African Languages. (arXiv:2204.06487v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06487","description":"<p>Multilingual pre-trained language models (PLMs) have demonstrated impressive\nperformance on several downstream tasks on both high resourced and\nlow-resourced languages. However, there is still a large performance drop for\nlanguages unseen during pre-training, especially African languages. One of the\nmost effective approaches to adapt to a new language is language adaptive\nfine-tuning (LAFT) -- fine-tuning a multilingual PLM on monolingual texts of a\nlanguage using the same pre-training objective. However, African languages with\nlarge monolingual texts are few, and adapting to each of them individually\ntakes large disk space and limits the cross-lingual transfer abilities of the\nresulting models because they have been specialized for a single language. In\nthis paper, we perform multilingual adaptive fine-tuning (MAFT) on 17\nmost-resourced African languages and three other high-resource languages widely\nspoken on the African continent -- English, French, and Arabic to encourage\ncross-lingual transfer learning. Additionally, to further specialize the\nmultilingual PLM, we removed vocabulary tokens from the embedding layer that\ncorresponds to non-African writing scripts before MAFT, thus reducing the model\nsize by around 50\\%. Our evaluation on two multilingual PLMs (AfriBERTa and\nXLM-R) and three NLP tasks (NER, news topic classification, and sentiment\nclassification) shows that our approach is competitive to applying LAFT on\nindividual languages while requiring significantly less disk space. Finally, we\nshow that our adapted PLM also improves the zero-shot cross-lingual transfer\nabilities of parameter efficient fine-tuning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Study of Indian English Pronunciation Variabilities relative to Received Pronunciation. (arXiv:2204.06502v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06502","description":"<p>In contrast to British or American English, labeled pronunciation data on the\nphonetic level is scarce for Indian English (IE). This has made it challenging\nto study pronunciations of Indian English. Moreover, IE has many varieties,\nresulting from various native language influences on L2 English. Indian English\nhas been studied in the past, by a few linguistic works. They report phonetic\nrules for such characterisation, however, the extent to which they can be\napplied to a diverse large-scale Indian pronunciation data remains\nunder-examined. We consider a corpus, IndicTIMIT, which is rich in the\ndiversity of IE varieties and is curated in a nativity balanced manner. It\ncontains data from 80 speakers corresponding to various regions of India. We\npresent an approach to validate the phonetic rules of IE along with reporting\nunexplored rules derived using a data-driven manner, on this corpus. We also\nprovide quantitative information regarding which rules are more prominently\nobserved than the others, attributing to their relevance in IE accordingly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_P/0/1/0/all/0/1\">Priyanshi Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shelly Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuppala_A/0/1/0/all/0/1\">Anil Vuppala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarra_C/0/1/0/all/0/1\">Chiranjeevi Yarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Prasanta Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations. (arXiv:2204.06508v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06508","description":"<p>Despite recent improvements in abstractive summarization, most current\napproaches generate summaries that are not factually consistent with the source\ndocument, severely restricting their trust and usage in real-world\napplications. Recent works have shown promising improvements in factuality\nerror identification using text or dependency arc entailments; however, they do\nnot consider the entire semantic graph simultaneously. To this end, we propose\nFactGraph, a method that decomposes the document and the summary into\nstructured meaning representations (MR), which are more suitable for factuality\nevaluation. MRs describe core semantic concepts and their relations,\naggregating the main content in both document and summary in a canonical form,\nand reducing data sparsity. FactGraph encodes such graphs using a graph encoder\naugmented with structure-aware adapters to capture interactions among the\nconcepts based on the graph connectivity, along with text representations using\nan adapter-based text encoder. Experiments on different benchmarks for\nevaluating factuality show that FactGraph outperforms previous approaches by up\nto 15%. Furthermore, FactGraph improves performance on identifying content\nverifiability errors and better captures subsentence-level factual\ninconsistencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Training of Language Models using JAX pjit and TPUv4. (arXiv:2204.06514v1 [cs.LG])","link":"http://arxiv.org/abs/2204.06514","description":"<p>Modern large language models require distributed training strategies due to\ntheir size. The challenges of efficiently and robustly training them are met\nwith rapid developments on both software and hardware frontiers. In this\ntechnical report, we explore challenges and design decisions associated with\ndeveloping a scalable training framework, and present a quantitative analysis\nof efficiency improvements coming from adopting new software and hardware\nsolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Joanna Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perlin_K/0/1/0/all/0/1\">Kuba Perlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalakara_S/0/1/0/all/0/1\">Siddhartha Rao Kamalakara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A pipeline and comparative study of 12 machine learning models for text classification. (arXiv:2204.06518v1 [cs.IR])","link":"http://arxiv.org/abs/2204.06518","description":"<p>Text-based communication is highly favoured as a communication method,\nespecially in business environments. As a result, it is often abused by sending\nmalicious messages, e.g., spam emails, to deceive users into relaying personal\ninformation, including online accounts credentials or banking details. For this\nreason, many machine learning methods for text classification have been\nproposed and incorporated into the services of most email providers. However,\noptimising text classification algorithms and finding the right tradeoff on\ntheir aggressiveness is still a major research problem.\n</p>\n<p>We present an updated survey of 12 machine learning text classifiers applied\nto a public spam corpus. A new pipeline is proposed to optimise hyperparameter\nselection and improve the models' performance by applying specific methods\n(based on natural language processing) in the preprocessing stage.\n</p>\n<p>Our study aims to provide a new methodology to investigate and optimise the\neffect of different feature sizes and hyperparameters in machine learning\nclassifiers that are widely used in text classification problems. The\nclassifiers are tested and evaluated on different metrics including F-score\n(accuracy), precision, recall, and run time. By analysing all these aspects, we\nshow how the proposed pipeline can be used to achieve a good accuracy towards\nspam filtering on the Enron dataset, a widely used public email corpus.\nStatistical tests and explainability techniques are applied to provide a robust\nanalysis of the proposed pipeline and interpret the classification outcomes of\nthe 12 machine learning models, also identifying words that drive the\nclassification results. Our analysis shows that it is possible to identify an\neffective machine learning model to classify the Enron dataset with an F-score\nof 94%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Occhipinti_A/0/1/0/all/0/1\">Annalisa Occhipinti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_L/0/1/0/all/0/1\">Louis Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angione_C/0/1/0/all/0/1\">Claudio Angione</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Event Linking to Wikidata. (arXiv:2204.06535v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06535","description":"<p>We present a task of multilingual linking of events to a knowledge base. We\nautomatically compile a large-scale dataset for this task, comprising of 1.8M\nmentions across 44 languages referring to over 10.9K events from Wikidata. We\npropose two variants of the event linking task: 1) multilingual, where event\ndescriptions are from the same language as the mention, and 2) crosslingual,\nwhere all event descriptions are in English. On the two proposed tasks, we\ncompare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and\nmultilingual adaptations of the biencoder and crossencoder architectures from\nBLINK (Wu et al., 2020). In our experiments on the two task variants, we find\nboth biencoder and crossencoder models significantly outperform the BM25+\nbaseline. Our results also indicate that the crosslingual task is in general\nmore challenging than the multilingual task. To test the out-of-domain\ngeneralization of the proposed linking systems, we additionally create a\nWikinews-based evaluation set. We present qualitative analysis highlighting\nvarious aspects captured by the proposed dataset, including the need for\ntemporal reasoning over context and tackling diverse event descriptions across\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1\">Adithya Pratapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rishubh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Uncertainty Quantification for Machine Translation Evaluation. (arXiv:2204.06546v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06546","description":"<p>Neural-based machine translation (MT) evaluation metrics are progressing\nfast. However, these systems are often hard to interpret and might produce\nunreliable scores when human references or assessments are noisy or when data\nis out-of-domain. Recent work leveraged uncertainty quantification techniques\nsuch as Monte Carlo dropout and deep ensembles to provide confidence intervals,\nbut these techniques (as we show) are limited in several ways. In this paper we\ninvestigate more powerful and efficient uncertainty predictors for MT\nevaluation metrics and their potential to capture aleatoric and epistemic\nuncertainty. To this end we train the COMET metric with new heteroscedastic\nregression, divergence minimization, and direct uncertainty prediction\nobjectives. Our experiments show improved results on WMT20 and WMT21 metrics\ntask datasets and a substantial reduction in computational costs. Moreover,\nthey demonstrate the ability of our predictors to identify low quality\nreferences and to reveal model uncertainty due to out-of-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glushkova_T/0/1/0/all/0/1\">Taisiya Glushkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Few-shot Debugging for NLU Test Suites. (arXiv:2204.06555v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06555","description":"<p>We study few-shot debugging of transformer based natural language\nunderstanding models, using recently popularized test suites to not just\ndiagnose but correct a problem. Given a few debugging examples of a certain\nphenomenon, and a held-out test set of the same phenomenon, we aim to maximize\naccuracy on the phenomenon at a minimal cost of accuracy on the original test\nset. We examine several methods that are faster than full epoch retraining. We\nintroduce a new fast method, which samples a few in-danger examples from the\noriginal training set. Compared to fast methods using parameter distance\nconstraints or Kullback-Leibler divergence, we achieve superior original\naccuracy for comparable debugging accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malon_C/0/1/0/all/0/1\">Christopher Malon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruus_E/0/1/0/all/0/1\">Erik Kruus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query Obfuscation Semantic Decomposition. (arXiv:1909.05819v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1909.05819","description":"<p>We propose a method to protect the privacy of search engine users by\ndecomposing the queries using semantically \\emph{related} and unrelated\n\\emph{distractor} terms. Instead of a single query, the search engine receives\nmultiple decomposed query terms. Next, we reconstruct the search results\nrelevant to the original query term by aggregating the search results retrieved\nfor the decomposed query terms. We show that the word embeddings learnt using a\ndistributed representation learning method can be used to find semantically\nrelated and distractor query terms. We derive the relationship between the\n\\emph{obfuscity} achieved through the proposed query anonymisation method and\nthe \\emph{reconstructability} of the original search results using the\ndecomposed queries. We analytically study the risk of discovering the search\nengine users' information intents under the proposed query obfuscation method,\nand empirically evaluate its robustness against clustering-based attacks. Our\nexperimental results show that the proposed method can accurately reconstruct\nthe search results for user queries, without compromising the privacy of the\nsearch engine users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machide_T/0/1/0/all/0/1\">Tomoya Machide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawarabayashi_K/0/1/0/all/0/1\">Ken-ichi Kawarabayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Language Model for Task-Oriented Dialogue. (arXiv:2005.00796v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00796","description":"<p>Task-oriented dialogue is often decomposed into three tasks: understanding\nuser input, deciding actions, and generating a response. While such\ndecomposition might suggest a dedicated model for each sub-task, we find a\nsimple, unified approach leads to state-of-the-art performance on the MultiWOZ\ndataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a\nsingle, causal language model trained on all sub-tasks recast as a single\nsequence prediction problem. This allows SimpleTOD to fully leverage transfer\nlearning from pre-trained, open domain, causal language models such as GPT-2.\nSimpleTOD improves over the prior state-of-the-art in joint goal accuracy for\ndialogue state tracking, and our analysis reveals robustness to noisy\nannotations in this setting. SimpleTOD also improves the main metrics used to\nevaluate action decisions and response generation in an end-to-end setting:\ninform rate by 8.1 points, success rate by 9.7 points, and combined score by\n7.2 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_Asl_E/0/1/0/all/0/1\">Ehsan Hosseini-Asl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCann_B/0/1/0/all/0/1\">Bryan McCann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1\">Richard Socher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memformer: A Memory-Augmented Transformer for Sequence Modeling. (arXiv:2010.06891v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.06891","description":"<p>Transformers have reached remarkable success in sequence modeling. However,\nthese models have efficiency issues as they need to store all the history\ntoken-level representations as memory. We present Memformer, an efficient\nneural network for sequence modeling, that utilizes an external dynamic memory\nto encode and retrieve past information. Our model achieves linear time\ncomplexity and constant memory space complexity when processing long sequences.\nWe also propose a new optimization scheme, memory replay back-propagation\n(MRBP), which promotes long-range back-propagation through time with a\nsignificantly reduced memory requirement. Experimental results show that\nMemformer has achieved comparable performance compared to the baselines by\nusing 8.1x less memory space and 3.2x faster on inference. Analysis of the\nattention pattern shows that our external memory slots can encode and retain\nimportant information through timesteps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jing Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla. (arXiv:2101.00204v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00204","description":"<p>In this short paper, we introduce `BanglaBERT', a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two new downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nEvaluation (BangLUE) benchmark. BanglaBERT achieves state-of-the-art results\noutperforming multilingual and monolingual models. We are making the BanglaBERT\nmodel, the new datasets, and a leaderboard publicly available at\n\\url{https://github.com/csebuetnlp/banglabert} to advance Bangla NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samin_K/0/1/0/all/0/1\">Kazi Samin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_A/0/1/0/all/0/1\">Anindya Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combating Temporal Drift in Crisis with Adapted Embeddings. (arXiv:2104.08535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08535","description":"<p>Language usage changes over time, and this can impact the effectiveness of\nNLP systems. This work investigates methods for adapting to changing discourse\nduring crisis events. We explore social media data during crisis, for which\neffective, time-sensitive methods are necessary. We experiment with two\nseparate methods to accommodate changing data: temporal pretraining, which uses\nunlabeled data for the target time periods to train better language models, and\na model of embedding shift based on tools for analyzing semantic change. This\nshift allows us to counteract temporal drift by normalizing incoming data based\non observed patterns of language change. Simulating scenarios in which we lack\naccess to incoming labeled data, we demonstrate the effectiveness of these\nmethods for a wide variety of crises, showing we can improve performance by up\nto 8.0 F1 score for relevance classification across datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stowe_K/0/1/0/all/0/1\">Kevin Stowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Structure aware Relational Graph Convolutional Networks for Ontology Population. (arXiv:2104.12950v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.12950","description":"<p>Ontologies comprising of concepts, their attributes, and relationships are\nused in many knowledge based AI systems. While there have been efforts towards\npopulating domain specific ontologies, we examine the role of document\nstructure in learning ontological relationships between concepts in any\ndocument corpus. Inspired by ideas from hypernym discovery and explainability,\nour method performs about 15 points more accurate than a stand-alone R-GCN\nmodel for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shalghar_A/0/1/0/all/0/1\">Abhay M Shalghar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1\">Balaji Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Aswin Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_A/0/1/0/all/0/1\">Akshay Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1\">Shobha G</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Cross-Lingual Sentence Representation Learning. (arXiv:2105.13856v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.13856","description":"<p>Large-scale models for learning fixed-dimensional cross-lingual sentence\nrepresentations like LASER (Artetxe and Schwenk, 2019b) lead to significant\nimprovement in performance on downstream tasks. However, further increases and\nmodifications based on such large-scale models are usually impractical due to\nmemory limitations. In this work, we introduce a lightweight dual-transformer\narchitecture with just 2 layers for generating memory-efficient cross-lingual\nsentence representations. We explore different training tasks and observe that\ncurrent cross-lingual training tasks leave a lot to be desired for this shallow\narchitecture. To ameliorate this, we propose a novel cross-lingual language\nmodel, which combines the existing single-word masked language model with the\nnewly proposed cross-lingual token-level reconstruction task. We further\naugment the training task by the introduction of two computationally-lite\nsentence-level contrastive learning tasks to enhance the alignment of\ncross-lingual sentence representation space, which compensates for the learning\nbottleneck of the lightweight transformer for generative tasks. Our comparisons\nwith competing models on cross-lingual sentence retrieval and multilingual\ndocument classification confirm the effectiveness of the newly proposed\ntraining tasks for a shallow model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BenchIE: A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation. (arXiv:2109.06850v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06850","description":"<p>Intrinsic evaluations of OIE systems are carried out either manually -- with\nhuman evaluators judging the correctness of extractions -- or automatically, on\nstandardized benchmarks. The latter, while much more cost-effective, is less\nreliable, primarily because of the incompleteness of the existing OIE\nbenchmarks: the ground truth extractions do not include all acceptable variants\nof the same fact, leading to unreliable assessment of the models' performance.\nMoreover, the existing OIE benchmarks are available for English only. In this\nwork, we introduce BenchIE: a benchmark and evaluation framework for\ncomprehensive evaluation of OIE systems for English, Chinese, and German. In\ncontrast to existing OIE benchmarks, BenchIE is fact-based, i.e., it takes into\naccount informational equivalence of extractions: our gold standard consists of\nfact synsets, clusters in which we exhaustively list all acceptable surface\nforms of the same fact. Moreover, having in mind common downstream applications\nfor OIE, we make BenchIE multi-faceted; i.e., we create benchmark variants that\nfocus on different facets of OIE evaluation, e.g., compactness or minimality of\nextractions. We benchmark several state-of-the-art OIE systems using BenchIE\nand demonstrate that these systems are significantly less effective than\nindicated by existing OIE benchmarks. We make BenchIE (data and evaluation\ncode) publicly available on https://github.com/gkiril/benchie.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gashteovski_K/0/1/0/all/0/1\">Kiril Gashteovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingying Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnnIE: An Annotation Platform for Constructing Complete Open Information Extraction Benchmark. (arXiv:2109.07464v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07464","description":"<p>Open Information Extraction (OIE) is the task of extracting facts from\nsentences in the form of relations and their corresponding arguments in\nschema-free manner. Intrinsic performance of OIE systems is difficult to\nmeasure due to the incompleteness of existing OIE benchmarks: the ground truth\nextractions do not group all acceptable surface realizations of the same fact\nthat can be extracted from a sentence. To measure performance of OIE systems\nmore realistically, it is necessary to manually annotate complete facts (i.e.,\nclusters of all acceptable surface realizations of the same fact) from input\nsentences. We propose AnnIE: an interactive annotation platform that\nfacilitates such challenging annotation tasks and supports creation of complete\nfact-oriented OIE evaluation benchmarks. AnnIE is modular and flexible in order\nto support different use case scenarios (i.e., benchmarks covering different\ntypes of facts). We use AnnIE to build two complete OIE benchmarks: one with\nverb-mediated facts and another with facts encompassing named entities.\nFinally, we evaluate several OIE systems on our complete benchmarks created\nwith AnnIE. Our results suggest that existing incomplete benchmarks are overly\nlenient, and that OIE systems are not as robust as previously reported. We\npublicly release AnnIE under non-restrictive license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_N/0/1/0/all/0/1\">Niklas Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gashteovski_K/0/1/0/all/0/1\">Kiril Gashteovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingying Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Template-free Prompt Tuning for Few-shot NER. (arXiv:2109.13532v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13532","description":"<p>Prompt-based methods have been successfully applied in sentence-level\nfew-shot learning tasks, mostly owing to the sophisticated design of templates\nand label words. However, when applied to token-level labeling tasks such as\nNER, it would be time-consuming to enumerate the template queries over all\npotential entity spans. In this work, we propose a more elegant method to\nreformulate NER tasks as LM problems without any templates. Specifically, we\ndiscard the template construction process while maintaining the word prediction\nparadigm of pre-training models to predict a class-related pivot word (or label\nword) at the entity position. Meanwhile, we also explore principled ways to\nautomatically search for appropriate label words that the pre-trained models\ncan easily adapt to. While avoiding complicated template-based process, the\nproposed LM objective also reduces the gap between different objectives used in\npre-training and fine-tuning, thus it can better benefit the few-shot\nperformance. Experimental results demonstrate the effectiveness of the proposed\nmethod over bert-tagger and template-based method under few-shot setting.\nMoreover, the decoding speed of the proposed method is up to 1930.12 times\nfaster than the template-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiding Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation. (arXiv:2110.08547v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08547","description":"<p>This paper demonstrates that multilingual pretraining and multilingual\nfine-tuning are both critical for facilitating cross-lingual transfer in\nzero-shot translation, where the neural machine translation (NMT) model is\ntested on source languages unseen during supervised training. Following this\nidea, we present SixT+, a strong many-to-English NMT model that supports 100\nsource languages but is trained with a parallel dataset in only six source\nlanguages. SixT+ initializes the decoder embedding and the full encoder with\nXLM-R large and then trains the encoder and decoder layers with a simple\ntwo-stage training strategy. SixT+ achieves impressive performance on\nmany-to-English translation. It significantly outperforms CRISS and m2m-100,\ntwo strong multilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU\nrespectively. Additionally, SixT+ offers a set of model parameters that can be\nfurther fine-tuned to other unsupervised tasks. We demonstrate that adding\nSixT+ initialization outperforms state-of-the-art explicitly designed\nunsupervised NMT models on Si&lt;-&gt;En and Ne&lt;-&gt;En by over 1.2 average BLEU. When\napplied to zero-shot cross-lingual abstractive summarization, it produces an\naverage performance gain of 12.3 ROUGE-L over mBART-ft. We conduct detailed\nanalyses to understand the key ingredients of SixT+, including multilinguality\nof the auxiliary parallel data, positional disentangled encoder, and the\ncross-lingual transferability of its encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why KDAC? A general activation function for knowledge discovery. (arXiv:2111.13858v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.13858","description":"<p>Deep learning oriented named entity recognition (DNER) has gradually become\nthe paradigm of knowledge discovery, which greatly promotes domain\nintelligence. However, the current activation function of DNER fails to treat\ngradient vanishing, no negative output or non-differentiable existence, which\nmay impede knowledge exploration caused by the omission and incomplete\nrepresentation of latent semantics. To break through the dilemma, we present a\nnovel activation function termed KDAC. Detailly, KDAC is an aggregation\nfunction with multiple conversion modes. The backbone of the activation region\nis the interaction between exponent and linearity, and the both ends extend\nthrough adaptive linear divergence, which surmounts the obstacle of gradient\nvanishing and no negative output. Crucially, the non-differentiable points are\nalerted and eliminated by an approximate smoothing algorithm. KDAC has a series\nof brilliant properties, including nonlinear, stable near-linear transformation\nand derivative, as well as dynamic style, etc. We perform experiments based on\nBERT-BiLSTM-CNN-CRF model on six benchmark datasets containing different domain\nknowledge, such as Weibo, Clinical, E-commerce, Resume, HAZOP and People's\ndaily. The evaluation results show that KDAC is advanced and effective, and can\nprovide more generalized activation to stimulate the performance of DNER. We\nhope that KDAC can be exploited as a promising activation function to devote\nitself to the construction of knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanglin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consensus Graph Representation Learning for Better Grounded Image Captioning. (arXiv:2112.00974v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00974","description":"<p>The contemporary visual captioning models frequently hallucinate objects that\nare not actually in a scene, due to the visual misclassification or\nover-reliance on priors that resulting in the semantic inconsistency between\nthe visual information and the target lexical words. The most common way is to\nencourage the captioning model to dynamically link generated object words or\nphrases to appropriate regions of the image, i.e., the grounded image\ncaptioning (GIC). However, GIC utilizes an auxiliary task (grounding objects)\nthat has not solved the key issue of object hallucination, i.e., the semantic\ninconsistency. In this paper, we take a novel perspective on the issue above -\nexploiting the semantic coherency between the visual and language modalities.\nSpecifically, we propose the Consensus Rraph Representation Learning framework\n(CGRL) for GIC that incorporates a consensus representation into the grounded\ncaptioning pipeline. The consensus is learned by aligning the visual graph\n(e.g., scene graph) to the language graph that consider both the nodes and\nedges in a graph. With the aligned consensus, the captioning model can capture\nboth the correct linguistic characteristics and visual relevance, and then\ngrounding appropriate image regions further. We validate the effectiveness of\nour model, with a significant decline in object hallucination (-9% CHAIRi) on\nthe Flickr30k Entities dataset. Besides, our CGRL also evaluated by several\nautomatic metrics and human evaluation, the results indicate that the proposed\napproach can simultaneously improve the performance of image captioning (+2.9\nCider) and grounding (+2.3 F1LOC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenIE: Generative Information Extraction. (arXiv:2112.08340v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08340","description":"<p>Structured and grounded representation of text is typically formalized by\nclosed information extraction, the problem of extracting an exhaustive set of\n(subject, relation, object) triplets that are consistent with a predefined set\nof entities and relations from a knowledge base schema. Most existing works are\npipelines prone to error accumulation, and all approaches are only applicable\nto unrealistically small numbers of entities and relations. We introduce GenIE\n(generative information extraction), the first end-to-end autoregressive\nformulation of closed information extraction. GenIE naturally exploits the\nlanguage knowledge from the pre-trained transformer by autoregressively\ngenerating relations and entities in textual form. Thanks to a new bi-level\nconstrained generation strategy, only triplets consistent with the predefined\nknowledge base schema are produced. Our experiments show that GenIE is\nstate-of-the-art on closed information extraction, generalizes from fewer\ntraining data points than baselines, and scales to a previously unmanageable\nnumber of entities and relations. With this work, closed information extraction\nbecomes practical in realistic scenarios, providing new opportunities for\ndownstream tasks. Finally, this work paves the way towards a unified end-to-end\napproach to the core tasks of information extraction. Code, data and models\navailable at https://github.com/epfl-dlab/GenIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1\">Martin Josifoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Augmented Language Models for Cause-Effect Relation Classification. (arXiv:2112.08615v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08615","description":"<p>Previous studies have shown the efficacy of knowledge augmentation methods in\npretrained language models. However, these methods behave differently across\ndomains and downstream tasks. In this work, we investigate the augmentation of\npretrained language models with knowledge graph data in the cause-effect\nrelation classification and commonsense causal reasoning tasks. After\nautomatically verbalizing triples in ATOMIC2020, a wide coverage commonsense\nreasoning knowledge graph, we continually pretrain BERT and evaluate the\nresulting model on cause-effect pair classification and answering commonsense\ncausal reasoning questions. Our results show that a continually pretrained\nlanguage model augmented with commonsense reasoning knowledge outperforms our\nbaselines on two commonsense causal reasoning benchmarks, COPA and BCOPA-CE,\nand a Temporal and Causal Reasoning (TCR) dataset, without additional\nimprovement in model architecture or using quality-enhanced data for\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1\">Pedram Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broniatowski_D/0/1/0/all/0/1\">David A. Broniatowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling the Knowledge of Romanian BERTs Using Multiple Teachers. (arXiv:2112.12650v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.12650","description":"<p>Running large-scale pre-trained language models in computationally\nconstrained environments remains a challenging problem yet to be addressed,\nwhile transfer learning from these models has become prevalent in Natural\nLanguage Processing tasks. Several solutions, including knowledge distillation,\nnetwork quantization, or network pruning have been previously proposed;\nhowever, these approaches focus mostly on the English language, thus widening\nthe gap when considering low-resource languages. In this work, we introduce\nthree light and fast versions of distilled BERT models for the Romanian\nlanguage: Distil-BERT-base-ro, Distil-RoBERT-base, and\nDistilMulti-BERT-base-ro. The first two models resulted from the individual\ndistillation of knowledge from two base versions of Romanian BERTs available in\nliterature, while the last one was obtained by distilling their ensemble. To\nour knowledge, this is the first attempt to create publicly available Romanian\ndistilled BERT models, which were thoroughly evaluated on five tasks:\npart-of-speech tagging, named entity recognition, sentiment analysis, semantic\ntextual similarity, and dialect identification. Our experimental results argue\nthat the three distilled models offer performance comparable to their teachers,\nwhile being twice as fast on a GPU and ~35% smaller. In addition, we further\ntest the similarity between the predictions of our students versus their\nteachers by measuring their label and probability loyalty, together with\nregression loyalty - a new metric introduced in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catrina_D/0/1/0/all/0/1\">Darius Catrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1\">Dumitru-Clementin Cercel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dascalu_M/0/1/0/all/0/1\">Mihai Dasc&#x103;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebedea_T/0/1/0/all/0/1\">Traian Rebedea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile P&#x103;i&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufi&#x15f;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v3 [q-bio.BM] UPDATED)","link":"http://arxiv.org/abs/2201.11147","description":"<p>Self-supervised protein language models have proved their effectiveness in\nlearning the proteins representations. With the increasing computational power,\ncurrent protein language models pre-trained with millions of diverse sequences\ncan advance the parameter scale from million-level to billion-level and achieve\nremarkable improvement. However, those prevailing approaches rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured\nknowledge facts for better protein representations. We argue that informative\nbiology knowledge in KGs can enhance protein representation with external\nknowledge. In this work, we propose OntoProtein, the first general framework\nthat makes use of structure in GO (Gene Ontology) into protein pre-training\nmodels. We construct a novel large-scale knowledge graph that consists of GO\nand its related proteins, and gene annotation texts or protein sequences\ndescribe all nodes in the graph. We propose novel contrastive learning with\nknowledge-aware negative sampling to jointly optimize the knowledge graph and\nprotein embedding during pre-training. Experimental results show that\nOntoProtein can surpass state-of-the-art methods with pre-trained protein\nlanguage models in TAPE benchmark and yield better performance compared with\nbaselines in protein-protein interaction and protein function prediction. Code\nand datasets are available in https://github.com/zjunlp/OntoProtein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hong_H/0/1/0/all/0/1\">Haosen Hong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lian_J/0/1/0/all/0/1\">Jiazhang Lian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender stereotypes in the mediated personalization of politics: Empirical evidence from a lexical, syntactic and sentiment analysis. (arXiv:2202.03083v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03083","description":"<p>The media attention to the personal sphere of famous and important\nindividuals has become a key element of the gender narrative. Here we combine\nlexical, syntactic and sentiment analysis to investigate the role of gender in\nthe personalization of a wide range of political office holders in Italy during\nthe period 2017-2020. On the basis of a score for words that is introduced to\naccount for gender unbalance in both representative and news coverage, we show\nthat the political personalization in Italy is more detrimental for women than\nmen, with the persistence of entrenched stereotypes including a masculine\nconnotation of leadership, the resulting women's unsuitability to hold\npolitical functions, and a greater deal of focus on their attractiveness and\nbody parts. In addition, women politicians are covered with a more negative\ntone than their men counterpart when personal details are reported. Further,\nthe major contribution to the observed gender differences comes from online\nnews rather than print news, suggesting that the expression of certain\nstereotypes may be better conveyed when click baiting and personal targeting\nhave a major impact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brugnoli_E/0/1/0/all/0/1\">Emanuele Brugnoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simone_R/0/1/0/all/0/1\">Rosaria Simone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delmastro_M/0/1/0/all/0/1\">Marco Delmastro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniSAr: A Unified Structure-Aware Autoregressive Language Model for Text-to-SQL. (arXiv:2203.07781v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07781","description":"<p>Existing text-to-SQL semantic parsers are typically designed for particular\nsettings such as handling queries that span multiple tables, domains or turns\nwhich makes them ineffective when applied to different settings. We present\nUniSAr (Unified Structure-Aware Autoregressive Language Model), which benefits\nfrom directly using an off-the-shelf language model architecture and\ndemonstrates consistently high performance under different settings.\nSpecifically, UniSAr extends existing autoregressive language models to\nincorporate three non-invasive extensions to make them structure-aware: (1)\nadding structure mark to encode database schema, conversation context, and\ntheir relationships; (2) constrained decoding to decode well structured SQL for\na given database schema; and (3) SQL completion to complete potential missing\nJOIN relationships in SQL based on database schema. On seven well-known\ntext-to-SQL datasets covering multi-domain, multi-table and multi-turn, UniSAr\ndemonstrates highly comparable or better performance to the most advanced\nspecifically-designed text-to-SQL models. Importantly, our UniSAr is\nnon-invasive, such that other core model advances in text-to-SQL can also adopt\nour extensions to further enhance performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Mingyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingzirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">Dechen Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XTREME-S: Evaluating Cross-lingual Speech Representations. (arXiv:2203.10752v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10752","description":"<p>We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual\nspeech representations in many languages. XTREME-S covers four task families:\nspeech recognition, classification, speech-to-text translation and retrieval.\nCovering 102 languages from 10+ language families, 3 different domains and 4\ntask families, XTREME-S aims to simplify multilingual speech representation\nevaluation, as well as catalyze research in \"universal\" speech representation\nlearning. This paper describes the new benchmark and establishes the first\nspeech-only and speech-text baselines using XLS-R and mSLAM on all downstream\ntasks. We motivate the design choices and detail how to use the benchmark.\nDatasets and fine-tuning scripts are made easily accessible at\nhttps://hf.co/datasets/google/xtreme_s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1\">Patrick von Platen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozhkov_A/0/1/0/all/0/1\">Anton Lozhkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1\">Clara Rivera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esch_D/0/1/0/all/0/1\">Daan Van Esch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1\">Simran Khanuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks. (arXiv:2204.04596v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04596","description":"<p>Parameter-efficient tuning aims to distill knowledge for downstream tasks by\noptimizing a few introduced parameters while freezing the pretrained language\nmodels (PLMs). Continuous prompt tuning which prepends a few trainable vectors\nto the embeddings of input is one of these methods and has drawn much attention\ndue to its effectiveness and efficiency. This family of methods can be\nillustrated as exerting nonlinear transformations of hidden states inside PLMs.\nHowever, a natural question is ignored: can the hidden states be directly used\nfor classification without changing them? In this paper, we aim to answer this\nquestion by proposing a simple tuning method which only introduces three\ntrainable vectors. Firstly, we integrate all layers hidden states using the\nintroduced vectors. And then, we input the integrated hidden state(s) to a\ntask-specific linear classifier to predict categories. This scheme is similar\nto the way ELMo utilises hidden states except that they feed the hidden states\nto LSTM-based models. Although our proposed tuning scheme is simple, it\nachieves comparable performance with prompt tuning methods like P-tuning and\nP-tuning v2, verifying that original hidden states do contain useful\ninformation for classification tasks. Moreover, our method has an advantage\nover prompt tuning in terms of time and the number of parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions. (arXiv:2204.05212v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05212","description":"<p>Current QA systems can generate reasonable-sounding yet false answers without\nexplanation or evidence for the generated answer, which is especially\nproblematic when humans cannot readily check the model's answers. This presents\na challenge for building trust in machine learning systems. We take inspiration\nfrom real-world situations where difficult questions are answered by\nconsidering opposing sides (see Irving et al., 2018). For multiple-choice QA\nexamples, we build a dataset of single arguments for both a correct and\nincorrect answer option in a debate-style set-up as an initial step in training\nmodels to produce explanations for two candidate answers. We use long contexts\n-- humans familiar with the context write convincing explanations for\npre-selected correct and incorrect answers, and we test if those explanations\nallow humans who have not read the full context to more accurately determine\nthe correct answer. We do not find that explanations in our set-up improve\nhuman accuracy, but a baseline condition shows that providing human-selected\ntext snippets does improve accuracy. We use these findings to suggest ways of\nimproving the debate set up for future data collection efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nangia_N/0/1/0/all/0/1\">Nikita Nangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposed Meta-Learning for Few-Shot Named Entity Recognition. (arXiv:2204.05751v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05751","description":"<p>Few-shot named entity recognition (NER) systems aim at recognizing\nnovel-class named entities based on only a few labeled examples. In this paper,\nwe present a decomposed meta-learning approach which addresses the problem of\nfew-shot NER by sequentially tackling few-shot span detection and few-shot\nentity typing using meta-learning. In particular, we take the few-shot span\ndetection as a sequence labeling problem and train the span detector by\nintroducing the model-agnostic meta-learning (MAML) algorithm to find a good\nmodel parameter initialization that could fast adapt to new entity classes. For\nfew-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced\nprototypical networks to find a good embedding space that can better\ndistinguish text span representations from different entity classes. Extensive\nexperiments on various benchmarks show that our approach achieves superior\nperformance over prior methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tingting Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huiqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qianhui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Yew Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Annotation of Therapeutic Working Alliance in Psychotherapy. (arXiv:2204.05522v1 [q-bio.NC] CROSS LISTED)","link":"http://arxiv.org/abs/2204.05522","description":"<p>The therapeutic working alliance is an important predictor of the outcome of\nthe psychotherapy treatment. In practice, the working alliance is estimated\nfrom a set of scoring questionnaires in an inventory that both the patient and\nthe therapists fill out. In this work, we propose an analytical framework of\ndirectly inferring the therapeutic working alliance from the natural language\nwithin the psychotherapy sessions in a turn-level resolution with deep\nembeddings such as the Doc2Vec and SentenceBERT models. The transcript of each\npsychotherapy session can be transcribed and generated in real-time from the\nsession speech recordings, and these embedded dialogues are compared with the\ndistributed representations of the statements in the working alliance\ninventory. We demonstrate, in a real-world dataset with over 950 sessions of\npsychotherapy treatments in anxiety, depression, schizophrenia and suicidal\npatients, the effectiveness of this method in mapping out trajectories of\npatient-therapist alignment and the interpretability that can offer insights in\nclinical psychiatry. We believe such a framework can be provide timely feedback\nto the therapist regarding the quality of the conversation in interview\nsessions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"AGQA 2.0: An Updated Benchmark for Compositional Spatio-Temporal Reasoning. (arXiv:2204.06105v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06105","description":"<p>Prior benchmarks have analyzed models' answers to questions about videos in\norder to measure visual compositional reasoning. Action Genome Question\nAnswering (AGQA) is one such benchmark. AGQA provides a training/test split\nwith balanced answer distributions to reduce the effect of linguistic biases.\nHowever, some biases remain in several AGQA categories. We introduce AGQA 2.0,\na version of this benchmark with several improvements, most namely a stricter\nbalancing procedure. We then report results on the updated benchmark for all\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grunde_McLaughlin_M/0/1/0/all/0/1\">Madeleine Grunde-McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawala_M/0/1/0/all/0/1\">Maneesh Agrawala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-World Instance Segmentation: Exploiting Pseudo Ground Truth From Learned Pairwise Affinity. (arXiv:2204.06107v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06107","description":"<p>Open-world instance segmentation is the task of grouping pixels into object\ninstances without any pre-determined taxonomy. This is challenging, as\nstate-of-the-art methods rely on explicit class semantics obtained from large\nlabeled datasets, and out-of-domain evaluation performance drops significantly.\nHere we propose a novel approach for mask proposals, Generic Grouping Networks\n(GGNs), constructed without semantic supervision. Our approach combines a local\nmeasure of pixel affinity with instance-level mask supervision, producing a\ntraining regimen designed to make the model as generic as the data diversity\nallows. We introduce a method for predicting Pairwise Affinities (PA), a\nlearned local relationship between pairs of pixels. PA generalizes very well to\nunseen categories. From PA we construct a large set of pseudo-ground-truth\ninstance masks; combined with human-annotated instance masks we train GGNs and\nsignificantly outperform the SOTA on open-world instance segmentation on\nvarious benchmarks including COCO, LVIS, ADE20K, and UVO. Code is available on\nproject website: https://sites.google.com/view/generic-grouping/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Du Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SRMD: Sparse Random Mode Decomposition. (arXiv:2204.06108v1 [eess.SP])","link":"http://arxiv.org/abs/2204.06108","description":"<p>Signal decomposition and multiscale signal analysis provide many useful tools\nfor time-frequency analysis. We proposed a random feature method for analyzing\ntime-series data by constructing a sparse approximation to the spectrogram. The\nrandomization is both in the time window locations and the frequency sampling,\nwhich lowers the overall sampling and computational cost. The sparsification of\nthe spectrogram leads to a sharp separation between time-frequency clusters\nwhich makes it easier to identify intrinsic modes, and thus leads to a new\ndata-driven mode decomposition. The applications include signal representation,\noutlier removal, and mode decomposition. On the benchmark tests, we show that\nour approach outperforms other state-of-the-art decomposition methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Richardson_N/0/1/0/all/0/1\">Nicholas Richardson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaeffer_H/0/1/0/all/0/1\">Hayden Schaeffer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_G/0/1/0/all/0/1\">Giang Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Baseline Computation for Attribution Methods Based on Interpolated Inputs. (arXiv:2204.06120v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06120","description":"<p>We discuss a way to find a well behaved baseline for attribution methods that\nwork by feeding a neural network with a sequence of interpolated inputs between\ntwo given inputs. Then, we test it with our novel Riemann-Stieltjes Integrated\nGradient-weighted Class Activation Mapping (RSI-Grad-CAM) attribution method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lerma_M/0/1/0/all/0/1\">Miguel Lerma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Mirtha Lucas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Text-Conditional Image Generation with CLIP Latents. (arXiv:2204.06125v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06125","description":"<p>Contrastive models like CLIP have been shown to learn robust representations\nof images that capture both semantics and style. To leverage these\nrepresentations for image generation, we propose a two-stage model: a prior\nthat generates a CLIP image embedding given a text caption, and a decoder that\ngenerates an image conditioned on the image embedding. We show that explicitly\ngenerating image representations improves image diversity with minimal loss in\nphotorealism and caption similarity. Our decoders conditioned on image\nrepresentations can also produce variations of an image that preserve both its\nsemantics and style, while varying the non-essential details absent from the\nimage representation. Moreover, the joint embedding space of CLIP enables\nlanguage-guided image manipulations in a zero-shot fashion. We use diffusion\nmodels for the decoder and experiment with both autoregressive and diffusion\nmodels for the prior, finding that the latter are computationally more\nefficient and produce higher-quality samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_A/0/1/0/all/0/1\">Aditya Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhariwal_P/0/1/0/all/0/1\">Prafulla Dhariwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1\">Alex Nichol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Casey Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mark Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Texture Extraction and Distribution for Controllable Person Image Synthesis. (arXiv:2204.06160v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06160","description":"<p>We deal with the controllable person image synthesis task which aims to\nre-render a human from a reference image with explicit control over body pose\nand appearance. Observing that person images are highly structured, we propose\nto generate desired images by extracting and distributing semantic entities of\nreference images. To achieve this goal, a neural texture extraction and\ndistribution operation based on double attention is described. This operation\nfirst extracts semantic neural textures from reference feature maps. Then, it\ndistributes the extracted neural textures according to the spatial\ndistributions learned from target poses. Our model is trained to predict human\nimages in arbitrary poses, which encourages it to extract disentangled and\nexpressive neural textures representing the appearance of different semantic\nentities. The disentangled representation further enables explicit appearance\ncontrol. Neural textures of different reference images can be fused to control\nthe appearance of the interested areas. Experimental comparisons show the\nsuperiority of the proposed model. Code is available at\nhttps://github.com/RenYurui/Neural-Texture-Extraction-Distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yurui Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Thomas H. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Neural Textures: Generating Talking-Face Videos with Continuously Controllable Expressions. (arXiv:2204.06180v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06180","description":"<p>Recently, talking-face video generation has received considerable attention.\nSo far most methods generate results with neutral expressions or expressions\nthat are implicitly determined by neural networks in an uncontrollable way. In\nthis paper, we propose a method to generate talking-face videos with\ncontinuously controllable expressions in real-time. Our method is based on an\nimportant observation: In contrast to facial geometry of moderate resolution,\nmost expression information lies in textures. Then we make use of neural\ntextures to generate high-quality talking face videos and design a novel neural\nnetwork that can generate neural textures for image frames (which we called\ndynamic neural textures) based on the input expression and continuous intensity\nexpression coding (CIEC). Our method uses 3DMM as a 3D model to sample the\ndynamic neural texture. The 3DMM does not cover the teeth area, so we propose a\nteeth submodule to complete the details in teeth. Results and an ablation study\nshow the effectiveness of our method in generating high-quality talking-face\nvideos with continuously controllable expressions. We also set up four baseline\nmethods by combining existing representative methods and compare them with our\nmethod. Experimental results including a user study show that our method has\nthe best performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zipeng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhiyao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yu-Hui Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tian Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViViD++: Vision for Visibility Dataset. (arXiv:2204.06183v1 [cs.RO])","link":"http://arxiv.org/abs/2204.06183","description":"<p>In this paper, we present a dataset capturing diverse visual data formats\nthat target varying luminance conditions. While RGB cameras provide nourishing\nand intuitive information, changes in lighting conditions potentially result in\ncatastrophic failure for robotic applications based on vision sensors.\nApproaches overcoming illumination problems have included developing more\nrobust algorithms or other types of visual sensors, such as thermal and event\ncameras. Despite the alternative sensors' potential, there still are few\ndatasets with alternative vision sensors. Thus, we provided a dataset recorded\nfrom alternative vision sensors, by handheld or mounted on a car, repeatedly in\nthe same space but in different conditions. We aim to acquire visible\ninformation from co-aligned alternative vision sensors. Our sensor system\ncollects data more independently from visible light intensity by measuring the\namount of infrared dissipation, depth by structured reflection, and\ninstantaneous temporal changes in luminance. We provide these measurements\nalong with inertial sensors and ground-truth for developing robust visual SLAM\nunder poor illumination. The full dataset is available at:\nhttps://visibilitydataset.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Alex Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Younggun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Young-sik Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Ayoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COAP: Compositional Articulated Occupancy of People. (arXiv:2204.06184v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06184","description":"<p>We present a novel neural implicit representation for articulated human\nbodies. Compared to explicit template meshes, neural implicit body\nrepresentations provide an efficient mechanism for modeling interactions with\nthe environment, which is essential for human motion reconstruction and\nsynthesis in 3D scenes. However, existing neural implicit bodies suffer from\neither poor generalization on highly articulated poses or slow inference time.\nIn this work, we observe that prior knowledge about the human body's shape and\nkinematic structure can be leveraged to improve generalization and efficiency.\nWe decompose the full-body geometry into local body parts and employ a\npart-aware encoder-decoder architecture to learn neural articulated occupancy\nthat models complex deformations locally. Our local shape encoder represents\nthe body deformation of not only the corresponding body part but also the\nneighboring body parts. The decoder incorporates the geometric constraints of\nlocal body shape which significantly improves pose generalization. We\ndemonstrate that our model is suitable for resolving self-intersections and\ncollisions with 3D environments. Quantitative and qualitative experiments show\nthat our method largely outperforms existing solutions in terms of both\nefficiency and accuracy. The code and models are available at\nhttps://neuralbodies.github.io/COAP/index.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1\">Marko Mihajlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Class Weights with Multi-Modal Information for Partial Video Domain Adaptation. (arXiv:2204.06187v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06187","description":"<p>Assuming the source label space subsumes the target one, Partial Video Domain\nAdaptation (PVDA) is a more general and practical scenario for cross-domain\nvideo classification problems. The key challenge of PVDA is to mitigate the\nnegative transfer caused by the source-only outlier classes. To tackle this\nchallenge, a crucial step is to aggregate target predictions to assign class\nweights by up-weighing target classes and down-weighing outlier classes.\nHowever, the incorrect predictions of class weights can mislead the network and\nlead to negative transfer. Previous works improve the class weight accuracy by\nutilizing temporal features and attention mechanisms, but these methods may\nfall short when trying to generate accurate class weight when domain shifts are\nsignificant, as in most real-world scenarios. To deal with these challenges, we\npropose the Multi-modality Cluster-calibrated partial Adversarial Network\n(MCAN). MCAN enhances video feature extraction with multi-modal features from\nmultiple temporal scales to form more robust overall features. It utilizes a\nnovel class weight calibration method to alleviate the negative transfer caused\nby incorrect class weights. The calibration method tries to identify and weigh\ncorrect and incorrect predictions using distributional information implied by\nunsupervised clustering. Extensive experiments are conducted on prevailing PVDA\nbenchmarks, and the proposed MCAN achieves significant improvements when\ncompared to state-of-the-art PVDA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Model with GA based Feature Selection and Context Integration. (arXiv:2204.06189v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06189","description":"<p>Deep learning models have been very successful in computer vision and image\nprocessing applications. Since its inception, Many top-performing methods for\nimage segmentation are based on deep CNN models. However, deep CNN models fail\nto integrate global and local context alongside visual features despite having\ncomplex multi-layer architectures. We propose a novel three-layered deep\nlearning model that assiminlate or learns independently global and local\ncontextual information alongside visual features. The novelty of the proposed\nmodel is that One-vs-All binary class-based learners are introduced to learn\nGenetic Algorithm (GA) optimized features in the visual layer, followed by the\ncontextual layer that learns global and local contexts of an image, and finally\nthe third layer integrates all the information optimally to obtain the final\nclass label. Stanford Background and CamVid benchmark image parsing datasets\nwere used for our model evaluation, and our model shows promising results. The\nempirical analysis reveals that optimized visual features with global and local\ncontextual information play a significant role to improve accuracy and produce\nstable predictions comparable to state-of-the-art deep CNN models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_R/0/1/0/all/0/1\">Ranju Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azam_B/0/1/0/all/0/1\">Basim Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_B/0/1/0/all/0/1\">Brijesh Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengjie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"5G Features and Standards for Vehicle Data Exploitation. (arXiv:2204.06211v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06211","description":"<p>Cars capture and generate huge volumes of data in real-time about the driving\ndynamics, the environment, and the driver and passengers' activities. Due to\nthe proliferation of cooperative, connected and automated mobility (CCAM), the\nvalue of data from vehicles is getting strategic, not just for the automotive\nindustry, but also for many diverse stakeholders including small and\nmedium-sized enterprises (SMEs) and start-ups. 5G can enable car-captured data\nto feed innovative applications and services deployed in the cloud ensuring\nlower latency and higher throughput than previous cellular technologies. This\npaper identifies and discusses the relevance of the main 5G features that can\ncontribute to a scalable, flexible, reliable and secure data pipeline, pointing\nto the standards and technical reports that specify their implementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velez_G/0/1/0/all/0/1\">Gorka Velez</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bonetto_E/0/1/0/all/0/1\">Edoardo Bonetto</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Brevi_D/0/1/0/all/0/1\">Daniele Brevi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Angel Martin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Rizzi_G/0/1/0/all/0/1\">Gianluca Rizzi</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Castaneda_O/0/1/0/all/0/1\">Oscar Casta&#xf1;eda</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Cherif_A/0/1/0/all/0/1\">Arslane Hamza Cherif</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1\">Marcos Nieto</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Otaegui_O/0/1/0/all/0/1\">Oihana Otaegui</a> (1) ((1) Vicomtech Foundation, (2) Links Foundation, (3) Wind Tre, (4) Dekra, (5) UNIMORE &amp; ICOOR)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defensive Patches for Robust Recognition in the Physical World. (arXiv:2204.06213v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06213","description":"<p>To operate in real-world high-stakes environments, deep learning systems have\nto endure noises that have been continuously thwarting their robustness.\nData-end defense, which improves robustness by operations on input data instead\nof modifying models, has attracted intensive attention due to its feasibility\nin practice. However, previous data-end defenses show low generalization\nagainst diverse noises and weak transferability across multiple models.\nMotivated by the fact that robust recognition depends on both local and global\nfeatures, we propose a defensive patch generation framework to address these\nproblems by helping models better exploit these features. For the\ngeneralization against diverse noises, we inject class-specific identifiable\npatterns into a confined local patch prior, so that defensive patches could\npreserve more recognizable features towards specific classes, leading models\nfor better recognition under noises. For the transferability across multiple\nmodels, we guide the defensive patches to capture more global feature\ncorrelations within a class, so that they could activate model-shared global\nperceptions and transfer better among models. Our defensive patches show great\npotentials to improve application robustness in practice by simply sticking\nthem around target objects. Extensive experiments show that we outperform\nothers by large margins (improve 20+\\% accuracy for both adversarial and\ncorruption robustness on average in the digital and physical world). Our codes\nare available at https://github.com/nlsde-safety-team/DefensivePatch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zixin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Pengfei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Renshuai Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-based Deep Learning Architecture with Optimal Integration Layer for Image Parsing. (arXiv:2204.06214v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06214","description":"<p>Deep learning models have been efficient lately on image parsing tasks.\nHowever, deep learning models are not fully capable of exploiting visual and\ncontextual information simultaneously. The proposed three-layer context-based\ndeep architecture is capable of integrating context explicitly with visual\ninformation. The novel idea here is to have a visual layer to learn visual\ncharacteristics from binary class-based learners, a contextual layer to learn\ncontext, and then an integration layer to learn from both via genetic\nalgorithm-based optimal fusion to produce a final decision. The experimental\noutcomes when evaluated on benchmark datasets are promising. Further analysis\nshows that optimized network weights can improve performance and make stable\npredictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_R/0/1/0/all/0/1\">Ranju Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azam_B/0/1/0/all/0/1\">Basim Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_B/0/1/0/all/0/1\">Brijesh Verma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization. (arXiv:2204.06228v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06228","description":"<p>Due to its high societal impact, deepfake detection is getting active\nattention in the computer vision community. Most deepfake detection methods\nrely on identity, facial attribute and adversarial perturbation based\nspatio-temporal modifications at the whole video or random locations, while\nkeeping the meaning of the content intact. However, a sophisticated deepfake\nmay contain only a small segment of video/audio manipulation, through which the\nmeaning of the content can be, for example, completely inverted from sentiment\nperspective. To address this gap, we introduce a content driven audio-visual\ndeepfake dataset, termed as Localized Audio Visual DeepFake (LAV-DF),\nexplicitly designed for the task of learning temporal forgery localization.\nSpecifically, the content driven audio-visual manipulations are performed at\nstrategic locations in order to change the sentiment polarity of the whole\nvideo. Our baseline method for benchmarking the proposed dataset is a 3DCNN\nmodel, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is\nguided via contrastive, boundary matching and frame classification loss\nfunctions. Our extensive quantitative analysis demonstrates the strong\nperformance of the proposed method for both task of temporal forgery\nlocalization and deepfake detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhixi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanov_K/0/1/0/all/0/1\">Kalin Stefanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1\">Abhinav Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid model transfer for medical image segmentation via iterative human-in-the-loop update: from labelled public to unlabelled clinical datasets for multi-organ segmentation in CT. (arXiv:2204.06243v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06243","description":"<p>Despite the remarkable success on medical image analysis with deep learning,\nit is still under exploration regarding how to rapidly transfer AI models from\none dataset to another for clinical applications. This paper presents a novel\nand generic human-in-the-loop scheme for efficiently transferring a\nsegmentation model from a small-scale labelled dataset to a larger-scale\nunlabelled dataset for multi-organ segmentation in CT. To achieve this, we\npropose to use an igniter network which can learn from a small-scale labelled\ndataset and generate coarse annotations to start the process of human-machine\ninteraction. Then, we use a sustainer network for our larger-scale dataset, and\niteratively updated it on the new annotated data. Moreover, we propose a\nflexible labelling strategy for the annotator to reduce the initial annotation\nworkload. The model performance and the time cost of annotation in each subject\nevaluated on our private dataset are reported and analysed. The results show\nthat our scheme can not only improve the performance by 19.7% on Dice, but also\nexpedite the cost time of manual labelling from 13.87 min to 1.51 min per CT\nvolume during the model transfer, demonstrating the clinical usefulness with\npromising potentials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Matters in Language Conditioned Robotic Imitation Learning. (arXiv:2204.06252v1 [cs.RO])","link":"http://arxiv.org/abs/2204.06252","description":"<p>A long-standing goal in robotics is to build robots that can perform a wide\nrange of daily tasks from perceptions obtained with their onboard sensors and\nspecified only via natural language. While recently substantial advances have\nbeen achieved in language-driven robotics by leveraging end-to-end learning\nfrom pixels, there is no clear and well-understood process for making various\ndesign choices due to the underlying variation in setups. In this paper, we\nconduct an extensive study of the most critical challenges in learning language\nconditioned policies from offline free-form imitation datasets. We further\nidentify architectural and algorithmic techniques that improve performance,\nsuch as a hierarchical decomposition of the robot control learning, a\nmultimodal transformer encoder, discrete latent plans and a self-supervised\ncontrastive loss that aligns video and language representations. By combining\nthe results of our investigation with our improved model components, we are\nable to present a novel approach that significantly outperforms the state of\nthe art on the challenging language conditioned long-horizon robot manipulation\nCALVIN benchmark. We have open-sourced our implementation to facilitate future\nresearch in learning to perform many complex manipulation skills in a row\nspecified with natural language. Codebase and trained models available at\n<a href=\"http://hulc.cs.uni-freiburg.de\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_L/0/1/0/all/0/1\">Lukas Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection. (arXiv:2204.06272v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06272","description":"<p>3D visual grounding aims to locate the referred target object in 3D point\ncloud scenes according to a free-form language description. Previous methods\nmostly follow a two-stage paradigm, i.e., language-irrelevant detection and\ncross-modal matching, which is limited by the isolated architecture. In such a\nparadigm, the detector needs to sample keypoints from raw point clouds due to\nthe inherent properties of 3D point clouds (irregular and large-scale), to\ngenerate the corresponding object proposal for each keypoint. However, sparse\nproposals may leave out the target in detection, while dense proposals may\nconfuse the matching model. Moreover, the language-irrelevant detection stage\ncan only sample a small proportion of keypoints on the target, deteriorating\nthe target prediction. In this paper, we propose a 3D Single-Stage Referred\nPoint Progressive Selection (3D-SPS) method, which progressively selects\nkeypoints with the guidance of language and directly locates the target.\nSpecifically, we propose a Description-aware Keypoint Sampling (DKS) module to\ncoarsely focus on the points of language-relevant objects, which are\nsignificant clues for grounding. Besides, we devise a Target-oriented\nProgressive Mining (TPM) module to finely concentrate on the points of the\ntarget, which is enabled by progressive intra-modal relation modeling and\ninter-modal target mining. 3D-SPS bridges the gap between detection and\nmatching in the 3D visual grounding task, localizing the target at a single\nstage. Experiments demonstrate that 3D-SPS achieves state-of-the-art\nperformance on both ScanRefer and Nr3D/Sr3D datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jiahui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xianghao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haibing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Huaxia Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing cloudiness in nonwovens. (arXiv:2204.06275v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06275","description":"<p>The homogeneity of filter media is important for material selection and\nquality control, along with the specific weight (nominal grammage) and the\ndistribution of the local weight. Cloudiness or formation is a concept used to\ndescribe deviations from homogeneity in filter media. We suggest to derive the\ncloudiness index from the power spectrum of the relative local areal weight,\nintegrated over a selected frequency range. The power spectrum captures the\nenergy density in a broad spectral range. Moreover, under certain conditions,\nthe structure of a nonwoven is fully characterized by the areal weight, the\nvariance of the local areal weight, and the power spectrum. Consequently, the\npower spectrum is the parameter that exclusively reflects the cloudiness. Here,\nwe address questions arising from practical application. The most prominent is\nthe choice of the spectral band. It certainly depends on the characteristic\n\"size of the clouds\", but is limited by the size and lateral resolution of the\nimages. We show that the cloudiness index based on the power spectrum of the\nrelative local areal weight is theoretically well founded and can be robustly\nmeasured from image data. Choosing the spectral band allows to capture the\ncloudiness either visually perceived or found to be decisive for product\nproperties. It is thus well suited to build a technical standard on it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godehardt_M/0/1/0/all/0/1\">Michael Godehardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghiseh_A/0/1/0/all/0/1\">Ali Moghiseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oetjen_C/0/1/0/all/0/1\">Christine Oetjen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohser_J/0/1/0/all/0/1\">Joachim Ohser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringger_S/0/1/0/all/0/1\">Simon Ringger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schladitz_K/0/1/0/all/0/1\">Katja Schladitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Windschiegel_I/0/1/0/all/0/1\">Ingo Windschiegel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reuse your features: unifying retrieval and feature-metric alignment. (arXiv:2204.06292v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06292","description":"<p>We propose a compact pipeline to unify all the steps of Visual Localization:\nimage retrieval, candidate re-ranking and initial pose estimation, and camera\npose refinement. Our key assumption is that the deep features used for these\nindividual tasks share common characteristics, so we should reuse them in all\nthe procedures of the pipeline. Our DRAN (Deep Retrieval and image Alignment\nNetwork) is able to extract global descriptors for efficient image retrieval,\nuse intermediate hierarchical features to re-rank the retrieval list and\nproduce an intial pose guess, which is finally refined by means of a\nfeature-metric optimization based on learned deep multi-scale dense features.\nDRAN is the first single network able to produce the features for the three\nsteps of visual localization. DRAN achieves a competitive performance in terms\nof robustness and accuracy specially in extreme day-night changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morlana_J/0/1/0/all/0/1\">Javier Morlana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1\">J.M.M. Montiel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Diffusion and VCA-Assisted Image Segmentation of Hyperspectral Images. (arXiv:2204.06298v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06298","description":"<p>Hyperspectral images encode rich structure that can be exploited for material\ndiscrimination by machine learning algorithms. This article introduces the\nActive Diffusion and VCA-Assisted Image Segmentation (ADVIS) for active\nmaterial discrimination. ADVIS selects high-purity, high-density pixels that\nare far in diffusion distance (a data-dependent metric) from other high-purity,\nhigh-density pixels in the hyperspectral image. The ground truth labels of\nthese pixels are queried and propagated to the rest of the image. The ADVIS\nactive learning algorithm is shown to strongly outperform its fully\nunsupervised clustering algorithm counterpart, suggesting that the\nincorporation of a very small number of carefully-selected ground truth labels\ncan result in substantially superior material discrimination in hyperspectral\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polk_S/0/1/0/all/0/1\">Sam L. Polk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kangning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plemmons_R/0/1/0/all/0/1\">Robert J. Plemmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_J/0/1/0/all/0/1\">James M. Murphy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIB-VA at SemEval-2022 Task 5: A Multimodal Architecture for the Detection and Classification of Misogynous Memes. (arXiv:2204.06299v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06299","description":"<p>The detection of offensive, hateful content on social media is a challenging\nproblem that affects many online users on a daily basis. Hateful content is\noften used to target a group of people based on ethnicity, gender, religion and\nother factors. The hate or contempt toward women has been increasing on social\nplatforms. Misogynous content detection is especially challenging when textual\nand visual modalities are combined to form a single context, e.g., an overlay\ntext embedded on top of an image, also known as meme. In this paper, we present\na multimodal architecture that combines textual and visual features in order to\ndetect misogynous meme content. The proposed architecture is evaluated in the\nSemEval-2022 Task 5: MAMI - Multimedia Automatic Misogyny Identification\nchallenge under the team name TIB-VA. Our solution obtained the best result in\nthe Task-B where the challenge is to classify whether a given document is\nmisogynous and further identify the main sub-classes of shaming, stereotype,\nobjectification, and violence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1\">Gullal S. Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Consistent Generative Adversarial Networks for 3D-aware Image Synthesis. (arXiv:2204.06307v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06307","description":"<p>3D-aware image synthesis aims to generate images of objects from multiple\nviews by learning a 3D representation. However, one key challenge remains:\nexisting approaches lack geometry constraints, hence usually fail to generate\nmulti-view consistent images. To address this challenge, we propose Multi-View\nConsistent Generative Adversarial Networks (MVCGAN) for high-quality 3D-aware\nimage synthesis with geometry constraints. By leveraging the underlying 3D\ngeometry information of generated images, i.e., depth and camera transformation\nmatrix, we explicitly establish stereo correspondence between views to perform\nmulti-view joint optimization. In particular, we enforce the photometric\nconsistency between pairs of views and integrate a stereo mixup mechanism into\nthe training process, encouraging the model to reason about the correct 3D\nshape. Besides, we design a two-stage training strategy with feature-level\nmulti-view joint optimization to improve the image quality. Extensive\nexperiments on three datasets demonstrate that MVCGAN achieves the\nstate-of-the-art performance for 3D-aware image synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Daiheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_P/0/1/0/all/0/1\">Pan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Framework for Automatic Cranial Defect Reconstruction and Implant Modeling. (arXiv:2204.06310v1 [eess.IV])","link":"http://arxiv.org/abs/2204.06310","description":"<p>The goal of this work is to propose a robust, fast, and fully automatic\nmethod for personalized cranial defect reconstruction and implant modeling.\n</p>\n<p>We propose a two-step deep learning-based method using a modified U-Net\narchitecture to perform the defect reconstruction, and a dedicated iterative\nprocedure to improve the implant geometry, followed by automatic generation of\nmodels ready for 3-D printing. We propose a cross-case augmentation based on\nimperfect image registration combining cases from different datasets. We\nperform ablation studies regarding different augmentation strategies and\ncompare them to other state-of-the-art methods.\n</p>\n<p>We evaluate the method on three datasets introduced during the AutoImplant\n2021 challenge, organized jointly with the MICCAI conference. We perform the\nquantitative evaluation using the Dice and boundary Dice coefficients, and the\nHausdorff distance. The average Dice coefficient, boundary Dice coefficient,\nand the 95th percentile of Hausdorff distance are 0.91, 0.94, and 1.53 mm\nrespectively. We perform an additional qualitative evaluation by 3-D printing\nand visualization in mixed reality to confirm the implant's usefulness.\n</p>\n<p>We propose a complete pipeline that enables one to create the cranial implant\nmodel ready for 3-D printing. The described method is a greatly extended\nversion of the method that scored 1st place in all AutoImplant 2021 challenge\ntasks. We freely release the source code, that together with the open datasets,\nmakes the results fully reproducible. The automatic reconstruction of cranial\ndefects may enable manufacturing personalized implants in a significantly\nshorter time, possibly allowing one to perform the 3-D printing process\ndirectly during a given intervention. Moreover, we show the usability of the\ndefect reconstruction in mixed reality that may further reduce the surgery\ntime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wodzinski_M/0/1/0/all/0/1\">Marek Wodzinski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daniol_M/0/1/0/all/0/1\">Mateusz Daniol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Socha_M/0/1/0/all/0/1\">Miroslaw Socha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hemmerling_D/0/1/0/all/0/1\">Daria Hemmerling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stanuch_M/0/1/0/all/0/1\">Maciej Stanuch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Skalski_A/0/1/0/all/0/1\">Andrzej Skalski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition of Freely Selected Keypoints on Human Limbs. (arXiv:2204.06326v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06326","description":"<p>Nearly all Human Pose Estimation (HPE) datasets consist of a fixed set of\nkeypoints. Standard HPE models trained on such datasets can only detect these\nkeypoints. If more points are desired, they have to be manually annotated and\nthe model needs to be retrained. Our approach leverages the Vision Transformer\narchitecture to extend the capability of the model to detect arbitrary\nkeypoints on the limbs of persons. We propose two different approaches to\nencode the desired keypoints. (1) Each keypoint is defined by its position\nalong the line between the two enclosing keypoints from the fixed set and its\nrelative distance between this line and the edge of the limb. (2) Keypoints are\ndefined as coordinates on a norm pose. Both approaches are based on the\nTokenPose architecture, while the keypoint tokens that correspond to the fixed\nkeypoints are replaced with our novel module. Experiments show that our\napproaches achieve similar results to TokenPose on the fixed keypoints and are\ncapable of detecting arbitrary keypoints on the limbs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ludwig_K/0/1/0/all/0/1\">Katja Ludwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kienzle_D/0/1/0/all/0/1\">Daniel Kienzle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienhart_R/0/1/0/all/0/1\">Rainer Lienhart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transparent Shape from Single Polarization Images. (arXiv:2204.06331v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06331","description":"<p>This paper presents a data-driven approach for transparent shape from\npolarization. Due to the inherent high transmittance, the previous shape from\npolarization(SfP) methods based on specular reflection model have difficulty in\nestimating transparent shape, and the lack of datasets for transparent SfP also\nlimits the application of the data-driven approach. Hence, we construct the\ntransparent SfP dataset which consists of both synthetic and real-world\ndatasets. To determine the reliability of the physics-based reflection model,\nwe define the physics-based prior confidence by exploiting the inherent fault\nof polarization information, then we propose a multi-branch fusion network to\nembed the confidence. Experimental results show that our approach outperforms\nother SfP methods. Compared with the previous method, the mean and median\nangular error of our approach are reduced from $19.00^\\circ$ and $14.91^\\circ$\nto $16.72^\\circ$ and $13.36^\\circ$, and the accuracy $11.25^\\circ, 22.5^\\circ,\n30^\\circ$ are improved from $38.36\\%, 77.36\\%, 87.48\\%$ to $45.51\\%, 78.86\\%,\n89.98\\%$, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mingqi_S/0/1/0/all/0/1\">Shao Mingqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chongkun_X/0/1/0/all/0/1\">Xia Chongkun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhendong_Y/0/1/0/all/0/1\">Yang Zhendong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junnan_H/0/1/0/all/0/1\">Huang Junnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xueqian_W/0/1/0/all/0/1\">Wang Xueqian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Bias in Facial Analysis Systems by Incorporating Label Diversity. (arXiv:2204.06364v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06364","description":"<p>Facial analysis models are increasingly applied in real-world applications\nthat have significant impact on peoples' lives. However, as previously shown,\nmodels that automatically classify facial attributes might exhibit algorithmic\ndiscrimination behavior with respect to protected groups, potentially posing\nnegative impacts on individuals and society. It is therefore critical to\ndevelop techniques that can mitigate unintended biases in facial classifiers.\nHence, in this work, we introduce a novel learning method that combines both\nsubjective human-based labels and objective annotations based on mathematical\ndefinitions of facial traits. Specifically, we generate new objective\nannotations from a large-scale human-annotated dataset, each capturing a\ndifferent perspective of the analyzed facial trait. We then propose an ensemble\nlearning method, which combines individual models trained on different types of\nannotations. We provide an in-depth analysis of the annotation procedure as\nwell as the dataset distribution. Moreover, we empirically demonstrate that, by\nincorporating label diversity, and without additional synthetic images, our\nmethod successfully mitigates unintended biases, while maintaining significant\naccuracy on the downstream task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolling_C/0/1/0/all/0/1\">Camila Kolling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Victor Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_A/0/1/0/all/0/1\">Adriano Veloso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musse_S/0/1/0/all/0/1\">Soraia Raupp Musse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning based automatic detection of offshore oil slicks using SAR data and contextual information. (arXiv:2204.06371v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06371","description":"<p>Ocean surface monitoring, especially oil slick detection, has become\nmandatory due to its importance for oil exploration and risk prevention on\necosystems. For years, the detection task has been performed manually by\nphoto-interpreters using Synthetic Aperture Radar (SAR) images with the help of\ncontextual data such as wind. This tedious manual work cannot handle the\nincreasing amount of data collected by the available sensors and thus requires\nautomation. Literature reports conventional and semi-automated detection\nmethods that generally focus either on oil slicks originating from\nanthropogenic (spills) or natural (seeps) sources on limited data collections.\nAs an extension, this paper presents the automation of offshore oil slicks on\nan extensive database with both kinds of slicks. It builds upon the slick\nannotations of specialized photo-interpreters on Sentinel-1 SAR data for 4\nyears over 3 exploration and monitoring areas worldwide. All the considered SAR\nimages and related annotation relate to real oil slick monitoring scenarios.\nFurther, wind estimation is systematically computed to enrich the data\ncollection. Paper contributions are the following : (i) a performance\ncomparison of two deep learning approaches: semantic segmentation using\nFC-DenseNet and instance segmentation using Mask-RCNN. (ii) the introduction of\nmeteorological information (wind speed) is deemed valuable for oil slick\ndetection in the performance evaluation. The main results of this study show\nthe effectiveness of slick detection by deep learning approaches, in particular\nFC-DenseNet, which captures more than 92% of oil instances in our test set.\nFurthermore, a strong correlation between model performances and contextual\ninformation such as slick size and wind speed is demonstrated in the\nperformance evaluation. This work opens perspectives to design models that can\nfuse SAR and wind information to reduce the false alarm rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amri_E/0/1/0/all/0/1\">Emna Amri</a> (LISTIC), <a href=\"http://arxiv.org/find/cs/1/au:+Courteille_H/0/1/0/all/0/1\">Hermann Courteille</a> (LISTIC), <a href=\"http://arxiv.org/find/cs/1/au:+Benoit_A/0/1/0/all/0/1\">A Benoit</a> (LISTIC), <a href=\"http://arxiv.org/find/cs/1/au:+Bolon_P/0/1/0/all/0/1\">Philippe Bolon</a> (LISTIC), <a href=\"http://arxiv.org/find/cs/1/au:+Dubucq_D/0/1/0/all/0/1\">Dominique Dubucq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poulain_G/0/1/0/all/0/1\">Gilles Poulain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Credoz_A/0/1/0/all/0/1\">Anthony Credoz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Receding Neuron Importances for Structured Pruning. (arXiv:2204.06404v1 [cs.LG])","link":"http://arxiv.org/abs/2204.06404","description":"<p>Structured pruning efficiently compresses networks by identifying and\nremoving unimportant neurons. While this can be elegantly achieved by applying\nsparsity-inducing regularisation on BatchNorm parameters, an L1 penalty would\nshrink all scaling factors rather than just those of superfluous neurons. To\ntackle this issue, we introduce a simple BatchNorm variation with bounded\nscaling parameters, based on which we design a novel regularisation term that\nsuppresses only neurons with low importance. Under our method, the weights of\nunnecessary neurons effectively recede, producing a polarised bimodal\ndistribution of importances. We show that neural networks trained this way can\nbe pruned to a larger extent and with less deterioration. We one-shot prune VGG\nand ResNet architectures at different ratios on CIFAR and ImagenNet datasets.\nIn the case of VGG-style networks, our method significantly outperforms\nexisting approaches particularly under a severe pruning regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suteu_M/0/1/0/all/0/1\">Mihai Suteu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DMCNet: Diversified Model Combination Network for Understanding Engagement from Video Screengrabs. (arXiv:2204.06454v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06454","description":"<p>Engagement is an essential indicator of the Quality-of-Learning Experience\n(QoLE) and plays a major role in developing intelligent educational interfaces.\nThe number of people learning through Massively Open Online Courses (MOOCs) and\nother online resources has been increasing rapidly because they provide us with\nthe flexibility to learn from anywhere at any time. This provides a good\nlearning experience for the students. However, such learning interface requires\nthe ability to recognize the level of engagement of the students for a holistic\nlearning experience. This is useful for both students and educators alike.\nHowever, understanding engagement is a challenging task, because of its\nsubjectivity and ability to collect data. In this paper, we propose a variety\nof models that have been trained on an open-source dataset of video\nscreengrabs. Our non-deep learning models are based on the combination of\npopular algorithms such as Histogram of Oriented Gradient (HOG), Support Vector\nMachine (SVM), Scale Invariant Feature Transform (SIFT) and Speeded Up Robust\nFeatures (SURF). The deep learning methods include Densely Connected\nConvolutional Networks (DenseNet-121), Residual Network (ResNet-18) and\nMobileNetV1. We show the performance of each models using a variety of metrics\nsuch as the Gini Index, Adjusted F-Measure (AGF), and Area Under receiver\noperating characteristic Curve (AUC). We use various dimensionality reduction\ntechniques such as Principal Component Analysis (PCA) and t-Distributed\nStochastic Neighbor Embedding (t-SNE) to understand the distribution of data in\nthe feature sub-space. Our work will thereby assist the educators and students\nin obtaining a fruitful and efficient online learning experience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Batra_S/0/1/0/all/0/1\">Sarthak Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hewei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nag_A/0/1/0/all/0/1\">Avishek Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brodeur_P/0/1/0/all/0/1\">Philippe Brodeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Checkley_M/0/1/0/all/0/1\">Marianne Checkley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinkert_A/0/1/0/all/0/1\">Annette Klinkert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Soumyabrata Dev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WSSS4LUAD: Grand Challenge on Weakly-supervised Tissue Semantic Segmentation for Lung Adenocarcinoma. (arXiv:2204.06455v1 [eess.IV])","link":"http://arxiv.org/abs/2204.06455","description":"<p>Lung cancer is the leading cause of cancer death worldwide, and\nadenocarcinoma (LUAD) is the most common subtype. Exploiting the potential\nvalue of the histopathology images can promote precision medicine in oncology.\nTissue segmentation is the basic upstream task of histopathology image\nanalysis. Existing deep learning models have achieved superior segmentation\nperformance but require sufficient pixel-level annotations, which is\ntime-consuming and expensive. To enrich the label resources of LUAD and to\nalleviate the annotation efforts, we organize this challenge WSSS4LUAD to call\nfor the outstanding weakly-supervised semantic segmentation techniques for\nhistopathology images of LUAD. Participants have to design the algorithm to\nsegment tumor epithelial, tumor-associated stroma and normal tissue with only\npatch-level labels. This challenge includes 10,091 patch-level annotations (the\ntraining set) and over 130 million labeled pixels (the validation and test\nsets), from 67 WSIs (47 from GDPH, 20 from TCGA). All the labels were generated\nby a pathologist-in-the-loop pipeline with the help of AI models and checked by\nthe label review board. Among 532 registrations, 28 teams submitted the results\nin the test phase with over 1,000 submissions. Finally, the first place team\nachieved mIoU of 0.8413 (tumor: 0.8389, stroma: 0.7931, normal: 0.8919).\nAccording to the technical reports of the top-tier teams, CAM is still the most\npopular approach in WSSS. Cutmix data augmentation has been widely adopted to\ngenerate more reliable samples. With the success of this challenge, we believe\nthat WSSS approaches with patch-level annotations can replace the traditional\npixel annotations while reducing the annotation efforts. The entire dataset has\nbeen released to encourage more researches on computational pathology in LUAD\nand more novel WSSS techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Chu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_L/0/1/0/all/0/1\">Lixu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Huan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bingbing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_S/0/1/0/all/0/1\">Su Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_S/0/1/0/all/0/1\">Shanshan Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Jinhai Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jiatai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zeyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhizhen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yumeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1\">Chunhui Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_L/0/1/0/all/0/1\">Lijian Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_L/0/1/0/all/0/1\">Luwen Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Jingsong Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dong Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Z/0/1/0/all/0/1\">Zijie Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongbing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_Y/0/1/0/all/0/1\">Yiwen Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yiduo Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haiming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Y/0/1/0/all/0/1\">Yanfen Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_G/0/1/0/all/0/1\">Guoqiang Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huihua Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chunming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenbing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_C/0/1/0/all/0/1\">Changhong Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingling Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zaiyi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpoofGAN: Synthetic Fingerprint Spoof Images. (arXiv:2204.06498v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06498","description":"<p>A major limitation to advances in fingerprint spoof detection is the lack of\npublicly available, large-scale fingerprint spoof datasets, a problem which has\nbeen compounded by increased concerns surrounding privacy and security of\nbiometric data. Furthermore, most state-of-the-art spoof detection algorithms\nrely on deep networks which perform best in the presence of a large amount of\ntraining data. This work aims to demonstrate the utility of synthetic (both\nlive and spoof) fingerprints in supplying these algorithms with sufficient data\nto improve the performance of fingerprint spoof detection algorithms beyond the\ncapabilities when training on a limited amount of publicly available real\ndatasets. First, we provide details of our approach in modifying a\nstate-of-the-art generative architecture to synthesize high quality live and\nspoof fingerprints. Then, we provide quantitative and qualitative analysis to\nverify the quality of our synthetic fingerprints in mimicking the distribution\nof real data samples. We showcase the utility of our synthetic live and spoof\nfingerprints in training a deep network for fingerprint spoof detection, which\ndramatically boosts the performance across three different evaluation datasets\ncompared to an identical model trained on real data alone. Finally, we\ndemonstrate that only 25% of the original (real) dataset is required to obtain\nsimilar detection performance when augmenting the training dataset with\nsynthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grosz_S/0/1/0/all/0/1\">Steven A. Grosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization. (arXiv:2204.06504v1 [cs.GR])","link":"http://arxiv.org/abs/2204.06504","description":"<p>Since 2016, we have witnessed the tremendous growth of artificial\nintelligence+visualization (AI+VIS) research. However, existing survey papers\non AI+VIS focus on visual analytics and information visualization, not\nscientific visualization (SciVis). In this paper, we survey related deep\nlearning (DL) works in SciVis, specifically in the direction of DL4SciVis:\ndesigning DL solutions for solving SciVis problems. To stay focused, we\nprimarily consider works that handle scalar and vector field data but exclude\nmesh data. We classify and discuss these works along six dimensions: domain\nsetting, research task, learning type, network architecture, loss function, and\nevaluation metric. The paper concludes with a discussion of the remaining gaps\nto fill along the discussed dimensions and the grand challenges we need to\ntackle as a community. This state-of-the-art survey guides SciVis researchers\nin gaining an overview of this emerging topic and points out future directions\nto grow this research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jun Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-distribution Detection with Deep Nearest Neighbors. (arXiv:2204.06507v1 [cs.LG])","link":"http://arxiv.org/abs/2204.06507","description":"<p>Out-of-distribution (OOD) detection is a critical task for deploying machine\nlearning models in the open world. Distance-based methods have demonstrated\npromise, where testing samples are detected as OOD if they are relatively far\naway from in-distribution (ID) data. However, prior methods impose a strong\ndistributional assumption of the underlying feature space, which may not always\nhold. In this paper, we explore the efficacy of non-parametric nearest-neighbor\ndistance for OOD detection, which has been largely overlooked in the\nliterature. Unlike prior works, our method does not impose any distributional\nassumption, hence providing stronger flexibility and generality. We demonstrate\nthe effectiveness of nearest-neighbor-based OOD detection on several benchmarks\nand establish superior performance. Under the same model trained on\nImageNet-1k, our method substantially reduces the false positive rate\n(FPR@TPR95) by 24.77% compared to a strong baseline SSD+, which uses a\nparametric approach Mahalanobis distance in detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiyou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yifei Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaojin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does depth estimation help object detection?. (arXiv:2204.06512v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06512","description":"<p>Ground-truth depth, when combined with color data, helps improve object\ndetection accuracy over baseline models that only use color. However, estimated\ndepth does not always yield improvements. Many factors affect the performance\nof object detection when estimated depth is used. In this paper, we\ncomprehensively investigate these factors with detailed experiments, such as\nusing ground-truth vs. estimated depth, effects of different state-of-the-art\ndepth estimation networks, effects of using different indoor and outdoor RGB-D\ndatasets as training data for depth estimation, and different architectural\nchoices for integrating depth to the base object detector network. We propose\nan early concatenation strategy of depth, which yields higher mAP than previous\nworks' while using significantly fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cetinkaya_B/0/1/0/all/0/1\">Bedrettin Cetinkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1\">Sinan Kalkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A9-Dataset: Multi-Sensor Infrastructure-Based Dataset for Mobility Research. (arXiv:2204.06527v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06527","description":"<p>Data-intensive machine learning based techniques increasingly play a\nprominent role in the development of future mobility solutions - from driver\nassistance and automation functions in vehicles, to real-time traffic\nmanagement systems realized through dedicated infrastructure. The availability\nof high quality real-world data is often an important prerequisite for the\ndevelopment and reliable deployment of such systems in large scale. Towards\nthis endeavour, we present the A9-Dataset based on roadside sensor\ninfrastructure from the 3 km long Providentia++ test field near Munich in\nGermany. The dataset includes anonymized and precision-timestamped multi-modal\nsensor and object data in high resolution, covering a variety of traffic\nsituations. As part of the first set of data, which we describe in this paper,\nwe provide camera and LiDAR frames from two overhead gantry bridges on the A9\nautobahn with the corresponding objects labeled with 3D bounding boxes. The\nfirst set includes in total more than 1000 sensor frames and 14000 traffic\nobjects. The dataset is available for download at https://a9-dataset.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cress_C/0/1/0/all/0/1\">Christian Cre&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_W/0/1/0/all/0/1\">Walter Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strand_L/0/1/0/all/0/1\">Leah Strand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarasimhan_V/0/1/0/all/0/1\">Venkatnarayanan Lakshminarasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortkord_M/0/1/0/all/0/1\">Maximilian Fortkord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Siyi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Vector Fields for Surface Representation and Inference. (arXiv:2204.06552v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06552","description":"<p>Neural implicit fields have recently been shown to represent 3D shapes\naccurately, opening up various applications in 3D shape analysis. Up to now,\nsuch implicit fields for 3D representation are scalar, encoding the signed\ndistance or binary volume occupancy and more recently the unsigned distance.\nHowever, the first two can only represent closed shapes, while the unsigned\ndistance has difficulties in accurate and fast shape inference. In this paper,\nwe propose a Neural Vector Field for shape representation in order to overcome\nthe two aforementioned problems. Mapping each point in space to the direction\ntowards the closest surface, we can represent any type of shape. Similarly the\nshape mesh can be reconstructed by applying the marching cubes algorithm, with\nproposed small changes, on top of the inferred vector field. We compare the\nmethod on ShapeNet where the proposed new neural implicit field shows superior\naccuracy in representing both closed and open shapes outperforming previous\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rella_E/0/1/0/all/0/1\">Edoardo Mello Rella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Video Generation through Global and Local Motion Dynamics. (arXiv:2204.06558v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06558","description":"<p>We present GLASS, a method for Global and Local Action-driven Sequence\nSynthesis. GLASS is a generative model that is trained on video sequences in an\nunsupervised manner and that can animate an input image at test time. The\nmethod learns to segment frames into foreground-background layers and to\ngenerate transitions of the foregrounds over time through a global and local\naction representation. Global actions are explicitly related to 2D shifts,\nwhile local actions are instead related to (both geometric and photometric)\nlocal deformations. GLASS uses a recurrent neural network to transition between\nframes and is trained through a reconstruction loss. We also introduce\nW-Sprites (Walking Sprites), a novel synthetic dataset with a predefined action\nspace. We evaluate our method on both W-Sprites and real datasets, and find\nthat GLASS is able to generate realistic video sequences from a single input\nimage and to successfully learn a more advanced action space than in prior\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davtyan_A/0/1/0/all/0/1\">Aram Davtyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaCL: Class-aware Codebook Learning for Weakly Supervised Segmentation on Diffuse Image Patterns. (arXiv:2011.00794v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.00794","description":"<p>Weakly supervised learning has been rapidly advanced in biomedical image\nanalysis to achieve pixel-wise labels (segmentation) from image-wise\nannotations (classification), as biomedical images naturally contain image-wise\nlabels in many scenarios. The current weakly supervised learning algorithms\nfrom the computer vision community are largely designed for focal objects\n(e.g., dogs and cats). However, such algorithms are not optimized for diffuse\npatterns in biomedical imaging (e.g., stains and fluorescence in microscopy\nimaging). In this paper, we propose a novel class-aware codebook learning\n(CaCL) algorithm to perform weakly supervised learning for diffuse image\npatterns. Specifically, the CaCL algorithm is deployed to segment protein\nexpressed brush border regions from histological images of human duodenum. Our\ncontribution is three-fold: (1) we approach the weakly supervised segmentation\nfrom a novel codebook learning perspective; (2) the CaCL algorithm segments\ndiffuse image patterns rather than focal objects; and (3) the proposed\nalgorithm is implemented in a multi-task framework based on Vector\nQuantised-Variational AutoEncoder (VQ-VAE) via joint image reconstruction,\nclassification, feature embedding, and segmentation. The experimental results\nshow that our method achieved superior performance compared with baseline\nweakly supervised algorithms. The code is available at\nhttps://github.com/ddrrnn123/CaCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shunxing Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Aadarsh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Catie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millis_B/0/1/0/all/0/1\">Bryan A. Millis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyska_M/0/1/0/all/0/1\">Matthew J. Tyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Outliers with Foreign Patch Interpolation. (arXiv:2011.04197v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.04197","description":"<p>In medical imaging, outliers can contain hypo/hyper-intensities, minor\ndeformations, or completely altered anatomy. To detect these irregularities it\nis helpful to learn the features present in both normal and abnormal images.\nHowever this is difficult because of the wide range of possible abnormalities\nand also the number of ways that normal anatomy can vary naturally. As such, we\nleverage the natural variations in normal anatomy to create a range of\nsynthetic abnormalities. Specifically, the same patch region is extracted from\ntwo independent samples and replaced with an interpolation between both\npatches. The interpolation factor, patch size, and patch location are randomly\nsampled from uniform distributions. A wide residual encoder decoder is trained\nto give a pixel-wise prediction of the patch and its interpolation factor. This\nencourages the network to learn what features to expect normally and to\nidentify where foreign patterns have been introduced. The estimate of the\ninterpolation factor lends itself nicely to the derivation of an outlier score.\nMeanwhile the pixel-wise output allows for pixel- and subject- level\npredictions using the same model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jeremy Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Benjamin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batten_J/0/1/0/all/0/1\">James Batten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huaqi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FPCC: Fast Point Cloud Clustering based Instance Segmentation for Industrial Bin-picking. (arXiv:2012.14618v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.14618","description":"<p>Instance segmentation is an important pre-processing task in numerous\nreal-world applications, such as robotics, autonomous vehicles, and\nhuman-computer interaction. Compared with the rapid development of deep\nlearning for two-dimensional (2D) image tasks, deep learning-based instance\nsegmentation of 3D point cloud still has a lot of room for development. In\nparticular, distinguishing a large number of occluded objects of the same class\nis a highly challenging problem, which is seen in a robotic bin-picking. In a\nusual bin-picking scene, many identical objects are stacked together and the\nmodel of the objects is known. Thus, the semantic information can be ignored;\ninstead, the focus in the bin-picking is put on the segmentation of instances.\nBased on this task requirement, we propose a Fast Point Cloud Clustering (FPCC)\nfor instance segmentation of bin-picking scene. FPCC includes a network named\nFPCC-Net and a fast clustering algorithm. FPCC-net has two subnets, one for\ninferring the geometric centers for clustering and the other for describing\nfeatures of each point. FPCC-Net extracts features of each point and infers\ngeometric center points of each instance simultaneously. After that, the\nproposed clustering algorithm clusters the remaining points to the closest\ngeometric center in feature embedding space. Experiments show that FPCC also\nsurpasses the existing works in bin-picking scenes and is more computationally\nefficient. Our code and data are available at https://github.com/xyjbaal/FPCC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yajun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arai_S/0/1/0/all/0/1\">Shogo Arai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Diyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangzhou Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosuge_K/0/1/0/all/0/1\">Kazuhiro Kosuge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Object Detection via Integrity Learning. (arXiv:2101.07663v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07663","description":"<p>Although current salient object detection (SOD) works have achieved\nsignificant progress, they are limited when it comes to the integrity of the\npredicted salient regions. We define the concept of integrity at both a micro\nand macro level. Specifically, at the micro level, the model should highlight\nall parts that belong to a certain salient object. Meanwhile, at the macro\nlevel, the model needs to discover all salient objects in a given image. To\nfacilitate integrity learning for SOD, we design a novel Integrity Cognition\nNetwork (ICON), which explores three important components for learning strong\nintegrity features. 1) Unlike existing models, which focus more on feature\ndiscriminability, we introduce a diverse feature aggregation (DFA) component to\naggregate features with various receptive fields (i.e., kernel shape and\ncontext) and increase feature diversity. Such diversity is the foundation for\nmining the integral salient objects. 2) Based on the DFA features, we introduce\nan integrity channel enhancement (ICE) component with the goal of enhancing\nfeature channels that highlight the integral salient objects, while suppressing\nthe other distracting ones. 3) After extracting the enhanced features, the\npart-whole verification (PWV) method is employed to determine whether the part\nand whole object features have strong agreement. Such part-whole agreements can\nfurther improve the micro-level integrity for each salient object. To\ndemonstrate the effectiveness of our ICON, comprehensive experiments are\nconducted on seven challenging benchmarks. Our ICON outperforms the baseline\nmethods in terms of a wide range of metrics. Notably, our ICON achieves about\n10% relative improvement over the previous best model in terms of average false\nnegative ratio (FNR), on six datasets. Codes and results are available at:\nhttps://github.com/mczhuge/ICON.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingchen Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly But Deeply Supervised Occlusion-Reasoned Parametric Road Layouts. (arXiv:2104.06730v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06730","description":"<p>We propose an end-to-end network that takes a single perspective RGB image of\na complex road scene as input, to produce occlusion-reasoned layouts in\nperspective space as well as a parametric bird's-eye-view (BEV) space. In\ncontrast to prior works that require dense supervision such as semantic labels\nin perspective view, our method only requires human annotations for parametric\nattributes that are cheaper and less ambiguous to obtain. To solve this\nchallenging task, our design is comprised of modules that incorporate inductive\nbiases to learn occlusion-reasoning, geometric transformation and semantic\nabstraction, where each module may be supervised by appropriately transforming\nthe parametric annotations. We demonstrate how our design choices and proposed\ndeep supervision help achieve meaningful representations and accurate\npredictions. We validate our approach on two public datasets, KITTI and\nNuScenes, to achieve state-of-the-art results with considerably less human\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Buyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bingbing Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images. (arXiv:2104.12137v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12137","description":"<p>The fully convolutional network (FCN) with an encoder-decoder architecture\nhas been the standard paradigm for semantic segmentation. The encoder-decoder\narchitecture utilizes an encoder to capture multilevel feature maps, which are\nincorporated into the final prediction by a decoder. As the context is crucial\nfor precise segmentation, tremendous effort has been made to extract such\ninformation in an intelligent fashion, including employing dilated/atrous\nconvolutions or inserting attention modules. However, these endeavors are all\nbased on the FCN architecture with ResNet or other backbones, which cannot\nfully exploit the context from the theoretical concept. By contrast, we\nintroduce the Swin Transformer as the backbone to extract the context\ninformation and design a novel decoder of densely connected feature aggregation\nmodule (DCFAM) to restore the resolution and produce the segmentation map. The\nexperimental results on two remotely sensed semantic segmentation datasets\ndemonstrate the effectiveness of the proposed scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shenghui Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-Augmented Feature Pyramid Network with Light Linear Spatial Transformers for Object Detection. (arXiv:2105.09464v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09464","description":"<p>As one of the prevalent components, Feature Pyramid Network (FPN) is widely\nused in current object detection models for improving multi-scale object\ndetection performance. However, its feature fusion mode is still in a\nmisaligned and local manner, thus limiting the representation power. To address\nthe inherit defects of FPN, a novel architecture termed Content-Augmented\nFeature Pyramid Network (CA-FPN) is proposed in this paper. Firstly, a Global\nContent Extraction Module (GCEM) is proposed to extract multi-scale context\ninformation. Secondly, lightweight linear spatial Transformer connections are\nadded in the top-down pathway to augment each feature map with multi-scale\nfeatures, where a linearized approximate self-attention function is designed\nfor reducing model complexity. By means of the self-attention mechanism in\nTransformer, there is no longer need to align feature maps during feature\nfusion, thus solving the misaligned defect. By setting the query scope to the\nentire feature map, the local defect can also be solved. Extensive experiments\non COCO and PASCAL VOC datasets demonstrated that our CA-FPN outperforms other\nFPN-based detectors without bells and whistles and is robust in different\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuncong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Synchronized Reprojection-based Model for 3D Human Pose Estimation. (arXiv:2106.04274v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04274","description":"<p>3D human pose estimation is still a challenging problem despite the large\namount of work that has been done in this field. Generally, most methods\ndirectly use neural networks and ignore certain constraints (e.g., reprojection\nconstraints and joint angle and bone length constraints). This paper proposes a\nweakly supervised GAN-based model for 3D human pose estimation that considers\n3D information along with 2D information simultaneously, in which a\nreprojection network is employed to learn the mapping of the distribution from\n3D poses to 2D poses. In particular, we train the reprojection network and the\ngenerative adversarial network synchronously. Furthermore, inspired by the\ntypical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix,\nwhich is added into the discriminator's input to impose joint angle and bone\nlength constraints. The experimental results on Human3.6M show that our method\noutperforms state-of-the-art methods by approximately 24.7\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yicheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yongqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiahui Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistency. (arXiv:2106.05616v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05616","description":"<p>Recovering 3D human pose from 2D joints is still a challenging problem,\nespecially without any 3D annotation, video information, or multi-view\ninformation. In this paper, we present an unsupervised GAN-based model\nconsisting of multiple weight-sharing generators to estimate a 3D human pose\nfrom a single image without 3D annotations. In our model, we introduce\nsingle-view-multi-angle consistency (SVMAC) to significantly improve the\nestimation performance. With 2D joint locations as input, our model estimates a\n3D pose and a camera simultaneously. During training, the estimated 3D pose is\nrotated by random angles and the estimated camera projects the rotated 3D poses\nback to 2D. The 2D reprojections will be fed into weight-sharing generators to\nestimate the corresponding 3D poses and cameras, which are then mixed to impose\nSVMAC constraints to self-supervise the training process. The experimental\nresults show that our method outperforms the state-of-the-art unsupervised\nmethods on Human 3.6M and MPI-INF-3DHP. Moreover, qualitative results on MPII\nand LSP show that our method can generalize well to unknown data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yicheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiahui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yongqi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts. (arXiv:2109.04379v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04379","description":"<p>Preserving maximal information is one of principles of designing\nself-supervised learning methodologies. To reach this goal, contrastive\nlearning adopts an implicit way which is contrasting image pairs. However, we\nbelieve it is not fully optimal to simply use the contrastive estimation for\npreservation. Moreover, it is necessary and complemental to introduce an\nexplicit solution to preserve more information. From this perspective, we\nintroduce Preservational Learning to reconstruct diverse image contexts in\norder to preserve more information in learned representations. Together with\nthe contrastive loss, we present Preservational Contrastive Representation\nLearning (PCRL) for learning self-supervised medical representations. PCRL\nprovides very competitive results under the pretraining-finetuning protocol,\noutperforming both self-supervised and supervised counterparts in 5\nclassification/segmentation tasks substantially.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chixiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sibei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNetFormer: An UNet-like Transformer for Efficient Semantic Segmentation of Remote Sensing Urban Scene Imagery. (arXiv:2109.08937v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08937","description":"<p>Semantic segmentation of remotely sensed urban scene images is required in a\nwide range of practical applications, such as land cover mapping, urban change\ndetection, environmental protection, and economic assessment. Driven by rapid\ndevelopments in deep learning technologies, the convolutional neural network\n(CNN) has dominated semantic segmentation for many years. CNN adopts\nhierarchical feature representation, demonstrating strong capabilities for\nlocal information extraction. However, the local property of the convolution\nlayer limits the network from capturing global context. Recently, as a hot\ntopic in the domain of computer vision, Transformer has demonstrated its great\npotential in global information modelling, boosting many vision-related tasks\nsuch as image classification, object detection, and particularly semantic\nsegmentation. In this paper, we propose an UNet-like Transformer (UNetFormer)\nfor real-time urban scene segmentation. The novel UNetFormer adopts a hybrid\nstructure with a CNN-based encoder and a Transformer-based decoder, learning\nglobal-local context with high computational efficiency. Extensive experiments\nreveal that the proposed UNetFormer not only runs faster during the inference\nstage but also produces higher accuracy compared with state-of-the-art\nlightweight models. Specifically, the proposed UNetFormer achieved a 67.8% mIoU\non the UAVid test set and a 52.4% mIoU on the LoveDA dataset, while the\ninference speed can achieve up to 322.4 FPS speed with the input in the shape\nof 512x512 on an NVIDIA GTX 3090 GPU. The source code will be freely available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shenghui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkinson_P/0/1/0/all/0/1\">Peter M. Atkinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PC$^2$-PU: Patch Correlation and Point Correlation for Effective Point Cloud Upsampling. (arXiv:2109.09337v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09337","description":"<p>Point cloud upsampling is to densify a sparse point set acquired from 3D\nsensors, providing a denser representation for the underlying surface. Existing\nmethods divide the input points into small patches and upsample each patch\nseparately, however, ignoring the global spatial consistency between patches.\nIn this paper, we present a novel method PC$^2$-PU, which explores\npatch-to-patch and point-to-point correlations for more effective and robust\npoint cloud upsampling. Specifically, our network has two appealing designs:\n(i) We take adjacent patches as supplementary inputs to compensate the loss\nstructure information within a single patch and introduce a Patch Correlation\nModule to capture the difference and similarity between patches. (ii) After\naugmenting each patch's geometry, we further introduce a Point Correlation\nModule to reveal the relationship of points inside each patch to maintain the\nlocal spatial consistency. Extensive experiments on both synthetic and real\nscanned datasets demonstrate that our method surpasses previous upsampling\nmethods, particularly with the noisy inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chen Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bisheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomy and Perception for Space Mining. (arXiv:2109.12109v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.12109","description":"<p>Future Moon bases will likely be constructed using resources mined from the\nsurface of the Moon. The difficulty of maintaining a human workforce on the\nMoon and communications lag with Earth means that mining will need to be\nconducted using collaborative robots with a high degree of autonomy. In this\npaper, we describe our solution for Phase 2 of the NASA Space Robotics\nChallenge, which provided a simulated lunar environment in which teams were\ntasked to develop software systems to achieve autonomous collaborative robots\nfor mining on the Moon. Our 3rd place and innovation award winning solution\nshows how machine learning-enabled vision could alleviate major challenges\nposed by the lunar environment towards autonomous space mining, chiefly the\nlack of satellite positioning systems, hazardous terrain, and delicate robot\ninteractions. A robust multi-robot coordinator was also developed to achieve\nlong-term operation and effective collaboration between robots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1\">Ragav Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammond_R/0/1/0/all/0/1\">Ravi Hammond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bockman_J/0/1/0/all/0/1\">James Bockman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arthur_A/0/1/0/all/0/1\">Alec Arthur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smart_B/0/1/0/all/0/1\">Brandon Smart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craggs_D/0/1/0/all/0/1\">Dustin Craggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1\">Anh-Dzung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowntree_T/0/1/0/all/0/1\">Thomas Rowntree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutz_E/0/1/0/all/0/1\">Elijah Schutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orenstein_A/0/1/0/all/0/1\">Adrian Orenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Andy Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays. (arXiv:2109.14187v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.14187","description":"<p>Deep learning has shown recent success in classifying anomalies in chest\nx-rays, but datasets are still small compared to natural image datasets.\nSupervision of abnormality localization has been shown to improve trained\nmodels, partially compensating for dataset sizes. However, explicitly labeling\nthese anomalies requires an expert and is very time-consuming. We propose a\npotentially scalable method for collecting implicit localization data using an\neye tracker to capture gaze locations and a microphone to capture a dictation\nof a report, imitating the setup of a reading room. The resulting REFLACX\n(Reports and Eye-Tracking Data for Localization of Abnormalities in Chest\nX-rays) dataset was labeled across five radiologists and contains 3,032\nsynchronized sets of eye-tracking data and timestamped report transcriptions\nfor 2,616 chest x-rays from the MIMIC-CXR dataset. We also provide auxiliary\nannotations, including bounding boxes around lungs and heart and validation\nlabels consisting of ellipses localizing abnormalities and image-level labels.\nFurthermore, a small subset of the data contains readings from all\nradiologists, allowing for the calculation of inter-rater scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lanfredi_R/0/1/0/all/0/1\">Ricardo Bigolin Lanfredi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Auffermann_W/0/1/0/all/0/1\">William F. Auffermann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_J/0/1/0/all/0/1\">Jessica Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_P/0/1/0/all/0/1\">Phuong-Anh T. Duong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drew_T/0/1/0/all/0/1\">Trafton Drew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schroeder_J/0/1/0/all/0/1\">Joyce D. Schroeder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Semantic Segmentation via Regional Contrastive Consistency Regularization. (arXiv:2110.05170v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05170","description":"<p>Unsupervised domain adaptation (UDA) for semantic segmentation has been\nwell-studied in recent years. However, most existing works largely neglect the\nlocal regional consistency across different domains and are less robust to\nchanges in outdoor environments. In this paper, we propose a novel and fully\nend-to-end trainable approach, called regional contrastive consistency\nregularization (RCCR) for domain adaptive semantic segmentation. Our core idea\nis to pull the similar regional features extracted from the same location of\ndifferent images, i.e., the original image and augmented image, to be closer,\nand meanwhile push the features from the different locations of the two images\nto be separated. We innovatively propose a region-wise contrastive loss with\ntwo sampling strategies to realize effective regional consistency. Besides, we\npresent momentum projection heads, where the teacher projection head is the\nexponential moving average of the student. Finally, a memory bank mechanism is\ndesigned to learn more robust and stable region-wise features under varying\nenvironments. Extensive experiments on two common UDA benchmarks, i.e., GTAV to\nCityscapes and SYNTHIA to Cityscapes, demonstrate that our approach outperforms\nthe state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1\">Chuyun Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Set Recognition: a Good Closed-Set Classifier is All You Need?. (arXiv:2110.06207v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06207","description":"<p>The ability to identify whether or not a test sample belongs to one of the\nsemantic classes in a classifier's training set is critical to practical\ndeployment of the model. This task is termed open-set recognition (OSR) and has\nreceived significant attention in recent years. In this paper, we first\ndemonstrate that the ability of a classifier to make the 'none-of-above'\ndecision is highly correlated with its accuracy on the closed-set classes. We\nfind that this relationship holds across loss objectives and architectures, and\nfurther demonstrate the trend both on the standard OSR benchmarks as well as on\na large-scale ImageNet evaluation. Second, we use this correlation to boost the\nperformance of a maximum logit score OSR 'baseline' by improving its closed-set\naccuracy, and with this strong baseline achieve state-of-the-art on a number of\nOSR benchmarks. Similarly, we boost the performance of the existing\nstate-of-the-art method by improving its closed-set accuracy, but the resulting\ndiscrepancy with the strong baseline is marginal. Our third contribution is to\npresent the 'Semantic Shift Benchmark' (SSB), which better respects the task of\ndetecting semantic novelty, in contrast to other forms of distribution shift\nalso considered in related sub-fields, such as out-of-distribution detection.\nOn this new evaluation, we again demonstrate that there is negligible\ndifference between the strong baseline and the existing state-of-the-art.\nProject Page: https://www.robots.ox.ac.uk/~vgg/research/osr/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaze_S/0/1/0/all/0/1\">Sagar Vaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QU-net++: Image Quality Detection Framework for Segmentation of Medical 3D Image Stacks. (arXiv:2110.14181v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.14181","description":"<p>Automated segmentation of pathological regions of interest aids medical image\ndiagnostics and follow-up care. However, accurate pathological segmentations\nrequire high quality of annotated data that can be both cost and time intensive\nto generate. In this work, we propose an automated two-step method that detects\na minimal image subset required to train segmentation models by evaluating the\nquality of medical images from 3D image stacks using a U-net++ model. These\nimages that represent a lack of quality training can then be annotated and used\nto fully train a U-net-based segmentation model. The proposed QU-net++ model\ndetects this lack of quality training based on the disagreement in\nsegmentations produced from the final two output layers. The proposed model\nisolates around 10% of the slices per 3D image stack and can scale across\nimaging modalities to segment cysts in OCT images and ground glass opacity\n(GGO) in lung CT images with Dice scores in the range 0.56-0.72. Thus, the\nproposed method can be applied for cost effective multi-modal pathology\nsegmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sohini Roychowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks. (arXiv:2111.05955v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.05955","description":"<p>Spiking neural networks (SNNs) have become an interesting alternative to\nconventional artificial neural networks (ANN) thanks to their temporal\nprocessing capabilities and energy efficient implementations in neuromorphic\nhardware. However the challenges involved in training SNNs have limited their\nperformance in terms of accuracy and thus their applications. Improving\nlearning algorithms and neural architectures for a more accurate feature\nextraction is therefore one of the current priorities in SNN research. In this\npaper we present a study on the key components of modern spiking architectures.\nWe empirically compare different techniques in image classification datasets\ntaken from the best performing networks. We design a spiking version of the\nsuccessful residual network architecture and provide an in-depth study on the\npossible implementations of spiking residual connections. Our results provide a\nstate of the art guide to SNN design, which allows to make informed choices\nwhen trying to build the optimal visual feature extractor. Finally, our network\noutperforms previous SNN architectures in CIFAR-10 (94.14%) and CIFAR-100\n(74.65%) datasets and matches the state of the art in DVS-CIFAR10 (72.98%),\nwith less parameters than the previous state of the art and without the need\nfor ANN-SNN conversion. Code available at\nhttps://github.com/VicenteAlex/Spiking_ResNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vicente_Sola_A/0/1/0/all/0/1\">Alex Vicente-Sola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manna_D/0/1/0/all/0/1\">Davide L. Manna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirkland_P/0/1/0/all/0/1\">Paul Kirkland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caterina_G/0/1/0/all/0/1\">Gaetano Di Caterina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bihl_T/0/1/0/all/0/1\">Trevor Bihl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Related Work on Image Quality Assessment. (arXiv:2111.06291v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.06291","description":"<p>Due to the existence of quality degradations introduced in various stages of\nvisual signal acquisition, compression, transmission and display, image quality\nassessment (IQA) plays a vital role in image-based applications. According to\nwhether the reference image is complete and available, image quality evaluation\ncan be divided into three categories: Full-Reference(FR), Reduced-\nReference(RR), and Non- Reference(NR). This article will review the\nstate-of-the-art image quality assessment algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dongxu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Lightweight Graph Transformer Network for Human Mesh Reconstruction from 2D Human Pose. (arXiv:2111.12696v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12696","description":"<p>Existing deep learning-based human mesh reconstruction approaches have a\ntendency to build larger networks in order to achieve higher accuracy.\nComputational complexity and model size are often neglected, despite being key\ncharacteristics for practical use of human mesh reconstruction models (e.g.\nvirtual try-on systems). In this paper, we present GTRS, a lightweight\npose-based method that can reconstruct human mesh from 2D human pose. We\npropose a pose analysis module that uses graph transformers to exploit\nstructured and implicit joint correlations, and a mesh regression module that\ncombines the extracted pose feature with the mesh template to reconstruct the\nfinal human mesh. We demonstrate the efficiency and generalization of GTRS by\nextensive evaluations on the Human3.6M and 3DPW datasets. In particular, GTRS\nachieves better accuracy than the SOTA pose-based method Pose2Mesh while only\nusing 10.2% of the parameters (Params) and 2.5% of the FLOPs on the challenging\nin-the-wild 3DPW dataset. Code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Ce Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1\">Matias Mendieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Aidong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shunted Self-Attention via Multi-Scale Token Aggregation. (arXiv:2111.15193v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15193","description":"<p>Recent Vision Transformer~(ViT) models have demonstrated encouraging results\nacross various computer vision tasks, thanks to their competence in modeling\nlong-range dependencies of image patches or tokens via self-attention. These\nmodels, however, usually designate the similar receptive fields of each token\nfeature within each layer. Such a constraint inevitably limits the ability of\neach self-attention layer in capturing multi-scale features, thereby leading to\nperformance degradation in handling images with multiple objects of different\nscales. To address this issue, we propose a novel and generic strategy, termed\nshunted self-attention~(SSA), that allows ViTs to model the attentions at\nhybrid scales per attention layer. The key idea of SSA is to inject\nheterogeneous receptive field sizes into tokens: before computing the\nself-attention matrix, it selectively merges tokens to represent larger object\nfeatures while keeping certain tokens to preserve fine-grained features. This\nnovel merging scheme enables the self-attention to learn relationships between\nobjects with different sizes and simultaneously reduces the token numbers and\nthe computational cost. Extensive experiments across various tasks demonstrate\nthe superiority of SSA. Specifically, the SSA-based transformer achieves 84.0\\%\nTop-1 accuracy and outperforms the state-of-the-art Focal Transformer on\nImageNet with only half of the model size and computation cost, and surpasses\nFocal Transformer by 1.3 mAP on COCO and 2.9 mIOU on ADE20K under similar\nparameter and computation cost. Code has been released at\nhttps://github.com/OliverRensu/Shunted-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Daquan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building extraction with vision transformer. (arXiv:2111.15637v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15637","description":"<p>As an important carrier of human productive activities, the extraction of\nbuildings is not only essential for urban dynamic monitoring but also necessary\nfor suburban construction inspection. Nowadays, accurate building extraction\nfrom remote sensing images remains a challenge due to the complex background\nand diverse appearances of buildings. The convolutional neural network (CNN)\nbased building extraction methods, although increased the accuracy\nsignificantly, are criticized for their inability for modelling global\ndependencies. Thus, this paper applies the Vision Transformer for building\nextraction. However, the actual utilization of the Vision Transformer often\ncomes with two limitations. First, the Vision Transformer requires more GPU\nmemory and computational costs compared to CNNs. This limitation is further\nmagnified when encountering large-sized inputs like fine-resolution remote\nsensing images. Second, spatial details are not sufficiently preserved during\nthe feature extraction of the Vision Transformer, resulting in the inability\nfor fine-grained building segmentation. To handle these issues, we propose a\nnovel Vision Transformer (BuildFormer), with a dual-path structure.\nSpecifically, we design a spatial-detailed context path to encode rich spatial\ndetails and a global context path to capture global dependencies. Besides, we\ndevelop a window-based linear multi-head self-attention to make the complexity\nof the multi-head self-attention linear with the window size, which strengthens\nthe global context extraction by using large windows and greatly improves the\npotential of the Vision Transformer in processing large-sized remote sensing\nimages. The proposed method yields state-of-the-art performance (75.74% IoU) on\nthe Massachusetts building dataset. Code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shenghui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Exponentially Tilted Gaussian Prior for Variational Autoencoders. (arXiv:2111.15646v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.15646","description":"<p>An important property for deep neural networks is the ability to perform\nrobust out-of-distribution detection on previously unseen data. This property\nis essential for safety purposes when deploying models for real world\napplications. Recent studies show that probabilistic generative models can\nperform poorly on this task, which is surprising given that they seek to\nestimate the likelihood of training data. To alleviate this issue, we propose\nthe exponentially tilted Gaussian prior distribution for the Variational\nAutoencoder (VAE) which pulls points onto the surface of a hyper-sphere in\nlatent space. This achieves state-of-the art results on the area under the\ncurve-receiver operator characteristics metric using just the log-likelihood\nthat the VAE naturally assigns. Because this prior is a simple modification of\nthe traditional VAE prior, it is faster and easier to implement than\ncompetitive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Floto_G/0/1/0/all/0/1\">Griffin Floto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kremer_S/0/1/0/all/0/1\">Stefan Kremer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nica_M/0/1/0/all/0/1\">Mihai Nica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consensus Graph Representation Learning for Better Grounded Image Captioning. (arXiv:2112.00974v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00974","description":"<p>The contemporary visual captioning models frequently hallucinate objects that\nare not actually in a scene, due to the visual misclassification or\nover-reliance on priors that resulting in the semantic inconsistency between\nthe visual information and the target lexical words. The most common way is to\nencourage the captioning model to dynamically link generated object words or\nphrases to appropriate regions of the image, i.e., the grounded image\ncaptioning (GIC). However, GIC utilizes an auxiliary task (grounding objects)\nthat has not solved the key issue of object hallucination, i.e., the semantic\ninconsistency. In this paper, we take a novel perspective on the issue above -\nexploiting the semantic coherency between the visual and language modalities.\nSpecifically, we propose the Consensus Rraph Representation Learning framework\n(CGRL) for GIC that incorporates a consensus representation into the grounded\ncaptioning pipeline. The consensus is learned by aligning the visual graph\n(e.g., scene graph) to the language graph that consider both the nodes and\nedges in a graph. With the aligned consensus, the captioning model can capture\nboth the correct linguistic characteristics and visual relevance, and then\ngrounding appropriate image regions further. We validate the effectiveness of\nour model, with a significant decline in object hallucination (-9% CHAIRi) on\nthe Flickr30k Entities dataset. Besides, our CGRL also evaluated by several\nautomatic metrics and human evaluation, the results indicate that the proposed\napproach can simultaneously improve the performance of image captioning (+2.9\nCider) and grounding (+2.3 F1LOC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Detect Every Thing in an Open World. (arXiv:2112.01698v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01698","description":"<p>Many open-world applications require the detection of novel objects, yet\nstate-of-the-art object detection and instance segmentation networks do not\nexcel at this task. The key issue lies in their assumption that regions without\nany annotations should be suppressed as negatives, which teaches the model to\ntreat the unannotated objects as background. To address this issue, we propose\na simple yet surprisingly powerful data augmentation and training scheme we\ncall Learning to Detect Every Thing (LDET). To avoid suppressing hidden\nobjects, background objects that are visible but unlabeled, we paste annotated\nobjects on a background image sampled from a small region of the original\nimage. Since training solely on such synthetically-augmented images suffers\nfrom domain shift, we decouple the training into two parts: 1) training the\nregion classification and regression head on augmented images, and 2)~training\nthe mask heads on original images. In this way, a model does not learn to\nclassify hidden objects as background while generalizing well to real images.\nLDET leads to significant improvements on many datasets in the open-world\ninstance segmentation task, outperforming baselines on cross-category\ngeneralization on COCO, as well as cross-dataset evaluation on UVO and\nCityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1\">Kuniaki Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Ping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Modality-Aware Multiple Granularity Pre-Training for RGB-Infrared Person Re-Identification. (arXiv:2112.06147v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06147","description":"<p>RGB-Infrared person re-identification (RGB-IR ReID) aims to associate people\nacross disjoint RGB and IR camera views. Currently, state-of-the-art\nperformance of RGB-IR ReID is not as impressive as that of conventional ReID.\nMuch of that is due to the notorious modality bias training issue brought by\nthe single-modality ImageNet pre-training, which might yield RGB-biased\nrepresentations that severely hinder the cross-modality image retrieval. This\npaper makes first attempt to tackle the task from a pre-training perspective.\nWe propose a self-supervised pre-training solution, named Modality-Aware\nMultiple Granularity Learning (MMGL), which directly trains models from scratch\nonly on multi-modal ReID datasets, but achieving competitive results against\nImageNet pre-training, without using any external data or sophisticated tuning\ntricks. First, we develop a simple-but-effective 'permutation recovery' pretext\ntask that globally maps shuffled RGB-IR images into a shared latent permutation\nspace, providing modality-invariant global representations for downstream ReID\ntasks. Second, we present a part-aware cycle-contrastive (PCC) learning\nstrategy that utilizes cross-modality cycle-consistency to maximize agreement\nbetween semantically similar RGB-IR image patches. This enables contrastive\nlearning for the unpaired multi-modal scenarios, further improving the\ndiscriminability of local features without laborious instance augmentation.\nBased on these designs, MMGL effectively alleviates the modality bias training\nproblem. Extensive experiments demonstrate that it learns better\nrepresentations (+8.03% Rank-1 accuracy) with faster training speed (converge\nonly in few hours) and higher data efficiency (&lt;5% data size) than ImageNet\npre-training. The results also suggest it generalizes well to various existing\nmodels, losses and has promising transferability across datasets. The code will\nbe released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Lin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1\">Qianyan Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zongyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yehansen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Raw High-Definition Radar for Multi-Task Learning. (arXiv:2112.10646v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10646","description":"<p>With their robustness to adverse weather conditions and ability to measure\nspeeds, radar sensors have been part of the automotive landscape for more than\ntwo decades. Recent progress toward High Definition (HD) Imaging radar has\ndriven the angular resolution below the degree, thus approaching laser scanning\nperformance. However, the amount of data a HD radar delivers and the\ncomputational cost to estimate the angular positions remain a challenge. In\nthis paper, we propose a novel HD radar sensing model, FFT-RadNet, that\neliminates the overhead of computing the range-azimuth-Doppler 3D tensor,\nlearning instead to recover angles from a range-Doppler spectrum. FFT-RadNet is\ntrained both to detect vehicles and to segment free driving space. On both\ntasks, it competes with the most recent radar-based models while requiring less\ncompute and memory. Also, we collected and annotated 2-hour worth of raw data\nfrom synchronized automotive-grade sensors (camera, laser, HD radar) in various\nenvironments (city street, highway, countryside road). This unique dataset,\nnick-named RADIal for \"Radar, Lidar et al.\", is available at\nhttps://github.com/valeoai/RADIal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rebut_J/0/1/0/all/0/1\">Julien Rebut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouaknine_A/0/1/0/all/0/1\">Arthur Ouaknine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_W/0/1/0/all/0/1\">Waqas Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Resolution Image Synthesis with Latent Diffusion Models. (arXiv:2112.10752v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10752","description":"<p>By decomposing the image formation process into a sequential application of\ndenoising autoencoders, diffusion models (DMs) achieve state-of-the-art\nsynthesis results on image data and beyond. Additionally, their formulation\nallows for a guiding mechanism to control the image generation process without\nretraining. However, since these models typically operate directly in pixel\nspace, optimization of powerful DMs often consumes hundreds of GPU days and\ninference is expensive due to sequential evaluations. To enable DM training on\nlimited computational resources while retaining their quality and flexibility,\nwe apply them in the latent space of powerful pretrained autoencoders. In\ncontrast to previous work, training diffusion models on such a representation\nallows for the first time to reach a near-optimal point between complexity\nreduction and detail preservation, greatly boosting visual fidelity. By\nintroducing cross-attention layers into the model architecture, we turn\ndiffusion models into powerful and flexible generators for general conditioning\ninputs such as text or bounding boxes and high-resolution synthesis becomes\npossible in a convolutional manner. Our latent diffusion models (LDMs) achieve\na new state of the art for image inpainting and highly competitive performance\non various tasks, including unconditional image generation, semantic scene\nsynthesis, and super-resolution, while significantly reducing computational\nrequirements compared to pixel-based DMs. Code is available at\nhttps://github.com/CompVis/latent-diffusion .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1\">Robin Rombach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1\">Andreas Blattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_D/0/1/0/all/0/1\">Dominik Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1\">Patrick Esser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeMask: Semantically Masked Transformers for Semantic Segmentation. (arXiv:2112.12782v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12782","description":"<p>Finetuning a pretrained backbone in the encoder part of an image transformer\nnetwork has been the traditional approach for the semantic segmentation task.\nHowever, such an approach leaves out the semantic context that an image\nprovides during the encoding stage. This paper argues that incorporating\nsemantic information of the image into pretrained hierarchical\ntransformer-based backbones while finetuning improves the performance\nconsiderably. To achieve this, we propose SeMask, a simple and effective\nframework that incorporates semantic information into the encoder with the help\nof a semantic attention operation. In addition, we use a lightweight semantic\ndecoder during training to provide supervision to the intermediate semantic\nprior maps at every stage. Our experiments demonstrate that incorporating\nsemantic priors enhances the performance of the established hierarchical\nencoders with a slight increase in the number of FLOPs. We provide empirical\nproof by integrating SeMask into Swin Transformer and Mix Transformer backbones\nas our encoder paired with different decoders. Our framework achieves a new\nstate-of-the-art of 58.25% mIoU on the ADE20K dataset and improvements of over\n3% in the mIoU metric on the Cityscapes dataset. The code and checkpoints are\npublicly available at\nhttps://github.com/Picsart-AI-Research/SeMask-Segmentation .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_J/0/1/0/all/0/1\">Jitesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anukriti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_N/0/1/0/all/0/1\">Nikita Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zilong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVPR: Transformer-based place recognition with multi-level attention aggregation. (arXiv:2201.02001v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02001","description":"<p>Visual place recognition is a challenging task for applications such as\nautonomous driving navigation and mobile robot localization. Distracting\nelements presenting in complex scenes often lead to deviations in the\nperception of visual place. To address this problem, it is crucial to integrate\ninformation from only task-relevant regions into image representations. In this\npaper, we introduce a novel holistic place recognition model, TransVPR, based\non vision Transformers. It benefits from the desirable property of the\nself-attention operation in Transformers which can naturally aggregate\ntask-relevant features. Attentions from multiple levels of the Transformer,\nwhich focus on different regions of interest, are further combined to generate\na global image representation. In addition, the output tokens from Transformer\nlayers filtered by the fused attention mask are considered as key-patch\ndescriptors, which are used to perform spatial matching to re-rank the\ncandidates retrieved by the global image features. The whole model allows\nend-to-end training with a single objective and image-level supervision.\nTransVPR achieves state-of-the-art performance on several real-world benchmarks\nwhile maintaining low computational time and storage requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruotong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Weiliang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"gDNA: Towards Generative Detailed Neural Avatars. (arXiv:2201.04123v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04123","description":"<p>To make 3D human avatars widely available, we must be able to generate a\nvariety of 3D virtual humans with varied identities and shapes in arbitrary\nposes. This task is challenging due to the diversity of clothed body shapes,\ntheir complex articulations, and the resulting rich, yet stochastic geometric\ndetail in clothing. Hence, current methods to represent 3D people do not\nprovide a full generative model of people in clothing. In this paper, we\npropose a novel method that learns to generate detailed 3D shapes of people in\na variety of garments with corresponding skinning weights. Specifically, we\ndevise a multi-subject forward skinning module that is learned from only a few\nposed, un-rigged scans per subject. To capture the stochastic nature of\nhigh-frequency details in garments, we leverage an adversarial loss formulation\nthat encourages the model to capture the underlying statistics. We provide\nempirical evidence that this leads to realistic generation of local details\nsuch as wrinkles. We show that our model is able to generate natural human\navatars wearing diverse and detailed clothing. Furthermore, we show that our\nmethod can be used on the task of fitting human models to raw scans,\noutperforming the previous state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianjian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinlong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions. (arXiv:2201.11407v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11407","description":"<p>Video frame interpolation aims to synthesize one or multiple frames between\ntwo consecutive frames in a video. It has a wide range of applications\nincluding slow-motion video generation, frame-rate up-scaling and developing\nvideo codecs. Some older works tackled this problem by assuming per-pixel\nlinear motion between video frames. However, objects often follow a non-linear\nmotion pattern in the real domain and some recent methods attempt to model\nper-pixel motion by non-linear models (e.g., quadratic). A quadratic model can\nalso be inaccurate, especially in the case of motion discontinuities over time\n(i.e. sudden jerks) and occlusions, where some of the flow information may be\ninvalid or inaccurate.\n</p>\n<p>In our paper, we propose to approximate the per-pixel motion using a\nspace-time convolution network that is able to adaptively select the motion\nmodel to be used. Specifically, we are able to softly switch between a linear\nand a quadratic model. Towards this end, we use an end-to-end 3D CNN\nencoder-decoder architecture over bidirectional optical flows and occlusion\nmaps to estimate the non-linear motion model of each pixel. Further, a motion\nrefinement module is employed to refine the non-linear motion and the\ninterpolated frames are estimated by a simple warping of the neighboring frames\nwith the estimated per-pixel motion. Through a set of comprehensive\nexperiments, we validate the effectiveness of our model and show that our\nmethod outperforms state-of-the-art algorithms on four datasets (Vimeo, DAVIS,\nHD and GoPro).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Saikat Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramaniam_A/0/1/0/all/0/1\">Arulkumar Subramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anurag Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. (arXiv:2201.11440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11440","description":"<p>Novel and high-performance medical image classification pipelines are heavily\nutilizing ensemble learning strategies. The idea of ensemble learning is to\nassemble diverse models or multiple predictions and, thus, boost prediction\nperformance. However, it is still an open question to what extent as well as\nwhich ensemble learning strategies are beneficial in deep learning based\nmedical image classification pipelines. In this work, we proposed a\nreproducible medical image classification pipeline for analyzing the\nperformance impact of the following ensemble learning techniques: Augmenting,\nStacking, and Bagging. The pipeline consists of state-of-the-art preprocessing\nand image augmentation methods as well as 9 deep convolution neural network\narchitectures. It was applied on four popular medical imaging datasets with\nvarying complexity. Furthermore, 12 pooling functions for combining multiple\npredictions were analyzed, ranging from simple statistical functions like\nunweighted averaging up to more complex learning-based functions like support\nvector machines. Our results revealed that Stacking achieved the largest\nperformance gain of up to 13% F1-score increase. Augmenting showed consistent\nimprovement capabilities by up to 4% and is also applicable to single model\nbased pipelines. Cross-validation based Bagging demonstrated significant\nperformance gain close to Stacking, which resulted in an F1-score increase up\nto +11%. Furthermore, we demonstrated that simple statistical pooling functions\nare equal or often even better than more complex pooling functions. We\nconcluded that the integration of ensemble learning techniques is a powerful\nmethod for any medical image classification pipeline to improve robustness and\nboost performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_D/0/1/0/all/0/1\">Dominik M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_Rey_I/0/1/0/all/0/1\">I&#xf1;aki Soto-Rey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramer_F/0/1/0/all/0/1\">Frank Kramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint. (arXiv:2202.00886v5 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.00886","description":"<p>Multi-perspective cameras are quickly gaining importance in many applications\nsuch as smart vehicles and virtual or augmented reality. However, a large\nsystem size or absence of overlap in neighbouring fields-of-view often\ncomplicate their calibration. We present a novel solution which relies on the\navailability of an external motion capture system. Our core contribution\nconsists of an extension to the hand-eye calibration problem which jointly\nsolves multi-eye-to-base problems in closed form. We furthermore demonstrate\nits equivalence to the multi-eye-in-hand problem. The practical validity of our\napproach is supported by our experiments, indicating that the method is highly\nefficient and accurate, and outperforms existing closed-form alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1\">S&#xf6;ren Schwertfeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1\">Laurent Kneip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mathematical Cookbook for Snapshot Compressive Imaging. (arXiv:2202.07437v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07437","description":"<p>The author intends to provide you with a beautiful, elegant, user-friendly\ncookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the\ncookbook is composed of introduction and conventional optimization, using\nregularization-based optimization algorithms for SCI. The latest releases are\nstrongly recommended! For any other questions, suggestions, or comments, feel\nfree to email the author.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02194","description":"<p>In some scenarios, classifier requires detecting out-of-distribution samples\nfar from its training data. With desirable characteristics, reconstruction\nautoencoder-based methods deal with this problem by using input reconstruction\nerror as a metric of novelty vs. normality. We formulate the essence of such\napproach as a quadruplet domain translation with an intrinsic bias to only\nquery for a proxy of conditional data uncertainty. Accordingly, an improvement\ndirection is formalized as maximumly compressing the autoencoder's latent space\nwhile ensuring its reconstructive power for acting as a described domain\ntranslator. From it, strategies are introduced including semantic\nreconstruction, data certainty decomposition and normalized L2 distance to\nsubstantially improve original methods, which together establish\nstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of\nCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method\nworks without any additional data, hard-to-implement structure, time-consuming\npipeline, and even harming the classification accuracy of known classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yibo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from All Vehicles. (arXiv:2203.11934v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.11934","description":"<p>In this paper, we present a system to train driving policies from experiences\ncollected not just from the ego-vehicle, but all vehicles that it observes.\nThis system uses the behaviors of other agents to create more diverse driving\nscenarios without collecting additional data. The main difficulty in learning\nfrom other vehicles is that there is no sensor information. We use a set of\nsupervisory tasks to learn an intermediate representation that is invariant to\nthe viewpoint of the controlling vehicle. This not only provides a richer\nsignal at training time but also allows more complex reasoning during\ninference. Learning how all vehicles drive helps predict their behavior at test\ntime and can avoid collisions. We evaluate this system in closed-loop driving\nsimulations. Our system outperforms all prior methods on the public CARLA\nLeaderboard by a wide margin, improving driving score by 25 and route\ncompletion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving\nchallenge. Code and data are available at https://github.com/dotchen/LAV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Microscopy Designs an All Optical Quantitative Phase Microscope. (arXiv:2203.14944v2 [physics.optics] UPDATED)","link":"http://arxiv.org/abs/2203.14944","description":"<p>Ever since the first microscope by Zacharias Janssen in the late 16th\ncentury, scientists have been inventing new types of microscopes for various\ntasks. Inventing a novel architecture demands years, if not decades, worth of\nscientific experience and creativity. In this work, we introduce Differentiable\nMicroscopy ($\\partial\\mu$), a deep learning-based design paradigm, to aid\nscientists design new interpretable microscope architectures. Differentiable\nmicroscopy first models a common physics-based optical system however with\ntrainable optical elements at key locations on the optical path. Using\npre-acquired data, we then train the model end-to-end for a task of interest.\nThe learnt design proposal can then be simplified by interpreting the learnt\noptical elements. As a first demonstration, based on the optical 4-$f$ system,\nwe present an all-optical quantitative phase microscope (QPM) design that\nrequires no computational post-reconstruction. A follow-up literature survey\nsuggested that the learnt architecture is similar to the generalized phase\nconcept developed two decades ago. We then incorporate the generalized phase\ncontrast concept to simplify the learning procedure. Furthermore, this physical\noptical setup is miniaturized using a diffractive deep neural network (D2NN).\nWe outperform the existing benchmark for all-optical phase-to-intensity\nconversion on multiple datasets, and ours is the first demonstration of its\nkind on D2NNs. The proposed differentiable microscopy framework supplements the\ncreative process of designing new optical systems and would perhaps lead to\nunconventional but better optical designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Herath_K/0/1/0/all/0/1\">Kithmini Herath</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Haputhanthri_U/0/1/0/all/0/1\">Udith Haputhanthri</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hettiarachchi_R/0/1/0/all/0/1\">Ramith Hettiarachchi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kariyawasam_H/0/1/0/all/0/1\">Hasindu Kariyawasam</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ahmad_A/0/1/0/all/0/1\">Azeem Ahmad</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ahluwalia_B/0/1/0/all/0/1\">Balpreet S. Ahluwalia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Edussooriya_C/0/1/0/all/0/1\">Chamira U. S. Edussooriya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wadduwage_D/0/1/0/all/0/1\">Dushan Wadduwage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Face Video Compression using Multiple Views. (arXiv:2203.15401v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15401","description":"<p>Recent advances in deep generative models led to the development of neural\nface video compression codecs that use an order of magnitude less bandwidth\nthan engineered codecs. These neural codecs reconstruct the current frame by\nwarping a source frame and using a generative model to compensate for\nimperfections in the warped source frame. Thereby, the warp is encoded and\ntransmitted using a small number of keypoints rather than a dense flow field,\nwhich leads to massive savings compared to traditional codecs. However, by\nrelying on a single source frame only, these methods lead to inaccurate\nreconstructions (e.g. one side of the head becomes unoccluded when turning the\nhead and has to be synthesized). Here, we aim to tackle this issue by relying\non multiple source frames (views of the face) and present encouraging results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Volokitin_A/0/1/0/all/0/1\">Anna Volokitin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brugger_S/0/1/0/all/0/1\">Stefan Brugger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benlalah_A/0/1/0/all/0/1\">Ali Benlalah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_S/0/1/0/all/0/1\">Sebastian Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amberg_B/0/1/0/all/0/1\">Brian Amberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1\">Michael Tschannen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leverage Your Local and Global Representations: A New Self-Supervised Learning Strategy. (arXiv:2203.17205v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17205","description":"<p>Self-supervised learning (SSL) methods aim to learn view-invariant\nrepresentations by maximizing the similarity between the features extracted\nfrom different crops of the same image regardless of cropping size and content.\nIn essence, this strategy ignores the fact that two crops may truly contain\ndifferent image information, e.g., background and small objects, and thus tends\nto restrain the diversity of the learned representations. In this work, we\naddress this issue by introducing a new self-supervised learning strategy,\nLoGo, that explicitly reasons about Local and Global crops. To achieve view\ninvariance, LoGo encourages similarity between global crops from the same\nimage, as well as between a global and a local crop. However, to correctly\nencode the fact that the content of smaller crops may differ entirely, LoGo\npromotes two local crops to have dissimilar representations, while being close\nto global crops. Our LoGo strategy can easily be applied to existing SSL\nmethods. Our extensive experiments on a variety of datasets and using different\nself-supervised learning frameworks validate its superiority over existing\napproaches. Noticeably, we achieve better results than supervised models on\ntransfer learning when using only 1/10 of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Congpei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1\">Wei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction. (arXiv:2204.01297v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01297","description":"<p>Human motion prediction is a challenge task due to the dynamic spatiotemporal\ngraph correlations in different motion sequences. How to efficiently represent\nspatiotemporal graph correlations and model dynamic correlation variances\nbetween different motion sequences is a challenge for spatiotemporal graph\nrepresentation in motion prediction. In this work, we present Dynamic\nSpatioTemporal Graph Convolution (DSTD-GC). The proposed DSTD-GC decomposes\ndynamic spatiotemporal graph modeling into a combination of Dynamic Spatial\nGraph Convolution (DS-GC) and Dynamic Temporal Graph Convolution (DT-GC). As\nhuman motions are subject to common constraints like body connections and\npresent dynamic motion patterns from different samples, we present Constrained\nDynamic Correlation Modeling strategy to represent the spatial/temporal graph\nas a shared spatial/temporal correlation and a function to extract\ntemporal-specific /spatial-specific adjustments for each sample. The modeling\nstrategy represents the spatiotemporal graph with 28.6\\% parameters of the\nstate-of-the-art static decomposition representation while also explicitly\nmodels sample-specific spatiotemporal correlation variances. Moreover, we also\nmathematically reformulating spatiotemporal graph convolutions and their\ndecomposed variants into a unified form and find that DSTD-GC relaxes strict\nconstraints of other graph convolutions, leading to a stronger representation\ncapability. Combining DSTD-GC with prior knowledge, we propose a powerful\nspatiotemporal graph convolution network called DSTD-GCN which outperforms\nstate-of-the-art methods on the Human3.6M and CMU Mocap datasets in prediction\naccuracy with fewest parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jiajun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fuxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection. (arXiv:2204.02553v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02553","description":"<p>Recent studies have addressed the concern of detecting and rejecting the\nout-of-distribution (OOD) samples as a major challenge in the safe deployment\nof deep learning (DL) models. It is desired that the DL model should only be\nconfident about the in-distribution (ID) data which reinforces the driving\nprinciple of the OOD detection. In this paper, we propose a simple yet\neffective generalized OOD detection method independent of out-of-distribution\ndatasets. Our approach relies on self-supervised feature learning of the\ntraining samples, where the embeddings lie on a compact low-dimensional space.\nMotivated by the recent studies that show self-supervised adversarial\ncontrastive learning helps robustify the model, we empirically show that a\npre-trained model with self-supervised contrastive learning yields a better\nmodel for uni-dimensional feature learning in the latent space. The method\nproposed in this work referred to as RODD outperforms SOTA detection\nperformance on an extensive suite of benchmark datasets on OOD detection tasks.\nOn the CIFAR-100 benchmarks, RODD achieves a 26.97 $\\%$ lower false-positive\nrate (FPR@95) compared to SOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1\">Umar Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeili_A/0/1/0/all/0/1\">Ashkan Esmaeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Digital Disguises: Leveraging Face Swaps to Protect Patient Privacy. (arXiv:2204.03559v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03559","description":"<p>With rapid advancements in image generation technology, face swapping for\nprivacy protection has emerged as an active area of research. The ultimate\nbenefit is improved access to video datasets, e.g. in healthcare settings.\nRecent literature has proposed deep network-based architectures to perform\nfacial swaps and reported the associated reduction in facial recognition\naccuracy. However, there is not much reporting on how well these methods\npreserve the types of semantic information needed for the privatized videos to\nremain useful for their intended application. Our main contribution is a novel\nend-to-end face swapping pipeline for recorded videos of standardized\nassessments of autism symptoms in children. Through this design, we are the\nfirst to provide a methodology for assessing the privacy-utility trade-offs for\nthe face swapping approach to patient privacy protection. Our methodology can\nshow, for example, that current deep network based face swapping is\nbottle-necked by face detection in real world videos, and the extent to which\ngaze and expression information is preserved by face swaps relative to baseline\nprivatization methods such as blurring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_E/0/1/0/all/0/1\">Ethan Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shic_F/0/1/0/all/0/1\">Frederick Shic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skytta_J/0/1/0/all/0/1\">Jenny Skytta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_E/0/1/0/all/0/1\">Eakta Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlutoNet: An Efficient Polyp Segmentation Network. (arXiv:2204.03652v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.03652","description":"<p>Polyps in the colon can turn into cancerous cells if not removed with early\nintervention. Deep learning models are used to minimize the number of polyps\nthat goes unnoticed by the experts, and to accurately segment the detected\npolyps during these interventions. Although these models perform well on these\ntasks, they require too many parameters, which can pose a problem with\nreal-time applications. To address this problem, we propose a novel\nsegmentation model called PlutoNet which requires only 2,626,337 parameters\nwhile outperforming state-of-the-art models on multiple medical image\nsegmentation tasks. We use EfficientNetB0 architecture as a backbone and\npropose the novel \\emph{modified partial decoder}, which is a combination of\npartial decoder and full scale connections, which further reduces the number of\nparameters required, as well as captures semantic details. We use asymmetric\nconvolutions to handle varying polyp sizes. Finally, we weight each feature map\nto improve segmentation by using a squeeze and excitation block. In addition to\npolyp segmentation in colonoscopy, we tested our model on segmentation of\nnuclei and surgical instruments to demonstrate its generalizability to\ndifferent medical image segmentation tasks. Our model outperformed the\nstate-of-the-art models with a Dice score of \\%92.3 in CVC-ClinicDB dataset and\n\\%89.3 in EndoScene dataset, a Dice score of \\%91.93 on the 2018 Data Science\nBowl Challenge dataset, and a Dice score of \\%94.8 on Kvasir-Instrument\ndataset. Our experiments and ablation studies show that our model is superior\nin terms of accuracy, and it is able generalize well to multiple medical\nsegmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Erol_T/0/1/0/all/0/1\">Tugberk Erol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarikaya_D/0/1/0/all/0/1\">Duygu Sarikaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Networks for Image Augmentation in Agriculture: A Systematic Review. (arXiv:2204.04707v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04707","description":"<p>In agricultural image analysis, optimal model performance is keenly pursued\nfor better fulfilling visual recognition tasks (e.g., image classification,\nsegmentation, object detection and localization), in the presence of challenges\nwith biological variability and unstructured environments. Large-scale,\nbalanced and ground-truthed image datasets, however, are often difficult to\nobtain to fuel the development of advanced, high-performance models. As\nartificial intelligence through deep learning is impacting analysis and\nmodeling of agricultural images, data augmentation plays a crucial role in\nboosting model performance while reducing manual efforts for data preparation,\nby algorithmically expanding training datasets. Beyond traditional data\naugmentation techniques, generative adversarial network (GAN) invented in 2014\nin the computer vision community, provides a suite of novel approaches that can\nlearn good data representations and generate highly realistic samples. Since\n2017, there has been a growth of research into GANs for image augmentation or\nsynthesis in agriculture for improved model performance. This paper presents an\noverview of the evolution of GAN architectures followed by a systematic review\nof their application to agriculture\n(https://github.com/Derekabc/GANs-Agriculture), involving various vision tasks\nfor plant health, weeds, fruits, aquaculture, animal farming, plant phenotyping\nas well as postharvest detection of fruit defects. Challenges and opportunities\nof GANs are discussed for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olaniyi_E/0/1/0/all/0/1\">Ebenezer Olaniyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuzhen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanbo Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glass Segmentation with RGB-Thermal Image Pairs. (arXiv:2204.05453v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05453","description":"<p>This paper proposes a new glass segmentation method utilizing paired RGB and\nthermal images. Due to the large difference between the transmission property\nof visible light and that of the thermal energy through the glass where most\nglass is transparent to the visible light but opaque to thermal energy, glass\nregions of a scene are made more distinguishable with a pair of RGB and thermal\nimages than solely with an RGB image. To exploit such a unique property, we\npropose a neural network architecture that effectively combines an RGB-thermal\nimage pair with a new multi-modal fusion module based on attention, and\nintegrate CNN and transformer to extract local features and long-range\ndependencies, respectively. As well, we have collected a new dataset containing\n5551 RGB-thermal image pairs with ground-truth segmentation annotations. The\nqualitative and quantitative evaluations demonstrate the effectiveness of the\nproposed approach on fusing RGB and thermal data for glass segmentation. Our\ncode and data are available at\nhttps://github.com/Dong-Huo/RGB-T-Glass-Segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Dong Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yee-Hong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisCUIT: Visual Auditor for Bias in CNN Image Classifier. (arXiv:2204.05899v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05899","description":"<p>CNN image classifiers are widely used, thanks to their efficiency and\naccuracy. However, they can suffer from biases that impede their practical\napplications. Most existing bias investigation techniques are either\ninapplicable to general image classification tasks or require significant user\nefforts in perusing all data subgroups to manually specify which data\nattributes to inspect. We present VisCUIT, an interactive visualization system\nthat reveals how and why a CNN classifier is biased. VisCUIT visually\nsummarizes the subgroups on which the classifier underperforms and helps users\ndiscover and characterize the cause of the underperformances by revealing image\nconcepts responsible for activating neurons that contribute to\nmisclassifications. VisCUIT runs in modern browsers and is open-source,\nallowing people to easily access and extend the tool to other model\narchitectures and datasets. VisCUIT is available at the following public demo\nlink: https://poloclub.github.io/VisCUIT. A video demo is available at\nhttps://youtu.be/eNDbSyM4R_4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijie J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Captioning: a comparative review of where we are and which could be the route. (arXiv:2204.05976v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05976","description":"<p>Video captioning is the process of describing the content of a sequence of\nimages capturing its semantic relationships and meanings. Dealing with this\ntask with a single image is arduous, not to mention how difficult it is for a\nvideo (or images sequence). The amount and relevance of the applications of\nvideo captioning are vast, mainly to deal with a significant amount of video\nrecordings in video surveillance, or assisting people visually impaired, to\nmention a few. To analyze where the efforts of our community to solve the video\ncaptioning task are, as well as what route could be better to follow, this\nmanuscript presents an extensive review of more than 105 papers for the period\nof 2016 to 2021. As a result, the most-used datasets and metrics are\nidentified. Also, the main approaches used and the best ones. We compute a set\nof rankings based on several performance metrics to obtain, according to its\nperformance, the best method with the best result on the video captioning task.\nFinally, some insights are concluded about which could be the next steps or\nopportunity areas to improve dealing with this complex task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moctezuma_D/0/1/0/all/0/1\">Daniela Moctezuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_delReal_T/0/1/0/all/0/1\">Tania Ram&#xed;rez-delReal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_G/0/1/0/all/0/1\">Guillermo Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Chavez_O/0/1/0/all/0/1\">Oth&#xf3;n Gonz&#xe1;lez-Ch&#xe1;vez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}