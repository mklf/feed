{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Sentiment and structure in word co-occurrence networks on Twitter. (arXiv:2110.00587v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00587","description":"<p>We explore the relationship between context and happiness scores in political\ntweets using word co-occurrence networks, where nodes in the network are the\nwords, and the weight of an edge is the number of tweets in the corpus for\nwhich the two connected words co-occur. In particular, we consider tweets with\nhashtags #imwithher and #crookedhillary, both relating to Hillary Clinton's\npresidential bid in 2016. We then analyze the network properties in conjunction\nwith the word scores by comparing with null models to separate the effects of\nthe network structure and the score distribution. Neutral words are found to be\ndominant and most words, regardless of polarity, tend to co-occur with neutral\nwords. We do not observe any score homophily among positive and negative words.\nHowever, when we perform network backboning, community detection results in\nword groupings with meaningful narratives, and the happiness scores of the\nwords in each group correspond to its respective theme. Thus, although we\nobserve no clear relationship between happiness scores and co-occurrence at the\nnode or edge level, a community-centric approach can isolate themes of\ncompeting sentiments in a corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">Mikaela Irene Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expected Validation Performance and Estimation of a Random Variable's Maximum. (arXiv:2110.00613v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00613","description":"<p>Research in NLP is often supported by experimental results, and improved\nreporting of such results can lead to better understanding and more\nreproducible science. In this paper we analyze three statistical estimators for\nexpected validation performance, a tool used for reporting performance (e.g.,\naccuracy) as a function of computational budget (e.g., number of hyperparameter\ntuning experiments). Where previous work analyzing such estimators focused on\nthe bias, we also examine the variance and mean squared error (MSE). In both\nsynthetic and realistic scenarios, we evaluate three estimators and find the\nunbiased estimator has the highest variance, and the estimator with the\nsmallest variance has the largest bias; the estimator with the smallest MSE\nstrikes a balance between bias and variance, displaying a classic bias-variance\ntradeoff. We use expected validation performance to compare between different\nmodels, and analyze how frequently each estimator leads to drawing incorrect\nconclusions about which of two models performs best. We find that the two\nbiased estimators lead to the fewest incorrect conclusions, which hints at the\nimportance of minimizing variance and MSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Attentive Constituency Parsing for UCCA-based Semantic Parsing. (arXiv:2110.00621v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00621","description":"<p>Semantic parsing provides a way to extract the semantic structure of a text\nthat could be understood by machines. It is utilized in various NLP\napplications that require text comprehension such as summarization and question\nanswering. Graph-based representation is one of the semantic representation\napproaches to express the semantic structure of a text. Such representations\ngenerate expressive and adequate graph-based target structures. In this paper,\nwe focus primarily on UCCA graph-based semantic representation. The paper not\nonly presents the existing approaches proposed for UCCA representation, but\nalso proposes a novel self-attentive neural parsing model for the UCCA\nrepresentation. We present the results for both single-lingual and\ncross-lingual tasks using zero-shot and few-shot learning for low-resource\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bolucu_N/0/1/0/all/0/1\">Necva B&#xf6;l&#xfc;c&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_B/0/1/0/all/0/1\">Burcu Can</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALBU: An approximate Loopy Belief message passing algorithm for LDA to improve performance on small data sets. (arXiv:2110.00635v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00635","description":"<p>Variational Bayes (VB) applied to latent Dirichlet allocation (LDA) has\nbecome the most popular algorithm for aspect modeling. While sufficiently\nsuccessful in text topic extraction from large corpora, VB is less successful\nin identifying aspects in the presence of limited data. We present a novel\nvariational message passing algorithm as applied to Latent Dirichlet Allocation\n(LDA) and compare it with the gold standard VB and collapsed Gibbs sampling. In\nsituations where marginalisation leads to non-conjugate messages, we use ideas\nfrom sampling to derive approximate update equations. In cases where conjugacy\nholds, Loopy Belief update (LBU) (also known as Lauritzen-Spiegelhalter) is\nused. Our algorithm, ALBU (approximate LBU), has strong similarities with\nVariational Message Passing (VMP) (which is the message passing variant of VB).\nTo compare the performance of the algorithms in the presence of limited data,\nwe use data sets consisting of tweets and news groups. Additionally, to perform\nmore fine grained evaluations and comparisons, we use simulations that enable\ncomparisons with the ground truth via Kullback-Leibler divergence (KLD). Using\ncoherence measures for the text corpora and KLD with the simulations we show\nthat ALBU learns latent distributions more accurately than does VB, especially\nfor smaller data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Rebecca M.C. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preez_J/0/1/0/all/0/1\">Johan A. du Preez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models. (arXiv:2110.00672v1 [cs.CY])","link":"http://arxiv.org/abs/2110.00672","description":"<p>We use a dataset of U.S. first names with labels based on predominant gender\nand racial group to examine the effect of training corpus frequency on\ntokenization, contextualization, similarity to initial representation, and bias\nin BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white\nnames are less frequent in the training corpora of these four language models.\nWe find that infrequent names are more self-similar across contexts, with\nSpearman's r between frequency and self-similarity as low as -.763. Infrequent\nnames are also less similar to initial representation, with Spearman's r\nbetween frequency and linear centered kernel alignment (CKA) similarity to\ninitial representation as high as .702. Moreover, we find Spearman's r between\nracial bias and name frequency in BERT of .492, indicating that lower-frequency\nminority group names are more associated with unpleasantness. Representations\nof infrequent names undergo more processing, but are more self-similar,\nindicating that models rely on less context-informed representations of\nuncommon and minority names which are overfit to a lower number of observed\ncontexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning. (arXiv:2110.00678v1 [eess.AS])","link":"http://arxiv.org/abs/2110.00678","description":"<p>To address the performance gap of English ASR models on L2 English speakers,\nwe evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;\nXu et al., 2021) on L2-ARCTIC, a non-native English speech corpus (Zhao et al.,\n2018) under different training settings. We compare \\textbf{(a)} models trained\nwith a combination of diverse accents to ones trained with only specific\naccents and \\textbf{(b)} results from different single-accent models. Our\nexperiments demonstrate the promise of developing ASR models for non-native\nEnglish speakers, even with small amounts of L2 training data and even without\na language model. Our models also excel in the zero-shot setting where we train\non multiple L2 datasets and test on a blind L2 test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shibano_T/0/1/0/all/0/1\">Toshiko Shibano</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyi Zhang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mia Taige Li</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Haejin Cho</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Sullivan_P/0/1/0/all/0/1\">Peter Sullivan</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a> (1) ((1) University of British Columbia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Robustness of Dialog Models to Popular Figurative Language Constructs. (arXiv:2110.00687v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00687","description":"<p>Humans often employ figurative language use in communication, including\nduring interactions with dialog systems. Thus, it is important for real-world\ndialog systems to be able to handle popular figurative language constructs like\nmetaphor and simile. In this work, we analyze the performance of existing\ndialog models in situations where the input dialog context exhibits use of\nfigurative language. We observe large gaps in handling of figurative language\nwhen evaluating the models on two open domain dialog datasets. When faced with\ndialog contexts consisting of figurative language, some models show very large\ndrops in performance compared to contexts without figurative language. We\nencourage future research in dialog modeling to separately analyze and report\nresults on figurative language in order to better test model capabilities\nrelevant to real-world use. Finally, we propose lightweight solutions to help\nexisting models become more robust to figurative language by simply using an\nexternal resource to translate figurative language to literal (non-figurative)\nforms while preserving the meaning to the best extent possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering and Network Analysis for the Embedding Spaces of Sentences and Sub-Sentences. (arXiv:2110.00697v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00697","description":"<p>Sentence embedding methods offer a powerful approach for working with short\ntextual constructs or sequences of words. By representing sentences as dense\nnumerical vectors, many natural language processing (NLP) applications have\nimproved their performance. However, relatively little is understood about the\nlatent structure of sentence embeddings. Specifically, research has not\naddressed whether the length and structure of sentences impact the sentence\nembedding space and topology. This paper reports research on a set of\ncomprehensive clustering and network analyses targeting sentence and\nsub-sentence embedding spaces. Results show that one method generates the most\nclusterable embeddings. In general, the embeddings of span sub-sentences have\nbetter clustering properties than the original sentences. The results have\nimplications for future sentence embedding models and applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1\">Yuan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinowski_A/0/1/0/all/0/1\">Alexander Kalinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenberg_J/0/1/0/all/0/1\">Jane Greenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-shot Multilingual Neural Machine Translation for Low-Resource Languages. (arXiv:2110.00712v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00712","description":"<p>Although the multilingual Neural Machine Translation(NMT), which extends\nGoogle's multilingual NMT, has ability to perform zero-shot translation and the\niterative self-learning algorithm can improve the quality of zero-shot\ntranslation, it confronts with two problems: the multilingual NMT model is\nprone to generate wrong target language when implementing zero-shot\ntranslation; the self-learning algorithm, which uses beam search to generate\nsynthetic parallel data, demolishes the diversity of the generated source\nlanguage and amplifies the impact of the same noise during the iterative\nlearning process. In this paper, we propose the tagged-multilingual NMT model\nand improve the self-learning algorithm to handle these two problems. Firstly,\nwe extend the Google's multilingual NMT model and add target tokens to the\ntarget languages, which associates the start tag with the target language to\nensure that the source language can be translated to the required target\nlanguage. Secondly, we improve the self-learning algorithm by replacing beam\nsearch with random sample to increases the diversity of the generated data and\nmakes it properly cover the true data distribution. Experimental results on\nIWSLT show that the adjusted tagged-multilingual NMT separately obtains 9.41\nand 7.85 BLEU scores over the multilingual NMT on 2010 and 2017\nRomanian-Italian test sets. Similarly, it obtains 9.08 and 7.99 BLEU scores on\nItalian-Romanian zero-shot translation. Furthermore, the improved self-learning\nalgorithm shows its superiorities over the conventional self-learning algorithm\non zero-shot translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gongxu Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is There More Pattern in Knowledge Graph? Exploring Proximity Pattern for Knowledge Graph Embedding. (arXiv:2110.00720v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00720","description":"<p>Modeling of relation pattern is the core focus of previous Knowledge Graph\nEmbedding works, which represents how one entity is related to another\nsemantically by some explicit relation. However, there is a more natural and\nintuitive relevancy among entities being always ignored, which is that how one\nentity is close to another semantically, without the consideration of any\nexplicit relation. We name such semantic phenomenon in knowledge graph as\nproximity pattern. In this work, we explore the problem of how to define and\nrepresent proximity pattern, and how it can be utilized to help knowledge graph\nembedding. Firstly, we define the proximity of any two entities according to\ntheir statistically shared queries, then we construct a derived graph structure\nand represent the proximity pattern from global view. Moreover, with the\noriginal knowledge graph, we design a Chained couPle-GNN (CP-GNN) architecture\nto deeply merge the two patterns (graphs) together, which can encode a more\ncomprehensive knowledge embedding. Being evaluated on FB15k-237 and WN18RR\ndatasets, CP-GNN achieves state-of-the-art results for Knowledge Graph\nCompletion task, and can especially boost the modeling capacity for complex\nqueries that contain multiple answer entities, proving the effectiveness of\nintroduced proximity pattern.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiannan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplify Your Law: Using Information Theory to Deduplicate Legal Documents. (arXiv:2110.00735v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00735","description":"<p>Textual redundancy is one of the main challenges to ensuring that legal texts\nremain comprehensible and maintainable. Drawing inspiration from the\nrefactoring literature in software engineering, which has developed methods to\nexpose and eliminate duplicated code, we introduce the duplicated phrase\ndetection problem for legal texts and propose the Dupex algorithm to solve it.\nLeveraging the Minimum Description Length principle from information theory,\nDupex identifies a set of duplicated phrases, called patterns, that together\nbest compress a given input text. Through an extensive set of experiments on\nthe Titles of the United States Code, we confirm that our algorithm works well\nin practice: Dupex will help you simplify your law.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coupette_C/0/1/0/all/0/1\">Corinna Coupette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jyotsna Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spamann_H/0/1/0/all/0/1\">Holger Spamann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopiOCQA: Open-domain Conversational Question Answeringwith Topic Switching. (arXiv:2110.00768v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00768","description":"<p>In a conversational question answering scenario, a questioner seeks to\nextract information about a topic through a series of interdependent questions\nand answers. As the conversation progresses, they may switch to related topics,\na phenomenon commonly observed in information-seeking search sessions. However,\ncurrent datasets for conversational question answering are limiting in two\nways: 1) they do not contain topic switches; and 2) they assume the reference\ntext for the conversation is given, i.e., the setting is not open-domain. We\nintroduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset\nwith topic switches on Wikipedia. TopiOCQA contains 3,920 conversations with\ninformation-seeking questions and free-form answers. TopiOCQA poses a\nchallenging test-bed for models, where efficient retrieval is required on\nmultiple turns of the same conversation, in conjunction with constructing valid\nresponses using conversational history. We evaluate several baselines, by\ncombining state-of-the-art document retrieval methods with neural reader\nmodels. Our best models achieves F1 of 51.9, and BLEU score of 42.1 which falls\nshort of human performance by 18.3 points and 17.6 points respectively,\nindicating the difficulty of our dataset. Our dataset and code will be\navailable at https://mcgill-nlp.github.io/topiocqa\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suleman_K/0/1/0/all/0/1\">Kaheer Suleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimizing LR(1) State Machines is NP-Hard. (arXiv:2110.00776v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00776","description":"<p>LR(1) parsing was a focus of extensive research in the past 50 years. Though\nmost fundamental mysteries have been resolved, a few remain hidden in the dark\ncorners. The one we bumped into is the minimization of the LR(1) state\nmachines, which we prove is NP-hard. It is the node-coloring problem that is\nreduced to the minimization puzzle. The reduction makes use of two technique:\nindirect reduction and incremental construction. Indirect reduction means the\ngraph to be colored is not reduced to an LR(1) state machine directly. Instead,\nit is reduced to a context-free grammar from which an LR(1) state machine is\nderived. Furthermore, by considering the nodes in the graph to be colored one\nat a time, the context-free grammar is incrementally extended from a template\ncontext-free grammar that is for a two-node graph. The extension is done by\nadding new grammar symbols and rules. A minimized LR(1) machine can be used to\nrecover a minimum coloring of the original graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wuu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect Sentiment Quad Prediction as Paraphrase Generation. (arXiv:2110.00796v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00796","description":"<p>Aspect-based sentiment analysis (ABSA) has been extensively studied in recent\nyears, which typically involves four fundamental sentiment elements, including\nthe aspect category, aspect term, opinion term, and sentiment polarity.\nExisting studies usually consider the detection of partial sentiment elements,\ninstead of predicting the four elements in one shot. In this work, we introduce\nthe Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all\nsentiment elements in quads for a given opinionated sentence, which can reveal\na more comprehensive and complete aspect-level sentiment structure. We further\npropose a novel \\textsc{Paraphrase} modeling paradigm to cast the ASQP task to\na paraphrase generation process. On one hand, the generation formulation allows\nsolving ASQP in an end-to-end manner, alleviating the potential error\npropagation in the pipeline solution. On the other hand, the semantics of the\nsentiment elements can be fully exploited by learning to generate them in the\nnatural language form. Extensive experiments on benchmark datasets show the\nsuperiority of our proposed method and the capacity of cross-task transfer with\nthe proposed unified \\textsc{Paraphrase} modeling framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yifei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swiss-Judgment-Prediction: A Multilingual Legal Judgment Prediction Benchmark. (arXiv:2110.00806v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00806","description":"<p>In many jurisdictions, the excessive workload of courts leads to high delays.\nSuitable predictive AI models can assist legal professionals in their work, and\nthus enhance and speed up the process. So far, Legal Judgment Prediction (LJP)\ndatasets have been released in English, French, and Chinese. We publicly\nrelease a multilingual (German, French, and Italian), diachronic (2000-2020)\ncorpus of 85K cases from the Federal Supreme Court of Switzerland (FSCS). We\nevaluate state-of-the-art BERT-based methods including two variants of BERT\nthat overcome the BERT input (text) length limitation (up to 512 tokens).\nHierarchical BERT has the best performance (approx. 68-70% Macro-F1-Score in\nGerman and French). Furthermore, we study how several factors (canton of\norigin, year of publication, text length, legal area) affect performance. We\nrelease both the benchmark dataset and our code to accelerate future research\nand ensure reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturmer_M/0/1/0/all/0/1\">Matthias St&#xfc;rmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning. (arXiv:2110.00842v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00842","description":"<p>Mapping natural language instructions to programs that computers can process\nis a fundamental challenge. Existing approaches focus on likelihood-based\ntraining or using reinforcement learning to fine-tune models based on a single\nreward. In this paper, we pose program generation from language as Inverse\nReinforcement Learning. We introduce several interpretable reward components\nand jointly learn (1) a reward function that linearly combines them, and (2) a\npolicy for program generation. Fine-tuning with our approach achieves\nsignificantly better performance than competitive methods using Reinforcement\nLearning (RL). On the VirtualHome framework, we get improvements of up to 9.0%\non the Longest Common Subsequence metric and 14.7% on recall-based metrics over\nprevious work on this framework (Puig et al., 2018). The approach is\ndata-efficient, showing larger gains in performance in the low-data regime.\nGenerated programs are also preferred by human evaluators over an RL-based\napproach, and rated higher on relevance, completeness, and human-likeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Sentiment Analysis Using NLP and Different Machine Learning Techniques on US Airline Twitter Data. (arXiv:2110.00859v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00859","description":"<p>Today's business ecosystem has become very competitive. Customer satisfaction\nhas become a major focus for business growth. Business organizations are\nspending a lot of money and human resources on various strategies to understand\nand fulfill their customer's needs. But, because of defective manual analysis\non multifarious needs of customers, many organizations are failing to achieve\ncustomer satisfaction. As a result, they are losing customer's loyalty and\nspending extra money on marketing. We can solve the problems by implementing\nSentiment Analysis. It is a combined technique of Natural Language Processing\n(NLP) and Machine Learning (ML). Sentiment Analysis is broadly used to extract\ninsights from wider public opinion behind certain topics, products, and\nservices. We can do it from any online available data. In this paper, we have\nintroduced two NLP techniques (Bag-of-Words and TF-IDF) and various ML\nclassification algorithms (Support Vector Machine, Logistic Regression,\nMultinomial Naive Bayes, Random Forest) to find an effective approach for\nSentiment Analysis on a large, imbalanced, and multi-classed dataset. Our best\napproaches provide 77% accuracy using Support Vector Machine and Logistic\nRegression with Bag-of-Words technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tusar_M/0/1/0/all/0/1\">Md. Taufiqul Haque Khan Tusar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Touhidul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Case Study to Reveal if an Area of Interest has a Trend in Ongoing Tweets Using Word and Sentence Embeddings. (arXiv:2110.00866v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00866","description":"<p>In the field of Natural Language Processing, information extraction from\ntexts has been the objective of many researchers for years. Many different\ntechniques have been applied in order to reveal the opinion that a tweet might\nhave, thus understanding the sentiment of the small writing up to 280\ncharacters. Other than figuring out the sentiment of a tweet, a study can also\nfocus on finding the correlation of the tweets with a certain area of interest,\nwhich constitutes the purpose of this study. In order to reveal if an area of\ninterest has a trend in ongoing tweets, we have proposed an easily applicable\nautomated methodology in which the Daily Mean Similarity Scores that show the\nsimilarity between the daily tweet corpus and the target words representing our\narea of interest is calculated by using a na\\\"ive correlation-based technique\nwithout training any Machine Learning Model. The Daily Mean Similarity Scores\nhave mainly based on cosine similarity and word/sentence embeddings computed by\nMultilanguage Universal Sentence Encoder and showed main opinion stream of the\ntweets with respect to a certain area of interest, which proves that an ongoing\ntrend of a specific subject on Twitter can easily be captured in almost real\ntime by using the proposed methodology in this study. We have also compared the\neffectiveness of using word versus sentence embeddings while applying our\nmethodology and realized that both give almost the same results, whereas using\nword embeddings requires less computational time than sentence embeddings, thus\nbeing more effective. This paper will start with an introduction followed by\nthe background information about the basics, then continue with the explanation\nof the proposed methodology and later on finish by interpreting the results and\nconcluding the findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aslan_I/0/1/0/all/0/1\">&#x130;smail Aslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_Y/0/1/0/all/0/1\">Y&#xfc;cel Top&#xe7;u</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subtractive mountain clustering algorithm applied to a chatbot to assist elderly people in medication intake. (arXiv:2110.00933v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00933","description":"<p>Errors in medication intake among elderly people are very common. One of the\nmain causes for this is their loss of ability to retain information. The high\namount of medicine intake required by the advanced age is another limiting\nfactor. Thence, the design of an interactive aid system, preferably using\nnatural language, to help the older population with medication is in demand. A\nchatbot based on a subtractive cluster algorithm, included in unsupervised\nlearned models, is the chosen solution since the processing of natural\nlanguages is a necessary step in view to construct a chatbot able to answer\nquestions that older people may pose upon themselves concerning a particular\ndrug. In this work, the subtractive mountain clustering algorithm has been\nadapted to the problem of natural languages processing. This algorithm version\nallows for the association of a set of words into clusters. After finding the\ncentre of every cluster -- the most relevant word, all the others are\naggregated according to a defined metric adapted to the language processing\nrealm. All the relevant stored information is processed, as well as the\nquestions, by the algorithm. The correct processing of the text enables the\nchatbot to produce answers that relate to the posed queries. To validate the\nmethod, we use the package insert of a drug as the available information and\nformulate associated questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clar_N/0/1/0/all/0/1\">Neuza Clar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salgado_P/0/1/0/all/0/1\">Paulo A. Salgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdicoulis_T/0/1/0/all/0/1\">T-P Azevedo Perdico&#xfa;lis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Likelihood Ratio Estimation for High- to Zero-frequency N-grams. (arXiv:2110.00946v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00946","description":"<p>Likelihood ratios (LRs), which are commonly used for probabilistic data\nprocessing, are often estimated based on the frequency counts of individual\nelements obtained from samples. In natural language processing, an element can\nbe a continuous sequence of $N$ items, called an $N$-gram, in which each item\nis a word, letter, etc. In this paper, we attempt to estimate LRs based on\n$N$-gram frequency information. A naive estimation approach that uses only\n$N$-gram frequencies is sensitive to low-frequency (rare) $N$-grams and not\napplicable to zero-frequency (unobserved) $N$-grams; these are known as the\nlow- and zero-frequency problems, respectively. To address these problems, we\npropose a method for decomposing $N$-grams into item units and then applying\ntheir frequencies along with the original $N$-gram frequencies. Our method can\nobtain the estimates of unobserved $N$-grams by using the unit frequencies.\nAlthough using only unit frequencies ignores dependencies between items, our\nmethod takes advantage of the fact that certain items often co-occur in\npractice and therefore maintains their dependencies by using the relevant\n$N$-gram frequencies. We also introduce a regularization to achieve robust\nestimation for rare $N$-grams. Our experimental results demonstrate that our\nmethod is effective at solving both problems and can effectively control\ndependencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_M/0/1/0/all/0/1\">Masato Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawakami_K/0/1/0/all/0/1\">Kento Kawakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1\">Kazuho Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_M/0/1/0/all/0/1\">Mitsuo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umemura_K/0/1/0/all/0/1\">Kyoji Umemura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. (arXiv:2110.00976v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00976","description":"<p>Law, interpretations of law, legal arguments, agreements, etc. are typically\nexpressed in writing, leading to the production of vast corpora of legal text.\nTheir analysis, which is at the center of legal practice, becomes increasingly\nelaborate as these collections grow in size. Natural language understanding\n(NLU) technologies can be a valuable tool to support legal practitioners in\nthese endeavors. Their usefulness, however, largely depends on whether current\nstate-of-the-art models can generalize across various tasks in the legal\ndomain. To answer this currently open question, we introduce the Legal General\nLanguage Understanding Evaluation (LexGLUE) benchmark, a collection of datasets\nfor evaluating model performance across a diverse set of legal NLU tasks in a\nstandardized way. We also provide an evaluation and analysis of several generic\nand legal-oriented models demonstrating that the latter consistently offer\nperformance improvements across multiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_A/0/1/0/all/0/1\">Abhik Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_D/0/1/0/all/0/1\">Dirk Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1\">Michael Bommarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Recurrent Neural Networks is all we need for clinical events predictions using EHR data. (arXiv:2110.00998v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00998","description":"<p>Recently, there is great interest to investigate the application of deep\nlearning models for the prediction of clinical events using electronic health\nrecords (EHR) data. In EHR data, a patient's history is often represented as a\nsequence of visits, and each visit contains multiple events. As a result, deep\nlearning models developed for sequence modeling, like recurrent neural networks\n(RNNs) are common architecture for EHR-based clinical events predictive models.\nWhile a large variety of RNN models were proposed in the literature, it is\nunclear if complex architecture innovations will offer superior predictive\nperformance. In order to move this field forward, a rigorous evaluation of\nvarious methods is needed. In this study, we conducted a thorough benchmark of\nRNN architectures in modeling EHR data. We used two prediction tasks: the risk\nfor developing heart failure and the risk of early readmission for inpatient\nhospitalization. We found that simple gated RNN models, including GRUs and\nLSTMs, often offer competitive results when properly tuned with Bayesian\nOptimization, which is in line with similar to findings in the natural language\nprocessing (NLP) domain. For reproducibility, Our codebase is shared at\nhttps://github.com/ZhiGroup/pytorch_ehr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasmy_L/0/1/0/all/0/1\">Laila Rasmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hong Thoai Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yujia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiryaki_F/0/1/0/all/0/1\">Firat Tiryaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_D/0/1/0/all/0/1\">Degui Zhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01013","description":"<p>Today's VQA models still tend to capture superficial linguistic correlations\nin the training set and fail to generalize to the test set with different QA\ndistributions. To reduce these language biases, recent VQA works introduce an\nauxiliary question-only model to regularize the training of targeted VQA model,\nand achieve dominating performance on diagnostic benchmarks for\nout-of-distribution testing. However, due to complex model design, these\nensemble-based methods are unable to equip themselves with two indispensable\ncharacteristics of an ideal VQA model: 1) Visual-explainable: The model should\nrely on the right visual regions when making decisions. 2) Question-sensitive:\nThe model should be sensitive to the linguistic variations in questions. To\nthis end, we propose a novel model-agnostic Counterfactual Samples Synthesizing\nand Training (CSST) strategy. After training with CSST, VQA models are forced\nto focus on all critical objects and words, which significantly improves both\nvisual-explainable and question-sensitive abilities. Specifically, CSST is\ncomposed of two parts: Counterfactual Samples Synthesizing (CSS) and\nCounterfactual Samples Training (CST). CSS generates counterfactual samples by\ncarefully masking critical objects in images or words in questions and\nassigning pseudo ground-truth answers. CST not only trains the VQA models with\nboth complementary samples to predict respective ground-truth answers, but also\nurges the VQA models to further distinguish the original samples and\nsuperficially similar counterfactual ones. To facilitate the CST training, we\npropose two variants of supervised contrastive loss for VQA, and design an\neffective positive and negative sample selection mechanism based on CSS.\nExtensive experiments have shown the effectiveness of CSST. Particularly, by\nbuilding on top of model LMH+SAR, we achieve record-breaking performance on all\nOOD benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuhang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Project Debater APIs: Decomposing the AI Grand Challenge. (arXiv:2110.01029v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01029","description":"<p>Project Debater was revealed in 2019 as the first AI system that can debate\nhuman experts on complex topics. Engaging in a live debate requires a diverse\nset of skills, and Project Debater has been developed accordingly as a\ncollection of components, each designed to perform a specific subtask. Project\nDebater APIs provide access to many of these capabilities, as well as to more\nrecently developed ones. This diverse set of web services, publicly available\nfor academic use, includes core NLP services, argument mining and analysis\ncapabilities, and higher-level services for content summarization. We describe\nthese APIs and their performance, and demonstrate how they can be used for\nbuilding practical solutions. In particular, we will focus on Key Point\nAnalysis, a novel technology that identifies the main points and their\nprevalence in a collection of texts such as survey responses and user reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bar_Haim_R/0/1/0/all/0/1\">Roy Bar-Haim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantor_Y/0/1/0/all/0/1\">Yoav Kantor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Document Keyphrase Extraction: A Literature Review and the First Dataset. (arXiv:2110.01073v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01073","description":"<p>Keyphrase extraction has been comprehensively researched within the\nsingle-document setting, with an abundance of methods and a wealth of datasets.\nIn contrast, multi-document keyphrase extraction has been infrequently studied,\ndespite its utility for describing sets of documents, and its use in\nsummarization. Moreover, no dataset existed for multi-document keyphrase\nextraction, hindering the progress of the task. Recent advances in multi-text\nprocessing make the task an even more appealing challenge to pursue. To\ninitiate this pursuit, we present here the first literature review and the\nfirst dataset for the task, MK-DUC-01, which can serve as a new benchmark. We\ntest several keyphrase extraction baselines on our data and show their results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shapira_O/0/1/0/all/0/1\">Ori Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amsterdamer_Y/0/1/0/all/0/1\">Yael Amsterdamer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Voice-Activated Framework using Self-supervised Learning. (arXiv:2110.01077v1 [eess.AS])","link":"http://arxiv.org/abs/2110.01077","description":"<p>Self-supervised learning methods such as wav2vec 2.0 have shown promising\nresults in learning speech representations from unlabelled and untranscribed\nspeech data that are useful for speech recognition. Since these representations\nare learned without any task-specific supervision, they can also be useful for\nother voice-activated tasks like speaker verification, keyword spotting,\nemotion classification etc. In our work, we propose a general purpose framework\nfor adapting a pre-trained wav2vec 2.0 model for different voice-activated\ntasks. We develop downstream network architectures that operate on the\ncontextualized speech representations of wav2vec 2.0 to adapt the\nrepresentations for solving a given task. Finally, we extend our framework to\nperform multi-task learning by jointly optimizing the network parameters on\nmultiple voice activated tasks using a shared transformer backbone. Both of our\nsingle and multi-task frameworks achieve state-of-the-art results in speaker\nverification and keyword spotting benchmarks. Our best performing models\nachieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on VoxCeleb2 and\nVoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0\nkeyword spotting dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1\">Shehzeen Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visser_E/0/1/0/all/0/1\">Erik Visser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Persuasion in Computational Argumentation. (arXiv:2110.01078v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01078","description":"<p>Opinion formation and persuasion in argumentation are affected by three major\nfactors: the argument itself, the source of the argument, and the properties of\nthe audience. Understanding the role of each and the interplay between them is\ncrucial for obtaining insights regarding argument interpretation and\ngeneration. It is particularly important for building effective argument\ngeneration systems that can take both the discourse and the audience\ncharacteristics into account. Having such personalized argument generation\nsystems would be helpful to expose individuals to different viewpoints and help\nthem make a more fair and informed decision on an issue. Even though studies in\nSocial Sciences and Psychology have shown that source and audience effects are\nessential components of the persuasion process, most research in computational\npersuasion has focused solely on understanding the characteristics of\npersuasive language. In this thesis, we make several contributions to\nunderstand the relative effect of the source, audience, and language in\ncomputational persuasion. We first introduce a large-scale dataset with\nextensive user information to study these factors' effects simultaneously.\nThen, we propose models to understand the role of the audience's prior beliefs\non their perception of arguments. We also investigate the role of social\ninteractions and engagement in understanding users' success in online debating\nover time. We find that the users' prior beliefs and social interactions play\nan essential role in predicting their success in persuasion. Finally, we\nexplore the importance of incorporating contextual information to predict\nargument impact and show improvements compared to encoding only the text of the\narguments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Examples Generation for Reducing Implicit Gender Bias in Pre-trained Models. (arXiv:2110.01094v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01094","description":"<p>Over the last few years, Contextualized Pre-trained Neural Language Models,\nsuch as BERT, GPT, have shown significant gains in various NLP tasks. To\nenhance the robustness of existing pre-trained models, one way is adversarial\nexamples generation and evaluation for conducting data augmentation or\nadversarial learning. In the meanwhile, gender bias embedded in the models\nseems to be a serious problem in practical applications. Many researches have\ncovered the gender bias produced by word-level information(e.g.\ngender-stereotypical occupations), while few researchers have investigated the\nsentence-level cases and implicit cases.\n</p>\n<p>In this paper, we proposed a method to automatically generate implicit gender\nbias samples at sentence-level and a metric to measure gender bias. Samples\ngenerated by our method will be evaluated in terms of accuracy. The metric will\nbe used to guide the generation of examples from Pre-trained models. Therefore,\nthose examples could be used to impose attacks on Pre-trained Models. Finally,\nwe discussed the evaluation efficacy of our generated examples on reducing\ngender bias for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wenqian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaojia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cassie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+A_J/0/1/0/all/0/1\">Ji A</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Language Models for Understanding of Temporal Expressions. (arXiv:2110.01113v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01113","description":"<p>We present three Natural Language Inference (NLI) challenge sets that can\nevaluate NLI models on their understanding of temporal expressions. More\nspecifically, we probe these models for three temporal properties: (a) the\norder between points in time, (b) the duration between two points in time, (c)\nthe relation between the magnitude of times specified in different units. We\nfind that although large language models fine-tuned on MNLI have some basic\nperception of the order between points in time, at large, these models do not\nhave a thorough understanding of the relation between temporal expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thukral_S/0/1/0/all/0/1\">Shivin Thukral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukreja_K/0/1/0/all/0/1\">Kunal Kukreja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavouras_C/0/1/0/all/0/1\">Christian Kavouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured abbreviation expansion in context. (arXiv:2110.01140v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01140","description":"<p>Ad hoc abbreviations are commonly found in informal communication channels\nthat favor shorter messages. We consider the task of reversing these\nabbreviations in context to recover normalized, expanded versions of\nabbreviated messages. The problem is related to, but distinct from, spelling\ncorrection, in that ad hoc abbreviations are intentional and may involve\nsubstantial differences from the original words. Ad hoc abbreviations are\nproductively generated on-the-fly, so they cannot be resolved solely by\ndictionary lookup. We generate a large, open-source data set of ad hoc\nabbreviations. This data is used to study abbreviation strategies and to\ndevelop two strong baselines for abbreviation expansion\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorman_K/0/1/0/all/0/1\">Kyle Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirov_C/0/1/0/all/0/1\">Christo Kirov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roark_B/0/1/0/all/0/1\">Brian Roark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1\">Richard Sproat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Interplay Between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis. (arXiv:2110.01147v1 [cs.SD])","link":"http://arxiv.org/abs/2110.01147","description":"<p>Are end-to-end text-to-speech (TTS) models over-parametrized? To what extent\ncan these models be pruned, and what happens to their synthesis capabilities?\nThis work serves as a starting point to explore pruning both spectrogram\nprediction networks and vocoders. We thoroughly investigate the tradeoffs\nbetween sparstiy and its subsequent effects on synthetic speech. Additionally,\nwe explored several aspects of TTS pruning: amount of finetuning data versus\nsparsity, TTS-Augmentation to utilize unspoken text, and combining knowledge\ndistillation and pruning. Our findings suggest that not only are end-to-end TTS\nmodels highly prunable, but also, perhaps surprisingly, pruned TTS models can\nproduce synthetic speech with equal or higher naturalness and intelligibility,\nwith similar prosody. All of our experiments are conducted on publicly\navailable models, and findings in this work are backed by large-scale\nsubjective tests and objective measures. Code and 200 pruned models are made\navailable to facilitate future research on efficiency in TTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_E/0/1/0/all/0/1\">Erica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kaizhi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi-Lun Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts. (arXiv:2110.01159v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01159","description":"<p>Recent models in developing summarization systems consist of millions of\nparameters and the model performance is highly dependent on the abundance of\ntraining data. While most existing summarization corpora contain data in the\norder of thousands to one million, generation of large-scale summarization\ndatasets in order of couple of millions is yet to be explored. Practically,\nmore data is better at generalizing the training patterns to unseen data. In\nthis paper, we introduce TLDR9+ -- a large-scale summarization dataset --\ncontaining over 9 million training instances extracted from Reddit discussion\nforum (https://github.com/sajastu/reddit_collector). This dataset is\nspecifically gathered to perform extreme summarization (i.e., generating\none-sentence summary in high compression and abstraction) and is more than\ntwice larger than the previously proposed dataset. We go one step further and\nwith the help of human annotations, we distill a more fine-grained dataset by\nsampling High-Quality instances from TLDR9+ and call it TLDRHQ dataset. We\nfurther pinpoint different state-of-the-art summarization models on our\nproposed datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sotudeh_S/0/1/0/all/0/1\">Sajad Sotudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deilamsalehi_H/0/1/0/all/0/1\">Hanieh Deilamsalehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1\">Nazli Goharian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Topics: Discovering Latent Healthcare Objectives from Event Sequences. (arXiv:2110.01160v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01160","description":"<p>A meaningful understanding of clinical protocols and patient pathways helps\nimprove healthcare outcomes. Electronic health records (EHR) reflect real-world\ntreatment behaviours that are used to enhance healthcare management but present\nchallenges; protocols and pathways are often loosely defined and with elements\nfrequently not recorded in EHRs, complicating the enhancement. To solve this\nchallenge, healthcare objectives associated with healthcare management\nactivities can be indirectly observed in EHRs as latent topics. Topic models,\nsuch as Latent Dirichlet Allocation (LDA), are used to identify latent patterns\nin EHR data. However, they do not examine the ordered nature of EHR sequences,\nnor do they appraise individual events in isolation. Our novel approach, the\nCategorical Sequence Encoder (CaSE) addresses these shortcomings. The\nsequential nature of EHRs is captured by CaSE's event-level representations,\nrevealing latent healthcare objectives. In synthetic EHR sequences, CaSE\noutperforms LDA by up to 37% at identifying healthcare objectives. In the\nreal-world MIMIC-III dataset, CaSE identifies meaningful representations that\ncould critically enhance protocol and pathway development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caruana_A/0/1/0/all/0/1\">Adrian Caruana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandara_M/0/1/0/all/0/1\">Madhushi Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catchpoole_D/0/1/0/all/0/1\">Daniel Catchpoole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_P/0/1/0/all/0/1\">Paul J Kennedy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Metric for Evaluating Semantics Preservation. (arXiv:2110.01176v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01176","description":"<p>In this paper, we leverage pre-trained language models (PLMs) to precisely\nevaluate the semantics preservation of edition process on sentences. Our\nmetric, Neighbor Distribution Divergence (NDD), evaluates the disturbance on\npredicted distribution of neighboring words from mask language model (MLM). NDD\nis capable of detecting precise changes in semantics which are easily ignored\nby text similarity. By exploiting the property of NDD, we implement a\nunsupervised and even training-free algorithm for extractive sentence\ncompression. We show that our NDD-based algorithm outperforms previous\nperplexity-based unsupervised algorithm by a large margin. For further\nexploration on interpretability, we evaluate NDD by pruning on syntactic\ndependency treebanks and apply NDD for predicate detection as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The state-of-the-art in text-based automatic personality prediction. (arXiv:2110.01186v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01186","description":"<p>Personality detection is an old topic in psychology and Automatic Personality\nPrediction (or Perception) (APP) is the automated (computationally) forecasting\nof the personality on different types of human generated/exchanged contents\n(such as text, speech, image, video). The principal objective of this study is\nto offer a shallow (overall) review of natural language processing approaches\non APP since 2010. With the advent of deep learning and following it\ntransfer-learning and pre-trained model in NLP, APP research area has been a\nhot topic, so in this review, methods are categorized into three; pre-trained\nindependent, pre-trained model based, multimodal approaches. Also, to achieve a\ncomprehensive comparison, reported results are informed by datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1\">Ali-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1\">Narjes Nikzad-Khasmakhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1\">Meysam Asgari-Chenaghlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akan_T/0/1/0/all/0/1\">Taymaz Akan</a> (Rahkar-Farshi), <a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1\">Mehrdad Ranjbar-Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafarni_Moattar_E/0/1/0/all/0/1\">Elnaz Zafarni-Moattar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Naghadeh_Z/0/1/0/all/0/1\">Zoleikha Jahanbakhsh-Naghadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LawSum: A weakly supervised approach for Indian Legal Document Summarization. (arXiv:2110.01188v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01188","description":"<p>Unlike the courts in western countries, public records of Indian judiciary\nare completely unstructured and noisy. No large scale publicly available\nannotated datasets of Indian legal documents exist till date. This limits the\nscope for legal analytics research. In this work, we propose a new dataset\nconsisting of over 10,000 judgements delivered by the supreme court of India\nand their corresponding hand written summaries. The proposed dataset is\npre-processed by normalising common legal abbreviations, handling spelling\nvariations in named entities, handling bad punctuations and accurate sentence\ntokenization. Each sentence is tagged with their rhetorical roles. We also\nannotate each judgement with several attributes like date, names of the\nplaintiffs, defendants and the people representing them, judges who delivered\nthe judgement, acts/statutes that are cited and the most common citations used\nto refer the judgement. Further, we propose an automatic labelling technique\nfor identifying sentences which have summary worthy information. We demonstrate\nthat this auto labeled data can be used effectively to train a weakly\nsupervised sentence extractor with high accuracy. Some possible applications of\nthis dataset besides legal document summarization can be in retrieval, citation\nanalysis and prediction of decisions by a particular judge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_V/0/1/0/all/0/1\">Vedant Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_V/0/1/0/all/0/1\">Vidit Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metha_P/0/1/0/all/0/1\">Parth Metha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_N/0/1/0/all/0/1\">Nimita Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention. (arXiv:2006.03654v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.03654","description":"<p>Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models' generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Named Entity Recognition for Kazakh. (arXiv:2007.13626v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2007.13626","description":"<p>We present several neural networks to address the task of named entity\nrecognition for morphologically complex languages (MCL). Kazakh is a\nmorphologically complex language in which each root/stem can produce hundreds\nor thousands of variant word forms. This nature of the language could lead to a\nserious data sparsity problem, which may prevent the deep learning models from\nbeing well trained for under-resourced MCLs. In order to model the MCLs' words\neffectively, we introduce root and entity tag embedding plus tensor layer to\nthe neural networks. The effects of those are significant for improving NER\nmodel performance of MCLs. The proposed models outperform state-of-the-art\nincluding character-based approaches, and can be potentially applied to other\nmorphologically complex languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolegen_G/0/1/0/all/0/1\">Gulmira Tolegen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toleu_A/0/1/0/all/0/1\">Alymzhan Toleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamyrbayev_O/0/1/0/all/0/1\">Orken Mamyrbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mussabayev_R/0/1/0/all/0/1\">Rustam Mussabayev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Text Generation with Pattern-Exploiting Training. (arXiv:2012.11926v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.11926","description":"<p>Providing pretrained language models with simple task descriptions in natural\nlanguage enables them to solve some tasks in a fully unsupervised fashion.\nMoreover, when combined with regular learning from examples, this idea yields\nimpressive few-shot results for a wide range of text classification tasks. It\nis also a promising direction to improve data efficiency in generative\nsettings, but there are several challenges to using a combination of task\ndescriptions and example-based learning for text generation. In particular, it\nis crucial to find task descriptions that are easy to understand for the\npretrained model and to ensure that it actually makes good use of them;\nfurthermore, effective measures against overfitting have to be implemented. In\nthis paper, we show how these challenges can be tackled: We introduce GenPET, a\nmethod for text generation that is based on pattern-exploiting training, a\nrecent approach for combining textual instructions with supervised learning\nthat only works for classification tasks. On several summarization and headline\ngeneration datasets, GenPET gives consistent improvements over strong baselines\nin few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast WordPiece Tokenization. (arXiv:2012.15524v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15524","description":"<p>Tokenization is a fundamental preprocessing step for almost all NLP tasks. In\nthis paper, we propose efficient algorithms for the WordPiece tokenization used\nin BERT, from single-word tokenization to general text (e.g., sentence)\ntokenization. When tokenizing a single word, WordPiece uses a\nlongest-match-first strategy, known as maximum matching. The best known\nalgorithms so far are O(n^2) (where n is the input length) or O(nm) (where m is\nthe maximum vocabulary token length). We propose a novel algorithm whose\ntokenization complexity is strictly O(n). Our method is inspired by the\nAho-Corasick algorithm. We introduce additional linkages on top of the trie\nbuilt from the vocabulary, allowing smart transitions when the trie matching\ncannot continue. For general text, we further propose an algorithm that\ncombines pre-tokenization (splitting the text into words) and our linear-time\nWordPiece method into a single pass. Experimental results show that our method\nis 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text\non average for general text tokenization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salcianu_A/0/1/0/all/0/1\">Alex Salcianu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dopson_D/0/1/0/all/0/1\">Dave Dopson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vy\\=akarana: A Colorless Green Benchmark for Syntactic Evaluation in Indic Languages. (arXiv:2103.00854v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.00854","description":"<p>While there has been significant progress towards developing NLU resources\nfor Indic languages, syntactic evaluation has been relatively less explored.\nUnlike English, Indic languages have rich morphosyntax, grammatical genders,\nfree linear word-order, and highly inflectional morphology. In this paper, we\nintroduce Vy\\=akarana: a benchmark of Colorless Green sentences in Indic\nlanguages for syntactic evaluation of multilingual language models. The\nbenchmark comprises four syntax-related tasks: PoS Tagging, Syntax Tree-depth\nPrediction, Grammatical Case Marking, and Subject-Verb Agreement. We use the\ndatasets from the evaluation tasks to probe five multilingual language models\nof varying architectures for syntax in Indic languages. Due to its prevalence,\nwe also include a code-switching setting in our experiments. Our results show\nthat the token-level and sentence-level representations from the Indic language\nmodels (IndicBERT and MuRIL) do not capture the syntax in Indic languages as\nefficiently as the other highly multilingual language models. Further, our\nlayer-wise probing experiments reveal that while mBERT, DistilmBERT, and XLM-R\nlocalize the syntax in middle layers, the Indic language models do not show\nsuch syntactic localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_J/0/1/0/all/0/1\">Jasleen Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahurkar_S/0/1/0/all/0/1\">Siddhant Mahurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1\">Saumitra Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malhotra_M/0/1/0/all/0/1\">Manav Malhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1\">Veeky Baths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Mandarin-English Code-switching Speech Recognition. (arXiv:2104.02258v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.02258","description":"<p>Mandarin-English code-switching (CS) is frequently used among East and\nSoutheast Asian people. However, the intra-sentence language switching of the\ntwo very different languages makes recognizing CS speech challenging.\nMeanwhile, the recent successful non-autoregressive (NAR) ASR models remove the\nneed for left-to-right beam decoding in autoregressive (AR) models and achieved\noutstanding performance and fast inference speed, but it has not been applied\nto Mandarin-English CS speech recognition. This paper takes advantage of the\nMask-CTC NAR ASR framework to tackle the CS speech recognition issue. We\nfurther propose to change the Mandarin output target of the encoder to Pinyin\nfor faster encoder training and introduce the Pinyin-to-Mandarin decoder to\nlearn contextualized information. Moreover, we use word embedding label\nsmoothing to regularize the decoder with contextualized information and\nprojection matrix regularization to bridge that gap between the encoder and\ndecoder. We evaluate these methods on the SEAME corpus and achieved exciting\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1\">Shun-Po Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sung-Feng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Datasets with Pretrained Language Models. (arXiv:2104.07540v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07540","description":"<p>To obtain high-quality sentence embeddings from pretrained language models\n(PLMs), they must either be augmented with additional pretraining objectives or\nfinetuned on a large set of labeled text pairs. While the latter approach\ntypically outperforms the former, it requires great human effort to generate\nsuitable datasets of sufficient size. In this paper, we show how PLMs can be\nleveraged to obtain high-quality sentence embeddings without the need for\nlabeled data, finetuning or modifications to the pretraining objective: We\nutilize the generative abilities of large and high-performing PLMs to generate\nentire datasets of labeled text pairs from scratch, which we then use for\nfinetuning much smaller and more efficient models. Our fully unsupervised\napproach outperforms strong baselines on several semantic textual similarity\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation. (arXiv:2104.08200v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08200","description":"<p>A benchmark provides an ecosystem to measure the advancement of models with\nstandard datasets and automatic and human evaluation metrics. We introduce\nIndoNLG, the first such benchmark for the Indonesian language for natural\nlanguage generation (NLG). It covers six tasks: summarization, question\nanswering, open chitchat, as well as three different language-pairs of machine\ntranslation tasks. We provide a vast and clean pre-training corpus of\nIndonesian, Sundanese, and Javanese datasets called Indo4B-Plus, which is used\nto train our pre-trained NLG model, IndoBART. We evaluate the effectiveness and\nefficiency of IndoBART by conducting extensive evaluation on all IndoNLG tasks.\nOur findings show that IndoBART achieves competitive performance on Indonesian\ntasks with five times fewer parameters compared to the largest multilingual\nmodel in our benchmark, mBART-LARGE (Liu et al., 2020), and an almost 4x and\n2.5x faster inference time on the CPU and GPU respectively. We additionally\ndemonstrate the ability of IndoBART to learn Javanese and Sundanese, and it\nachieves decent performance on machine translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincentio_K/0/1/0/all/0/1\">Karissa Vincentio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_Z/0/1/0/all/0/1\">Zhi Yuan Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahar_S/0/1/0/all/0/1\">Syafri Bahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodra_M/0/1/0/all/0/1\">Masayu Leylia Khodra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Imitating Metrics for Training and Evaluating Privacy Preserving Emotion Recognition Models Using Sociolinguistic Knowledge. (arXiv:2104.08792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08792","description":"<p>Privacy preservation is a crucial component of any real-world application.\nBut, in applications relying on machine learning backends, privacy is\nchallenging because models often capture more than what the model was initially\ntrained for, resulting in the potential leakage of sensitive information. In\nthis paper, we propose an automatic and quantifiable metric that allows us to\nevaluate humans' perception of a model's ability to preserve privacy with\nrespect to sensitive variables. In this paper, we focus on saliency-based\nexplanations, explanations that highlight regions of the input text, to infer\ninternal workings of a black box model. We use the degree with which\ndifferences in interpretation of general vs privacy preserving models correlate\nwith sociolinguistic biases to inform metric design. We show how certain\ncommonly-used methods that seek to preserve privacy do not align with human\nperception of privacy preservation leading to distrust about model's claims. We\ndemonstrate the versatility of our proposed metric by validating its utility\nfor measuring cross corpus generalization for both privacy and emotion.\nFinally, we conduct crowdsourcing experiments to evaluate the inclination of\nthe evaluators to choose a particular model for a given purpose when model\nexplanations are provided, and show a positive relationship with the proposed\nmetric. To the best of our knowledge, we take the first step in proposing\nautomatic and quantifiable metrics that best align with human perception of\nmodel's ability for privacy preservation, allowing for cost-effective model\ndevelopment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_M/0/1/0/all/0/1\">Mimansa Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Provost_E/0/1/0/all/0/1\">Emily Mower Provost</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks. (arXiv:2104.08815v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08815","description":"<p>Increasing concerns and regulations about data privacy and sparsity\nnecessitate the study of privacy-preserving, decentralized learning methods for\nnatural language processing (NLP) tasks. Federated learning (FL) provides\npromising approaches for a large number of clients (e.g., personal devices or\norganizations) to collaboratively learn a shared global model to benefit all\nclients while allowing users to keep their data locally. Despite interest in\nstudying FL methods for NLP tasks, a systematic comparison and analysis is\nlacking in the literature. Herein, we present the FedNLP, a benchmarking\nframework for evaluating federated learning methods on four different task\nformulations: text classification, sequence tagging, question answering, and\nseq2seq. We propose a universal interface between Transformer-based language\nmodels (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under\nvarious non-IID partitioning strategies. Our extensive experiments with FedNLP\nprovide empirical comparisons between FL methods and helps us better understand\nthe inherent challenges of this direction. The comprehensive analysis points to\nintriguing and exciting future research aimed at developing FL methods for NLP\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zihang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Gender by First Name Using Character-level Machine Learning. (arXiv:2106.10156v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.10156","description":"<p>Predicting gender by the first name is not a simple task. In many\napplications, especially in the natural language processing (NLP) field, this\ntask may be necessary, mainly when considering foreign names. In this paper, we\nexamined and implemented several machine learning algorithms, such as extra\ntrees, KNN, Naive Bayes, SVM, random forest, gradient boosting, light GBM,\nlogistic regression, ridge classifier, and deep neural network models, such as\nMLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first name. A\ndataset of Brazilian names is used to train and evaluate the models. We\nanalyzed the accuracy, recall, precision, f1 score, and confusion matrix to\nmeasure the models' performances. The results indicate that the gender\nprediction can be performed from the feature extraction strategy looking at the\nnames as a set of strings. Some models accurately predict gender in more than\n95% of the cases. The recurrent models overcome the feedforward models in this\nbinary classification problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1\">Rosana C. B. Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Ver&#xf4;nica M. L. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_V/0/1/0/all/0/1\">Victor M. Fernandes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03228","description":"<p>Recent studies on compression of pretrained language models (e.g., BERT)\nusually use preserved accuracy as the metric for evaluation. In this paper, we\npropose two new metrics, label loyalty and probability loyalty that measure how\nclosely a compressed model (i.e., student) mimics the original model (i.e.,\nteacher). We also explore the effect of compression with regard to robustness\nunder adversarial attacks. We benchmark quantization, pruning, knowledge\ndistillation and progressive module replacing with loyalty and robustness. By\ncombining multiple compression techniques, we provide a practical strategy to\nachieve better accuracy, loyalty and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Search for a Search Method -- Simple Heuristics Suffice for Adversarial Text Attacks. (arXiv:2109.07926v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07926","description":"<p>Recently more attention has been given to adversarial attacks on neural\nnetworks for natural language processing (NLP). A central research topic has\nbeen the investigation of search algorithms and search constraints, accompanied\nby benchmark algorithms and tasks. We implement an algorithm inspired by zeroth\norder optimization-based attacks and compare with the benchmark results in the\nTextAttack framework. Surprisingly, we find that optimization-based methods do\nnot yield any improvement in a constrained setup and slightly benefit from\napproximate gradient information only in unconstrained setups where search\nspaces are larger. In contrast, simple heuristics exploiting nearest neighbors\nwithout querying the target function yield substantial success rates in\nconstrained setups, and nearly full success rate in unconstrained setups, at an\norder of magnitude fewer queries. We conclude from these results that current\nTextAttack benchmark tasks are too easy and constraints are too strict,\npreventing meaningful research on black-box adversarial text attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berger_N/0/1/0/all/0/1\">Nathaniel Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_S/0/1/0/all/0/1\">Sebastian Ebert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08722","description":"<p>Prerequisite chain learning helps people acquire new knowledge efficiently.\nWhile people may quickly determine learning paths over concepts in a domain,\nfinding such paths in other domains can be challenging. We introduce\nDomain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this\ncross-domain prerequisite chain learning task efficiently. Our novel model\nconsists of a variational graph autoencoder (VGAE) and a domain discriminator.\nThe VGAE is trained to predict concept relations through link prediction, while\nthe domain discriminator takes both source and target domain data as input and\nis trained to predict domain labels. Most importantly, this method only needs\nsimple homogeneous graphs as input, compared with the current state-of-the-art\nmodel. We evaluate our model on the LectureBankCD dataset, and results show\nthat our model outperforms recent graph-based benchmarks while using only 1/10\nof graph scale and 1/3 computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From None to Severe: Predicting Severity in Movie Scripts. (arXiv:2109.09276v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09276","description":"<p>In this paper, we introduce the task of predicting severity of age-restricted\naspects of movie content based solely on the dialogue script. We first\ninvestigate categorizing the ordinal severity of movies on 5 aspects: Sex,\nViolence, Profanity, Substance consumption, and Frightening scenes. The problem\nis handled using a siamese network-based multitask framework which concurrently\nimproves the interpretability of the predictions. The experimental results show\nthat our method outperforms the previous state-of-the-art model and provides\nuseful information to interpret model predictions. The proposed dataset and\nsource code are publicly available at our GitHub repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yigeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_M/0/1/0/all/0/1\">Mahsa Shafaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation. (arXiv:2109.10164v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10164","description":"<p>Intermediate layer knowledge distillation (KD) can improve the standard KD\ntechnique (which only targets the output of teacher and student models)\nespecially over large pre-trained language models. However, intermediate layer\ndistillation suffers from excessive computational burdens and engineering\nefforts required for setting up a proper layer mapping. To address these\nproblems, we propose a RAndom Intermediate Layer Knowledge Distillation\n(RAIL-KD) approach in which, intermediate layers from the teacher model are\nselected randomly to be distilled into the intermediate layers of the student\nmodel. This randomized selection enforce that: all teacher layers are taken\ninto account in the training process, while reducing the computational cost of\nintermediate layer distillation. Also, we show that it act as a regularizer for\nimproving the generalizability of the student model. We perform extensive\nexperiments on GLUE tasks as well as on out-of-domain test sets. We show that\nour proposed RAIL-KD approach outperforms other state-of-the-art intermediate\nlayer KD methods considerably in both performance and training-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haidar_M/0/1/0/all/0/1\">Md Akmal Haidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anchuri_N/0/1/0/all/0/1\">Nithin Anchuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1\">Pascal Poupart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords. (arXiv:2109.11491v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11491","description":"<p>We present a method for exploring regions around individual points in a\ncontextualized vector space (particularly, BERT space), as a way to investigate\nhow these regions correspond to word senses. By inducing a contextualized\n\"pseudoword\" as a stand-in for a static embedding in the input layer, and then\nperforming masked prediction of a word in the sentence, we are able to\ninvestigate the geometry of the BERT-space in a controlled manner around\nindividual instances. Using our method on a set of carefully constructed\nsentences targeting ambiguous English words, we find substantial regularity in\nthe contextualized space, with regions that correspond to distinct word senses;\nbut between these regions there are occasionally \"sense voids\" -- regions that\ndo not correspond to any intelligible sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1\">Taelin Karidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT got a Date: Introducing Transformers to Temporal Tagging. (arXiv:2109.14927v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14927","description":"<p>Temporal expressions in text play a significant role in language\nunderstanding and correctly identifying them is fundamental to various\nretrieval and natural language processing systems. Previous works have slowly\nshifted from rule-based to neural architectures, capable of tagging expressions\nwith higher accuracy. However, neural models can not yet distinguish between\ndifferent expression types at the same level as their rule-based counterparts.\nIn this work, we aim to identify the most suitable transformer architecture for\njoint temporal tagging and type classification, as well as, investigating the\neffect of semi-supervised training on the performance of these systems. Based\non our study of token classification variants and encoder-decoder\narchitectures, we present a transformer encoder-decoder model using the RoBERTa\nlanguage model as our best performing system. By supplementing training\nresources with weakly labeled data from rule-based systems, our model surpasses\nprevious works in temporal tagging and type classification, especially on rare\nclasses. Our code and pre-trained experiments are available at:\nhttps://github.com/satya77/Transformer_Temporal_Tagger\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almasian_S/0/1/0/all/0/1\">Satya Almasian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aumiller_D/0/1/0/all/0/1\">Dennis Aumiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_M/0/1/0/all/0/1\">Michael Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Reconstructing group wavelet transform from feature maps with a reproducing kernel iteration. (arXiv:2110.00600v1 [math.NA])","link":"http://arxiv.org/abs/2110.00600","description":"<p>In this paper we consider the problem of reconstructing an image that is\ndownsampled in the space of its $SE(2)$ wavelet transform, which is motivated\nby classical models of simple cells receptive fields and feature preference\nmaps in primary visual cortex. We prove that, whenever the problem is solvable,\nthe reconstruction can be obtained by an elementary project and replace\niterative scheme based on the reproducing kernel arising from the group\nstructure, and show numerical results on real images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Barbieri_D/0/1/0/all/0/1\">Davide Barbieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithm Fairness in AI for Medicine and Healthcare. (arXiv:2110.00603v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00603","description":"<p>In the current development and deployment of many artificial intelligence\n(AI) systems in healthcare, algorithm fairness is a challenging problem in\ndelivering equitable care. Recent evaluation of AI models stratified across\nrace sub-populations have revealed enormous inequalities in how patients are\ndiagnosed, given treatments, and billed for healthcare costs. In this\nperspective article, we summarize the intersectional field of fairness in\nmachine learning through the context of current issues in healthcare, outline\nhow algorithmic biases (e.g. - image acquisition, genetic variation,\nintra-observer labeling variability) arise in current clinical workflows and\ntheir resulting healthcare disparities. Lastly, we also review emerging\nstrategies for mitigating bias via decentralized learning, disentanglement, and\nmodel explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipkova_J/0/1/0/all/0/1\">Jana Lipkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Judy J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F.K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahai_S/0/1/0/all/0/1\">Sharifa Sahai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPEC: Seeing People in the Wild with an Estimated Camera. (arXiv:2110.00620v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00620","description":"<p>Due to the lack of camera parameter information for in-the-wild images,\nexisting 3D human pose and shape (HPS) estimation methods make several\nsimplifying assumptions: weak-perspective projection, large constant focal\nlength, and zero camera rotation. These assumptions often do not hold and we\nshow, quantitatively and qualitatively, that they cause errors in the\nreconstructed 3D shape and pose. To address this, we introduce SPEC, the first\nin-the-wild 3D HPS method that estimates the perspective camera from a single\nimage and employs this to reconstruct 3D human bodies more accurately. %regress\n3D human bodies. First, we train a neural network to estimate the field of\nview, camera pitch, and roll given an input image. We employ novel losses that\nimprove the calibration accuracy over previous work. We then train a novel\nnetwork that concatenates the camera calibration to the image features and uses\nthese together to regress 3D body shape and pose. SPEC is more accurate than\nthe prior art on the standard benchmark (3DPW) as well as two new datasets with\nmore challenging camera views and varying focal lengths. Specifically, we\ncreate a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D\nbodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and\nhigh-quality reference bodies. Both qualitative and quantitative analysis\nconfirm that knowing camera parameters during inference regresses better human\nbodies. Code and datasets are available for research purposes at\nhttps://spec.is.tue.mpg.de.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1\">Muhammed Kocabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chun-Hao P. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tesch_J/0/1/0/all/0/1\">Joachim Tesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lea M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoomStructNet: Learning to Rank Non-Cuboidal Room Layouts From Single View. (arXiv:2110.00644v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00644","description":"<p>In this paper, we present a new approach to estimate the layout of a room\nfrom its single image. While recent approaches for this task use robust\nfeatures learnt from data, they resort to optimization for detecting the final\nlayout. In addition to using learnt robust features, our approach learns an\nadditional ranking function to estimate the final layout instead of using\noptimization. To learn this ranking function, we propose a framework to train a\nCNN using max-margin structure cost. Also, while most approaches aim at\ndetecting cuboidal layouts, our approach detects non-cuboidal layouts for which\nwe explicitly estimates layout complexity parameters. We use these parameters\nto propose layout candidates in a novel way. Our approach shows\nstate-of-the-art results on standard datasets with mostly cuboidal layouts and\nalso performs well on a dataset containing rooms with non-cuboidal layouts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chun-Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kenan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yago_Vicente_T/0/1/0/all/0/1\">Tomas Yago-Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view SA-LA Net: A framework for simultaneous segmentation of RV on multi-view cardiac MR Images. (arXiv:2110.00682v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00682","description":"<p>We proposed a multi-view SA-LA model for simultaneous segmentation of RV on\nthe short-axis (SA) and long-axis (LA) cardiac MR images. The multi-view SA-LA\nmodel is a multi-encoder, multi-decoder U-Net architecture based on the U-Net\nmodel. One encoder-decoder pair segments the RV on SA images and the other pair\non LA images. Multi-view SA-LA model assembles an extremely rich set of\nsynergistic features, at the root of the encoder branch, by combining feature\nmaps learned from matched SA and LA cardiac MR images. Segmentation performance\nis further enhanced by: (1) incorporating spatial context of LV as a prior and\n(2) performing deep supervision in the last three layers of the decoder branch.\nMulti-view SA-LA model was extensively evaluated on the MICCAI 2021 Multi-\nDisease, Multi-View, and Multi- Centre RV Segmentation Challenge dataset\n(M&amp;Ms-2021). M&amp;Ms-2021 dataset consists of multi-phase, multi-view cardiac MR\nimages of 360 subjects acquired at four clinical centers with three different\nvendors. On the challenge cohort (160 subjects), the proposed multi-view SA-LA\nmodel achieved a Dice Score of 91% and Hausdorff distance of 11.2 mm on\nshort-axis images and a Dice Score of 89.6% and Hausdorff distance of 8.1 mm on\nlong-axis images. Moreover, multi-view SA-LA model exhibited strong\ngeneralization to unseen RV related pathologies including Dilated Right\nVentricle (DSC: SA 91.41%, LA 89.63%) and Tricuspidal Regurgitation (DSC: SA\n91.40%, LA 90.40%) with low variance (std_DSC: SA &lt;5%, LA&lt;6%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jabbar_S/0/1/0/all/0/1\">Sana Jabbar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bukhari_S/0/1/0/all/0/1\">Syed Talha Bukhari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohy_ud_Din_H/0/1/0/all/0/1\">Hassan Mohy-ud-Din</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Field Saliency Detection with Dual Local Graph Learning andReciprocative Guidance. (arXiv:2110.00698v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00698","description":"<p>The application of light field data in salient object de-tection is becoming\nincreasingly popular recently. The diffi-culty lies in how to effectively fuse\nthe features within the fo-cal stack and how to cooperate them with the feature\nof theall-focus image. Previous methods usually fuse focal stackfeatures via\nconvolution or ConvLSTM, which are both lesseffective and ill-posed. In this\npaper, we model the infor-mation fusion within focal stack via graph networks.\nTheyintroduce powerful context propagation from neighbouringnodes and also\navoid ill-posed implementations. On the onehand, we construct local graph\nconnections thus avoidingprohibitive computational costs of traditional graph\nnet-works. On the other hand, instead of processing the twokinds of data\nseparately, we build a novel dual graph modelto guide the focal stack fusion\nprocess using all-focus pat-terns. To handle the second difficulty, previous\nmethods usu-ally implement one-shot fusion for focal stack and\nall-focusfeatures, hence lacking a thorough exploration of their sup-plements.\nWe introduce a reciprocative guidance schemeand enable mutual guidance between\nthese two kinds of in-formation at multiple steps. As such, both kinds of\nfeaturescan be enhanced iteratively, finally benefiting the saliencyprediction.\nExtensive experimental results show that theproposed models are all beneficial\nand we achieve signif-icantly better results than state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wangbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Adversarial Spoofing Attacks against Face Recognition. (arXiv:2110.00708v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00708","description":"<p>We assess the vulnerabilities of deep face recognition systems for images\nthat falsify/spoof multiple identities simultaneously. We demonstrate that, by\nmanipulating the deep feature representation extracted from a face image via\nimperceptibly small perturbations added at the pixel level using our proposed\nUniversal Adversarial Spoofing Examples (UAXs), one can fool a face\nverification system into recognizing that the face image belongs to multiple\ndifferent identities with a high success rate. One characteristic of the UAXs\ncrafted with our method is that they are universal (identity-agnostic); they\nare successful even against identities not known in advance. For a certain deep\nneural network, we show that we are able to spoof almost all tested identities\n(99\\%), including those not known beforehand (not included in training). Our\nresults indicate that a multiple-identity attack is a real threat and should be\ntaken into account when deploying face recognition systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amada_T/0/1/0/all/0/1\">Takuma Amada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1\">Seng Pei Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakizaki_K/0/1/0/all/0/1\">Kazuya Kakizaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_T/0/1/0/all/0/1\">Toshinori Araki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking questions on handwritten document collections. (arXiv:2110.00711v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00711","description":"<p>This work addresses the problem of Question Answering (QA) on handwritten\ndocument collections. Unlike typical QA and Visual Question Answering (VQA)\nformulations where the answer is a short text, we aim to locate a document\nsnippet where the answer lies. The proposed approach works without recognizing\nthe text in the documents. We argue that the recognition-free approach is\nsuitable for handwritten documents and historical collections where robust text\nrecognition is often difficult. At the same time, for human users, document\nimage snippets containing answers act as a valid alternative to textual\nanswers. The proposed approach uses an off-the-shelf deep embedding network\nwhich can project both textual words and word images into a common sub-space.\nThis embedding bridges the textual and visual domains and helps us retrieve\ndocument snippets that potentially answer a question. We evaluate results of\nthe proposed approach on two new datasets: (i) HW-SQuAD: a synthetic,\nhandwritten document image counterpart of SQuAD1.0 dataset and (ii) BenthamQA:\na smaller set of QA pairs defined on documents from the popular Bentham\nmanuscripts collection. We also present a thorough analysis of the proposed\nrecognition-free approach compared to a recognition-based approach which uses\ntext recognized from the images using an OCR. Datasets presented in this work\nare available to download at docvqa.org\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">CV Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Optimization-Based Meta-Learning Model for MRI Reconstruction with Diverse Dataset. (arXiv:2110.00715v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00715","description":"<p>Purpose: This work aims at developing a generalizable MRI reconstruction\nmodel in the meta-learning framework. The standard benchmarks in meta-learning\nare challenged by learning on diverse task distributions. The proposed network\nlearns the regularization function in a variational model and reconstructs MR\nimages with various under-sampling ratios or patterns that may or may not be\nseen in the training data by leveraging a heterogeneous dataset. Methods: We\npropose an unrolling network induced by learnable optimization algorithms (LOA)\nfor solving our nonconvex nonsmooth variational model for MRI reconstruction.\nIn this model, the learnable regularization function contains a task-invariant\ncommon feature encoder and task-specific learner represented by a shallow\nnetwork. To train the network we split the training data into two parts:\ntraining and validation, and introduce a bilevel optimization algorithm. The\nlower-level optimization trains task-invariant parameters for the feature\nencoder with fixed parameters of the task-specific learner on the training\ndataset, and the upper-level optimizes the parameters of the task-specific\nlearner on the validation dataset. Results: The average PSNR increases\nsignificantly compared to the network trained through conventional supervised\nlearning on the seen CS ratios. We test the result of quick adaption on the\nunseen tasks after meta-training and in the meanwhile saving half of the\ntraining time; Conclusion: We proposed a meta-learning framework consisting of\nthe base network architecture, design of regularization, and bi-level\noptimization-based training. The network inherits the convergence property of\nthe LOA and interpretation of the variational model. The generalization ability\nis improved by the designated regularization and bilevel optimization-based\ntraining algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_W/0/1/0/all/0/1\">Wanyu Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingchao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Specific Bias Filtering for Single Labeled Domain Generalization. (arXiv:2110.00726v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00726","description":"<p>Domain generalization (DG) utilizes multiple labeled source datasets to train\na generalizable model for unseen target domains. However, due to expensive\nannotation costs, the requirements of labeling all the source data are hard to\nbe met in real-world applications. In this paper, we investigate a Single\nLabeled Domain Generalization (SLDG) task with only one source domain being\nlabeled, which is more practical and challenging than the Conventional Domain\nGeneralization (CDG). A major obstacle in the SLDG task is the\ndiscriminability-generalization bias: discriminative information in the labeled\nsource dataset may contain domain-specific bias, constraining the\ngeneralization of the trained model. To tackle this challenging task, we\npropose a novel method called Domain-Specific Bias Filtering (DSBF), which\ninitializes a discriminative model with the labeled source data and filters out\nits domain-specific bias with the unlabeled source data for generalization\nimprovement. We divide the filtering process into: (1) Feature extractor\ndebiasing using k-means clustering-based semantic feature re-extraction; and\n(2) Classifier calibrating using attention-guided semantic feature projection.\nDSBF unifies the exploration of the labeled and the unlabeled source data to\nenhance the discriminability and generalization of the trained model, resulting\nin a highly generalizable model. We further provide theoretical analysis to\nverify the proposed domain-specific bias filtering process. Extensive\nexperiments on multiple datasets show the superior performance of DSBF in\ntackling both the challenging SLDG task and the CDG task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junkun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FICGAN: Facial Identity Controllable GAN for De-identification. (arXiv:2110.00740v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00740","description":"<p>In this work, we present Facial Identity Controllable GAN (FICGAN) for not\nonly generating high-quality de-identified face images with ensured privacy\nprotection, but also detailed controllability on attribute preservation for\nenhanced data utility. We tackle the less-explored yet desired functionality in\nface de-identification based on the two factors. First, we focus on the\nchallenging issue to obtain a high level of privacy protection in the\nde-identification task while uncompromising the image quality. Second, we\nanalyze the facial attributes related to identity and non-identity and explore\nthe trade-off between the degree of face de-identification and preservation of\nthe source attributes for enhanced data utility. Based on the analysis, we\ndevelop Facial Identity Controllable GAN (FICGAN), an autoencoder-based\nconditional generative model that learns to disentangle the identity attributes\nfrom non-identity attributes on a face image. By applying the manifold k-same\nalgorithm to satisfy k-anonymity for strengthened security, our method achieves\nenhanced privacy protection in de-identified face images. Numerous experiments\ndemonstrate that our model outperforms others in various scenarios of face\nde-identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Youngmin Ro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_H/0/1/0/all/0/1\">Heonseok Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Event Recognition. (arXiv:2110.00755v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00755","description":"<p>The literature shows outstanding capabilities for CNNs in event recognition\nin images. However, fewer attempts are made to analyze the potential causes\nbehind the decisions of the models and exploring whether the predictions are\nbased on event-salient objects or regions? To explore this important aspect of\nevent recognition, in this work, we propose an explainable event recognition\nframework relying on Grad-CAM and an Xception architecture-based CNN model.\nExperiments are conducted on three large-scale datasets covering a diversified\nset of natural disasters, social, and sports events. Overall, the model showed\noutstanding generalization capabilities obtaining overall F1-scores of 0.91,\n0.94, and 0.97 on natural disasters, social, and sports events, respectively.\nMoreover, for subjective analysis of activation maps generated through Grad-CAM\nfor the predicted samples of the model, a crowdsourcing study is conducted to\nanalyze whether the model's predictions are based on event-related\nobjects/regions or not? The results of the study indicate that 78%, 84%, and\n78% of the model decisions on natural disasters, sports, and social events\ndatasets, respectively, are based onevent-related objects or regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1\">Imran Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gul_N/0/1/0/all/0/1\">Namra Gul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Talhat Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1\">Nasir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ala-Al-Fuqaha/0/1/0/all/0/1\">Ala-Al-Fuqaha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Seed Quality Testing System using GAN & Active Learning. (arXiv:2110.00777v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00777","description":"<p>Quality assessment of agricultural produce is a crucial step in minimizing\nfood stock wastage. However, this is currently done manually and often requires\nexpert supervision, especially in smaller seeds like corn. We propose a novel\ncomputer vision-based system for automating this process. We build a novel seed\nimage acquisition setup, which captures both the top and bottom views. Dataset\ncollection for this problem has challenges of data annotation costs/time and\nclass imbalance. We address these challenges by i.) using a Conditional\nGenerative Adversarial Network (CGAN) to generate real-looking images for the\nclasses with lesser images and ii.) annotate a large dataset with minimal\nexpert human intervention by using a Batch Active Learning (BAL) based\nannotation tool. We benchmark different image classification models on the\ndataset obtained. We are able to get accuracies of up to 91.6% for testing the\nphysical purity of seed samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagar_S/0/1/0/all/0/1\">Sandeep Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pani_P/0/1/0/all/0/1\">Prateek Pani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_R/0/1/0/all/0/1\">Raj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_G/0/1/0/all/0/1\">Girish Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking Visual Discomfort: Curiosity-driven Representations for Reinforcement Learning. (arXiv:2110.00784v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00784","description":"<p>Vision-based reinforcement learning (RL) is a promising approach to solve\ncontrol tasks involving images as the main observation. State-of-the-art RL\nalgorithms still struggle in terms of sample efficiency, especially when using\nimage observations. This has led to increased attention on integrating state\nrepresentation learning (SRL) techniques into the RL pipeline. Work in this\nfield demonstrates a substantial improvement in sample efficiency among other\nbenefits. However, to take full advantage of this paradigm, the quality of\nsamples used for training plays a crucial role. More importantly, the diversity\nof these samples could affect the sample efficiency of vision-based RL, but\nalso its generalization capability. In this work, we present an approach to\nimprove sample diversity for state representation learning. Our method enhances\nthe exploration capability of RL algorithms, by taking advantage of the SRL\nsetup. Our experiments show that our proposed approach boosts the visitation of\nproblematic states, improves the learned state representation, and outperforms\nthe baselines for all tested environments. These results are most apparent for\nenvironments where the baseline methods struggle. Even in simple environments,\nour method stabilizes the training, reduces the reward variance, and promotes\nsample efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aljalbout_E/0/1/0/all/0/1\">Elie Aljalbout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulmer_M/0/1/0/all/0/1\">Maximilian Ulmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference-InfoGAN: Inference Independence via Embedding Orthogonal Basis Expansion. (arXiv:2110.00788v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00788","description":"<p>Disentanglement learning aims to construct independent and interpretable\nlatent variables in which generative models are a popular strategy. InfoGAN is\na classic method via maximizing Mutual Information (MI) to obtain interpretable\nlatent variables mapped to the target space. However, it did not emphasize\nindependent characteristic. To explicitly infer latent variables with\ninter-independence, we propose a novel GAN-based disentanglement framework via\nembedding Orthogonal Basis Expansion (OBE) into InfoGAN network\n(Inference-InfoGAN) in an unsupervised way. Under the OBE module, one set of\northogonal basis can be adaptively found to expand arbitrary data with\nindependence property. To ensure the target-wise interpretable representation,\nwe add a consistence constraint between the expansion coefficients and latent\nvariables on the base of MI maximization. Additionally, we design an\nalternating optimization step on the consistence constraint and orthogonal\nrequirement updating, so that the training of Inference-InfoGAN can be more\nconvenient. Finally, experiments validate that our proposed OBE module obtains\nadaptive orthogonal basis, which can express better independent characteristics\nthan fixed basis expression of Discrete Cosine Transform (DCT). To depict the\nperformance in downstream tasks, we compared with the state-of-the-art\nGAN-based and even VAE-based approaches on different datasets. Our\nInference-InfoGAN achieves higher disentanglement score in terms of FactorVAE,\nSeparated Attribute Predictability (SAP), Mutual Information Gap (MIG) and\nVariation Predictability (VP) metrics without model fine-tuning. All the\nexperimental results illustrate that our method has inter-independence\ninference ability because of the OBE module, and provides a good trade-off\nbetween it and target-wise interpretability of latent variables via jointing\nthe alternating optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hongxiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jihao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoyan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuxiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Neural Network for Computer Vision task in Edge Device. (arXiv:2110.00791v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00791","description":"<p>The field of computer vision has grown very rapidly in the past few years due\nto networks like convolution neural networks and their variants. The memory\nrequired to store the model and computational expense are very high for such a\nnetwork limiting it to deploy on the edge device. Many times, applications rely\non the cloud but that makes it hard for working in real-time due to round-trip\ndelays. We overcome these problems by deploying the neural network on the edge\ndevice itself. The computational expense for edge devices is reduced by\nreducing the floating-point precision of the parameters in the model. After\nthis the memory required for the model decreases and the speed of the\ncomputation increases where the performance of the model is least affected.\nThis makes an edge device to predict from the neural network all by itself.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_R/0/1/0/all/0/1\">Ranjith M S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parameshwara_S/0/1/0/all/0/1\">S Parameshwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+A_P/0/1/0/all/0/1\">Pavan Yadav A</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Shriganesh Hegde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Welsch Based Multiview Disparity Estimation. (arXiv:2110.00803v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00803","description":"<p>In this work, we explore disparity estimation from a high number of views. We\nexperimentally identify occlusions as a key challenge for disparity estimation\nfor applications with high numbers of views. In particular, occlusions can\nactually result in a degradation in accuracy as more views are added to a\ndataset. We propose the use of a Welsch loss function for the data term in a\nglobal variational framework for disparity estimation. We also propose a\ndisciplined warping strategy and a progressive inclusion of views strategy that\ncan reduce the need for coarse to fine strategies that discard high spatial\nfrequency components from the early iterations. Experimental results\ndemonstrate that the proposed approach produces superior and/or more robust\nestimates than other conventional variational approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gray_J/0/1/0/all/0/1\">James L. Gray</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naman_A/0/1/0/all/0/1\">Aous T. Naman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taubman_D/0/1/0/all/0/1\">David S. Taubman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProTo: Program-Guided Transformer for Program-Guided Tasks. (arXiv:2110.00804v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00804","description":"<p>Programs, consisting of semantic and structural information, play an\nimportant role in the communication between humans and agents. Towards learning\ngeneral program executors to unify perception, reasoning, and decision making,\nwe formulate program-guided tasks which require learning to execute a given\nprogram on the observed task specification. Furthermore, we propose the\nProgram-guided Transformer (ProTo), which integrates both semantic and\nstructural guidance of a program by leveraging cross-attention and masked\nself-attention to pass messages between the specification and routines in the\nprogram. ProTo executes a program in a learned latent space and enjoys stronger\nrepresentation ability than previous neural-symbolic approaches. We demonstrate\nthat ProTo significantly outperforms the previous state-of-the-art methods on\nGQA visual reasoning and 2D Minecraft policy learning datasets. Additionally,\nProTo demonstrates better generalization to unseen, complex, and human-written\nprograms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zelin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samel_K/0/1/0/all/0/1\">Karan Samel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Binghong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit and Explicit Attention for Zero-Shot Learning. (arXiv:2110.00860v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00860","description":"<p>Most of the existing Zero-Shot Learning (ZSL) methods focus on learning a\ncompatibility function between the image representation and class attributes.\nFew others concentrate on learning image representation combining local and\nglobal features. However, the existing approaches still fail to address the\nbias issue towards the seen classes. In this paper, we propose implicit and\nexplicit attention mechanisms to address the existing bias problem in ZSL\nmodels. We formulate the implicit attention mechanism with a self-supervised\nimage angle rotation task, which focuses on specific image features aiding to\nsolve the task. The explicit attention mechanism is composed with the\nconsideration of a multi-headed self-attention mechanism via Vision Transformer\nmodel, which learns to map image features to semantic space during the training\nstage. We conduct comprehensive experiments on three popular benchmarks: AWA2,\nCUB and SUN. The performance of our proposed attention mechanisms has proved\nits effectiveness, and has achieved the state-of-the-art harmonic mean on all\nthe three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alamri_F/0/1/0/all/0/1\">Faisal Alamri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BdSL36: A Dataset for Bangladeshi Sign Letters Recognition. (arXiv:2110.00869v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00869","description":"<p>Bangladeshi Sign Language (BdSL) is a commonly used medium of communication\nfor the hearing-impaired people in Bangladesh. A real-time BdSL interpreter\nwith no controlled lab environment has a broad social impact and an interesting\navenue of research as well. Also, it is a challenging task due to the variation\nin different subjects (age, gender, color, etc.), complex features, and\nsimilarities of signs and clustered backgrounds. However, the existing dataset\nfor BdSL classification task is mainly built in a lab friendly setup which\nlimits the application of powerful deep learning technology. In this paper, we\nintroduce a dataset named BdSL36 which incorporates background augmentation to\nmake the dataset versatile and contains over four million images belonging to\n36 categories. Besides, we annotate about 40,000 images with bounding boxes to\nutilize the potentiality of object detection algorithms. Furthermore, several\nintensive experiments are performed to establish the baseline performance of\nour BdSL36. Moreover, we employ beta testing of our classifiers at the user\nlevel to justify the possibilities of real-world application with this dataset.\nWe believe our BdSL36 will expedite future research on practical sign letter\nclassification. We make the datasets and all the pre-trained models available\nfor further researcher.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoque_O/0/1/0/all/0/1\">Oishee Bintey Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jubair_M/0/1/0/all/0/1\">Mohammad Imrul Jubair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akash_A/0/1/0/all/0/1\">Al-Farabi Akash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Saiful Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Attention-based Models Using Activation Maps for Citrus Mite and Insect Pest Classification. (arXiv:2110.00881v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00881","description":"<p>Citrus juices and fruits are commodities with great economic potential in the\ninternational market, but productivity losses caused by mites and other pests\nare still far from being a good mark. Despite the integrated pest mechanical\naspect, only a few works on automatic classification have handled images with\norange mite characteristics, which means tiny and noisy regions of interest. On\nthe computational side, attention-based models have gained prominence in deep\nlearning research, and, along with weakly supervised learning algorithms, they\nhave improved tasks performed with some label restrictions. In agronomic\nresearch of pests and diseases, these techniques can improve classification\nperformance while pointing out the location of mites and insects without\nspecific labels, reducing deep learning development costs related to generating\nbounding boxes. In this context, this work proposes an attention-based\nactivation map approach developed to improve the classification of tiny regions\ncalled Two-Weighted Activation Mapping, which also produces locations using\nfeature map scores learned from class labels. We apply our method in a\ntwo-stage network process called Attention-based Multiple Instance Learning\nGuided by Saliency Maps. We analyze the proposed approach in two challenging\ndatasets, the Citrus Pest Benchmark, which was captured directly in the field\nusing magnifying glasses, and the Insect Pest, a large pest image benchmark. In\naddition, we evaluate and compare our models with weakly supervised methods,\nsuch as Attention-based Deep MIL and WILDCAT. The results show that our\nclassifier is superior to literature methods that use tiny regions in their\nclassification tasks, surpassing them in all scenarios by at least 16\npercentage points. Moreover, our approach infers bounding box locations for\nsalient insects, even training without any location labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bollis_E/0/1/0/all/0/1\">Edson Bollis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maia_H/0/1/0/all/0/1\">Helena Maia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrini_H/0/1/0/all/0/1\">Helio Pedrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disarranged Zone Learning (DZL): An unsupervised and dynamic automatic stenosis recognition methodology based on coronary angiography. (arXiv:2110.00896v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00896","description":"<p>We proposed a novel unsupervised methodology named Disarranged Zone Learning\n(DZL) to automatically recognize stenosis in coronary angiography. The\nmethodology firstly disarranges the frames in a video, secondly it generates an\neffective zone and lastly trains an encoder-decoder GRU model to learn the\ncapability to recover disarranged frames. The breakthrough of our study is to\ndiscover and validate the Sequence Intensity (Recover Difficulty) is a measure\nof Coronary Artery Stenosis Status. Hence, the prediction accuracy of DZL is\nused as an approximator of coronary stenosis indicator. DZL is an unsupervised\nmethodology and no label engineering effort is needed, the sub GRU model in DZL\nworks as a self-supervised approach. So DZL could theoretically utilize\ninfinitely huge amounts of coronary angiographies to learn and improve\nperformance without laborious data labeling. There is no data preprocessing\nprecondition to run DZL as it dynamically utilizes the whole video, hence it is\neasy to be implemented and generalized to overcome the data heterogeneity of\ncoronary angiography. The overall average precision score achieves 0.93, AUC\nachieves 0.8 for this pure methodology. The highest segmented average precision\nscore is 0.98 and the highest segmented AUC is 0.87 for coronary occlusion\nindicator. Finally, we developed a software demo to implement DZL methodology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_Y/0/1/0/all/0/1\">Yanan Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_P/0/1/0/all/0/1\">Pengxiong Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_B/0/1/0/all/0/1\">Bangde Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ling_Y/0/1/0/all/0/1\">Yun Ling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xibao Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geng_L/0/1/0/all/0/1\">Liang Geng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anti-aliasing Deep Image Classifiers using Novel Depth Adaptive Blurring and Activation Function. (arXiv:2110.00899v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00899","description":"<p>Deep convolutional networks are vulnerable to image translation or shift,\npartly due to common down-sampling layers, e.g., max-pooling and strided\nconvolution. These operations violate the Nyquist sampling rate and cause\naliasing. The textbook solution is low-pass filtering (blurring) before\ndown-sampling, which can benefit deep networks as well. Even so, non-linearity\nunits, such as ReLU, often re-introduce the problem, suggesting that blurring\nalone may not suffice. In this work, first, we analyse deep features with\nFourier transform and show that Depth Adaptive Blurring is more effective, as\nopposed to monotonic blurring. To this end, we outline how this can replace\nexisting down-sampling methods. Second, we introduce a novel activation\nfunction -- with a built-in low pass filter, to keep the problem from\nreappearing. From experiments, we observe generalisation on other forms of\ntransformations and corruptions as well, e.g., rotation, scale, and noise. We\nevaluate our method under three challenging settings: (1) a variety of image\ntranslations; (2) adversarial attacks -- both $\\ell_{p}$ bounded and unbounded;\nand (3) data corruptions and perturbations. In each setting, our method\nachieves state-of-the-art results and improves clean accuracy on various\nbenchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Tahmid Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Shyh Wei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohel_F/0/1/0/all/0/1\">Ferdous Sohel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guojun Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GROWN: GRow Only When Necessary for Continual Learning. (arXiv:2110.00908v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00908","description":"<p>Catastrophic forgetting is a notorious issue in deep learning, referring to\nthe fact that Deep Neural Networks (DNN) could forget the knowledge about\nearlier tasks when learning new tasks. To address this issue, continual\nlearning has been developed to learn new tasks sequentially and perform\nknowledge transfer from the old tasks to the new ones without forgetting. While\nrecent structure-based learning methods show the capability of alleviating the\nforgetting problem, these methods start from a redundant full-size network and\nrequire a complex learning process to gradually grow-and-prune or search the\nnetwork structure for each task, which is inefficient. To address this problem\nand enable efficient network expansion for new tasks, we first develop a\nlearnable sparse growth method eliminating the additional pruning/searching\nstep in previous structure-based methods. Building on this learnable sparse\ngrowth method, we then propose GROWN, a novel end-to-end continual learning\nframework to dynamically grow the model only when necessary. Different from all\nprevious structure-based methods, GROWN starts from a small seed network,\ninstead of a full-sized one. We validate GROWN on multiple datasets against\nstate-of-the-art methods, which shows superior performance in both accuracy and\nmodel size. For example, we achieve 1.0\\% accuracy gain on average compared to\nthe current SOTA results on CIFAR-100 Superclass 20 tasks setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deliang Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does deep learning model calibration improve performance in class-imbalanced medical image classification?. (arXiv:2110.00918v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00918","description":"<p>In medical image classification tasks, it is common to find that the number\nof normal samples far exceeds the number of abnormal samples. In such\nclass-imbalanced situations, reliable training of deep neural networks\ncontinues to be a major challenge. Under these circumstances, the predicted\nclass confidence may be biased toward the majority class. Calibration has been\nsuggested to alleviate some of these effects. However, there is insufficient\nanalysis explaining when and whether calibrating a model would be beneficial in\nimproving performance. In this study, we perform a systematic analysis of the\neffect of model calibration on its performance on two medical image modalities,\nnamely, chest X-rays (CXRs) and fundus images, using various deep learning\nclassifier backbones. For this, we study the following variations: (i) the\ndegree of imbalances in the dataset used for training; (ii) calibration\nmethods; and, (iii) two classification thresholds, namely, default decision\nthreshold of 0.5, and optimal threshold from precision-recall (PR) curves. Our\nresults indicate that at the default operating threshold of 0.5, the\nperformance achieved through calibration is significantly superior (p &lt; 0.05)\nto an uncalibrated model. However, at the PR-guided threshold, these gains were\nnot significantly different (p &gt; 0.05). This finding holds for both image\nmodalities and at varying degrees of imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaraman_S/0/1/0/all/0/1\">Sivaramakrishnan Rajaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_P/0/1/0/all/0/1\">Prasanth Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antani_S/0/1/0/all/0/1\">Sameer Antani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention module improves both performance and interpretability of 4D fMRI decoding neural network. (arXiv:2110.00920v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00920","description":"<p>Decoding brain cognitive states from neuroimaging signals is an important\ntopic in neuroscience. In recent years, deep neural networks (DNNs) have been\nrecruited for multiple brain state decoding and achieved good performance.\nHowever, the open question of how to interpret the DNN black box remains\nunanswered. Capitalizing on advances in machine learning, we integrated\nattention modules into brain decoders to facilitate an in-depth interpretation\nof DNN channels. A 4D convolution operation was also included to extract\ntemporo-spatial interaction within the fMRI signal. The experiments showed that\nthe proposed model obtains a very high accuracy (97.4%) and outperforms\nprevious researches on the 7 different task benchmarks from the Human\nConnectome Project (HCP) dataset. The visualization analysis further\nillustrated the hierarchical emergence of task-specific masks with depth.\nFinally, the model was retrained to regress individual traits within the HCP\nand to classify viewing images from the BOLD5000 dataset, respectively.\nTransfer learning also achieves good performance. A further visualization\nanalysis shows that, after transfer learning, low-level attention masks\nremained similar to the source domain, whereas high-level attention masks\nchanged adaptively. In conclusion, the proposed 4D model with attention module\nperformed well and facilitated interpretation of DNNs, which is helpful for\nsubsequent research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhoufan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yanming Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">ChenWei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yueyang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_R/0/1/0/all/0/1\">Rongjie Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Shishuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1\">Sheng Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoxiao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_B/0/1/0/all/0/1\">Bensheng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bounding Box Tightness Prior for Weakly Supervised Image Segmentation. (arXiv:2110.00934v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00934","description":"<p>This paper presents a weakly supervised image segmentation method that adopts\ntight bounding box annotations. It proposes generalized multiple instance\nlearning (MIL) and smooth maximum approximation to integrate the bounding box\ntightness prior into the deep neural network in an end-to-end manner. In\ngeneralized MIL, positive bags are defined by parallel crossing lines with a\nset of different angles, and negative bags are defined as individual pixels\noutside of any bounding boxes. Two variants of smooth maximum approximation,\ni.e., $\\alpha$-softmax function and $\\alpha$-quasimax function, are exploited\nto conquer the numeral instability introduced by maximum function of bag\nprediction. The proposed approach was evaluated on two pubic medical datasets\nusing Dice coefficient. The results demonstrate that it outperforms the\nstate-of-the-art methods. The codes are available at\n\\url{https://github.com/wangjuan313/wsis-boundingbox}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Juan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anatomical Landmarks Localization for 3D Foot Point Clouds. (arXiv:2110.00937v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00937","description":"<p>3D anatomical landmarks play an important role in health research. Their\nautomated prediction/localization thus becomes a vital task. In this paper, we\nintroduce a deformation method for 3D anatomical landmarks prediction. It\nutilizes a source model with anatomical landmarks which are annotated by\nclinicians, and deforms this model non-rigidly to match the target model. Two\nconstraints are introduced in the optimization, which are responsible for\nalignment and smoothness, respectively. Experiments are performed on our\ndataset and the results demonstrate the robustness of our method, and show that\nit yields better performance than the state-of-the-art techniques in most\ncases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fung_S/0/1/0/all/0/1\">Sheldon Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mykolaitis_M/0/1/0/all/0/1\">Mantas Mykolaitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostkevicius_G/0/1/0/all/0/1\">Gediminas Kostkevicius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozerenskis_D/0/1/0/all/0/1\">Domantas Ozerenskis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Intelligence For Breast Cancer Detection: Trends & Directions. (arXiv:2110.00942v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00942","description":"<p>In the last decade, researchers working in the domain of computer vision and\nArtificial Intelligence (AI) have beefed up their efforts to come up with the\nautomated framework that not only detects but also identifies stage of breast\ncancer. The reason for this surge in research activities in this direction are\nmainly due to advent of robust AI algorithms (deep learning), availability of\nhardware that can train those robust and complex AI algorithms and\naccessibility of large enough dataset required for training AI algorithms.\nDifferent imaging modalities that have been exploited by researchers to\nautomate the task of breast cancer detection are mammograms, ultrasound,\nmagnetic resonance imaging, histopathological images or any combination of\nthem. This article analyzes these imaging modalities and presents their\nstrengths, limitations and enlists resources from where their datasets can be\naccessed for research purpose. This article then summarizes AI and computer\nvision based state-of-the-art methods proposed in the last decade, to detect\nbreast cancer using various imaging modalities. Generally, in this article we\nhave focused on to review frameworks that have reported results using\nmammograms as it is most widely used breast imaging modality that serves as\nfirst test that medical practitioners usually prescribe for the detection of\nbreast cancer. Second reason of focusing on mammogram imaging modalities is the\navailability of its labeled datasets. Datasets availability is one of the most\nimportant aspect for the development of AI based frameworks as such algorithms\nare data hungry and generally quality of dataset affects performance of AI\nbased algorithms. In a nutshell, this research article will act as a primary\nresource for the research community working in the field of automated breast\nimaging analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1\">Shahid Munir Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_R/0/1/0/all/0/1\">Rizwan Ahmed Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arif_S/0/1/0/all/0/1\">Sheeraz Arif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sajid_U/0/1/0/all/0/1\">Unaiza Sajid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Cup-to-Disc Ratio Measurement with Tight Bounding Box Supervision in Fundus Photography. (arXiv:2110.00943v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00943","description":"<p>The cup-to-disc ratio (CDR) is one of the most significant indicator for\nglaucoma diagnosis. Different from the use of costly fully supervised learning\nformulation with pixel-wise annotations in the literature, this study\ninvestigates the feasibility of accurate CDR measurement in fundus images using\nonly tight bounding box supervision. For this purpose, we develop a two-task\nnetwork for accurate CDR measurement, one for weakly supervised image\nsegmentation, and the other for bounding-box regression. The weakly supervised\nimage segmentation task is implemented based on generalized multiple instance\nlearning formulation and smooth maximum approximation, and the bounding-box\nregression task outputs class-specific bounding box prediction in a single\nscale at the original image resolution. To get accurate bounding box\nprediction, a class-specific bounding-box normalizer and an expected\nintersection-over-union are proposed. In the experiments, the proposed approach\nwas evaluated by a testing set with 1200 images using CDR error and F1 score\nfor CDR measurement and dice coefficient for image segmentation. A grader study\nwas conducted to compare the performance of the proposed approach with those of\nindividual graders. The results demonstrate that the proposed approach\noutperforms the state-of-the-art performance obtained from the fully supervised\nimage segmentation (FSIS) approach using pixel-wise annotation for CDR\nmeasurement, which is also better than those of individual graders. It also\ngets performance close to the state-of-the-art obtained from FSIS for optic cup\nand disc segmentation, similar to those of individual graders. The codes are\navailable at \\url{https://github.com/wangjuan313/CDRNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Juan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Segmentation for COVID-19 Infection Quantification on Longitudinal CT scans. (arXiv:2110.00948v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00948","description":"<p>Consistent segmentation of COVID-19 patient's CT scans across multiple time\npoints is essential to assess disease progression and response to therapy\naccurately. Existing automatic and interactive segmentation models for medical\nimages only use data from a single time point (static). However, valuable\nsegmentation information from previous time points is often not used to aid the\nsegmentation of a patient's follow-up scans. Also, fully automatic segmentation\ntechniques frequently produce results that would need further editing for\nclinical use. In this work, we propose a new single network model for\ninteractive segmentation that fully utilizes all available past information to\nrefine the segmentation of follow-up scans. In the first segmentation round,\nour model takes 3D volumes of medical images from two-time points (target and\nreference) as concatenated slices with the additional reference time point\nsegmentation as a guide to segment the target scan. In subsequent segmentation\nrefinement rounds, user feedback in the form of scribbles that correct the\nsegmentation and the target's previous segmentation results are additionally\nfed into the model. This ensures that the segmentation information from\nprevious refinement rounds is retained. Experimental results on our in-house\nmulticlass longitudinal COVID-19 dataset show that the proposed model\noutperforms its static version and can assist in localizing COVID-19 infections\nin patient's follow-up scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Foo_M/0/1/0/all/0/1\">Michelle Xiao-Lin Foo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong Tae Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paschali_M/0/1/0/all/0/1\">Magdalini Paschali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goli_L/0/1/0/all/0/1\">Leili Goli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burian_E/0/1/0/all/0/1\">Egon Burian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus Makowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wendler_T/0/1/0/all/0/1\">Thomas Wendler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Unsupervised Video Game Playstyle Metric via State Discretization. (arXiv:2110.00950v1 [cs.AI])","link":"http://arxiv.org/abs/2110.00950","description":"<p>On playing video games, different players usually have their own playstyles.\nRecently, there have been great improvements for the video game AIs on the\nplaying strength. However, past researches for analyzing the behaviors of\nplayers still used heuristic rules or the behavior features with the\ngame-environment support, thus being exhausted for the developers to define the\nfeatures of discriminating various playstyles. In this paper, we propose the\nfirst metric for video game playstyles directly from the game observations and\nactions, without any prior specification on the playstyle in the target game.\nOur proposed method is built upon a novel scheme of learning discrete\nrepresentations that can map game observations into latent discrete states,\nsuch that playstyles can be exhibited from these discrete states. Namely, we\nmeasure the playstyle distance based on game observations aligned to the same\nstates. We demonstrate high playstyle accuracy of our metric in experiments on\nsome video game platforms, including TORCS, RGSK, and seven Atari games, and\nfor different agents including rule-based AI bots, learning-based AI bots, and\nhuman players.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chiu-Chou Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_I/0/1/0/all/0/1\">I-Chen Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Representation Learning for Spatial Image Steganalysis. (arXiv:2110.00957v1 [cs.MM])","link":"http://arxiv.org/abs/2110.00957","description":"<p>In this paper, we introduce a graph representation learning architecture for\nspatial image steganalysis, which is motivated by the assumption that\nsteganographic modifications unavoidably distort the statistical\ncharacteristics of the hidden graph features derived from cover images. In the\ndetailed architecture, we translate each image to a graph, where nodes\nrepresent the patches of the image and edges indicate the local associations\nbetween the patches. Each node is associated with a feature vector determined\nfrom the corresponding patch by a shallow convolutional neural network (CNN)\nstructure. By feeding the graph to an attention network, the discriminative\nfeatures can be learned for efficient steganalysis. Experiments indicate that\nthe reported architecture achieves a competitive performance compared to the\nbenchmark CNN model, which has shown the potential of graph learning for\nsteganalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fingerprint Matching using the Onion Peeling Approach and Turning Function. (arXiv:2110.00958v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00958","description":"<p>Fingerprint, as one of the most popular and robust biometric traits, can be\nused in automatic identification and verification systems to identify\nindividuals. Fingerprint matching is a vital and challenging issue in\nfingerprint recognition systems. Most fingerprint matching algorithms are\nminutiae-based. The minutiae in fingerprints can be determined by their\ndiscontinuity. Ridge ending and ridge bifurcation are two frequently used\nminutiae in most fingerprint-based matching algorithms.\n</p>\n<p>This paper presents a new minutiae-based fingerprint matching using the onion\npeeling approach. In the proposed method, fingerprints are aligned to find the\nmatched minutiae points. Then, the nested convex polygons of matched minutiae\npoints are constructed and the comparison between peer-to-peer polygons is\nperformed by the turning function distance. Simplicity, accuracy, and low time\ncomplexity of the Onion peeling approach are three important factors that make\nit a standard method for fingerprint matching purposes. The performance of the\nproposed algorithm is evaluated on the database $FVC2002$. The results show\nthat fingerprints of the same fingers have higher scores than different\nfingers. Since the fingerprints that the difference between the number of their\nlayers is more than $2$ and the minutiae matching score lower than 0.15 are\nignored, the better results are obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padkan_N/0/1/0/all/0/1\">Nazanin Padkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_B/0/1/0/all/0/1\">B. Sadeghi Bigham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faraji_M/0/1/0/all/0/1\">Mohammad Reza Faraji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Images into Maps. (arXiv:2110.00966v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00966","description":"<p>We approach instantaneous mapping, converting images to a top-down view of\nthe world, as a translation problem. We show how a novel form of transformer\nnetwork can be used to map from images and video directly to an overhead map or\nbird's-eye-view (BEV) of the world, in a single end-to-end network. We assume a\n1-1 correspondence between a vertical scanline in the image, and rays passing\nthrough the camera location in an overhead map. This lets us formulate map\ngeneration from an image as a set of sequence-to-sequence translations. Posing\nthe problem as translation allows the network to use the context of the image\nwhen interpreting the role of each pixel. This constrained formulation, based\nupon a strong physical grounding of the problem, leads to a restricted\ntransformer network that is convolutional in the horizontal direction only. The\nstructure allows us to make efficient use of data when training, and obtains\nstate-of-the-art results for instantaneous mapping of three large-scale\ndatasets, including a 15% and 30% relative gain against existing best\nperforming methods on the nuScenes and Argoverse datasets, respectively. We\nmake our code available on\nhttps://github.com/avishkarsaha/translating-images-into-maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Avishkar Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maldonado_O/0/1/0/all/0/1\">Oscar Mendez Maldonado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Guided Zero-Shot Learning for Low-Light Image/Video Enhancement. (arXiv:2110.00970v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00970","description":"<p>Low-light images challenge both human perceptions and computer vision\nalgorithms. It is crucial to make algorithms robust to enlighten low-light\nimages for computational photography and computer vision applications such as\nreal-time detection and segmentation tasks. This paper proposes a\nsemantic-guided zero-shot low-light enhancement network which is trained in the\nabsence of paired images, unpaired datasets, and segmentation annotation.\nFirstly, we design an efficient enhancement factor extraction network using\ndepthwise separable convolution. Secondly, we propose a recurrent image\nenhancement network for progressively enhancing the low-light image. Finally,\nwe introduce an unsupervised semantic segmentation network for preserving the\nsemantic information. Extensive experiments on various benchmark datasets and a\nlow-light video demonstrate that our model outperforms the previous\nstate-of-the-art qualitatively and quantitatively. We further discuss the\nbenefits of the proposed method for low-light detection and segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Scheme for 3D Point Cloud Copy Detection. (arXiv:2110.00972v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00972","description":"<p>Most existing 3D geometry copy detection research focused on 3D watermarking,\nwhich first embeds ``watermarks'' and then detects the added watermarks.\nHowever, this kind of methods is non-straightforward and may be less robust to\nattacks such as cropping and noise. In this paper, we focus on a fundamental\nand practical research problem: judging whether a point cloud is plagiarized or\ncopied to another point cloud in the presence of several manipulations (e.g.,\nsimilarity transformation, smoothing). We propose a novel method to address\nthis critical problem. Our key idea is first to align the two point clouds and\nthen calculate their similarity distance. We design three different measures to\ncompute the similarity. We also introduce two strategies to speed up our\nmethod. Comprehensive experiments and comparisons demonstrate the effectiveness\nand robustness of our method in estimating the similarity of two given 3D point\nclouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzhi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Unfolding Total Variation Network for Low-Light Image Enhancement. (arXiv:2110.00984v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00984","description":"<p>Real-world low-light images suffer from two main degradations, namely,\ninevitable noise and poor visibility. Since the noise exhibits different\nlevels, its estimation has been implemented in recent works when enhancing\nlow-light images from raw Bayer space. When it comes to sRGB color space, the\nnoise estimation becomes more complicated due to the effect of the image\nprocessing pipeline. Nevertheless, most existing enhancing algorithms in sRGB\nspace only focus on the low visibility problem or suppress the noise under a\nhypothetical noise level, leading them impractical due to the lack of\nrobustness. To address this issue,we propose an adaptive unfolding total\nvariation network (UTVNet), which approximates the noise level from the real\nsRGB low-light image by learning the balancing parameter in the model-based\ndenoising method with total variation regularization. Meanwhile, we learn the\nnoise level map by unrolling the corresponding minimization process for\nproviding the inferences of smoothness and fidelity constraints. Guided by the\nnoise level map, our UTVNet can recover finer details and is more capable to\nsuppress noise in real captured low-light scenes. Extensive experiments on\nreal-world low-light images clearly demonstrate the superior performance of\nUTVNet over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanjun Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_D/0/1/0/all/0/1\">Daming Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1\">Wentian Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keypoint Communities. (arXiv:2110.00988v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00988","description":"<p>We present a fast bottom-up method that jointly detects over 100 keypoints on\nhumans or objects, also referred to as human/object pose estimation. We model\nall keypoints belonging to a human or an object -- the pose -- as a graph and\nleverage insights from community detection to quantify the independence of\nkeypoints. We use a graph centrality measure to assign training weights to\ndifferent parts of a pose. Our proposed measure quantifies how tightly a\nkeypoint is connected to its neighborhood. Our experiments show that our method\noutperforms all previous methods for human pose estimation with fine-grained\nkeypoint annotations on the face, the hands and the feet with a total of 133\nkeypoints. We also show that our method generalizes to car poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zauss_D/0/1/0/all/0/1\">Duncan Zauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_S/0/1/0/all/0/1\">Sven Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild. (arXiv:2110.00990v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00990","description":"<p>This paper addresses the problem of 3D human body shape and pose estimation\nfrom an RGB image. This is often an ill-posed problem, since multiple plausible\n3D bodies may match the visual evidence present in the input - particularly\nwhen the subject is occluded. Thus, it is desirable to estimate a distribution\nover 3D body shape and pose conditioned on the input image instead of a single\n3D reconstruction. We train a deep neural network to estimate a hierarchical\nmatrix-Fisher distribution over relative 3D joint rotation matrices (i.e. body\npose), which exploits the human body's kinematic tree structure, as well as a\nGaussian distribution over SMPL body shape parameters. To further ensure that\nthe predicted shape and pose distributions match the visual evidence in the\ninput image, we implement a differentiable rejection sampler to impose a\nreprojection loss between ground-truth 2D joint coordinates and samples from\nthe predicted distributions, projected onto the image plane. We show that our\nmethod is competitive with the state-of-the-art in terms of 3D shape and pose\nmetrics on the SSP-3D and 3DPW datasets, while also yielding a structured\nprobability distribution over 3D body shape and pose, with which we can\nmeaningfully quantify prediction uncertainty and sample multiple plausible 3D\nreconstructions to explain a given input image. Code is available at\nhttps://github.com/akashsengupta1997/HierarchicalProbabilistic3DHuman .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1\">Akash Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budvytis_I/0/1/0/all/0/1\">Ignas Budvytis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Precise Object Placement with Pose Distance Estimations for Different Objects and Grippers. (arXiv:2110.00992v1 [cs.RO])","link":"http://arxiv.org/abs/2110.00992","description":"<p>This paper introduces a novel approach for the grasping and precise placement\nof various known rigid objects using multiple grippers within highly cluttered\nscenes. Using a single depth image of the scene, our method estimates multiple\n6D object poses together with an object class, a pose distance for object pose\nestimation, and a pose distance from a target pose for object placement for\neach automatically obtained grasp pose with a single forward pass of a neural\nnetwork. By incorporating model knowledge into the system, our approach has\nhigher success rates for grasping than state-of-the-art model-free approaches.\nFurthermore, our method chooses grasps that result in significantly more\nprecise object placements than prior model-based work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kleeberger_K/0/1/0/all/0/1\">Kilian Kleeberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnitzler_J/0/1/0/all/0/1\">Jonathan Schnitzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_M/0/1/0/all/0/1\">Muhammad Usman Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bormann_R/0/1/0/all/0/1\">Richard Bormann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_W/0/1/0/all/0/1\">Werner Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Marco F. Huber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01013","description":"<p>Today's VQA models still tend to capture superficial linguistic correlations\nin the training set and fail to generalize to the test set with different QA\ndistributions. To reduce these language biases, recent VQA works introduce an\nauxiliary question-only model to regularize the training of targeted VQA model,\nand achieve dominating performance on diagnostic benchmarks for\nout-of-distribution testing. However, due to complex model design, these\nensemble-based methods are unable to equip themselves with two indispensable\ncharacteristics of an ideal VQA model: 1) Visual-explainable: The model should\nrely on the right visual regions when making decisions. 2) Question-sensitive:\nThe model should be sensitive to the linguistic variations in questions. To\nthis end, we propose a novel model-agnostic Counterfactual Samples Synthesizing\nand Training (CSST) strategy. After training with CSST, VQA models are forced\nto focus on all critical objects and words, which significantly improves both\nvisual-explainable and question-sensitive abilities. Specifically, CSST is\ncomposed of two parts: Counterfactual Samples Synthesizing (CSS) and\nCounterfactual Samples Training (CST). CSS generates counterfactual samples by\ncarefully masking critical objects in images or words in questions and\nassigning pseudo ground-truth answers. CST not only trains the VQA models with\nboth complementary samples to predict respective ground-truth answers, but also\nurges the VQA models to further distinguish the original samples and\nsuperficially similar counterfactual ones. To facilitate the CST training, we\npropose two variants of supervised contrastive loss for VQA, and design an\neffective positive and negative sample selection mechanism based on CSS.\nExtensive experiments have shown the effectiveness of CSST. Particularly, by\nbuilding on top of model LMH+SAR, we achieve record-breaking performance on all\nOOD benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuhang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EAR-U-Net: EfficientNet and attention-based residual U-Net for automatic liver segmentation in CT. (arXiv:2110.01014v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01014","description":"<p>Purpose: This paper proposes a new network framework called EAR-U-Net, which\nleverages EfficientNetB4, attention gate, and residual learning techniques to\nachieve automatic and accurate liver segmentation. Methods: The proposed method\nis based on the U-Net framework. First, we use EfficientNetB4 as the encoder to\nextract more feature information during the encoding stage. Then, an attention\ngate is introduced in the skip connection to eliminate irrelevant regions and\nhighlight features of a specific segmentation task. Finally, to alleviate the\nproblem of gradient vanishment, we replace the traditional convolution of the\ndecoder with a residual block to improve the segmentation accuracy. Results: We\nverified the proposed method on the LiTS17 and SLiver07 datasets and compared\nit with classical networks such as FCN, U-Net, Attention U-Net, and Attention\nRes-U-Net. In the Sliver07 evaluation, the proposed method achieved the best\nsegmentation performance on all five standard metrics. Meanwhile, in the LiTS17\nassessment, the best performance is obtained except for a slight inferior on\nRVD. Moreover, we also participated in the MICCIA-LiTS17 challenge, and the\nDice per case score was 0.952. Conclusion: The proposed method's qualitative\nand quantitative results demonstrated its applicability in liver segmentation\nand proved its good prospect in computer-assisted liver segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinke Wang</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyang Zhang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1\">Peiqing Lv</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Lubiao Zhou</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haiying Wang</a> (1) ((1) School of Automation, Harbin University of Science and Technology, Harbin, 150080, China, (2) Rongcheng College, Harbin University of Science and Technology, Rongcheng, 264300, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Video Representation Learning for AI Based Video Playback Style Prediction. (arXiv:2110.01015v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01015","description":"<p>Ever-increasing smartphone-generated video content demands intelligent\ntechniques to edit and enhance videos on power-constrained devices. Most of the\nbest performing algorithms for video understanding tasks like action\nrecognition, localization, etc., rely heavily on rich spatio-temporal\nrepresentations to make accurate predictions. For effective learning of the\nspatio-temporal representation, it is crucial to understand the underlying\nobject motion patterns present in the video. In this paper, we propose a novel\napproach for understanding object motions via motion type classification. The\nproposed motion type classifier predicts a motion type for the video based on\nthe trajectories of the objects present. Our classifier assigns a motion type\nfor the given video from the following five primitive motion classes: linear,\nprojectile, oscillatory, local and random. We demonstrate that the\nrepresentations learned from the motion type classification generalizes well\nfor the challenging downstream task of video retrieval. Further, we proposed a\nrecommendation system for video playback style based on the motion type\nclassifier predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parihar_R/0/1/0/all/0/1\">Rishubh Parihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramola_G/0/1/0/all/0/1\">Gaurav Ramola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1\">Ranajit Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kini_R/0/1/0/all/0/1\">Ravi Kini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rege_A/0/1/0/all/0/1\">Aniket Rege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velusamy_S/0/1/0/all/0/1\">Sudha Velusamy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Viral Pneumonia X-ray Images with the Aucmedi Framework. (arXiv:2110.01017v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01017","description":"<p>In this work we use the AUCMEDI-Framework to train a deep neural network to\nclassify chest X-ray images as either normal or viral pneumonia. Stratified\nk-fold cross-validation with k=3 is used to generate the validation-set and 15%\nof the data are set aside for the evaluation of the models of the different\nfolds and ensembles each. A random-forest ensemble as well as a\nSoft-Majority-Vote ensemble are built from the predictions of the different\nfolds. Evaluation metrics (Classification-Report, macro f1-scores,\nConfusion-Matrices, ROC-Curves) of the individual folds and the ensembles show\nthat the classifier works well. Finally Grad-CAM and LIME explainable\nartificial intelligence (XAI) algorithms are applied to visualize the image\nfeatures that are most important for the prediction. For Grad-CAM the heatmaps\nof the three folds are furthermore averaged for all images in order to\ncalculate a mean XAI-heatmap. As the heatmaps of the different folds for most\nimages differ only slightly this averaging procedure works well. However, only\nmedical professionals can evaluate the quality of the features marked by the\nXAI. A comparison of the evaluation metrics with metrics of standard procedures\nsuch as PCR would also be important. Further limitations are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pia Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_D/0/1/0/all/0/1\">Dominik M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramer_F/0/1/0/all/0/1\">Frank Kramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DARDet: A Dense Anchor-free Rotated Object Detector in Aerial Images. (arXiv:2110.01025v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01025","description":"<p>Rotated object detection in aerial images has received increasing attention\nfor a wide range of applications. However, it is also a challenging task due to\nthe huge variations of scale, rotation, aspect ratio, and densely arranged\ntargets. Most existing methods heavily rely on a large number of pre-defined\nanchors with different scales, angles, and aspect ratios, and are optimized\nwith a distance loss. Therefore, these methods are sensitive to anchor\nhyper-parameters and easily suffer from performance degradation caused by\nboundary discontinuity. To handle this problem, in this paper, we propose a\ndense anchor-free rotated object detector (DARDet) for rotated object detection\nin aerial images. Our DARDet directly predicts five parameters of rotated boxes\nat each foreground pixel of feature maps. We design a new alignment convolution\nmodule to extracts aligned features and introduce a PIoU loss for precise and\nstable regression. Our method achieves state-of-the-art performance on three\ncommonly used aerial objects datasets (i.e., DOTA, HRSC2016, and UCAS-AOD)\nwhile keeping high efficiency. Code is available at\nhttps://github.com/zf020114/DARDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Face Restoration With Memorized Modulation. (arXiv:2110.01033v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01033","description":"<p>Blind face restoration (BFR) is a challenging problem because of the\nuncertainty of the degradation patterns. This paper proposes a Restoration with\nMemorized Modulation (RMM) framework for universal BFR in diverse degraded\nscenes and heterogeneous domains. We apply random noise as well as unsupervised\nwavelet memory to adaptively modulate the face-enhancement generator,\nconsidering attentional denormalization in both layer and instance levels.\nSpecifically, in the training stage, the low-level spatial feature embedding,\nthe wavelet memory embedding obtained by wavelet transformation of the\nhigh-resolution image, as well as the disentangled high-level noise embeddings\nare integrated, with the guidance of attentional maps generated from layer\nnormalization, instance normalization, and the original feature map. These\nthree embeddings are respectively associated with the spatial content,\nhigh-frequency texture details, and a learnable universal prior against other\nblind image degradation patterns. We store the spatial feature of the\nlow-resolution image and the corresponding wavelet style code as key and value\nin the memory unit, respectively. In the test stage, the wavelet memory value\nwhose corresponding spatial key is the most matching with that of the inferred\nimage is retrieved to modulate the generator. Moreover, the universal prior\nlearned from the random noise has been memorized by training the modulation\nnetwork. Experimental results show the superiority of the proposed method\ncompared with the state-of-the-art methods, and a good generalization in the\nwild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huaibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaofei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAP-Net: Region Attention Predictive Network for Precipitation Nowcasting. (arXiv:2110.01035v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01035","description":"<p>Natural disasters caused by heavy rainfall often cost huge loss of life and\nproperty. To avoid it, the task of precipitation nowcasting is imminent. To\nsolve the problem, increasingly deep learning methods are proposed to forecast\nfuture radar echo images and then the predicted maps have converted the\ndistribution of rainfall. The prevailing spatiotemporal sequence prediction\nmethods apply ConvRNN structure which combines the Convolution and Recurrent\nneural network. Although improvements based on ConvRNN achieve remarkable\nsuccess, these methods ignore capturing both local and global spatial features\nsimultaneously, which degrades the nowcasting in the region of heavy rainfall.\nTo address this issue, we proposed the Region Attention Block (RAB) and embed\nit into ConvRNN to enhance the forecast in the area with strong rainfall.\nBesides, the ConvRNN models are hard to memory longer history representations\nwith limited parameters. Considering it, we propose Recall Attention Mechanism\n(RAM) to improve the prediction. By preserving longer temporal information, RAM\ncontributes to the forecasting, especially in the middle rainfall intensity.\nThe experiments show that the proposed model Region Attention Predictive\nNetwork (RAP-Net) has outperformed the state-of-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chuyao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ZhengZhang/0/1/0/all/0/1\">ZhengZhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunming Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marginally calibrated response distributions for end-to-end learning in autonomous driving. (arXiv:2110.01050v1 [stat.ML])","link":"http://arxiv.org/abs/2110.01050","description":"<p>End-to-end learners for autonomous driving are deep neural networks that\npredict the instantaneous steering angle directly from images of the\nahead-lying street. These learners must provide reliable uncertainty estimates\nfor their predictions in order to meet safety requirements and initiate a\nswitch to manual control in areas of high uncertainty. Yet end-to-end learners\ntypically only deliver point predictions, since distributional predictions are\nassociated with large increases in training time or additional computational\nresources during prediction. To address this shortcoming we investigate\nefficient and scalable approximate inference for the implicit copula neural\nlinear model of Klein, Nott and Smith (2021) in order to quantify uncertainty\nfor the predictions of end-to-end learners. The result are densities for the\nsteering angle that are marginally calibrated, i.e.~the average of the\nestimated densities equals the empirical distribution of steering angles. To\nensure the scalability to large $n$ regimes, we develop efficient estimation\nbased on variational inference as a fast alternative to computationally\nintensive, exact inference via Hamiltonian Monte Carlo. We demonstrate the\naccuracy and speed of the variational approach in comparison to Hamiltonian\nMonte Carlo on two end-to-end learners trained for highway driving using the\ncomma2k19 data set. The implicit copula neural linear model delivers accurate\ncalibration, high-quality prediction intervals and allows to identify\noverconfident learners. Our approach also contributes to the explainability of\nblack-box end-to-end learners, since predictive densities can be used to\nunderstand which steering actions the end-to-end learner sees as valid.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Hoffmann_C/0/1/0/all/0/1\">Clara Hoffmann</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Klein_N/0/1/0/all/0/1\">Nadja Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control. (arXiv:2110.01052v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01052","description":"<p>We introduce Learn then Test (LTT), a framework for calibrating machine\nlearning models so that their predictions satisfy explicit, finite-sample\nstatistical guarantees regardless of the underlying model and (unknown)\ndata-generating distribution. The framework addresses, among other examples,\nfalse discovery rate control in multi-label classification,\nintersection-over-union control in instance segmentation, and the simultaneous\ncontrol of the type-1 error of outlier detection and confidence set coverage in\nclassification or regression. To accomplish this, we solve a key technical\nchallenge: the control of arbitrary risks that are not necessarily monotonic.\nOur main insight is to reframe the risk-control problem as multiple hypothesis\ntesting, enabling techniques and mathematical arguments different from those in\nthe previous literature. We use our framework to provide new calibration\nmethods for several core machine learning tasks with detailed worked examples\nin computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios N. Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candes_E/0/1/0/all/0/1\">Emmanuel J. Cand&#xe8;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lihua Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Approach for Image Authentication Framework for Media Forensics Purpose. (arXiv:2110.01065v1 [cs.CR])","link":"http://arxiv.org/abs/2110.01065","description":"<p>With the increasing widely spread digital media become using in most fields\nsuch as medical care, Oceanography, Exploration processing, security purpose,\nmilitary fields and astronomy, evidence in criminals and more vital fields and\nthen digital Images become have different appreciation values according to what\nis important of carried information by digital images. Due to the easy\nmanipulation property of digital images (by proper computer software) makes us\ndoubtful when are juries using digital images as forensic evidence in courts,\nespecially, if the digital images are main evidence to demonstrate the\nrelationship between suspects and the criminals. Obviously, here demonstrate\nimportance of data Originality Protection methods to detect unauthorized\nprocess like modification or duplication and then enhancement protection of\nevidence to guarantee rights of incriminatory. In this paper, we shall\nintroduce a novel digital forensic security framework for digital image\nauthentication and originality identification techniques and related\nmethodologies, algorithms and protocols that are applied on camera captured\nimages. The approach depends on implanting secret code into RGB images that\nshould indicate any unauthorized modification on the image under investigation.\nThe secret code generation depends mainly on two main parameter types, namely\nthe image characteristics and capturing device identifier. In this paper, the\narchitecture framework will be analyzed, explained and discussed together with\nthe associated protocols, algorithms and methodologies. Also, the secret code\ndeduction and insertion techniques will be analyzed and discussed, in addition\nto the image benchmarking and quality testing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagm_A/0/1/0/all/0/1\">Ahmad M Nagm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youssef_K/0/1/0/all/0/1\">Khaled Y Youssef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youssef_M/0/1/0/all/0/1\">Mohammad I Youssef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhance Images as You Like with Unpaired Learning. (arXiv:2110.01161v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01161","description":"<p>Low-light image enhancement exhibits an ill-posed nature, as a given image\nmay have many enhanced versions, yet recent studies focus on building a\ndeterministic mapping from input to an enhanced version. In contrast, we\npropose a lightweight one-path conditional generative adversarial network\n(cGAN) to learn a one-to-many relation from low-light to normal-light image\nspace, given only sets of low- and normal-light training images without any\ncorrespondence. By formulating this ill-posed problem as a modulation code\nlearning task, our network learns to generate a collection of enhanced images\nfrom a given input conditioned on various reference images. Therefore our\ninference model easily adapts to various user preferences, provided with a few\nfavorable photos from each user. Our model achieves competitive visual and\nquantitative results on par with fully supervised methods on both noisy and\nclean datasets, while being 6 to 10 times lighter than state-of-the-art\ngenerative adversarial networks (GANs) approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaopeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muxingzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lubin Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Kernel Representation for Image Reconstruction in PET. (arXiv:2110.01174v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01174","description":"<p>Image reconstruction for positron emission tomography (PET) is challenging\nbecause of the ill-conditioned tomographic problem and low counting statistics.\nKernel methods address this challenge by using kernel representation to\nincorporate image prior information in the forward model of iterative PET image\nreconstruction. Existing kernel methods construct the kernels commonly using an\nempirical process, which may lead to suboptimal performance. In this paper, we\ndescribe the equivalence between the kernel representation and a trainable\nneural network model. A deep kernel method is proposed by exploiting deep\nneural networks to enable an automated learning of an optimized kernel model.\nThe proposed method is directly applicable to single subjects. The training\nprocess utilizes available image prior data to seek the best way to form a set\nof robust kernels optimally rather than empirically. The results from computer\nsimulations and a real patient dataset demonstrate that the proposed deep\nkernel method can outperform existing kernel method and neural network method\nfor dynamic PET image reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Siqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guobao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BPFNet: A Unified Framework for Bimodal Palmprint Alignment and Fusion. (arXiv:2110.01179v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01179","description":"<p>Bimodal palmprint recognition leverages palmprint and palm vein images\nsimultaneously,which achieves high accuracy by multi-model information fusion\nand has strong anti-falsification property. In the recognition pipeline, the\ndetection of palm and the alignment of region-of-interest (ROI) are two crucial\nsteps for accurate matching. Most existing methods localize palm ROI by\nkeypoint detection algorithms, however the intrinsic difficulties of keypoint\ndetection tasks make the results unsatisfactory. Besides, the ROI alignment and\nfusion algorithms at image-level are not fully investigaged.To bridge the gap,\nin this paper, we propose Bimodal Palmprint Fusion Network (BPFNet) which\nfocuses on ROI localization, alignment and bimodal image fusion.BPFNet is an\nend-to-end framework containing two subnets: The detection network directly\nregresses the palmprint ROIs based on bounding box prediction and conducts\nalignment by translation estimation.In the downstream,the bimodal fusion\nnetwork implements bimodal ROI image fusion leveraging a novel proposed\ncross-modal selection scheme. To show the effectiveness of BPFNet,we carry out\nexperiments on the large-scale touchless palmprint datasets CUHKSZ-v1 and\nTongJi and the proposed method achieves state-of-the-art performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoqun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Dandan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adding Quaternion Representations to Attention Networks for Classification. (arXiv:2110.01185v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01185","description":"<p>This paper introduces a novel modification to axial-attention networks to\nimprove their image classification accuracy. The modification involves\nsupplementing axial-attention modules with quaternion input representations to\nimprove image classification accuracy. We chose axial-attention networks\nbecause they factor 2D attention operations into two consecutive 1D operations\n(similar to separable convolution) and are thus less resource intensive than\nnon-axial attention networks. We chose a quaternion encoder because of they\nshare weights across four real-valued input channels and the weight-sharing has\nbeen shown to produce a more interlinked/interwoven output representation. We\nhypothesize that an attention module can be more effective using these\ninterlinked representations as input. Our experiments support this hypothesis\nas reflected in the improved classification accuracy compared to standard\naxial-attention networks. We think this happens because the attention modules\nhave better input representations to work with.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahadat_N/0/1/0/all/0/1\">Nazmul Shahadat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1\">Anthony S. Maida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Structural Representations for Recipe Generation and Food Retrieval. (arXiv:2110.01209v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01209","description":"<p>Food is significant to human daily life. In this paper, we are interested in\nlearning structural representations for lengthy recipes, that can benefit the\nrecipe generation and food retrieval tasks. We mainly investigate an open\nresearch task of generating cooking instructions based on food images and\ningredients, which is similar to the image captioning task. However, compared\nwith image captioning datasets, the target recipes are lengthy paragraphs and\ndo not have annotations on structure information. To address the above\nlimitations, we propose a novel framework of Structure-aware Generation Network\n(SGN) to tackle the food recipe generation task. Our approach brings together\nseveral novel ideas in a systematic framework: (1) exploiting an unsupervised\nlearning approach to obtain the sentence-level tree structure labels before\ntraining; (2) generating trees of target recipes from images with the\nsupervision of tree structure labels learned from (1); and (3) integrating the\ninferred tree structures into the recipe generation procedure. Our proposed\nmodel can produce high-quality and coherent recipes, and achieve the\nstate-of-the-art performance on the benchmark Recipe1M dataset. We also\nvalidate the usefulness of our learned tree structures in the food cross-modal\nretrieval task, where the proposed model with tree representations can\noutperform state-of-the-art benchmark results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Max and Coincidence Neurons in Neural Networks. (arXiv:2110.01218v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01218","description":"<p>Network design has been a central topic in machine learning. Large amounts of\neffort have been devoted towards creating efficient architectures through\nmanual exploration as well as automated neural architecture search. However,\ntodays architectures have yet to consider the diversity of neurons and the\nexistence of neurons with specific processing functions. In this work, we\noptimize networks containing models of the max and coincidence neurons using\nneural architecture search, and analyze the structure, operations, and neurons\nof optimized networks to develop a signal-processing ResNet. The developed\nnetwork achieves an average of 2% improvement in accuracy and a 25% improvement\nin network size across a variety of datasets, demonstrating the importance of\nneuronal functions in creating compact, efficient networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Albert Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kang L. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation for Human Action Anticipation. (arXiv:1904.04868v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.04868","description":"<p>We consider the task of training a neural network to anticipate human actions\nin video. This task is challenging given the complexity of video data, the\nstochastic nature of the future, and the limited amount of annotated training\ndata. In this paper, we propose a novel knowledge distillation framework that\nuses an action recognition network to supervise the training of an action\nanticipation network, guiding the latter to attend to the relevant information\nneeded for correctly anticipating the future actions. This framework is\npossible thanks to a novel loss function to account for positional shifts of\nsemantic concepts in a dynamic video. The knowledge distillation framework is a\nform of self-supervised learning, and it takes advantage of unlabeled data.\nExperimental results on JHMDB and EPIC-KITCHENS dataset show the effectiveness\nof our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1\">Minh Hoai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new approach for measuring semantic similarity of ontology concepts using dynamic programming. (arXiv:1904.08501v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.08501","description":"<p>Today, with the emergence of semantic web technologies and increasing of\ninformation quantity, searching for information based on the semantic web has\nbecome a fertile area of research. For this reason, a large number of studies\nare performed based on the measure of semantic similarity. Therefore, in this\npaper, we propose a new method of semantic similarity measuring which uses the\ndynamic programming to compute the semantic distance between any two concepts\ndefined in the same hierarchy of ontology. Then, we base on this result to\ncompute the semantic similarity. Finally, we present an experimental comparison\nbetween our method and other methods of similarity measuring. Where we will\nshow the limits of these methods and how we avoid them with our method. This\none bases on a function of weight allocation, which allows finding different\nrate of semantic similarity between a given concept and two other sibling\nconcepts which is impossible using the other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gherabi_N/0/1/0/all/0/1\">Noreddine Gherabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoui_A/0/1/0/all/0/1\">Abdelhadi Daoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzouk_A/0/1/0/all/0/1\">Abderrahim Marzouk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Merging-ISP: Multi-Exposure High Dynamic Range Image Signal Processing. (arXiv:1911.04762v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1911.04762","description":"<p>High dynamic range (HDR) imaging combines multiple images with different\nexposure times into a single high-quality image. The image signal processing\npipeline (ISP) is a core component in digital cameras to perform these\noperations. It includes demosaicing of raw color filter array (CFA) data at\ndifferent exposure times, alignment of the exposures, conversion to HDR domain,\nand exposure merging into an HDR image. Traditionally, such pipelines cascade\nalgorithms that address these individual subtasks. However, cascaded designs\nsuffer from error propagation, since simply combining multiple steps is not\nnecessarily optimal for the entire imaging task. This paper proposes a\nmulti-exposure HDR image signal processing pipeline (Merging-ISP) to jointly\nsolve all these subtasks. Our pipeline is modeled by a deep neural network\narchitecture. As such, it is end-to-end trainable, circumvents the use of\nhand-crafted and potentially complex algorithms, and mitigates error\npropagation. Merging-ISP enables direct reconstructions of HDR images of\ndynamic scenes from multiple raw CFA images with different exposures. We\ncompare Merging-ISP against several state-of-the-art cascaded pipelines. The\nproposed method provides HDR reconstructions of high perceptual quality and it\nquantitatively outperforms competing ISPs by more than 1 dB in terms of PSNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chaudhari_P/0/1/0/all/0/1\">Prashant Chaudhari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schirrmacher_F/0/1/0/all/0/1\">Franziska Schirrmacher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riess_C/0/1/0/all/0/1\">Christian Riess</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohler_T/0/1/0/all/0/1\">Thomas K&#xf6;hler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection and Tracking Meet Drones Challenge. (arXiv:2001.06303v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.06303","description":"<p>Drones, or general UAVs, equipped with cameras have been fast deployed with a\nwide range of applications, including agriculture, aerial photography, and\nsurveillance. Consequently, automatic understanding of visual data collected\nfrom drones becomes highly demanding, bringing computer vision and drones more\nand more closely. To promote and track the developments of object detection and\ntracking algorithms, we have organized three challenge workshops in conjunction\nwith ECCV 2018, ICCV 2019 and ECCV 2020, attracting more than 100 teams around\nthe world. We provide a large-scale drone captured dataset, VisDrone, which\nincludes four tracks, i.e., (1) image object detection, (2) video object\ndetection, (3) single object tracking, and (4) multi-object tracking. In this\npaper, we first present a thorough review of object detection and tracking\ndatasets and benchmarks, and discuss the challenges of collecting large-scale\ndrone-based object detection and tracking datasets with fully manual\nannotations. After that, we describe our VisDrone dataset, which is captured\nover various urban/suburban areas of 14 different cities across China from\nNorth to South. Being the largest such dataset ever published, VisDrone enables\nextensive evaluation and investigation of visual analysis algorithms for the\ndrone platform. We provide a detailed analysis of the current state of the\nfield of large-scale object detection and tracking on drones, and conclude the\nchallenge as well as propose future directions. We expect the benchmark largely\nboost the research and development in video analysis on drone platforms. All\nthe datasets and experimental results can be downloaded from\nhttps://github.com/VisDrone/VisDrone-Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1\">Xiao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VPR-Bench: An Open-Source Visual Place Recognition Evaluation Framework with Quantifiable Viewpoint and Appearance Change. (arXiv:2005.08135v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.08135","description":"<p>Visual Place Recognition (VPR) is the process of recognising a previously\nvisited place using visual information, often under varying appearance\nconditions and viewpoint changes and with computational constraints. VPR is\nrelated to the concepts of localisation, loop closure, image retrieval and is a\ncritical component of many autonomous navigation systems ranging from\nautonomous vehicles to drones and computer vision systems. While the concept of\nplace recognition has been around for many years, VPR research has grown\nrapidly as a field over the past decade due to improving camera hardware and\nits potential for deep learning-based techniques, and has become a widely\nstudied topic in both the computer vision and robotics communities. This growth\nhowever has led to fragmentation and a lack of standardisation in the field,\nespecially concerning performance evaluation. Moreover, the notion of viewpoint\nand illumination invariance of VPR techniques has largely been assessed\nqualitatively and hence ambiguously in the past. In this paper, we address\nthese gaps through a new comprehensive open-source framework for assessing the\nperformance of VPR techniques, dubbed \"VPR-Bench\". VPR-Bench (Open-sourced at:\nhttps://github.com/MubarizZaffar/VPR-Bench) introduces two much-needed\ncapabilities for VPR researchers: firstly, it contains a benchmark of 12\nfully-integrated datasets and 10 VPR techniques, and secondly, it integrates a\ncomprehensive variation-quantified dataset for quantifying viewpoint and\nillumination invariance. We apply and analyse popular evaluation metrics for\nVPR from both the computer vision and robotics communities, and discuss how\nthese different metrics complement and/or replace each other, depending upon\nthe underlying applications and system requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaffar_M/0/1/0/all/0/1\">Mubariz Zaffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooij_J/0/1/0/all/0/1\">Julian Kooij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flynn_D/0/1/0/all/0/1\">David Flynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_Maier_K/0/1/0/all/0/1\">Klaus McDonald-Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsan_S/0/1/0/all/0/1\">Shoaib Ehsan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-View Fine-grained Classification of Plant Species. (arXiv:2005.09110v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.09110","description":"<p>Automatic plant classification is a challenging problem due to the wide\nbiodiversity of the existing plant species in a fine-grained scenario. Powerful\ndeep learning architectures have been used to improve the classification\nperformance in such a fine-grained problem, but usually building models that\nare highly dependent on a large training dataset and which are not scalable. In\nthis paper, we propose a novel method based on a two-view leaf image\nrepresentation and a hierarchical classification strategy for fine-grained\nrecognition of plant species. It uses the botanical taxonomy as a basis for a\ncoarse-to-fine strategy applied to identify the plant genus and species. The\ntwo-view representation provides complementary global and local features of\nleaf images. A deep metric based on Siamese convolutional neural networks is\nused to reduce the dependence on a large number of training samples and make\nthe method scalable to new plant species. The experimental results on two\nchallenging fine-grained datasets of leaf images (i.e. LifeCLEF 2015 and\nLeafSnap) have shown the effectiveness of the proposed method, which achieved\nrecognition accuracy of 0.87 and 0.96 respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Voncarlos M. Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1\">Alceu S. Britto Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1\">Luiz E. S. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerich_A/0/1/0/all/0/1\">Alessandro L. Koerich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks. (arXiv:2007.11679v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.11679","description":"<p>We present a new versatile building block for deep point cloud processing\narchitectures that is equally suited for diverse tasks. This building block\ncombines the ideas of spatial transformers and multi-view convolutional\nnetworks with the efficiency of standard convolutional layers in two and\nthree-dimensional dense grids. The new block operates via multiple parallel\nheads, whereas each head differentiably rasterizes feature representations of\nindividual points into a low-dimensional space, and then uses dense convolution\nto propagate information across points. The results of the processing of\nindividual heads are then combined together resulting in the update of point\nfeatures. Using the new block, we build architectures for both discriminative\n(point cloud segmentation, point cloud classification) and generative (point\ncloud inpainting and image-based point cloud reconstruction) tasks. The\nresulting architectures achieve state-of-the-art performance for these tasks,\ndemonstrating the versatility of the new block for point cloud processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mazur_K/0/1/0/all/0/1\">Kirill Mazur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1\">Victor Lempitsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Families In Wild Multimedia: A Multimodal Database for Recognizing Kinship. (arXiv:2007.14509v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.14509","description":"<p>Kinship, a soft biometric detectable in media, is fundamental for a myriad of\nuse-cases. Despite the difficulty of detecting kinship, annual data challenges\nusing still-images have consistently improved performances and attracted new\nresearchers. Now, systems reach performance levels unforeseeable a decade ago,\nclosing in on performances acceptable to deploy in practice. Like other\nbiometric tasks, we expect systems can receive help from other modalities. We\nhypothesize that adding modalities to FIW, which has only still-images, will\nimprove performance. Thus, to narrow the gap between research and reality and\nenhance the power of kinship recognition systems, we extend FIW with multimedia\n(MM) data (i.e., video, audio, and text captions). Specifically, we introduce\nthe first publicly available multi-task MM kinship dataset. To build FIW MM, we\ndeveloped machinery to automatically collect, annotate, and prepare the data,\nrequiring minimal human input and no financial cost. The proposed MM corpus\nallows the problem statements to be more realistic template-based protocols. We\nshow significant improvements in all benchmarks with the added modalities. The\nresults highlight edge cases to inspire future research with different areas of\nimprovement. FIW MM supplies the data needed to increase the potential of\nautomated systems to detect kinship in MM. It also allows experts from diverse\nfields to collaborate in novel ways.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joseph P. Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Ming Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Color Transfer. (arXiv:2008.13626v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.13626","description":"<p>Color transfer, which plays a key role in image editing, has attracted\nnoticeable attention recently. It has remained a challenge to date due to\nvarious issues such as time-consuming manual adjustments and prior segmentation\nissues. In this paper, we propose to model color transfer under a probability\nframework and cast it as a parameter estimation problem. In particular, we\nrelate the transferred image with the example image under the Gaussian Mixture\nModel (GMM) and regard the transferred image color as the GMM centroids. We\nemploy the Expectation-Maximization (EM) algorithm (E-step and M-step) for\noptimization. To better preserve gradient information, we introduce a Laplacian\nbased regularization term to the objective function at the M-step which is\nsolved by deriving a gradient descent algorithm. Given the input of a source\nimage and an example image, our method is able to generate continuous color\ntransfer results with increasing EM iterations. Various experiments show that\nour approach generally outperforms other competitive color transfer methods,\nboth visually and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chunzhi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grow-Push-Prune: aligning deep discriminants for effective structural network compression. (arXiv:2009.13716v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.13716","description":"<p>Most of today's popular deep architectures are hand-engineered to be\ngeneralists. However, this design procedure usually leads to massive redundant,\nuseless, or even harmful features for specific tasks. Unnecessarily high\ncomplexities render deep nets impractical for many real-world applications,\nespecially those without powerful GPU support. In this paper, we attempt to\nderive task-dependent compact models from a deep discriminant analysis\nperspective. We propose an iterative and proactive approach for classification\ntasks which alternates between (1) a pushing step, with an objective to\nsimultaneously maximize class separation, penalize co-variances, and push deep\ndiscriminants into alignment with a compact set of neurons, and (2) a pruning\nstep, which discards less useful or even interfering neurons. Deconvolution is\nadopted to reverse 'unimportant' filters' effects and recover useful\ncontributing sources. A simple network growing strategy based on the basic\nInception module is proposed for challenging tasks requiring larger capacity\nthan what the base net can offer. Experiments on the MNIST, CIFAR10, and\nImageNet datasets demonstrate our approach's efficacy. On ImageNet, by pushing\nand pruning our grown Inception-88 model, we achieve more accurate models than\nInception nets generated during growing, residual nets, and popular compact\nnets at similar sizes. We also show that our grown Inception nets (without\nhard-coded dimension alignment) clearly outperform residual nets of similar\ncomplexities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qing Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">James J. Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmark for Anonymous Video Analytics. (arXiv:2009.14684v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.14684","description":"<p>Out-of-home audience measurement aims to count and characterize the people\nexposed to advertising content in the physical world. While audience\nmeasurement solutions based on computer vision are of increasing interest, no\ncommonly accepted benchmark exists to evaluate and compare their performance.\nIn this paper, we propose the first benchmark for digital out-of-home audience\nmeasurement that evaluates the vision-based tasks of audience localization and\ncounting, and audience demographics. The benchmark is composed of a novel,\ndataset captured at multiple locations and a set of performance measures. Using\nthe benchmark, we present an in-depth comparison of eight open-source\nalgorithms on four hardware platforms with GPU and CPU-optimized inferences and\nof two commercial off-the-shelf solutions for localization, count, age, and\ngender estimation. This benchmark and related open-source codes are available\nat <a href=\"http://ava.eecs.qmul.ac.uk.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Matilla_R/0/1/0/all/0/1\">Ricardo Sanchez-Matilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposure Trajectory Recovery from Motion Blur. (arXiv:2010.02484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.02484","description":"<p>Motion blur in dynamic scenes is an important yet challenging research topic.\nRecently, deep learning methods have achieved impressive performance for\ndynamic scene deblurring. However, the motion information contained in a blurry\nimage has yet to be fully explored and accurately formulated because: (i) the\nground truth of dynamic motion is difficult to obtain; (ii) the temporal\nordering is destroyed during the exposure; and (iii) the motion estimation from\na blurry image is highly ill-posed. By revisiting the principle of camera\nexposure, motion blur can be described by the relative motions of sharp content\nwith respect to each exposed position. In this paper, we define exposure\ntrajectories, which represent the motion information contained in a blurry\nimage and explain the causes of motion blur. A novel motion offset estimation\nframework is proposed to model pixel-wise displacements of the latent sharp\nimage at multiple timepoints. Under mild constraints, our method can recover\ndense, (non-)linear exposure trajectories, which significantly reduce temporal\ndisorder and ill-posed problems. Finally, experiments demonstrate that the\nrecovered exposure trajectories not only capture accurate and interpretable\nmotion information from a blurry image, but also benefit motion-aware image\ndeblurring and warping-based video extraction tasks. Codes are available on\nhttps://github.com/yjzhang96/Motion-ETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1\">Stephen J. Maybank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Action Understanding. (arXiv:2010.06647v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.06647","description":"<p>Many believe that the successes of deep learning on image understanding\nproblems can be replicated in the realm of video understanding. However, due to\nthe scale and temporal nature of video, the span of video understanding\nproblems and the set of proposed deep learning solutions is arguably wider and\nmore diverse than those of their 2D image siblings. Finding, identifying, and\npredicting actions are a few of the most salient tasks in this emerging and\nrapidly evolving field. With a pedagogical emphasis, this tutorial introduces\nand systematizes fundamental topics, basic concepts, and notable examples in\nsupervised video action understanding. Specifically, we clarify a taxonomy of\naction problems, catalog and highlight video datasets, describe common video\ndata preparation methods, present the building blocks of state-of-the art deep\nlearning model architectures, and formalize domain-specific metrics to baseline\nproposed solutions. This tutorial is intended to be accessible to a general\ncomputer science audience and assumes a conceptual understanding of supervised\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_M/0/1/0/all/0/1\">Matthew Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadepally_V/0/1/0/all/0/1\">Vijay Gadepally</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v11 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.04076","description":"<p>Visual attention is one of the most significant characteristics for selecting\nand understanding the outside redundancy world. The human vision system cannot\nprocess all information simultaneously, due to the visual information\nbottleneck. In order to reduce the redundant input of visual information, the\nhuman visual system mainly focuses on dominant parts of scenes. This is\ncommonly known as visual saliency map prediction. This paper proposed a new\npsychophysical saliency prediction architecture, WECSF, inspired by\nmulti-channel model of visual cortex functioning in humans. The model consists\nof opponent color channels, wavelet transform, wavelet energy map, and contrast\nsensitivity function for extracting low-level image features and providing\nmaximum approximation to the human visual system. The proposed model is\nevaluated using several datasets, including the MIT1003, MIT300, TORONTO,\nSID4VAM, and UCF Sports datasets. We also quantitatively and qualitatively\ncompare the saliency prediction performance with that of other state-of-the-art\nmodels. Our model achieved strongly stable and better performance with\ndifferent metrics on nature images, psychophysical synthetic images and dynamic\nvideos. Additionally, we found that Fourier and spectral-inspired saliency\nprediction models outperformed other state-of-the-art non-neural network and\neven deep neural network models on psychophysical synthetic images, it can be\nexplained and supported the Fourier Vision Hypothesis. Finally, the proposed\nmodel could be used as a computational model of primate vision system and help\nus understand mechanism of vision system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Representations of Positive Functions via First and Second-Order Pseudo-Mirror Descent. (arXiv:2011.07142v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2011.07142","description":"<p>We consider expected risk minimization when the range of the estimator is\nrequired to be nonnegative, motivated by the settings of maximum likelihood\nestimation (MLE) and trajectory optimization. To facilitate nonlinear\ninterpolation, we hypothesize that search is conducted over a Reproducing\nKernel Hilbert Space (RKHS). To solve it, we develop first and second-order\nvariants of stochastic mirror descent employing (i) pseudo-gradients and (ii)\ncomplexity-reducing projections. Compressive projection in first-order scheme\nis executed via kernel orthogonal matching pursuit (KOMP), and overcome the\nfact that the vanilla RKHS parameterization grows unbounded with time.\nMoreover, pseudo-gradients are needed when stochastic estimates of the gradient\nof the expected cost are only computable up to some numerical errors, which\narise in, e.g., integral approximations. The second-order scheme develops a\nHessian inverse approximation via recursively averaged pseudo-gradient outer\nproducts. For the first-order scheme, we establish tradeoffs between accuracy\nof convergence in mean and the projection budget parameter under constant\nstep-size and compression budget are established, as well as non-asymptotic\nbounds on the model complexity. Analogous convergence results are established\nfor the second-order scheme under an additional eigenvalue decay condition on\nthe Hessian of the optimal RKHS element. Experiments demonstrate favorable\nperformance on inhomogeneous Poisson Process intensity estimation in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Chakraborty_A/0/1/0/all/0/1\">Abhishek Chakraborty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rajawat_K/0/1/0/all/0/1\">Ketan Rajawat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CM-Net: Concentric Mask based Arbitrary-Shaped Text Detection. (arXiv:2011.14714v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14714","description":"<p>Recently fast arbitrary-shaped text detection has become an attractive\nresearch topic. However, most existing methods are non-real-time, which may\nfall short in intelligent systems. Although a few real-time text methods are\nproposed, the detection accuracy is far behind non-real-time methods. To\nimprove the detection accuracy and speed simultaneously, we propose a novel\nfast and accurate text detection framework, namely CM-Net, which is constructed\nbased on a new text representation method and a multi-perspective feature (MPF)\nmodule. The former can fit arbitrary-shaped text contours by concentric mask\n(CM) in an efficient and robust way. The latter encourages the network to learn\nmore CM-related discriminative features from multiple perspectives and brings\nno extra computational cost. Benefiting the advantages of CM and MPF, the\nproposed CM-Net only needs to predict one CM of the text instance to rebuild\nthe text contour and achieves the best balance between detection accuracy and\nspeed compared with previous works. Moreover, to ensure that multi-perspective\nfeatures are effectively learned, the multi-factor constraints loss is\nproposed. Extensive experiments demonstrate the proposed CM is efficient and\nrobust to fit arbitrary-shaped text instances, and also validate the\neffectiveness of MPF and constraints loss for discriminative text features\nrecognition. Furthermore, experimental results show that the proposed CM-Net is\nsuperior to existing state-of-the-art (SOTA) real-time text detection methods\nin both detection speed and accuracy on MSRA-TD500, CTW1500, Total-Text, and\nICDAR2015 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhitong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member_S/0/1/0/all/0/1\">Senior Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Member_S/0/1/0/all/0/1\">Senior Member</a>, <a href=\"http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1\">IEEE</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in Vision: A Survey. (arXiv:2101.01169v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.01169","description":"<p>Astounding results from Transformer models on natural language tasks have\nintrigued the vision community to study their application to computer vision\nproblems. Among their salient benefits, Transformers enable modeling long\ndependencies between input sequence elements and support parallel processing of\nsequence as compared to recurrent networks e.g., Long short-term memory (LSTM).\nDifferent from convolutional networks, Transformers require minimal inductive\nbiases for their design and are naturally suited as set-functions. Furthermore,\nthe straightforward design of Transformers allows processing multiple\nmodalities (e.g., images, videos, text and speech) using similar processing\nblocks and demonstrates excellent scalability to very large capacity networks\nand huge datasets. These strengths have led to exciting progress on a number of\nvision tasks using Transformer networks. This survey aims to provide a\ncomprehensive overview of the Transformer models in the computer vision\ndiscipline. We start with an introduction to fundamental concepts behind the\nsuccess of Transformers i.e., self-attention, large-scale pre-training, and\nbidirectional encoding. We then cover extensive applications of transformers in\nvision including popular recognition tasks (e.g., image classification, object\ndetection, action recognition, and segmentation), generative modeling,\nmulti-modal tasks (e.g., visual-question answering, visual reasoning, and\nvisual grounding), video processing (e.g., activity recognition, video\nforecasting), low-level vision (e.g., image super-resolution, image\nenhancement, and colorization) and 3D analysis (e.g., point cloud\nclassification and segmentation). We compare the respective advantages and\nlimitations of popular techniques both in terms of architectural design and\ntheir experimental value. Finally, we provide an analysis on open research\ndirections and possible future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_S/0/1/0/all/0/1\">Syed Waqas Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN-Control: Explicitly Controllable GANs. (arXiv:2101.02477v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.02477","description":"<p>We present a framework for training GANs with explicit control over generated\nimages. We are able to control the generated image by settings exact attributes\nsuch as age, pose, expression, etc. Most approaches for editing GAN-generated\nimages achieve partial control by leveraging the latent space disentanglement\nproperties, obtained implicitly after standard GAN training. Such methods are\nable to change the relative intensity of certain attributes, but not explicitly\nset their values. Recently proposed methods, designed for explicit control over\nhuman faces, harness morphable 3D face models to allow fine-grained control\ncapabilities in GANs. Unlike these methods, our control is not constrained to\nmorphable 3D face model parameters and is extendable beyond the domain of human\nfaces. Using contrastive learning, we obtain GANs with an explicitly\ndisentangled latent space. This disentanglement is utilized to train\ncontrol-encoders mapping human-interpretable inputs to suitable latent vectors,\nthus allowing explicit control. In the domain of human faces we demonstrate\ncontrol over identity, age, pose, expression, hair color and illumination. We\nalso demonstrate control capabilities of our framework in the domains of\npainted portraits and dog image generation. We demonstrate that our approach\nachieves state-of-the-art performance both qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shoshan_A/0/1/0/all/0/1\">Alon Shoshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhonker_N/0/1/0/all/0/1\">Nadav Bhonker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kviatkovsky_I/0/1/0/all/0/1\">Igor Kviatkovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medioni_G/0/1/0/all/0/1\">Gerard Medioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised driven consistency training for annotation efficient histopathology image analysis. (arXiv:2102.03897v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03897","description":"<p>Training a neural network with a large labeled dataset is still a dominant\nparadigm in computational histopathology. However, obtaining such exhaustive\nmanual annotations is often expensive, laborious, and prone to inter and\nIntra-observer variability. While recent self-supervised and semi-supervised\nmethods can alleviate this need by learn-ing unsupervised feature\nrepresentations, they still struggle to generalize well to downstream tasks\nwhen the number of labeled instances is small. In this work, we overcome this\nchallenge by leveraging both task-agnostic and task-specific unlabeled data\nbased on two novel strategies: i) a self-supervised pretext task that harnesses\nthe underlying multi-resolution contextual cues in histology whole-slide images\nto learn a powerful supervisory signal for unsupervised representation\nlearning; ii) a new teacher-student semi-supervised consistency paradigm that\nlearns to effectively transfer the pretrained representations to downstream\ntasks based on prediction consistency with the task-specific un-labeled data.\nWe carry out extensive validation experiments on three histopathology benchmark\ndatasets across two classification and one regression-based tasks, i.e., tumor\nmetastasis detection, tissue type classification, and tumor cellularity\nquantification. Under limited-label data, the proposed method yields tangible\nimprovements, which is close or even outperforming other state-of-the-art\nself-supervised and supervised baselines. Furthermore, we empirically show that\nthe idea of bootstrapping the self-supervised pretrained features is an\neffective way to improve the task-specific semi-supervised learning on standard\nbenchmarks. Code and pretrained models will be made available at:\nhttps://github.com/srinidhiPY/SSL_CR_Histo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinidhi_C/0/1/0/all/0/1\">Chetan L. Srinidhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fu-Der Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking Pixels for Reinforcement Learning via Implicit Attention. (arXiv:2102.04353v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.04353","description":"<p>There has recently been significant interest in training reinforcement\nlearning (RL) agents in vision-based environments. This poses many challenges,\nsuch as high dimensionality and the potential for observational overfitting\nthrough spurious correlations. A promising approach to solve both of these\nproblems is an attention bottleneck, which provides a simple and effective\nframework for learning high performing policies, even in the presence of\ndistractions. However, due to poor scalability of attention architectures,\nthese methods cannot be applied beyond low resolution visual inputs, using\nlarge patches (thus small attention matrices). In this paper we make use of new\nefficient attention algorithms, recently shown to be highly effective for\nTransformers, and demonstrate that these techniques can be successfully adopted\nfor the RL setting. This allows our attention-based controllers to scale to\nlarger visual inputs, and facilitate the use of smaller patches, even\nindividual pixels, improving generalization. We show this on a range of tasks\nfrom the Distracting Control Suite to vision-based quadruped robots locomotion.\nWe provide rigorous theoretical analysis of the proposed algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Marcin Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Deepali Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyou Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingnan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1\">Valerii Likhosherstov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1\">Aldo Pacchiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santara_A/0/1/0/all/0/1\">Anirban Santara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yunhao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jie Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"State-of-the-Art in Human Scanpath Prediction. (arXiv:2102.12239v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.12239","description":"<p>The last years have seen a surge in models predicting the scanpaths of\nfixations made by humans when viewing images. However, the field is lacking a\nprincipled comparison of those models with respect to their predictive power.\nIn the past, models have usually been evaluated based on comparing human\nscanpaths to scanpaths generated from the model. Here, instead we evaluate\nmodels based on how well they predict each fixation in a scanpath given the\nprevious scanpath history. This makes model evaluation closely aligned with the\nbiological processes thought to underly scanpath generation and allows to apply\nestablished saliency metrics like AUC and NSS in an intuitive and interpretable\nway. We evaluate many existing models of scanpath prediction on the datasets\nMIT1003, MIT300, CAT2000 train and CAT200 test, for the first time giving a\ndetailed picture of the current state of the art of human scanpath prediction.\nWe also show that the discussed method of model benchmarking allows for more\ndetailed analyses leading to interesting insights about where and when models\nfail to predict human behaviour. The MIT/Tuebingen Saliency Benchmark will\nimplement the evaluation of scanpath models as detailed here, allowing\nresearchers to score their models on the established benchmark datasets MIT300\nand CAT2000.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kummerer_M/0/1/0/all/0/1\">Matthias K&#xfc;mmerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Evaluation of Stacked Generative Adversarial Networks using Metamorphic Testing. (arXiv:2103.02870v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2103.02870","description":"<p>Synthesising photo-realistic images from natural language is one of the\nchallenging problems in computer vision. Over the past decade, a number of\napproaches have been proposed, of which the improved Stacked Generative\nAdversarial Network (StackGAN-v2) has proven capable of generating high\nresolution images that reflect the details specified in the input text\ndescriptions. In this paper, we aim to assess the robustness and\nfault-tolerance capability of the StackGAN-v2 model by introducing variations\nin the training data. However, due to the working principle of Generative\nAdversarial Network (GAN), it is difficult to predict the output of the model\nwhen the training data are modified. Hence, in this work, we adopt Metamorphic\nTesting technique to evaluate the robustness of the model with a variety of\nunexpected training dataset. As such, we first implement StackGAN-v2 algorithm\nand test the pre-trained model provided by the original authors to establish a\nground truth for our experiments. We then identify a metamorphic relation, from\nwhich test cases are generated. Further, metamorphic relations were derived\nsuccessively based on the observations of prior test results. Finally, we\nsynthesise the results from our experiment of all the metamorphic relations and\nfound that StackGAN-v2 algorithm is susceptible to input images with obtrusive\nobjects, even if it overlaps with the main object minimally, which was not\nreported by the authors and users of StackGAN-v2 model. The proposed\nmetamorphic relations can be applied to other text-to-image synthesis models to\nnot only verify the robustness but also to help researchers understand and\ninterpret the results made by the machine learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyejin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waseem_T/0/1/0/all/0/1\">Taaha Waseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_W/0/1/0/all/0/1\">Wen Qi Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_Y/0/1/0/all/0/1\">Ying Hwei Low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1\">Mei Kuan Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_C/0/1/0/all/0/1\">Chun Yong Chong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R-PointHop: A Green, Accurate and Unsupervised Point Cloud Registration Method. (arXiv:2103.08129v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08129","description":"<p>Inspired by the recent PointHop classification method, an unsupervised 3D\npoint cloud registration method, called R-PointHop, is proposed in this work.\nR-PointHop first determines a local reference frame (LRF) for every point using\nits nearest neighbors and finds its local attributes. Next, R-PointHop obtains\nlocal-to-global hierarchical features by point downsampling, neighborhood\nexpansion, attribute construction and dimensionality reduction steps. Thus, we\ncan build the correspondence of points in the hierarchical feature space using\nthe nearest neighbor rule. Afterwards, a subset of salient points of good\ncorrespondence is selected to estimate the 3D transformation. The use of LRF\nallows for hierarchical features of points to be invariant with respect to\nrotation and translation, thus making R-PointHop more robust in building point\ncorrespondence even when rotation angles are large. Experiments are conducted\non the 3DMatch, ModelNet40 and the Stanford Bunny dataset, which demonstrate\nthe effectiveness of R-PointHop on the 3D point cloud registration task.\nR-PointHop is a green and accurate solution since its model size and training\ntime are smaller than those of deep learning methods by an order of magnitude\nwhile its registration errors are smaller. Our codes are available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadam_P/0/1/0/all/0/1\">Pranav Kadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-FFS: Faster 3D object detection with Focused Frustum Search in sensor fusion based networks. (arXiv:2103.08294v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08294","description":"<p>In this work we propose 3D-FFS, a novel approach to make sensor fusion based\n3D object detection networks significantly faster using a class of\ncomputationally inexpensive heuristics. Existing sensor fusion based networks\ngenerate 3D region proposals by leveraging inferences from 2D object detectors.\nHowever, as images have no depth information, these networks rely on extracting\nsemantic features of points from the entire scene to locate the object. By\nleveraging aggregated intrinsic properties (e.g. point density) of point cloud\ndata, 3D-FFS can substantially constrain the 3D search space and thereby\nsignificantly reduce training time, inference time and memory consumption\nwithout sacrificing accuracy. To demonstrate the efficacy of 3D-FFS, we have\nintegrated it with Frustum ConvNet (F-ConvNet), a prominent sensor fusion based\n3D object detection model. We assess the performance of 3D-FFS on the KITTI\ndataset. Compared to F-ConvNet, we achieve improvements in training and\ninference times by up to 62.80% and 58.96%, respectively, while reducing the\nmemory usage by up to 58.53%. Additionally, we achieve 0.36%, 0.59% and 2.19%\nimprovements in accuracy for the Car, Pedestrian and Cyclist classes,\nrespectively. 3D-FFS shows a lot of promise in domains with limited computing\npower, such as autonomous vehicles, drones and robotics where LiDAR-Camera\nbased sensor fusion perception systems are widely used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Aniruddha Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishmam_T/0/1/0/all/0/1\">Tasin Ishmam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_K/0/1/0/all/0/1\">Khandker Aftarul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Zahidur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayzid_M/0/1/0/all/0/1\">Md. Shamsuzzoha Bayzid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pros and Cons of GAN Evaluation Measures: New Developments. (arXiv:2103.09396v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.09396","description":"<p>This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimization for Oriented Object Detection via Representation Invariance Loss. (arXiv:2103.11636v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11636","description":"<p>Arbitrary-oriented objects exist widely in natural scenes, and thus the\noriented object detection has received extensive attention in recent years. The\nmainstream rotation detectors use oriented bounding boxes (OBB) or\nquadrilateral bounding boxes (QBB) to represent the rotating objects. However,\nthese methods suffer from the representation ambiguity for oriented object\ndefinition, which leads to suboptimal regression optimization and the\ninconsistency between the loss metric and the localization accuracy of the\npredictions. In this paper, we propose a Representation Invariance Loss (RIL)\nto optimize the bounding box regression for the rotating objects. Specifically,\nRIL treats multiple representations of an oriented object as multiple\nequivalent local minima, and hence transforms bounding box regression into an\nadaptive matching process with these local minima. Then, the Hungarian matching\nalgorithm is adopted to obtain the optimal regression strategy. We also propose\na normalized rotation loss to alleviate the weak correlation between different\nvariables and their unbalanced loss contribution in OBB representation.\nExtensive experiments on remote sensing datasets and scene text datasets show\nthat our method achieves consistent and substantial improvement. The source\ncode and trained models are available at https://github.com/ming71/RIDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_L/0/1/0/all/0/1\">Lingjuan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yunpeng Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial expression and attributes recognition based on multi-task learning of lightweight neural networks. (arXiv:2103.17107v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.17107","description":"<p>In this paper, the multi-task learning of lightweight convolutional neural\nnetworks is studied for face identification and classification of facial\nattributes (age, gender, ethnicity) trained on cropped faces without margins.\nThe necessity to fine-tune these networks to predict facial expressions is\nhighlighted. Several models are presented based on MobileNet, EfficientNet and\nRexNet architectures. It was experimentally demonstrated that they lead to near\nstate-of-the-art results in age, gender and race recognition on the UTKFace\ndataset and emotion classification on the AffectNet dataset. Moreover, it is\nshown that the usage of the trained models as feature extractors of facial\nregions in video frames leads to 4.5% higher accuracy than the previously known\nstate-of-the-art single models for the AFEW and the VGAF datasets from the\nEmotiW challenges. The models and source code are publicly available at\nhttps://github.com/HSE-asavchenko/face-emotion-recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1\">Andrey V. Savchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiP-Net: Bidirectional Perspective Strategy based Arbitrary-Shaped Text Detection Network. (arXiv:2104.04903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04903","description":"<p>Detecting irregular-shaped text instances is the main challenge for text\ndetection. Existing approaches can be roughly divided into top-down and\nbottom-up perspective methods. The former encodes text contours into unified\nunits, which always fails to fit highly curved text contours. The latter\nrepresents text instances by a number of local units, where the complicated\nnetwork and post-processing lead to slow detection speed. In this paper, to\ndetect arbitrary-shaped text instances with high detection accuracy and speed\nsimultaneously, we propose a \\textbf{Bi}directional \\textbf{P}erspective\nstrategy based \\textbf{Net}work (BiP-Net). Specifically, a new text\nrepresentation strategy is proposed to represent text contours from a top-down\nperspective, which can fit highly curved text contours effectively. Moreover, a\ncontour connecting (CC) algorithm is proposed to avoid the information loss of\ntext contours by rebuilding interval contours from a bottom-up perspective. The\nexperimental results on MSRA-TD500, CTW1500, and ICDAR2015 datasets demonstrate\nthe superiority of BiP-Net against several state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discover the Unknown Biased Attribute of an Image Classifier. (arXiv:2104.14556v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14556","description":"<p>Recent works find that AI algorithms learn biases from data. Therefore, it is\nurgent and vital to identify biases in AI algorithms. However, the previous\nbias identification pipeline overly relies on human experts to conjecture\npotential biases (e.g., gender), which may neglect other underlying biases not\nrealized by humans. To help human experts better find the AI algorithms'\nbiases, we study a new problem in this work -- for a classifier that predicts a\ntarget attribute of the input image, discover its unknown biased attribute.\n</p>\n<p>To solve this challenging problem, we use a hyperplane in the generative\nmodel's latent space to represent an image attribute; thus, the original\nproblem is transformed to optimizing the hyperplane's normal vector and offset.\nWe propose a novel total-variation loss within this framework as the objective\nfunction and a new orthogonalization penalty as a constraint. The latter\nprevents trivial solutions in which the discovered biased attribute is\nidentical with the target or one of the known-biased attributes. Extensive\nexperiments on both disentanglement datasets and real-world datasets show that\nour method can discover biased attributes and achieve better disentanglement\nw.r.t. target attributes. Furthermore, the qualitative results show that our\nmethod can discover unnoticeable biased attributes for various object and scene\nclassifiers, proving our method's generalizability for detecting biased\nattributes in diverse domains of images. The code is available at\nhttps://git.io/J3kMh.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to drive from a world on rails. (arXiv:2105.00636v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.00636","description":"<p>We learn an interactive vision-based driving policy from pre-recorded driving\nlogs via a model-based approach. A forward model of the world supervises a\ndriving policy that predicts the outcome of any potential driving trajectory.\nTo support learning from pre-recorded logs, we assume that the world is on\nrails, meaning neither the agent nor its actions influence the environment.\nThis assumption greatly simplifies the learning problem, factorizing the\ndynamics into a nonreactive world model and a low-dimensional and compact\nforward model of the ego-vehicle. Our approach computes action-values for each\ntraining trajectory using a tabular dynamic-programming evaluation of the\nBellman equations; these action-values in turn supervise the final vision-based\ndriving policy. Despite the world-on-rails assumption, the final driving policy\nacts well in a dynamic and reactive world. At the time of writing, our method\nranks first on the CARLA leaderboard, attaining a 25% higher driving score\nwhile using 40 times less data. Our method is also an order of magnitude more\nsample-efficient than state-of-the-art model-free reinforcement learning\ntechniques on navigational tasks in the ProcGen benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia. (arXiv:2105.03464v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03464","description":"<p>Drug-induced parkinsonism affects many older adults with dementia, often\ncausing gait disturbances. New advances in vision-based human pose-estimation\nhave opened possibilities for frequent and unobtrusive analysis of gait in\nresidential settings. This work leverages novel spatial-temporal graph\nconvolutional network (ST-GCN) architectures and training procedures to predict\nclinical scores of parkinsonism in gait from video of individuals with\ndementia. We propose a two-stage training approach consisting of a\nself-supervised pretraining stage that encourages the ST-GCN model to learn\nabout gait patterns before predicting clinical scores in the finetuning stage.\nThe proposed ST-GCN models are evaluated on joint trajectories extracted from\nvideo and are compared against traditional (ordinal, linear, random forest)\nregression models and temporal convolutional network baselines. Three 2D human\npose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft\nKinect (2D and 3D) are used to extract joint trajectories of 4787 natural\nwalking bouts from 53 older adults with dementia. A subset of 399 walks from 14\nparticipants is annotated with scores of parkinsonism severity on the gait\ncriteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the\nSimpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating\non 3D joint trajectories extracted from the Kinect consistently outperform all\nother models and feature sets. Prediction of parkinsonism scores in natural\nwalking bouts of unseen participants remains a challenging task, with the best\nmodels achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02\nfor UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for\nthis work is available:\nhttps://github.com/TaatiTeam/stgcn_parkinsonism_prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabo_A/0/1/0/all/0/1\">Andrea Sabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdizadeh_S/0/1/0/all/0/1\">Sina Mehdizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iaboni_A/0/1/0/all/0/1\">Andrea Iaboni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1\">Babak Taati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revitalizing Optimization for 3D Human Pose and Shape Estimation: A Sparse Constrained Formulation. (arXiv:2105.13965v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13965","description":"<p>We propose a novel sparse constrained formulation and from it derive a\nreal-time optimization method for 3D human pose and shape estimation. Our\noptimization method, SCOPE (Sparse Constrained Optimization for 3D human Pose\nand shapE estimation), is orders of magnitude faster (avg. 4 ms convergence)\nthan existing optimization methods, while being mathematically equivalent to\ntheir dense unconstrained formulation under mild assumptions. We achieve this\nby exploiting the underlying sparsity and constraints of our formulation to\nefficiently compute the Gauss-Newton direction. We show that this computation\nscales linearly with the number of joints and measurements of a complex 3D\nhuman model, in contrast to prior work where it scales cubically due to their\ndense unconstrained formulation. Based on our optimization method, we present a\nreal-time motion capture framework that estimates 3D human poses and shapes\nfrom a single image at over 30 FPS. In benchmarks against state-of-the-art\nmethods on multiple public datasets, our framework outperforms other\noptimization methods and achieves competitive accuracy against regression\nmethods. Project page with code and videos:\nhttps://sites.google.com/view/scope-human/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Taosha Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwala_K/0/1/0/all/0/1\">Kalyan Vasudev Alwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Donglai Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphey_T/0/1/0/all/0/1\">Todd Murphey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1\">Mustafa Mukadam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Seamless and High-Performance Out-of-Distribution Detection Approach Simply Replacing the SoftMax Loss. (arXiv:2105.14399v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14399","description":"<p>Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Besides being competitive with or\noutperforming all major current approaches, our solution avoids all their\ncurrent limitations in addition to being much easier to use, as just a simple\nloss replacement for training the neural network is required. Code available at\nhttps://github.com/dlmacedo/entropic-out-of-distribution-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Never Cluster Alone. (arXiv:2106.01908v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01908","description":"<p>Recent advances in self-supervised learning with instance-level contrastive\nobjectives facilitate unsupervised clustering. However, a standalone datum is\nnot perceiving the context of the holistic cluster, and may undergo sub-optimal\nassignment. In this paper, we extend the mainstream contrastive learning\nparadigm to a cluster-level scheme, where all the data subjected to the same\ncluster contribute to a unified representation that encodes the context of each\ndata group. Contrastive learning with this representation then rewards the\nassignment of each datum. To implement this vision, we propose twin-contrast\nclustering (TCC). We define a set of categorical variables as clustering\nassignment confidence, which links the instance-level learning track with the\ncluster-level one. On one hand, with the corresponding assignment variables\nbeing the weight, a weighted aggregation along the data points implements the\nset representation of a cluster. We further propose heuristic cluster\naugmentation equivalents to enable cluster-level contrastive learning. On the\nother hand, we derive the evidence lower-bound of the instance-level\ncontrastive objective with the assignments. By reparametrizing the assignment\nvariables, TCC is trained end-to-end, requiring no alternating steps. Extensive\nexperiments show that TCC outperforms the state-of-the-art on challenging\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Ziyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Menghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Implicit 3D Shapes from Single Images with Spatial Patterns. (arXiv:2106.03087v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03087","description":"<p>3D shape reconstruction from a single image has been a long-standing problem\nin computer vision. Recent advances have led to 3D representation learning,\nwherein pixel-aligned 3D reconstruction methods show impressive performance.\nHowever, it is normally hard to exploit meaningful local image features to\ndescribe 3D point samplings from the aligned pixels when large variations of\nocclusions, views, and appearances exist. In this paper, we study a general\nkernel to encode local image features with considering geometric relationships\nof point samplings from the underlying surfaces. The kernel is derived from the\nproposed spatial pattern, in a way the kernel points are obtained as the 2D\nprojections of a number of 3D pattern points around a sampling. Supported by\nthe spatial pattern, the 2D kernel encodes geometric information that is\nessential for 3D reconstruction tasks, while traditional 2D kernels mainly\nconsider appearance information. Furthermore, to enable the network to discover\nmore adaptive spatial patterns for further capturing non-local contextual\ninformation, the spatial pattern is devised to be deformable. Experimental\nresults on both synthetic datasets and real datasets demonstrate the\nsuperiority of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yixin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunzhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Densely connected normalizing flows. (arXiv:2106.04627v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04627","description":"<p>Normalizing flows are bijective mappings between inputs and latent\nrepresentations with a fully factorized distribution. They are very attractive\ndue to exact likelihood evaluation and efficient sampling. However, their\neffective capacity is often insufficient since the bijectivity constraint\nlimits the model width. We address this issue by incrementally padding\nintermediate representations with noise. We precondition the noise in\naccordance with previous invertible units, which we describe as cross-unit\ncoupling. Our invertible glow-like modules express intra-unit affine coupling\nas a fusion of a densely connected block and Nystr\\\"om self-attention. We refer\nto our architecture as DenseFlow since both cross-unit and intra-unit couplings\nrely on dense connectivity. Experiments show significant improvements due to\nthe proposed contributions, and reveal state-of-the-art density estimation\namong all generative models under moderate computing budgets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grcic_M/0/1/0/all/0/1\">Matej Grci&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1\">Ivan Grubi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Meta-learning with Disentanglement for Domain-generalised Medical Image Segmentation. (arXiv:2106.13292v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13292","description":"<p>Generalising deep models to new data from new centres (termed here domains)\nremains a challenge. This is largely attributed to shifts in data statistics\n(domain shifts) between source and unseen domains. Recently, gradient-based\nmeta-learning approaches where the training data are split into meta-train and\nmeta-test sets to simulate and handle the domain shifts during training have\nshown improved generalisation performance. However, the current fully\nsupervised meta-learning approaches are not scalable for medical image\nsegmentation, where large effort is required to create pixel-wise annotations.\nMeanwhile, in a low data regime, the simulated domain shifts may not\napproximate the true domain shifts well across source and unseen domains. To\naddress this problem, we propose a novel semi-supervised meta-learning\nframework with disentanglement. We explicitly model the representations related\nto domain shifts. Disentangling the representations and combining them to\nreconstruct the input image allows unlabeled data to be used to better\napproximate the true domain shifts for meta-learning. Hence, the model can\nachieve better generalisation performance, especially when there is a limited\namount of labeled data. Experiments show that the proposed method is robust on\ndifferent segmentation tasks and achieves state-of-the-art generalisation\nperformance on two public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Technical Document Classification. (arXiv:2106.14269v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14269","description":"<p>In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have only focused on processing text for\nclassification, whereas technical documents often contain multimodal\ninformation. This paper presents a novel multimodal deep learning architecture,\nTechDoc, for technical document classification, which utilizes three types of\ninformation, including natural language texts and descriptive images within\ndocuments and the associations among the documents. The architecture\nsynthesizes the convolutional neural network, recurrent neural network, and\ngraph neural network through an integrated multimodal training process. We\napplied the architecture to a large multimodal technical document database and\ntrained the model for classifying documents based on the hierarchical\nInternational Patent Classification system. Our results show that TechDoc\npresents a greater classification accuracy than the unimodal methods and other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images. (arXiv:2106.14403v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.14403","description":"<p>We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages. In this framework, the slice images of a CT-scan volume are first\nproprocessed using segmentation techniques to filter out images of closed lung,\nand to remove the useless background. Then a resampling method is used to\nselect one or multiple sets of a fixed number of slice images for training and\nvalidation. A 3D CNN network with BERT is used to classify this set of selected\nslice images. In this network, an embedding feature is also extracted. In cases\nwhere there are more than one set of slice images in a volume, the features of\nall sets are extracted and pooled into a global feature vector for the whole\nCT-scan volume. A simple multiple-layer perceptron (MLP) network is used to\nfurther classify the aggregated feature vector. The models are trained and\nevaluated on the provided training and validation datasets. On the validation\ndataset, the accuracy is 0.9278 and the F1 score is 0.9261.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prior-Guided Multi-View 3D Head Reconstruction. (arXiv:2107.04277v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04277","description":"<p>Recovery of a 3D head model including the complete face and hair regions is\nstill a challenging problem in computer vision and graphics. In this paper, we\nconsider this problem using only a few multi-view portrait images as input.\nPrevious multi-view stereo methods that have been based, either on optimization\nstrategies or deep learning techniques, suffer from low-frequency geometric\nstructures such as unclear head structures and inaccurate reconstruction in\nhair regions. To tackle this problem, we propose a prior-guided implicit neural\nrendering network. Specifically, we model the head geometry with a learnable\nsigned distance field (SDF) and optimize it via an implicit differentiable\nrenderer with the guidance of some human head priors, including the facial\nprior knowledge, head semantic segmentation information and 2D hair orientation\nmaps. The utilization of these priors can improve the reconstruction accuracy\nand robustness, leading to a high-quality integrated 3D head model. Extensive\nablation studies and comparisons with state-of-the-art methods demonstrate that\nour method can generate high-fidelity 3D head geometries with the guidance of\nthese priors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yudong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.08221","description":"<p>An important component for generalization in machine learning is to uncover\nunderlying latent factors of variation as well as the mechanism through which\neach factor acts in the world. In this paper, we test whether 17 unsupervised,\nweakly supervised, and fully supervised representation learning approaches\ncorrectly infer the generative factors of variation in simple datasets\n(dSprites, Shapes3D, MPI3D) from controlled environments, and on our\ncontributed CelebGlow dataset. In contrast to prior robustness work that\nintroduces novel factors of variation during test time, such as blur or other\n(un)structured noise, we here recompose, interpolate, or extrapolate only\nexisting factors of variation from the training data set (e.g., small and\nmedium-sized objects during training and large objects during testing). Models\nthat learn the correct mechanism should be able to generalize to this\nbenchmark. In total, we train and test 2000+ models and observe that all of\nthem struggle to learn the underlying mechanism regardless of supervision\nsignal and architectural bias. Moreover, the generalization capabilities of all\ntested models drop significantly as we move from artificial datasets towards\nmore realistic real-world datasets. Despite their inability to identify the\ncorrect mechanism, the models are quite modular as their ability to infer other\nin-distribution factors remains fairly stable, providing only a single factor\nis out-of-distribution. These results point to an important yet understudied\nproblem of learning mechanistic models of observations that can facilitate\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1\">Lukas Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering. (arXiv:2107.11576v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11576","description":"<p>Encouraging progress has been made towards Visual Question Answering (VQA) in\nrecent years, but it is still challenging to enable VQA models to adaptively\ngeneralize to out-of-distribution (OOD) samples. Intuitively, recompositions of\nexisting visual concepts (\\ie, attributes and objects) can generate unseen\ncompositions in the training set, which will promote VQA models to generalize\nto OOD samples. In this paper, we formulate OOD generalization in VQA as a\ncompositional generalization problem and propose a graph generative\nmodeling-based training scheme (X-GGM) to implicitly model the problem. X-GGM\nleverages graph generative modeling to iteratively generate a relation matrix\nand node representations for the predefined graph that utilizes\nattribute-object pairs as nodes. Furthermore, to alleviate the unstable\ntraining issue in graph generative modeling, we propose a gradient distribution\nconsistency loss to constrain the data distribution with adversarial\nperturbations and the generated distribution. The baseline VQA model (LXMERT)\ntrained with the X-GGM scheme achieves state-of-the-art OOD performance on two\nstandard VQA OOD benchmarks, \\ie, VQA-CP v2 and GQA-OOD. Extensive ablation\nstudies demonstrate the effectiveness of X-GGM components. Code is available at\n\\url{https://github.com/jingjing12110/x-ggm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingjing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1\">Zhixiong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Visual Domain Transfer Learning Approach for Heartbeat Sound Classification. (arXiv:2107.13237v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2107.13237","description":"<p>Heart disease is the most common reason for human mortality that causes\nalmost one-third of deaths throughout the world. Detecting the disease early\nincreases the chances of survival of the patient and there are several ways a\nsign of heart disease can be detected early. This research proposes to convert\ncleansed and normalized heart sound into visual mel scale spectrograms and then\nusing visual domain transfer learning approaches to automatically extract\nfeatures and categorize between heart sounds. Some of the previous studies\nfound that the spectrogram of various types of heart sounds is visually\ndistinguishable to human eyes, which motivated this study to experiment on\nvisual domain classification approaches for automated heart sound\nclassification. It will use convolution neural network-based architectures i.e.\nResNet, MobileNetV2, etc as the automated feature extractors from spectrograms.\nThese well-accepted models in the image domain showed to learn generalized\nfeature representations of cardiac sounds collected from different environments\nwith varying amplitude and noise levels. Model evaluation criteria used were\ncategorical accuracy, precision, recall, and AUROC as the chosen dataset is\nunbalanced. The proposed approach has been implemented on datasets A and B of\nthe PASCAL heart sound collection and resulted in ~ 90% categorical accuracy\nand AUROC of ~0.97 for both sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_U/0/1/0/all/0/1\">Uddipan Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1\">Sidharth Pancholi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiFT: Hierarchical Feature Transformer for Aerial Tracking. (arXiv:2108.00202v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00202","description":"<p>Most existing Siamese-based tracking methods execute the classification and\nregression of the target object based on the similarity maps. However, they\neither employ a single map from the last convolutional layer which degrades the\nlocalization accuracy in complex scenarios or separately use multiple maps for\ndecision making, introducing intractable computations for aerial mobile\nplatforms. Thus, in this work, we propose an efficient and effective\nhierarchical feature transformer (HiFT) for aerial tracking. Hierarchical\nsimilarity maps generated by multi-level convolutional layers are fed into the\nfeature transformer to achieve the interactive fusion of spatial (shallow\nlayers) and semantics cues (deep layers). Consequently, not only the global\ncontextual information can be raised, facilitating the target search, but also\nour end-to-end architecture with the transformer can efficiently learn the\ninterdependencies among multi-level features, thereby discovering a\ntracking-tailored feature space with strong discriminability. Comprehensive\nevaluations on four aerial benchmarks have proven the effectiveness of HiFT.\nReal-world tests on the aerial platform have strongly validated its\npracticability with a real-time speed. Our code is available at\nhttps://github.com/vision4robotics/HiFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIDCD -- Bosch Industrial Depth Completion Dataset. (arXiv:2108.04706v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04706","description":"<p>We introduce BIDCD -- the Bosch Industrial Depth Completion Dataset. BIDCD is\na new RGBD dataset of metallic industrial objects, collected with a depth\ncamera mounted on a robotic manipulator. The main purpose of this dataset is to\nfacilitate the training of domain-specific depth completion models, to be used\nin logistics and manufacturing tasks. We trained a State-of-the-Art depth\ncompletion model on this dataset, and report the results, setting an initial\nbenchmark. Further, we propose to use this dataset for learning\nsynthetic-to-depth-camera domain adaptation. Modifying synthetic RGBD data to\nmimic characteristics of real-world depth acquisition could potentially enhance\ntraining on synthetic data. For this end, we trained a Generative Adversarial\nNetwork (GAN) on a synthetic industrial dataset and our real-world data.\nFinally, to address geometric distortions in the generated images, we introduce\nan auxiliary loss that promotes preservation of the original shape. The BIDCD\ndata is publicly available at https://zenodo.org/communities/bidcd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botach_A/0/1/0/all/0/1\">Adam Botach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_Y/0/1/0/all/0/1\">Yuri Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_Y/0/1/0/all/0/1\">Yakov Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapiro_Y/0/1/0/all/0/1\">Yoel Shapiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRB-GAN: A Dynamic ResBlock Generative Adversarial Network for Artistic Style Transfer. (arXiv:2108.07379v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07379","description":"<p>The paper proposes a Dynamic ResBlock Generative Adversarial Network\n(DRB-GAN) for artistic style transfer. The style code is modeled as the shared\nparameters for Dynamic ResBlocks connecting both the style encoding network and\nthe style transfer network. In the style encoding network, a style class-aware\nattention mechanism is used to attend the style feature representation for\ngenerating the style codes. In the style transfer network, multiple Dynamic\nResBlocks are designed to integrate the style code and the extracted CNN\nsemantic feature and then feed into the spatial window Layer-Instance\nNormalization (SW-LIN) decoder, which enables high-quality synthetic images\nwith artistic style transfer. Moreover, the style collection conditional\ndiscriminator is designed to equip our DRB-GAN model with abilities for both\narbitrary style transfer and collection style transfer during the training\nstage. No matter for arbitrary style transfer or collection style transfer,\nextensive experiments strongly demonstrate that our proposed DRB-GAN\noutperforms state-of-the-art methods and exhibits its superior performance in\nterms of visual quality and efficiency. Our source code is available at\n\\color{magenta}{\\url{https://github.com/xuwenju123/DRB-GAN}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenju Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruisheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network (CNN) vs Visual Transformer (ViT) for Digital Holography. (arXiv:2108.09147v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09147","description":"<p>In Digital Holography (DH), it is crucial to extract the object distance from\na hologram in order to reconstruct its amplitude and phase. This step is called\nauto-focusing and it is conventionally solved by first reconstructing a stack\nof images and then by sharpening each reconstructed image using a focus metric\nsuch as entropy or variance. The distance corresponding to the sharpest image\nis considered the focal position. This approach, while effective, is\ncomputationally demanding and time-consuming. In this paper, the determination\nof the distance is performed by Deep Learning (DL). Two deep learning (DL)\narchitectures are compared: Convolutional Neural Network (CNN)and Visual\ntransformer (ViT). ViT and CNN are used to cope with the problem of\nauto-focusing as a classification problem. Compared to a first attempt [11] in\nwhich the distance between two consecutive classes was 100$\\mu$m, our proposal\nallows us to drastically reduce this distance to 1$\\mu$m. Moreover, ViT reaches\nsimilar accuracy and is more robust than CNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuenat_S/0/1/0/all/0/1\">St&#xe9;phane Cuenat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DKM: Differentiable K-Means Clustering Layer for Neural Network Compression. (arXiv:2108.12659v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.12659","description":"<p>Deep neural network (DNN) model compression for efficient on-device inference\nis becoming increasingly important to reduce memory requirements and keep user\ndata on-device. To this end, we propose a novel differentiable k-means\nclustering layer (DKM) and its application to train-time weight\nclustering-based DNN model compression. DKM casts k-means clustering as an\nattention problem and enables joint optimization of the DNN parameters and\nclustering centroids. Unlike prior works that rely on additional regularizers\nand parameters, DKM-based compression keeps the original loss function and\nmodel architecture fixed. We evaluated DKM-based compression on various DNN\nmodels for computer vision and natural language processing (NLP) tasks. Our\nresults demonstrate that DKM delivers superior compression and accuracy\ntrade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression\ncan offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB\nmodel size (29.4x model compression factor). For MobileNet-v1, which is a\nchallenging DNN to compress, DKM delivers 62.8% top-1 ImageNet1k accuracy with\n0.74 MB model size (22.4x model compression factor). This result is 6.8% higher\ntop-1accuracy and 33% relatively smaller model size than the current\nstate-of-the-art DNN compression algorithms. Additionally, DKM enables\ncompression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on\nGLUE NLP benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahid_K/0/1/0/all/0/1\">Keivan A. Vahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1\">Saurabh Adya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets. (arXiv:2109.03229v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03229","description":"<p>Many existing works have made great strides towards reducing racial bias in\nface recognition. However, most of these methods attempt to rectify bias that\nmanifests in models during training instead of directly addressing a major\nsource of the bias, the dataset itself. Exceptions to this are\nBUPT-Balancedface/RFW and Fairface, but these works assume that primarily\ntraining on a single race or not racially balancing the dataset are inherently\ndisadvantageous. We demonstrate that these assumptions are not necessarily\nvalid. In our experiments, training on only African faces induced less bias\nthan training on a balanced distribution of faces and distributions skewed to\ninclude more African faces produced more equitable models. We additionally\nnotice that adding more images of existing identities to a dataset in place of\nadding new identities can lead to accuracy boosts across racial categories. Our\ncode is available at $\\small\n\\href{https://github.com/j-alex-hanson/rethinking-race-face-datasets}{\\text{this\nhttps URL}}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1\">Matthew Gwilliam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Srinidhi Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinubu_L/0/1/0/all/0/1\">Lade Tinubu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanson_A/0/1/0/all/0/1\">Alex Hanson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Person Re-Identification: A Systematic Survey of Challenges and Solutions. (arXiv:2109.06057v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06057","description":"<p>Person re-identification (Re-ID) has been a significant research topic in the\npast decade due to its real-world applications and research significance. While\nsupervised person Re-ID methods achieve superior performance over unsupervised\ncounterparts, they can not scale to large unlabelled datasets and new domains\ndue to the prohibitive labelling cost. Therefore, unsupervised person Re-ID has\ndrawn increasing attention for its potential to address the scalability issue\nin person Re-ID. Unsupervised person Re-ID is challenging primarily due to\nlacking identity labels to supervise person feature learning. The corresponding\nsolutions are diverse and complex, with various merits and limitations.\nTherefore, comprehensive surveys on this topic are essential to summarise\nchallenges and solutions to foster future research. Existing person Re-ID\nsurveys have focused on supervised methods from classifications and\napplications but lack detailed discussion on how the person Re-ID solutions\naddress the underlying challenges. This survey review recent works on\nunsupervised person Re-ID from the perspective of challenges and solutions.\nSpecifically, we provide an in-depth analysis of highly influential methods\nconsidering the four significant challenges in unsupervised person Re-ID: 1)\nlacking ground-truth identity labels to supervise person feature learning; 2)\nlearning discriminative person features with pseudo-supervision; 3) learning\ncross-camera invariant person feature, and 4) the domain shift between\ndatasets. We summarise and analyse evaluation results and provide insights on\nthe effectiveness of the solutions. Finally, we discuss open issues and suggest\nsome promising future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiangtan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chung-Hsing Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lina Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_A/0/1/0/all/0/1\">Andy Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Task Cross-Task Learning Architecture for Ad-hoc Uncertainty Estimation in 3D Cardiac MRI Image Segmentation. (arXiv:2109.07702v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.07702","description":"<p>Medical image segmentation has significantly benefitted thanks to deep\nlearning architectures. Furthermore, semi-supervised learning (SSL) has\nrecently been a growing trend for improving a model's overall performance by\nleveraging abundant unlabeled data. Moreover, learning multiple tasks within\nthe same model further improves model generalizability. To generate smoother\nand accurate segmentation masks from 3D cardiac MR images, we present a\nMulti-task Cross-task learning consistency approach to enforce the correlation\nbetween the pixel-level (segmentation) and the geometric-level (distance map)\ntasks. Our extensive experimentation with varied quantities of labeled data in\nthe training sets justifies the effectiveness of our model for the segmentation\nof the left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR)\nimages. With the incorporation of uncertainty estimates to detect failures in\nthe segmentation masks generated by CNNs, our study further showcases the\npotential of our model to flag low-quality segmentation from a given model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hasan_S/0/1/0/all/0/1\">S. M. Kamrul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linte_C/0/1/0/all/0/1\">Cristian A. Linte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHFC: Multi-Head Feature Collaboration for Few-Shot Learning. (arXiv:2109.07785v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07785","description":"<p>Few-shot learning (FSL) aims to address the data-scarce problem. A standard\nFSL framework is composed of two components: (1) Pre-train. Employ the base\ndata to generate a CNN-based feature extraction model (FEM). (2) Meta-test.\nApply the trained FEM to acquire the novel data's features and recognize them.\nFSL relies heavily on the design of the FEM. However, various FEMs have\ndistinct emphases. For example, several may focus more attention on the contour\ninformation, whereas others may lay particular emphasis on the texture\ninformation. The single-head feature is only a one-sided representation of the\nsample. Besides the negative influence of cross-domain (e.g., the trained FEM\ncan not adapt to the novel class flawlessly), the distribution of novel data\nmay have a certain degree of deviation compared with the ground truth\ndistribution, which is dubbed as distribution-shift-problem (DSP). To address\nthe DSP, we propose Multi-Head Feature Collaboration (MHFC) algorithm, which\nattempts to project the multi-head features (e.g., multiple features extracted\nfrom a variety of FEMs) to a unified space and fuse them to capture more\ndiscriminative information. Typically, first, we introduce a subspace learning\nmethod to transform the multi-head features to aligned low-dimensional\nrepresentations. It corrects the DSP via learning the feature with more\npowerful discrimination and overcomes the problem of inconsistent measurement\nscales from different head features. Then, we design an attention block to\nupdate combination weights for each head feature automatically. It\ncomprehensively considers the contribution of various perspectives and further\nimproves the discrimination of features. We evaluate the proposed method on\nfive benchmark datasets (including cross-domain experiments) and achieve\nsignificant improvements of 2.1%-7.8% compared with state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chunyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Jiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bao-Di Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logo Generation Using Regional Features: A Faster R-CNN Approach to Generative Adversarial Networks. (arXiv:2109.12628v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12628","description":"<p>In this paper we introduce Local Logo Generative Adversarial Network (LL-GAN)\nthat uses regional features extracted from Faster R-CNN for logo generation. We\ndemonstrate the strength of this approach by training the framework on a small\nstyle-rich dataset of real heavy metal logos to generate new ones. LL-GAN\nachieves Inception Score of 5.29 and Frechet Inception Distance of 223.94,\nimproving on state-of-the-art models StyleGAN2 and Self-Attention GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ter_Sarkisov_A/0/1/0/all/0/1\">Aram Ter-Sarkisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_E/0/1/0/all/0/1\">Eduardo Alonso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster Analysis with Deep Embeddings and Contrastive Learning. (arXiv:2109.12714v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.12714","description":"<p>Unsupervised disentangled representation learning is a long-standing problem\nin computer vision. This work proposes a novel framework for performing image\nclustering from deep embeddings by combining instance-level contrastive\nlearning with a deep embedding based cluster center predictor. Our approach\njointly learns representations and predicts cluster centers in an end-to-end\nmanner. This is accomplished via a three-pronged approach that combines a\nclustering loss, an instance-wise contrastive loss, and an anchor loss. Our\nfundamental intuition is that using an ensemble loss that incorporates\ninstance-level features and a clustering procedure focusing on semantic\nsimilarity reinforces learning better representations in the latent space. We\nobserve that our method performs exceptionally well on popular vision datasets\nwhen evaluated using standard clustering metrics such as Normalized Mutual\nInformation (NMI), in addition to producing geometrically well-separated\ncluster embeddings as defined by the Euclidean distance. Our framework performs\non par with widely accepted clustering methods and outperforms the\nstate-of-the-art contrastive learning method on the CIFAR-10 dataset with an\nNMI score of 0.772, a 7-8% improvement on the strong baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundareswaran_R/0/1/0/all/0/1\">Ramakrishnan Sundareswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_Gerena_J/0/1/0/all/0/1\">Jansel Herrera-Gerena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Just_J/0/1/0/all/0/1\">John Just</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1\">Ali Jannesari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FathomNet: A global underwater image training set for enabling artificial intelligence in the ocean. (arXiv:2109.14646v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14646","description":"<p>Ocean-going platforms are integrating high-resolution camera feeds for\nobservation and navigation, producing a deluge of visual data. The volume and\nrate of this data collection can rapidly outpace researchers' abilities to\nprocess and analyze them. Recent advances in machine learning enable fast,\nsophisticated analysis of visual data, but have had limited success in the\nocean due to lack of data set standardization, insufficient formatting, and\naggregation of existing, expertly curated imagery for use by data scientists.\nTo address this need, we have built FathomNet, a public platform that makes use\nof existing, expertly curated data. Initial efforts have leveraged MBARI's\nVideo Annotation and Reference System and annotated deep sea video database,\nwhich has more than 7M annotations, 1M frame grabs, and 5k terms in the\nknowledgebase, with additional contributions by National Geographic Society\n(NGS) and NOAA's Office of Ocean Exploration and Research. FathomNet has over\n160k localizations of 1.4k midwater and benthic classes, and contains more than\n70k iconic and non-iconic views of marine animals, underwater equipment,\ndebris, etc. We demonstrate how machine learning models trained on FathomNet\ndata can be applied across different institutional video data, and enable\nautomated acquisition and tracking of midwater animals using a remotely\noperated vehicle. As FathomNet continues to develop and incorporate more image\ndata from other oceanographic community members, this effort will enable\nscientists, explorers, policymakers, storytellers, and the public to understand\nand care for our ocean.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katija_K/0/1/0/all/0/1\">Kakani Katija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orenstein_E/0/1/0/all/0/1\">Eric Orenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlining_B/0/1/0/all/0/1\">Brian Schlining</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundsten_L/0/1/0/all/0/1\">Lonny Lundsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnard_K/0/1/0/all/0/1\">Kevin Barnard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainz_G/0/1/0/all/0/1\">Giovanna Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1\">Oceane Boulais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodward_B/0/1/0/all/0/1\">Benjamin Woodward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_K/0/1/0/all/0/1\">Katy Croff Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Pose Transfer with Correspondence Learning and Mesh Refinement. (arXiv:2109.15025v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15025","description":"<p>3D pose transfer is one of the most challenging 3D generation tasks. It aims\nto transfer the pose of a source mesh to a target mesh and keep the identity\n(e.g., body shape) of the target mesh. Some previous works require key point\nannotations to build reliable correspondence between the source and target\nmeshes, while other methods do not consider any shape correspondence between\nsources and targets, which leads to limited generation quality. In this work,\nwe propose a correspondence-refinement network to help the 3D pose transfer for\nboth human and animal meshes. The correspondence between source and target\nmeshes is first established by solving an optimal transport problem. Then, we\nwarp the source mesh according to the dense correspondence and obtain a coarse\nwarped mesh. The warped mesh will be better refined with our proposed Elastic\nInstance Normalization, which is a conditional normalization layer and can help\nto generate high-quality meshes. Extensive experimental results show that the\nproposed architecture can effectively transfer the poses from source to target\nmeshes and produce better results with satisfied visual performance than\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chaoyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiacheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruibo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}