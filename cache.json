{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A smile is all you need: Predicting limiting activity coefficients from SMILES with natural language processing. (arXiv:2206.07048v1 [physics.chem-ph])","link":"http://arxiv.org/abs/2206.07048","description":"<p>Knowledge of mixtures' phase equilibria is crucial in nature and technical\nchemistry. Phase equilibria calculations of mixtures require activity\ncoefficients. However, experimental data on activity coefficients is often\nlimited due to high cost of experiments. For an accurate and efficient\nprediction of activity coefficients, machine learning approaches have been\nrecently developed. However, current machine learning approaches still\nextrapolate poorly for activity coefficients of unknown molecules. In this\nwork, we introduce the SMILES-to-Properties-Transformer (SPT), a natural\nlanguage processing network to predict binary limiting activity coefficients\nfrom SMILES codes. To overcome the limitations of available experimental data,\nwe initially train our network on a large dataset of synthetic data sampled\nfrom COSMO-RS (10 Million data points) and then fine-tune the model on\nexperimental data (20 870 data points). This training strategy enables SPT to\naccurately predict limiting activity coefficients even for unknown molecules,\ncutting the mean prediction error in half compared to state-of-the-art models\nfor activity coefficient predictions such as COSMO-RS, UNIFAC, and improving on\nrecent machine learning approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Winter_B/0/1/0/all/0/1\">Benedikt Winter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Winter_C/0/1/0/all/0/1\">Clemens Winter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schilling_J/0/1/0/all/0/1\">Johannes Schilling</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bardow_A/0/1/0/all/0/1\">Andr&#xe9; Bardow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsEdits: A News Article Revision Dataset and a Document-Level Reasoning Challenge. (arXiv:2206.07106v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07106","description":"<p>News article revision histories provide clues to narrative and factual\nevolution in news articles. To facilitate analysis of this evolution, we\npresent the first publicly available dataset of news revision histories,\nNewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million\narticles with 4.6 million versions from over 22 English- and French-language\nnewspaper sources based in three countries, spanning 15 years of coverage\n(2006-2021).\n</p>\n<p>We define article-level edit actions: Addition, Deletion, Edit and Refactor,\nand develop a high-accuracy extraction algorithm to identify these actions. To\nunderscore the factual nature of many edit actions, we conduct analyses showing\nthat added and deleted sentences are more likely to contain updating events,\nmain content and quotes than unchanged sentences.\n</p>\n<p>Finally, to explore whether edit actions are predictable, we introduce three\nnovel tasks aimed at predicting actions performed during version updates. We\nshow that these tasks are possible for expert humans but are challenging for\nlarge NLP models. We hope this can spur research in narrative framing and help\nprovide predictive tools for journalists chasing breaking news.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alexander Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"If it Bleeds, it Leads: A Computational Approach to Covering Crime in Los Angeles. (arXiv:2206.07115v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07115","description":"<p>Developing and improving computational approaches to covering news can\nincrease journalistic output and improve the way stories are covered. In this\nwork we approach the problem of covering crime stories in Los Angeles. We\npresent a machine-in-the-loop system that covers individual crimes by (1)\nlearning the prototypical coverage archetypes from classical news articles on\ncrime to learn their structure and (2) using output from the Los Angeles Police\ndepartment to generate \"lede paragraphs\", first structural unit of\ncrime-articles. We introduce a probabilistic graphical model for learning\narticle structure and a rule-based system for generating ledes. We hope our\nwork can lead to systems that use these components together to form the\nskeletons of news articles covering crime.\n</p>\n<p>This work was done for a class project in Jonathan May's Advanced Natural\nLanguage Processing Course, Fall, 2019.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alexander Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_D/0/1/0/all/0/1\">Divya Choudhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07136","description":"<p>Per-example gradient clipping is a key algorithmic step that enables\npractical differential private (DP) training for deep learning models. The\nchoice of clipping norm $R$, however, is shown to be vital for achieving high\naccuracy under DP. We propose an easy-to-use replacement, called AutoClipping,\nthat eliminates the need to tune $R$ for any DP optimizers, including DP-SGD,\nDP-Adam, DP-LAMB and many others. The automatic variants are as private and\ncomputationally efficient as existing DP optimizers, but require no DP-specific\nhyperparameters and thus make DP training as amenable as the standard\nnon-private training. We give a rigorous convergence analysis of automatic\nDP-SGD in the non-convex setting, which shows that it enjoys an asymptotic\nconvergence rate that matches the standard SGD. We also demonstrate on various\nlanguage and vision tasks that automatic clipping outperforms or matches the\nstate-of-the-art, and can be easily employed with minimal changes to existing\ncodebases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Narratives through Dimensions of Analogy. (arXiv:2206.07167v1 [cs.AI])","link":"http://arxiv.org/abs/2206.07167","description":"<p>Analogical reasoning is a powerful qualitative reasoning tool that enables\nhumans to connect two situations, and to generalize their knowledge from\nfamiliar to novel situations. Cognitive Science research provides valuable\ninsights into the richness and complexity of analogical reasoning, together\nwith implementations of expressive analogical reasoners with limited\nscalability. Modern scalable AI techniques with the potential to reason by\nanalogy have been only applied to the special case of proportional analogy, and\nnot to understanding higher-order analogies. In this paper, we aim to bridge\nthe gap by: 1) formalizing six dimensions of analogy based on mature insights\nfrom Cognitive Science research, 2) annotating a corpus of fables with each of\nthese dimensions, and 3) defining four tasks with increasing complexity that\nenable scalable evaluation of AI techniques. Experiments with language models\nand neuro-symbolic AI reasoners on these tasks reveal that state-of-the-art\nmethods can be applied to reason by analogy with a limited success, motivating\nthe need for further research towards comprehensive and scalable analogical\nreasoning by AI. We make all our code and data available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagarajah_T/0/1/0/all/0/1\">Thiloshon Nagarajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency-centroid features for word recognition of non-native English speakers. (arXiv:2206.07176v1 [cs.SD])","link":"http://arxiv.org/abs/2206.07176","description":"<p>The objective of this work is to investigate complementary features which can\naid the quintessential Mel frequency cepstral coefficients (MFCCs) in the task\nof closed, limited set word recognition for non-native English speakers of\ndifferent mother-tongues. Unlike the MFCCs, which are derived from the spectral\nenergy of the speech signal, the proposed frequency-centroids (FCs) encapsulate\nthe spectral centres of the different bands of the speech spectrum, with the\nbands defined by the Mel filterbank. These features, in combination with the\nMFCCs, are observed to provide relative performance improvement in English word\nrecognition, particularly under varied noisy conditions. A two-stage\nConvolution Neural Network (CNN) is used to model the features of the English\nwords uttered with Arabic, French and Spanish accents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berjon_P/0/1/0/all/0/1\">Pierre Berjon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajib Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nag_A/0/1/0/all/0/1\">Avishek Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Soumyabrata Dev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Codec at SemEval-2022 Task 5: Multi-Modal Multi-Transformer Misogynous Meme Classification Framework. (arXiv:2206.07190v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07190","description":"<p>In this paper we describe our work towards building a generic framework for\nboth multi-modal embedding and multi-label binary classification tasks, while\nparticipating in task 5 (Multimedia Automatic Misogyny Identification) of\nSemEval 2022 competition.\n</p>\n<p>Since pretraining deep models from scratch is a resource and data hungry\ntask, our approach is based on three main strategies. We combine different\nstate-of-the-art architectures to capture a wide spectrum of semantic signals\nfrom the multi-modal input. We employ a multi-task learning scheme to be able\nto use multiple datasets from the same knowledge domain to help increase the\nmodel's performance. We also use multiple objectives to regularize and fine\ntune different system components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahran_A/0/1/0/all/0/1\">Ahmed Mahran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borella_C/0/1/0/all/0/1\">Carlo Alessandro Borella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perifanos_K/0/1/0/all/0/1\">Konstantinos Perifanos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World. (arXiv:2206.07207v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07207","description":"<p>Understanding how events described or shown in multimedia content relate to\none another is a critical component to developing robust artificially\nintelligent systems which can reason about real-world media. While much\nresearch has been devoted to event understanding in the text, image, and video\ndomains, none have explored the complex relations that events experience across\ndomains. For example, a news article may describe a `protest' event while a\nvideo shows an `arrest' event. Recognizing that the visual `arrest' event is a\nsubevent of the broader `protest' event is a challenging, yet important problem\nthat prior work has not explored. In this paper, we propose the novel task of\nMultiModal Event Event Relations to recognize such cross-modal event relations.\nWe contribute a large-scale dataset consisting of 100k video-news article\npairs, as well as a benchmark of densely annotated data. We also propose a\nweakly supervised multimodal method which integrates commonsense knowledge from\nan external knowledge base (KB) to predict rich multimodal event hierarchies.\nExperiments show that our model outperforms a number of competitive baselines\non our proposed benchmark. We also perform a detailed analysis of our model's\nperformance and suggest directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayyubi_H/0/1/0/all/0/1\">Hammad A. Ayyubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1\">Christopher Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_L/0/1/0/all/0/1\">Lovish Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lokesh_R/0/1/0/all/0/1\">Rahul Lokesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1\">Jaywon Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Sounak Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Location-based Twitter Filtering for the Creation of Low-Resource Language Datasets in Indonesian Local Languages. (arXiv:2206.07238v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07238","description":"<p>Twitter contains an abundance of linguistic data from the real world. We\nexamine Twitter for user-generated content in low-resource languages such as\nlocal Indonesian. For NLP to work in Indonesian, it must consider local\ndialects, geographic context, and regional culture influence Indonesian\nlanguages. This paper identifies the problems we faced when constructing a\nLocal Indonesian NLP dataset. Furthermore, we are developing a framework for\ncreating, collecting, and classifying Local Indonesian datasets for NLP. Using\ntwitter's geolocation tool for automatic annotating.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amien_M/0/1/0/all/0/1\">Mukhlis Amien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TeKo: Text-Rich Graph Neural Networks with External Knowledge. (arXiv:2206.07253v1 [cs.SI])","link":"http://arxiv.org/abs/2206.07253","description":"<p>Graph Neural Networks (GNNs) have gained great popularity in tackling various\nanalytical tasks on graph-structured data (i.e., networks). Typical GNNs and\ntheir variants follow a message-passing manner that obtains network\nrepresentations by the feature propagation process along network topology,\nwhich however ignore the rich textual semantics (e.g., local word-sequence)\nthat exist in many real-world networks. Existing methods for text-rich networks\nintegrate textual semantics by mainly utilizing internal information such as\ntopics or phrases/words, which often suffer from an inability to\ncomprehensively mine the text semantics, limiting the reciprocal guidance\nbetween network structure and text semantics. To address these problems, we\npropose a novel text-rich graph neural network with external knowledge (TeKo),\nin order to take full advantage of both structural and textual information\nwithin text-rich networks. Specifically, we first present a flexible\nheterogeneous semantic network that incorporates high-quality entities and\ninteractions among documents and entities. We then introduce two types of\nexternal knowledge, that is, structured triplets and unstructured entity\ndescription, to gain a deeper insight into textual semantics. We further design\na reciprocal convolutional mechanism for the constructed heterogeneous semantic\nnetwork, enabling network structure and textual semantics to collaboratively\nenhance each other and learn high-level network representations. Extensive\nexperimental results on four public text-rich networks as well as a large-scale\ne-commerce searching dataset illustrate the superior performance of TeKo over\nstate-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhizhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jianguo Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yue Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Born for Auto-Tagging: Faster and better with new objective functions. (arXiv:2206.07264v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07264","description":"<p>Keyword extraction is a task of text mining. It is applied to increase search\nvolume in SEO and ads. Implemented in auto-tagging, it makes tagging on a mass\nscale of online articles and photos efficiently and accurately. BAT is invented\nfor auto-tagging which served as awoo's AI marketing platform (AMP). awoo AMP\nnot only provides service as a customized recommender system but also increases\nthe converting rate in E-commerce. The strength of BAT converges faster and\nbetter than other SOTA models, as its 4-layer structure achieves the best F\nscores at 50 epochs. In other words, it performs better than other models which\nrequire deeper layers at 100 epochs. To generate rich and clean tags, awoo\ncreates new objective functions to maintain similar ${\\rm F_1}$ scores with\ncross-entropy while enhancing ${\\rm F_2}$ scores simultaneously. To assure the\neven better performance of F scores awoo revamps the learning rate strategy\nproposed by Transformer \\cite{Transformer} to increase ${\\rm F_1}$ and ${\\rm\nF_2}$ scores at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chiung-ju Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shieh_H/0/1/0/all/0/1\">Huang-Ting Shieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Heuristics for AI-Generated Language Are Flawed. (arXiv:2206.07271v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07271","description":"<p>Human communication is increasingly intermixed with language generated by AI.\nAcross chat, email, and social media, AI systems produce smart replies,\nautocompletes, and translations. AI-generated language is often not identified\nas such but poses as human language, raising concerns about novel forms of\ndeception and manipulation. Here, we study how humans discern whether one of\nthe most personal and consequential forms of language - a self-presentation -\nwas generated by AI. Across six experiments, participants (N = 4,650) tried to\nidentify self-presentations generated by state-of-the-art language models.\nAcross professional, hospitality, and romantic settings, we find that humans\nare unable to identify AI-generated self-presentations. Combining qualitative\nanalyses with language feature engineering, we find that human judgments of\nAI-generated language are handicapped by intuitive but flawed heuristics such\nas associating first-person pronouns, authentic words, or family topics with\nhumanity. We show that these heuristics make human judgment of generated\nlanguage predictable and manipulable, allowing AI systems to produce language\nperceived as more human than human. We conclude by discussing solutions - such\nas AI accents or fair use policies - to reduce the deceptive potential of\ngenerated language, limiting the subversion of human intuition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakesch_M/0/1/0/all/0/1\">Maurice Jakesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hancock_J/0/1/0/all/0/1\">Jeffrey Hancock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naaman_M/0/1/0/all/0/1\">Mor Naaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Knowledge Selection for Grounded Dialogues via Document Semantic Graphs. (arXiv:2206.07296v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07296","description":"<p>Providing conversation models with background knowledge has been shown to\nmake open-domain dialogues more informative and engaging. Existing models treat\nknowledge selection as a sentence ranking or classification problem where each\nsentence is handled individually, ignoring the internal semantic connection\namong sentences in the background document. In this work, we propose to\nautomatically convert the background knowledge documents into document semantic\ngraphs and then perform knowledge selection over such graphs. Our document\nsemantic graphs preserve sentence-level information through the use of sentence\nnodes and provide concept connections between sentences. We jointly apply\nmulti-task learning for sentence-level and concept-level knowledge selection\nand show that it improves sentence-level selection. Our experiments show that\nour semantic graph-based knowledge selection improves over sentence selection\nbaselines for both the knowledge selection task and the end-to-end response\ngeneration task on HollE and improves generalization on unseen topics in WoW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1\">Madhi Namazifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Management System with NLP-Assisted Annotations: A Brief Survey and Outlook. (arXiv:2206.07304v1 [cs.DB])","link":"http://arxiv.org/abs/2206.07304","description":"<p>Knowledge management systems are in high demand for industrial researchers,\nchemical or research enterprises, or evidence-based decision making. However,\nexisting systems have limitations in categorizing and organizing paper insights\nor relationships. Traditional databases are usually disjoint with logging\nsystems, which limit its utility in generating concise, collated overviews. In\nthis work, we briefly survey existing approaches of this problem space and\npropose a unified framework that utilizes relational databases to log\nhierarchical information to facilitate the research and writing process, or\ngenerate useful knowledge from references or insights from connected concepts.\nThis framework of knowledge management system enables novel functionalities\nencompassing improved hierarchical notetaking, AI-assisted brainstorming, and\nmulti-directional relationships. Potential applications include managing\ninventories and changes for manufacture or research enterprises, or generating\nanalytic reports with evidence-based decision making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMNEROne at SemEval-2022 Task 11: Code-Mixed Named Entity Recognition by leveraging multilingual data. (arXiv:2206.07318v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07318","description":"<p>Identifying named entities is, in general, a practical and challenging task\nin the field of Natural Language Processing. Named Entity Recognition on the\ncode-mixed text is further challenging due to the linguistic complexity\nresulting from the nature of the mixing. This paper addresses the submission of\nteam CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER\ntask aimed to identify named entities on the code-mixed dataset. Our work\nconsists of Named Entity Recognition (NER) on the code-mixed dataset by\nleveraging the multilingual data. We achieved a weighted average F1 score of\n0.7044, i.e., 6% greater than the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dowlagar_S/0/1/0/all/0/1\">Suman Dowlagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey : Neural Networks for AMR-to-Text. (arXiv:2206.07328v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07328","description":"<p>AMR-to-text is one of the key techniques in the NLP community that aims at\ngenerating sentences from the Abstract Meaning Representation (AMR) graphs.\nSince AMR was proposed in 2013, the study on AMR-to-Text has become\nincreasingly prevalent as an essential branch of structured data to text\nbecause of the unique advantages of AMR as a high-level semantic description of\nnatural language. In this paper, we provide a brief survey of AMR-to-Text.\nFirstly, we introduce the current scenario of this technique and point out its\ndifficulties. Secondly, based on the methods used in previous studies, we\nroughly divided them into five categories according to their respective\nmechanisms, i.e., Rules-based, Seq-to-Seq-based, Graph-to-Seq-based,\nTransformer-based, and Pre-trained Language Model (PLM)-based. In particular,\nwe detail the neural network-based method and present the latest progress of\nAMR-to-Text, which refers to AMR reconstruction, Decoder optimization, etc.\nFurthermore, we present the benchmarks and evaluation methods of AMR-to-Text.\nEventually, we provide a summary of current techniques and the outlook for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1\">Hongyu Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangtong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huafeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation. (arXiv:2206.07359v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07359","description":"<p>In emotion recognition in conversation (ERC), the emotion of the current\nutterance is predicted by considering the previous context, which can be\nutilized in many natural language processing tasks. Although multiple emotions\ncan coexist in a given sentence, most previous approaches take the perspective\nof a classification task to predict only a given label. However, it is\nexpensive and difficult to label the emotion of a sentence with confidence or\nmulti-label. In this paper, we automatically construct a grayscale label\nconsidering the correlation between emotions and use it for learning. That is,\ninstead of using a given label as a one-hot encoding, we construct a grayscale\nlabel by measuring scores for different emotions. We introduce several methods\nfor constructing grayscale labels and confirm that each method improves the\nemotion recognition performance. Our method is simple, effective, and\nuniversally applicable to previous systems. The experiments show a significant\nimprovement in the performance of baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciTweets -- A Dataset and Annotation Framework for Detecting Scientific Online Discourse. (arXiv:2206.07360v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07360","description":"<p>Scientific topics, claims and resources are increasingly debated as part of\nonline discourse, where prominent examples include discourse related to\nCOVID-19 or climate change. This has led to both significant societal impact\nand increased interest in scientific online discourse from various disciplines.\nFor instance, communication studies aim at a deeper understanding of biases,\nquality or spreading pattern of scientific information whereas computational\nmethods have been proposed to extract, classify or verify scientific claims\nusing NLP and IR techniques. However, research across disciplines currently\nsuffers from both a lack of robust definitions of the various forms of\nscience-relatedness as well as appropriate ground truth data for distinguishing\nthem. In this work, we contribute (a) an annotation framework and corresponding\ndefinitions for different forms of scientific relatedness of online discourse\nin Tweets, (b) an expert-annotated dataset of 1261 tweets obtained through our\nlabeling framework reaching an average Fleiss Kappa $\\kappa$ of 0.63, (c) a\nmulti-label classifier trained on our data able to detect science-relatedness\nwith 89% F1 and also able to detect distinct forms of scientific knowledge\n(claims, references). With this work we aim to lay the foundation for\ndeveloping and evaluating robust methods for analysing science as part of\nlarge-scale online discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hafid_S/0/1/0/all/0/1\">Salim Hafid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schellhammer_S/0/1/0/all/0/1\">Sebastian Schellhammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bringay_S/0/1/0/all/0/1\">Sandra Bringay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorov_K/0/1/0/all/0/1\">Konstantin Todorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1\">Stefan Dietze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NatiQ: An End-to-end Text-to-Speech System for Arabic. (arXiv:2206.07373v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07373","description":"<p>NatiQ is end-to-end text-to-speech system for Arabic. Our speech synthesizer\nuses an encoder-decoder architecture with attention. We used both\ntacotron-based models (tacotron-1 and tacotron-2) and the faster transformer\nmodel for generating mel-spectrograms from characters. We concatenated\nTacotron1 with the WaveRNN vocoder, Tacotron2 with the WaveGlow vocoder and\nESPnet transformer with the parallel wavegan vocoder to synthesize waveforms\nfrom the spectrograms. We used in-house speech data for two voices: 1) neutral\nmale \"Hamza\"- narrating general content and news, and 2) expressive female\n\"Amina\"- narrating children story books to train our models. Our best systems\nachieve an average Mean Opinion Score (MOS) of 4.21 and 4.40 for Amina and\nHamza respectively. The objective evaluation of the systems using word and\ncharacter error rate (WER and CER) as well as the response time measured by\nreal-time factor favored the end-to-end architecture ESPnet. NatiQ demo is\navailable on-line at https://tts.qcri.org\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiroglu_C/0/1/0/all/0/1\">Cenk Demiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Structure Search for Parameter-Efficient Tuning. (arXiv:2206.07382v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07382","description":"<p>Adapting large pre-trained models (PTMs) through fine-tuning imposes\nprohibitive computational and storage burdens. Recent studies of\nparameter-efficient tuning (PET) find that only optimizing a small portion of\nparameters conditioned on PTMs could yield on-par performance compared to\nconventional fine-tuning. Generally, PET methods exquisitely design\nparameter-efficient modules (PET modules) which could be applied to arbitrary\nfine-grained positions inside PTMs. However, the effectiveness of these\nfine-grained positions largely relies on sophisticated manual designation,\nthereby usually producing sub-optimal results. In contrast to the manual\ndesignation, we explore constructing PET modules in an automatic manner. We\nautomatically \\textbf{S}earch for the \\textbf{S}parse \\textbf{S}tructure of\n\\textbf{P}arameter-\\textbf{E}fficient \\textbf{T}uning (S$^3$PET). Based on a\nunified framework of various PET methods, S$^3$PET conducts the differentiable\nPET structure search through bi-level optimization and proposes shifted global\nsigmoid method to explicitly control the number of trainable parameters.\nExtensive experiments show that S$^3$PET surpasses manual and random structures\nwith less trainable parameters. The searched structures preserve more than 99\\%\nfine-tuning performance with 0.01\\% trainable parameters. Moreover, the\nadvantage of S$^3$PET is amplified with extremely low trainable parameters\nbudgets (0.0009\\%$\\sim$0.01\\%). The searched structures are transferable and\nexplainable, providing suggestions and guidance for the future design of PET\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yadao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Confidence of Predictions of Individual Classifiers and Their Ensembles for the Genre Classification Task. (arXiv:2206.07427v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07427","description":"<p>Genre identification is a subclass of non-topical text classification. The\nmain difference between this task and topical classification is that genres,\nunlike topics, usually do not correspond to simple keywords, and thus they need\nto be defined in terms of their functions in communication. Neural models based\non pre-trained transformers, such as BERT or XLM-RoBERTa, demonstrate SOTA\nresults in many NLP tasks, including non-topical classification. However, in\nmany cases, their downstream application to very large corpora, such as those\nextracted from social media, can lead to unreliable results because of dataset\nshifts, when some raw texts do not match the profile of the training set. To\nmitigate this problem, we experiment with individual models as well as with\ntheir ensembles. To evaluate the robustness of all models we use a prediction\nconfidence metric, which estimates the reliability of a prediction in the\nabsence of a gold standard label. We can evaluate robustness via the confidence\ngap between the correctly classified texts and the misclassified ones on a\nlabeled test corpus, higher gaps make it easier to improve our confidence that\nour classifier made the right decision. Our results show that for all of the\nclassifiers tested in this study, there is a confidence gap, but for the\nensembles, the gap is bigger, meaning that ensembles are more robust than their\nindividual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lepekhin_M/0/1/0/all/0/1\">Mikhail Lepekhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharoff_S/0/1/0/all/0/1\">Serge Sharoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BaIT: Barometer for Information Trustworthiness. (arXiv:2206.07535v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07535","description":"<p>This paper presents a new approach to the FNC-1 fake news classification task\nwhich involves employing pre-trained encoder models from similar NLP tasks,\nnamely sentence similarity and natural language inference, and two neural\nnetwork architectures using this approach are proposed. Methods in data\naugmentation are explored as a means of tackling class imbalance in the\ndataset, employing common pre-existing methods and proposing a method for\nsample generation in the under-represented class using a novel sentence\nnegation algorithm. Comparable overall performance with existing baselines is\nachieved, while significantly increasing accuracy on an under-represented but\nnonetheless important class for FNC-1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nolan_O/0/1/0/all/0/1\">Ois&#xed;n Nolan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mourik_J/0/1/0/all/0/1\">Jeroen van Mourik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tilbury_C/0/1/0/all/0/1\">Callum Tilbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPI: Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07550","description":"<p>Originated as a philosophical quest, personality discerns how individuals\ndiffer from each other in terms of thinking, feeling, and behaving. Towards\nbuilding social machines that work with humans on a daily basis, we are\nmotivated to ask: (1) Do existing pre-trained language models possess\npersonality, akin to their human counterpart? If so, (2) how can we evaluate\nthem? Further, given this evaluation framework, (3) how can we induce a certain\npersonality in a fully controllable fashion? To tackle these three questions,\nwe propose the Machine Personality Inventory (MPI) dataset for evaluating the\nmachine personality; MPI follows standardized personality tests, built upon the\nBig Five Personality Factors (Big Five) theory and personality assessment\ninventories. By evaluating models with MPI, we provide the first piece of\nevidence showing the existence of personality in pre-trained language models.\nWe further devise a Chain Prompting method to induce the language model with a\nspecific personality in a controllable manner, capable of producing diversified\nbehaviors. We hope to shed light on future studies by adopting personality as\nthe essential psychological guidance for various downstream tasks, building\nmore human-like and in situ dialogue agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guangyuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Manjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KGEA: A Knowledge Graph Enhanced Article Quality Identification Dataset. (arXiv:2206.07556v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07556","description":"<p>With so many articles of varying quality being produced at every moment, it\nis a very urgent task to screen this data for quality articles and commit them\nout to social media. It is worth noting that high quality articles have many\ncharacteristics, such as relevance, text quality, straightforward, multi-sided,\nbackground, novelty and sentiment. Thus, it would be inadequate to purely use\nthe content of an article to identify its quality. Therefore, we plan to use\nthe external knowledge interaction to refine the performance and propose a\nknowledge graph enhanced article quality identification dataset (KGEA) based on\nBaidu Encyclopedia. We quantified the articles through 7 dimensions and use\nco-occurrence of the entities between the articles and the Baidu encyclopedia\nto construct the knowledge graph for every article. We also compared some text\nclassification baselines and found that external knowledge can guide the\narticles to a more competitive classification with the graph neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ai_C/0/1/0/all/0/1\">Chunhui Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Derui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wenrui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziqiang Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualization and Generalization in Entity and Relation Extraction. (arXiv:2206.07558v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07558","description":"<p>During the past decade, neural networks have become prominent in Natural\nLanguage Processing (NLP), notably for their capacity to learn relevant word\nrepresentations from large unlabeled corpora. These word embeddings can then be\ntransferred and finetuned for diverse end applications during a supervised\ntraining phase. More recently, in 2018, the transfer of entire pretrained\nLanguage Models and the preservation of their contextualization capacities\nenabled to reach unprecedented performance on virtually every NLP benchmark,\nsometimes even outperforming human baselines. However, as models reach such\nimpressive scores, their comprehension abilities still appear as shallow, which\nreveal limitations of benchmarks to provide useful insights on their factors of\nperformance and to accurately measure understanding capabilities.\n</p>\n<p>In this thesis, we study the behaviour of state-of-the-art models regarding\ngeneralization to facts unseen during training in two important Information\nExtraction tasks: Named Entity Recognition (NER) and Relation Extraction (RE).\nIndeed, traditional benchmarks present important lexical overlap between\nmentions and relations used for training and evaluating models, whereas the\nmain interest of Information Extraction is to extract previously unknown\ninformation. We propose empirical studies to separate performance based on\nmention and relation overlap with the training set and find that pretrained\nLanguage Models are mainly beneficial to detect unseen mentions, in particular\nout-of-domain. While this makes them suited for real use cases, there is still\na gap in performance between seen and unseen mentions that hurts generalization\nto new facts. In particular, even state-of-the-art ERE models rely on a shallow\nretention heuristic, basing their prediction more on arguments surface forms\nthan context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taille_B/0/1/0/all/0/1\">Bruno Taill&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMR Alignment: Paying Attention to Cross-Attention. (arXiv:2206.07587v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07587","description":"<p>With the surge of Transformer models, many have investigated how attention\nacts on the learned representations. However, attention is still overlooked for\nspecific tasks, such as Semantic Parsing. A popular approach to the formal\nrepresentation of a sentence's meaning is Abstract Meaning Representation\n(AMR). Until now, the alignment between a sentence and its AMR representation\nhas been explored in different ways, such as through rules or via the\nExpectation Maximization (EM) algorithm. In this paper, we investigate the\nability of Transformer-based parsing models to yield effective alignments\nwithout ad-hoc strategies. We present the first in-depth exploration of\ncross-attention for AMR by proxy of alignment between the sentence spans and\nthe semantic units in the graph. We show how current Transformer-based parsers\nimplicitly encode the alignment information in the cross-attention weights and\nhow to leverage it to extract such alignment. Furthermore, we supervise and\nguide cross-attention using alignment, dropping the need for English- and\nAMR-specific rules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cabot_P/0/1/0/all/0/1\">Pere-Llu&#xed;s Huguet Cabot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzo_A/0/1/0/all/0/1\">Abelardo Carlos Mart&#xed;nez Lorenzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navigli_R/0/1/0/all/0/1\">Roberto Navigli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HICEM: A High-Coverage Emotion Model for Artificial Emotional Intelligence. (arXiv:2206.07593v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07593","description":"<p>As social robots and other intelligent machines enter the home, artificial\nemotional intelligence (AEI) is taking center stage to address users' desire\nfor deeper, more meaningful human-machine interaction. To accomplish such\nefficacious interaction, the next-generation AEI need comprehensive human\nemotion models for training. Unlike theory of emotion, which has been the\nhistorical focus in psychology, emotion models are a descriptive tools. In\npractice, the strongest models need robust coverage, which means defining the\nsmallest core set of emotions from which all others can be derived. To achieve\nthe desired coverage, we turn to word embeddings from natural language\nprocessing. Using unsupervised clustering techniques, our experiments show that\nwith as few as 15 discrete emotion categories, we can provide maximum coverage\nacross six major languages--Arabic, Chinese, English, French, Spanish, and\nRussian. In support of our findings, we also examine annotations from two\nlarge-scale emotion recognition datasets to assess the validity of existing\nemotion models compared to human perception at scale. Because robust,\ncomprehensive emotion models are foundational for developing real-world\naffective computing applications, this work has broad implications in social\nrobotics, human-machine interaction, mental healthcare, and computational\npsychology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortman_B/0/1/0/all/0/1\">Benjamin Wortman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation. (arXiv:2206.07615v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07615","description":"<p>The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems\nto decompose a word into a sequence of morphemes and covered most types of\nmorphology: compounds, derivations, and inflections. Subtask 1, word-level\nmorpheme segmentation, covered 5 million words in 9 languages (Czech, English,\nSpanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13\nsystem submissions from 7 teams and the best system averaged 97.29% F1 score\nacross all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2,\nsentence-level morpheme segmentation, covered 18,735 sentences in 3 languages\n(Czech, English, Mongolian), received 10 system submissions from 3 teams, and\nthe best systems outperformed all three state-of-the-art subword tokenization\nmethods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis\nand support any type of future studies, we released all system predictions, the\nevaluation script, and all gold standard datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Batsuren_K/0/1/0/all/0/1\">Khuyagbaatar Batsuren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">G&#xe1;bor Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Aryaman Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinovic_V/0/1/0/all/0/1\">Viktor Martinovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorman_K/0/1/0/all/0/1\">Kyle Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabokrtsky_Z/0/1/0/all/0/1\">Zden&#x11b;k &#x17d;abokrtsk&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganbold_A/0/1/0/all/0/1\">Amarsanaa Ganbold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohnalova_S/0/1/0/all/0/1\">&#x160;&#xe1;rka Dohnalov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevcikova_M/0/1/0/all/0/1\">Magda &#x160;ev&#x10d;&#xed;kov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelegrinova_K/0/1/0/all/0/1\">Kate&#x159;ina Pelegrinov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vylomova_E/0/1/0/all/0/1\">Ekaterina Vylomova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Capabilities of Monolingual Audio Transformers using Large Datasets in Automatic Speech Recognition of Czech. (arXiv:2206.07627v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07627","description":"<p>In this paper, we present our progress in pretraining Czech monolingual audio\ntransformers from a large dataset containing more than 80 thousand hours of\nunlabeled speech, and subsequently fine-tuning the model on automatic speech\nrecognition tasks using a combination of in-domain data and almost 6 thousand\nhours of out-of-domain transcribed speech. We are presenting a large palette of\nexperiments with various fine-tuning setups evaluated on two public datasets\n(CommonVoice and VoxPopuli) and one extremely challenging dataset from the\nMALACH project. Our results show that monolingual Wav2Vec 2.0 models are robust\nASR systems, which can take advantage of large labeled and unlabeled datasets\nand successfully compete with state-of-the-art LVCSR systems. Moreover, Wav2Vec\nmodels proved to be good zero-shot learners when no training data are available\nfor the target ASR task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lehecka_J/0/1/0/all/0/1\">Jan Lehe&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svec_J/0/1/0/all/0/1\">Jan &#x160;vec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_A/0/1/0/all/0/1\">Ale&#x161; Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psutka_J/0/1/0/all/0/1\">Josef V. Psutka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. (arXiv:2206.07643v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07643","description":"<p>Vision-language (VL) pre-training has recently received considerable\nattention. However, most existing end-to-end pre-training approaches either\nonly aim to tackle VL tasks such as image-text retrieval, visual question\nanswering (VQA) and image captioning that test high-level understanding of\nimages, or only target region-level understanding for tasks such as phrase\ngrounding and object detection. We present FIBER (Fusion-In-the-Backbone-based\ntransformER), a new VL model architecture that can seamlessly handle both these\ntypes of tasks. Instead of having dedicated transformer layers for fusion after\nthe uni-modal backbones, FIBER pushes multimodal fusion deep into the model by\ninserting cross-attention into the image and text backbones, bringing gains in\nterms of memory and performance. In addition, unlike previous work that is\neither only pre-trained on image-text data or on fine-grained data with\nbox-level annotations, we present a two-stage pre-training strategy that uses\nboth these kinds of data efficiently: (i) coarse-grained pre-training based on\nimage-text data; followed by (ii) fine-grained pre-training based on\nimage-text-box data. We conduct comprehensive experiments on a wide range of VL\ntasks, ranging from VQA, image captioning, and retrieval, to phrase grounding,\nreferring expression comprehension, and object detection. Using deep multimodal\nfusion coupled with the two-stage pre-training, FIBER provides consistent\nperformance improvements over strong baselines across all tasks, often\noutperforming methods using magnitudes more data. Code is available at\nhttps://github.com/microsoft/FIBER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Aishwarya Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Automatic Speech Recognition of Formal and Colloquial Czech in MALACH Project. (arXiv:2206.07666v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07666","description":"<p>Czech is a very specific language due to its large differences between the\nformal and the colloquial form of speech. While the formal (written) form is\nused mainly in official documents, literature, and public speeches, the\ncolloquial (spoken) form is used widely among people in casual speeches. This\ngap introduces serious problems for ASR systems, especially when training or\nevaluating ASR models on datasets containing a lot of colloquial speech, such\nas the MALACH project. In this paper, we are addressing this problem in the\nlight of a new paradigm in end-to-end ASR systems -- recently introduced\nself-supervised audio Transformers. Specifically, we are investigating the\ninfluence of colloquial speech on the performance of Wav2Vec 2.0 models and\ntheir ability to transcribe colloquial speech directly into formal transcripts.\nWe are presenting results with both formal and colloquial forms in the training\ntranscripts, language models, and evaluation transcripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lehecka_J/0/1/0/all/0/1\">Jan Lehe&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psutka_J/0/1/0/all/0/1\">Josef V. Psutka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psutka_J/0/1/0/all/0/1\">Josef Psutka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Sequence Interface for Vision Tasks. (arXiv:2206.07669v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07669","description":"<p>While language tasks are naturally expressed in a single, unified, modeling\nframework, i.e., generating sequences of tokens, this has not been the case in\ncomputer vision. As a result, there is a proliferation of distinct\narchitectures and loss functions for different vision tasks. In this work we\nshow that a diverse set of \"core\" computer vision tasks can also be unified if\nformulated in terms of a shared pixel-to-sequence interface. We focus on four\ntasks, namely, object detection, instance segmentation, keypoint detection, and\nimage captioning, all with diverse types of outputs, e.g., bounding boxes or\ndense masks. Despite that, by formulating the output of each task as a sequence\nof discrete tokens with a unified interface, we show that one can train a\nneural network with a single model architecture and loss function on all these\ntasks, with no task-specific customization. To solve a specific task, we use a\nshort prompt as task description, and the sequence output adapts to the prompt\nso it can produce task-specific output. We show that such a model can achieve\ncompetitive performance compared to well-established task-specific models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Saurabh Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Abilities of Large Language Models. (arXiv:2206.07682v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07682","description":"<p>Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIRECTOR: Generator-Classifiers For Supervised Language Modeling. (arXiv:2206.07694v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07694","description":"<p>Current language models achieve low perplexity but their resulting\ngenerations still suffer from toxic responses, repetitiveness and\ncontradictions. The standard language modeling setup fails to address these\nissues. In this paper, we introduce a new architecture, {\\sc Director}, that\nconsists of a unified generator-classifier with both a language modeling and a\nclassification head for each output token. Training is conducted jointly using\nboth standard language modeling data, and data labeled with desirable and\nundesirable sequences. Experiments in several settings show that the model has\ncompetitive training and decoding speed compared to standard language models\nwhile yielding superior results, alleviating known issues while maintaining\ngeneration quality. It also outperforms existing model guiding approaches in\nterms of both accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_K/0/1/0/all/0/1\">Kushal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prefix Language Models are Unified Modal Learners. (arXiv:2206.07699v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07699","description":"<p>With the success of vision-language pre-training, we have witnessed the\nstate-of-the-art has been pushed on multi-modal understanding and generation.\nHowever, the current pre-training paradigm is either incapable of targeting all\nmodalities at once (e.g., text generation and image generation), or requires\nmulti-fold well-designed tasks which significantly limits the scalability. We\ndemonstrate that a unified modal model could be learned with a prefix language\nmodeling objective upon text and image sequences. Thanks to the simple but\npowerful pre-training paradigm, our proposed model, DaVinci, is simple to\ntrain, scalable to huge data, and adaptable to a variety of downstream tasks\nacross modalities (language / vision / vision+language), types (understanding /\ngeneration) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with\na single unified architecture. DaVinci achieves the competitive performance on\na wide range of 26 understanding / generation tasks, and outperforms previous\nunified vision-language models on most tasks, including ImageNet classification\n(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and\nCOCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data\nscale. Furthermore, we offer a well-defined benchmark for future research by\nreporting the performance on different scales of the pre-training dataset on a\nheterogeneous and wide distribution coverage. Our results establish new,\nstronger baselines for future comparisons at different data scales and shed\nlight on the difficulties of comparing VLP models more generally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiawei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adposition and Case Supersenses v2.6: Guidelines for English. (arXiv:1704.02134v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1704.02134","description":"<p>This document offers a detailed linguistic description of SNACS (Semantic\nNetwork of Adposition and Case Supersenses; Schneider et al., 2018), an\ninventory of 52 semantic labels (\"supersenses\") that characterize the use of\nadpositions and case markers at a somewhat coarse level of granularity, as\ndemonstrated in the STREUSLE corpus (https://github.com/nert-nlp/streusle/;\nversion 4.5 tracks guidelines version 2.6). Though the SNACS inventory aspires\nto be universal, this document is specific to English; documentation for other\nlanguages will be published separately.\n</p>\n<p>Version 2 is a revision of the supersense inventory proposed for English by\nSchneider et al. (2015, 2016) (henceforth \"v1\"), which in turn was based on\nprevious schemes. The present inventory was developed after extensive review of\nthe v1 corpus annotations for English, plus previously unanalyzed genitive case\npossessives (Blodgett and Schneider, 2018), as well as consideration of\nadposition and case phenomena in Hebrew, Hindi, Korean, and German. Hwang et\nal. (2017) present the theoretical underpinnings of the v2 scheme. Schneider et\nal. (2018) summarize the scheme, its application to English corpus data, and an\nautomatic disambiguation task. Liu et al. (2021) offer an English Lexical\nSemantic Recognition tagger that includes SNACS labels in its output.\n</p>\n<p>This documentation can also be browsed alongside corpus data on the Xposition\nwebsite (Gessler et al., 2022): <a href=\"http://www.xposition.org/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1\">Archna Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_N/0/1/0/all/0/1\">Na-Rae Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_S/0/1/0/all/0/1\">Sarah R. Moeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_A/0/1/0/all/0/1\">Adi Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_A/0/1/0/all/0/1\">Austin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prange_J/0/1/0/all/0/1\">Jakob Prange</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-evaluating Word Mover's Distance. (arXiv:2105.14403v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14403","description":"<p>The word mover's distance (WMD) is a fundamental technique for measuring the\nsimilarity of two documents. As the crux of WMD, it can take advantage of the\nunderlying geometry of the word space by employing an optimal transport\nformulation. The original study on WMD reported that WMD outperforms classical\nbaselines such as bag-of-words (BOW) and TF-IDF by significant margins in\nvarious datasets. In this paper, we point out that the evaluation in the\noriginal study could be misleading. We re-evaluate the performances of WMD and\nthe classical baselines and find that the classical baselines are competitive\nwith WMD if we employ an appropriate preprocessing, i.e., L1 normalization. In\naddition, we introduce an analogy between WMD and L1-normalized BOW and find\nthat not only the performance of WMD but also the distance values resemble\nthose of BOW in high dimensional spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1\">Makoto Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Enhancing Multi-filter Sequence-to-Sequence Model. (arXiv:2109.12399v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12399","description":"<p>In sequence-to-sequence tasks, sentences with heterogeneous semantics or\ngrammatical structures may lead to some difficulties in the model's\nconvergence. To resolve this problem, we introduce a feature-concentrated model\nthat focuses on each of the heterogeneous features in the input-output\nsequences. Building upon the encoder-decoder architecture, we design a\nlatent-enhanced multi-filter sequence-to-sequence model (LMS2S) that analyzes\nthe features preserved by latent space representations and constructs the\noutputs accordingly. We divide the latent space into subspaces using a\nclustering algorithm and train a set of decoders in which each decoder only\nconcentrates on the features from its corresponding subspace. We then design a\nself-enhancing mechanism that uses reinforcement learning to optimize the\nclustering algorithm. We perform two sets of experiments on semantic parsing\nand machine translation. We empirically demonstrate the advantages of the\nmulti-filter architecture and show the performance improvement made by the\nself-enhancing mechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Dataset Difficulty with $\\mathcal{V}$-Usable Information. (arXiv:2110.08420v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08420","description":"<p>Estimating the difficulty of a dataset typically involves comparing\nstate-of-the-art models to humans; the bigger the performance gap, the harder\nthe dataset is said to be. However, this comparison provides little\nunderstanding of how difficult each instance in a given distribution is, or\nwhat attributes make the dataset difficult for a given model. To address these\nquestions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as\nthe lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019),\nwhere a lower value indicates a more difficult dataset for $\\mathcal{V}$. We\nfurther introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for\nmeasuring the difficulty of individual instances w.r.t. a given distribution.\nWhile standard evaluation metrics typically only compare different models for\nthe same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also\npermit the converse: for a given model $\\mathcal{V}$, we can compare different\ndatasets, as well as different instances/slices of the same dataset.\nFurthermore, our framework allows for the interpretability of different input\nattributes via transformations of the input, which we use to discover\nannotation artefacts in widely-used NLP benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Private Language Model Adaptation for Speech Recognition. (arXiv:2110.10026v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.10026","description":"<p>Speech model adaptation is crucial to handle the discrepancy between\nserver-side proxy training data and actual data received on local devices of\nusers. With the use of federated learning (FL), we introduce an efficient\napproach on continuously adapting neural network language models (NNLMs) on\nprivate devices with applications on automatic speech recognition (ASR). To\naddress the potential speech transcription errors in the on-device training\ncorpus, we perform empirical studies on comparing various strategies of\nleveraging token-level confidence scores to improve the NNLM quality in the FL\nsettings. Experiments show that compared with no model adaptation, the proposed\nmethod achieves relative 2.6% and 10.8% word error rate (WER) reductions on two\nspeech evaluation datasets, respectively. We also provide analysis in\nevaluating privacy guarantees of our presented procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakshi_S/0/1/0/all/0/1\">Shreyan Bakshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_F/0/1/0/all/0/1\">Fuchun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are not Models of Natural Language: they are Corpus Models. (arXiv:2112.07055v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07055","description":"<p>Natural Language Processing (NLP) has become one of the leading application\nareas in the current Artificial Intelligence boom. Transfer learning has\nenabled large deep learning neural networks trained on the language modeling\ntask to vastly improve performance in almost all downstream language tasks.\nInterestingly, when the language models are trained with data that includes\nsoftware code, they demonstrate remarkable abilities in generating functioning\ncomputer code from natural language specifications. We argue that this creates\na conundrum for the claim that eliminative neural models are a radical\nrestructuring in our understanding of cognition in that they eliminate the need\nfor symbolic abstractions like generative phrase structure grammars. Because\nthe syntax of programming languages is by design determined by phrase structure\ngrammars, neural models that produce syntactic code are apparently\nuninformative about the theoretical foundations of programming languages. The\ndemonstration that neural models perform well on tasks that involve clearly\nsymbolic systems, proves that they cannot be used as an argument that language\nand other cognitive systems are not symbolic. Finally, we argue as a corollary\nthat the term language model is misleading and propose the adoption of the\nworking term corpus model instead, which better reflects the genesis and\ncontents of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veres_C/0/1/0/all/0/1\">Csaba Veres</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperPrompt: Prompt-based Task-Conditioning of Transformers. (arXiv:2203.00759v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00759","description":"<p>Prompt-Tuning is a new paradigm for finetuning pre-trained language models in\na parameter-efficient way. Here, we explore the use of HyperNetworks to\ngenerate hyper-prompts: we propose HyperPrompt, a novel architecture for\nprompt-based task-conditioning of self-attention in Transformers. The\nhyper-prompts are end-to-end learnable via generation by a HyperNetwork.\nHyperPrompt allows the network to learn task-specific feature maps where the\nhyper-prompts serve as task global memories for the queries to attend to, at\nthe same time enabling flexible information sharing among tasks. We show that\nHyperPrompt is competitive against strong multi-task learning baselines with as\nfew as $0.14\\%$ of additional task-conditioning parameters, achieving great\nparameter and computational efficiency. Through extensive empirical\nexperiments, we demonstrate that HyperPrompt can achieve superior performances\nover strong T5 multi-task learning baselines and parameter-efficient adapter\nvariants including Prompt-Tuning and HyperFormer++ on Natural Language\nUnderstanding benchmarks of GLUE and SuperGLUE across many model sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aribandi_V/0/1/0/all/0/1\">Vamsi Aribandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">YaGuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition. (arXiv:2203.05008v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05008","description":"<p>Language model fusion helps smart assistants recognize words which are rare\nin acoustic data but abundant in text-only corpora (typed search logs).\nHowever, such corpora have properties that hinder downstream performance,\nincluding being (1) too large, (2) beset with domain-mismatched content, and\n(3) heavy-headed rather than heavy-tailed (excessively many duplicate search\nqueries such as \"weather\"). We show that three simple strategies for selecting\nlanguage modeling data can dramatically improve rare-word recognition without\nharming overall performance. First, to address the heavy-headedness, we\ndownsample the data according to a soft log function, which tunably reduces\nhigh frequency (head) sentences. Second, to encourage rare-word exposure, we\nexplicitly filter for words rare in the acoustic data. Finally, we tackle\ndomain-mismatch via perplexity-based contrastive selection, filtering for\nexamples matched to the target domain. We down-select a large corpus of web\nsearch queries by a factor of 53x and achieve better LM perplexities than\nwithout down-selection. When shallow-fused with a state-of-the-art, production\nspeech engine, our LM achieves WER reductions of up to 24% relative on\nrare-word sentences (without changing overall WER) compared to a baseline LM\ntrained on the raw corpus. These gains are further validated through favorable\nside-by-side evaluations on live voice search traffic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1\">Cal Peyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-based Discriminative Autoencoders for Speech Recognition. (arXiv:2203.13687v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.13687","description":"<p>In our previous work, we proposed a discriminative autoencoder (DcAE) for\nspeech recognition. DcAE combines two training schemes into one. First, since\nDcAE aims to learn encoder-decoder mappings, the squared error between the\nreconstructed speech and the input speech is minimized. Second, in the code\nlayer, frame-based phonetic embeddings are obtained by minimizing the\ncategorical cross-entropy between ground truth labels and predicted\ntriphone-state scores. DcAE is developed based on the Kaldi toolkit by treating\nvarious TDNN models as encoders. In this paper, we further propose three new\nversions of DcAE. First, a new objective function that considers both\ncategorical cross-entropy and mutual information between ground truth and\npredicted triphone-state sequences is used. The resulting DcAE is called a\nchain-based DcAE (c-DcAE). For application to robust speech recognition, we\nfurther extend c-DcAE to hierarchical and parallel structures, resulting in\nhc-DcAE and pc-DcAE. In these two models, both the error between the\nreconstructed noisy speech and the input noisy speech and the error between the\nenhanced speech and the reference clean speech are taken into the objective\nfunction. Experimental results on the WSJ and Aurora-4 corpora show that our\nDcAE models outperform baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pin-Tuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifted Chunk Encoder for Transformer Based Streaming End-to-End ASR. (arXiv:2203.15206v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.15206","description":"<p>Currently, there are mainly three Transformer encoder based streaming End to\nEnd (E2E) Automatic Speech Recognition (ASR) approaches, namely time-restricted\nmethods, chunk-wise methods, and memory based methods. However, all of them\nhave some limitations in aspects of global context modeling, linear\ncomputational complexity, and model parallelism. In this work, we aim to build\na single model to achieve the benefits of all the three aspects for streaming\nE2E ASR. Particularly, we propose to use a shifted chunk mechanism instead of\nthe conventional chunk mechanism for streaming Transformer and Conformer. This\nshifted chunk mechanism can significantly enhance modeling power through\nallowing chunk self-attention to capture global context across local chunks,\nwhile keeping linear computational complexity and parallel trainable. We name\nthe Shifted Chunk Transformer and Conformer as SChunk-Transofromer and\nSChunk-Conformer, respectively. And we verify their performance on the widely\nused AISHELL-1 benckmark. Experiments show that the SChunk-Transformer and\nSChunk-Conformer achieve CER 6.43% and 5.77%, respectively. That surpasses the\nexisting chunk-wise and memory based methods by a large margin, and is\ncompetitive even compared with the state-of-the-art time-restricted methods\nwhich have quadratic computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fangyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vakyansh: ASR Toolkit for Low Resource Indic languages. (arXiv:2203.16512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16512","description":"<p>We present Vakyansh, an end to end toolkit for Speech Recognition in Indic\nlanguages. India is home to almost 121 languages and around 125 crore speakers.\nYet most of the languages are low resource in terms of data and pretrained\nmodels. Through Vakyansh, we introduce automatic data pipelines for data\ncreation, model training, model evaluation and deployment. We create 14,000\nhours of speech data in 23 Indic languages and train wav2vec 2.0 based\npretrained models. These pretrained models are then finetuned to create state\nof the art speech recognition models for 18 Indic languages which are followed\nby language models and punctuation restoration models. We open source all these\nresources with a mission that this will inspire the speech community to develop\nspeech first applications using our ASR models in Indic languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Speech Recognition for Indic Languages using Language Model. (arXiv:2203.16595v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16595","description":"<p>We study the effect of applying a language model (LM) on the output of\nAutomatic Speech Recognition (ASR) systems for Indic languages. We fine-tune\nwav2vec $2.0$ models for $18$ Indic languages and adjust the results with\nlanguage models trained on text derived from a variety of sources. Our findings\ndemonstrate that the average Character Error Rate (CER) decreases by over $28$\n\\% and the average Word Error Rate (WER) decreases by about $36$ \\% after\ndecoding with LM. We show that a large LM may not provide a substantial\nimprovement as compared to a diverse one. We also demonstrate that high quality\ntranscriptions can be obtained on domain-specific data without retraining the\nASR model and show results on biomedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Word Error Rate a good evaluation metric for Speech Recognition in Indic Languages?. (arXiv:2203.16601v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16601","description":"<p>We propose a new method for the calculation of error rates in Automatic\nSpeech Recognition (ASR). This new metric is for languages that contain half\ncharacters and where the same character can be written in different forms. We\nimplement our methodology in Hindi which is one of the main languages from\nIndic context and we think this approach is scalable to other similar languages\ncontaining a large character set. We call our metrics Alternate Word Error Rate\n(AWER) and Alternate Character Error Rate (ACER).\n</p>\n<p>We train our ASR models using wav2vec 2.0\\cite{baevski2020wav2vec} for Indic\nlanguages. Additionally we use language models to improve our model\nperformance. Our results show a significant improvement in analyzing the error\nrates at word and character level and the interpretability of the ASR system is\nimproved upto $3$\\% in AWER and $7$\\% in ACER for Hindi. Our experiments\nsuggest that in languages which have complex pronunciation, there are multiple\nways of writing words without changing their meaning. In such cases AWER and\nACER will be more useful rather than WER and CER as metrics. Further, we open\nsource a new benchmarking dataset of 21 hours for Hindi with the new metric\nscripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-end Unsupervised Speech Recognition. (arXiv:2204.02492v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02492","description":"<p>Unsupervised speech recognition has shown great potential to make Automatic\nSpeech Recognition (ASR) systems accessible to every language. However,\nexisting methods still heavily rely on hand-crafted pre-processing. Similar to\nthe trend of making supervised speech recognition end-to-end, we introduce\nwav2vec-U 2.0 which does away with all audio-side pre-processing and improves\naccuracy through better architecture. In addition, we introduce an auxiliary\nself-supervised objective that ties model predictions back to the input.\nExperiments show that wav2vec-U 2.0 improves unsupervised recognition results\nacross different languages while being conceptually simpler.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Complexity Randomized Self-attention Mechanism. (arXiv:2204.04667v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.04667","description":"<p>Recently, random feature attentions (RFAs) are proposed to approximate the\nsoftmax attention in linear time and space complexity by linearizing the\nexponential kernel. In this paper, we first propose a novel perspective to\nunderstand the bias in such approximation by recasting RFAs as self-normalized\nimportance samplers. This perspective further sheds light on an \\emph{unbiased}\nestimator for the whole softmax attention, called randomized attention (RA). RA\nconstructs positive random features via query-specific distributions and enjoys\ngreatly improved approximation fidelity, albeit exhibiting quadratic\ncomplexity. By combining the expressiveness in RA and the efficiency in RFA, we\ndevelop a novel linear complexity self-attention mechanism called linear\nrandomized attention (LARA). Extensive experiments across various domains\ndemonstrate that RA and LARA significantly improve the performance of RFAs by a\nsubstantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling sign language transcription system with the computer-friendly numerical multilabels. (arXiv:2204.06924v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06924","description":"<p>This paper presents our recent developments in the field of automatic\nprocessing of sign language corpora using the Hamburg Sign Language Annotation\nSystem (HamNoSys). We designed an automated tool to convert HamNoSys\nannotations into numerical labels for defined initial features of body and hand\npositions. Our proposed numerical multilabels greatly simplify the structure of\nHamNoSys annotation without significant loss of gloss meaning. These numerical\nmultilabels can potentially be used to feed the machine learning models, which\nwould accelerate the development of vision-based sign language recognition. In\naddition, this tool can assist experts in the annotation process to help\nidentify semantic errors. The code and sample annotations are publicly\navailable at https://github.com/hearai/parse-hamnosys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1\">Sylwia Majchrowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plantykow_M/0/1/0/all/0/1\">Marta Plantykow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olech_M/0/1/0/all/0/1\">Milena Olech</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection using generative-based and mutation-based data augmentation. (arXiv:2204.08198v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08198","description":"<p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse\nsomeone. It is commonly used on social media. The metaphorical and creative\nnature of sarcasm presents a significant difficulty for sentiment analysis\nsystems based on affective computing. The methodology and results of our team,\nUTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in\nthis paper. We put different models, and data augmentation approaches to the\ntest and report on which one works best. The tests begin with traditional\nmachine learning models and progress to transformer-based and attention-based\nmodels. We employed data augmentation based on data mutation and data\ngeneration. Using RoBERTa and mutation-based data augmentation, our best\napproach achieved an F1-sarcastic of 0.38 in the competition's evaluation\nphase. After the competition, we fixed our model's flaws and achieved an\nF1-sarcastic of 0.414.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1\">Arash Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeraati_T/0/1/0/all/0/1\">Tanin Zeraati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrak_B/0/1/0/all/0/1\">Behnam Bahrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR. (arXiv:2204.10749v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.10749","description":"<p>Improving the performance of end-to-end ASR models on long utterances ranging\nfrom minutes to hours in length is an ongoing challenge in speech recognition.\nA common solution is to segment the audio in advance using a separate voice\nactivity detector (VAD) that decides segment boundary locations based purely on\nacoustic speech/non-speech information. VAD segmenters, however, may be\nsub-optimal for real-world speech where, e.g., a complete sentence that should\nbe taken as a whole may contain hesitations in the middle (\"set an alarm for...\n5 o'clock\").\n</p>\n<p>We propose to replace the VAD with an end-to-end ASR model capable of\npredicting segment boundaries in a streaming fashion, allowing the segmentation\ndecision to be conditioned not only on better acoustic features but also on\nsemantic features from the decoded text with negligible extra computation. In\nexperiments on real world long-form audio (YouTube) with lengths of up to 30\nminutes, we demonstrate 8.5% relative WER improvement and 250 ms reduction in\nmedian end-of-segment latency compared to the VAD segmenter baseline on a\nstate-of-the-art Conformer RNN-T model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-yiin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybach_D/0/1/0/all/0/1\">David Rybach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allauzen_C/0/1/0/all/0/1\">Cyril Allauzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1\">Cal Peyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyun Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Speech Representation Learning: A Review. (arXiv:2205.10643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10643","description":"<p>Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgholt_L/0/1/0/all/0/1\">Lasse Borgholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1\">Jakob D. Havtorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edin_J/0/1/0/all/0/1\">Joakim Edin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1\">Lars Maal&#xf8;e</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"History Compression via Language Models in Reinforcement Learning. (arXiv:2205.12258v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12258","description":"<p>In a partially observable Markov decision process (POMDP), an agent typically\nuses a representation of the past to approximate the underlying MDP. We propose\nto utilize a frozen Pretrained Language Transformer (PLT) for history\nrepresentation and compression to improve sample efficiency. To avoid training\nof the Transformer, we introduce FrozenHopfield, which automatically associates\nobservations with pretrained token embeddings. To form these associations, a\nmodern Hopfield network stores these token embeddings, which are retrieved by\nqueries that are obtained by a random but fixed projection of observations. Our\nnew method, HELM, enables actor-critic network architectures that contain a\npretrained language Transformer for history representation as a memory module.\nSince a representation of the past need not be learned, HELM is much more\nsample efficient than competitors. On Minigrid and Procgen environments HELM\nachieves new state-of-the-art results. Our code is available at\nhttps://github.com/ml-jku/helm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paischer_F/0/1/0/all/0/1\">Fabian Paischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_T/0/1/0/all/0/1\">Thomas Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1\">Vihang Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitto_Nemling_A/0/1/0/all/0/1\">Angela Bitto-Nemling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzleitner_M/0/1/0/all/0/1\">Markus Holzleitner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehner_S/0/1/0/all/0/1\">Sebastian Lehner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eghbal_zadeh_H/0/1/0/all/0/1\">Hamid Eghbal-zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1\">Sepp Hochreiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14014","description":"<p>Transformers have made progress in miscellaneous tasks, but suffer from\nquadratic computational and memory complexities. Recent works propose sparse\nTransformers with attention on sparse graphs to reduce complexity and remain\nstrong performance. While effective, the crucial parts of how dense a graph\nneeds to be to perform well are not fully explored. In this paper, we propose\nNormalized Information Payload (NIP), a graph scoring function measuring\ninformation transfer on graph, which provides an analysis tool for trade-offs\nbetween performance and complexity. Guided by this theoretical analysis, we\npresent Hypercube Transformer, a sparse Transformer that models token\ninteractions in a hypercube and shows comparable or even better results with\nvanilla Transformer while yielding $O(N\\log N)$ complexity with sequence length\n$N$. Experiments on tasks requiring various sequence lengths lay validation for\nour graph function well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chu-Tak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval. (arXiv:2206.02873v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2206.02873","description":"<p>Recent work has shown that small distilled language models are strong\ncompetitors to models that are orders of magnitude larger and slower in a wide\nrange of information retrieval tasks. This has made distilled and dense models,\ndue to latency constraints, the go-to choice for deployment in real-world\nretrieval applications. In this work, we question this practice by showing that\nthe number of parameters and early query-document interaction play a\nsignificant role in the generalization ability of retrieval models. Our\nexperiments show that increasing model size results in marginal gains on\nin-domain test sets, but much larger gains in new domains never seen during\nfine-tuning. Furthermore, we show that rerankers largely outperform dense ones\nof similar size in several tasks. Our largest reranker reaches the state of the\nart in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the\nprevious state of the art by 3 average points. Finally, we confirm that\nin-domain effectiveness is not a good indicator of zero-shot effectiveness.\nCode is available at\nhttps://github.com/guilhermemr04/scaling-zero-shot-retrieval.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Guilherme Moraes Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeronymo_V/0/1/0/all/0/1\">Vitor Jeronymo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1\">Hugo Abonizio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadaee_M/0/1/0/all/0/1\">Marzieh Fadaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities. (arXiv:2206.03132v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2206.03132","description":"<p>An increasing number of monitoring systems have been developed in smart\ncities to ensure that real-time operations of a city satisfy safety and\nperformance requirements. However, many existing city requirements are written\nin English with missing, inaccurate, or ambiguous information. There is a high\ndemand for assisting city policy makers in converting human-specified\nrequirements to machine-understandable formal specifications for monitoring\nsystems. To tackle this limitation, we build CitySpec, the first intelligent\nassistant system for requirement specification in smart cities. To create\nCitySpec, we first collect over 1,500 real-world city requirements across\ndifferent domains from over 100 cities and extract city-specific knowledge to\ngenerate a dataset of city vocabulary with 3,061 words. We also build a\ntranslation model and enhance it through requirement synthesis and develop a\nnovel online learning framework with validation under uncertainty. The\nevaluation results on real-world city requirements show that CitySpec increases\nthe sentence-level accuracy of requirement specification from 59.02% to 86.64%,\nand has strong adaptability to a new city and a new domain (e.g., F1 score for\nrequirements in Seattle increases from 77.6% to 93.75% with online learning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zirong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Isaac Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preum_S/0/1/0/all/0/1\">Sarah Preum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stankovic_J/0/1/0/all/0/1\">John A. Stankovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Meiyi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Words are all you need? Capturing human sensory similarity with textual descriptors. (arXiv:2206.04105v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04105","description":"<p>Recent advances in multimodal training use textual descriptions to\nsignificantly enhance machine understanding of images and videos. Yet, it\nremains unclear to what extent language can fully capture sensory experiences\nacross different modalities. A well-established approach for characterizing\nsensory experiences relies on similarity judgments, namely, the degree to which\npeople perceive two distinct stimuli as similar. We explore the relation\nbetween human similarity judgments and language in a series of large-scale\nbehavioral studies ($N=1,823$ participants) across three modalities (images,\naudio, and video) and two types of text descriptors: simple word tags and\nfree-text captions. In doing so, we introduce a novel adaptive pipeline for tag\nmining that is both efficient and domain-general. We show that our prediction\npipeline based on text descriptors exhibits excellent performance, and we\ncompare it against a comprehensive array of 611 baseline models based on\nvision-, audio-, and video-processing architectures. We further show that the\ndegree to which textual descriptors and models predict human similarity varies\nacross and within modalities. Taken together, these studies illustrate the\nvalue of integrating machine learning and cognitive science approaches to\nbetter understand the similarities and differences between human and machine\nrepresentations. We present an interactive visualization at\nhttps://words-are-all-you-need.s3.amazonaws.com/index.html for exploring the\nsimilarity between stimuli as experienced by humans and different methods\nreported in the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1\">Pol van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1\">Ilia Sucholutsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumers_T/0/1/0/all/0/1\">Theodore R. Sumers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural Machine Translation. (arXiv:2206.04922v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04922","description":"<p>Chinese dialect text-to-speech(TTS) system usually can only be utilized by\nnative linguists, because the written form of Chinese dialects has different\ncharacters, idioms, grammar and usage from Mandarin, and even the local speaker\ncannot input a correct sentence. For Mandarin text inputs, Chinese dialect TTS\ncan only generate partly-meaningful speech with relatively poor prosody and\nnaturalness. To lower the bar of use and make it more practical in commercial,\nwe propose a novel Chinese dialect TTS frontend with a translation module. It\nhelps to convert Mandarin text into idiomatic expressions with correct\northography and grammar, so that the intelligibility and naturalness of the\nsynthesized speech can be improved. A non-autoregressive neural machine\ntranslation model with a glancing sampling strategy is proposed for the\ntranslation task. It is the first known work to incorporate translation with\nTTS frontend. Our experiments on Cantonese approve that the proposed frontend\ncan help Cantonese TTS system achieve a 0.27 improvement in MOS with Mandarin\ninputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wudi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junjie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks. (arXiv:2206.06565v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.06565","description":"<p>Fine-tuning pretrained language models (LMs) without making any architectural\nchanges has become a norm for learning various language downstream tasks.\nHowever, for non-language downstream tasks, a common practice is to employ\ntask-specific designs for input, output layers, and loss functions. For\ninstance, it is possible to fine-tune an LM into an MNIST classifier by\nreplacing the word embedding layer with an image patch embedding layer, the\nword token output layer with a 10-way output layer, and the word prediction\nloss with a 10-way classification loss, respectively. A natural question\narises: can LM fine-tuning solve non-language downstream tasks without changing\nthe model architecture or loss function? To answer this, we propose\nLanguage-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations\nby conducting an extensive empirical study on a suite of non-language\nclassification and regression tasks. LIFT does not make any changes to the\nmodel architecture or loss function, and it solely relies on the natural\nlanguage interface, enabling \"no-code machine learning with LMs.\" We find that\nLIFT performs relatively well across a wide range of low-dimensional\nclassification and regression tasks, matching the performances of the best\nbaselines in many cases, especially for the classification tasks. We report the\nexperimental results on the fundamental properties of LIFT, including its\ninductive bias, sample efficiency, ability to extrapolate, robustness to\noutliers and label noise, and generalization. We also analyze a few\nproperties/techniques specific to LIFT, e.g., context-aware learning via\nappropriate prompting, quantification of predictive uncertainty, and two-stage\nfine-tuning. Our code is available at\nhttps://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruisu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ziqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gira_M/0/1/0/all/0/1\">Michael Gira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1\">Shashank Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization. (arXiv:2206.06581v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.06581","description":"<p>The quest for seeking health information has swamped the web with consumers'\nhealth-related questions. Generally, consumers use overly descriptive and\nperipheral information to express their medical condition or other healthcare\nneeds, contributing to the challenges of natural language understanding. One\nway to address this challenge is to summarize the questions and distill the key\ninformation of the original question. To address this issue, we introduce a new\ndataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health\nquestions and corresponding summaries. The dataset is derived from the\ncommunity question-answering forum and therefore provides a valuable resource\nfor understanding consumer health-related posts on social media. We benchmark\nthe dataset on multiple state-of-the-art summarization models to show the\neffectiveness of the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Shweta Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demner_Fushman_D/0/1/0/all/0/1\">Dina Demner-Fushman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Applications of Generative Adversarial Networks in Neuroimaging and Clinical Neuroscience. (arXiv:2206.07081v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07081","description":"<p>Generative adversarial networks (GANs) are one powerful type of deep learning\nmodels that have been successfully utilized in numerous fields. They belong to\na broader family called generative methods, which generate new data with a\nprobabilistic model by learning sample distribution from real examples. In the\nclinical context, GANs have shown enhanced capabilities in capturing spatially\ncomplex, nonlinear, and potentially subtle disease effects compared to\ntraditional generative methods. This review appraises the existing literature\non the applications of GANs in imaging studies of various neurological\nconditions, including Alzheimer's disease, brain tumors, brain aging, and\nmultiple sclerosis. We provide an intuitive explanation of various GAN methods\nfor each application and further discuss the main challenges, open questions,\nand promising future directions of leveraging GANs in neuroimaging. We aim to\nbridge the gap between advanced deep learning methods and neurology research by\nhighlighting how GANs can be leveraged to support clinical decision making and\ncontribute to a better understanding of the structural and functional patterns\nof brain diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashyam_V/0/1/0/all/0/1\">Vishnu Bashyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fanyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tassopoulou_V/0/1/0/all/0/1\">Vasiliki Tassopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreepada_L/0/1/0/all/0/1\">Lasya P. Sreepada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chintapalli_S/0/1/0/all/0/1\">Sai Spandana Chintapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_D/0/1/0/all/0/1\">Dushyant Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skampardoni_I/0/1/0/all/0/1\">Ioanna Skampardoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikita_K/0/1/0/all/0/1\">Konstantina Nikita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulkadir_A/0/1/0/all/0/1\">Ahmed Abdulkadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Junhao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1\">Christos Davatzikos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation. (arXiv:2206.07117v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07117","description":"<p>3D hand pose estimation methods have made significant progress recently.\nHowever, estimation accuracy is often far from sufficient for specific\nreal-world applications, and thus there is significant room for improvement.\nThis paper proposes TriHorn-Net, a novel model that uses specific innovations\nto improve hand pose estimation accuracy on depth images. The first innovation\nis the decomposition of the 3D hand pose estimation into the estimation of 2D\njoint locations in the depth image space (UV), and the estimation of their\ncorresponding depths aided by two complementary attention maps. This\ndecomposition prevents depth estimation, which is a more difficult task, from\ninterfering with the UV estimations at both the prediction and feature levels.\nThe second innovation is PixDropout, which is, to the best of our knowledge,\nthe first appearance-based data augmentation method for hand depth images.\nExperimental results demonstrate that the proposed model outperforms the\nstate-of-the-art methods on three public benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1\">Mohammad Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Loss Functions for Classification using Structured Entropy. (arXiv:2206.07122v1 [stat.ML])","link":"http://arxiv.org/abs/2206.07122","description":"<p>Cross-entropy loss is the standard metric used to train classification models\nin deep learning and gradient boosting. It is well-known that this loss\nfunction fails to account for similarities between the different values of the\ntarget. We propose a generalization of entropy called {\\em structured entropy}\nwhich uses a random partition to incorporate the structure of the target\nvariable in a manner which retains many theoretical properties of standard\nentropy. We show that a structured cross-entropy loss yields better results on\nseveral classification problems where the target variable has an a priori known\nstructure. The approach is simple, flexible, easily computable, and does not\nrely on a hierarchically defined notion of structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Lucena_B/0/1/0/all/0/1\">Brian Lucena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Pretraining for Differentially Private Learning. (arXiv:2206.07125v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07125","description":"<p>We demonstrate self-supervised pretraining (SSP) is a scalable solution to\ndeep learning with differential privacy (DP) regardless of the size of\navailable public datasets in image classification. When facing the lack of\npublic datasets, we show the features generated by SSP on only one single image\nenable a private classifier to obtain much better utility than the non-learned\nhandcrafted features under the same privacy budget. When a moderate or large\nsize public dataset is available, the features produced by SSP greatly\noutperform the features trained with labels on various complex private datasets\nunder the same private budget. We also compared multiple DP-enabled training\nframeworks to train a private classifier on the features generated by SSP.\nFinally, we report a non-trivial utility 25.3\\% of a private ImageNet-1K\ndataset when $\\epsilon=3$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asadian_A/0/1/0/all/0/1\">Arash Asadian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weidner_E/0/1/0/all/0/1\">Evan Weidner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lei Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07136","description":"<p>Per-example gradient clipping is a key algorithmic step that enables\npractical differential private (DP) training for deep learning models. The\nchoice of clipping norm $R$, however, is shown to be vital for achieving high\naccuracy under DP. We propose an easy-to-use replacement, called AutoClipping,\nthat eliminates the need to tune $R$ for any DP optimizers, including DP-SGD,\nDP-Adam, DP-LAMB and many others. The automatic variants are as private and\ncomputationally efficient as existing DP optimizers, but require no DP-specific\nhyperparameters and thus make DP training as amenable as the standard\nnon-private training. We give a rigorous convergence analysis of automatic\nDP-SGD in the non-convex setting, which shows that it enjoys an asymptotic\nconvergence rate that matches the standard SGD. We also demonstrate on various\nlanguage and vision tasks that automatic clipping outperforms or matches the\nstate-of-the-art, and can be easily employed with minimal changes to existing\ncodebases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's Time for Artistic Correspondence in Music and Video. (arXiv:2206.07148v1 [cs.MM])","link":"http://arxiv.org/abs/2206.07148","description":"<p>We present an approach for recommending a music track for a given video, and\nvice versa, based on both their temporal alignment and their correspondence at\nan artistic level. We propose a self-supervised approach that learns this\ncorrespondence directly from data, without any need of human annotations. In\norder to capture the high-level concepts that are required to solve the task,\nwe propose modeling the long-term temporal context of both the video and the\nmusic signals, using Transformer networks for each modality. Experiments show\nthat this approach strongly outperforms alternatives that do not exploit the\ntemporal context. The combination of our contributions improve retrieval\naccuracy up to 10x over prior state of the art. This strong improvement allows\nus to introduce a wide range of analyses and applications. For instance, we can\ncondition music retrieval based on visually defined attributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suris_D/0/1/0/all/0/1\">Didac Suris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1\">Bryan Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salamon_J/0/1/0/all/0/1\">Justin Salamon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervision on Images and Text Reduces Reliance on Visual Shortcut Features. (arXiv:2206.07155v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07155","description":"<p>Deep learning models trained in a fully supervised manner have been shown to\nrely on so-called \"shortcut\" features. Shortcut features are inputs that are\nassociated with the outcome of interest in the training data, but are either no\nlonger associated or not present in testing or deployment settings. Here we\nprovide experiments that show recent self-supervised models trained on images\nand text provide more robust image representations and reduce the model's\nreliance on visual shortcut features on a realistic medical imaging example.\nAdditionally, we find that these self-supervised models \"forget\" shortcut\nfeatures more quickly than fully supervised ones when fine-tuned on labeled\ndata. Though not a complete solution, our experiments provide compelling\nevidence that self-supervised models trained on images and text provide some\nresilience to visual shortcut features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1\">Anil Palepu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beam_A/0/1/0/all/0/1\">Andrew L Beam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Multi-organ Segmentation with Partially Labeled Data. (arXiv:2206.07156v1 [eess.IV])","link":"http://arxiv.org/abs/2206.07156","description":"<p>Federated learning is an emerging paradigm allowing large-scale decentralized\nlearning without sharing data across different data owners, which helps address\nthe concern of data privacy in medical image analysis. However, the requirement\nfor label consistency across clients by the existing methods largely narrows\nits application scope. In practice, each clinical site may only annotate\ncertain organs of interest with partial or no overlap with other sites.\nIncorporating such partially labeled data into a unified federation is an\nunexplored problem with clinical significance and urgency. This work tackles\nthe challenge by using a novel federated multi-encoding U-Net (Fed-MENU) method\nfor multi-organ segmentation. In our method, a multi-encoding U-Net (MENU-Net)\nis proposed to extract organ-specific features through different encoding\nsub-networks. Each sub-network can be seen as an expert of a specific organ and\ntrained for that client. Moreover, to encourage the organ-specific features\nextracted by different sub-networks to be informative and distinctive, we\nregularize the training of the MENU-Net by designing an auxiliary generic\ndecoder (AGD). Extensive experiments on four public datasets show that our\nFed-MENU method can effectively obtain a federated learning model using the\npartially labeled datasets with superior performance to other models trained by\neither localized or centralized learning methods. Source code will be made\npublicly available at the time of paper publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xuanang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling. (arXiv:2206.07160v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07160","description":"<p>Unified vision-language frameworks have greatly advanced in recent years,\nmost of which adopt an encoder-decoder architecture to unify image-text tasks\nas sequence-to-sequence generation. However, existing video-language (VidL)\nmodels still require task-specific designs in model architecture and training\nobjectives for each task. In this work, we explore a unified VidL framework\nLAVENDER, where Masked Language Modeling (MLM) is used as the common interface\nfor all pre-training and downstream tasks. Such unification leads to a\nsimplified model architecture, where only a lightweight MLM head, instead of a\ndecoder with much more parameters, is needed on top of the multimodal encoder.\nSurprisingly, experimental results show that this unified framework achieves\ncompetitive performance on 14 VidL benchmarks, covering video question\nanswering, text-to-video retrieval and video captioning. Extensive analyses\nfurther demonstrate the advantage of LAVENDER over existing VidL methods in:\n(i) supporting all downstream tasks with just a single set of parameter values\nwhen multi-task finetuned; (ii) few-shot generalization on various downstream\ntasks; and (iii) enabling zero-shot evaluation on video question answering\ntasks. Code is available at https://github.com/microsoft/LAVENDER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category-Agnostic 6D Pose Estimation with Conditional Neural Processes. (arXiv:2206.07162v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07162","description":"<p>We present a novel meta-learning approach for 6D pose estimation on unknown\nobjects. In contrast to \"instance-level\" pose estimation methods, our algorithm\nlearns object representation in a category-agnostic way, which endows it with\nstrong generalization capabilities within and across object categories.\nSpecifically, we employ a conditional neural process-based meta-learning\napproach to train an encoder to capture texture and geometry of an object in a\nlatent representation, based on very few RGB-D images and ground-truth\nkeypoints. The latent representation is then used by a simultaneously\nmeta-trained decoder to predict the 6D pose of the object in new images. To\nevaluate our algorithm, experiments are conducted on our new fully-annotated\nsynthetic datasets generated from Multiple Categories in Multiple Scenes\n(MCMS). Experimental results demonstrate that our model performs well on unseen\nobjects with various shapes and appearances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yumeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Ning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1\">Hanna Ziesche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepRecon: Joint 2D Cardiac Segmentation and 3D Volume Reconstruction via A Structure-Specific Generative Method. (arXiv:2206.07163v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07163","description":"<p>Joint 2D cardiac segmentation and 3D volume reconstruction are fundamental to\nbuilding statistical cardiac anatomy models and understanding functional\nmechanisms from motion patterns. However, due to the low through-plane\nresolution of cine MR and high inter-subject variance, accurately segmenting\ncardiac images and reconstructing the 3D volume are challenging. In this study,\nwe propose an end-to-end latent-space-based framework, DeepRecon, that\ngenerates multiple clinically essential outcomes, including accurate image\nsegmentation, synthetic high-resolution 3D image, and 3D reconstructed volume.\nOur method identifies the optimal latent representation of the cine image that\ncontains accurate semantic information for cardiac structures. In particular,\nour model jointly generates synthetic images with accurate semantic information\nand segmentation of the cardiac structures using the optimal latent\nrepresentation. We further explore downstream applications of 3D shape\nreconstruction and 4D motion pattern adaptation by the different latent-space\nmanipulation strategies.The simultaneously generated high-resolution images\npresent a high interpretable value to assess the cardiac shape and\nmotion.Experimental results demonstrate the effectiveness of our approach on\nmultiple fronts including 2D segmentation, 3D reconstruction, downstream 4D\nmotion pattern adaption performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhennan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawalha_K/0/1/0/all/0/1\">Khalid Sawalha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Meng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhangli_Q/0/1/0/all/0/1\">Qilong Zhangli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanski_M/0/1/0/all/0/1\">Mikael Kanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aref_S/0/1/0/all/0/1\">Subhi Al Aref</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axel_L/0/1/0/all/0/1\">Leon Axel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated image analysis in large-scale cellular electron microscopy: A literature survey. (arXiv:2206.07171v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07171","description":"<p>Large-scale electron microscopy (EM) datasets generated using (semi-)\nautomated microscopes are becoming the standard in EM. Given the vast amounts\nof data, manual analysis of all data is not feasible, thus automated analysis\nis crucial. The main challenges in automated analysis include the annotation\nthat is needed to analyse and interpret biomedical images, coupled with\nachieving high-throughput. Here, we review the current state-of-the-art of\nautomated computer techniques and major challenges for the analysis of\nstructures in cellular EM. The advanced computer vision, deep learning and\nsoftware tools that have been developed in the last five years for automatic\nbiomedical image analysis are discussed with respect to annotation,\nsegmentation and scalability for EM data. Integration of automatic image\nacquisition and analysis will allow for high-throughput analysis of\nmillimeter-range datasets with nanometer resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aswath_A/0/1/0/all/0/1\">Anusha Aswath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsahaf_A/0/1/0/all/0/1\">Ahmad Alsahaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giepmans_B/0/1/0/all/0/1\">Ben N. G. Giepmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azzopardi_G/0/1/0/all/0/1\">George Azzopardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Representational Harms in Image Captioning. (arXiv:2206.07173v1 [cs.CY])","link":"http://arxiv.org/abs/2206.07173","description":"<p>Previous work has largely considered the fairness of image captioning systems\nthrough the underspecified lens of \"bias.\" In contrast, we present a set of\ntechniques for measuring five types of representational harms, as well as the\nresulting measurements obtained for two of the most popular image captioning\ndatasets using a state-of-the-art image captioning system. Our goal was not to\naudit this image captioning system, but rather to develop normatively grounded\nmeasurement techniques, in turn providing an opportunity to reflect on the many\nchallenges involved. We propose multiple measurement techniques for each type\nof harm. We argue that by doing so, we are better able to capture the\nmulti-faceted nature of each type of harm, in turn improving the (collective)\nvalidity of the resulting measurements. Throughout, we discuss the assumptions\nunderlying our measurement approach and point out when they do not hold.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Angelina Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barocas_S/0/1/0/all/0/1\">Solon Barocas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_K/0/1/0/all/0/1\">Kristen Laird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallach_H/0/1/0/all/0/1\">Hanna Wallach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proximal Splitting Adversarial Attacks for Semantic Segmentation. (arXiv:2206.07179v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07179","description":"<p>Classification has been the focal point of research on adversarial attacks,\nbut only a few works investigate methods suited to denser prediction tasks,\nsuch as semantic segmentation. The methods proposed in these works do not\naccurately solve the adversarial segmentation problem and, therefore, are\noveroptimistic in terms of size of the perturbations required to fool models.\nHere, we propose a white-box attack for these models based on a proximal\nsplitting to produce adversarial perturbations with much smaller $\\ell_1$,\n$\\ell_2$, or $\\ell_\\infty$ norms. Our attack can handle large numbers of\nconstraints within a nonconvex minimization framework via an Augmented\nLagrangian approach, coupled with adaptive constraint scaling and masking\nstrategies. We demonstrate that our attack significantly outperforms previously\nproposed ones, as well as classification attacks that we adapted for\nsegmentation, providing a first comprehensive benchmark for this dense task.\nOur results push current limits concerning robustness evaluations in\nsegmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesquet_J/0/1/0/all/0/1\">Jean-Christophe Pesquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surgical Phase Recognition in Laparoscopic Cholecystectomy. (arXiv:2206.07198v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07198","description":"<p>Automatic recognition of surgical phases in surgical videos is a fundamental\ntask in surgical workflow analysis. In this report, we propose a\nTransformer-based method that utilizes calibrated confidence scores for a\n2-stage inference pipeline, which dynamically switches between a baseline model\nand a separately trained transition model depending on the calibrated\nconfidence level. Our method outperforms the baseline model on the Cholec80\ndataset, and can be applied to a variety of action segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunfan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_V/0/1/0/all/0/1\">Vinayak Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_I/0/1/0/all/0/1\">I.V. Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World. (arXiv:2206.07207v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07207","description":"<p>Understanding how events described or shown in multimedia content relate to\none another is a critical component to developing robust artificially\nintelligent systems which can reason about real-world media. While much\nresearch has been devoted to event understanding in the text, image, and video\ndomains, none have explored the complex relations that events experience across\ndomains. For example, a news article may describe a `protest' event while a\nvideo shows an `arrest' event. Recognizing that the visual `arrest' event is a\nsubevent of the broader `protest' event is a challenging, yet important problem\nthat prior work has not explored. In this paper, we propose the novel task of\nMultiModal Event Event Relations to recognize such cross-modal event relations.\nWe contribute a large-scale dataset consisting of 100k video-news article\npairs, as well as a benchmark of densely annotated data. We also propose a\nweakly supervised multimodal method which integrates commonsense knowledge from\nan external knowledge base (KB) to predict rich multimodal event hierarchies.\nExperiments show that our model outperforms a number of competitive baselines\non our proposed benchmark. We also perform a detailed analysis of our model's\nperformance and suggest directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayyubi_H/0/1/0/all/0/1\">Hammad A. Ayyubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1\">Christopher Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_L/0/1/0/all/0/1\">Lovish Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lokesh_R/0/1/0/all/0/1\">Rahul Lokesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1\">Jaywon Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Sounak Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects. (arXiv:2206.07219v1 [eess.IV])","link":"http://arxiv.org/abs/2206.07219","description":"<p>The recent development of deep learning combined with compressed sensing\nenables fast reconstruction of undersampled MR images and has achieved\nstate-of-the-art performance for Cartesian k-space trajectories. However,\nnon-Cartesian trajectories such as the radial trajectory need to be transformed\nonto a Cartesian grid in each iteration of the network training, slowing down\nthe training process and posing inconvenience and delay during training.\nMultiple iterations of nonuniform Fourier transform in the networks offset the\ndeep learning advantage of fast inference. Current approaches typically either\nwork on image-to-image networks or grid the non-Cartesian trajectories before\nthe network training to avoid the repeated gridding process. However, the\nimage-to-image networks cannot ensure the k-space data consistency in the\nreconstructed images and the pre-processing of non-Cartesian k-space leads to\ngridding errors which cannot be compensated by the network training. Inspired\nby the Transformer network to handle long-range dependencies in sequence\ntransduction tasks, we propose to rearrange the radial spokes to sequential\ndata based on the chronological order of acquisition and use the Transformer to\npredict unacquired radial spokes from acquired ones. We propose novel data\naugmentation methods to generate a large amount of training data from a limited\nnumber of subjects. The network can be generated to different anatomical\nstructures. Experimental results show superior performance of the proposed\nframework compared to state-of-the-art deep neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shih_S/0/1/0/all/0/1\">Shu-Fu Shih</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Finn_J/0/1/0/all/0/1\">J. Paul Finn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_X/0/1/0/all/0/1\">Xiaodong Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07240","description":"<p>Self-supervised pretraining has been able to produce transferable\nrepresentations for various visual document understanding (VDU) tasks. However,\nthe ability of such representations to adapt to new distribution shifts at\ntest-time has not been studied yet. We propose DocTTA, a novel test-time\nadaptation approach for documents that leverages cross-modality self-supervised\nlearning via masked visual language modeling as well as pseudo labeling to\nadapt models learned on a \\textit{source} domain to an unlabeled\n\\textit{target} domain at test time. We also introduce new benchmarks using\nexisting public datasets for various VDU tasks including entity recognition,\nkey-value extraction, and document visual question answering tasks where DocTTA\nimproves the source model performance up to 1.79\\% in (F1 score), 3.43\\% (F1\nscore), and 17.68\\% (ANLS score), respectively while drastically reducing\ncalibration error on target data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1\">Sercan O. Arik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds. (arXiv:2206.07255v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07255","description":"<p>Recent works have shown that 3D-aware GANs trained on unstructured single\nimage collections can generate multiview images of novel instances. The key\nunderpinnings to achieve this are a 3D radiance field generator and a volume\nrendering process. However, existing methods either cannot generate\nhigh-resolution images (e.g., up to 256X256) due to the high computation cost\nof neural volume rendering, or rely on 2D CNNs for image-space upsampling which\njeopardizes the 3D consistency across different views. This paper proposes a\nnovel 3D-aware GAN that can generate high resolution images (up to 1024X1024)\nwhile keeping strict 3D consistency as in volume rendering. Our motivation is\nto achieve super-resolution directly in the 3D space to preserve 3D\nconsistency. We avoid the otherwise prohibitively-expensive computation cost by\napplying 2D convolutions on a set of 2D radiance manifolds defined in the\nrecent generative radiance manifold (GRAM) approach, and apply dedicated loss\nfunctions for effective GAN training at high resolution. Experiments on FFHQ\nand AFHQv2 datasets show that our method can produce high-quality 3D-consistent\nresults that significantly outperform existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jianfeng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yu Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Image Scale and Orientation. (arXiv:2206.07259v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07259","description":"<p>We study the problem of learning to assign a characteristic pose, i.e., scale\nand orientation, for an image region of interest. Despite its apparent\nsimplicity, the problem is non-trivial; it is hard to obtain a large-scale set\nof image regions with explicit pose annotations that a model directly learns\nfrom. To tackle the issue, we propose a self-supervised learning framework with\na histogram alignment technique. It generates pairs of image patches by random\nrescaling/rotating and then train an estimator to predict their\nscale/orientation values so that their relative difference is consistent with\nthe rescaling/rotating used. The estimator learns to predict a non-parametric\nhistogram distribution of scale/orientation without any supervision.\nExperiments show that it significantly outperforms previous methods in\nscale/orientation estimation and also improves image matching and 6 DoF camera\npose estimation by incorporating our patch poses into a matching process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yoonwoo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Enforcing Better Conditioned Meta-Learning for Rapid Few-Shot Adaptation. (arXiv:2206.07260v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07260","description":"<p>Inspired by the concept of preconditioning, we propose a novel method to\nincrease adaptation speed for gradient-based meta-learning methods without\nincurring extra parameters. We demonstrate that recasting the optimization\nproblem to a non-linear least-squares formulation provides a principled way to\nactively enforce a $\\textit{well-conditioned}$ parameter space for\nmeta-learning models based on the concepts of the condition number and local\ncurvature. Our comprehensive evaluations show that the proposed method\nsignificantly outperforms its unconstrained counterpart especially during\ninitial adaptation steps, while achieving comparable or better overall results\non several few-shot classification tasks -- creating the possibility of\ndynamically choosing the number of adaptation steps at inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hiller_M/0/1/0/all/0/1\">Markus Hiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1\">Tom Drummond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Generalization in Few-Shot Classification. (arXiv:2206.07267v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07267","description":"<p>Single image-level annotations only correctly describe an often small subset\nof an image's content, particularly when complex real-world scenes are\ndepicted. While this might be acceptable in many classification scenarios, it\nposes a significant challenge for applications where the set of classes differs\nsignificantly between training and test time. In this paper, we take a closer\nlook at the implications in the context of $\\textit{few-shot learning}$.\nSplitting the input samples into patches and encoding these via the help of\nVision Transformers allows us to establish semantic correspondences between\nlocal regions across images and independent of their respective class. The most\ninformative patch embeddings for the task at hand are then determined as a\nfunction of the support set via online optimization at inference time,\nadditionally providing visual interpretability of `$\\textit{what matters\nmost}$' in the image. We build on recent advances in unsupervised training of\nnetworks via masked image modelling to overcome the lack of fine-grained labels\nand learn the more general statistical structure of the data while avoiding\nnegative image-level annotation influence, $\\textit{aka}$ supervision collapse.\nExperimental results show the competitiveness of our approach, achieving new\nstate-of-the-art results on four popular few-shot classification benchmarks for\n$5$-shot and $1$-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hiller_M/0/1/0/all/0/1\">Markus Hiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rongkai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1\">Tom Drummond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine vision for vial positioning detection toward the safe automation of material synthesis. (arXiv:2206.07272v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07272","description":"<p>Although robot-based automation in chemistry laboratories can accelerate the\nmaterial development process, surveillance-free environments may lead to\ndangerous accidents primarily due to machine control errors. Object detection\ntechniques can play vital roles in addressing these safety issues; however,\nstate-of-the-art detectors, including single-shot detector (SSD) models, suffer\nfrom insufficient accuracy in environments involving complex and noisy scenes.\nWith the aim of improving safety in a surveillance-free laboratory, we report a\nnovel deep learning (DL)-based object detector, namely, DenseSSD. For the\nforemost and frequent problem of detecting vial positions, DenseSSD achieved a\nmean average precision (mAP) over 95% based on a complex dataset involving both\nempty and solution-filled vials, greatly exceeding those of conventional\ndetectors; such high precision is critical to minimizing failure-induced\naccidents. Additionally, DenseSSD was observed to be highly insensitive to the\nenvironmental changes, maintaining its high precision under the variations of\nsolution colors or testing view angles. The robustness of DenseSSD would allow\nthe utilized equipment settings to be more flexible. This work demonstrates\nthat DenseSSD is useful for enhancing safety in an automated material synthesis\nenvironment, and it can be extended to various applications where high\ndetection accuracy and speed are both needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiong_L/0/1/0/all/0/1\">Leslie Ching Ow Tiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Hyuk Jun Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Na Yeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kwan-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sang Soo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNAS: An Evolutionary Neural Architecture Search for Magnetic Resonance Image Reconstructions. (arXiv:2206.07280v1 [eess.IV])","link":"http://arxiv.org/abs/2206.07280","description":"<p>Magnetic resonance imaging (MRI) is one of the noninvasive imaging modalities\nthat can produce high-quality images. However, the scan procedure is relatively\nslow, which causes patient discomfort and motion artifacts in images.\nAccelerating MRI hardware is constrained by physical and physiological\nlimitations. A popular alternative approach to accelerated MRI is to\nundersample the k-space data. While undersampling speeds up the scan procedure,\nit generates artifacts in the images, and advanced reconstruction algorithms\nare needed to produce artifact-free images. Recently deep learning has emerged\nas a promising MRI reconstruction method to address this problem. However,\nstraightforward adoption of the existing deep learning neural network\narchitectures in MRI reconstructions is not usually optimal in terms of\nefficiency and reconstruction quality. In this work, MRI reconstruction from\nundersampled data was carried out using an optimized neural network using a\nnovel evolutionary neural architecture search algorithm. Brain and knee MRI\ndatasets show that the proposed algorithm outperforms manually designed neural\nnetwork-based MR reconstruction models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Eslahi_S/0/1/0/all/0/1\">Samira Vafay Eslahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1\">Jian Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_J/0/1/0/all/0/1\">Jim Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-resolution image display using diffractive decoders. (arXiv:2206.07281v1 [physics.optics])","link":"http://arxiv.org/abs/2206.07281","description":"<p>High-resolution synthesis/projection of images over a large field-of-view\n(FOV) is hindered by the restricted space-bandwidth-product (SBP) of wavefront\nmodulators. We report a deep learning-enabled diffractive display design that\nis based on a jointly-trained pair of an electronic encoder and a diffractive\noptical decoder to synthesize/project super-resolved images using\nlow-resolution wavefront modulators. The digital encoder, composed of a trained\nconvolutional neural network (CNN), rapidly pre-processes the high-resolution\nimages of interest so that their spatial information is encoded into\nlow-resolution (LR) modulation patterns, projected via a low SBP wavefront\nmodulator. The diffractive decoder processes this LR encoded information using\nthin transmissive layers that are structured using deep learning to\nall-optically synthesize and project super-resolved images at its output FOV.\nOur results indicate that this diffractive image display can achieve a\nsuper-resolution factor of ~4, demonstrating a ~16-fold increase in SBP. We\nalso experimentally validate the success of this diffractive super-resolution\ndisplay using 3D-printed diffractive decoders that operate at the THz spectrum.\nThis diffractive image decoder can be scaled to operate at visible wavelengths\nand inspire the design of large FOV and high-resolution displays that are\ncompact, low-power, and computationally efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Isil_C/0/1/0/all/0/1\">Cagatay Isil</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mengu_D/0/1/0/all/0/1\">Deniz Mengu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tabassum_A/0/1/0/all/0/1\">Anika Tabassum</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1\">Jingxi Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1\">Yi Luo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jarrahi_M/0/1/0/all/0/1\">Mona Jarrahi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Eyes Inspired Recurrent Neural Networks are More Robust Against Adversarial Noises. (arXiv:2206.07282v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07282","description":"<p>Compared to human vision, computer vision based on convolutional neural\nnetworks (CNN) are more vulnerable to adversarial noises. This difference is\nlikely attributable to how the eyes sample visual input and how the brain\nprocesses retinal samples through its dorsal and ventral visual pathways, which\nare under-explored for computer vision. Inspired by the brain, we design\nrecurrent neural networks, including an input sampler that mimics the human\nretina, a dorsal network that guides where to look next, and a ventral network\nthat represents the retinal samples. Taking these modules together, the models\nlearn to take multiple glances at an image, attend to a salient part at each\nglance, and accumulate the representation over time to recognize the image. We\ntest such models for their robustness against a varying level of adversarial\nnoises with a special focus on the effect of different input sampling\nstrategies. Our findings suggest that retinal foveation and sampling renders a\nmodel more robust against adversarial noises, and the model may correct itself\nfrom an attack when it is given a longer time to take more glances at an image.\nIn conclusion, robust visual recognition can benefit from the combined use of\nthree brain-inspired mechanisms: retinal transformation, attention guided eye\nmovement, and recurrent processing, as opposed to feedforward-only CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Minkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaokai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Top-k Classification Learning. (arXiv:2206.07290v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07290","description":"<p>The top-k classification accuracy is one of the core metrics in machine\nlearning. Here, k is conventionally a positive integer, such as 1 or 5, leading\nto top-1 or top-5 training objectives. In this work, we relax this assumption\nand optimize the model for multiple k simultaneously instead of using a single\nk. Leveraging recent advances in differentiable sorting and ranking, we propose\na differentiable top-k cross-entropy classification loss. This allows training\nthe network while not only considering the top-1 prediction, but also, e.g.,\nthe top-2 and top-5 predictions. We evaluate the proposed loss function for\nfine-tuning on state-of-the-art architectures, as well as for training from\nscratch. We find that relaxing k does not only produce better top-5 accuracies,\nbut also leads to top-1 accuracy improvements. When fine-tuning publicly\navailable ImageNet models, we achieve a new state-of-the-art for these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1\">Felix Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1\">Christian Borgelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1\">Oliver Deussen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S\\textsuperscript{2}-FPN: Scale-ware Strip Attention Guided Feature Pyramid Network for Real-time Semantic Segmentation. (arXiv:2206.07298v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07298","description":"<p>Modern high-performance semantic segmentation methods employ a heavy backbone\nand dilated convolution to extract the relevant feature. Although extracting\nfeatures with both contextual and semantic information is critical for the\nsegmentation tasks, it brings a memory footprint and high computation cost for\nreal-time applications. This paper presents a new model to achieve a trade-off\nbetween accuracy/speed for real-time road scene semantic segmentation.\nSpecifically, we proposed a lightweight model named Scale-aware Strip Attention\nGuided Feature Pyramid Network (S\\textsuperscript{2}-FPN). Our network consists\nof three main modules: Attention Pyramid Fusion (APF) module, Scale-aware Strip\nAttention Module (SSAM), and Global Feature Upsample (GFU) module. APF adopts\nan attention mechanisms to learn discriminative multi-scale features and help\nclose the semantic gap between different levels. APF uses the scale-aware\nattention to encode global context with vertical stripping operation and models\nthe long-range dependencies, which helps relate pixels with similar semantic\nlabel. In addition, APF employs channel-wise reweighting block (CRB) to\nemphasize the channel features. Finally, the decoder of\nS\\textsuperscript{2}-FPN then adopts GFU, which is used to fuse features from\nAPF and the encoder. Extensive experiments have been conducted on two\nchallenging semantic segmentation benchmarks, which demonstrate that our\napproach achieves better accuracy/speed trade-off with different model\nsettings. The proposed models have achieved a results of 76.2\\%mIoU/87.3FPS,\n77.4\\%mIoU/67FPS, and 77.8\\%mIoU/30.5FPS on Cityscapes dataset, and\n69.6\\%mIoU,71.0\\% mIoU, and 74.2\\% mIoU on Camvid dataset. The code for this\nwork will be made available at \\url{https://github.com/mohamedac29/S2-FPN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elhassan_M/0/1/0/all/0/1\">Mohammed A. M. Elhassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenhui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenxi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munea_T/0/1/0/all/0/1\">Tewodros Legesse Munea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xin Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VCT: A Video Compression Transformer. (arXiv:2206.07307v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07307","description":"<p>We show how transformers can be used to vastly simplify neural video\ncompression. Previous methods have been relying on an increasing number of\narchitectural biases and priors, including motion prediction and warping\noperations, resulting in complex models. Instead, we independently map input\nframes to representations and use a transformer to model their dependencies,\nletting it predict the distribution of future representations given the past.\nThe resulting video compression transformer outperforms previous methods on\nstandard video compression data sets. Experiments on synthetic data show that\nour model learns to handle complex motion patterns such as panning, blurring\nand fading purely from data. Our approach is easy to implement, and we release\ncode to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mentzer_F/0/1/0/all/0/1\">Fabian Mentzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toderici_G/0/1/0/all/0/1\">George Toderici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minnen_D/0/1/0/all/0/1\">David Minnen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung-Jin Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caelles_S/0/1/0/all/0/1\">Sergi Caelles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agustsson_E/0/1/0/all/0/1\">Eirikur Agustsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in Scene Image Representation and Classification. (arXiv:2206.07326v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07326","description":"<p>With the rise of deep learning algorithms nowadays, scene image\nrepresentation methods on big data (e.g., SUN-397) have achieved a significant\nperformance boost in classification. However, the performance is still limited\nbecause the scene images are mostly complex in nature having higher intra-class\ndissimilarity and inter-class similarity problems. To deal with such problems,\nthere are several methods proposed in the literature with their own advantages\nand limitations. A detailed study of previous works is necessary to understand\ntheir pros and cons in image representation and classification. In this paper,\nwe review the existing scene image representation methods that are being used\nwidely for image classification. For this, we, first, devise the taxonomy using\nthe seminal existing methods proposed in the literature to this date. Next, we\ncompare their performance both qualitatively (e.g., quality of outputs,\npros/cons, etc.) and quantitatively (e.g., accuracy). Last, we speculate the\nprominent research directions in scene image representation tasks. Overall,\nthis survey provides in-depth insights and applications of recent scene image\nrepresentation methods for traditional Computer Vision (CV)-based methods, Deep\nLearning (DL)-based methods, and Search Engine (SE)-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitaula_C/0/1/0/all/0/1\">Chiranjibi Sitaula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_T/0/1/0/all/0/1\">Tej Bahadur Shahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzbanrad_F/0/1/0/all/0/1\">Faezeh Marzbanrad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection of Rice Disease in Images of Various Leaf Sizes. (arXiv:2206.07344v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07344","description":"<p>Fast, accurate and affordable rice disease detection method is required to\nassist rice farmers tackling equipment and expertise shortages problems. In\nthis paper, we focused on the solution using computer vision technique to\ndetect rice diseases from rice field photograph images. Dealing with images\ntook in real-usage situation by general farmers is quite challenging due to\nvarious environmental factors, and rice leaf object size variation is one major\nfactor caused performance gradation. To solve this problem, we presented a\ntechnique combining a CNN object detection with image tiling technique, based\non automatically estimated width size of rice leaves in the images as a size\nreference for dividing the original input image. A model to estimate leaf width\nwas created by small size CNN such as 18 layer ResNet architecture model. A new\ndivided tiled sub-image set with uniformly sized object was generated and used\nas input for training a rice disease prediction model. Our technique was\nevaluated on 4,960 images of eight different types of rice leaf diseases,\nincluding blast, blight, brown spot, narrow brown spot, orange, red stripe,\nrice grassy stunt virus, and streak disease. The mean absolute percentage error\n(MAPE) for leaf width prediction task evaluated on all eight classes was 11.18%\nin the experiment, indicating that the leaf width prediction model performed\nwell. The mean average precision (mAP) of the prediction performance on YOLOv4\narchitecture was enhanced from 87.56% to 91.14% when trained and tested with\nthe tiled dataset. According to our study, the proposed image tiling technique\nimproved rice disease detection efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiratiratanapruk_K/0/1/0/all/0/1\">Kantip Kiratiratanapruk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temniranrat_P/0/1/0/all/0/1\">Pitchayagan Temniranrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinthupinyo_W/0/1/0/all/0/1\">Wasin Sinthupinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marukatat_S/0/1/0/all/0/1\">Sanparith Marukatat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patarapuwadol_S/0/1/0/all/0/1\">Sujin Patarapuwadol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Capsule Networks of High-Dimension Point Clouds classification. (arXiv:2206.07348v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07348","description":"<p>Three-dimensional point clouds learning is widely applied, but the point\nclouds are still unable to deal with classification and recognition tasks\nsatisfactorily in the cases of irregular geometric structures and\nhigh-dimensional space. In 3D space, point clouds tend to have regular\nEuclidean structure because of their density. On the contrary, due to the high\ndimensionality, the spatial structure of high-dimensional space is more\ncomplex, and point clouds are mostly presented in non-European structure.\nFurthermore, among current 3D point clouds classification algorithms, Canonical\nCapsules algorithm based on Euclidean distance is difficult to decompose and\nidentify non-Euclidean structures effectively. Thus, aiming at the point clouds\nclassification task of non-Euclidean structure in 3D and high-dimensional\nspace, this paper refers to the LLE algorithm based on geodesic distance for\noptimizing and proposes the unsupervised algorithm of high-dimensional point\nclouds capsule. In this paper, the geometric features of point clouds are\nconsidered in the extraction process, so as to transform the high-dimensional\nnon-Euclidean structure into a lower-dimensional Euclidean structure with\nretaining spatial geometric features. To verify the feasibility of the\nunsupervised algorithm of high-dimensional point clouds capsule, experiments\nare conducted in Swiss Roll dataset, point clouds MNIST dataset and point\nclouds LFW dataset. The results show that (1) non-Euclidean structures can be\ncan effectively identified by this model in Swiss Roll dataset; (2) a\nsignificant unsupervised learning effect is realized in point clouds MNIST\ndataset. In conclusion, the high-dimensional point clouds capsule unsupervised\nalgorithm proposed in this paper is conducive to expand the application\nscenarios of current point clouds classification and recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Quanfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Y/0/1/0/all/0/1\">Yumei She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zuo Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention. (arXiv:2206.07349v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07349","description":"<p>An effective backbone network is important to deep learning-based Deformable\nMedical Image Registration (DMIR), because it extracts and matches the features\nbetween two images to discover the mutual correspondence for fine registration.\nHowever, the existing deep networks focus on single image situation and are\nlimited in registration task which is performed on paired images. Therefore, we\nadvance a novel backbone network, XMorpher, for the effective corresponding\nfeature representation in DMIR. 1) It proposes a novel full transformer\narchitecture including dual parallel feature extraction networks which exchange\ninformation through cross attention, thus discovering multi-level semantic\ncorrespondence while extracting respective features gradually for final\neffective registration. 2) It advances the Cross Attention Transformer (CAT)\nblocks to establish the attention mechanism between images which is able to\nfind the correspondence automatically and prompts the features to fuse\nefficiently in the network. 3) It constrains the attention computation between\nbase windows and searching windows with different sizes, and thus focuses on\nthe local transformation of deformable registration and enhances the computing\nefficiency at the same time. Without any bells and whistles, our XMorpher gives\nVoxelmorph 2.8% improvement on DSC , demonstrating its effective representation\nof the features from the paired images in DMIR. We believe that our XMorpher\nhas great application potential in more paired medical images. Our XMorpher is\nopen on https://github.com/Solemoon/XMorpher\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiacheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuting He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Youyong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coatrieux_J/0/1/0/all/0/1\">Jean-Louis Coatrieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Huazhong Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust SAR ATR on MSTAR with Deep Learning Models trained on Full Synthetic MOCEM data. (arXiv:2206.07352v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07352","description":"<p>The promising potential of Deep Learning for Automatic Target Recognition\n(ATR) on Synthetic Aperture Radar (SAR) images vanishes when considering the\ncomplexity of collecting training datasets measurements. Simulation can\novercome this issue by producing synthetic training datasets. However, because\nof the limited representativeness of simulation, models trained in a classical\nway with synthetic images have limited generalization abilities when dealing\nwith real measurement at test time. Previous works identified a set of equally\npromising deep-learning algorithms to tackle this issue. However, these\napproaches have been evaluated in a very favorable scenario with a synthetic\ntraining dataset that overfits the ground truth of the measured test data. In\nthis work, we study the ATR problem outside of this ideal condition, which is\nunlikely to occur in real operational contexts. Our contribution is threefold.\n(1) Using the MOCEM simulator (developed by SCALIAN DS for the French MoD/DGA),\nwe produce a synthetic MSTAR training dataset that differs significantly from\nthe real measurements. (2) We experimentally demonstrate the limits of the\nstate-of-the-art. (3) We show that domain randomization techniques and\nadversarial training can be combined to overcome this issue. We demonstrate\nthat this approach is more robust than the state-of-the-art, with an accuracy\nof 75 %, while having a limited impact on computing performance during\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camus_B/0/1/0/all/0/1\">Benjamin Camus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbu_C/0/1/0/all/0/1\">Corentin Le Barbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteux_E/0/1/0/all/0/1\">Eric Monteux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking Common Ground While Reserving Differences: Multiple Anatomy Collaborative Framework for Undersampled MRI Reconstruction. (arXiv:2206.07364v1 [eess.IV])","link":"http://arxiv.org/abs/2206.07364","description":"<p>Recently, deep neural networks have greatly advanced undersampled Magnetic\nResonance Image (MRI) reconstruction, wherein most studies follow the\none-anatomy-one-network fashion, i.e., each expert network is trained and\nevaluated for a specific anatomy. Apart from inefficiency in training multiple\nindependent models, such convention ignores the shared de-aliasing knowledge\nacross various anatomies which can benefit each other. To explore the shared\nknowledge, one naive way is to combine all the data from various anatomies to\ntrain an all-round network. Unfortunately, despite the existence of the shared\nde-aliasing knowledge, we reveal that the exclusive knowledge across different\nanatomies can deteriorate specific reconstruction targets, yielding overall\nperformance degradation. Observing this, in this study, we present a novel deep\nMRI reconstruction framework with both anatomy-shared and anatomy-specific\nparameterized learners, aiming to \"seek common ground while reserving\ndifferences\" across different anatomies.Particularly, the primary\nanatomy-shared learners are exposed to different anatomies to model flourishing\nshared knowledge, while the efficient anatomy-specific learners are trained\nwith their target anatomy for exclusive knowledge. Four different\nimplementations of anatomy-specific learners are presented and explored on the\ntop of our framework in two MRI reconstruction networks. Comprehensive\nexperiments on brain, knee and cardiac MRI datasets demonstrate that three of\nthese learners are able to enhance reconstruction performance via multiple\nanatomy collaborative learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiangpeng_Y/0/1/0/all/0/1\">Yan Jiangpeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chenghui_Y/0/1/0/all/0/1\">Yu Chenghui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hanbo_C/0/1/0/all/0/1\">Chen Hanbo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhe_X/0/1/0/all/0/1\">Xu Zhe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junzhou_H/0/1/0/all/0/1\">Huang Junzhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiu_L/0/1/0/all/0/1\">Li Xiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jianhua_Y/0/1/0/all/0/1\">Yao Jianhua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoGround: Detecting Monocular 3D Objects from the Ground. (arXiv:2206.07372v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07372","description":"<p>Monocular 3D object detection has attracted great attention for its\nadvantages in simplicity and cost. Due to the ill-posed 2D to 3D mapping\nessence from the monocular imaging process, monocular 3D object detection\nsuffers from inaccurate depth estimation and thus has poor 3D detection\nresults. To alleviate this problem, we propose to introduce the ground plane as\na prior in the monocular 3d object detection. The ground plane prior serves as\nan additional geometric condition to the ill-posed mapping and an extra source\nin depth estimation. In this way, we can get a more accurate depth estimation\nfrom the ground. Meanwhile, to take full advantage of the ground plane prior,\nwe propose a depth-align training strategy and a precise two-stage depth\ninference method tailored for the ground plane prior. It is worth noting that\nthe introduced ground plane prior requires no extra data sources like LiDAR,\nstereo images, and depth information. Extensive experiments on the KITTI\nbenchmark show that our method could achieve state-of-the-art results compared\nwith other methods while maintaining a very fast speed. Our code and models are\navailable at https://github.com/cfzd/MonoGround.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zequn Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Manifold Hypothesis for Gradient-Based Explanations. (arXiv:2206.07387v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07387","description":"<p>When do gradient-based explanation algorithms provide meaningful\nexplanations? We propose a necessary criterion: their feature attributions need\nto be aligned with the tangent space of the data manifold. To provide evidence\nfor this hypothesis, we introduce a framework based on variational autoencoders\nthat allows to estimate and generate image manifolds. Through experiments\nacross a range of different datasets -- MNIST, EMNIST, CIFAR10, X-ray pneumonia\nand Diabetic Retinopathy detection -- we demonstrate that the more a feature\nattribution is aligned with the tangent space of the data, the more structured\nand explanatory it tends to be. In particular, the attributions provided by\npopular post-hoc methods such as Integrated Gradients, SmoothGrad and Input\n$\\times$ Gradient tend to be more strongly aligned with the data manifold than\nthe raw gradient. As a consequence, we suggest that explanation algorithms\nshould actively strive to align their explanations with the data manifold. In\npart, this can be achieved by adversarial training, which leads to better\nalignment across all datasets. Some form of adjustment to the model\narchitecture or training algorithm is necessary, since we show that\ngeneralization of neural networks alone does not imply the alignment of model\ngradients with the data manifold.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bordt_S/0/1/0/all/0/1\">Sebastian Bordt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luxburg_U/0/1/0/all/0/1\">Ulrike von Luxburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subsurface Depths Structure Maps Reconstruction with Generative Adversarial Networks. (arXiv:2206.07388v1 [physics.geo-ph])","link":"http://arxiv.org/abs/2206.07388","description":"<p>This paper described a method for reconstruction of detailed-resolution depth\nstructure maps, usually obtained after the 3D seismic surveys, using the data\nfrom 2D seismic depth maps. The method uses two algorithms based on the\ngenerative-adversarial neural network architecture. The first algorithm\nStyleGAN2-ADA accumulates in the hidden space of the neural network the\nsemantic images of mountainous terrain forms first, and then with help of\ntransfer learning, in the ideal case - the structure geometry of stratigraphic\nhorizons. The second algorithm, the Pixel2Style2Pixel encoder, using the\nsemantic level of generalization of the first algorithm, learns to reconstruct\nthe original high-resolution images from their degraded copies\n(super-resolution technology). There was demonstrated a methodological approach\nto transferring knowledge on the structural forms of stratigraphic horizon\nboundaries from the well-studied areas to the underexplored ones. Using the\nmultimodal synthesis of Pixel2Style2Pixel encoder, it is proposed to create a\nprobabilistic depth space, where each point of the project area is represented\nby the density of probabilistic depth distribution of equally probable\nreconstructed geological forms of structural images. Assessment of the\nreconstruction quality was carried out for two blocks. Using this method,\ncredible detailed depth reconstructions comparable with the quality of 3D\nseismic maps have been obtained from 2D seismic maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Ivlev_D/0/1/0/all/0/1\">Dmitry Ivlev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultra Fast Deep Lane Detection with Hybrid Anchor Driven Ordinal Classification. (arXiv:2206.07389v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07389","description":"<p>Modern methods mainly regard lane detection as a problem of pixel-wise\nsegmentation, which is struggling to address the problems of efficiency and\nchallenging scenarios like severe occlusions and extreme lighting conditions.\nInspired by human perception, the recognition of lanes under severe occlusions\nand extreme lighting conditions is mainly based on contextual and global\ninformation. Motivated by this observation, we propose a novel, simple, yet\neffective formulation aiming at ultra fast speed and the problem of challenging\nscenarios. Specifically, we treat the process of lane detection as an\nanchor-driven ordinal classification problem using global features. First, we\nrepresent lanes with sparse coordinates on a series of hybrid (row and column)\nanchors. With the help of the anchor-driven representation, we then reformulate\nthe lane detection task as an ordinal classification problem to get the\ncoordinates of lanes. Our method could significantly reduce the computational\ncost with the anchor-driven representation. Using the large receptive field\nproperty of the ordinal classification formulation, we could also handle\nchallenging scenarios. Extensive experiments on four lane detection datasets\nshow that our method could achieve state-of-the-art performance in terms of\nboth speed and accuracy. A lightweight version could even achieve 300+ frames\nper second(FPS). Our code is at\nhttps://github.com/cfzd/Ultra-Fast-Lane-Detection-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zequn Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Adaptive Ensembling for Image Classification. (arXiv:2206.07394v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07394","description":"<p>In recent times, except for sporadic cases, the trend in Computer Vision is\nto achieve minor improvements over considerable increases in complexity.\n</p>\n<p>To reverse this tendency, we propose a novel method to boost image\nclassification performances without an increase in complexity.\n</p>\n<p>To this end, we revisited ensembling, a powerful approach, not often\nadequately used due to its nature of increased complexity and training time,\nmaking it viable by specific design choices. First, we trained end-to-end two\nEfficientNet-b0 models (known to be the architecture with the best overall\naccuracy/complexity trade-off in image classification) on disjoint subsets of\ndata (i.e. bagging). Then, we made an efficient adaptive ensemble by performing\nfine-tuning of a trainable combination layer. In this way, we were able to\noutperform the state-of-the-art by an average of 0.5\\% on the accuracy with\nrestrained complexity both in terms of number of parameters (by 5-60 times),\nand FLoating point Operations Per Second (by 10-100 times) on several major\nbenchmark datasets, fully embracing the green AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruno_A/0/1/0/all/0/1\">Antonio Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moroni_D/0/1/0/all/0/1\">Davide Moroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinelli_M/0/1/0/all/0/1\">Massimo Martinelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable differential diagnosis for Alzheimer's disease and Frontotemporal dementia. (arXiv:2206.07417v1 [eess.IV])","link":"http://arxiv.org/abs/2206.07417","description":"<p>Alzheimer's disease and Frontotemporal dementia are two major types of\ndementia. Their accurate diagnosis and differentiation is crucial for\ndetermining specific intervention and treatment. However, differential\ndiagnosis of these two types of dementia remains difficult at the early stage\nof disease due to similar patterns of clinical symptoms. Therefore, the\nautomatic classification of multiple types of dementia has an important\nclinical value. So far, this challenge has not been actively explored. Recent\ndevelopment of deep learning in the field of medical image has demonstrated\nhigh performance for various classification tasks. In this paper, we propose to\ntake advantage of two types of biomarkers: structure grading and structure\natrophy. To this end, we propose first to train a large ensemble of 3D U-Nets\nto locally discriminate healthy versus dementia anatomical patterns. The result\nof these models is an interpretable 3D grading map capable of indicating\nabnormal brain regions. This map can also be exploited in various\nclassification tasks using graph convolutional neural network. Finally, we\npropose to combine deep grading and atrophy-based classifications to improve\ndementia type discrimination. The proposed framework showed competitive\nperformance compared to state-of-the-art methods for different tasks of disease\ndetection and differential diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy-Dung Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clement_M/0/1/0/all/0/1\">Micha&#xeb;l Cl&#xe9;ment</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansencal_B/0/1/0/all/0/1\">Boris Mansencal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coupe_P/0/1/0/all/0/1\">Pierrick Coup&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Network Pruning for Nuclei Instance Segmentation in Hematoxylin & Eosin-Stained Histological Images. (arXiv:2206.07422v1 [eess.IV])","link":"http://arxiv.org/abs/2206.07422","description":"<p>Recently, pruning deep neural networks (DNNs) has received a lot of attention\nfor improving accuracy and generalization power, reducing network size, and\nincreasing inference speed on specialized hardwares. Although pruning was\nmainly tested on computer vision tasks, its application in the context of\nmedical image analysis has hardly been explored. This work investigates the\nimpact of well-known pruning techniques, namely layer-wise and network-wide\nmagnitude pruning, on the nuclei instance segmentation performance in\nhistological images. Our utilized instance segmentation model consists of two\nmain branches: (1) a semantic segmentation branch, and (2) a deep regression\nbranch. We investigate the impact of weight pruning on the performance of both\nbranches separately and on the final nuclei instance segmentation result.\nEvaluated on two publicly available datasets, our results show that layer-wise\npruning delivers slightly better performance than networkwide pruning for small\ncompression ratios (CRs) while for large CRs, network-wide pruning yields\nsuperior performance. For semantic segmentation, deep regression and final\ninstance segmentation, 93.75 %, 95 %, and 80 % of the model weights can be\npruned by layer-wise pruning with less than 2 % reduction in the performance of\nrespective models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mahbod_A/0/1/0/all/0/1\">Amirreza Mahbod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Entezari_R/0/1/0/all/0/1\">Rahim Entezari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ellinger_I/0/1/0/all/0/1\">Isabella Ellinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saukh_O/0/1/0/all/0/1\">Olga Saukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot object goal visual navigation. (arXiv:2206.07423v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07423","description":"<p>Object goal visual navigation is a challenging task that aims to guide a\nrobot to find the target object only based on its visual observation, and the\ntarget is limited to the classes specified in the training stage. However, in\nreal households, there may exist numerous object classes that the robot needs\nto deal with, and it is hard for all of these classes to be contained in the\ntraining stage. To address this challenge, we propose a zero-shot object\nnavigation task by combining zero-shot learning with object goal visual\nnavigation, which aims at guiding robots to find objects belonging to novel\nclasses without any training samples. This task gives rise to the need to\ngeneralize the learned policy to novel classes, which is a less addressed issue\nof object navigation using deep reinforcement learning. To address this issue,\nwe utilize \"class-unrelated\" data as input to alleviate the overfitting of the\nclasses specified in the training stage. The class-unrelated input consists of\ndetection results and cosine similarity of word embeddings, and does not\ncontain any class-related visual features or knowledge graphs. Extensive\nexperiments on the AI2-THOR platform show that our model outperforms the\nbaseline models in both seen and unseen classes, which proves that our model is\nless class-sensitive and generalizes better. Our code is available at\nhttps://github.com/pioneer-innovation/Zero-Shot-Object-Navigation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qianfan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1\">Hong Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physically-admissible polarimetric data augmentation for road-scene analysis. (arXiv:2206.07431v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07431","description":"<p>Polarimetric imaging, along with deep learning, has shown improved\nperformances on different tasks including scene analysis. However, its\nrobustness may be questioned because of the small size of the training\ndatasets. Though the issue could be solved by data augmentation, polarization\nmodalities are subject to physical feasibility constraints unaddressed by\nclassical data augmentation techniques. To address this issue, we propose to\nuse CycleGAN, an image translation technique based on deep generative models\nthat solely relies on unpaired data, to transfer large labeled road scene\ndatasets to the polarimetric domain. We design several auxiliary loss terms\nthat, alongside the CycleGAN losses, deal with the physical constraints of\npolarimetric images. The efficiency of this solution is demonstrated on road\nscene object detection tasks where generated realistic polarimetric images\nallow to improve performances on cars and pedestrian detection up to 9%. The\nresulting constrained CycleGAN is publicly released, allowing anyone to\ngenerate their own polarimetric images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruffino_C/0/1/0/all/0/1\">Cyprien Ruffino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blin_R/0/1/0/all/0/1\">Rachel Blin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainouz_S/0/1/0/all/0/1\">Samia Ainouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1\">Gilles Gasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herault_R/0/1/0/all/0/1\">Romain H&#xe9;rault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meriaudeau_F/0/1/0/all/0/1\">Fabrice Meriaudeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canu_S/0/1/0/all/0/1\">St&#xe9;phane Canu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Implicit Attention: Guided Attention by The Model Itself. (arXiv:2206.07434v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07434","description":"<p>We propose Self-Supervised Implicit Attention (SSIA), a new approach that\nadaptively guides deep neural network models to gain attention by exploiting\nthe properties of the models themselves. SSIA is a novel attention mechanism\nthat does not require any extra parameters, computation, or memory access costs\nduring inference, which is in contrast to existing attention mechanism. In\nshort, by considering attention weights as higher-level semantic information,\nwe reconsidered the implementation of existing attention mechanisms and further\npropose generating supervisory signals from higher network layers to guide\nlower network layers for parameter updates. We achieved this by building a\nself-supervised learning task using the hierarchical features of the network\nitself, which only works at the training stage. To verify the effectiveness of\nSSIA, we performed a particular implementation (called an SSIA block) in\nconvolutional neural network models and validated it on several image\nclassification datasets. The experimental results show that an SSIA block can\nsignificantly improve the model performance, even outperforms many popular\nattention methods that require additional parameters and computation costs,\nsuch as Squeeze-and-Excitation and Convolutional Block Attention Module. Our\nimplementation will be available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhemin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting of depth and ego-motion with transformers and self-supervision. (arXiv:2206.07435v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07435","description":"<p>This paper addresses the problem of end-to-end self-supervised forecasting of\ndepth and ego motion. Given a sequence of raw images, the aim is to forecast\nboth the geometry and ego-motion using a self supervised photometric loss. The\narchitecture is designed using both convolution and transformer modules. This\nleverages the benefits of both modules: Inductive bias of CNN, and the\nmulti-head attention of transformers, thus enabling a rich spatio-temporal\nrepresentation that enables accurate depth forecasting. Prior work attempts to\nsolve this problem using multi-modal input/output with supervised ground-truth\ndata which is not practical since a large annotated dataset is required.\nAlternatively to prior methods, this paper forecasts depth and ego motion using\nonly self-supervised raw images as input. The approach performs significantly\nwell on the KITTI dataset benchmark with several performance criteria being\neven comparable to prior non-forecasting self-supervised monocular depth\ninference methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boulahbal_H/0/1/0/all/0/1\">Houssem Boulahbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voicila_A/0/1/0/all/0/1\">Adrian Voicila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1\">Andrew Comport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection. (arXiv:2206.07458v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07458","description":"<p>The goal of this work is to reconstruct speech from a silent talking face\nvideo. Recent studies have shown impressive performance on synthesizing speech\nfrom silent talking face videos. However, they have not explicitly considered\non varying identity characteristics of different speakers, which place a\nchallenge in the video-to-speech synthesis, and this becomes more critical in\nunseen-speaker settings. Distinct from the previous methods, our approach is to\nseparate the speech content and the visage-style from a given silent talking\nface video. By guiding the model to independently focus on modeling the two\nrepresentations, we can obtain the speech of high intelligibility from the\nmodel even when the input video of an unseen subject is given. To this end, we\nintroduce speech-visage selection module that separates the speech content and\nthe speaker identity from the visual features of the input video. The\ndisentangled representations are jointly incorporated to synthesize speech\nthrough visage-style based synthesizer which generates speech by coating the\nvisage-styles while maintaining the speech content. Thus, the proposed\nframework brings the advantage of synthesizing the speech containing the right\ncontent even when the silent talking face video of an unseen subject is given.\nWe validate the effectiveness of the proposed framework on the GRID, TCD-TIMIT\nvolunteer, and LRW datasets. The synthesized speech can be heard in\nsupplementary materials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joanna Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Yong Man Ro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"READ: Aggregating Reconstruction Error into Out-of-distribution Detection. (arXiv:2206.07459v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07459","description":"<p>Detecting out-of-distribution (OOD) samples is crucial to the safe deployment\nof a classifier in the real world. However, deep neural networks are known to\nbe overconfident for abnormal data. Existing works directly design score\nfunction by mining the inconsistency from classifier for in-distribution (ID)\nand OOD. In this paper, we further complement this inconsistency with\nreconstruction error, based on the assumption that an autoencoder trained on ID\ndata can not reconstruct OOD as well as ID. We propose a novel method, READ\n(Reconstruction Error Aggregated Detector), to unify inconsistencies from\nclassifier and autoencoder. Specifically, the reconstruction error of raw\npixels is transformed to latent space of classifier. We show that the\ntransformed reconstruction error bridges the semantic gap and inherits\ndetection performance from the original. Moreover, we propose an adjustment\nstrategy to alleviate the overconfidence problem of autoencoder according to a\nfine-grained characterization of OOD data. Under two scenarios of pre-training\nand retraining, we respectively present two variants of our method, namely\nREAD-MD (Mahalanobis Distance) only based on pre-trained classifier and READ-ED\n(Euclidean Distance) which retrains the classifier. Our methods do not require\naccess to test time OOD data for fine-tuning hyperparameters. Finally, we\ndemonstrate the effectiveness of the proposed methods through extensive\ncomparisons with state-of-the-art OOD detection algorithms. On a CIFAR-10\npre-trained WideResNet, our method reduces the average FPR@95TPR by up to 9.8%\ncompared with previous state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shuai Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yuxin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-fine Deep Video Coding with Hyperprior-guided Mode Prediction. (arXiv:2206.07460v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07460","description":"<p>The previous deep video compression approaches only use the single scale\nmotion compensation strategy and rarely adopt the mode prediction technique\nfrom the traditional standards like H.264/H.265 for both motion and residual\ncompression. In this work, we first propose a coarse-to-fine (C2F) deep video\ncompression framework for better motion compensation, in which we perform\nmotion estimation, compression and compensation twice in a coarse to fine\nmanner. Our C2F framework can achieve better motion compensation results\nwithout significantly increasing bit costs. Observing hyperprior information\n(i.e., the mean and variance values) from the hyperprior networks contains\ndiscriminant statistical information of different patches, we also propose two\nefficient hyperprior-guided mode prediction methods. Specifically, using\nhyperprior information as the input, we propose two mode prediction networks to\nrespectively predict the optimal block resolutions for better motion coding and\ndecide whether to skip residual information from each block for better residual\ncoding without introducing additional bit cost while bringing negligible extra\ncomputation cost. Comprehensive experimental results demonstrate our proposed\nC2F video compression framework equipped with the new hyperprior-guided mode\nprediction methods achieves the state-of-the-art performance on HEVC, UVG and\nMCL-JCV datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhihao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyU-BPCoMa: A Dataset and Benchmark Towards Mobile Colorized Mapping Using a Backpack Multisensorial System. (arXiv:2206.07468v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07468","description":"<p>Constructing colorized point clouds from mobile laser scanning and images is\na fundamental work in surveying and mapping. It is also an essential\nprerequisite for building digital twins for smart cities. However, existing\npublic datasets are either in relatively small scales or lack accurate\ngeometrical and color ground truth. This paper documents a multisensorial\ndataset named PolyU-BPCoMA which is distinctively positioned towards mobile\ncolorized mapping. The dataset incorporates resources of 3D LiDAR, spherical\nimaging, GNSS and IMU on a backpack platform. Color checker boards are pasted\nin each surveyed area as targets and ground truth data are collected by an\nadvanced terrestrial laser scanner (TLS). 3D geometrical and color information\ncan be recovered in the colorized point clouds produced by the backpack system\nand the TLS, respectively. Accordingly, we provide an opportunity to benchmark\nthe mapping and colorization accuracy simultaneously for a mobile\nmultisensorial system. The dataset is approximately 800 GB in size covering\nboth indoor and outdoor environments. The dataset and development kits are\navailable at https://github.com/chenpengxin/PolyU-BPCoMa.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenzhong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Muyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Sheng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1\">Haodong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Daping Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Detection Methods for Die Attachment and Wire Bonding Defects in Integrated Circuit Manufacturing. (arXiv:2206.07481v1 [eess.SP])","link":"http://arxiv.org/abs/2206.07481","description":"<p>Defect detection plays a vital role in the manufacturing process of\nintegrated circuits (ICs). Die attachment and wire bonding are two steps of the\nmanufacturing process that determine the quality and reliability of the power\nand signal transmission in an IC. This paper presents a survey or literature\nreview of the methods used for detecting these defects based on different\nsensing modalities used including optical, radiological, acoustical, and\ninfrared thermography. A discussion of the detection methods used is provided\nin this survey. Both conventional and deep learning approaches for detecting\ndie attachment and wire bonding defects are considered along with challenges\nand future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alam_L/0/1/0/all/0/1\">Lamia Alam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kehtarnavaz_N/0/1/0/all/0/1\">Nasser Kehtarnavaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Multi-Task Networks For Occluded Pedestrian Pose Estimation. (arXiv:2206.07510v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07510","description":"<p>Most of the existing works on pedestrian pose estimation do not consider\nestimating the pose of an occluded pedestrians, as the annotations of the\noccluded parts are not available in relevant automotive datasets. For example,\nCityPersons, a well-known dataset for pedestrian detection in automotive scenes\ndoes not provide pose annotations, whereas MS-COCO, a non-automotive dataset,\ncontains human pose estimation. In this work, we propose a multi-task framework\nto extract pedestrian features through detection and instance segmentation\ntasks performed separately on these two distributions. Thereafter, an encoder\nlearns pose specific features using an unsupervised instance-level domain\nadaptation method for the pedestrian instances from both distributions. The\nproposed framework has improved state-of-the-art performances of pose\nestimation, pedestrian detection, and instance segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Arindam Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sudip Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horgan_J/0/1/0/all/0/1\">Jonathan Horgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Ujjwal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Edward Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1\">Martin Glavin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1\">Ciar&#xe1;n Eising</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Body Gesture Recognition to Control a Social Robot. (arXiv:2206.07538v1 [cs.RO])","link":"http://arxiv.org/abs/2206.07538","description":"<p>In this work, we propose a gesture based language to allow humans to interact\nwith robots using their body in a natural way. We have created a new gesture\ndetection model using neural networks and a custom dataset of humans performing\na set of body gestures to train our network. Furthermore, we compare body\ngesture communication with other communication channels to acknowledge the\nimportance of adding this knowledge to robots. The presented approach is\nextensively validated in diverse simulations and real-life experiments with\nnon-trained volunteers. This attains remarkable results and shows that it is a\nvaluable framework for social robotics applications, such as human robot\ncollaboration or human-robot interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laplaza_J/0/1/0/all/0/1\">Javier Laplaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliver_J/0/1/0/all/0/1\">Joan Jaume Oliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_R/0/1/0/all/0/1\">Ram&#xf3;n Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanfeliu_A/0/1/0/all/0/1\">Alberto Sanfeliu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrell_A/0/1/0/all/0/1\">Ana&#xed;s Garrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Generative Model of Neonatal Cortical Surface Development. (arXiv:2206.07542v1 [q-bio.NC])","link":"http://arxiv.org/abs/2206.07542","description":"<p>The neonatal cortical surface is known to be affected by preterm birth, and\nthe subsequent changes to cortical organisation have been associated with\npoorer neurodevelopmental outcomes. Deep Generative models have the potential\nto lead to clinically interpretable models of disease, but developing these on\nthe cortical surface is challenging since established techniques for learning\nconvolutional filters are inappropriate on non-flat topologies. To close this\ngap, we implement a surface-based CycleGAN using mixture model CNNs (MoNet) to\ntranslate sphericalised neonatal cortical surface features (curvature and\nT1w/T2w cortical myelin) between different stages of cortical maturity. Results\nshow our method is able to reliably predict changes in individual patterns of\ncortical organisation at later stages of gestation, validated by comparison to\nlongitudinal data; and translate appearance between preterm and term gestation\n(&gt; 37 weeks gestation), validated through comparison with a trained\nterm/preterm classifier. Simulated differences in cortical maturation are\nconsistent with observations in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Fawaz_A/0/1/0/all/0/1\">Abdulah Fawaz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Williams_L/0/1/0/all/0/1\">Logan Z. Williams</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_E/0/1/0/all/0/1\">Emma Robinson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Edwards_A/0/1/0/all/0/1\">A. David Edwards</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Reduce Change Detection to Semantic Segmentation. (arXiv:2206.07557v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07557","description":"<p>Change detection (CD) aims to identify changes that occur in an image pair\ntaken different times. Prior methods devise specific networks from scratch to\npredict change masks in pixel-level, and struggle with general segmentation\nproblems. In this paper, we propose a new paradigm that reduces CD to semantic\nsegmentation which means tailoring an existing and powerful semantic\nsegmentation network to solve CD. This new paradigm conveniently enjoys the\nmainstream semantic segmentation techniques to deal with general segmentation\nproblems in CD. Hence we can concentrate on studying how to detect changes. We\npropose a novel and importance insight that different change types exist in CD\nand they should be learned separately. Based on it, we devise a module named\nMTF to extract the change information and fuse temporal features. MTF enjoys\nhigh interpretability and reveals the essential characteristic of CD. And most\nsegmentation networks can be adapted to solve the CD problems with our MTF\nmodule. Finally, we propose C-3PO, a network to detect changes at pixel-level.\nC-3PO achieves state-of-the-art performance without bells and whistles. It is\nsimple but effective and can be considered as a new baseline in this field. Our\ncode will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guo-Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bin-Bin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Meta-Analysis of Distributionally-Robust Models. (arXiv:2206.07565v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07565","description":"<p>State-of-the-art image classifiers trained on massive datasets (such as\nImageNet) have been shown to be vulnerable to a range of both intentional and\nincidental distribution shifts. On the other hand, several recent classifiers\nwith favorable out-of-distribution (OOD) robustness properties have emerged,\nachieving high accuracy on their target tasks while maintaining their\nin-distribution accuracy on challenging benchmarks. We present a meta-analysis\non a wide range of publicly released models, most of which have been published\nover the last twelve months. Through this meta-analysis, we empirically\nidentify four main commonalities for all the best-performing OOD-robust models,\nall of which illuminate the considerable promise of vision-language\npre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feuer_B/0/1/0/all/0/1\">Benjamin Feuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Ameya Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations. (arXiv:2206.07578v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07578","description":"<p>Event cameras respond to brightness changes in the scene asynchronously and\nindependently for every pixel. Due to the properties, these cameras have\ndistinct features: high dynamic range (HDR), high temporal resolution, and low\npower consumption. However, the results of event cameras should be processed\ninto an alternative representation for computer vision tasks. Also, they are\nusually noisy and cause poor performance in areas with few events. In recent\nyears, numerous researchers have attempted to reconstruct videos from events.\nHowever, they do not provide good quality videos due to a lack of temporal\ninformation from irregular and discontinuous data. To overcome these\ndifficulties, we introduce an E2V-SDE whose dynamics are governed in a latent\nspace by Stochastic differential equations (SDE). Therefore, E2V-SDE can\nrapidly reconstruct images at arbitrary time steps and make realistic\npredictions on unseen data. In addition, we successfully adopted a variety of\nimage composition techniques for improving image clarity and temporal\nconsistency. By conducting extensive experiments on simulated and real-scene\ndatasets, we verify that our model outperforms state-of-the-art approaches\nunder various video reconstruction settings. In terms of image quality, the\nLPIPS score improves by up to 12% and the reconstruction speed is 87% higher\nthan that of ET-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">DongJin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1\">Byunggook Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongsik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jeonghee Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating object detector ensembles for improving the robustness of artifact detection in endoscopic video streams. (arXiv:2206.07580v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07580","description":"<p>In this contribution we use an ensemble deep-learning method for combining\nthe prediction of two individual one-stage detectors (i.e., YOLOv4 and Yolact)\nwith the aim to detect artefacts in endoscopic images. This ensemble strategy\nenabled us to improve the robustness of the individual models without harming\ntheir real-time computation capabilities. We demonstrated the effectiveness of\nour approach by training and testing the two individual models and various\nensemble configurations on the \"Endoscopic Artifact Detection Challenge\"\ndataset. Extensive experiments show the superiority, in terms of mean average\nprecision, of the ensemble approach over the individual models and previous\nworks in the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavarrias_Solano_P/0/1/0/all/0/1\">Pedro Esteban Chavarrias-Solano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Vega_C/0/1/0/all/0/1\">Carlos Axel Garcia-Vega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Tiro_F/0/1/0/all/0/1\">Francisco Javier Lopez-Tiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazin_T/0/1/0/all/0/1\">Thomas Bazin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamarque_D/0/1/0/all/0/1\">Dominique Lamarque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daul_C/0/1/0/all/0/1\">Christian Daul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIO-CXRNET: A Robust Multimodal Stacking Machine Learning Technique for Mortality Risk Prediction of COVID-19 Patients using Chest X-Ray Images and Clinical Data. (arXiv:2206.07595v1 [eess.IV])","link":"http://arxiv.org/abs/2206.07595","description":"<p>Fast and accurate detection of the disease can significantly help in reducing\nthe strain on the healthcare facility of any country to reduce the mortality\nduring any pandemic. The goal of this work is to create a multimodal system\nusing a novel machine learning framework that uses both Chest X-ray (CXR)\nimages and clinical data to predict severity in COVID-19 patients. In addition,\nthe study presents a nomogram-based scoring technique for predicting the\nlikelihood of death in high-risk patients. This study uses 25 biomarkers and\nCXR images in predicting the risk in 930 COVID-19 patients admitted during the\nfirst wave of COVID-19 (March-June 2020) in Italy. The proposed multimodal\nstacking technique produced the precision, sensitivity, and F1-score, of\n89.03%, 90.44%, and 89.03%, respectively to identify low or high-risk patients.\nThis multimodal approach improved the accuracy by 6% in comparison to the CXR\nimage or clinical data alone. Finally, nomogram scoring system using\nmultivariate logistic regression -- was used to stratify the mortality risk\namong the high-risk patients identified in the first stage. Lactate\nDehydrogenase (LDH), O2 percentage, White Blood Cells (WBC) Count, Age, and\nC-reactive protein (CRP) were identified as useful predictor using random\nforest feature selection model. Five predictors parameters and a CXR image\nbased nomogram score was developed for quantifying the probability of death and\ncategorizing them into two risk groups: survived (&lt;50%), and death (&gt;=50%),\nrespectively. The multi-modal technique was able to predict the death\nprobability of high-risk patients with an F1 score of 92.88 %. The area under\nthe curves for the development and validation cohorts are 0.981 and 0.939,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1\">Tawsifur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Muhammad E. H. Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1\">Amith Khandakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahbub_Z/0/1/0/all/0/1\">Zaid Bin Mahbub</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_M/0/1/0/all/0/1\">Md Sakib Abrar Hossain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alhatou_A/0/1/0/all/0/1\">Abraham Alhatou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdalla_E/0/1/0/all/0/1\">Eynas Abdalla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muthiyal_S/0/1/0/all/0/1\">Sreekumar Muthiyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islam_K/0/1/0/all/0/1\">Khandaker Farzana Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kashem_S/0/1/0/all/0/1\">Saad Bin Abul Kashem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Salman Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zughaier_S/0/1/0/all/0/1\">Susu M. Zughaier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossain_M/0/1/0/all/0/1\">Maqsud Hossain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How GNNs Facilitate CNNs in Mining Geometric Information from Large-Scale Medical Images. (arXiv:2206.07599v1 [eess.IV])","link":"http://arxiv.org/abs/2206.07599","description":"<p>Gigapixel medical images provide massive data, both morphological textures\nand spatial information, to be mined. Due to the large data scale in histology,\ndeep learning methods play an increasingly significant role as feature\nextractors. Existing solutions heavily rely on convolutional neural networks\n(CNNs) for global pixel-level analysis, leaving the underlying local geometric\nstructure such as the interaction between cells in the tumor microenvironment\nunexplored. The topological structure in medical images, as proven to be\nclosely related to tumor evolution, can be well characterized by graphs. To\nobtain a more comprehensive representation for downstream oncology tasks, we\npropose a fusion framework for enhancing the global image-level representation\ncaptured by CNNs with the geometry of cell-level spatial information learned by\ngraph neural networks (GNN). The fusion layer optimizes an integration between\ncollaborative features of global images and cell graphs. Two fusion strategies\nhave been developed: one with MLP which is simple but turns out efficient\nthrough fine-tuning, and the other with Transformer gains a champion in fusing\nmultiple networks. We evaluate our fusion strategies on histology datasets\ncurated from large patient cohorts of colorectal and gastric cancers for three\nbiomarker prediction tasks. Both two models outperform plain CNNs or GNNs,\nreaching a consistent AUC improvement of more than 5% on various network\nbackbones. The experimental results yield the necessity for combining\nimage-level morphological features with cell spatial relations in medical image\nanalysis. Codes are available at https://github.com/yiqings/HEGnnEnhanceCnn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqing Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1\">Bingxin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_X/0/1/0/all/0/1\">Xinye Xiong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">Ruitian Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Guang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real3D-Aug: Point Cloud Augmentation by Placing Real Objects with Occlusion Handling for 3D Detection and Segmentation. (arXiv:2206.07634v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07634","description":"<p>Object detection and semantic segmentation with the 3D lidar point cloud data\nrequire expensive annotation. We propose a data augmentation method that takes\nadvantage of already annotated data multiple times. We propose an augmentation\nframework that reuses real data, automatically finds suitable placements in the\nscene to be augmented, and handles occlusions explicitly. Due to the usage of\nthe real data, the scan points of newly inserted objects in augmentation\nsustain the physical characteristics of the lidar, such as intensity and\nraydrop. The pipeline proves competitive in training top-performing models for\n3D object detection and semantic segmentation. The new augmentation provides a\nsignificant performance gain in rare and essential classes, notably 6.65%\naverage precision gain for \"Hard\" pedestrian class in KITTI object detection or\n2.14 mean IoU gain in the SemanticKITTI segmentation challenge over the state\nof the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sebek_P/0/1/0/all/0/1\">Petr &#x160;ebek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokorny_S/0/1/0/all/0/1\">&#x160;imon Pokorn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vacek_P/0/1/0/all/0/1\">Patrik Vacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svoboda_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Svoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. (arXiv:2206.07643v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07643","description":"<p>Vision-language (VL) pre-training has recently received considerable\nattention. However, most existing end-to-end pre-training approaches either\nonly aim to tackle VL tasks such as image-text retrieval, visual question\nanswering (VQA) and image captioning that test high-level understanding of\nimages, or only target region-level understanding for tasks such as phrase\ngrounding and object detection. We present FIBER (Fusion-In-the-Backbone-based\ntransformER), a new VL model architecture that can seamlessly handle both these\ntypes of tasks. Instead of having dedicated transformer layers for fusion after\nthe uni-modal backbones, FIBER pushes multimodal fusion deep into the model by\ninserting cross-attention into the image and text backbones, bringing gains in\nterms of memory and performance. In addition, unlike previous work that is\neither only pre-trained on image-text data or on fine-grained data with\nbox-level annotations, we present a two-stage pre-training strategy that uses\nboth these kinds of data efficiently: (i) coarse-grained pre-training based on\nimage-text data; followed by (ii) fine-grained pre-training based on\nimage-text-box data. We conduct comprehensive experiments on a wide range of VL\ntasks, ranging from VQA, image captioning, and retrieval, to phrase grounding,\nreferring expression comprehension, and object detection. Using deep multimodal\nfusion coupled with the two-stage pre-training, FIBER provides consistent\nperformance improvements over strong baselines across all tasks, often\noutperforming methods using magnitudes more data. Code is available at\nhttps://github.com/microsoft/FIBER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Aishwarya Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SP-ViT: Learning 2D Spatial Priors for Vision Transformers. (arXiv:2206.07662v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07662","description":"<p>Recently, transformers have shown great potential in image classification and\nestablished state-of-the-art results on the ImageNet benchmark. However,\ncompared to CNNs, transformers converge slowly and are prone to overfitting in\nlow-data regimes due to the lack of spatial inductive biases. Such spatial\ninductive biases can be especially beneficial since the 2D structure of an\ninput image is not well preserved in transformers. In this work, we present\nSpatial Prior-enhanced Self-Attention (SP-SA), a novel variant of vanilla\nSelf-Attention (SA) tailored for vision transformers. Spatial Priors (SPs) are\nour proposed family of inductive biases that highlight certain groups of\nspatial relations. Unlike convolutional inductive biases, which are forced to\nfocus exclusively on hard-coded local regions, our proposed SPs are learned by\nthe model itself and take a variety of spatial relations into account.\nSpecifically, the attention score is calculated with emphasis on certain kinds\nof spatial relations at each head, and such learned spatial foci can be\ncomplementary to each other. Based on SP-SA we propose the SP-ViT family, which\nconsistently outperforms other ViT models with similar GFlops or parameters.\nOur largest model SP-ViT-L achieves a record-breaking 86.3% Top-1 accuracy with\na reduction in the number of parameters by almost 50% compared to previous\nstate-of-the-art model (150M for SP-ViT-L vs 271M for CaiT-M-36) among all\nImageNet-1K models trained on 224x224 and fine-tuned on 384x384 resolution w/o\nextra data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wangmeng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Biao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xihan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xiansheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRISP - Reliable Uncertainty Estimation for Medical Image Segmentation. (arXiv:2206.07664v1 [eess.IV])","link":"http://arxiv.org/abs/2206.07664","description":"<p>Accurate uncertainty estimation is a critical need for the medical imaging\ncommunity. A variety of methods have been proposed, all direct extensions of\nclassification uncertainty estimations techniques. The independent pixel-wise\nuncertainty estimates, often based on the probabilistic interpretation of\nneural networks, do not take into account anatomical prior knowledge and\nconsequently provide sub-optimal results to many segmentation tasks. For this\nreason, we propose CRISP a ContRastive Image Segmentation for uncertainty\nPrediction method. At its core, CRISP implements a contrastive method to learn\na joint latent space which encodes a distribution of valid segmentations and\ntheir corresponding images. We use this joint latent space to compare\npredictions to thousands of latent vectors and provide anatomically consistent\nuncertainty maps. Comprehensive studies performed on four medical image\ndatabases involving different modalities and organs underlines the superiority\nof our method compared to state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Judge_T/0/1/0/all/0/1\">Thierry Judge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bernard_O/0/1/0/all/0/1\">Olivier Bernard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Porumb_M/0/1/0/all/0/1\">Mihaela Porumb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agis Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Sequence Interface for Vision Tasks. (arXiv:2206.07669v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07669","description":"<p>While language tasks are naturally expressed in a single, unified, modeling\nframework, i.e., generating sequences of tokens, this has not been the case in\ncomputer vision. As a result, there is a proliferation of distinct\narchitectures and loss functions for different vision tasks. In this work we\nshow that a diverse set of \"core\" computer vision tasks can also be unified if\nformulated in terms of a shared pixel-to-sequence interface. We focus on four\ntasks, namely, object detection, instance segmentation, keypoint detection, and\nimage captioning, all with diverse types of outputs, e.g., bounding boxes or\ndense masks. Despite that, by formulating the output of each task as a sequence\nof discrete tokens with a unified interface, we show that one can train a\nneural network with a single model architecture and loss function on all these\ntasks, with no task-specific customization. To solve a specific task, we use a\nshort prompt as task description, and the sequence output adapts to the prompt\nso it can produce task-specific output. We show that such a model can achieve\ncompetitive performance compared to well-established task-specific models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Saurabh Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVATAR: Unconstrained Audiovisual Speech Recognition. (arXiv:2206.07684v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07684","description":"<p>Audio-visual automatic speech recognition (AV-ASR) is an extension of ASR\nthat incorporates visual cues, often from the movements of a speaker's mouth.\nUnlike works that simply focus on the lip motion, we investigate the\ncontribution of entire visual frames (visual actions, objects, background\netc.). This is particularly useful for unconstrained videos, where the speaker\nis not necessarily visible. To solve this task, we propose a new\nsequence-to-sequence AudioVisual ASR TrAnsformeR (AVATAR) which is trained\nend-to-end from spectrograms and full-frame RGB. To prevent the audio stream\nfrom dominating training, we propose different word-masking strategies, thereby\nencouraging our model to pay attention to the visual stream. We demonstrate the\ncontribution of the visual modality on the How2 AV-ASR benchmark, especially in\nthe presence of simulated noise, and show that our model outperforms all other\nprior work by a large margin. Finally, we also create a new, real-world test\nbed for AV-ASR called VisSpeech, which demonstrates the contribution of the\nvisual modality under challenging audio conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabeur_V/0/1/0/all/0/1\">Valentin Gabeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuck Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Sparsity Connection Learning for Efficient Video Super-Resolution. (arXiv:2206.07687v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07687","description":"<p>Lighter and faster models are crucial for the deployment of video\nsuper-resolution (VSR) on resource-limited devices, e.g., smartphones and\nwearable devices. In this paper, we develop Residual Sparsity Connection\nLearning (RSCL), a structured pruning scheme, to reduce the redundancy of\nconvolution kernels and obtain a compact VSR network with a minor performance\ndrop. However, residual blocks require the pruned filter indices of skip and\nresidual connections to be the same, which is tricky for pruning. Thus, to\nmitigate the pruning restrictions of residual blocks, we design a Residual\nSparsity Connection (RSC) scheme by preserving the feature channels and only\noperating on the important channels. Moreover, for the pixel-shuffle operation,\nwe design a special pruning scheme by grouping several filters as pruning units\nto guarantee the accuracy of feature channel-space conversion after pruning. In\naddition, we introduce Temporal Finetuning (TF) to reduce the pruning error\namplification of hidden states with temporal propagation. Extensive experiments\nshow that the proposed RSCL significantly outperforms recent methods\nquantitatively and qualitatively. Codes and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingwen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_Y/0/1/0/all/0/1\">Yucheng Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Video Tokens @ Ego4D PNR Temporal Localization Challenge 2022. (arXiv:2206.07689v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07689","description":"<p>This technical report describes the SViT approach for the Ego4D Point of No\nReturn (PNR) Temporal Localization Challenge. We propose a learning framework\nStructureViT (SViT for short), which demonstrates how utilizing the structure\nof a small number of images only available during training can improve a video\nmodel. SViT relies on two key insights. First, as both images and videos\ncontain structured information, we enrich a transformer model with a set of\n\\emph{object tokens} that can be used across images and videos. Second, the\nscene representations of individual frames in video should \"align\" with those\nof still images. This is achieved via a \"Frame-Clip Consistency\" loss, which\nensures the flow of structured information between images and videos. SViT\nobtains strong performance on the challenge test set with 0.656 absolute\ntemporal localization error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Avraham_E/0/1/0/all/0/1\">Elad Ben-Avraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1\">Roei Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1\">Amir Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELUDE: Generating interpretable explanations via a decomposition into labelled and unlabelled features. (arXiv:2206.07690v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07690","description":"<p>Deep learning models have achieved remarkable success in different areas of\nmachine learning over the past decade; however, the size and complexity of\nthese models make them difficult to understand. In an effort to make them more\ninterpretable, several recent works focus on explaining parts of a deep neural\nnetwork through human-interpretable, semantic attributes. However, it may be\nimpossible to completely explain complex models using only semantic attributes.\nIn this work, we propose to augment these attributes with a small set of\nuninterpretable features. Specifically, we develop a novel explanation\nframework ELUDE (Explanation via Labelled and Unlabelled DEcomposition) that\ndecomposes a model's prediction into two parts: one that is explainable through\na linear combination of the semantic attributes, and another that is dependent\non the set of uninterpretable features. By identifying the latter, we are able\nto analyze the \"unexplained\" portion of the model, obtaining insights into the\ninformation used by the model. We show that the set of unlabelled features can\ngeneralize to multiple models trained with the same feature space and compare\nour work to two popular attribute-oriented methods, Interpretable Basis\nDecomposition and Concept Bottleneck, and discuss the additional insights ELUDE\nprovides.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramaswamy_V/0/1/0/all/0/1\">Vikram V. Ramaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunnie S. Y. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_N/0/1/0/all/0/1\">Nicole Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1\">Ruth Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Data Mixing Prior for Improving Self-Supervised Learning. (arXiv:2206.07692v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07692","description":"<p>Data mixing (e.g., Mixup, Cutmix, ResizeMix) is an essential component for\nadvancing recognition models. In this paper, we focus on studying its\neffectiveness in the self-supervised setting. By noticing the mixed images that\nshare the same source images are intrinsically related to each other, we hereby\npropose SDMP, short for $\\textbf{S}$imple $\\textbf{D}$ata $\\textbf{M}$ixing\n$\\textbf{P}$rior, to capture this straightforward yet essential prior, and\nposition such mixed images as additional $\\textbf{positive pairs}$ to\nfacilitate self-supervised representation learning. Our experiments verify that\nthe proposed SDMP enables data mixing to help a set of self-supervised learning\nframeworks (e.g., MoCo) achieve better accuracy and out-of-distribution\nrobustness. More notably, our SDMP is the first method that successfully\nleverages data mixing to improve (rather than hurt) the performance of Vision\nTransformers in the self-supervised setting. Code is publicly available at\nhttps://github.com/OliverRensu/SDMP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids. (arXiv:2206.07695v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07695","description":"<p>State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to\nparameterize 3D radiance fields. While demonstrating impressive results,\nquerying an MLP for every sample along each ray leads to slow rendering.\nTherefore, existing approaches often render low-resolution feature maps and\nprocess them with an upsampling network to obtain the final image. Albeit\nefficient, neural rendering often entangles viewpoint and content such that\nchanging the camera pose results in unwanted changes of geometry or appearance.\nMotivated by recent results in voxel-based novel view synthesis, we investigate\nthe utility of sparse voxel grid representations for fast and 3D-consistent\ngenerative modeling in this paper. Our results demonstrate that monolithic MLPs\ncan indeed be replaced by 3D convolutions when combining sparse voxel grids\nwith progressive growing, free space pruning and appropriate regularization. To\nobtain a compact representation of the scene and allow for scaling to higher\nvoxel resolutions, our model disentangles the foreground object (modeled in 3D)\nfrom the background (modeled in 2D). In contrast to existing approaches, our\nmethod requires only a single forward pass to generate a full 3D scene. It\nhence allows for efficient rendering from arbitrary viewpoints while yielding\n3D consistent results with high visual fidelity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_K/0/1/0/all/0/1\">Katja Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_A/0/1/0/all/0/1\">Axel Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niemeyer_M/0/1/0/all/0/1\">Michael Niemeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Models for Video Prediction and Infilling. (arXiv:2206.07696v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07696","description":"<p>To predict and anticipate future outcomes or reason about missing information\nin a sequence is a key ability for agents to be able to make intelligent\ndecisions. This requires strong temporally coherent generative capabilities.\nDiffusion models have shown huge success in several generative tasks lately,\nbut have not been extensively explored in the video domain. We present\nRandom-Mask Video Diffusion (RaMViD), which extends image diffusion models to\nvideos using 3D convolutions, and introduces a new conditioning technique\nduring training. By varying the mask we condition on, the model is able to\nperform video prediction, infilling and upsampling. Since we do not use\nconcatenation to condition on a mask, as done in most conditionally trained\ndiffusion models, we are able to decrease the memory footprint. We evaluated\nthe model on two benchmark datasets for video prediction and one for video\ngeneration on which we achieved competitive results. On Kinetics-600 we\nachieved state-of-the-art for video prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoppe_T/0/1/0/all/0/1\">Tobias H&#xf6;ppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrjou_A/0/1/0/all/0/1\">Arash Mehrjou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_D/0/1/0/all/0/1\">Didrik Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Deformable Voxel Grid for Fast Optimization of Dynamic View Synthesis. (arXiv:2206.07698v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07698","description":"<p>Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel\nview synthesis (NVS) for its superior performance. However, NeRF and its\nvariants generally require a lengthy per-scene training procedure, where a\nmulti-layer perceptron (MLP) is fitted to the captured images. To remedy the\nchallenge, the voxel-grid representation has been proposed to significantly\nspeed up the training. However, these existing methods can only deal with\nstatic scenes. How to develop an efficient and accurate dynamic view synthesis\nmethod remains an open problem. Extending the methods for static scenes to\ndynamic scenes is not straightforward as both the scene geometry and appearance\nchange over time. In this paper, built on top of the recent advances in\nvoxel-grid optimization, we propose a fast deformable radiance field method to\nhandle dynamic scenes. Our method consists of two modules. The first module\nadopts a deformation grid to store 3D dynamic features, and a light-weight MLP\nfor decoding the deformation that maps a 3D point in observation space to the\ncanonical space using the interpolated features. The second module contains a\ndensity and a color grid to model the geometry and density of the scene. The\nocclusion is explicitly modeled to further improve the rendering quality.\nExperimental results show that our method achieves comparable performance to\nD-NeRF using only 20 minutes for training, which is more than 70x faster than\nD-NeRF, clearly demonstrating the efficiency of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiadai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prefix Language Models are Unified Modal Learners. (arXiv:2206.07699v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07699","description":"<p>With the success of vision-language pre-training, we have witnessed the\nstate-of-the-art has been pushed on multi-modal understanding and generation.\nHowever, the current pre-training paradigm is either incapable of targeting all\nmodalities at once (e.g., text generation and image generation), or requires\nmulti-fold well-designed tasks which significantly limits the scalability. We\ndemonstrate that a unified modal model could be learned with a prefix language\nmodeling objective upon text and image sequences. Thanks to the simple but\npowerful pre-training paradigm, our proposed model, DaVinci, is simple to\ntrain, scalable to huge data, and adaptable to a variety of downstream tasks\nacross modalities (language / vision / vision+language), types (understanding /\ngeneration) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with\na single unified architecture. DaVinci achieves the competitive performance on\na wide range of 26 understanding / generation tasks, and outperforms previous\nunified vision-language models on most tasks, including ImageNet classification\n(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and\nCOCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data\nscale. Furthermore, we offer a well-defined benchmark for future research by\nreporting the performance on different scales of the pre-training dataset on a\nheterogeneous and wide distribution coverage. Our results establish new,\nstronger baselines for future comparisons at different data scales and shed\nlight on the difficulties of comparing VLP models more generally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiawei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Siamese ConvNets. (arXiv:2206.07700v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07700","description":"<p>Self-supervised learning has shown superior performances over supervised\nmethods on various vision benchmarks. The siamese network, which encourages\nembeddings to be invariant to distortions, is one of the most successful\nself-supervised visual representation learning approaches. Among all the\naugmentation methods, masking is the most general and straightforward method\nthat has the potential to be applied to all kinds of input and requires the\nleast amount of domain knowledge. However, masked siamese networks require\nparticular inductive bias and practically only work well with Vision\nTransformers. This work empirically studies the problems behind masked siamese\nnetworks with ConvNets. We propose several empirical designs to overcome these\nproblems gradually. Our method performs competitively on low-shot image\nclassification and outperforms previous methods on object detection benchmarks.\nWe discuss several remaining issues and hope this work can provide useful data\npoints for future general-purpose self-supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Li Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiachen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Waymo Open Dataset: Panoramic Video Panoptic Segmentation. (arXiv:2206.07704v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07704","description":"<p>Panoptic image segmentation is the computer vision task of finding groups of\npixels in an image and assigning semantic classes and object instance\nidentifiers to them. Research in image segmentation has become increasingly\npopular due to its critical applications in robotics and autonomous driving.\nThe research community thereby relies on publicly available benchmark dataset\nto advance the state-of-the-art in computer vision. Due to the high costs of\ndensely labeling the images, however, there is a shortage of publicly available\nground truth labels that are suitable for panoptic segmentation. The high\nlabeling costs also make it challenging to extend existing datasets to the\nvideo domain and to multi-camera setups. We therefore present the Waymo Open\nDataset: Panoramic Video Panoptic Segmentation Dataset, a large-scale dataset\nthat offers high-quality panoptic segmentation labels for autonomous driving.\nWe generate our dataset using the publicly available Waymo Open Dataset,\nleveraging the diverse set of camera images. Our labels are consistent over\ntime for video processing and consistent across multiple cameras mounted on the\nvehicles for full panoramic scene understanding. Specifically, we offer labels\nfor 28 semantic categories and 2,860 temporal sequences that were captured by\nfive cameras mounted on autonomous vehicles driving in three different\ngeographical locations, leading to a total of 100k labeled camera images. To\nthe best of our knowledge, this makes our dataset an order of magnitude larger\nthan existing datasets that offer video panoptic segmentation labels. We\nfurther propose a new benchmark for Panoramic Video Panoptic Segmentation and\nestablish a number of strong baselines based on the DeepLab family of models.\nWe will make the benchmark and the code publicly available. Find the dataset at\nhttps://waymo.com/open.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jieru Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">Alex Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xinchen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Siyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kretzschmar_H/0/1/0/all/0/1\">Henrik Kretzschmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LET-3D-AP: Longitudinal Error Tolerant 3D Average Precision for Camera-Only 3D Detection. (arXiv:2206.07705v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07705","description":"<p>The popular object detection metric 3D Average Precision (3D AP) relies on\nthe intersection over union between predicted bounding boxes and ground truth\nbounding boxes. However, depth estimation based on cameras has limited\naccuracy, which may cause otherwise reasonable predictions that suffer from\nsuch longitudinal localization errors to be treated as false positives and\nfalse negatives. We therefore propose variants of the popular 3D AP metric that\nare designed to be more permissive with respect to depth estimation errors.\nSpecifically, our novel longitudinal error tolerant metrics, LET-3D-AP and\nLET-3D-APL, allow longitudinal localization errors of the predicted bounding\nboxes up to a given tolerance. The proposed metrics have been used in the Waymo\nOpen Dataset 3D Camera-Only Detection Challenge. We believe that they will\nfacilitate advances in the field of camera-only 3D detection by providing more\ninformative performance signals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kretzschmar_H/0/1/0/all/0/1\">Henrik Kretzschmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casser_V/0/1/0/all/0/1\">Vincent Casser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jyh-Jing Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Frequency Modeling for Self-Supervised Visual Pre-Training. (arXiv:2206.07706v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07706","description":"<p>We present Masked Frequency Modeling (MFM), a unified frequency-domain-based\napproach for self-supervised pre-training of visual models. Instead of randomly\ninserting mask tokens to the input embeddings in the spatial domain, in this\npaper, we shift the perspective to the frequency domain. Specifically, MFM\nfirst masks out a portion of frequency components of the input image and then\npredicts the missing frequencies on the frequency spectrum. Our key insight is\nthat predicting masked components in the frequency domain is more ideal to\nreveal underlying image patterns rather than predicting masked patches in the\nspatial domain, due to the heavy spatial redundancy. Our findings suggest that\nwith the right configuration of mask-and-predict strategy, both the structural\ninformation within high-frequency components and the low-level statistics among\nlow-frequency counterparts are useful in learning good representations. For the\nfirst time, MFM demonstrates that, for both ViT and CNN, a simple non-Siamese\nframework can learn meaningful representations even using none of the\nfollowing: (i) extra data, (ii) extra model, (iii) mask token. Experimental\nresults on ImageNet and several robustness benchmarks show the competitive\nperformance and advanced robustness of MFM compared with recent masked image\nmodeling approaches. Furthermore, we also comprehensively investigate the\neffectiveness of classical image restoration tasks for representation learning\nfrom a unified frequency perspective and reveal their intriguing relations with\nour MFM approach. Project page:\nhttps://www.mmlab-ntu.com/project/mfm/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiahao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xiaohang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew Soon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable Bitrate Neural Fields. (arXiv:2206.07707v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07707","description":"<p>Neural approximations of scalar and vector fields, such as signed distance\nfunctions and radiance fields, have emerged as accurate, high-quality\nrepresentations. State-of-the-art results are obtained by conditioning a neural\napproximation with a lookup from trainable feature grids that take on part of\nthe learning task and allow for smaller, more efficient neural networks.\nUnfortunately, these feature grids usually come at the cost of significantly\nincreased memory consumption compared to stand-alone neural network models. We\npresent a dictionary method for compressing such feature grids, reducing their\nmemory consumption by up to 100x and permitting a multiresolution\nrepresentation which can be useful for out-of-core streaming. We formulate the\ndictionary optimization as a vector-quantized auto-decoder problem which lets\nus learn end-to-end discrete neural representations in a space where no direct\nsupervision is available and with dynamic topology and structure. Our source\ncode will be available at https://github.com/nv-tlabs/vqad.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takikawa_T/0/1/0/all/0/1\">Towaki Takikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_A/0/1/0/all/0/1\">Alex Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1\">Jonathan Tremblay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuire_M/0/1/0/all/0/1\">Morgan McGuire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1\">Alec Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlanarRecon: Real-time 3D Plane Detection and Reconstruction from Posed Monocular Videos. (arXiv:2206.07710v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07710","description":"<p>We present PlanarRecon -- a novel framework for globally coherent detection\nand reconstruction of 3D planes from a posed monocular video. Unlike previous\nworks that detect planes in 2D from a single image, PlanarRecon incrementally\ndetects planes in 3D for each video fragment, which consists of a set of key\nframes, from a volumetric representation of the scene using neural networks. A\nlearning-based tracking and fusion module is designed to merge planes from\nprevious fragments to form a coherent global plane reconstruction. Such design\nallows PlanarRecon to integrate observations from multiple views within each\nfragment and temporal information across different ones, resulting in an\naccurate and coherent reconstruction of the scene abstraction with\nlow-polygonal geometry. Experiments show that the proposed approach achieves\nstate-of-the-art performances on the ScanNet dataset while being real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yiming Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadelha_M/0/1/0/all/0/1\">Matheus Gadelha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fengting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huaizu Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Are How You Walk: Uncooperative MoCap Gait Identification for Video Surveillance with Incomplete and Noisy Data. (arXiv:1706.09443v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1706.09443","description":"<p>This work offers a design of a video surveillance system based on a soft\nbiometric -- gait identification from MoCap data. The main focus is on two\nsubstantial issues of the video surveillance scenario: (1) the walkers do not\ncooperate in providing learning data to establish their identities and (2) the\ndata are often noisy or incomplete. We show that only a few examples of human\ngait cycles are required to learn a projection of raw MoCap data onto a\nlow-dimensional sub-space where the identities are well separable. Latent\nfeatures learned by Maximum Margin Criterion (MMC) method discriminate better\nthan any collection of geometric features. The MMC method is also highly robust\nto noisy data and works properly even with only a fraction of joints tracked.\nThe overall workflow of the design is directly applicable for a day-to-day\noperation based on the available MoCap technology and algorithms for gait\nanalysis. In the concept we introduce, a walker's identity is represented by a\ncluster of gait data collected at their incidents within the surveillance\nsystem: They are how they walk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balazia_M/0/1/0/all/0/1\">Michal Balazia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Adversarial Robustness of Optical Flow Networks. (arXiv:2103.16255v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16255","description":"<p>Recent work demonstrated the lack of robustness of optical flow networks to\nphysical patch-based adversarial attacks. The possibility to physically attack\na basic component of automotive systems is a reason for serious concerns. In\nthis paper, we analyze the cause of the problem and show that the lack of\nrobustness is rooted in the classical aperture problem of optical flow\nestimation in combination with bad choices in the details of the network\narchitecture. We show how these mistakes can be rectified in order to make\noptical flow networks robust to physical patch-based attacks. Additionally, we\ntake a look at global white-box attacks in the scope of optical flow. We find\nthat targeted white-box attacks can be crafted to bias flow estimation models\ntowards any desired output, but this requires access to the input images and\nmodel weights. However, in the case of universal attacks, we find that optical\nflow networks are robust. Code is available at\nhttps://github.com/lmb-freiburg/understanding_flow_robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schrodi_S/0/1/0/all/0/1\">Simon Schrodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikia_T/0/1/0/all/0/1\">Tonmoy Saikia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graphical Modeling for Multi-Source Domain Adaptation. (arXiv:2104.13057v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13057","description":"<p>Multi-Source Domain Adaptation (MSDA) focuses on transferring the knowledge\nfrom multiple source domains to the target domain, which is a more practical\nand challenging problem compared to the conventional single-source domain\nadaptation. In this problem, it is essential to model multiple source domains\nand target domain jointly, and an effective domain combination scheme is also\nhighly required. The graphical structure among different domains is useful to\ntackle these challenges, in which the interdependency among various\ninstances/categories can be effectively modeled. In this work, we propose two\ntypes of graphical models, i.e. Conditional Random Field for MSDA (CRF-MSDA)\nand Markov Random Field for MSDA (MRF-MSDA), for cross-domain joint modeling\nand learnable domain combination. In a nutshell, given an observation set\ncomposed of a query sample and the semantic prototypes (i.e. representative\ncategory embeddings) on various domains, the CRF-MSDA model seeks to learn the\njoint distribution of labels conditioned on the observations. We attain this\ngoal by constructing a relational graph over all observations and conducting\nlocal message passing on it. By comparison, MRF-MSDA aims to model the joint\ndistribution of observations over different Markov networks via an energy-based\nformulation, and it can naturally perform label prediction by summing the joint\nlikelihoods over several specific networks. Compared to the CRF-MSDA\ncounterpart, the MRF-MSDA model is more expressive and possesses lower\ncomputational cost. We evaluate these two models on four standard benchmark\ndata sets of MSDA with distinct domain shift and data complexity, and both\nmodels achieve superior performance over existing methods on all benchmarks. In\naddition, the analytical studies illustrate the effect of different model\ncomponents and provide insights about how the cross-domain joint modeling\nperforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformers with Hierarchical Attention. (arXiv:2106.03180v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03180","description":"<p>This paper tackles the low-efficiency flaw of the vision transformer caused\nby the high computational/space complexity in Multi-Head Self-Attention (MHSA).\nTo this end, we propose the Hierarchical MHSA (H-MHSA), whose representation is\ncomputed in a hierarchical manner. Specifically, we first divide the input\nimage into patches as commonly done, and each patch is viewed as a token. Then,\nthe proposed H-MHSA learns token relationships within local patches, serving as\nlocal relationship modeling. Then, the small patches are merged into larger\nones, and H-MHSA models the global dependencies for the small number of the\nmerged tokens. At last, the local and global attentive features are aggregated\nto obtain features with powerful representation capacity. Since we only\ncalculate attention for a limited number of tokens at each step, the\ncomputational load is reduced dramatically. Hence, H-MHSA can efficiently model\nglobal relationships among tokens without sacrificing fine-grained information.\nWith the H-MHSA module incorporated, we build a family of\nHierarchical-Attention-based Transformer Networks, namely HAT-Net. To\ndemonstrate the superiority of HAT-Net in scene understanding, we conduct\nextensive experiments on fundamental vision tasks, including image\nclassification, semantic segmentation, object detection, and instance\nsegmentation. Therefore, HAT-Net provides a new perspective for the vision\ntransformer. Code and pretrained models are available at\nhttps://github.com/yun-liu/HAT-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guolei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis. (arXiv:2108.03880v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03880","description":"<p>Multi-View Stereo (MVS) is a core task in 3D computer vision. With the surge\nof novel deep learning methods, learned MVS has surpassed the accuracy of\nclassical approaches, but still relies on building a memory intensive dense\ncost volume. Novel View Synthesis (NVS) is a parallel line of research and has\nrecently seen an increase in popularity with Neural Radiance Field (NeRF)\nmodels, which optimize a per scene radiance field. However, NeRF methods do not\ngeneralize to novel scenes and are slow to train and test. We propose to bridge\nthe gap between these two methodologies with a novel network that can recover\n3D scene geometry as a distance function, together with high-resolution color\nimages. Our method uses only a sparse set of images as input and can generalize\nwell to novel scenes. Additionally, we propose a coarse-to-fine sphere tracing\napproach in order to significantly increase speed. We show on various datasets\nthat our method reaches comparable accuracy to per-scene optimized methods\nwhile being able to generalize and running significantly faster. We provide the\nsource code at https://github.com/AIS-Bonn/neural_mvs\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.04352","description":"<p>Correctly capturing intraoperative brain shift in image-guided neurosurgical\nprocedures is a critical task for aligning preoperative data with\nintraoperative geometry for ensuring accurate surgical navigation. While the\nfinite element method (FEM) is a proven technique to effectively approximate\nsoft tissue deformation through biomechanical formulations, their degree of\nsuccess boils down to a trade-off between accuracy and speed. To circumvent\nthis problem, the most recent works in this domain have proposed leveraging\ndata-driven models obtained by training various machine learning algorithms,\ne.g. random forests, artificial neural networks (ANNs), with the results of\nfinite element analysis (FEA) to speed up tissue deformation approximations by\nprediction. These methods, however, do not account for the structure of the\nfinite element (FE) mesh during training that provides information on node\nconnectivities as well as the distance between them, which can aid with\napproximating tissue deformation based on the proximity of force load points\nwith the rest of the mesh nodes. Therefore, this work proposes a novel\nframework, PhysGNN, a data-driven model that approximates the solution of FEM\nby leveraging graph neural networks (GNNs), which are capable of accounting for\nthe mesh structural information and inductive learning over unstructured grids\nand complex topological structures. Empirically, we demonstrate that the\nproposed architecture, PhysGNN, promises accurate and fast soft tissue\ndeformation approximations and is competitive with the state-of-the-art (SOTA)\nalgorithms while promising enhanced computational feasibility, therefore\nsuitable for neurosurgical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Salehi_Y/0/1/0/all/0/1\">Yasmin Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giannacopoulos_D/0/1/0/all/0/1\">Dennis Giannacopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ripple Attention for Visual Perception with Sub-quadratic Complexity. (arXiv:2110.02453v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02453","description":"<p>Transformer architectures are now central to sequence modeling tasks. At its\nheart is the attention mechanism, which enables effective modeling of long-term\ndependencies in a sequence. Recently, transformers have been successfully\napplied in the computer vision domain, where 2D images are first segmented into\npatches and then treated as 1D sequences. Such linearization, however, impairs\nthe notion of spatial locality in images, which bears important visual clues.\nTo bridge the gap, we propose ripple attention, a sub-quadratic attention\nmechanism for vision transformers. Built upon the recent kernel-based efficient\nattention mechanisms, we design a novel dynamic programming algorithm that\nweights contributions of different tokens to a query with respect to their\nrelative spatial distances in the 2D space in linear observed time. Extensive\nexperiments and analyses demonstrate the effectiveness of ripple attention on\nvarious visual tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Huijie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifting Capsule Networks from the Cloud to the Deep Edge. (arXiv:2110.02911v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.02911","description":"<p>Capsule networks (CapsNets) are an emerging trend in image processing. In\ncontrast to a convolutional neural network, CapsNets are not vulnerable to\nobject deformation, as the relative spatial information of the objects is\npreserved across the network. However, their complexity is mainly related to\nthe capsule structure and the dynamic routing mechanism, which makes it almost\nunreasonable to deploy a CapsNet, in its original form, in a\nresource-constrained device powered by a small microcontroller (MCU). In an era\nwhere intelligence is rapidly shifting from the cloud to the edge, this high\ncomplexity imposes serious challenges to the adoption of CapsNets at the very\nedge. To tackle this issue, we present an API for the execution of quantized\nCapsNets in Arm Cortex-M and RISC-V MCUs. Our software kernels extend the Arm\nCMSIS-NN and RISC-V PULP-NN to support capsule operations with 8-bit integers\nas operands. Along with it, we propose a framework to perform post-training\nquantization of a CapsNet. Results show a reduction in memory footprint of\nalmost 75%, with accuracy loss ranging from 0.07% to 0.18%. In terms of\nthroughput, our Arm Cortex-M API enables the execution of primary capsule and\ncapsule layers with medium-sized kernels in just 119.94 and 90.60 milliseconds\n(ms), respectively (STM32H755ZIT6U, Cortex-M7 @ 480 MHz). For the GAP-8 SoC\n(RISC-V RV32IMCXpulp @ 170 MHz), the latency drops to 7.02 and 38.03 ms,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_M/0/1/0/all/0/1\">Miguel Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1\">Diogo Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_T/0/1/0/all/0/1\">Tiago Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_S/0/1/0/all/0/1\">Sandro Pinto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TLDR: Twin Learning for Dimensionality Reduction. (arXiv:2110.09455v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09455","description":"<p>Dimensionality reduction methods are unsupervised approaches which learn\nlow-dimensional spaces where some properties of the initial space, typically\nthe notion of \"neighborhood\", are preserved. Such methods usually require\npropagation on large k-NN graphs or complicated optimization solvers. On the\nother hand, self-supervised learning approaches, typically used to learn\nrepresentations from scratch, rely on simple and more scalable frameworks for\nlearning. In this paper, we propose TLDR, a dimensionality reduction method for\ngeneric input spaces that is porting the recent self-supervised learning\nframework of Zbontar et al. (2021) to the specific task of dimensionality\nreduction, over arbitrary representations. We propose to use nearest neighbors\nto build pairs from a training set and a redundancy reduction loss to learn an\nencoder that produces representations invariant across such pairs. TLDR is a\nmethod that is simple, easy to train, and of broad applicability; it consists\nof an offline nearest neighbor computation step that can be highly\napproximated, and a straightforward learning process. Aiming for scalability,\nwe focus on improving linear dimensionality reduction, and show consistent\ngains on image and document retrieval tasks, e.g. gaining +4% mAP over PCA on\nROxford for GeM- AP, improving the performance of DINO on ImageNet or retaining\nit with a 10x compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalantidis_Y/0/1/0/all/0/1\">Yannis Kalantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazan_J/0/1/0/all/0/1\">Jon Almazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1\">Diane Larlus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network Kalman filtering for 3D object tracking from linear array ultrasound data. (arXiv:2111.09631v3 [stat.AP] UPDATED)","link":"http://arxiv.org/abs/2111.09631","description":"<p>Many interventional surgical procedures rely on medical imaging to visualise\nand track instruments. Such imaging methods not only need to be real-time\ncapable, but also provide accurate and robust positional information. In\nultrasound applications, typically only two-dimensional data from a linear\narray are available, and as such obtaining accurate positional estimation in\nthree dimensions is non-trivial. In this work, we first train a neural network,\nusing realistic synthetic training data, to estimate the out-of-plane offset of\nan object with the associated axial aberration in the reconstructed ultrasound\nimage. The obtained estimate is then combined with a Kalman filtering approach\nthat utilises positioning estimates obtained in previous time-frames to improve\nlocalisation robustness and reduce the impact of measurement noise. The\naccuracy of the proposed method is evaluated using simulations, and its\npractical applicability is demonstrated on experimental data obtained using a\nnovel optical ultrasound imaging setup. Accurate and robust positional\ninformation is provided in real-time. Axial and lateral coordinates for\nout-of-plane objects are estimated with a mean error of 0.1mm for simulated\ndata and a mean error of 0.2mm for experimental data. Three-dimensional\nlocalisation is most accurate for elevational distances larger than 1mm, with a\nmaximum distance of 6mm considered for a 25mm aperture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Arjas_A/0/1/0/all/0/1\">Arttu Arjas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alles_E/0/1/0/all/0/1\">Erwin J. Alles</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maneas_E/0/1/0/all/0/1\">Efthymios Maneas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Arridge_S/0/1/0/all/0/1\">Simon Arridge</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Desjardins_A/0/1/0/all/0/1\">Adrien Desjardins</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sillanpaa_M/0/1/0/all/0/1\">Mikko J. Sillanp&#xe4;&#xe4;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hauptmann_A/0/1/0/all/0/1\">Andreas Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection. (arXiv:2111.13336v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13336","description":"<p>In object detection, the detection backbone consumes more than half of the\noverall inference cost. Recent researches attempt to reduce this cost by\noptimizing the backbone architecture with the help of Neural Architecture\nSearch (NAS). However, existing NAS methods for object detection require\nhundreds to thousands of GPU hours of searching, making them impractical in\nfast-paced research and development. In this work, we propose a novel zero-shot\nNAS method to address this issue. The proposed method, named MAE-DET,\nautomatically designs efficient detection backbones via the Maximum Entropy\nPrinciple without training network parameters, reducing the architecture design\ncost to nearly zero yet delivering the state-of-the-art (SOTA) performance.\nUnder the hood, MAE-DET maximizes the differential entropy of detection\nbackbones, leading to a better feature extractor for object detection under the\nsame computational budgets. After merely one GPU day of fully automatic design,\nMAE-DET innovates SOTA detection backbones on multiple detection benchmark\ndatasets with little human intervention. Comparing to ResNet-50 backbone,\nMAE-DET is $+2.0\\%$ better in mAP when using the same amount of\nFLOPs/parameters, and is $1.54$ times faster on NVIDIA V100 at the same mAP.\nCode and pre-trained models are available at\nhttps://github.com/alibaba/lightweight-neuralarchitecture-search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning a model of shape selectivity in V4 cells reveals shape encoding mechanisms in the brain. (arXiv:2111.14250v3 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2111.14250","description":"<p>The mechanisms involved in transforming early visual signals to curvature\nrepresentations in V4 are unknown. We propose a hierarchical model that reveals\nV1/V2 encodings that are essential components for this transformation to the\nreported curvature representations in V4. Then, by relaxing the often-imposed\nprior of a single Gaussian, V4 shape selectivity is learned in the last layer\nof the hierarchy from Macaque V4 responses. We found that V4 cells integrate\nmultiple shape parts from the full spatial extent of their receptive fields\nwith similar excitatory and inhibitory contributions. Our results uncover new\ndetails in existing data about shape selectivity in V4 neurons that with\nfurther experiments can enhance our understanding of processing in this area.\nAccordingly, we propose designs for a stimulus set that allow removing shape\nparts without disturbing the curvature signal to isolate part contributions to\nV4 responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Mehrani_P/0/1/0/all/0/1\">Paria Mehrani</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial-Sketch Synthesis: A New Challenge. (arXiv:2112.15439v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15439","description":"<p>This paper aims to conduct a comprehensive study on facial-sketch synthesis\n(FSS). However, due to the high costs of obtaining hand-drawn sketch datasets,\nthere lacks a complete benchmark for assessing the development of FSS\nalgorithms over the last decade. We first introduce a high-quality dataset for\nFSS, named FS2K, which consists of 2,104 image-sketch pairs spanning three\ntypes of sketch styles, image backgrounds, lighting conditions, skin colors,\nand facial attributes. FS2K differs from previous FSS datasets in difficulty,\ndiversity, and scalability and should thus facilitate the progress of FSS\nresearch. Second, we present the largest-scale FSS investigation by reviewing\n89 classical methods, including 25 handcrafted feature-based facial-sketch\nsynthesis approaches, 29 general translation methods, and 35 image-to-sketch\napproaches. Besides, we elaborate comprehensive experiments on the existing 19\ncutting-edge models. Third, we present a simple baseline for FSS, named FSGAN.\nWith only two straightforward components, i.e., facial-aware masking and\nstyle-vector expansion, FSGAN surpasses the performance of all previous\nstate-of-the-art models on the proposed FS2K dataset by a large margin.\nFinally, we conclude with lessons learned over the past years and point out\nseveral unsolved challenges. Our code is available at\nhttps://github.com/DengPingFan/FSGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised continual learning for class-incremental segmentation. (arXiv:2201.01029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01029","description":"<p>Transfer learning is a powerful way to adapt existing deep learning models to\nnew emerging use-cases in remote sensing. Starting from a neural network\nalready trained for semantic segmentation, we propose to modify its label space\nto swiftly adapt it to new classes under weak supervision. To alleviate the\nbackground shift and the catastrophic forgetting problems inherent to this form\nof continual learning, we compare different regularization terms and leverage a\npseudo-label strategy. We experimentally show the relevance of our approach on\nthree public remote sensing datasets. Code is open-source and released in this\nrepository:\nhttps://github.com/alteia-ai/ICSS}{https://github.com/alteia-ai/ICSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lenczner_G/0/1/0/all/0/1\">Gaston Lenczner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_Hon_Tong_A/0/1/0/all/0/1\">Adrien Chan-Hon-Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luminari_N/0/1/0/all/0/1\">Nicola Luminari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Egocentric 3D Pose Estimation with Third Person Views. (arXiv:2201.02017v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02017","description":"<p>In this paper, we propose a novel approach to enhance the 3D body pose\nestimation of a person computed from videos captured from a single wearable\ncamera. The key idea is to leverage high-level features linking first- and\nthird-views in a joint embedding space. To learn such embedding space we\nintroduce First2Third-Pose, a new paired synchronized dataset of nearly 2,000\nvideos depicting human activities captured from both first- and third-view\nperspectives. We explicitly consider spatial- and motion-domain features,\ncombined using a semi-Siamese architecture trained in a self-supervised\nfashion. Experimental results demonstrate that the joint multi-view embedded\nspace learned with our dataset is useful to extract discriminatory features\nfrom arbitrary single-view egocentric videos, without needing domain adaptation\nnor knowledge of camera parameters. We achieve significant improvement of\negocentric 3D body pose estimation performance on two unconstrained datasets,\nover three supervised state-of-the-art approaches. Our dataset and code will be\navailable for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhamanaskar_A/0/1/0/all/0/1\">Ameya Dhamanaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corona_E/0/1/0/all/0/1\">Enric Corona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pumarola_A/0/1/0/all/0/1\">Albert Pumarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video. (arXiv:2201.04127v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04127","description":"<p>We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on\na given monocular video of a human performing complex body motions, e.g. a\nvideo from YouTube. Our method enables pausing the video at any frame and\nrendering the subject from arbitrary new camera viewpoints or even a full\n360-degree camera path for that particular frame and body pose. This task is\nparticularly challenging, as it requires synthesizing photorealistic details of\nthe body, as seen from various camera angles that may not exist in the input\nvideo, as well as synthesizing fine details such as cloth folds and facial\nappearance. Our method optimizes for a volumetric representation of the person\nin a canonical T-pose, in concert with a motion field that maps the estimated\ncanonical representation to every frame of the video via backward warps. The\nmotion field is decomposed into skeletal rigid and non-rigid motions, produced\nby deep networks. We show significant performance improvements over prior work,\nand compelling examples of free-viewpoint renderings from monocular video of\nmoving humans in challenging uncontrolled capture scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chung-Yi Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1\">Brian Curless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pratul P. Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemelmacher_Shlizerman_I/0/1/0/all/0/1\">Ira Kemelmacher-Shlizerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximizing Self-supervision from Thermal Image for Effective Self-supervised Learning of Depth and Ego-motion. (arXiv:2201.04387v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2201.04387","description":"<p>Recently, self-supervised learning of depth and ego-motion from thermal\nimages shows strong robustness and reliability under challenging scenarios.\nHowever, the inherent thermal image properties such as weak contrast, blurry\nedges, and noise hinder to generate effective self-supervision from thermal\nimages. Therefore, most research relies on additional self-supervision sources\nsuch as well-lit RGB images, generative models, and Lidar information. In this\npaper, we conduct an in-depth analysis of thermal image characteristics that\ndegenerates self-supervision from thermal images. Based on the analysis, we\npropose an effective thermal image mapping method that significantly increases\nimage information, such as overall structure, contrast, and details, while\npreserving temporal consistency. The proposed method shows outperformed depth\nand pose results than previous state-of-the-art networks without leveraging\nadditional RGB guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_U/0/1/0/all/0/1\">Ukcheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byeong-Uk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Cut Once: Boosting Data Augmentation with a Single Cut. (arXiv:2201.12078v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12078","description":"<p>We present You Only Cut Once (YOCO) for performing data augmentations. YOCO\ncuts one image into two pieces and performs data augmentations individually\nwithin each piece. Applying YOCO improves the diversity of the augmentation per\nsample and encourages neural networks to recognize objects from partial\ninformation. YOCO enjoys the properties of parameter-free, easy usage, and\nboosting almost all augmentations for free. Thorough experiments are conducted\nto evaluate its effectiveness. We first demonstrate that YOCO can be seamlessly\napplied to varying data augmentations, neural network architectures, and brings\nperformance gains on CIFAR and ImageNet classification tasks, sometimes\nsurpassing conventional image-level augmentation by large margins. Moreover, we\nshow YOCO benefits contrastive pre-training toward a more powerful\nrepresentation that can be better transferred to multiple downstream tasks.\nFinally, we study a number of variants of YOCO and empirically analyze the\nperformance for respective settings. Code is available at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junlin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jie Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VRT: A Video Restoration Transformer. (arXiv:2201.12288v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12288","description":"<p>Video restoration (e.g., video super-resolution) aims to restore high-quality\nframes from low-quality frames. Different from single image restoration, video\nrestoration generally requires to utilize temporal information from multiple\nadjacent but usually misaligned video frames. Existing deep methods generally\ntackle with this by exploiting a sliding window strategy or a recurrent\narchitecture, which either is restricted by frame-by-frame restoration or lacks\nlong-range modelling ability. In this paper, we propose a Video Restoration\nTransformer (VRT) with parallel frame prediction and long-range temporal\ndependency modelling abilities. More specifically, VRT is composed of multiple\nscales, each of which consists of two kinds of modules: temporal mutual self\nattention (TMSA) and parallel warping. TMSA divides the video into small clips,\non which mutual attention is applied for joint motion estimation, feature\nalignment and feature fusion, while self attention is used for feature\nextraction. To enable cross-clip interactions, the video sequence is shifted\nfor every other layer. Besides, parallel warping is used to further fuse\ninformation from neighboring frames by parallel feature warping. Experimental\nresults on five tasks, including video super-resolution, video deblurring,\nvideo denoising, video frame interpolation and space-time video\nsuper-resolution, demonstrate that VRT outperforms the state-of-the-art methods\nby large margins ($\\textbf{up to 2.16dB}$) on fourteen benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yuchen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1\">Rakesh Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPC: Transformation-Specific Smoothing for Point Cloud Models. (arXiv:2201.12733v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12733","description":"<p>Point cloud models with neural network architectures have achieved great\nsuccess and have been widely used in safety-critical applications, such as\nLidar-based recognition systems in autonomous vehicles. However, such models\nare shown vulnerable to adversarial attacks which aim to apply stealthy\nsemantic transformations such as rotation and tapering to mislead model\npredictions. In this paper, we propose a transformation-specific smoothing\nframework TPC, which provides tight and scalable robustness guarantees for\npoint cloud models against semantic transformation attacks. We first categorize\ncommon 3D transformations into three categories: additive (e.g., shearing),\ncomposable (e.g., rotation), and indirectly composable (e.g., tapering), and we\npresent generic robustness certification strategies for all categories\nrespectively. We then specify unique certification protocols for a range of\nspecific semantic transformations and their compositions. Extensive experiments\non several common 3D transformations show that TPC significantly outperforms\nthe state of the art. For example, our framework boosts the certified accuracy\nagainst twisting transformation along z-axis (within 20$^\\circ$) from 20.3$\\%$\nto 83.8$\\%$. Codes and models are available at\nhttps://github.com/Qianhewu/Point-Cloud-Smoothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wenda Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eikonal Fields for Refractive Novel-View Synthesis. (arXiv:2202.00948v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2202.00948","description":"<p>We tackle the problem of generating novel-view images from collections of 2D\nimages showing refractive and reflective objects. Current solutions assume\nopaque or transparent light transport along straight paths following the\nemission-absorption model. Instead, we optimize for a field of 3D-varying Index\nof Refraction (IoR) and trace light through it that bends toward the spatial\ngradients of said IoR according to the laws of eikonal light transport.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bemana_M/0/1/0/all/0/1\">Mojtaba Bemana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myszkowski_K/0/1/0/all/0/1\">Karol Myszkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisvad_J/0/1/0/all/0/1\">Jeppe Revall Frisvad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1\">Hans-Peter Seidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Specific Attention is one more thing you need for object detection. (arXiv:2202.09048v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09048","description":"<p>Various models have been proposed to perform object detection. However, most\nrequire many handdesigned components such as anchors and\nnon-maximum-suppression(NMS) to demonstrate good performance. To mitigate these\nissues, Transformer-based DETR and its variant, Deformable DETR, were\nsuggested. These have solved much of the complex issue in designing a head for\nobject detection models; however, doubts about performance still exist when\nconsidering Transformer-based models as state-of-the-art methods in object\ndetection for other models depending on anchors and NMS revealed better\nresults. Furthermore, it has been unclear whether it would be possible to build\nan end-to-end pipeline in combination only with attention modules, because the\nDETR-adapted Transformer method used a convolutional neural network (CNN) for\nthe backbone body. In this study, we propose that combining several attention\nmodules with our new Task Specific Split Transformer (TSST) is a powerful\nmethod to produce the state-of-the art performance on COCO results without\ntraditionally hand-designed components. By splitting the general-purpose\nattention module into two separated goal-specific attention modules, the\nproposed method allows for the design of simpler object detection models.\nExtensive experiments on the COCO benchmark demonstrate the effectiveness of\nour approach. Code is available at https://github.com/navervision/tsst\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang Yon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction. (arXiv:2203.01474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01474","description":"<p>Predicting future motion based on historical motion sequence is a fundamental\nproblem in computer vision, and it has wide applications in autonomous driving\nand robotics. Some recent works have shown that Graph Convolutional\nNetworks(GCN) are instrumental in modeling the relationship between different\njoints. However, considering the variants and diverse action types in human\nmotion data, the cross-dependency of the spatio-temporal relationships will be\ndifficult to depict due to the decoupled modeling strategy, which may also\nexacerbate the problem of insufficient generalization. Therefore, we propose\nthe Spatio-Temporal Gating-Adjacency GCN(GAGCN) to learn the complex\nspatio-temporal dependencies over diverse action types. Specifically, we adopt\ngating networks to enhance the generalization of GCN via the trainable adaptive\nadjacency matrix obtained by blending the candidate spatio-temporal adjacency\nmatrices. Moreover, GAGCN addresses the cross-dependency of space and time by\nbalancing the weights of spatio-temporal modeling and fusing the decoupled\nspatio-temporal features. Extensive experiments on Human 3.6M, AMASS, and 3DPW\ndemonstrate that GAGCN achieves state-of-the-art performance in both short-term\nand long-term predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Chongyang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yongjing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shihong Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PETR: Position Embedding Transformation for Multi-View 3D Object Detection. (arXiv:2203.05625v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05625","description":"<p>In this paper, we develop position embedding transformation (PETR) for\nmulti-view 3D object detection. PETR encodes the position information of 3D\ncoordinates into image features, producing the 3D position-aware features.\nObject query can perceive the 3D position-aware features and perform end-to-end\nobject detection. PETR achieves state-of-the-art performance (50.4% NDS and\n44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.\nIt can serve as a simple yet strong baseline for future research. Code is\navailable at \\url{https://github.com/megvii-research/PETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transform your Smartphone into a DSLR Camera: Learning the ISP in the Wild. (arXiv:2203.10636v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10636","description":"<p>We propose a trainable Image Signal Processing (ISP) framework that produces\nDSLR quality images given RAW images captured by a smartphone. To address the\ncolor misalignments between training image pairs, we employ a color-conditional\nISP network and optimize a novel parametric color mapping between each input\nRAW and reference DSLR image. During inference, we predict the target color\nimage by designing a color prediction network with efficient Global Context\nTransformer modules. The latter effectively leverage global information to\nlearn consistent color and tone mappings. We further propose a robust masked\naligned loss to identify and discard regions with inaccurate motion estimation\nduring training. Lastly, we introduce the ISP in the Wild (ISPW) dataset,\nconsisting of weakly paired phone RAW and DSLR sRGB images. We extensively\nevaluate our method, setting a new state-of-the-art on two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1\">Ardhendu Shekhar Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Samarth Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11442","description":"<p>This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computation resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\njointly and collaboratively. In detail, AOT employs an identification mechanism\nto associate multiple targets into the same high-dimensional embedding space.\nThus, we can simultaneously process multiple objects' matching and segmentation\ndecoding as efficiently as processing a single object. To sufficiently model\nmulti-object association, a Long Short-Term Transformer (LSTT) is devised to\nconstruct hierarchical matching and propagation. Based on AOT, we further\npropose a more flexible and robust framework, Associating Objects with Scalable\nTransformers (AOST), in which a scalable version of LSTT is designed to enable\nrun-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces\na better layer-wise manner to couple identification and vision embeddings. We\nconduct extensive experiments on multi-object and single-object benchmarks to\nexamine AOT series frameworks. Compared to the state-of-the-art competitors,\nour methods can maintain times of run-time efficiency with superior\nperformance. Notably, we achieve new state-of-the-art performance on three\npopular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test\n(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:\nhttps://github.com/z-x-yang/AOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jiaxu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Multi-Scale Feature Fusion for Semantic Segmentation. (arXiv:2203.12683v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12683","description":"<p>It is commonly believed that high internal resolution combined with expensive\noperations (e.g. atrous convolutions) are necessary for accurate semantic\nsegmentation, resulting in slow speed and large memory usage. In this paper, we\nquestion this belief and demonstrate that neither high internal resolution nor\natrous convolutions are necessary. Our intuition is that although segmentation\nis a dense per-pixel prediction task, the semantics of each pixel often depend\non both nearby neighbors and far-away context; therefore, a more powerful\nmulti-scale feature fusion network plays a critical role. Following this\nintuition, we revisit the conventional multi-scale feature space (typically\ncapped at P5) and extend it to a much richer space, up to P9, where the\nsmallest features are only 1/512 of the input size and thus have very large\nreceptive fields. To process such a rich feature space, we leverage the recent\nBiFPN to fuse the multi-scale features. Based on these insights, we develop a\nsimplified segmentation model, named ESeg, which has neither high internal\nresolution nor expensive atrous convolutions. Perhaps surprisingly, our simple\nmethod can achieve better accuracy with faster speed than prior art across\nmultiple datasets. In real-time settings, ESeg-Lite-S achieves 76.0% mIoU on\nCityScapes [12] at 189 FPS, outperforming FasterSeg [9] (73.1% mIoU at 170\nFPS). Our ESeg-Lite-L runs at 79 FPS and achieves 80.1% mIoU, largely closing\nthe gap between real-time and high-performance segmentation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1\">Tianjian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahjourian_R/0/1/0/all/0/1\">Reza Mahjourian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Reduce Information Bottleneck for Object Detection in Aerial Images. (arXiv:2204.02033v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02033","description":"<p>Object detection in aerial images is a fundamental research task in the\ndomain of geoscience and remote sensing. However, the advanced progress on this\ntopic mainly focuses on designing progressive backbone architectures or head\nnetworks but ignores the neck network. In this letter, we first analyze the\nimportance of the neck network in object detection from the perspective of\ninformation bottleneck. Then, to alleviate the information deficiency problem\nin the current neck networks, we propose a Global Semantic Network (GSNet),\nwhich acts as a bridge from the backbone to the head network in a bidirectional\nglobal pattern. Compared to the existing neck networks, our model can capture\nrich and detailed image features with less computational costs. Besides, we\nfurther propose a feature Fusion Refinement Module (FRM) for different levels\nof feature maps, which are suffering from a big information gap. To demonstrate\nthe effectiveness and efficiency of our approach, experiments are carried out\non two challenging datasets (i.e., DOTA and HRSC2016). Experimental results in\nterms of recognition accuracy and computational complexity validate the\nsuperiority of our method. The code has been open-sourced at GSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuchen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhihao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuesong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qiaolin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Complexity Randomized Self-attention Mechanism. (arXiv:2204.04667v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.04667","description":"<p>Recently, random feature attentions (RFAs) are proposed to approximate the\nsoftmax attention in linear time and space complexity by linearizing the\nexponential kernel. In this paper, we first propose a novel perspective to\nunderstand the bias in such approximation by recasting RFAs as self-normalized\nimportance samplers. This perspective further sheds light on an \\emph{unbiased}\nestimator for the whole softmax attention, called randomized attention (RA). RA\nconstructs positive random features via query-specific distributions and enjoys\ngreatly improved approximation fidelity, albeit exhibiting quadratic\ncomplexity. By combining the expressiveness in RA and the efficiency in RFA, we\ndevelop a novel linear complexity self-attention mechanism called linear\nrandomized attention (LARA). Extensive experiments across various domains\ndemonstrate that RA and LARA significantly improve the performance of RFAs by a\nsubstantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastMapSVM: Classifying Complex Objects Using the FastMap Algorithm and Support-Vector Machines. (arXiv:2204.05112v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05112","description":"<p>Neural Networks and related Deep Learning methods are currently at the\nleading edge of technologies used for classifying objects. However, they\ngenerally demand large amounts of time and data for model training; and their\nlearned models can sometimes be difficult to interpret. In this paper, we\nadvance FastMapSVM -- an interpretable Machine Learning framework for\nclassifying complex objects -- as an advantageous alternative to Neural\nNetworks for general classification tasks. FastMapSVM extends the applicability\nof Support-Vector Machines (SVMs) to domains with complex objects by combining\nthe complementary strengths of FastMap and SVMs. FastMap is an efficient\nlinear-time algorithm that maps complex objects to points in a Euclidean space\nwhile preserving pairwise domain-specific distances between them. We\ndemonstrate the efficiency and effectiveness of FastMapSVM in the context of\nclassifying seismograms. We show that its performance, in terms of precision,\nrecall, and accuracy, is comparable to that of other state-of-the-art methods.\nHowever, compared to other methods, FastMapSVM uses significantly smaller\namounts of time and data for model training. It also provides a perspicuous\nvisualization of the objects and the classification boundaries between them. We\nexpect FastMapSVM to be viable for classification tasks in many other\nreal-world domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Malcolm C. A. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Kushal Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_T/0/1/0/all/0/1\">T. K. Satish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakata_N/0/1/0/all/0/1\">Nori Nakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Dual Emotion with Fusion of Visual Sentiment for Rumor Detection. (arXiv:2204.11515v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2204.11515","description":"<p>In recent years, rumors have had a devastating impact on society, making\nrumor detection a significant challenge. However, the studies on rumor\ndetection ignore the intense emotions of images in the rumor content. This\npaper verifies that the image emotion improves the rumor detection efficiency.\nA Multimodal Dual Emotion feature in rumor detection, which consists of visual\nand textual emotions, is proposed. To the best of our knowledge, this is the\nfirst study which uses visual emotion in rumor detection. The experiments on\nreal datasets verify that the proposed features outperform the state-of-the-art\nsentiment features, and can be extended in rumor detectors while improving\ntheir performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Li Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1\">Ziliang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">He Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Expression Transformer: A Computer Vision Approach for Symbolic Regression. (arXiv:2205.11798v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11798","description":"<p>Symbolic Regression (SR) is a type of regression analysis to automatically\nfind the mathematical expression that best fits the data. Currently, SR still\nbasically relies on various searching strategies so that a sample-specific\nmodel is required to be optimized for every expression, which significantly\nlimits the model's generalization and efficiency. Inspired by the fact that\nhuman beings can infer a mathematical expression based on the curve of it, we\npropose Symbolic Expression Transformer (SET), a sample-agnostic model from the\nperspective of computer vision for SR. Specifically, the collected data is\nrepresented as images and an image caption model is employed for translating\nimages to symbolic expressions. A large-scale dataset without overlap between\ntraining and testing sets in the image domain is released. Our results\ndemonstrate the effectiveness of SET and suggest the promising direction of\nimage-based model for solving the challenging SR problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hong-Bin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose. (arXiv:2205.12583v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12583","description":"<p>Reconstructing multi-human body mesh from a single monocular image is an\nimportant but challenging computer vision problem. In addition to the\nindividual body mesh models, we need to estimate relative 3D positions among\nsubjects to generate a coherent representation. In this work, through a single\ngraph neural network, named MUG (Multi-hUman Graph network), we construct\ncoherent multi-human meshes using only multi-human 2D pose as input. Compared\nwith existing methods, which adopt a detection-style pipeline (i.e., extracting\nimage features and then locating human instances and recovering body meshes\nfrom that) and suffer from the significant domain gap between lab-collected\ntraining datasets and in-the-wild testing datasets, our method benefits from\nthe 2D pose which has a relatively consistent geometric property across\ndatasets. Our method works like the following: First, to model the multi-human\nenvironment, it processes multi-human 2D poses and builds a novel heterogeneous\ngraph, where nodes from different people and within one person are connected to\ncapture inter-human interactions and draw the body geometry (i.e., skeleton and\nmesh structure). Second, it employs a dual-branch graph neural network\nstructure -- one for predicting inter-human depth relation and the other one\nfor predicting root-joint-relative mesh coordinates. Finally, the entire\nmulti-human 3D meshes are constructed by combining the output from both\nbranches. Extensive experiments demonstrate that MUG outperforms previous\nmulti-human mesh estimation methods on standard 3D human benchmarks --\nPanoptic, MuPoTS-3D and 3DPW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yandong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianfeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01992","description":"<p>Detection of object anomalies is crucial in industrial processes, but\nunsupervised anomaly detection and localization is particularly important due\nto the difficulty of obtaining a large number of defective samples and the\nunpredictable types of anomalies in real life. Among the existing unsupervised\nanomaly detection and localization methods, the NF-based scheme has achieved\nbetter results. However, the two subnets (complex functions) $s_{i}(u_{i})$ and\n$t_{i}(u_{i})$ in NF are usually multilayer perceptrons, which need to squeeze\nthe input visual features from 2D flattening to 1D, destroying the spatial\nlocation relationship in the feature map and losing the spatial structure\ninformation. In order to retain and effectively extract spatial structure\ninformation, we design in this study a complex function model with alternating\nCBAM embedded in a stacked $3\\times3$ full convolution, which is able to retain\nand effectively extract spatial structure information in the normalized flow\nmodel. Extensive experimental results on the MVTec AD dataset show that\nCAINNFlow achieves advanced levels of accuracy and inference efficiency based\non CNN and Transformer backbone networks as feature extractors, and CAINNFlow\nachieves a pixel-level AUC of $98.64\\%$ for anomaly detection in MVTec AD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ruiqing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mengyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dongyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingrong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qianjin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Linghan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utility of Equivariant Message Passing in Cortical Mesh Segmentation. (arXiv:2206.03164v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03164","description":"<p>The automated segmentation of cortical areas has been a long-standing\nchallenge in medical image analysis. The complex geometry of the cortex is\ncommonly represented as a polygon mesh, whose segmentation can be addressed by\ngraph-based learning methods. When cortical meshes are misaligned across\nsubjects, current methods produce significantly worse segmentation results,\nlimiting their ability to handle multi-domain data. In this paper, we\ninvestigate the utility of E(n)-equivariant graph neural networks (EGNNs),\ncomparing their performance against plain graph neural networks (GNNs). Our\nevaluation shows that GNNs outperform EGNNs on aligned meshes, due to their\nability to leverage the presence of a global coordinate system. On misaligned\nmeshes, the performance of plain GNNs drop considerably, while E(n)-equivariant\nmessage passing maintains the same segmentation results. The best results can\nalso be obtained by using plain GNNs on realigned data (co-registered meshes in\na global coordinate system).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unyi_D/0/1/0/all/0/1\">D&#xe1;niel Unyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Insalata_F/0/1/0/all/0/1\">Ferdinando Insalata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyires_Toth_B/0/1/0/all/0/1\">B&#xe1;lint Gyires-T&#xf3;th</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeMF: Neural Motion Fields for Kinematic Animation. (arXiv:2206.03287v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03287","description":"<p>We present an implicit neural representation to learn the spatio-temporal\nspace of kinematic motions. Unlike previous work that represents motion as\ndiscrete sequential samples, we propose to express the vast motion space as a\ncontinuous function over time, hence the name Neural Motion Fields (NeMF).\nSpecifically, we use a neural network to learn this function for miscellaneous\nsets of motions, which is designed to be a generative model conditioned on a\ntemporal coordinate $t$ and a random vector $z$ for controlling the style. The\nmodel is then trained as a Variational Autoencoder (VAE) with motion encoders\nto sample the latent space. We train our model with diverse human motion\ndataset and quadruped dataset to prove its versatility, and finally deploy it\nas a generic motion prior to solve task-agnostic problems and show its\nsuperiority in different motion generation and editing applications, such as\nmotion interpolation, in-betweening, and re-navigating. More details can be\nfound on our project page: https://cs.yale.edu/homes/che/projects/nemf/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chengan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zachary_J/0/1/0/all/0/1\">James Zachary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rushmeier_H/0/1/0/all/0/1\">Holly Rushmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving into the Pre-training Paradigm of Monocular 3D Object Detection. (arXiv:2206.03657v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03657","description":"<p>The labels of monocular 3D object detection (M3OD) are expensive to obtain.\nMeanwhile, there usually exists numerous unlabeled data in practical\napplications, and pre-training is an efficient way of exploiting the knowledge\nin unlabeled data. However, the pre-training paradigm for M3OD is hardly\nstudied. We aim to bridge this gap in this work. To this end, we first draw two\nobservations: (1) The guideline of devising pre-training tasks is imitating the\nrepresentation of the target task. (2) Combining depth estimation and 2D object\ndetection is a promising M3OD pre-training baseline. Afterwards, following the\nguideline, we propose several strategies to further improve this baseline,\nwhich mainly include target guided semi-dense depth estimation, keypoint-aware\n2D object detection, and class-level loss adjustment. Combining all the\ndeveloped techniques, the obtained pre-training framework produces pre-trained\nbackbones that improve M3OD performance significantly on both the KITTI-3D and\nnuScenes benchmarks. For example, by applying a DLA34 backbone to a naive\ncenter-based M3OD detector, the moderate ${\\rm AP}_{3D}70$ score of Car on the\nKITTI-3D testing set is boosted by 18.71\\% and the NDS score on the nuScenes\nvalidation set is improved by 40.41\\% relatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuanrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1\">En Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bringing Image Scene Structure to Video via Frame-Clip Consistency of Object Tokens. (arXiv:2206.06346v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06346","description":"<p>Recent action recognition models have achieved impressive results by\nintegrating objects, their locations and interactions. However, obtaining dense\nstructured annotations for each frame is tedious and time-consuming, making\nthese methods expensive to train and less scalable. At the same time, if a\nsmall set of annotated images is available, either within or outside the domain\nof interest, how could we leverage these for a video downstream task? We\npropose a learning framework StructureViT (SViT for short), which demonstrates\nhow utilizing the structure of a small number of images only available during\ntraining can improve a video model. SViT relies on two key insights. First, as\nboth images and videos contain structured information, we enrich a transformer\nmodel with a set of \\emph{object tokens} that can be used across images and\nvideos. Second, the scene representations of individual frames in video should\n\"align\" with those of still images. This is achieved via a \\emph{Frame-Clip\nConsistency} loss, which ensures the flow of structured information between\nimages and videos. We explore a particular instantiation of scene structure,\nnamely a \\emph{Hand-Object Graph}, consisting of hands and objects with their\nlocations as nodes, and physical relations of contact/no-contact as edges. SViT\nshows strong performance improvements on multiple video understanding tasks and\ndatasets. Furthermore, it won in the Ego4D CVPR'22 Object State Localization\nchallenge. For code and pretrained models, visit the project page at\n\\url{https://eladb3.github.io/SViT/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Avraham_E/0/1/0/all/0/1\">Elad Ben-Avraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1\">Roei Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1\">Amir Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fitting Segmentation Networks on Varying Image Resolutions using Splatting. (arXiv:2206.06445v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.06445","description":"<p>Data used in image segmentation are not always defined on the same grid. This\nis particularly true for medical images, where the resolution, field-of-view\nand orientation can differ across channels and subjects. Images and labels are\ntherefore commonly resampled onto the same grid, as a pre-processing step.\nHowever, the resampling operation introduces partial volume effects and\nblurring, thereby changing the effective resolution and reducing the contrast\nbetween structures. In this paper we propose a splat layer, which automatically\nhandles resolution mismatches in the input data. This layer pushes each image\nonto a mean space where the forward pass is performed. As the splat operator is\nthe adjoint to the resampling operator, the mean-space prediction can be pulled\nback to the native label space, where the loss function is computed. Thus, the\nneed for explicit resolution adjustment using interpolation is removed. We show\non two publicly available datasets, with simulated and real multi-modal\nmagnetic resonance images, that this model improves segmentation results\ncompared to resampling as a pre-processing step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Brudfors_M/0/1/0/all/0/1\">Mikael Brudfors</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balbastre_Y/0/1/0/all/0/1\">Yael Balbastre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashburner_J/0/1/0/all/0/1\">John Ashburner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rees_G/0/1/0/all/0/1\">Geraint Rees</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nachev_P/0/1/0/all/0/1\">Parashkev Nachev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks. (arXiv:2206.06637v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06637","description":"<p>Temporal/spatial receptive fields of models play an important role in\nsequential/spatial tasks. Large receptive fields facilitate long-term\nrelations, while small receptive fields help to capture the local details.\nExisting methods construct models with hand-designed receptive fields in\nlayers. Can we effectively search for receptive field combinations to replace\nhand-designed patterns? To answer this question, we propose to find better\nreceptive field combinations through a global-to-local search scheme. Our\nsearch scheme exploits both global search to find the coarse combinations and\nlocal search to get the refined receptive field combinations further. The\nglobal search finds possible coarse combinations other than human-designed\npatterns. On top of the global search, we propose an expectation-guided\niterative local search scheme to refine combinations effectively. Our RF-Next\nmodels, plugging receptive field search to various models, boost the\nperformance on many tasks, e.g., temporal action segmentation, object\ndetection, instance segmentation, and speech synthesis. The source code is\npublicly available on <a href=\"http://mmcheng.net/rfnext.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shanghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong-Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Gait Recognition by Granger Causality. (arXiv:2206.06714v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06714","description":"<p>Which joint interactions in the human gait cycle can be used as biometric\ncharacteristics? Most current methods on gait recognition suffer from the lack\nof interpretability. We propose an interpretable feature representation of gait\nsequences by the graphical Granger causal inference. Gait sequence of a person\nin the standardized motion capture format, constituting a set of 3D joint\nspatial trajectories, is envisaged as a causal system of joints interacting in\ntime. We apply the graphical Granger model (GGM) to obtain the so-called\nGranger causal graph among joints as a discriminative and visually\ninterpretable representation of a person's gait. We evaluate eleven distance\nfunctions in the GGM feature space by established classification and\nclass-separability evaluation metrics. Our experiments indicate that, depending\non the metric, the most appropriate distance functions for the GGM are the\ntotal norm distance and the Ky-Fan 1-norm distance. Experiments also show that\nthe GGM is able to detect the most discriminative joint interactions and that\nit outperforms five related interpretable models in correct classification rate\nand in Davies-Bouldin index. The proposed GGM model can serve as a\ncomplementary tool for gait analysis in kinesiology or for gait recognition in\nvideo surveillance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balazia_M/0/1/0/all/0/1\">Michal Balazia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hlavackova_Schindler_K/0/1/0/all/0/1\">Katerina Hlavackova-Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plant_C/0/1/0/all/0/1\">Claudia Plant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Decoder-free Object Detection with Transformers. (arXiv:2206.06829v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06829","description":"<p>Vision transformers (ViTs) are changing the landscape of object detection\napproaches. A natural usage of ViTs in detection is to replace the CNN-based\nbackbone with a transformer-based backbone, which is straightforward and\neffective, with the price of bringing considerable computation burden for\ninference. More subtle usage is the DETR family, which eliminates the need for\nmany hand-designed components in object detection but introduces a decoder\ndemanding an extra-long time to converge. As a result, transformer-based object\ndetection can not prevail in large-scale applications. To overcome these\nissues, we propose a novel decoder-free fully transformer-based (DFFT) object\ndetector, achieving high efficiency in both training and inference stages, for\nthe first time. We simplify objection detection into an encoder-only\nsingle-level anchor-based dense prediction problem by centering around two\nentry points: 1) Eliminate the training-inefficient decoder and leverage two\nstrong encoders to preserve the accuracy of single-level feature map\nprediction; 2) Explore low-level semantic features for the detection task with\nlimited computational resources. In particular, we design a novel lightweight\ndetection-oriented transformer backbone that efficiently captures low-level\nfeatures with rich semantics based on a well-conceived ablation study.\nExtensive experiments on the MS COCO benchmark demonstrate that DFFT_SMALL\noutperforms DETR by 2.5% AP with 28% computation cost reduction and more than\n$10\\times$ fewer training epochs. Compared with the cutting-edge anchor-based\ndetector RetinaNet, DFFT_SMALL obtains over 5.5% AP gain while cutting down 70%\ncomputation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peixian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a> (Tencent Youtu Lab)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Segmentation of Mitochondria from Electron Microscopy Images Using Spatial Continuity. (arXiv:2206.02392v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2206.02392","description":"<p>Morphology of mitochondria plays critical roles in mediating their\nphysiological functions. Accurate segmentation of mitochondria from 3D electron\nmicroscopy (EM) images is essential to quantitative characterization of their\nmorphology at the nanometer scale. Fully supervised deep learning models\ndeveloped for this task achieve excellent performance but require substantial\namounts of annotated data for training. However, manual annotation of EM images\nis laborious and time-consuming because of their large volumes, limited\ncontrast, and low signal-to-noise ratios (SNRs). To overcome this challenge, we\npropose a semi-supervised deep learning model that segments mitochondria by\nleveraging the spatial continuity of their structural, morphological, and\ncontextual information in both labeled and unlabeled images. We use random\npiecewise affine transformation to synthesize comprehensive and realistic\nmitochondrial morphology for augmentation of training data. Experiments on the\nEPFL dataset show that our model achieves performance similar as that of\nstate-of-the-art fully supervised models but requires only ~20% of their\nannotated training data. Our semi-supervised model is versatile and can also\naccurately segment other spatially continuous structures from EM images. Data\nand code of this study are openly accessible at\nhttps://github.com/cbmi-group/MPP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yunpeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Youpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}