{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v1 [eess.AS])","link":"http://arxiv.org/abs/2109.08186","description":"<p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.\nFaST-VGS is a Transformer-based model for learning the associations between raw\nspeech waveforms and visual images. The model unifies dual-encoder and\ncross-attention architectures into a single model, reaping the superior\nretrieval speed of the former along with the accuracy of the latter. FaST-VGS\nachieves state-of-the-art speech-image retrieval accuracy on benchmark\ndatasets, and its learned representations exhibit strong performance on the\nZeroSpeech 2021 phonetic and semantic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Numerical reasoning in machine reading comprehension tasks: are we there yet?. (arXiv:2109.08207v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08207","description":"<p>Numerical reasoning based machine reading comprehension is a task that\ninvolves reading comprehension along with using arithmetic operations such as\naddition, subtraction, sorting, and counting. The DROP benchmark (Dua et al.,\n2019) is a recent dataset that has inspired the design of NLP models aimed at\nsolving this task. The current standings of these models in the DROP\nleaderboard, over standard metrics, suggest that the models have achieved\nnear-human performance. However, does this mean that these models have learned\nto reason? In this paper, we present a controlled study on some of the\ntop-performing model architectures for the task of numerical reasoning. Our\nobservations suggest that the standard metrics are incapable of measuring\nprogress towards such tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Negheimish_H/0/1/0/all/0/1\">Hadeel Al-Negheimish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08214","description":"<p>When humans conceive how to perform a particular task, they do so\nhierarchically: splitting higher-level tasks into smaller sub-tasks. However,\nin the literature on natural language (NL) command of situated agents, most\nworks have treated the procedures to be executed as flat sequences of simple\nactions, or any hierarchies of procedures have been shallow at best. In this\npaper, we propose a formalism of procedures as programs, a powerful yet\nintuitive method of representing hierarchical procedural knowledge for agent\ncommand and control. We further propose a modeling paradigm of hierarchical\nmodular networks, which consist of a planner and reactors that convert NL\nintents to predictions of executable programs and probe the environment for\ninformation necessary to complete the program execution. We instantiate this\nframework on the IQA and ALFRED datasets for NL instruction following. Our\nmodel outperforms reactive baselines by a large margin on both datasets. We\nalso demonstrate that our framework is more data-efficient, and that it allows\nfor fast iterative development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bag of Tricks for Dialogue Summarization. (arXiv:2109.08232v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08232","description":"<p>Dialogue summarization comes with its own peculiar challenges as opposed to\nnews or scientific articles summarization. In this work, we explore four\ndifferent challenges of the task: handling and differentiating parts of the\ndialogue belonging to multiple speakers, negation understanding, reasoning\nabout the situation, and informal language understanding. Using a pretrained\nsequence-to-sequence language model, we explore speaker name substitution,\nnegation scope highlighting, multi-task learning with relevant tasks, and\npretraining on in-domain data. Our experiments show that our proposed\ntechniques indeed improve summarization performance, outperforming strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularized Training of Nearest Neighbor Language Models. (arXiv:2109.08249v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08249","description":"<p>Including memory banks in a natural language processing architecture\nincreases model capacity by equipping it with additional data at inference\ntime. In this paper, we build upon $k$NN-LM \\citep{khandelwal20generalization},\nwhich uses a pre-trained language model together with an exhaustive $k$NN\nsearch through the training data (memory bank) to achieve state-of-the-art\nresults. We investigate whether we can improve the $k$NN-LM performance by\ninstead training a LM with the knowledge that we will be using a $k$NN\npost-hoc. We achieved significant improvement using our method on language\nmodeling tasks on \\texttt{WIKI-2} and \\texttt{WIKI-103}. The main phenomenon\nthat we encounter is that adding a simple L2 regularization on the activations\n(not weights) of the model, a transformer, improves the post-hoc $k$NN\nclassification performance. We explore some possible reasons for this\nimprovement. In particular, we find that the added L2 regularization seems to\nimprove the performance for high-frequency words without deteriorating the\nperformance for low frequency ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ton_J/0/1/0/all/0/1\">Jean-Francois Ton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbott_W/0/1/0/all/0/1\">Walter Talbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing out Bias: Achieving Fairness Through Training Reweighting. (arXiv:2109.08253v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08253","description":"<p>Bias in natural language processing arises primarily from models learning\ncharacteristics of the author such as gender and race when modelling tasks such\nas sentiment and syntactic parsing. This problem manifests as disparities in\nerror rates across author demographics, typically disadvantaging minority\ngroups. Existing methods for mitigating and measuring bias do not directly\naccount for correlations between author demographics and linguistic variables.\nMoreover, evaluation of bias has been inconsistent in previous work, in terms\nof dataset balance and evaluation methods. This paper introduces a very simple\nbut highly effective method for countering bias using instance reweighting,\nbased on the frequency of both task labels and author demographics. We extend\nthe method in the form of a gated model which incorporates the author\ndemographic as an input, and show that while it is highly vulnerable to input\ndata bias, it provides debiased predictions through demographic input\nperturbation, and outperforms all other bias mitigation techniques when\ncombined with instance reweighting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08256","description":"<p>The importance and pervasiveness of emotions in our lives makes affective\ncomputing a tremendously important and vibrant line of work. Systems for\nautomatic emotion recognition (AER) and sentiment analysis can be facilitators\nof enormous progress (e.g., in improving public health and commerce) but also\nenablers of great harm (e.g., for suppressing dissidents and manipulating\nvoters). Thus, it is imperative that the affective computing community actively\nengage with the ethical ramifications of their creations. In this paper, I have\nsynthesized and organized information from AI Ethics and Emotion Recognition\nliterature to present fifty ethical considerations relevant to AER. Notably,\nthe sheet fleshes out assumptions hidden in how AER is commonly framed, and in\nthe choices often made regarding the data, method, and evaluation. Special\nattention is paid to the implications of AER on privacy and social groups. The\nobjective of the sheet is to facilitate and encourage more thoughtfulness on\nwhy to automate, how to automate, and how to judge success well before the\nbuilding of AER systems. Additionally, the sheet acts as a useful introductory\ndocument on emotion recognition (complementing survey articles).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-training with Few-shot Rationalization: Teacher Explanations Aid Student in Few-shot NLU. (arXiv:2109.08259v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08259","description":"<p>While pre-trained language models have obtained state-of-the-art performance\nfor several natural language understanding tasks, they are quite opaque in\nterms of their decision-making process. While some recent works focus on\nrationalizing neural predictions by highlighting salient concepts in the text\nas justifications or rationales, they rely on thousands of labeled training\nexamples for both task labels as well as an-notated rationales for every\ninstance. Such extensive large-scale annotations are infeasible to obtain for\nmany tasks. To this end, we develop a multi-task teacher-student framework\nbased on self-training language models with limited task-specific labels and\nrationales, and judicious sample selection to learn from informative\npseudo-labeled examples1. We study several characteristics of what constitutes\na good rationale and demonstrate that the neural model performance can be\nsignificantly improved by making it aware of its rationalized predictions,\nparticularly in low-resource settings. Extensive experiments in several\nbench-mark datasets demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Meghana Moorthy Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v1 [cs.AI])","link":"http://arxiv.org/abs/2109.08270","description":"<p>Language models (LMs) are sentence-completion engines trained on massive\ncorpora. LMs have emerged as a significant breakthrough in natural-language\nprocessing, providing capabilities that go far beyond sentence completion\nincluding question answering, summarization, and natural-language inference.\nWhile many of these capabilities have potential application to cognitive\nsystems, exploiting language models as a source of task knowledge, especially\nfor task learning, offers significant, near-term benefits. We introduce\nlanguage models and the various tasks to which they have been applied and then\nreview methods of knowledge extraction from language models. The resulting\nanalysis outlines both the challenges and opportunities for using language\nmodels as a new knowledge source for cognitive systems. It also identifies\npossible ways to improve knowledge extraction from language models using the\ncapabilities provided by cognitive systems. Central to success will be the\nability of a cognitive agent to itself learn an abstract model of the knowledge\nimplicit in the LM as well as methods to extract high-quality knowledge\neffectively and efficiently. To illustrate, we introduce a hypothetical robot\nagent and describe how language models could extend its task knowledge and\nimprove its performance and the kinds of knowledge and methods the agent can\nuse to exploit the knowledge within a language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wray_R/0/1/0/all/0/1\">Robert E. Wray, III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_J/0/1/0/all/0/1\">James R. Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_J/0/1/0/all/0/1\">John E. Laird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis. (arXiv:2109.08306v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08306","description":"<p>Aspect-based sentiment analysis (ABSA) is an emerging fine-grained sentiment\nanalysis task that aims to extract aspects, classify corresponding sentiment\npolarities and find opinions as the causes of sentiment. The latest research\ntends to solve the ABSA task in a unified way with end-to-end frameworks. Yet,\nthese frameworks get fine-tuned from downstream tasks without any task-adaptive\nmodification. Specifically, they do not use task-related knowledge well or\nexplicitly model relations between aspect and opinion terms, hindering them\nfrom better performance. In this paper, we propose SentiPrompt to use sentiment\nknowledge enhanced prompts to tune the language model in the unified framework.\nWe inject sentiment knowledge regarding aspects, opinions, and polarities into\nprompt and explicitly model term relations via constructing consistency and\npolarity judgment templates from the ground truth triplets. Experimental\nresults demonstrate that our approach can outperform strong baselines on\nTriplet Extraction, Pair Extraction, and Aspect Term Extraction with Sentiment\nClassification by a notable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feiyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jiajun Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zirui Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongpan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal Sentiment Dataset for Video Recommendation. (arXiv:2109.08333v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08333","description":"<p>Recently, multimodal sentiment analysis has seen remarkable advance and a lot\nof datasets are proposed for its development. In general, current multimodal\nsentiment analysis datasets usually follow the traditional system of\nsentiment/emotion, such as positive, negative and so on. However, when applied\nin the scenario of video recommendation, the traditional sentiment/emotion\nsystem is hard to be leveraged to represent different contents of videos in the\nperspective of visual senses and language understanding. Based on this, we\npropose a multimodal sentiment analysis dataset, named baiDu Video Sentiment\ndataset (DuVideoSenti), and introduce a new sentiment system which is designed\nto describe the sentimental style of a video on recommendation scenery.\nSpecifically, DuVideoSenti consists of 5,630 videos which displayed on Baidu,\neach video is manually annotated with a sentimental style label which describes\nthe user's real feeling of a video. Furthermore, we propose UNIMO as our\nbaseline for DuVideoSenti. Experimental results show that DuVideoSenti brings\nnew challenges to multimodal sentiment analysis, and could be used as a new\nbenchmark for evaluating approaches designed for video understanding and\nmultimodal fusion. We also expect our proposed DuVideoSenti could further\nimprove the development of multimodal sentiment analysis and its application to\nvideo recommendations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-adaptive Pre-training of Language Models with Word Embedding Regularization. (arXiv:2109.08354v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08354","description":"<p>Pre-trained language models (PTLMs) acquire domain-independent linguistic\nknowledge through pre-training with massive textual resources. Additional\npre-training is effective in adapting PTLMs to domains that are not well\ncovered by the pre-training corpora. Here, we focus on the static word\nembeddings of PTLMs for domain adaptation to teach PTLMs domain-specific\nmeanings of words. We propose a novel fine-tuning process: task-adaptive\npre-training with word embedding regularization (TAPTER). TAPTER runs\nadditional pre-training by making the static word embeddings of a PTLM close to\nthe word embeddings obtained in the target domain with fastText. TAPTER\nrequires no additional corpus except for the training data of the downstream\ntask. We confirmed that TAPTER improves the performance of the standard\nfine-tuning and the task-adaptive pre-training on BioASQ (question answering in\nthe biomedical domain) and on SQuAD (the Wikipedia domain) when their\npre-training corpora were not dominated by in-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kyosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_S/0/1/0/all/0/1\">Sen Yoshida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Linguistic Context for Language Model Compression. (arXiv:2109.08359v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08359","description":"<p>A computationally expensive and memory intensive neural network lies behind\nthe recent success of language representation learning. Knowledge distillation,\na major technique for deploying such a vast language model in resource-scarce\nenvironments, transfers the knowledge on individual word representations\nlearned without restrictions. In this paper, inspired by the recent\nobservations that language representations are relatively positioned and have\nmore semantic knowledge as a whole, we present a new knowledge distillation\nobjective for language representation learning that transfers the contextual\nknowledge via two types of relationships across representations: Word Relation\nand Layer Transforming Relation. Unlike other recent distillation techniques\nfor the language models, our contextual distillation does not have any\nrestrictions on architectural changes between teacher and student. We validate\nthe effectiveness of our method on challenging benchmarks of language\nunderstanding tasks, not only in architectures of various sizes, but also in\ncombination with DynaBERT, the recently proposed adaptive size pruning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Geondo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongman Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeQA: A Question Answering Dataset for Source Code Comprehension. (arXiv:2109.08365v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08365","description":"<p>We propose CodeQA, a free-form question answering dataset for the purpose of\nsource code comprehension: given a code snippet and a question, a textual\nanswer is required to be generated. CodeQA contains a Java dataset with 119,778\nquestion-answer pairs and a Python dataset with 70,085 question-answer pairs.\nTo obtain natural and faithful questions and answers, we implement syntactic\nrules and semantic analysis to transform code comments into question-answer\npairs. We present the construction process and conduct systematic analysis of\nour dataset. Experiment results achieved by several neural baselines on our\ndataset are shown and discussed. While research on question-answering and\nmachine reading comprehension develops rapidly, few prior work has drawn\nattention to code question answering. This new dataset can serve as a useful\nresearch benchmark for source code comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To be Closer: Learning to Link up Aspects with Opinions. (arXiv:2109.08382v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08382","description":"<p>Dependency parse trees are helpful for discovering the opinion words in\naspect-based sentiment analysis (ABSA). However, the trees obtained from\noff-the-shelf dependency parsers are static, and could be sub-optimal in ABSA.\nThis is because the syntactic trees are not designed for capturing the\ninteractions between opinion words and aspect words. In this work, we aim to\nshorten the distance between aspects and corresponding opinion words by\nlearning an aspect-centric tree structure. The aspect and opinion words are\nexpected to be closer along such tree structure compared to the standard\ndependency parse tree. The learning process allows the tree structure to\nadaptively correlate the aspect and opinion words, enabling us to better\nidentify the polarity in the ABSA task. We conduct experiments on five\naspect-based sentiment datasets, and the proposed model significantly\noutperforms recent strong baselines. Furthermore, our thorough analysis\ndemonstrates the average distance between aspect and opinion words are\nshortened by at least 19% on the standard SemEval Restaurant14 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lejian Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"reproducing \"ner and pos when nothing is capitalized\". (arXiv:2109.08396v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08396","description":"<p>Capitalization is an important feature in many NLP tasks such as Named Entity\nRecognition (NER) or Part of Speech Tagging (POS). We are trying to reproduce\nresults of paper which shows how to mitigate a significant performance drop\nwhen casing is mismatched between training and testing data. In particular we\nshow that lowercasing 50% of the dataset provides the best performance,\nmatching the claims of the original paper. We also show that we got slightly\nlower performance in almost all experiments we have tried to reproduce,\nsuggesting that there might be some hidden factors impacting our performance.\nLastly, we make all of our work available in a public github repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuster_A/0/1/0/all/0/1\">Andreas Kuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipek_J/0/1/0/all/0/1\">Jakub Filipek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muppirala_V/0/1/0/all/0/1\">Viswa Virinchi Muppirala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers. (arXiv:2109.08406v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08406","description":"<p>Despite the success of fine-tuning pretrained language encoders like BERT for\ndownstream natural language understanding (NLU) tasks, it is still poorly\nunderstood how neural networks change after fine-tuning. In this work, we use\ncentered kernel alignment (CKA), a method for comparing learned\nrepresentations, to measure the similarity of representations in task-tuned\nmodels across layers. In experiments across twelve NLU tasks, we discover a\nconsistent block diagonal structure in the similarity of representations within\nfine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of\nearlier and later layers, but not between them. The similarity of later layer\nrepresentations implies that later layers only marginally contribute to task\nperformance, and we verify in experiments that the top few layers of fine-tuned\nTransformers can be discarded without hurting performance, even with no further\ntuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haokun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis. (arXiv:2109.08412v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08412","description":"<p>Chatbot is increasingly thriving in different domains, however, because of\nunexpected discourse complexity and training data sparseness, its potential\ndistrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff\n(MHCH), predicting chatbot failure and enabling human-algorithm collaboration\nto enhance chatbot quality, has attracted increasing attention from industry\nand academia. In this study, we propose a novel model, Role-Selected Sharing\nNetwork (RSSN), which integrates both dialogue satisfaction estimation and\nhandoff prediction in one multi-task learning framework. Unlike prior efforts\nin dialog mining, by utilizing local user satisfaction as a bridge, global\nsatisfaction detector and handoff predictor can effectively exchange critical\ninformation. Specifically, we decouple the relation and interaction between the\ntwo tasks by the role information after the shared encoder. Extensive\nexperiments on two public datasets demonstrate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaisong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Guoxiu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuoren Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Students on Sesame Street: What Order-Aware Matrix Embeddings Can Learn from BERT. (arXiv:2109.08449v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08449","description":"<p>Large-scale pretrained language models (PreLMs) are revolutionizing natural\nlanguage processing across all benchmarks. However, their sheer size is\nprohibitive in low-resource or large-scale applications. While common\napproaches reduce the size of PreLMs via same-architecture distillation or\npruning, we explore distilling PreLMs into more efficient order-aware embedding\nmodels. Our results on the GLUE benchmark show that embedding-centric students,\nwhich have learned from BERT, yield scores comparable to DistilBERT on QQP and\nRTE, often match or exceed the scores of ELMo, and only fall behind on\ndetecting linguistic acceptability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuber_I/0/1/0/all/0/1\">Isabelle Cuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1\">Christoph Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nolscher_H/0/1/0/all/0/1\">Henrik Ferdinand N&#xf6;lscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonderecker_A/0/1/0/all/0/1\">Angelina Sonderecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Unification for Logic Reasoning over Natural Language. (arXiv:2109.08460v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08460","description":"<p>Automated Theorem Proving (ATP) deals with the development of computer\nprograms being able to show that some conjectures (queries) are a logical\nconsequence of a set of axioms (facts and rules). There exists several\nsuccessful ATPs where conjectures and axioms are formally provided (e.g.\nformalised as First Order Logic formulas). Recent approaches, such as (Clark et\nal., 2020), have proposed transformer-based architectures for deriving\nconjectures given axioms expressed in natural language (English). The\nconjecture is verified through a binary text classifier, where the transformers\nmodel is trained to predict the truth value of a conjecture given the axioms.\nThe RuleTaker approach of (Clark et al., 2020) achieves appealing results both\nin terms of accuracy and in the ability to generalize, showing that when the\nmodel is trained with deep enough queries (at least 3 inference steps), the\ntransformers are able to correctly answer the majority of queries (97.6%) that\nrequire up to 5 inference steps. In this work we propose a new architecture,\nnamely the Neural Unifier, and a relative training procedure, which achieves\nstate-of-the-art results in term of generalisation, showing that mimicking a\nwell-known inference procedure, the backward chaining, it is possible to answer\ndeep queries even when the model is trained only on shallow ones. The approach\nis demonstrated in experiments using a diverse set of benchmark data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Picco_G/0/1/0/all/0/1\">Gabriele Picco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1\">Hoang Thanh Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbodio_M/0/1/0/all/0/1\">Marco Luca Sbodio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_V/0/1/0/all/0/1\">Vanessa Lopez Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08475","description":"<p>Visual dialog, which aims to hold a meaningful conversation with humans about\na given image, is a challenging task that requires models to reason the complex\ndependencies among visual content, dialog history, and current questions. Graph\nneural networks are recently applied to model the implicit relations between\nobjects in an image or dialog. However, they neglect the importance of 1)\ncoreference relations among dialog history and dependency relations between\nwords for the question representation; and 2) the representation of the image\nbased on the fully represented question. Therefore, we propose a novel\nrelation-aware graph-over-graph network (GoG) for visual dialog. Specifically,\nGoG consists of three sequential graphs: 1) H-Graph, which aims to capture\ncoreference relations among dialog history; 2) History-aware Q-Graph, which\naims to fully understand the question through capturing dependency relations\nbetween words based on coreference resolution on the dialog history; and 3)\nQuestion-aware I-Graph, which aims to capture the relations between objects in\nan image based on fully question representation. As an additional feature\nrepresentation module, we add GoG to the existing visual dialogue model.\nExperimental results show that our model outperforms the strong baseline in\nboth generative and discriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation. (arXiv:2109.08478v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08478","description":"<p>Visual dialogue is a challenging task since it needs to answer a series of\ncoherent questions on the basis of understanding the visual environment.\nPrevious studies focus on the implicit exploration of multimodal co-reference\nby implicitly attending to spatial image features or object-level image\nfeatures but neglect the importance of locating the objects explicitly in the\nvisual content, which is associated with entities in the textual content.\nTherefore, in this paper we propose a {\\bf M}ultimodal {\\bf I}ncremental {\\bf\nT}ransformer with {\\bf V}isual {\\bf G}rounding, named MITVG, which consists of\ntwo key parts: visual grounding and multimodal incremental transformer. Visual\ngrounding aims to explicitly locate related objects in the image guided by\ntextual entities, which helps the model exclude the visual content that does\nnot need attention. On the basis of visual grounding, the multimodal\nincremental transformer encodes the multi-turn dialogue history combined with\nvisual scene step by step according to the order of the dialogue and then\ngenerates a contextually and visually coherent response. Experimental results\non the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the\nproposed model, which achieves comparable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Entity-Centric Questions Challenge Dense Retrievers. (arXiv:2109.08535v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08535","description":"<p>Open-domain question answering has exploded in popularity recently due to the\nsuccess of dense retrieval models, which have surpassed sparse models using\nonly a few supervised training examples. However, in this paper, we demonstrate\ncurrent dense models are not yet the holy grail of retrieval. We first\nconstruct EntityQuestions, a set of simple, entity-rich questions based on\nfacts from Wikidata (e.g., \"Where was Arve Furset born?\"), and observe that\ndense retrievers drastically underperform sparse methods. We investigate this\nissue and uncover that dense retrievers can only generalize to common entities\nunless the question pattern is explicitly observed during training. We discuss\ntwo simple solutions towards addressing this critical problem. First, we\ndemonstrate that data augmentation is unable to fix the generalization problem.\nSecond, we argue a more robust passage encoder helps facilitate better question\nadaptation using specialized question encoders. We hope our work can shed light\non the challenges in creating a robust, universal dense retriever that works\nwell across different input distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sciavolino_C/0/1/0/all/0/1\">Christopher Sciavolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules. (arXiv:2109.08544v1 [cs.AI])","link":"http://arxiv.org/abs/2109.08544","description":"<p>One of the challenges faced by conversational agents is their inability to\nidentify unstated presumptions of their users' commands, a task trivial for\nhumans due to their common sense. In this paper, we propose a zero-shot\ncommonsense reasoning system for conversational agents in an attempt to achieve\nthis. Our reasoner uncovers unstated presumptions from user commands satisfying\na general template of if-(state), then-(action), because-(goal). Our reasoner\nuses a state-of-the-art transformer-based generative commonsense knowledge base\n(KB) as its source of background knowledge for reasoning. We propose a novel\nand iterative knowledge query mechanism to extract multi-hop reasoning chains\nfrom the neural KB which uses symbolic logic rules to significantly reduce the\nsearch space. Similar to any KBs gathered to date, our commonsense KB is prone\nto missing knowledge. Therefore, we propose to conversationally elicit the\nmissing knowledge from human users with our novel dynamic question generation\nstrategy, which generates and presents contextualized queries to human users.\nWe evaluate the model with a user study with human users that achieves a 35%\nhigher success rate compared to SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1\">Forough Arabshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jennifer Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slot Filling for Biomedical Information Extraction. (arXiv:2109.08564v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08564","description":"<p>Information Extraction (IE) from text refers to the task of extracting\nstructured knowledge from unstructured text. The task typically consists of a\nseries of sub-tasks such as Named Entity Recognition and Relation Extraction.\nSourcing entity and relation type specific training data is a major bottleneck\nin the above sub-tasks.In this work we present a slot filling approach to the\ntask of biomedical IE, effectively replacing the need for entity and\nrelation-specific training data, allowing to deal with zero-shot settings. We\nfollow the recently proposed paradigm of coupling a Tranformer-based\nbi-encoder, Dense Passage Retrieval, with a Transformer-based reader model to\nextract relations from biomedical text. We assemble a biomedical slot filling\ndataset for both retrieval and reading comprehension and conduct a series of\nexperiments demonstrating that our approach outperforms a number of simpler\nbaselines. We also evaluate our approach end-to-end for standard as well as\nzero-shot settings. Our work provides a fresh perspective on how to solve\nbiomedical IE tasks, in the absence of relevant training data. Our code, models\nand pretrained data are available at\nhttps://github.com/healx/biomed-slot-filling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papanikolaou_Y/0/1/0/all/0/1\">Yannis Papanikolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_F/0/1/0/all/0/1\">Francine Bennett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Multitask Learning for Low-Resource AbstractiveSummarization. (arXiv:2109.08565v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08565","description":"<p>This paper explores the effect of using multitask learning for abstractive\nsummarization in the context of small training corpora. In particular, we\nincorporate four different tasks (extractive summarization, language modeling,\nconcept detection, and paraphrase detection) both individually and in\ncombination, with the goal of enhancing the target task of abstractive\nsummarization via multitask learning. We show that for many task combinations,\na model trained in a multitask setting outperforms a model trained only for\nabstractive summarization, with no additional summarization data introduced.\nAdditionally, we do a comprehensive search and find that certain tasks (e.g.\nparaphrase detection) consistently benefit abstractive summarization, not only\nwhen combined with other tasks but also when using different architectures and\ntraining corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magooda_A/0/1/0/all/0/1\">Ahmed Magooda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elaraby_M/0/1/0/all/0/1\">Mohamed Elaraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization. (arXiv:2109.08569v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08569","description":"<p>This paper explores three simple data manipulation techniques (synthesis,\naugmentation, curriculum) for improving abstractive summarization models\nwithout the need for any additional data. We introduce a method of data\nsynthesis with paraphrasing, a data augmentation technique with sample mixing,\nand curriculum learning with two new difficulty metrics based on specificity\nand abstractiveness. We conduct experiments to show that these three techniques\ncan help improve abstractive summarization across two summarization models and\ntwo different small datasets. Furthermore, we show that these techniques can\nimprove performance when applied in isolation and when combined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magooda_A/0/1/0/all/0/1\">Ahmed Magooda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchy-Aware T5 with Path-Adaptive Mask Mechanism for Hierarchical Text Classification. (arXiv:2109.08585v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08585","description":"<p>Hierarchical Text Classification (HTC), which aims to predict text labels\norganized in hierarchical space, is a significant task lacking in investigation\nin natural language processing. Existing methods usually encode the entire\nhierarchical structure and fail to construct a robust label-dependent model,\nmaking it hard to make accurate predictions on sparse lower-level labels and\nachieving low Macro-F1. In this paper, we propose a novel PAMM-HiA-T5 model for\nHTC: a hierarchy-aware T5 model with path-adaptive mask mechanism that not only\nbuilds the knowledge of upper-level labels into low-level ones but also\nintroduces path dependency information in label prediction. Specifically, we\ngenerate a multi-level sequential label structure to exploit hierarchical\ndependency across different levels with Breadth-First Search (BFS) and T5\nmodel. To further improve label dependency prediction within each path, we then\npropose an original path-adaptive mask mechanism (PAMM) to identify the label's\npath information, eliminating sources of noises from other paths. Comprehensive\nexperiments on three benchmark datasets show that our novel PAMM-HiA-T5 model\ngreatly outperforms all state-of-the-art HTC approaches especially in Macro-F1.\nThe ablation studies show that the improvements mainly come from our innovative\napproach instead of T5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yihua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhaoming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guiquan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Commonsense help in detecting Sarcasm?. (arXiv:2109.08588v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08588","description":"<p>Sarcasm detection is important for several NLP tasks such as sentiment\nidentification in product reviews, user feedback, and online forums. It is a\nchallenging task requiring a deep understanding of language, context, and world\nknowledge. In this paper, we investigate whether incorporating commonsense\nknowledge helps in sarcasm detection. For this, we incorporate commonsense\nknowledge into the prediction process using a graph convolution network with\npre-trained language model embeddings as input. Our experiments with three\nsarcasm detection datasets indicate that the approach does not outperform the\nbaseline model. We perform an exhaustive set of experiments to analyze where\ncommonsense support adds value and where it hurts classification. Our\nimplementation is publicly available at:\nhttps://github.com/brcsomnath/commonsense-sarcasm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Transformers for Job Expression Extraction and Classification in a Low-Resource Setting. (arXiv:2109.08597v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08597","description":"<p>In this paper, we explore possible improvements of transformer models in a\nlow-resource setting. In particular, we present our approaches to tackle the\nfirst two of three subtasks of the MEDDOPROF competition, i.e., the extraction\nand classification of job expressions in Spanish clinical texts. As neither\nlanguage nor domain experts, we experiment with the multilingual XLM-R\ntransformer model and tackle these low-resource information extraction tasks as\nsequence-labeling problems. We explore domain- and language-adaptive\npretraining, transfer learning and strategic datasplits to boost the\ntransformer model. Our results show strong improvements using these methods by\nup to 5.3 F1 points compared to a fine-tuned XLM-R model. Our best models\nachieve 83.2 and 79.3 F1 for the first two tasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strotgen_J/0/1/0/all/0/1\">Jannik Str&#xf6;tgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The futility of STILTs for the classification of lexical borrowings in Spanish. (arXiv:2109.08607v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08607","description":"<p>The first edition of the IberLEF 2021 shared task on automatic detection of\nborrowings (ADoBo) focused on detecting lexical borrowings that appeared in the\nSpanish press and that have recently been imported into the Spanish language.\nIn this work, we tested supplementary training on intermediate labeled-data\ntasks (STILTs) from part of speech (POS), named entity recognition (NER),\ncode-switching, and language identification approaches to the classification of\nborrowings at the token level using existing pre-trained transformer-based\nlanguage models. Our extensive experimental results suggest that STILTs do not\nprovide any improvement over direct fine-tuning of multilingual models.\nHowever, multilingual models trained on small subsets of languages perform\nreasonably better than multilingual BERT but not as good as multilingual\nRoBERTa for the given dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_J/0/1/0/all/0/1\">Javier de la Rosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Scrubbing of Demographic Information for Text Classification. (arXiv:2109.08613v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08613","description":"<p>Contextual representations learned by language models can often encode\nundesirable attributes, like demographic associations of the users, while being\ntrained for an unrelated target task. We aim to scrub such undesirable\nattributes and learn fair representations while maintaining performance on the\ntarget task. In this paper, we present an adversarial learning framework\n\"Adversarial Scrubber\" (ADS), to debias contextual representations. We perform\ntheoretical analysis to show that our framework converges without leaking\ndemographic information under certain conditions. We extend previous evaluation\ntechniques by evaluating debiasing performance using Minimum Description Length\n(MDL) probing. Experimental evaluations on 8 datasets show that ADS generates\nrepresentations with minimal information about demographic attributes while\nbeing maximally informative about the target task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier B. Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CKMorph: A Comprehensive Morphological Analyzer for Central Kurdish. (arXiv:2109.08615v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08615","description":"<p>A morphological analyzer, which is a significant component of many natural\nlanguage processing applications especially for morphologically rich languages,\ndivides an input word into all its composing morphemes and identifies their\nmorphological roles. In this paper, we introduce a comprehensive morphological\nanalyzer for Central Kurdish (CK), a low-resourced language with a rich\nmorphology. Building upon the limited existing literature, we first assembled\nand systematically categorized a comprehensive collection of the morphological\nand morphophonological rules of the language. Additionally, we collected and\nmanually labeled a generative lexicon containing nearly 10,000 verb, noun and\nadjective stems, named entities, and other types of word stems. We used these\nrule sets and resources to implement CKMorph Analyzer based on finite-state\ntransducers. In order to provide a benchmark for future research, we collected,\nmanually labeled, and publicly shared test sets for evaluating accuracy and\ncoverage of the analyzer. CKMorph was able to correctly analyze 95.9% of the\naccuracy test set, containing 1,000 CK words morphologically analyzed according\nto the context. Moreover, CKMorph gave at least one analysis for 95.5% of 4.22M\nCK tokens of the coverage test set. The demonstration of the application and\nresources including CK verb database and test sets are openly accessible at\nhttps://github.com/CKMorph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naserzade_M/0/1/0/all/0/1\">Morteza Naserzade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmudi_A/0/1/0/all/0/1\">Aso Mahmudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veisi_H/0/1/0/all/0/1\">Hadi Veisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1\">Hawre Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MohammadAmini_M/0/1/0/all/0/1\">Mohammad MohammadAmini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications. (arXiv:2109.08627v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08627","description":"<p>Sentence-level Quality estimation (QE) of machine translation is\ntraditionally formulated as a regression task, and the performance of QE models\nis typically measured by Pearson correlation with human labels. Recent QE\nmodels have achieved previously-unseen levels of correlation with human\njudgments, but they rely on large multilingual contextualized language models\nthat are computationally expensive and make them infeasible for real-world\napplications. In this work, we evaluate several model compression techniques\nfor QE and find that, despite their popularity in other NLP tasks, they lead to\npoor performance in this regression setting. We observe that a full model\nparameterization is required to achieve SoTA results in a regression task.\nHowever, we argue that the level of expressiveness of a model in a continuous\nrange is unnecessary given the downstream applications of QE, and show that\nreframing QE as a classification problem and evaluating QE models using\nclassification metrics would better reflect their actual performance in\nreal-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?. (arXiv:2109.08634v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08634","description":"<p>Models designed for intelligent process automation are required to be capable\nof grounding user interface elements. This task of interface element grounding\nis centred on linking instructions in natural language to their target\nreferents. Even though BERT and similar pre-trained language models have\nexcelled in several NLP tasks, their use has not been widely explored for the\nUI grounding domain. This work concentrates on testing and probing the\ngrounding abilities of three different transformer-based models: BERT, RoBERTa\nand LayoutLM. Our primary focus is on these models' spatial reasoning skills,\ngiven their importance in this domain. We observe that LayoutLM has a promising\nadvantage for applications in this domain, even though it was created for a\ndifferent original purpose (representing scanned documents): the learned\nspatial features appear to be transferable to the UI grounding setting,\nespecially as they demonstrate the ability to discriminate between target\ndirections in natural language instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubba_K/0/1/0/all/0/1\">Krishna Dubba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Weiwei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Measuring of Readability to Improve Documents Accessibility for Arabic Language Learners. (arXiv:2109.08648v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08648","description":"<p>This paper presents an approach based on supervised machine learning methods\nto build a classifier that can identify text complexity in order to present\nArabic language learners with texts suitable to their levels. The approach is\nbased on machine learning classification methods to discriminate between the\ndifferent levels of difficulty in reading and understanding a text. Several\nmodels were trained on a large corpus mined from online Arabic websites and\nmanually annotated. The model uses both Count and TF-IDF representations and\napplies five machine learning algorithms; Multinomial Naive Bayes, Bernoulli\nNaive Bayes, Logistic Regression, Support Vector Machine and Random Forest,\nusing unigrams and bigrams features. With the goal of extracting the text\ncomplexity, the problem is usually addressed by formulating the level\nidentification as a classification task. Experimental results showed that\nn-gram features could be indicative of the reading level of a text and could\nsubstantially improve performance, and showed that SVM and Multinomial Naive\nBayes are the most accurate in predicting the complexity level. Best results\nwere achieved using TF-IDF Vectors trained by a combination of word-based\nunigrams and bigrams with an overall accuracy of 87.14% over four classes of\ncomplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bessou_S/0/1/0/all/0/1\">Sadik Bessou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenni_G/0/1/0/all/0/1\">Ghozlane Chenni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Handling Unconstrained User Preferences in Dialogue. (arXiv:2109.08650v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08650","description":"<p>A user input to a schema-driven dialogue information navigation system, such\nas venue search, is typically constrained by the underlying database which\nrestricts the user to specify a predefined set of preferences, or slots,\ncorresponding to the database fields. We envision a more natural information\nnavigation dialogue interface where a user has flexibility to specify\nunconstrained preferences that may not match a predefined schema. We propose to\nuse information retrieval from unstructured knowledge to identify entities\nrelevant to a user request. We update the Cambridge restaurants database with\nunstructured knowledge snippets (reviews and information from the web) for each\nof the restaurants and annotate a set of query-snippet pairs with a relevance\nlabel. We use the annotated dataset to train and evaluate snippet relevance\nclassifiers, as a proxy to evaluating recommendation accuracy. We show that\nwith a pretrained transformer model as an encoder, an unsupervised/supervised\nclassifier achieves a weighted F1 of .661/.856.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Suraj Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanchev_S/0/1/0/all/0/1\">Svetlana Stoyanchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Primer: Searching for Efficient Transformers for Language Modeling. (arXiv:2109.08668v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08668","description":"<p>Large Transformer models have been central to recent advances in natural\nlanguage processing. The training and inference costs of these models, however,\nhave grown rapidly and become prohibitively expensive. Here we aim to reduce\nthe costs of Transformers by searching for a more efficient variant. Compared\nto previous approaches, our search is performed at a lower level, over the\nprimitives that define a Transformer TensorFlow program. We identify an\narchitecture, named Primer, that has a smaller training cost than the original\nTransformer and other variants for auto-regressive language modeling. Primer's\nimprovements can be mostly attributed to two simple modifications: squaring\nReLU activations and adding a depthwise convolution layer after each Q, K, and\nV projection in self-attention.\n</p>\n<p>Experiments show Primer's gains over Transformer increase as compute scale\ngrows and follow a power law with respect to quality at optimal model sizes. We\nalso verify empirically that Primer can be dropped into different codebases to\nsignificantly speed up training without additional tuning. For example, at a\n500M parameter size, Primer improves the original T5 architecture on C4\nauto-regressive language modeling, reducing the training cost by 4X.\nFurthermore, the reduced training cost means Primer needs much less compute to\nreach a target one-shot performance. For instance, in a 1.9B parameter\nconfiguration similar to GPT-3 XL, Primer uses 1/3 of the training compute to\nachieve the same one-shot performance as Transformer. We open source our models\nand several comparisons in T5 to help with reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David R. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manke_W/0/1/0/all/0/1\">Wojciech Ma&#x144;ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering. (arXiv:2109.08678v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08678","description":"<p>Existing KBQA approaches, despite achieving strong performance on i.i.d. test\ndata, often struggle in generalizing to questions involving unseen KB schema\nitems. Prior ranking-based approaches have shown some success in\ngeneralization, but suffer from the coverage issue. We present RnG-KBQA, a\nRank-and-Generate approach for KBQA, which remedies the coverage issue with a\ngeneration model while preserving a strong generalization capability. Our\napproach first uses a contrastive ranker to rank a set of candidate logical\nforms obtained by searching over the knowledge graph. It then introduces a\ntailored generation model conditioned on the question and the top-ranked\ncandidates to compose the final logical form. We achieve new state-of-the-art\nresults on GrailQA and WebQSP datasets. In particular, our method surpasses the\nprior state-of-the-art by a large margin on the GrailQA leaderboard. In\naddition, RnG-KBQA outperforms all prior approaches on the popular WebQSP\nbenchmark, even including the ones that use the oracle entity linking. The\nexperimental results demonstrate the effectiveness of the interplay between\nranking and generation, which leads to the superior performance of our proposed\napproach across all settings with especially strong improvements in zero-shot\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Global Informativeness in Open Domain Keyphrase Extraction. (arXiv:2004.13639v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13639","description":"<p>Open-domain KeyPhrase Extraction (KPE) aims to extract keyphrases from\ndocuments without domain or quality restrictions, e.g., web pages with variant\ndomains and qualities. Recently, neural methods have shown promising results in\nmany KPE tasks due to their powerful capacity for modeling contextual semantics\nof the given documents. However, we empirically show that most neural KPE\nmethods prefer to extract keyphrases with good phraseness, such as short and\nentity-style n-grams, instead of globally informative keyphrases from\nopen-domain documents. This paper presents JointKPE, an open-domain KPE\narchitecture built on pre-trained language models, which can capture both local\nphraseness and global informativeness when extracting keyphrases. JointKPE\nlearns to rank keyphrases by estimating their informativeness in the entire\ndocument and is jointly trained on the keyphrase chunking task to guarantee the\nphraseness of keyphrase candidates. Experiments on two large KPE datasets with\ndiverse domains, OpenKP and KP20k, demonstrate the effectiveness of JointKPE on\ndifferent pre-trained variants in open-domain scenarios. Further analyses\nreveal the significant advantages of JointKPE in predicting long and non-entity\nkeyphrases, which are challenging for previous neural KPE methods. Our code is\npublicly available at https://github.com/thunlp/BERT-KPE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Si Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jie Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.15779","description":"<p>Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning\nBenchmark) at https://aka.ms/BLURB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Michael Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Document Clustering Based on BERT with Data Augment. (arXiv:2011.08523v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.08523","description":"<p>Contrastive learning is a promising approach to unsupervised learning, as it\ninherits the advantages of well-studied deep models without a dedicated and\ncomplex model design. In this paper, based on bidirectional encoder\nrepresentations from transformers, we propose self-supervised contrastive\nlearning (SCL) as well as few-shot contrastive learning (FCL) with unsupervised\ndata augmentation (UDA) for text clustering. SCL outperforms state-of-the-art\nunsupervised clustering approaches for short texts and those for long texts in\nterms of several clustering evaluation measures. FCL achieves performance close\nto supervised learning, and FCL with UDA further improves the performance for\nshort texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To what extent do human explanations of model behavior align with actual model behavior?. (arXiv:2012.13354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.13354","description":"<p>Given the increasingly prominent role NLP models (will) play in our lives, it\nis important for human expectations of model behavior to align with actual\nmodel behavior. Using Natural Language Inference (NLI) as a case study, we\ninvestigate the extent to which human-generated explanations of models'\ninference decisions align with how models actually make these decisions. More\nspecifically, we define three alignment metrics that quantify how well natural\nlanguage explanations align with model sensitivity to input words, as measured\nby integrated gradients. Then, we evaluate eight different models (the base and\nlarge versions of BERT, RoBERTa and ELECTRA, as well as anRNN and bag-of-words\nmodel), and find that the BERT-base model has the highest alignment with\nhuman-generated explanations, for all alignment metrics. Focusing in on\ntransformers, we find that the base versions tend to have higher alignment with\nhuman-generated explanations than their larger counterparts, suggesting that\nincreasing the number of model parameters leads, in some cases, to worse\nalignment with human explanations. Finally, we find that a model's alignment\nwith human explanations is not predicted by the model's accuracy, suggesting\nthat accuracy and alignment are complementary ways to evaluate models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1\">Grusha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yixin Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15283","description":"<p>While pre-trained language models (PTLMs) have achieved noticeable success on\nmany NLP tasks, they still struggle for tasks that require event temporal\nreasoning, which is essential for event-centric applications. We present a\ncontinual pre-training approach that equips PTLMs with targeted knowledge about\nevent temporal relations. We design self-supervised learning objectives to\nrecover masked-out event and temporal indicators and to discriminate sentences\nfrom their corrupted counterparts (where event or temporal indicators got\nreplaced). By further pre-training a PTLM with these objectives jointly, we\nreinforce its attention to event and temporal information, yielding enhanced\ncapability on event temporal reasoning. This effective continual pre-training\nframework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning\nperformances across five relation extraction and question answering tasks and\nachieves new or on-par state-of-the-art performances in most of our downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora. (arXiv:2012.15674v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15674","description":"<p>Recent studies have demonstrated that pre-trained cross-lingual models\nachieve impressive performance in downstream cross-lingual tasks. This\nimprovement benefits from learning a large amount of monolingual and parallel\ncorpora. Although it is generally acknowledged that parallel corpora are\ncritical for improving the model performance, existing methods are often\nconstrained by the size of parallel corpora, especially for low-resource\nlanguages. In this paper, we propose ERNIE-M, a new training method that\nencourages the model to align the representation of multiple languages with\nmonolingual corpora, to overcome the constraint that the parallel corpus size\nplaces on the model performance. Our key insight is to integrate\nback-translation into the pre-training process. We generate pseudo-parallel\nsentence pairs on a monolingual corpus to enable the learning of semantic\nalignments between different languages, thereby enhancing the semantic modeling\nof cross-lingual models. Experimental results show that ERNIE-M outperforms\nexisting cross-lingual models and delivers new state-of-the-art results in\nvarious cross-lingual downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xuan Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Politics via Contextualized Discourse Processing. (arXiv:2012.15784v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15784","description":"<p>Politicians often have underlying agendas when reacting to events. Arguments\nin contexts of various events reflect a fairly consistent set of agendas for a\ngiven entity. In spite of recent advances in Pretrained Language Models (PLMs),\nthose text representations are not designed to capture such nuanced patterns.\nIn this paper, we propose a Compositional Reader model consisting of encoder\nand composer modules, that attempts to capture and leverage such information to\ngenerate more effective representations for entities, issues, and events. These\nrepresentations are contextualized by tweets, press releases, issues, news\narticles, and participating entities. Our model can process several documents\nat once and generate composed representations for multiple entities over\nseveral issues or events. Via qualitative and quantitative empirical analysis,\nwe show that these representations are meaningful and effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pujari_R/0/1/0/all/0/1\">Rajkumar Pujari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Everything in Order? A Simple Way to Order Sentences. (arXiv:2104.07064v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07064","description":"<p>The task of organizing a shuffled set of sentences into a coherent text has\nbeen used to evaluate a machine's understanding of causal and temporal\nrelations. We formulate the sentence ordering task as a conditional\ntext-to-marker generation problem. We present Reorder-BART (Re-BART) that\nleverages a pre-trained Transformer-based model to identify a coherent order\nfor a given set of shuffled sentences. The model takes a set of shuffled\nsentences with sentence-specific markers as input and generates a sequence of\nposition markers of the sentences in the ordered text. Re-BART achieves the\nstate-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and\nKendall's tau ($\\tau$). We perform evaluations in a zero-shot setting,\nshowcasing that our model is able to generalize well across other datasets. We\nadditionally perform several experiments to understand the functioning and\nlimitations of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Learning for Generation with Long Source Sequences. (arXiv:2104.07545v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07545","description":"<p>One of the challenges for current sequence to sequence (seq2seq) models is\nprocessing long sequences, such as those in summarization and document level\nmachine translation tasks. These tasks require the model to reason at the token\nlevel as well as the sentence and paragraph level. We design and study a new\nHierarchical Attention Transformer-based architecture (HAT) that outperforms\nstandard Transformers on several sequence to sequence tasks. Furthermore, our\nmodel achieves state-of-the-art ROUGE scores on four summarization tasks,\nincluding PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms\ndocument-level machine translation baseline on the WMT20 English to German\ntranslation task. We investigate what the hierarchical layers learn by\nvisualizing the hierarchical encoder-decoder attention. Finally, we study\nhierarchical learning on encoder-only pre-training and analyze its performance\non classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohde_T/0/1/0/all/0/1\">Tobias Rohde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinhan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04484","description":"<p>Deep learning algorithms have shown promising results in visual question\nanswering (VQA) tasks, but a more careful look reveals that they often do not\nunderstand the rich signal they are being fed with. To understand and better\nmeasure the generalization capabilities of VQA systems, we look at their\nrobustness to counterfactually augmented data. Our proposed augmentations are\ndesigned to make a focused intervention on a specific property of the question\nsuch that the answer changes. Using these augmentations, we propose a new\nrobustness measure, Robustness to Augmented Data (RAD), which measures the\nconsistency of model predictions between original and augmented examples.\nThrough extensive experimentation, we show that RAD, unlike classical accuracy\nmeasures, can quantify when state-of-the-art systems are not robust to\ncounterfactuals. We find substantial failure cases which reveal that current\nVQA systems are still brittle. Finally, we connect between robustness and\ngeneralization, demonstrating the predictive power of RAD for performance on\nunseen augmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1\">Daniel Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1\">Itai Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2106.13375","description":"<p>Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1\">Richard Rogahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul N. Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics Sheets for AI Tasks. (arXiv:2107.01183v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2107.01183","description":"<p>Recent innovations such as Datasheets for Datasets and Model Cards for Model\nReporting have made useful contributions to furthering ethical research. Yet,\nseveral high-profile events, such as the mass testing of emotion recognition\nsystems on vulnerable sub-populations, have highlighted how technology will\noften lead to more adverse outcomes for those that are already marginalized. In\nthis paper, I will make a case for thinking about ethical considerations not\njust at the level of individual models and datasets, but also at the level of\nAI tasks. I will present a new form of such an effort, Ethics Sheets for AI\nTasks, dedicated to fleshing out the assumptions and ethical considerations\nhidden in how a task is commonly framed and in the choices we make regarding\nthe data, method, and evaluation. Finally, I will provide an example ethics\nsheet for automatic emotion recognition. Ethics sheets are a mechanism to\ndocument ethical considerations \\textit{before} building datasets and systems.\nSuch pre-production activities (e.g., ethics analyses) and associated artifacts\n(e.g., accessible documentation) are crucial for responsible AI: for\ncommunicating risks to all stakeholders, to help decision and policy making,\nand for developing more effective post-production documents such as Data Sheets\nand Model Cards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pattern-based Acquisition of Scientific Entities from Scholarly Article Titles. (arXiv:2109.00199v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.00199","description":"<p>We describe a rule-based approach for the automatic acquisition of salient\nscientific entities from Computational Linguistics (CL) scholarly article\ntitles. Two observations motivated the approach: (i) noting salient aspects of\nan article's contribution in its title; and (ii) pattern regularities capturing\nthe salient terms that could be expressed in a set of rules. Only those\nlexico-syntactic patterns were selected that were easily recognizable, occurred\nfrequently, and positionally indicated a scientific entity type. The rules were\ndeveloped on a collection of 50,237 CL titles covering all articles in the ACL\nAnthology. In total, 19,799 research problems, 18,111 solutions, 20,033\nresources, 1,059 languages, 6,878 tools, and 21,687 methods were extracted at\nan average precision of 75%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">Soeren Auer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00627","description":"<p>Contextual knowledge is important for real-world automatic speech recognition\n(ASR) applications. In this paper, a novel tree-constrained pointer generator\n(TCPGen) component is proposed that incorporates such knowledge as a list of\nbiasing words into both attention-based encoder-decoder and transducer\nend-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing\nwords into an efficient prefix tree to serve as its symbolic input and creates\na neural shortcut between the tree and the final ASR output distribution to\nfacilitate recognising biasing words during decoding. Systems were trained and\nevaluated on the Librispeech corpus where biasing words were extracted at the\nscales of an utterance, a chapter, or a book to simulate different application\nscenarios. Experimental results showed that TCPGen consistently improved word\nerror rates (WERs) compared to the baselines, and in particular, achieved\nsignificant WER reductions on the biasing words. TCPGen is highly efficient: it\ncan handle 5,000 biasing words and distractors and only add a small overhead to\nmemory use and computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03570","description":"<p>This work presents biomedical and clinical language models for Spanish by\nexperimenting with different pretraining choices, such as masking at word and\nsubword level, varying the vocabulary size and testing with domain data,\nlooking for better language representations. Interestingly, in the absence of\nenough clinical data to train a model from scratch, we applied mixed-domain\npretraining and cross-domain transfer approaches to generate a performant\nbio-clinical model suitable for real-world clinical data. We evaluated our\nmodels on Named Entity Recognition (NER) tasks for biomedical documents and\nchallenging hospital discharge reports. When compared against the competitive\nmBERT and BETO models, we outperform them in all NER tasks by a significant\nmargin. Finally, we studied the impact of the model's vocabulary on the NER\nperformances by offering an interesting vocabulary-centric analysis. The\nresults confirm that domain-specific pretraining is fundamental to achieving\nhigher performances in downstream NER tasks, even within a mid-resource\nscenario. To the best of our knowledge, we provide the first biomedical and\nclinical transformer-based pretrained language models for Spanish, intending to\nboost native Spanish NLP applications in biomedicine. Our best models are\nfreely available in the HuggingFace hub: https://huggingface.co/BSC-TeMU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04114","description":"<p>We apply imitation learning (IL) to tackle the NMT exposure bias problem with\nerror-correcting oracles, and evaluate an SMT lattice-based oracle which,\ndespite its excellent performance in an unconstrained oracle translation task,\nturned out to be too pruned and idiosyncratic to serve as the oracle for IL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hormann_L/0/1/0/all/0/1\">Luca Hormann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cartography Active Learning. (arXiv:2109.04282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04282","description":"<p>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)\nalgorithm that exploits the behavior of the model on individual instances\nduring training as a proxy to find the most informative instances for labeling.\nCAL is inspired by data maps, which were recently proposed to derive insights\ninto dataset quality (Swayamdipta et al., 2020). We compare our method on\npopular text classification tasks to commonly used AL strategies, which instead\nrely on post-training behavior. We demonstrate that CAL is competitive to other\ncommon AL methods, showing that training dynamics derived from small seed data\ncan be successfully used for AL. We provide insights into our new AL method by\nanalyzing batch-level statistics utilizing the data maps. Our results further\nshow that CAL results in a more data-efficient learning strategy, achieving\ncomparable or better results with considerably less training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05179","description":"<p>Generating high quality question-answer pairs is a hard but meaningful task.\nAlthough previous works have achieved great results on answer-aware question\ngeneration, it is difficult to apply them into practical application in the\neducation field. This paper for the first time addresses the question-answer\npair generation task on the real-world examination data, and proposes a new\nunified framework on RACE. To capture the important information of the input\npassage we first automatically generate(rather than extracting) keyphrases,\nthus this task is reduced to keyphrase-question-answer triplet joint\ngeneration. Accordingly, we propose a multi-agent communication model to\ngenerate and optimize the question and keyphrases iteratively, and then apply\nthe generated question and keyphrases to guide the generation of answers. To\nestablish a solid benchmark, we build our model on the strong generative\npre-training model. Experimental results show that our model makes great\nbreakthroughs in the question-answer pair generation task. Moreover, we make a\ncomprehensive analysis on our model, suggesting new directions for this\nchallenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_F/0/1/0/all/0/1\">Fanyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankNAS: Efficient Neural Architecture Search by Pairwise Ranking. (arXiv:2109.07383v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07383","description":"<p>This paper addresses the efficiency challenge of Neural Architecture Search\n(NAS) by formulating the task as a ranking problem. Previous methods require\nnumerous training examples to estimate the accurate performance of\narchitectures, although the actual goal is to find the distinction between\n\"good\" and \"bad\" candidates. Here we do not resort to performance predictors.\nInstead, we propose a performance ranking method (RankNAS) via pairwise\nranking. It enables efficient architecture search using much fewer training\nexamples. Moreover, we develop an architecture selection method to prune the\nsearch space and concentrate on more promising candidates. Extensive\nexperiments on machine translation and language modeling tasks show that\nRankNAS can design high-performance architectures while being orders of\nmagnitude faster than state-of-the-art NAS systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiangnan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xia Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changliang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"TANet: A new Paradigm for Global Face Super-resolution via Transformer-CNN Aggregation Network. (arXiv:2109.08174v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08174","description":"<p>Recently, face super-resolution (FSR) methods either feed whole face image\ninto convolutional neural networks (CNNs) or utilize extra facial priors (e.g.,\nfacial parsing maps, facial landmarks) to focus on facial structure, thereby\nmaintaining the consistency of the facial structure while restoring facial\ndetails. However, the limited receptive fields of CNNs and inaccurate facial\npriors will reduce the naturalness and fidelity of the reconstructed face. In\nthis paper, we propose a novel paradigm based on the self-attention mechanism\n(i.e., the core of Transformer) to fully explore the representation capacity of\nthe facial structure feature. Specifically, we design a Transformer-CNN\naggregation network (TANet) consisting of two paths, in which one path uses\nCNNs responsible for restoring fine-grained facial details while the other\nutilizes a resource-friendly Transformer to capture global information by\nexploiting the long-distance visual relation modeling. By aggregating the\nfeatures from the above two paths, the consistency of global facial structure\nand fidelity of local facial detail restoration are strengthened\nsimultaneously. Experimental results of face reconstruction and recognition\nverify that the proposed method can significantly outperform the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanduo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KATANA: Simple Post-Training Robustness Using Test Time Augmentations. (arXiv:2109.08191v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08191","description":"<p>Although Deep Neural Networks (DNNs) achieve excellent performance on many\nreal-world tasks, they are highly vulnerable to adversarial attacks. A leading\ndefense against such attacks is adversarial training, a technique in which a\nDNN is trained to be robust to adversarial attacks by introducing adversarial\nnoise to its input. This procedure is effective but must be done during the\ntraining phase. In this work, we propose a new simple and easy-to-use\ntechnique, KATANA, for robustifying an existing pretrained DNN without\nmodifying its weights. For every image, we generate N randomized Test Time\nAugmentations (TTAs) by applying diverse color, blur, noise, and geometric\ntransforms. Next, we utilize the DNN's logits output to train a simple random\nforest classifier to predict the real class label. Our strategy achieves\nstate-of-the-art adversarial robustness on diverse attacks with minimal\ncompromise on the natural images' classification. We test KATANA also against\ntwo adaptive white-box attacks and it shows excellent results when combined\nwith adversarial training. Code is available in\nhttps://github.com/giladcohen/KATANA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_G/0/1/0/all/0/1\">Gilad Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision. (arXiv:2109.08203v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08203","description":"<p>In this paper I investigate the effect of random seed selection on the\naccuracy when using popular deep learning architectures for computer vision. I\nscan a large amount of seeds (up to $10^4$) on CIFAR 10 and I also scan fewer\nseeds on Imagenet using pre-trained models to investigate large scale datasets.\nThe conclusions are that even if the variance is not very large, it is\nsurprisingly easy to find an outlier that performs much better or much worse\nthan the average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1\">David Picard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Divide-and-Merge Point Cloud Clustering Algorithm for LiDAR Panoptic Segmentation. (arXiv:2109.08224v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08224","description":"<p>Clustering objects from the LiDAR point cloud is an important research\nproblem with many applications such as autonomous driving. To meet the\nreal-time requirement, existing research proposed to apply the\nconnected-component-labeling (CCL) technique on LiDAR spherical range image\nwith a heuristic condition to check if two neighbor points are connected.\nHowever, LiDAR range image is different from a binary image which has a\ndeterministic condition to tell if two pixels belong to the same component. The\nheuristic condition used on the LiDAR range image only works empirically, which\nsuggests the LiDAR clustering algorithm should be robust to potential failures\nof the empirical heuristic condition. To overcome this challenge, this paper\nproposes a divide-and-merge LiDAR clustering algorithm. This algorithm firstly\nconducts clustering in each evenly divided local region, then merges the local\nclustered small components by voting on edge point pairs. Assuming there are\n$N$ LiDAR points of objects in total with $m$ divided local regions, the time\ncomplexity of the proposed algorithm is $O(N)+O(m^2)$. A smaller $m$ means the\nvoting will involve more neighbor points, but the time complexity will become\nlarger. So the $m$ controls the trade-off between the time complexity and the\nclustering accuracy. A proper $m$ helps the proposed algorithm work in\nreal-time as well as maintain good performance. We evaluate the\ndivide-and-merge clustering algorithm on the SemanticKITTI panoptic\nsegmentation benchmark by cascading it with a state-of-the-art semantic\nsegmentation model. The final performance evaluated through the leaderboard\nachieves the best among all published methods. The proposed algorithm is\nimplemented with C++ and wrapped as a python function. It can be easily used\nwith the modern deep learning framework in python.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery. (arXiv:2109.08227v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08227","description":"<p>We introduce the task of stereo video reconstruction or, equivalently,\n2D-to-3D video conversion for minimally invasive surgical video. We design and\nimplement a series of end-to-end U-Net-based solutions for this task by varying\nthe input (single frame vs. multiple consecutive frames), loss function (MSE,\nMAE, or perceptual losses), and network architecture. We evaluate these\nsolutions by surveying ten experts - surgeons who routinely perform endoscopic\nsurgery. We run two separate reader studies: one evaluating individual frames\nand the other evaluating fully reconstructed 3D video played on a VR headset.\nIn the first reader study, a variant of the U-Net that takes as input multiple\nconsecutive video frames and outputs the missing view performs best. We draw\ntwo conclusions from this outcome. First, motion information coming from\nmultiple past frames is crucial in recreating stereo vision. Second, the\nproposed U-Net variant can indeed exploit such motion information for solving\nthis task. The result from the second study further confirms the effectiveness\nof the proposed U-Net variant. The surgeons reported that they could\nsuccessfully perceive depth from the reconstructed 3D video clips. They also\nexpressed a clear preference for the reconstructed 3D video over the original\n2D video. These two reader studies strongly support the usefulness of the\nproposed task of stereo reconstruction for minimally invasive surgical video\nand indicate that deep learning is a promising approach to this task. Finally,\nwe identify two automatic metrics, LPIPS and DISTS, that are strongly\ncorrelated with expert judgement and that could serve as proxies for the latter\nin future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Brundyn_A/0/1/0/all/0/1\">Annika Brundyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Swanson_J/0/1/0/all/0/1\">Jesse Swanson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kondziolka_D/0/1/0/all/0/1\">Doug Kondziolka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oermann_E/0/1/0/all/0/1\">Eric Oermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI. (arXiv:2109.08238v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08238","description":"<p>We present the Habitat-Matterport 3D (HM3D) dataset. HM3D is a large-scale\ndataset of 1,000 building-scale 3D reconstructions from a diverse set of\nreal-world locations. Each scene in the dataset consists of a textured 3D mesh\nreconstruction of interiors such as multi-floor residences, stores, and other\nprivate indoor spaces.\n</p>\n<p>HM3D surpasses existing datasets available for academic research in terms of\nphysical scale, completeness of the reconstruction, and visual fidelity. HM3D\ncontains 112.5k m^2 of navigable space, which is 1.4 - 3.7x larger than other\nbuilding-scale datasets such as MP3D and Gibson. When compared to existing\nphotorealistic 3D datasets such as Replica, MP3D, Gibson, and ScanNet, images\nrendered from HM3D have 20 - 85% higher visual fidelity w.r.t. counterpart\nimages captured with real cameras, and HM3D meshes have 34 - 91% fewer\nartifacts due to incomplete surface reconstruction.\n</p>\n<p>The increased scale, fidelity, and diversity of HM3D directly impacts the\nperformance of embodied AI agents trained using it. In fact, we find that HM3D\nis `pareto optimal' in the following sense -- agents trained to perform\nPointGoal navigation on HM3D achieve the highest performance regardless of\nwhether they are evaluated on HM3D, Gibson, or MP3D. No similar claim can be\nmade about training on other datasets. HM3D-trained PointNav agents achieve\n100% performance on Gibson-test dataset, suggesting that it might be time to\nretire that episode dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh K. Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1\">Alex Clegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1\">John Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1\">Eric Undersander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1\">Wojciech Galuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westbury_A/0/1/0/all/0/1\">Andrew Westbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel X. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yili Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A computationally efficient framework for vector representation of persistence diagrams. (arXiv:2109.08239v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08239","description":"<p>In Topological Data Analysis, a common way of quantifying the shape of data\nis to use a persistence diagram (PD). PDs are multisets of points in\n$\\mathbb{R}^2$ computed using tools of algebraic topology. However, this\nmulti-set structure limits the utility of PDs in applications. Therefore, in\nrecent years efforts have been directed towards extracting informative and\nefficient summaries from PDs to broaden the scope of their use for machine\nlearning tasks. We propose a computationally efficient framework to convert a\nPD into a vector in $\\mathbb{R}^n$, called a vectorized persistence block\n(VPB). We show that our representation possesses many of the desired properties\nof vector-based summaries such as stability with respect to input noise, low\ncomputational cost and flexibility. Through simulation studies, we demonstrate\nthe effectiveness of VPBs in terms of performance and computational cost within\nvarious learning tasks, namely clustering, classification and change point\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kit C. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islambekov_U/0/1/0/all/0/1\">Umar Islambekov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luchinsky_A/0/1/0/all/0/1\">Alexey Luchinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanders_R/0/1/0/all/0/1\">Rebecca Sanders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards agricultural autonomy: crop row detection under varying field conditions using deep learning. (arXiv:2109.08247v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08247","description":"<p>This paper presents a novel metric to evaluate the robustness of deep\nlearning based semantic segmentation approaches for crop row detection under\ndifferent field conditions encountered by a field robot. A dataset with ten\nmain categories encountered under various field conditions was used for\ntesting. The effect on these conditions on the angular accuracy of crop row\ndetection was compared. A deep convolutional encoder decoder network is\nimplemented to predict crop row masks using RGB input images. The predicted\nmask is then sent to a post processing algorithm to extract the crop rows. The\ndeep learning model was found to be robust against shadows and growth stages of\nthe crop while the performance was reduced under direct sunlight, increasing\nweed density, tramlines and discontinuities in crop rows when evaluated with\nthe novel metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Rajitha de Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1\">Grzegorz Cielniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are we ready for beyond-application high-volume data? The Reeds robot perception benchmark dataset. (arXiv:2109.08250v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08250","description":"<p>This paper presents a dataset, called Reeds, for research on robot perception\nalgorithms. The dataset aims to provide demanding benchmark opportunities for\nalgorithms, rather than providing an environment for testing\napplication-specific solutions. A boat was selected as a logging platform in\norder to provide highly dynamic kinematics. The sensor package includes six\nhigh-performance vision sensors, two long-range lidars, radar, as well as GNSS\nand an IMU. The spatiotemporal resolution of sensors were maximized in order to\nprovide large variations and flexibility in the data, offering evaluation at a\nlarge number of different resolution presets based on the resolution found in\nother datasets. Reeds also provides means of a fair and reproducible comparison\nof algorithms, by running all evaluations on a common server backend. As the\ndataset contains massive-scale data, the evaluation principle also serves as a\nway to avoid moving data unnecessarily.\n</p>\n<p>It was also found that naive evaluation of algorithms, where each evaluation\nis computed sequentially, was not practical as the fetch and decode task of\neach frame would not scale well. Instead, each frame is only decoded once and\nthen fed to all algorithms in parallel, including for GPU-based algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benderius_O/0/1/0/all/0/1\">Ola Benderius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christian Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanch_K/0/1/0/all/0/1\">Krister Blanch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network Based Lidar Gesture Recognition for Realtime Robot Teleoperation. (arXiv:2109.08263v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08263","description":"<p>We propose a novel low-complexity lidar gesture recognition system for mobile\nrobot control robust to gesture variation. Our system uses a modular approach,\nconsisting of a pose estimation module and a gesture classifier. Pose estimates\nare predicted from lidar scans using a Convolutional Neural Network trained\nusing an existing stereo-based pose estimation system. Gesture classification\nis accomplished using a Long Short-Term Memory network and uses a sequence of\nestimated body poses as input to predict a gesture. Breaking down the pipeline\ninto two modules reduces the dimensionality of the input, which could be lidar\nscans, stereo imagery, or any other modality from which body keypoints can be\nextracted, making our system lightweight and suitable for mobile robot control\nwith limited computing power. The use of lidar contributes to the robustness of\nthe system, allowing it to operate in most outdoor conditions, to be\nindependent of lighting conditions, and for input to be detected 360 degrees\naround the robot. The lidar-based pose estimator and gesture classifier use\ndata augmentation and automated labeling techniques, requiring a minimal amount\nof data collection and avoiding the need for manual labeling. We report\nexperimental results for each module of our system and demonstrate its\neffectiveness by testing it in a real-world robot teleoperation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chamorro_S/0/1/0/all/0/1\">Simon Chamorro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Collier_J/0/1/0/all/0/1\">Jack Collier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grondin_F/0/1/0/all/0/1\">Fran&#xe7;ois Grondin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Visual Similarity Based Personalized Tourist Attraction Recommendation Using Geo-Tagged Photos. (arXiv:2109.08275v1 [cs.MM])","link":"http://arxiv.org/abs/2109.08275","description":"<p>Geo-tagged photo based tourist attraction recommendation can discover users'\ntravel preferences from their taken photos, so as to recommend suitable tourist\nattractions to them. However, existing visual content based methods cannot\nfully exploit the user and tourist attraction information of photos to extract\nvisual features, and do not differentiate the significances of different\nphotos. In this paper, we propose multi-level visual similarity based\npersonalized tourist attraction recommendation using geo-tagged photos (MEAL).\nMEAL utilizes the visual contents of photos and interaction behavior data to\nobtain the final embeddings of users and tourist attractions, which are then\nused to predict the visit probabilities. Specifically, by crossing the user and\ntourist attraction information of photos, we define four visual similarity\nlevels and introduce a corresponding quintuplet loss to embed the visual\ncontents of photos. In addition, to capture the significances of different\nphotos, we exploit the self-attention mechanism to obtain the visual\nrepresentations of users and tourist attractions. We conducted experiments on a\ndataset crawled from Flickr, and the experimental results proved the advantage\nof this method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_D/0/1/0/all/0/1\">Dandan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shanshan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gencai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Hierarchical Dual Consistency for Semi-Supervised Left Atrium Segmentation on Cross-Domain Data. (arXiv:2109.08311v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08311","description":"<p>Semi-supervised learning provides great significance in left atrium (LA)\nsegmentation model learning with insufficient labelled data. Generalising\nsemi-supervised learning to cross-domain data is of high importance to further\nimprove model robustness. However, the widely existing distribution difference\nand sample mismatch between different data domains hinder the generalisation of\nsemi-supervised learning. In this study, we alleviate these problems by\nproposing an Adaptive Hierarchical Dual Consistency (AHDC) for the\nsemi-supervised LA segmentation on cross-domain data. The AHDC mainly consists\nof a Bidirectional Adversarial Inference module (BAI) and a Hierarchical Dual\nConsistency learning module (HDC). The BAI overcomes the difference of\ndistributions and the sample mismatch between two different domains. It mainly\nlearns two mapping networks adversarially to obtain two matched domains through\nmutual adaptation. The HDC investigates a hierarchical dual learning paradigm\nfor cross-domain semi-supervised segmentation based on the obtained matched\ndomains. It mainly builds two dual-modelling networks for mining the\ncomplementary information in both intra-domain and inter-domain. For the\nintra-domain learning, a consistency constraint is applied to the\ndual-modelling targets to exploit the complementary modelling information. For\nthe inter-domain learning, a consistency constraint is applied to the LAs\nmodelled by two dual-modelling networks to exploit the complementary knowledge\namong different data domains. We demonstrated the performance of our proposed\nAHDC on four 3D late gadolinium enhancement cardiac MR (LGE-CMR) datasets from\ndifferent centres and a 3D CT dataset. Compared to other state-of-the-art\nmethods, our proposed AHDC achieved higher segmentation accuracy, which\nindicated its capability in the cross-domain semi-supervised LA segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Heye Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohiaddin_R/0/1/0/all/0/1\">Raad Mohiaddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_T/0/1/0/all/0/1\">Tom Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Firmin_D/0/1/0/all/0/1\">David Firmin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keegan_J/0/1/0/all/0/1\">Jennifer Keegan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mass Segmentation in Automated 3-D Breast Ultrasound Using Dual-Path U-net. (arXiv:2109.08330v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08330","description":"<p>Automated 3-D breast ultrasound (ABUS) is a newfound system for breast\nscreening that has been proposed as a supplementary modality to mammography for\nbreast cancer detection. While ABUS has better performance in dense breasts,\nreading ABUS images is exhausting and time-consuming. So, a computer-aided\ndetection system is necessary for interpretation of these images. Mass\nsegmentation plays a vital role in the computer-aided detection systems and it\naffects the overall performance. Mass segmentation is a challenging task\nbecause of the large variety in size, shape, and texture of masses. Moreover,\nan imbalanced dataset makes segmentation harder. A novel mass segmentation\napproach based on deep learning is introduced in this paper. The deep network\nthat is used in this study for image segmentation is inspired by U-net, which\nhas been used broadly for dense segmentation in recent years. The system's\nperformance was determined using a dataset of 50 masses including 38 malign and\n12 benign lesions. The proposed segmentation method attained a mean Dice of\n0.82 which outperformed a two-stage supervised edge-based method with a mean\nDice of 0.74 and an adaptive region growing method with a mean Dice of 0.65.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fayyaz_H/0/1/0/all/0/1\">Hamed Fayyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kozegar_E/0/1/0/all/0/1\">Ehsan Kozegar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1\">Tao Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soryani_M/0/1/0/all/0/1\">Mohsen Soryani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoGG3D-Net: Locally Guided Global Descriptor Learning for 3D Place Recognition. (arXiv:2109.08336v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08336","description":"<p>Retrieval-based place recognition is an efficient and effective solution for\nenabling re-localization within a pre-built map or global data association for\nSimultaneous Localization and Mapping (SLAM). The accuracy of such an approach\nis heavily dependent on the quality of the extracted scene-level\nrepresentation. While end-to-end solutions, which learn a global descriptor\nfrom input point clouds, have demonstrated promising results, such approaches\nare limited in their ability to enforce desirable properties at the local\nfeature level. In this paper, we demonstrate that the inclusion of an\nadditional training signal (local consistency loss) can guide the network to\nlearning local features which are consistent across revisits, hence leading to\nmore repeatable global descriptors resulting in an overall improvement in place\nrecognition performance. We formulate our approach in an end-to-end trainable\narchitecture called LoGG3D-Net. Experiments on two large-scale public\nbenchmarks (KITTI and MulRan) show that our method achieves mean $F1_{max}$\nscores of $0.939$ and $0.968$ on KITTI and MulRan, respectively while operating\nin near real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vidanapathirana_K/0/1/0/all/0/1\">Kavisha Vidanapathirana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Milad Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1\">Peyman Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraFormer: Graph Convolution Transformer for 3D Pose Estimation. (arXiv:2109.08364v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08364","description":"<p>Exploiting relations among 2D joints plays a crucial role yet remains\nsemi-developed in 2D-to-3D pose estimation. To alleviate this issue, we propose\nGraFormer, a novel transformer architecture combined with graph convolution for\n3D pose estimation. The proposed GraFormer comprises two repeatedly stacked\ncore modules, GraAttention and ChebGConv block. GraAttention enables all 2D\njoints to interact in global receptive field without weakening the graph\nstructure information of joints, which introduces vital features for later\nmodules. Unlike vanilla graph convolutions that only model the apparent\nrelationship of joints, ChebGConv block enables 2D joints to interact in the\nhigh-order sphere, which formulates their hidden implicit relations. We\nempirically show the superiority of GraFormer through conducting extensive\nexperiments across popular benchmarks. Specifically, GraFormer outperforms\nstate of the art on Human3.6M dataset while using 18$\\%$ parameters. The code\nis available at https://github.com/Graformer/GraFormer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weixi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yunjie Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jianbin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bio-Inspired Audio-Visual Cues Integration for Visual Attention Prediction. (arXiv:2109.08371v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08371","description":"<p>Visual Attention Prediction (VAP) methods simulates the human selective\nattention mechanism to perceive the scene, which is significant and imperative\nin many vision tasks. Most existing methods only consider visual cues, while\nneglect the accompanied audio information, which can provide complementary\ninformation for the scene understanding. In fact, there exists a strong\nrelation between auditory and visual cues, and humans generally perceive the\nsurrounding scene by simultaneously sensing these cues. Motivated by this, a\nbio-inspired audio-visual cues integration method is proposed for the VAP task,\nwhich explores the audio modality to better predict the visual attention map by\nassisting vision modality. The proposed method consists of three parts: 1)\naudio-visual encoding, 2) audio-visual location, and 3) multi-cues aggregation\nparts. Firstly, a refined SoundNet architecture is adopted to encode audio\nmodality for obtaining corresponding features, and a modified 3D ResNet-50\narchitecture is employed to learn visual features, containing both spatial\nlocation and temporal motion information. Secondly, an audio-visual location\npart is devised to locate the sound source in the visual scene by learning the\ncorrespondence between audio-visual information. Thirdly, a multi-cues\naggregation part is devised to adaptively aggregate audio-visual information\nand center-bias prior to generate the final visual attention map. Extensive\nexperiments are conducted on six challenging audiovisual eye-tracking datasets,\nincluding DIEM, AVAD, Coutrot1, Coutrot2, SumMe, and ETMD, which shows\nsignificant superiority over state-of-the-art visual attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Hailong Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering. (arXiv:2109.08379v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08379","description":"<p>Generating portrait images by controlling the motions of existing faces is an\nimportant task of great consequence to social media industries. For easy use\nand intuitive control, semantically meaningful and fully disentangled\nparameters should be used as modifications. However, many existing techniques\ndo not provide such fine-grained controls or use indirect editing methods i.e.\nmimic motions of other individuals. In this paper, a Portrait Image Neural\nRenderer (PIRenderer) is proposed to control the face motions with the\nparameters of three-dimensional morphable face models (3DMMs). The proposed\nmodel can generate photo-realistic portrait images with accurate movements\naccording to intuitive modifications. Experiments on both direct and indirect\nediting tasks demonstrate the superiority of this model. Meanwhile, we further\nextend this model to tackle the audio-driven facial reenactment task by\nextracting sequential motions from audio inputs. We show that our model can\ngenerate coherent videos with convincing movements from only a single reference\nimage and a driving audio stream. Our source code is available at\nhttps://github.com/RenYurui/PIRender.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yurui Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Thomas H. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Snapping for Guided Multi-View Visualization Design. (arXiv:2109.08384v1 [cs.HC])","link":"http://arxiv.org/abs/2109.08384","description":"<p>Visual information displays are typically composed of multiple visualizations\nthat are used to facilitate an understanding of the underlying data. A common\nexample are dashboards, which are frequently used in domains such as finance,\nprocess monitoring and business intelligence. However, users may not be aware\nof existing guidelines and lack expert design knowledge when composing such\nmulti-view visualizations. In this paper, we present semantic snapping, an\napproach to help non-expert users design effective multi-view visualizations\nfrom sets of pre-existing views. When a particular view is placed on a canvas,\nit is \"aligned\" with the remaining views -- not with respect to its geometric\nlayout, but based on aspects of the visual encoding itself, such as how data\ndimensions are mapped to channels. Our method uses an on-the-fly procedure to\ndetect and suggest resolutions for conflicting, misleading, or ambiguous\ndesigns, as well as to provide suggestions for alternative presentations. With\nthis approach, users can be guided to avoid common pitfalls encountered when\ncomposing visualizations. Our provided examples and case studies demonstrate\nthe usefulness and validity of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kristiansen_Y/0/1/0/all/0/1\">Yngve S. Kristiansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrison_L/0/1/0/all/0/1\">Laura Garrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruckner_S/0/1/0/all/0/1\">Stefan Bruckner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expression Snippet Transformer for Robust Video-based Facial Expression Recognition. (arXiv:2109.08409v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08409","description":"<p>The recent success of Transformer has provided a new direction to various\nvisual understanding tasks, including video-based facial expression recognition\n(FER). By modeling visual relations effectively, Transformer has shown its\npower for describing complicated patterns. However, Transformer still performs\nunsatisfactorily to notice subtle facial expression movements, because the\nexpression movements of many videos can be too small to extract meaningful\nspatial-temporal relations and achieve robust performance. To this end, we\npropose to decompose each video into a series of expression snippets, each of\nwhich contains a small number of facial movements, and attempt to augment the\nTransformer's ability for modeling intra-snippet and inter-snippet visual\nrelations, respectively, obtaining the Expression snippet Transformer (EST). In\nparticular, for intra-snippet modeling, we devise an attention-augmented\nsnippet feature extractor (AA-SFE) to enhance the encoding of subtle facial\nmovements of each snippet by gradually attending to more salient information.\nIn addition, for inter-snippet modeling, we introduce a shuffled snippet order\nprediction (SSOP) head and a corresponding loss to improve the modeling of\nsubtle motion changes across subsequent snippets by training the Transformer to\nidentify shuffled snippet orders. Extensive experiments on four challenging\ndatasets (i.e., BU-3DFE, MMI, AFEW, and DFEW) demonstrate that our EST is\nsuperior to other CNN-based methods, obtaining state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chuanxu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Modification Attention Based Deliberation Model for Image Captioning. (arXiv:2109.08411v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08411","description":"<p>The conventional encoder-decoder framework for image captioning generally\nadopts a single-pass decoding process, which predicts the target descriptive\nsentence word by word in temporal order. Despite the great success of this\nframework, it still suffers from two serious disadvantages. Firstly, it is\nunable to correct the mistakes in the predicted words, which may mislead the\nsubsequent prediction and result in error accumulation problem. Secondly, such\na framework can only leverage the already generated words but not the possible\nfuture words, and thus lacks the ability of global planning on linguistic\ninformation. To overcome these limitations, we explore a universal two-pass\ndecoding framework, where a single-pass decoding based model serving as the\nDrafting Model first generates a draft caption according to an input image, and\na Deliberation Model then performs the polishing process to refine the draft\ncaption to a better image description. Furthermore, inspired from the\ncomplementarity between different modalities, we propose a novel Cross\nModification Attention (CMA) module to enhance the semantic expression of the\nimage features and filter out error information from the draft captions. We\nintegrate CMA with the decoder of our Deliberation Model and name it as Cross\nModification Attention based Deliberation Model (CMA-DM). We train our proposed\nframework by jointly optimizing all trainable components from scratch with a\ntrade-off coefficient. Experiments on MS COCO dataset demonstrate that our\napproach obtains significant improvements over single-pass decoding baselines\nand achieves competitive performances compared with other state-of-the-art\ntwo-pass decoding based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zheng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haichang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohui Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-Unet: Raw Image Processing with Unet. (arXiv:2109.08417v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08417","description":"<p>Medical image segmentation have drawn massive attention as it is important in\nbiomedical image analysis. Good segmentation results can assist doctors with\ntheir judgement and further improve patients' experience. Among many available\npipelines in medical image analysis, Unet is one of the most popular neural\nnetworks as it keeps raw features by adding concatenation between encoder and\ndecoder, which makes it still widely used in industrial field. In the mean\ntime, as a popular model which dominates natural language process tasks,\ntransformer is now introduced to computer vision tasks and have seen promising\nresults in object detection, image classification and semantic segmentation\ntasks. Therefore, the combination of transformer and Unet is supposed to be\nmore efficient than both methods working individually. In this article, we\npropose Transformer-Unet by adding transformer modules in raw images instead of\nfeature maps in Unet and test our network in CT82 datasets for Pancreas\nsegmentation accordingly. We form an end-to-end network and gain segmentation\nresults better than many previous Unet based algorithms in our experiment. We\ndemonstrate our network and show our experimental results in this paper\naccordingly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sha_Y/0/1/0/all/0/1\">Youyang Sha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yonghong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_X/0/1/0/all/0/1\">Xuquan Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_L/0/1/0/all/0/1\">Lei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Messing Up 3D Virtual Environments: Transferable Adversarial 3D Objects. (arXiv:2109.08465v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08465","description":"<p>In the last few years, the scientific community showed a remarkable and\nincreasing interest towards 3D Virtual Environments, training and testing\nMachine Learning-based models in realistic virtual worlds. On one hand, these\nenvironments could also become a mean to study the weaknesses of Machine\nLearning algorithms, or to simulate training settings that allow Machine\nLearning models to gain robustness to 3D adversarial attacks. On the other\nhand, their growing popularity might also attract those that aim at creating\nadversarial conditions to invalidate the benchmarking process, especially in\nthe case of public environments that allow the contribution from a large\ncommunity of people. Most of the existing Adversarial Machine Learning\napproaches are focused on static images, and little work has been done in\nstudying how to deal with 3D environments and how a 3D object should be altered\nto fool a classifier that observes it. In this paper, we study how to craft\nadversarial 3D objects by altering their textures, using a tool chain composed\nof easily accessible elements. We show that it is possible, and indeed simple,\nto create adversarial objects using off-the-shelf limited surrogate renderers\nthat can compute gradients with respect to the parameters of the rendering\nprocess, and, to a certain extent, to transfer the attacks to more advanced 3D\nengines. We propose a saliency-based attack that intersects the two classes of\nrenderers in order to focus the alteration to those texture elements that are\nestimated to be effective in the target engine, evaluating its impact in\npopular neural classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meloni_E/0/1/0/all/0/1\">Enrico Meloni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiezzi_M/0/1/0/all/0/1\">Matteo Tiezzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasqualini_L/0/1/0/all/0/1\">Luca Pasqualini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOF: Structure-Aware Line Tracking based on Optical Flow. (arXiv:2109.08466v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08466","description":"<p>Lines provide the significantly richer geometric structural information about\nthe environment than points, so lines are widely used in recent Visual Odometry\n(VO) works. Since VO with lines use line tracking results to locate and map,\nline tracking is a crucial component in VO. Although the state-of-the-art line\ntracking methods have made great progress, they are still heavily dependent on\nline detection or the predicted line segments. In order to relieve the\ndependencies described above to track line segments completely, accurately, and\nrobustly at higher computational efficiency, we propose a structure-aware Line\ntracking algorithm based entirely on Optical Flow (LOF). Firstly, we propose a\ngradient-based strategy to sample pixels on lines that are suitable for line\noptical flow calculation. Then, in order to align the lines by fully using the\nstructural relationship between the sampled points on it and effectively\nremoving the influence of sampled points on it occluded by other objects, we\npropose a two-step structure-aware line segment alignment method. Furthermore,\nwe propose a line refinement method to refine the orientation, position, and\nendpoints of the aligned line segments. Extensive experimental results\ndemonstrate that the proposed LOF outperforms the state-of-the-art performance\nin line tracking accuracy, robustness, and efficiency, which also improves the\nlocation accuracy and robustness of VO system with lines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quan_M/0/1/0/all/0/1\">Meixiang Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ActionCLIP: A New Paradigm for Video Action Recognition. (arXiv:2109.08472v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08472","description":"<p>The canonical approach to video action recognition dictates a neural model to\ndo a classic and standard 1-of-N majority vote task. They are trained to\npredict a fixed set of predefined categories, limiting their transferable\nability on new datasets with unseen concepts. In this paper, we provide a new\nperspective on action recognition by attaching importance to the semantic\ninformation of label texts rather than simply mapping them into numbers.\nSpecifically, we model this task as a video-text matching problem within a\nmultimodal learning framework, which strengthens the video representation with\nmore semantic language supervision and enables our model to do zero-shot action\nrecognition without any further labeled data or parameters requirements.\nMoreover, to handle the deficiency of label texts and make use of tremendous\nweb data, we propose a new paradigm based on this multimodal learning framework\nfor action recognition, which we dub \"pre-train, prompt and fine-tune\". This\nparadigm first learns powerful representations from pre-training on a large\namount of web image-text or video-text data. Then it makes the action\nrecognition task to act more like pre-training problems via prompt engineering.\nFinally, it end-to-end fine-tunes on target datasets to obtain strong\nperformance. We give an instantiation of the new paradigm, ActionCLIP, which\nnot only has superior and flexible zero-shot/few-shot transfer ability but also\nreaches a top performance on general action recognition task, achieving 83.8%\ntop-1 accuracy on Kinetics-400 with a ViT-B/16 as the backbone. Code is\navailable at https://github.com/sallymmx/ActionCLIP.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jiazheng Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08475","description":"<p>Visual dialog, which aims to hold a meaningful conversation with humans about\na given image, is a challenging task that requires models to reason the complex\ndependencies among visual content, dialog history, and current questions. Graph\nneural networks are recently applied to model the implicit relations between\nobjects in an image or dialog. However, they neglect the importance of 1)\ncoreference relations among dialog history and dependency relations between\nwords for the question representation; and 2) the representation of the image\nbased on the fully represented question. Therefore, we propose a novel\nrelation-aware graph-over-graph network (GoG) for visual dialog. Specifically,\nGoG consists of three sequential graphs: 1) H-Graph, which aims to capture\ncoreference relations among dialog history; 2) History-aware Q-Graph, which\naims to fully understand the question through capturing dependency relations\nbetween words based on coreference resolution on the dialog history; and 3)\nQuestion-aware I-Graph, which aims to capture the relations between objects in\nan image based on fully question representation. As an additional feature\nrepresentation module, we add GoG to the existing visual dialogue model.\nExperimental results show that our model outperforms the strong baseline in\nboth generative and discriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Including Keyword Position in Image-based Models for Act Segmentation of Historical Registers. (arXiv:2109.08477v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08477","description":"<p>The segmentation of complex images into semantic regions has seen a growing\ninterest these last years with the advent of Deep Learning. Until recently,\nmost existing methods for Historical Document Analysis focused on the visual\nappearance of documents, ignoring the rich information that textual content can\noffer. However, the segmentation of complex documents into semantic regions is\nsometimes impossible relying only on visual features and recent models embed\nboth visual and textual information. In this paper, we focus on the use of both\nvisual and textual information for segmenting historical registers into\nstructured and meaningful units such as acts. An act is a text recording\ncontaining valuable knowledge such as demographic information (baptism,\nmarriage or death) or royal decisions (donation or pardon). We propose a simple\npipeline to enrich document images with the position of text lines containing\nkey-phrases and show that running a standard image-based layout analysis system\non these images can lead to significant gains. Our experiments show that the\ndetection of acts increases from 38 % of mAP to 74 % when adding textual\ninformation, in real use-case conditions where text lines positions and content\nare extracted with an automatic recognition system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boillet_M/0/1/0/all/0/1\">M&#xe9;lodie Boillet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maarand_M/0/1/0/all/0/1\">Martin Maarand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paquet_T/0/1/0/all/0/1\">Thierry Paquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kermorvant_C/0/1/0/all/0/1\">Christopher Kermorvant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation. (arXiv:2109.08478v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08478","description":"<p>Visual dialogue is a challenging task since it needs to answer a series of\ncoherent questions on the basis of understanding the visual environment.\nPrevious studies focus on the implicit exploration of multimodal co-reference\nby implicitly attending to spatial image features or object-level image\nfeatures but neglect the importance of locating the objects explicitly in the\nvisual content, which is associated with entities in the textual content.\nTherefore, in this paper we propose a {\\bf M}ultimodal {\\bf I}ncremental {\\bf\nT}ransformer with {\\bf V}isual {\\bf G}rounding, named MITVG, which consists of\ntwo key parts: visual grounding and multimodal incremental transformer. Visual\ngrounding aims to explicitly locate related objects in the image guided by\ntextual entities, which helps the model exclude the visual content that does\nnot need attention. On the basis of visual grounding, the multimodal\nincremental transformer encodes the multi-turn dialogue history combined with\nvisual scene step by step according to the order of the dialogue and then\ngenerates a contextually and visually coherent response. Experimental results\non the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the\nproposed model, which achieves comparable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CardiSort: a convolutional neural network for cross vendor automated sorting of cardiac MR images. (arXiv:2109.08479v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08479","description":"<p>Objectives: To develop an image-based automatic deep learning method to\nclassify cardiac MR images by sequence type and imaging plane for improved\nclinical post-processing efficiency. Methods: Multi-vendor cardiac MRI studies\nwere retrospectively collected from 4 centres and 3 vendors. A two-head\nconvolutional neural network ('CardiSort') was trained to classify 35 sequences\nby imaging sequence (n=17) and plane (n=10). Single vendor training (SVT) on\nsingle centre images (n=234 patients) and multi-vendor training (MVT) with\nmulticentre images (n = 479 patients, 3 centres) was performed. Model accuracy\nwas compared to manual ground truth labels by an expert radiologist on a\nhold-out test set for both SVT and MVT. External validation of MVT\n(MVTexternal) was performed on data from 3 previously unseen magnet systems\nfrom 2 vendors (n=80 patients). Results: High sequence and plane accuracies\nwere observed for SVT (85.2% and 93.2% respectively), and MVT (96.5% and 98.1%\nrespectively) on the hold-out test set. MVTexternal yielded sequence accuracy\nof 92.7% and plane accuracy of 93.0%. There was high accuracy for common\nsequences and conventional cardiac planes. Poor accuracy was observed for\nunderrepresented classes and sequences where there was greater variability in\nacquisition parameters across centres, such as perfusion imaging. Conclusions:\nA deep learning network was developed on multivendor data to classify MRI\nstudies into component sequences and planes, with external validation. With\nrefinement, it has potential to improve workflow by enabling automated sequence\nselection, an important first step in completely automated post-processing\npipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lim_R/0/1/0/all/0/1\">Ruth P Lim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kachel_S/0/1/0/all/0/1\">Stefan Kachel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Villa_A/0/1/0/all/0/1\">Adriana DM Villa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kearney_L/0/1/0/all/0/1\">Leighton Kearney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bettencourt_N/0/1/0/all/0/1\">Nuno Bettencourt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_A/0/1/0/all/0/1\">Alistair A Young</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiribiri_A/0/1/0/all/0/1\">Amedeo Chiribiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scannell_C/0/1/0/all/0/1\">Cian M Scannell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What we see and What we don't see: Imputing Occluded Crowd Structures from Robot Sensing. (arXiv:2109.08494v1 [cs.RO])","link":"http://arxiv.org/abs/2109.08494","description":"<p>We consider the navigation of mobile robots in crowded environments, for\nwhich onboard sensing of the crowd is typically limited by occlusions. We\naddress the problem of inferring the human occupancy in the space around the\nrobot, in blind spots, beyond the range of its sensing capabilities. This\nproblem is rather unexplored in spite of the important impact it has on the\nrobot crowd navigation efficiency and safety, which requires the estimation and\nthe prediction of the crowd state around it. In this work, we propose the first\nsolution to sample predictions of possible human presence based on the state of\na fewer set of sensed people around the robot as well as previous observations\nof the crowd activity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amirian_J/0/1/0/all/0/1\">Javad Amirian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayet_J/0/1/0/all/0/1\">Jean-Bernard Hayet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettre_J/0/1/0/all/0/1\">Julien Pettre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus on Impact: Indoor Exploration with Intrinsic Motivation. (arXiv:2109.08521v1 [cs.RO])","link":"http://arxiv.org/abs/2109.08521","description":"<p>Exploration of indoor environments has recently experienced a significant\ninterest, also thanks to the introduction of deep neural agents built in a\nhierarchical fashion and trained with Deep Reinforcement Learning (DRL) on\nsimulated environments. Current state-of-the-art methods employ a dense\nextrinsic reward that requires the complete a priori knowledge of the layout of\nthe training environment to learn an effective exploration policy. However,\nsuch information is expensive to gather in terms of time and resources. In this\nwork, we propose to train the model with a purely intrinsic reward signal to\nguide exploration, which is based on the impact of the robot's actions on the\nenvironment. So far, impact-based rewards have been employed for simple tasks\nand in procedurally generated synthetic environments with countable states.\nSince the number of states observable by the agent in realistic indoor\nenvironments is non-countable, we include a neural-based density model and\nreplace the traditional count-based regularization with an estimated\npseudo-count of previously visited states. The proposed exploration approach\noutperforms DRL-based competitors relying on intrinsic rewards and surpasses\nthe agents trained with a dense extrinsic reward computed with the environment\nlayouts. We also show that a robot equipped with the proposed approach\nseamlessly adapts to point-goal navigation and real-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bigazzi_R/0/1/0/all/0/1\">Roberto Bigazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pointly-supervised 3D Scene Parsing with Viewpoint Bottleneck. (arXiv:2109.08553v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08553","description":"<p>Semantic understanding of 3D point clouds is important for various robotics\napplications. Given that point-wise semantic annotation is expensive, in this\npaper, we address the challenge of learning models with extremely sparse\nlabels. The core problem is how to leverage numerous unlabeled points. To this\nend, we propose a self-supervised 3D representation learning framework named\nviewpoint bottleneck. It optimizes a mutual-information based objective, which\nis applied on point clouds under different viewpoints. A principled analysis\nshows that viewpoint bottleneck leads to an elegant surrogate loss function\nthat is suitable for large-scale point cloud data. Compared with former arts\nbased upon contrastive learning, viewpoint bottleneck operates on the feature\ndimension instead of the sample dimension. This paradigm shift has several\nadvantages: It is easy to implement and tune, does not need negative samples\nand performs better on our goal down-streaming task. We evaluate our method on\nthe public benchmark ScanNet, under the pointly-supervised setting. We achieve\nthe best quantitative results among comparable solutions. Meanwhile we provide\nan extensive qualitative inspection on various challenging scenes. They\ndemonstrate that our models can produce fairly good scene parsing results for\nrobotics applications. Our code, data and models will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Liyi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Beiwen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guyue Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Neural Architecture Search for Imbalanced Datasets. (arXiv:2109.08580v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08580","description":"<p>Neural Architecture Search (NAS) provides state-of-the-art results when\ntrained on well-curated datasets with annotated labels. However, annotating\ndata or even having balanced number of samples can be a luxury for\npractitioners from different scientific fields, e.g., in the medical domain. To\nthat end, we propose a NAS-based framework that bears the threefold\ncontributions: (a) we focus on the self-supervised scenario, i.e., where no\nlabels are required to determine the architecture, and (b) we assume the\ndatasets are imbalanced, (c) we design each component to be able to run on a\nresource constrained setup, i.e., on a single GPU (e.g. Google Colab). Our\ncomponents build on top of recent developments in self-supervised\nlearning~\\citep{zbontar2021barlow}, self-supervised NAS~\\citep{kaplan2020self}\nand extend them for the case of imbalanced datasets. We conduct experiments on\nan (artificially) imbalanced version of CIFAR-10 and we demonstrate our\nproposed method outperforms standard neural networks, while using $27\\times$\nless parameters. To validate our assumption on a naturally imbalanced dataset,\nwe also conduct experiments on ChestMNIST and COVID-19 X-ray. The results\ndemonstrate how the proposed method can be used in imbalanced datasets, while\nit can be fully run on a single GPU. Code is available\n\\href{https://github.com/TimofeevAlex/ssnas_imbalanced}{here}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Timofeev_A/0/1/0/all/0/1\">Aleksandr Timofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1\">Grigorios G. Chrysos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevher_V/0/1/0/all/0/1\">Volkan Cevher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Generation from a Single Video Made Possible. (arXiv:2109.08591v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08591","description":"<p>Most advanced video generation and manipulation methods train on a large\ncollection of videos. As such, they are restricted to the types of video\ndynamics they train on. To overcome this limitation, GANs trained on a single\nvideo were recently proposed. While these provide more flexibility to a wide\nvariety of video dynamics, they require days to train on a single tiny input\nvideo, rendering them impractical. In this paper we present a fast and\npractical method for video generation and manipulation from a single natural\nvideo, which generates diverse high-quality video outputs within seconds (for\nbenchmark videos). Our method can be further applied to Full-HD video clips\nwithin minutes. Our approach is inspired by a recent advanced\npatch-nearest-neighbor based approach [Granot et al. 2021], which was shown to\nsignificantly outperform single-image GANs, both in run-time and in visual\nquality. Here we generalize this approach from images to videos, by casting\nclassical space-time patch-based methods as a new generative video model. We\nadapt the generative image patch nearest neighbor approach to efficiently cope\nwith the huge number of space-time patches in a single video. Our method\ngenerates more realistic and higher quality results than single-video GANs\n(confirmed by quantitative and qualitative evaluations). Moreover, it is\ndisproportionally faster (runtime reduced from several days to seconds). Other\nthan diverse video generation, we demonstrate several other challenging video\napplications, including spatio-temporal video retargeting, video structural\nanalogies and conditional video-inpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haim_N/0/1/0/all/0/1\">Niv Haim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feinstein_B/0/1/0/all/0/1\">Ben Feinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granot_N/0/1/0/all/0/1\">Niv Granot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shocher_A/0/1/0/all/0/1\">Assaf Shocher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagon_S/0/1/0/all/0/1\">Shai Bagon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A review of deep learning methods for MRI reconstruction. (arXiv:2109.08618v1 [eess.IV])","link":"http://arxiv.org/abs/2109.08618","description":"<p>Following the success of deep learning in a wide range of applications,\nneural network-based machine-learning techniques have received significant\ninterest for accelerating magnetic resonance imaging (MRI) acquisition and\nreconstruction strategies. A number of ideas inspired by deep learning\ntechniques for computer vision and image processing have been successfully\napplied to nonlinear image reconstruction in the spirit of compressed sensing\nfor accelerated MRI. Given the rapidly growing nature of the field, it is\nimperative to consolidate and summarize the large number of deep learning\nmethods that have been reported in the literature, to obtain a better\nunderstanding of the field in general. This article provides an overview of the\nrecent developments in neural-network based approaches that have been proposed\nspecifically for improving parallel imaging. A general background and\nintroduction to parallel MRI is also given from a classical view of k-space\nbased reconstruction methods. Image domain based techniques that introduce\nimproved regularizers are covered along with k-space based methods which focus\non better interpolation strategies using neural networks. While the field is\nrapidly evolving with thousands of papers published each year, in this review,\nwe attempt to cover broad categories of methods that have shown good\nperformance on publicly available data sets. Limitations and open problems are\nalso discussed and recent efforts for producing open data sets and benchmarks\nfor the community are examined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pal_A/0/1/0/all/0/1\">Arghya Pal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Vision-based UAV Landing with Collision Avoidance using Deep Learning. (arXiv:2109.08628v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08628","description":"<p>There is a risk of collision when multiple UAVs land simultaneously without\ncommunication on the same platform. This work accomplishes vision-based\nautonomous landing and uses a deep-learning-based method to realize collision\navoidance during the landing process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Tianpei Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haridevan_A/0/1/0/all/0/1\">Amal Haridevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1\">Jinjun Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitoring Indoor Activity of Daily Living Using Thermal Imaging: A Case Study. (arXiv:2109.08672v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08672","description":"<p>Monitoring indoor activities of daily living (ADLs) of a person is neither an\neasy nor an accurate process. It is subjected to dependency on sensor type,\npower supply stability, and connectivity stability without mentioning artifacts\nintroduced by the person himself. Multiple challenges have to be overcome in\nthis field, such as; monitoring the precise spatial location of the person, and\nestimating vital signs like an individuals average temperature. Privacy is\nanother domain of the problem to be thought of with care. Identifying the\npersons posture without a camera is another challenge. Posture identification\nassists in the persons fall detection. Thermal imaging could be a proper\nsolution for most of the mentioned challenges. It provides monitoring both the\npersons average temperature and spatial location while maintaining privacy. In\nthis research, we propose an IoT system for monitoring an indoor ADL using\nthermal sensor array (TSA). Three classes of ADLs are introduced, which are\ndaily activity, sleeping activity and no-activity respectively. Estimating\nperson average temperature using TSAs is introduced as well in this paper.\nResults have shown that the three activity classes can be identified as well as\nthe persons average temperature during day and night. The persons spatial\nlocation can be determined while his/her privacy is maintained as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_H/0/1/0/all/0/1\">Hassan M. Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulrazak_B/0/1/0/all/0/1\">Bessam Abdulrazak</a> (AMI-Lab Faculte des sciences, Universite de Sherbrooke)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic PointGoal Navigation via Auxiliary Losses and Information Bottleneck. (arXiv:2109.08677v1 [cs.CV])","link":"http://arxiv.org/abs/2109.08677","description":"<p>We propose a novel architecture and training paradigm for training realistic\nPointGoal Navigation -- navigating to a target coordinate in an unseen\nenvironment under actuation and sensor noise without access to ground-truth\nlocalization. Specifically, we find that the primary challenge under this\nsetting is learning localization -- when stripped of idealized localization,\nagents fail to stop precisely at the goal despite reliably making progress\ntowards it. To address this we introduce a set of auxiliary losses to help the\nagent learn localization. Further, we explore the idea of treating the precise\nlocation of the agent as privileged information -- it is unavailable during\ntest time, however, it is available during training time in simulation. We\ngrant the agent restricted access to ground-truth localization readings during\ntraining via an information bottleneck. Under this setting, the agent incurs a\npenalty for using this privileged information, encouraging the agent to only\nleverage this information when it is crucial to learning. This enables the\nagent to first learn navigation and then learn localization instead of\nconflating these two objectives in training. We evaluate our proposed method\nboth in a semi-idealized (noiseless simulation without Compass+GPS) and\nrealistic (addition of noisy simulation) settings. Specifically, our method\noutperforms existing baselines on the semi-idealized setting by 18\\%/21\\%\nSPL/Success and by 15\\%/20\\% SPL in the realistic setting. Our improved Success\nand SPL metrics indicate our agent's improved ability to accurately\nself-localize while maintaining a strong navigation policy. Our implementation\ncan be found at https://github.com/NicoGrande/habitat-pointnav-via-ib.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grande_G/0/1/0/all/0/1\">Guillermo Grande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance. (arXiv:1909.10837v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.10837","description":"<p>Spiking neural network (SNN) is interesting both theoretically and\npractically because of its strong bio-inspiration nature and potentially\noutstanding energy efficiency. Unfortunately, its development has fallen far\nbehind the conventional deep neural network (DNN), mainly because of difficult\ntraining and lack of widely accepted hardware experiment platforms. In this\npaper, we show that a deep temporal-coded SNN can be trained easily and\ndirectly over the benchmark datasets CIFAR10 and ImageNet, with testing\naccuracy within 1% of the DNN of equivalent size and architecture. Training\nbecomes similar to DNN thanks to the closed-form solution to the spiking\nwaveform dynamics. Considering that SNNs should be implemented in practical\nneuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2\nbits and with weights perturbed by random noise to demonstrate its robustness\nin practical applications. In addition, we develop a phase-domain signal\nprocessing circuit schematic to implement our spiking neuron with 90% gain of\nenergy efficiency over existing work. This paper demonstrates that the\ntemporal-coded deep SNN is feasible for applications with high performance and\nhigh energy efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shibo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LI_X/0/1/0/all/0/1\">Xiaohua LI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Sanjeev T. Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1\">Arindam Sanyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoDG: Generalizing Temporal Relations in Videos to Novel Domains. (arXiv:1912.03716v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.03716","description":"<p>This paper introduces video domain generalization where most video\nclassification networks degenerate due to the lack of exposure to the target\ndomains of divergent distributions. We observe that the global temporal\nfeatures are less generalizable, due to the temporal domain shift that videos\nfrom other unseen domains may have an unexpected absence or misalignment of the\ntemporal relations. This finding has motivated us to solve video domain\ngeneralization by effectively learning the local-relation features of different\ntimescales that are more generalizable, and exploiting them along with the\nglobal-relation features to maintain the discriminability. This paper presents\nthe VideoDG framework with two technical contributions. The first is a new deep\narchitecture named the Adversarial Pyramid Network, which improves the\ngeneralizability of video features by capturing the local-relation,\nglobal-relation, and cross-relation features progressively. On the basis of\npyramid features, the second contribution is a new and robust approach of\nadversarial data augmentation that can bridge different video domains by\nimproving the diversity and quality of augmented data. We construct three video\ndomain generalization benchmarks in which domains are divided according to\ndifferent datasets, different consequences of actions, or different camera\nviews, respectively. VideoDG consistently outperforms the combinations of\nprevious video classification models and existing domain generalization methods\non all benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhiyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSS: Transformation-Specific Smoothing for Robustness Certification. (arXiv:2002.12398v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2002.12398","description":"<p>As machine learning (ML) systems become pervasive, safeguarding their\nsecurity is critical. However, recently it has been demonstrated that motivated\nadversaries are able to mislead ML systems by perturbing test data using\nsemantic transformations. While there exists a rich body of research providing\nprovable robustness guarantees for ML models against $\\ell_p$ norm bounded\nadversarial perturbations, guarantees against semantic perturbations remain\nlargely underexplored. In this paper, we provide TSS -- a unified framework for\ncertifying ML robustness against general adversarial semantic transformations.\nFirst, depending on the properties of each transformation, we divide common\ntransformations into two categories, namely resolvable (e.g., Gaussian blur)\nand differentially resolvable (e.g., rotation) transformations. For the former,\nwe propose transformation-specific randomized smoothing strategies and obtain\nstrong robustness certification. The latter category covers transformations\nthat involve interpolation errors, and we propose a novel approach based on\nstratified sampling to certify the robustness. Our framework TSS leverages\nthese certification strategies and combines with consistency-enhanced training\nto provide rigorous certification of robustness. We conduct extensive\nexperiments on over ten types of challenging semantic transformations and show\nthat TSS significantly outperforms the state of the art. Moreover, to the best\nof our knowledge, TSS is the first approach that achieves nontrivial certified\nrobustness on the large-scale ImageNet dataset. For instance, our framework\nachieves 30.4% certified robust accuracy against rotation attack (within $\\pm\n30^\\circ$) on ImageNet. Moreover, to consider a broader range of\ntransformations, we show TSS is also robust against adaptive attacks and\nunforeseen image corruptions such as CIFAR-10-C and ImageNet-C.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1\">Maurice Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaojun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rimanic_L/0/1/0/all/0/1\">Luka Rimanic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Standardised convolutional filtering for radiomics. (arXiv:2006.05470v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2006.05470","description":"<p>The Image Biomarker Standardisation Initiative (IBSI) aims to improve\nreproducibility of radiomics studies by standardising the computational process\nof extracting image biomarkers (features) from images. We have previously\nestablished reference values for 169 commonly used features, created a standard\nradiomics image processing scheme, and developed reporting guidelines for\nradiomic studies. However, several aspects are not standardised.\n</p>\n<p>Here we present a preliminary version of a reference manual on the use of\nconvolutional image filters in radiomics. Filters, such as wavelets or\nLaplacian of Gaussian filters, play an important part in emphasising specific\nimage characteristics such as edges and blobs. Features derived from filter\nresponse maps have been found to be poorly reproducible. This reference manual\nforms the basis of ongoing work on standardising convolutional filters in\nradiomics, and will be updated as this work progresses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Depeursinge_A/0/1/0/all/0/1\">Adrien Depeursinge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andrearczyk_V/0/1/0/all/0/1\">Vincent Andrearczyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whybra_P/0/1/0/all/0/1\">Philip Whybra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Griethuysen_J/0/1/0/all/0/1\">Joost van Griethuysen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_H/0/1/0/all/0/1\">Henning M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaer_R/0/1/0/all/0/1\">Roger Schaer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vallieres_M/0/1/0/all/0/1\">Martin Valli&#xe8;res</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zwanenburg_A/0/1/0/all/0/1\">Alex Zwanenburg</a> (for the Image Biomarker Standardisation Initiative)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rescaling Egocentric Vision. (arXiv:2006.13256v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.13256","description":"<p>This paper introduces the pipeline to extend the largest dataset in\negocentric vision, EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a\ncollection of 100 hours, 20M frames, 90K actions in 700 variable-length videos,\ncapturing long-term unscripted activities in 45 environments, using\nhead-mounted cameras. Compared to its previous version, EPIC-KITCHENS-100 has\nbeen annotated using a novel pipeline that allows denser (54% more actions per\nminute) and more complete annotations of fine-grained actions (+128% more\naction segments). This collection enables new challenges such as action\ndetection and evaluating the \"test of time\" - i.e. whether models trained on\ndata collected in 2018 can generalise to new footage collected two years later.\nThe dataset is aligned with 6 challenges: action recognition (full and weak\nsupervision), action detection, action anticipation, cross-modal retrieval\n(from captions), as well as unsupervised domain adaptation for action\nrecognition. For each challenge, we define the task, provide baselines and\nevaluation metrics\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1\">Hazel Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazakos_E/0/1/0/all/0/1\">Evangelos Kazakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moltisanti_D/0/1/0/all/0/1\">Davide Moltisanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munro_J/0/1/0/all/0/1\">Jonathan Munro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrett_T/0/1/0/all/0/1\">Toby Perrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1\">Will Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_M/0/1/0/all/0/1\">Michael Wray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Example Difficulty Using Variance of Gradients. (arXiv:2008.11600v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.11600","description":"<p>In machine learning, a question of great interest is understanding what\nexamples are challenging for a model to classify. Identifying atypical examples\nensures the safe deployment of models, isolates samples that require further\nhuman inspection and provides interpretability into model behavior. In this\nwork, we propose Variance of Gradients (VoG) as a valuable and efficient metric\nto rank data by difficulty and to surface a tractable subset of the most\nchallenging examples for human-in-the-loop auditing. We show that data points\nwith high VoG scores are far more difficult for the model to learn and\nover-index on corrupted or memorized examples. Further, restricting the\nevaluation to the test set instances with the lowest VoG improves the model's\ngeneralization performance. Finally, we show that VoG is a valuable and\nefficient ranking for out-of-distribution detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimal Adversarial Examples for Deep Learning on 3D Point Clouds. (arXiv:2008.12066v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.12066","description":"<p>With recent developments of convolutional neural networks, deep learning for\n3D point clouds has shown significant progress in various 3D scene\nunderstanding tasks, e.g., object recognition, semantic segmentation. In a\nsafety-critical environment, it is however not well understood how such deep\nlearning models are vulnerable to adversarial examples. In this work, we\nexplore adversarial attacks for point cloud-based neural networks. We propose a\nunified formulation for adversarial point cloud generation that can generalise\ntwo different attack strategies. Our method generates adversarial examples by\nattacking the classification ability of point cloud-based networks while\nconsidering the perceptibility of the examples and ensuring the minimal level\nof point manipulations. Experimental results show that our method achieves the\nstate-of-the-art performance with higher than 89% and 90% of attack success\nrate on synthetic and real-world data respectively, while manipulating only\nabout 4% of the total points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaeyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai-Kit Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProCAN: Progressive Growing Channel Attentive Non-Local Network for Lung Nodule Classification. (arXiv:2010.15417v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.15417","description":"<p>Lung cancer classification in screening computed tomography (CT) scans is one\nof the most crucial tasks for early detection of this disease. Many lives can\nbe saved if we are able to accurately classify malignant/cancerous lung\nnodules. Consequently, several deep learning based models have been proposed\nrecently to classify lung nodules as malignant or benign. Nevertheless, the\nlarge variation in the size and heterogeneous appearance of the nodules makes\nthis task an extremely challenging one. We propose a new Progressive Growing\nChannel Attentive Non-Local (ProCAN) network for lung nodule classification.\nThe proposed method addresses this challenge from three different aspects.\nFirst, we enrich the Non-Local network by adding channel-wise attention\ncapability to it. Second, we apply Curriculum Learning principles, whereby we\nfirst train our model on easy examples before hard ones. Third, as the\nclassification task gets harder during the Curriculum learning, our model is\nprogressively grown to increase its capability of handling the task at hand. We\nexamined our proposed method on two different public datasets and compared its\nperformance with state-of-the-art methods in the literature. The results show\nthat the ProCAN model outperforms state-of-the-art methods and achieves an AUC\nof 98.05% and an accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we\nconducted extensive ablation studies to analyze the contribution and effects of\neach new component of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Al_Shabi_M/0/1/0/all/0/1\">Mundher Al-Shabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shak_K/0/1/0/all/0/1\">Kelvin Shak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_M/0/1/0/all/0/1\">Maxine Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEAN: graph-based pruning for convolutional neural networks by extracting longest chains. (arXiv:2011.06923v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.06923","description":"<p>Convolutional neural networks (CNNs) have proven to be highly successful at a\nrange of image-to-image tasks. CNNs can be prohibitively computationally\nexpensive for real time use, which can limit their applicability in practice.\nModel pruning can improve computational efficiency by sparsifying trained\nnetworks. Common methods for pruning CNNs determine what convolutional filters\nto remove by ranking filters on an individual basis. However, filters are not\nindependent, as CNNs consist of chains of convolutions, which can result in\nsub-optimal filter selection.\n</p>\n<p>We propose a novel pruning method, LongEst-chAiN (LEAN) pruning, which takes\nthe interdependency between the convolution operations into account. We propose\nto prune CNNs by using graph-based algorithms to select relevant chains of\nconvolutions. A CNN is interpreted as a graph, with the operator norm of each\noperator as distance metric for the edges. LEAN pruning iteratively extracts\nthe highest value path from the graph to keep. In our experiments, we test LEAN\npruning for several image-to-image tasks, including the well-known CamVid\ndataset, and a real-world dynamic X-ray CT dataset. When pruning CNNs with\nLEAN, we achieve a higher accuracy than pruning filters individually, and\ndifferent pruned substructures emerge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schoonhoven_R/0/1/0/all/0/1\">Richard Schoonhoven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendriksen_A/0/1/0/all/0/1\">Allard A. Hendriksen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelt_D/0/1/0/all/0/1\">Dani&#xeb;l M. Pelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batenburg_K/0/1/0/all/0/1\">K. Joost Batenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval. (arXiv:2011.12663v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12663","description":"<p>Uncertainty quantification in image retrieval is crucial for downstream\ndecisions, yet it remains a challenging and largely unexplored problem. Current\nmethods for estimating uncertainties are poorly calibrated, computationally\nexpensive, or based on heuristics. We present a new method that views image\nembeddings as stochastic features rather than deterministic features. Our two\nmain contributions are (1) a likelihood that matches the triplet constraint and\nthat evaluates the probability of an anchor being closer to a positive than a\nnegative; and (2) a prior over the feature space that justifies the\nconventional l2 normalization. To ensure computational efficiency, we derive a\nvariational approximation of the posterior, called the Bayesian triplet loss,\nthat produces state-of-the-art uncertainty estimates and matches the predictive\nperformance of current state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Warburg_F/0/1/0/all/0/1\">Frederik Warburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorgensen_M/0/1/0/all/0/1\">Martin J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1\">S&#xf8;ren Hauberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning for Sparsely-Labeled Sequential Data: Application to Healthcare Video Processing. (arXiv:2011.14101v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14101","description":"<p>Labeled data is a critical resource for training and evaluating machine\nlearning models. However, many real-life datasets are only partially labeled.\nWe propose a semi-supervised machine learning training strategy to improve\nevent detection performance on sequential data, such as video recordings, when\nonly sparse labels are available, such as event start times without their\ncorresponding end times. Our method uses noisy guesses of the events' end times\nto train event detection models. Depending on how conservative these guesses\nare, mislabeled false positives may be introduced into the training set (i.e.,\nnegative sequences mislabeled as positives). We further propose a mathematical\nmodel for estimating how many inaccurate labels a model is exposed to, based on\nhow noisy the end time guesses are. Finally, we show that neural networks can\nimprove their detection performance by leveraging more training data with less\nconservative approximations despite the higher proportion of incorrect labels.\nWe adapt sequential versions of MNIST and CIFAR-10 to empirically evaluate our\nmethod, and find that our risk-tolerant strategy outperforms conservative\nestimates by 12 points of mean average precision for MNIST, and 3.5 points for\nCIFAR. Then, we leverage the proposed training strategy to tackle a real-life\napplication: processing continuous video recordings of epilepsy patients to\nimprove seizure detection, and show that our method outperforms baseline\nlabeling methods by 10 points of average precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubost_F/0/1/0/all/0/1\">Florian Dubost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_E/0/1/0/all/0/1\">Erin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskhar_N/0/1/0/all/0/1\">Nandita Bhaskhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Messer_C/0/1/0/all/0/1\">Christopher Lee-Messer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGRNet: Adaptive Graph Representation Learning and Reasoning for Face Parsing. (arXiv:2101.07034v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07034","description":"<p>Face parsing infers a pixel-wise label to each facial component, which has\ndrawn much attention recently. Previous methods have shown their success in\nface parsing, which however overlook the correlation among facial components.\nAs a matter of fact, the component-wise relationship is a critical clue in\ndiscriminating ambiguous pixels in facial area. To address this issue, we\npropose adaptive graph representation learning and reasoning over facial\ncomponents, aiming to learn representative vertices that describe each\ncomponent, exploit the component-wise relationship and thereby produce accurate\nparsing results against ambiguity. In particular, we devise an adaptive and\ndifferentiable graph abstraction method to represent the components on a graph\nvia pixel-to-vertex projection under the initial condition of a predicted\nparsing map, where pixel features within a certain facial region are aggregated\nonto a vertex. Further, we explicitly incorporate the image edge as a prior in\nthe model, which helps to discriminate edge and non-edge pixels during the\nprojection, thus leading to refined parsing results along the edges. Then, our\nmodel learns and reasons over the relations among components by propagating\ninformation across vertices on the graph. Finally, the refined vertex features\nare projected back to pixel grids for the prediction of the final parsing map.\nTo train our model, we propose a discriminative loss to penalize small\ndistances between vertices in the feature space, which leads to distinct\nvertices with strong semantics. Experimental results show the superior\nperformance of the proposed model on multiple face parsing datasets, along with\nthe validation on the human parsing task to demonstrate the generalizability of\nour model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Te_G/0/1/0/all/0/1\">Gusi Te</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinglu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-OCR Paragraph Recognition by Graph Convolutional Networks. (arXiv:2101.12741v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.12741","description":"<p>We propose a new approach for paragraph recognition in document images by\nspatial graph convolutional networks (GCN) applied on OCR text boxes. Two\nsteps, namely line splitting and line clustering, are performed to extract\nparagraphs from the lines in OCR results. Each step uses a beta-skeleton graph\nconstructed from bounding boxes, where the graph edges provide efficient\nsupport for graph convolution operations. With only pure layout input features,\nthe GCN model size is 3~4 orders of magnitude smaller compared to R-CNN based\nmodels, while achieving comparable or better accuracies on PubLayNet and other\ndatasets. Furthermore, the GCN models show good generalization from synthetic\ntraining data to real-world images, and good adaptivity for variable document\nstyles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1\">Ashok C. Popat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths. (arXiv:2103.01435v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01435","description":"<p>Quantizing deep networks with adaptive bit-widths is a promising technique\nfor efficient inference across many devices and resource constraints. In\ncontrast to static methods that repeat the quantization process and train\ndifferent models for different constraints, adaptive quantization enables us to\nflexibly adjust the bit-widths of a single deep network during inference for\ninstant adaptation in different scenarios. While existing research shows\nencouraging results on common image classification benchmarks, this paper\ninvestigates how to train such adaptive networks more effectively.\nSpecifically, we present two novel techniques for quantizing deep neural\nnetworks with adaptive bit-widths of weights and activations. First, we propose\na collaborative strategy to choose a high-precision teacher for transferring\nknowledge to the low-precision student while jointly optimizing the model with\nall bit-widths. Second, to effectively transfer knowledge, we develop a dynamic\nblock swapping method by randomly replacing the blocks in the lower-precision\nstudent network with the corresponding blocks in the higher-precision teacher\nnetwork. Extensive experiments on multiple image classification datasets\nincluding video classification benchmarks for the first time, well demonstrate\nthe efficacy of our approach over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Ximeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bowen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Kailash Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images. (arXiv:2106.00116v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.00116","description":"<p>Transfer learning aims to exploit pre-trained models for more efficient\nfollow-up training on wide range of downstream tasks and datasets, enabling\nsuccessful training also on small data. Recently, strong improvement was shown\nfor transfer learning and model generalization when increasing model, data and\ncompute budget scale in the pre-training. To compare effect of scale both in\nintra- and inter-domain full and few-shot transfer, in this study we combine\nfor the first time large openly available medical X-Ray chest imaging datasets\nto reach a dataset scale comparable to ImageNet-1k. We then conduct\npre-training and transfer to different natural or medical targets while varying\nnetwork size and source data scale and domain, being either large natural\n(ImageNet-1k/21k) or large medical chest X-Ray datasets. We observe strong\nimprovement due to larger pre-training scale for intra-domain natural-natural\nand medical-medical transfer. For inter-domain natural-medical transfer, we\nfind improvements due to larger pre-training scale on larger X-Ray targets in\nfull shot regime, while for smaller targets and for few-shot regime the\nimprovement is not visible. Remarkably, large networks pre-trained on very\nlarge natural ImageNet-21k are as good or better than networks pre-trained on\nlargest available medical X-Ray data when performing transfer to large X-Ray\ntargets. We conclude that high quality models for inter-domain transfer can be\nalso obtained by substantially increasing scale of model and generic natural\nsource data, removing necessity for large domain-specific medical source data\nin the pre-training. Code is available at:\n\\url{https://github.com/SLAMPAI/large-scale-pretraining-transfer}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1\">Mehdi Cherti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1\">Jenia Jitsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04484","description":"<p>Deep learning algorithms have shown promising results in visual question\nanswering (VQA) tasks, but a more careful look reveals that they often do not\nunderstand the rich signal they are being fed with. To understand and better\nmeasure the generalization capabilities of VQA systems, we look at their\nrobustness to counterfactually augmented data. Our proposed augmentations are\ndesigned to make a focused intervention on a specific property of the question\nsuch that the answer changes. Using these augmentations, we propose a new\nrobustness measure, Robustness to Augmented Data (RAD), which measures the\nconsistency of model predictions between original and augmented examples.\nThrough extensive experimentation, we show that RAD, unlike classical accuracy\nmeasures, can quantify when state-of-the-art systems are not robust to\ncounterfactuals. We find substantial failure cases which reveal that current\nVQA systems are still brittle. Finally, we connect between robustness and\ngeneralization, demonstrating the predictive power of RAD for performance on\nunseen augmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1\">Daniel Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1\">Itai Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is this Harmful? Learning to Predict Harmfulness Ratings from Video. (arXiv:2106.08323v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08323","description":"<p>Automatically identifying harmful content in video is an important task with\na wide range of applications. However, due to the difficulty of collecting\nhigh-quality labels as well as demanding computational requirements, the task\nhas not yet had a fully general approach. Typically, only small subsets of the\nproblem are considered, such as identifying violent content. In cases where the\ngeneral problem is tackled, approximations and simplifications are made to deal\nwith the lack of labels and computational complexity. In this work, we identify\nand tackle some of the main obstacles. First, we create an open dataset of 3589\nvideo clips from film trailers and annotated by professionals in the field.\nSecond, we perform an analysis of our constructed dataset, investigating among\nother things the relation between clip and trailer level annotations. Lastly,\nwe train audiovisual models on our dataset and conduct an in-depth study on our\nmodeling choices. We find that results greatly improve by combining the visual\nand audio modality and that pre-training on large-scale video recognition\ndatasets as well as class balanced sampling further improves performance.\nFurther details of our dataset is available at this webpage:\nhttps://vidharm.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1\">Johan Edstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Amanda Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_J/0/1/0/all/0/1\">Johan Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benavente_F/0/1/0/all/0/1\">Francisca Benavente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_A/0/1/0/all/0/1\">Anette Novak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GKNet: grasp keypoint network for grasp candidates detection. (arXiv:2106.08497v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2106.08497","description":"<p>Contemporary grasp detection approaches employ deep learning to achieve\nrobustness to sensor and object model uncertainty. The two dominant approaches\ndesign either grasp-quality scoring or anchor-based grasp recognition networks.\nThis paper presents a different approach to grasp detection by treating it as\nkeypoint detection. The deep network detects each grasp candidate as a pair of\nkeypoints, convertible to the grasp representation g = {x, y, w, {\\theta}}^T,\nrather than a triplet or quartet of corner points. Decreasing the detection\ndifficulty by grouping keypoints into pairs boosts performance. To further\npromote dependencies between keypoints, the general non-local module is\nincorporated into the proposed learning framework. A final filtering strategy\nbased on discrete and continuous orientation prediction removes false\ncorrespondences and further improves grasp detection performance. GKNet, the\napproach presented here, achieves the best balance of accuracy and speed on the\nCornell and the abridged Jacquard dataset (96.9% and 98.39% at 41.67 and 23.26\nfps). Follow-up experiments on a manipulator evaluate GKNet using 4 types of\ngrasping experiments reflecting different nuisance sources: static grasping,\ndynamic grasping, grasping at varied camera angles, and bin picking. GKNet\noutperforms reference baselines in static and dynamic grasping experiments\nwhile showing robustness to varied camera viewpoints and bin picking\nexperiments. The results confirm the hypothesis that grasp keypoints are an\neffective output representation for deep grasp networks that provide robustness\nto expected nuisance factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruinian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1\">Fu-Jen Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vela_P/0/1/0/all/0/1\">Patricio A. Vela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-N-Out: Towards Good Initialization for Inpainting and Outpainting. (arXiv:2106.13953v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13953","description":"<p>In computer vision, recovering spatial information by filling in masked\nregions, e.g., inpainting, has been widely investigated for its usability and\nwide applicability to other various applications: image inpainting, image\nextrapolation, and environment map estimation. Most of them are studied\nseparately depending on the applications. Our focus, however, is on\naccommodating the opposite task, e.g., image outpainting, which would benefit\nthe target applications, e.g., image inpainting. Our self-supervision method,\nIn-N-Out, is summarized as a training approach that leverages the knowledge of\nthe opposite task into the target model. We empirically show that In-N-Out --\nwhich explores the complementary information -- effectively takes advantage\nover the traditional pipelines where only task-specific learning takes place in\ntraining. In experiments, we compare our method to the traditional procedure\nand analyze the effectiveness of our method on different applications: image\ninpainting, image extrapolation, and environment map estimation. For these\ntasks, we demonstrate that In-N-Out consistently improves the performance of\nthe recent works with In-N-Out self-supervision to their training procedure.\nAlso, we show that our approach achieves better results than an existing\ntraining approach for outpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_C/0/1/0/all/0/1\">Changho Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_W/0/1/0/all/0/1\">Woobin Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sung-Eui Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSN: Multi-Style Network for Trajectory Prediction. (arXiv:2107.00932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00932","description":"<p>It is essential to predict future trajectories of various agents in complex\nscenes. Whether it is internal personality factors of agents, interactive\nbehavior of the neighborhood, or the influence of surroundings, it will have an\nimpact on their future plannings. It means that even for the same physical type\nof agents, there are huge differences in their behavior styles. We concentrate\non the problem of modeling agents' multi-style characteristics when predicting\ntheir trajectories. We propose the Multi-Style Network (MSN) to focus on this\nproblem by dividing agents' behaviors into several hidden behavior categories\nadaptively, and then train each category's prediction network jointly, thus\ngiving agents multiple styles of predictions simultaneously. Experiments show\nthat MSN outperforms current state-of-the-art methods with 10\\% - 20\\%\nperformance improvement on two widely used datasets, and presents better\nmulti-style characteristics in predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Conghao Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hepatocellular Carcinoma Segmentation from Digital Subtraction Angiography Videos using Learnable Temporal Difference. (arXiv:2107.04306v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.04306","description":"<p>Automatic segmentation of hepatocellular carcinoma (HCC) in Digital\nSubtraction Angiography (DSA) videos can assist radiologists in efficient\ndiagnosis of HCC and accurate evaluation of tumors in clinical practice. Few\nstudies have investigated HCC segmentation from DSA videos. It shows great\nchallenging due to motion artifacts in filming, ambiguous boundaries of tumor\nregions and high similarity in imaging to other anatomical tissues. In this\npaper, we raise the problem of HCC segmentation in DSA videos, and build our\nown DSA dataset. We also propose a novel segmentation network called\nDSA-LTDNet, including a segmentation sub-network, a temporal difference\nlearning (TDL) module and a liver region segmentation (LRS) sub-network for\nproviding additional guidance. DSA-LTDNet is preferable for learning the latent\nmotion information from DSA videos proactively and boosting segmentation\nperformance. All of experiments are conducted on our self-collected dataset.\nExperimental results show that DSA-LTDNet increases the DICE score by nearly 4%\ncompared to the U-Net baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1\">Wenting Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Y/0/1/0/all/0/1\">Yicheng Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Changmiao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locally Enhanced Self-Attention: Rethinking Self-Attention as Local and Context Terms. (arXiv:2107.05637v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05637","description":"<p>Self-Attention has become prevalent in computer vision models. Inspired by\nfully connected Conditional Random Fields (CRFs), we decompose it into local\nand context terms. They correspond to the unary and binary terms in CRF and are\nimplemented by attention mechanisms with projection matrices. We observe that\nthe unary terms only make small contributions to the outputs, and meanwhile\nstandard CNNs that rely solely on the unary terms achieve great performances on\na variety of tasks. Therefore, we propose Locally Enhanced Self-Attention\n(LESA), which enhances the unary term by incorporating it with convolutions,\nand utilizes a fusion module to dynamically couple the unary and binary\noperations. In our experiments, we replace the self-attention modules with\nLESA. The results on ImageNet and COCO show the superiority of LESA over\nconvolution and self-attention baselines for the tasks of image recognition,\nobject detection, and instance segmentation. The code is made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Siyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests. (arXiv:2108.02356v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02356","description":"<p>Although deep neural networks (DNNs) enable great progress in video abnormal\nevent detection (VAD), existing solutions typically suffer from two issues: (1)\nThe localization of video events cannot be both precious and comprehensive. (2)\nThe semantics and temporal context are under-explored. To tackle those issues,\nwe are motivated by the prevalent cloze test in education and propose a novel\napproach named Visual Cloze Completion (VCC), which conducts VAD by learning to\ncomplete \"visual cloze tests\" (VCTs). Specifically, VCC first localizes each\nvideo event and encloses it into a spatio-temporal cube (STC). To achieve both\nprecise and comprehensive localization, appearance and motion are used as\ncomplementary cues to mark the object region associated with each event. For\neach marked region, a normalized patch sequence is extracted from current and\nadjacent frames and stacked into a STC. With each patch and the patch sequence\nof a STC compared to a visual \"word\" and \"sentence\" respectively, we\ndeliberately erase a certain \"word\" (patch) to yield a VCT. Then, the VCT is\ncompleted by training DNNs to infer the erased patch and its optical flow via\nvideo semantics. Meanwhile, VCC fully exploits temporal context by\nalternatively erasing each patch in temporal context and creating multiple\nVCTs. Furthermore, we propose localization-level, event-level, model-level and\ndecision-level solutions to enhance VCC, which can further exploit VCC's\npotential and produce significant performance improvement gain. Extensive\nexperiments demonstrate that VCC achieves state-of-the-art VAD performance. Our\ncodes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiping Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">En Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianping Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. (arXiv:2108.04308v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04308","description":"<p>Data is a crucial component of machine learning. The field is reliant on data\nto train, validate, and test models. With increased technical capabilities,\nmachine learning research has boomed in both academic and industry settings,\nand one major focus has been on computer vision. Computer vision is a popular\ndomain of machine learning increasingly pertinent to real-world applications,\nfrom facial recognition in policing to object detection for autonomous\nvehicles. Given computer vision's propensity to shape machine learning research\nand impact human life, we seek to understand disciplinary practices around\ndataset documentation - how data is collected, curated, annotated, and packaged\ninto datasets for computer vision researchers and practitioners to use for\nmodel tuning and development. Specifically, we examine what dataset\ndocumentation communicates about the underlying values of vision data and the\nlarger practices and goals of computer vision as a field. To conduct this\nstudy, we collected a corpus of about 500 computer vision datasets, from which\nwe sampled 114 dataset publications across different vision tasks. Through both\na structured and thematic content analysis, we document a number of values\naround accepted data practices, what makes desirable data, and the treatment of\nhumans in the dataset construction process. We discuss how computer vision\ndatasets authors value efficiency at the expense of care; universality at the\nexpense of contextuality; impartiality at the expense of positionality; and\nmodel work at the expense of data work. Many of the silenced values we identify\nsit in opposition with social computing practices. We conclude with suggestions\non how to better incorporate silenced values into the dataset creation and\ncuration process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheuerman_M/0/1/0/all/0/1\">Morgan Klaus Scheuerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1\">Emily Denton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanna_A/0/1/0/all/0/1\">Alex Hanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation. (arXiv:2108.08202v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.08202","description":"<p>Internet video delivery has undergone a tremendous explosion of growth over\nthe past few years. However, the quality of video delivery system greatly\ndepends on the Internet bandwidth. Deep Neural Networks (DNNs) are utilized to\nimprove the quality of video delivery recently. These methods divide a video\ninto chunks, and stream LR video chunks and corresponding content-aware models\nto the client. The client runs the inference of models to super-resolve the LR\nchunks. Consequently, a large number of models are streamed in order to deliver\na video. In this paper, we first carefully study the relation between models of\ndifferent chunks, then we tactfully design a joint training framework along\nwith the Content-aware Feature Modulation (CaFM) layer to compress these models\nfor neural video delivery. {\\bf With our method, each video chunk only requires\nless than $1\\% $ of original parameters to be streamed, achieving even better\nSR performance.} We conduct extensive experiments across various SR backbones,\nvideo time length, and scaling factors to demonstrate the advantages of our\nmethod. Besides, our method can be also viewed as a new approach of video\ncoding. Our primary experiments achieve better video quality compared with the\ncommercial H.264 and H.265 standard under the same storage cost, showing the\ngreat potential of the proposed method. Code is available\nat:\\url{https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1\">Kaixin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shizun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoqing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOKI: Long Term and Key Intentions for Trajectory Prediction. (arXiv:2108.08236v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08236","description":"<p>Recent advances in trajectory prediction have shown that explicit reasoning\nabout agents' intent is important to accurately forecast their motion. However,\nthe current research activities are not directly applicable to intelligent and\nsafety critical systems. This is mainly because very few public datasets are\navailable, and they only consider pedestrian-specific intents for a short\ntemporal horizon from a restricted egocentric view. To this end, we propose\nLOKI (LOng term and Key Intentions), a novel large-scale dataset that is\ndesigned to tackle joint trajectory and intention prediction for heterogeneous\ntraffic agents (pedestrians and vehicles) in an autonomous driving setting. The\nLOKI dataset is created to discover several factors that may affect intention,\nincluding i) agent's own will, ii) social interactions, iii) environmental\nconstraints, and iv) contextual information. We also propose a model that\njointly performs trajectory and intention prediction, showing that recurrently\nreasoning about intention can assist with trajectory prediction. We show our\nmethod outperforms state-of-the-art trajectory prediction methods by upto\n$27\\%$ and also provide a baseline for frame-wise intention estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girase_H/0/1/0/all/0/1\">Harshayu Girase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1\">Haiming Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1\">Srikanth Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanehara_A/0/1/0/all/0/1\">Akira Kanehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"External Knowledge Augmented Text Visual Question Answering. (arXiv:2108.09717v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09717","description":"<p>The open-ended question answering task of Text-VQA requires reading and\nreasoning about local, often previously unseen, scene-text content of an image\nto generate answers. In this work, we propose the generalized use of external\nknowledge to augment our understanding of the said scene-text. We design a\nframework to extract, validate, and reason with knowledge using a standard\nmultimodal transformer for vision language understanding tasks. Through\nempirical evidence and qualitative results, we demonstrate how external\nknowledge can highlight instance-only cues and thus help deal with training\ndata bias, improve answer entity type correctness, and detect multiword named\nentities. We generate results comparable to the state-of-the-art on two\npublicly available datasets, under the constraints of similar upstream OCR\nsystems and training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arka Ujjal Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harit_G/0/1/0/all/0/1\">Gaurav Harit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LUAI Challenge 2021 on Learning to Understand Aerial Images. (arXiv:2108.13246v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13246","description":"<p>This report summarizes the results of Learning to Understand Aerial Images\n(LUAI) 2021 challenge held on ICCV 2021, which focuses on object detection and\nsemantic segmentation in aerial images. Using DOTA-v2.0 and GID-15 datasets,\nthis challenge proposes three tasks for oriented object detection, horizontal\nobject detection, and semantic segmentation of common categories in aerial\nimages. This challenge received a total of 146 registrations on the three\ntasks. Through the challenge, we hope to draw attention from a wide range of\ncommunities and call for more efforts on the problems of learning to understand\naerial images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1\">Ming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaming Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datcu_M/0/1/0/all/0/1\">Mihai Datcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelillo_M/0/1/0/all/0/1\">Marcello Pelillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chao-hui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kaixuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Y/0/1/0/all/0/1\">Yingjia Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiaxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tianzhi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zi-han Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yi Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_Y/0/1/0/all/0/1\">Yiming Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhuojun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qianyue Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zixiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of GAN-synthesized street videos. (arXiv:2109.04991v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04991","description":"<p>Research on the detection of AI-generated videos has focused almost\nexclusively on face videos, usually referred to as deepfakes. Manipulations\nlike face swapping, face reenactment and expression manipulation have been the\nsubject of an intense research with the development of a number of efficient\ntools to distinguish artificial videos from genuine ones. Much less attention\nhas been paid to the detection of artificial non-facial videos. Yet, new tools\nfor the generation of such kind of videos are being developed at a fast pace\nand will soon reach the quality level of deepfake videos. The goal of this\npaper is to investigate the detectability of a new kind of AI-generated videos\nframing driving street sequences (here referred to as DeepStreets videos),\nwhich, by their nature, can not be analysed with the same tools used for facial\ndeepfakes. Specifically, we present a simple frame-based detector, achieving\nvery good performance on state-of-the-art DeepStreets videos generated by the\nVid2vid architecture. Noticeably, the detector retains very good performance on\ncompressed videos, even when the compression level used during training does\nnot match that used for the test videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alamayreh_O/0/1/0/all/0/1\">Omran Alamayreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1\">Mauro Barni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.05021","description":"<p>Red-lesions, i.e., microaneurysms (MAs) and hemorrhages (HMs), are the early\nsigns of diabetic retinopathy (DR). The automatic detection of MAs and HMs on\nretinal fundus images is a challenging task. Most of the existing methods\ndetect either only MAs or only HMs because of the difference in their texture,\nsizes, and morphology. Though some methods detect both MAs and HMs, they suffer\nfrom the curse of dimensionality of shape and colors features and fail to\ndetect all shape variations of HMs such as flame-shaped HM. Leveraging the\nprogress in deep learning, we proposed a two-stream red lesions detection\nsystem dealing simultaneously with small and large red lesions. For this\nsystem, we introduced a new ROIs candidates generation method for large red\nlesions fundus images; it is based on blood vessel segmentation and\nmorphological operations, and reduces the computational complexity, and\nenhances the detection accuracy by generating a small number of potential\ncandidates. For detection, we adapted the Faster RCNN framework with two\nstreams. We used pre-trained VGGNet as a bone model and carried out several\nextensive experiments to tune it for vessels segmentation and candidates\ngeneration, and finally learning the appropriate mapping, which yields better\ndetection of the red lesions comparing with the state-of-the-art methods. The\nexperimental results validated the effectiveness of the system in the detection\nof both MAs and HMs; the method yields higher performance for per lesion\ndetection according to sensitivity under 4 FPIs on DiaretDB1-MA and\nDiaretDB1-HM datasets, and 1 FPI on e-ophtha and ROCh datasets than the state\nof the art methods w.r.t. various evaluation metrics. For DR screening, the\nsystem outperforms other methods on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asiri_N/0/1/0/all/0/1\">Norah Asiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_M/0/1/0/all/0/1\">Muhammad Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adel_F/0/1/0/all/0/1\">Fadwa Al Adel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aboalsamh_H/0/1/0/all/0/1\">Hatim Aboalsamh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Tensor Completion via Element-wise Weighted Low-rank Tensor Train with Overlapping Ket Augmentation. (arXiv:2109.05736v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05736","description":"<p>In recent years, there have been an increasing number of applications of\ntensor completion based on the tensor train (TT) format because of its\nefficiency and effectiveness in dealing with higher-order tensor data. However,\nexisting tensor completion methods using TT decomposition have two obvious\ndrawbacks. One is that they only consider mode weights according to the degree\nof mode balance, even though some elements are recovered better in an\nunbalanced mode. The other is that serious blocking artifacts appear when the\nmissing element rate is relatively large. To remedy such two issues, in this\nwork, we propose a novel tensor completion approach via the element-wise\nweighted technique. Accordingly, a novel formulation for tensor completion and\nan effective optimization algorithm, called as tensor completion by parallel\nweighted matrix factorization via tensor train (TWMac-TT), is proposed. In\naddition, we specifically consider the recovery quality of edge elements from\nadjacent blocks. Different from traditional reshaping and ket augmentation, we\nutilize a new tensor augmentation technique called overlapping ket\naugmentation, which can further avoid blocking artifacts. We then conduct\nextensive performance evaluations on synthetic data and several real image data\nsets. Our experimental results demonstrate that the proposed algorithm TWMac-TT\noutperforms several other competing tensor completion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi&#x27;ai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yandong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication. (arXiv:2109.07644v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07644","description":"<p>Employing Vehicle-to-Vehicle communication to enhance perception performance\nin self-driving technology has attracted considerable attention recently;\nhowever, the absence of a suitable open dataset for benchmarking algorithms has\nmade it difficult to develop and assess cooperative perception technologies. To\nthis end, we present the first large-scale open simulated dataset for\nVehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464\nframes, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns\nin CARLA and a digital town of Culver City, Los Angeles. We then construct a\ncomprehensive benchmark with a total of 16 implemented models to evaluate\nseveral information fusion strategies~(i.e. early, late, and intermediate\nfusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose\na new Attentive Intermediate Fusion pipeline to aggregate information from\nmultiple connected vehicles. Our experiments show that the proposed pipeline\ncan be easily integrated with existing 3D LiDAR detectors and achieve\noutstanding performance even with large compression rates. To encourage more\nresearchers to investigate Vehicle-to-Vehicle perception, we will release the\ndataset, benchmark methods, and all related codes in\nhttps://mobility-lab.seas.ucla.edu/opv2v/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1\">Hao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinlong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROS-X-Habitat: Bridging the ROS Ecosystem with Embodied AI. (arXiv:2109.07703v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.07703","description":"<p>We introduce ROS-X-Habitat, a software interface that bridges the AI Habitat\nplatform for embodied reinforcement learning agents with other robotics\nresources via ROS. This interface not only offers standardized communication\nprotocols between embodied agents and simulators, but also enables\nphysics-based simulation. With this interface, roboticists are able to train\ntheir own Habitat RL agents in another simulation environment or to develop\ntheir own robotic algorithms inside Habitat Sim. Through in silico experiments,\nwe demonstrate that ROS-X-Habitat has minimal impact on the navigation\nperformance and simulation speed of Habitat agents; that a standard set of ROS\nmapping, planning and navigation tools can run in the Habitat simulator, and\nthat a Habitat agent can run in the standard ROS simulator Gazebo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_I/0/1/0/all/0/1\">Ian M. Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Temporal Sentence Grounding in Videos. (arXiv:2109.08039v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08039","description":"<p>Temporal sentence grounding in videos(TSGV), which aims to localize one\ntarget segment from an untrimmed video with respect to a given sentence query,\nhas drawn increasing attentions in the research community over the past few\nyears. Different from the task of temporal action localization, TSGV is more\nflexible since it can locate complicated activities via natural languages,\nwithout restrictions from predefined action categories. Meanwhile, TSGV is more\nchallenging since it requires both textual and visual understanding for\nsemantic alignment between two modalities(i.e., text and video). In this\nsurvey, we give a comprehensive overview for TSGV, which i) summarizes the\ntaxonomy of existing methods, ii) provides a detailed description of the\nevaluation protocols(i.e., datasets and metrics) to be used in TSGV, and iii)\nin-depth discusses potential problems of current benchmarking designs and\nresearch directions for further investigations. To the best of our knowledge,\nthis is the first systematic survey on temporal sentence grounding. More\nspecifically, we first discuss existing TSGV approaches by grouping them into\nfour categories, i.e., two-stage methods, end-to-end methods, reinforcement\nlearning-based methods, and weakly supervised methods. Then we present the\nbenchmark datasets and evaluation metrics to assess current research progress.\nFinally, we discuss some limitations in TSGV through pointing out potential\nproblems improperly resolved in the current evaluation protocols, which may\npush forwards more cutting edge research in TSGV. Besides, we also share our\ninsights on several promising directions, including three typical tasks with\nnew and practical settings based on TSGV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xiaohan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yitian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Machine Learning Framework for Automatic Prediction of Human Semen Motility. (arXiv:2109.08049v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08049","description":"<p>In this paper, human semen samples from the visem dataset collected by the\nSimula Research Laboratory are automatically assessed with machine learning\nmethods for their quality in respect to sperm motility. Several regression\nmodels are trained to automatically predict the percentage (0 to 100) of\nprogressive, non-progressive, and immotile spermatozoa in a given sample. The\nvideo samples are adopted for three different feature extraction methods, in\nparticular custom movement statistics, displacement features, and motility\nspecific statistics have been utilised. Furthermore, four machine learning\nmodels, including linear Support Vector Regressor (SVR), Multilayer Perceptron\n(MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN),\nhave been trained on the extracted features for the task of automatic motility\nprediction. Best results for predicting motility are achieved by using the\nCrocker-Grier algorithm to track sperm cells in an unsupervised way and\nextracting individual mean squared displacement features for each detected\ntrack. These features are then aggregated into a histogram representation\napplying a Bag-of-Words approach. Finally, a linear SVR is trained on this\nfeature representation. Compared to the best submission of the Medico\nMultimedia for Medicine challenge, which used the same dataset and splits, the\nMean Absolute Error (MAE) could be reduced from 8.83 to 7.31. For the sake of\nreproducibility, we provide the source code for our experiments on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ottl_S/0/1/0/all/0/1\">Sandra Ottl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerczuk_M/0/1/0/all/0/1\">Maurice Gerczuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign-MAML: Efficient Model-Agnostic Meta-Learning by SignSGD. (arXiv:2109.07497v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2109.07497","description":"<p>We propose a new computationally-efficient first-order algorithm for\nModel-Agnostic Meta-Learning (MAML). The key enabling technique is to interpret\nMAML as a bilevel optimization (BLO) problem and leverage the sign-based\nSGD(signSGD) as a lower-level optimizer of BLO. We show that MAML, through the\nlens of signSGD-oriented BLO, naturally yields an alternating optimization\nscheme that just requires first-order gradients of a learned meta-model. We\nterm the resulting MAML algorithm Sign-MAML. Compared to the conventional\nfirst-order MAML (FO-MAML) algorithm, Sign-MAML is theoretically-grounded as it\ndoes not impose any assumption on the absence of second-order derivatives\nduring meta training. In practice, we show that Sign-MAML outperforms FO-MAML\nin various few-shot image classification tasks, and compared to MAML, it\nachieves a much more graceful tradeoff between classification accuracy and\ncomputation efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1\">Parikshit Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}