{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"BioRED: A Comprehensive Biomedical Relation Extraction Dataset. (arXiv:2204.04263v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04263","description":"<p>Automated relation extraction (RE) from biomedical literature is critical for\nmany downstream text mining applications in both research and real-world\nsettings. However, most existing benchmarking datasets for bio-medical RE only\nfocus on relations of a single type (e.g., protein-protein interactions) at the\nsentence level, greatly limiting the development of RE systems in biomedicine.\nIn this work, we first review commonly used named entity recognition (NER) and\nRE datasets. Then we present BioRED, a first-of-its-kind biomedical RE corpus\nwith multiple entity types (e.g., gene/protein, disease, chemical) and relation\npairs (e.g., gene-disease; chemical-chemical), on a set of 600 PubMed articles.\nFurther, we label each relation as describing either a novel finding or\npreviously known background knowledge, enabling automated algorithms to\ndifferentiate between novel and background information. We assess the utility\nof BioRED by benchmarking several existing state-of-the-art methods, including\nBERT-based models, on the NER and RE tasks. Our results show that while\nexisting approaches can reach high performance on the NER task (F-score of\n89.3%), there is much room for improvement for the RE task, especially when\nextracting novel relations (F-score of 47.7%). Our experiments also demonstrate\nthat such a comprehensive dataset can successfully facilitate the development\nof more accurate, efficient, and robust RE systems for biomedicine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Po-Ting Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arighi_C/0/1/0/all/0/1\">Cecilia N Arighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Natural Language Processing Techniques for Requirements Engineering. (arXiv:2204.04282v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04282","description":"<p>Research in applying natural language processing (NLP) techniques to\nrequirements engineering (RE) tasks spans more than 40 years, from initial\nefforts carried out in the 1980s to more recent attempts with machine learning\n(ML) and deep learning (DL) techniques. However, in spite of the progress, our\nrecent survey shows that there is still a lack of systematic understanding and\norganization of commonly used NLP techniques in RE. We believe one hurdle\nfacing the industry is lack of shared knowledge of NLP techniques and their\nusage in RE tasks. In this paper, we present our effort to synthesize and\norganize 57 most frequently used NLP techniques in RE. We classify these NLP\ntechniques in two ways: first, by their NLP tasks in typical pipelines and\nsecond, by their linguist analysis levels. We believe these two ways of\nclassification are complementary, contributing to a better understanding of the\nNLP techniques in RE and such understanding is crucial to the development of\nbetter NLP tools for RE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhoshan_W/0/1/0/all/0/1\">Waad Alhoshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_A/0/1/0/all/0/1\">Alessio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Letsholo_K/0/1/0/all/0/1\">Keletso J. Letsholo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models. (arXiv:2204.04289v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04289","description":"<p>With a growing number of BERTology work analyzing different components of\npre-trained language models, we extend this line of research through an\nin-depth analysis of discourse information in pre-trained and fine-tuned\nlanguage models. We move beyond prior work along three dimensions: First, we\ndescribe a novel approach to infer discourse structures from arbitrarily long\ndocuments. Second, we propose a new type of analysis to explore where and how\naccurately intrinsic discourse is captured in the BERT and BART models.\nFinally, we assess how similar the generated structures are to a variety of\nbaselines as well as their distribution within and between models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1\">Patrick Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMTAfrica: Multilingual Machine Translation for African Languages. (arXiv:2204.04306v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04306","description":"<p>In this paper, we focus on the task of multilingual machine translation for\nAfrican languages and describe our contribution in the 2021 WMT Shared Task:\nLarge-Scale Multilingual Machine Translation. We introduce MMTAfrica, the first\nmany-to-many multilingual translation system for six African languages: Fon\n(fon), Igbo (ibo), Kinyarwanda (kin), Swahili/Kiswahili (swa), Xhosa (xho), and\nYoruba (yor) and two non-African languages: English (eng) and French (fra). For\nmultilingual translation concerning African languages, we introduce a novel\nbacktranslation and reconstruction objective, BT\\&amp;REC, inspired by the random\nonline back translation and T5 modeling framework respectively, to effectively\nleverage monolingual data. Additionally, we report improvements from MMTAfrica\nover the FLORES 101 benchmarks (spBLEU gains ranging from $+0.58$ in Swahili to\nFrench to $+19.46$ in French to Xhosa). We release our dataset and code source\nat https://github.com/edaiofficial/mmtafrica.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris C. Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Hindsight Instructions in Multi-Goal Reinforcement Learning for Robotics. (arXiv:2204.04308v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04308","description":"<p>This paper focuses on robotic reinforcement learning with sparse rewards for\nnatural language goal representations. An open problem is the\nsample-inefficiency that stems from the compositionality of natural language,\nand from the grounding of language in sensory data and actions. We address\nthese issues with three contributions. We first present a mechanism for\nhindsight instruction replay utilizing expert feedback. Second, we propose a\nseq2seq model to generate linguistic hindsight instructions. Finally, we\npresent a novel class of language-focused learning tasks. We show that\nhindsight instructions improve the learning performance, as expected. In\naddition, we also provide an unexpected result: We show that the learning\nperformance of our agent can be improved by one third if, in a sense, the agent\nlearns to talk to itself in a self-supervised manner. We achieve this by\nlearning to generate linguistic instructions that would have been appropriate\nas a natural language goal for an originally unintended behavior. Our results\nindicate that the performance gain increases with the task-complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roder_F/0/1/0/all/0/1\">Frank R&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eppe_M/0/1/0/all/0/1\">Manfred Eppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Show, Don't Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue. (arXiv:2204.04327v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04327","description":"<p>Building universal dialogue systems that can seamlessly operate across\nmultiple domains/APIs and generalize to new ones with minimal supervision and\nmaintenance is a critical challenge. Recent works have leveraged natural\nlanguage descriptions for schema elements to enable such systems; however,\ndescriptions can only indirectly convey schema semantics. In this work, we\npropose Show, Don't Tell, a prompt format for seq2seq modeling which uses a\nshort labeled example dialogue to show the semantics of schema elements rather\nthan tell the model via descriptions. While requiring similar effort from\nservice developers, we show that using short examples as schema representations\nwith large language models results in stronger performance and better\ngeneralization on two popular dialogue state tracking benchmarks: the\nSchema-Guided Dialogue dataset and the MultiWoZ leave-one-out benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raghav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harrison Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Chinese-centric Neural Machine Translation for Low-resource Languages. (arXiv:2204.04344v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04344","description":"<p>The last decade has witnessed enormous improvements in science and\ntechnology, stimulating the growing demand for economic and cultural exchanges\nin various countries. Building a neural machine translation (NMT) system has\nbecome an urgent trend, especially in the low-resource setting. However, recent\nwork tends to study NMT systems for low-resource languages centered on English,\nwhile few works focus on low-resource NMT systems centered on other languages\nsuch as Chinese. To achieve this, the low-resource multilingual translation\nchallenge of the 2021 iFLYTEK AI Developer Competition provides the\nChinese-centric multilingual low-resource NMT tasks, where participants are\nrequired to build NMT systems based on the provided low-resource samples. In\nthis paper, we present the winner competition system that leverages monolingual\nword embeddings data enhancement, bilingual curriculum learning, and\ncontrastive re-ranking. In addition, a new Incomplete-Trust (In-trust) loss\nfunction is proposed to replace the traditional cross-entropy loss when\ntraining. The experimental results demonstrate that the implementation of these\nideas leads better performance than other state-of-the-art methods. All the\nexperimental codes are released at:\nhttps://github.com/WENGSYX/Low-resource-text-translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hanjun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Karaka Framework in Multi-modal Grounding. (arXiv:2204.04347v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04347","description":"<p>Computational Paninian Grammar model helps in decoding a natural language\nexpression as a series of modifier-modified relations and therefore facilitates\nin identifying dependency relations closer to language (context) semantics\ncompared to the usual Stanford dependency relations. However, the importance of\nthis CPG dependency scheme has not been studied in the context of multi-modal\nvision and language applications. At IIIT Hyderabad, we plan to perform a novel\nstudy to explore the potential advantages and disadvantages of CPG framework in\na vision-language navigation task setting, a popular and challenging\nmulti-modal grounding task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorthi_S/0/1/0/all/0/1\">Sai Kiran Gorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should we tweet this? Generative response modeling for predicting reception of public health messaging on Twitter. (arXiv:2204.04353v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04353","description":"<p>The way people respond to messaging from public health organizations on\nsocial media can provide insight into public perceptions on critical health\nissues, especially during a global crisis such as COVID-19. It could be\nvaluable for high-impact organizations such as the US Centers for Disease\nControl and Prevention (CDC) or the World Health Organization (WHO) to\nunderstand how these perceptions impact reception of messaging on health policy\nrecommendations. We collect two datasets of public health messages and their\nresponses from Twitter relating to COVID-19 and Vaccines, and introduce a\npredictive method which can be used to explore the potential reception of such\nmessages. Specifically, we harness a generative model (GPT-2) to directly\npredict probable future responses and demonstrate how it can be used to\noptimize expected reception of important health guidance. Finally, we introduce\na novel evaluation scheme with extensive statistical testing which allows us to\nconclude that our models capture the semantics and sentiment found in actual\npublic health responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanders_A/0/1/0/all/0/1\">Abraham Sanders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_Majumder_D/0/1/0/all/0/1\">Debjani Ray-Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erickson_J/0/1/0/all/0/1\">John S. Erickson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_K/0/1/0/all/0/1\">Kristin P. Bennett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization. (arXiv:2204.04362v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04362","description":"<p>The most advanced abstractive dialogue summarizers lack generalization\nability on new domains and the existing researches for domain adaptation in\nsummarization generally rely on large-scale pre-trainings. To explore the\nlightweight fine-tuning methods for domain adaptation of dialogue\nsummarization, in this paper, we propose an efficient and generalizable\nDomain-Oriented Prefix-tuning model, which utilizes a domain word initialized\nprefix module to alleviate domain entanglement and adopts discrete prompts to\nguide the model to focus on key contents of dialogues and enhance model\ngeneralization. We conduct zero-shot experiments and build domain adaptation\nbenchmarks on two multi-domain dialogue summarization datasets, TODSum and\nQMSum. Adequate experiments and qualitative analysis prove the effectiveness of\nour methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lulu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fujia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Weihao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huixing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINER: Improving Out-of-Vocabulary Named Entity Recognition from an Information Theoretic Perspective. (arXiv:2204.04391v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04391","description":"<p>NER model has achieved promising performance on standard NER benchmarks.\nHowever, recent studies show that previous approaches may over-rely on entity\nmention information, resulting in poor performance on out-of-vocabulary (OOV)\nentity recognition. In this work, we propose MINER, a novel NER learning\nframework, to remedy this issue from an information-theoretic perspective. The\nproposed approach contains two mutual information-based training objectives: i)\ngeneralizing information maximization, which enhances representation via deep\nunderstanding of context and entity surface forms; ii) superfluous information\nminimization, which discourages representation from rote memorizing entity\nnames or exploiting biased cues in data. Experiments on various settings and\ndatasets demonstrate that it achieves better performance in predicting OOV\nentities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Limao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04392","description":"<p>Pretrained language models can be effectively stimulated by textual prompts\nor demonstrations, especially in low-data scenarios. Recent works have focused\non automatically searching discrete or continuous prompts or optimized\nverbalizers, yet studies for the demonstration are still limited. Concretely,\nthe demonstration examples are crucial for an excellent final performance of\nprompt-tuning. In this paper, we propose a novel pluggable, extensible, and\nefficient approach named contrastive demonstration tuning, which is free of\ndemonstration sampling. Furthermore, the proposed approach can be: (i) Plugged\nto any previous prompt-tuning approaches; (ii) Extended to widespread\nclassification tasks with a large number of categories. Experimental results on\n16 datasets illustrate that our method integrated with previous approaches\nLM-BFF and P-tuning can yield better performance. Code is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Neural Network for News Recommendation with Positive and Negative Implicit Feedback. (arXiv:2204.04397v1 [cs.IR])","link":"http://arxiv.org/abs/2204.04397","description":"<p>News recommendation is different from movie or e-commercial recommendation as\npeople usually do not grade the news. Therefore, user feedback for news is\nalways implicit (click behavior, reading time, etc). Inevitably, there are\nnoises in implicit feedback. On one hand, the user may exit immediately after\nclicking the news as he dislikes the news content, leaving the noise in his\npositive implicit feedback; on the other hand, the user may be recommended\nmultiple interesting news at the same time and only click one of them,\nproducing the noise in his negative implicit feedback. Opposite implicit\nfeedback could construct more integrated user preferences and help each other\nto minimize the noise influence. Previous works on news recommendation only\nused positive implicit feedback and suffered from the noise impact. In this\npaper, we propose a denoising neural network for news recommendation with\npositive and negative implicit feedback, named DRPN. DRPN utilizes both\nfeedback for recommendation with a module to denoise both positive and negative\nimplicit feedback to further enhance the performance. Experiments on the\nreal-world large-scale dataset demonstrate the state-of-the-art performance of\nDRPN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunfan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaopeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization. (arXiv:2204.04413v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04413","description":"<p>Few-shot abstractive summarization has become a challenging task in natural\nlanguage generation. To support it, we designed a novel soft prompts\narchitecture coupled with a prompt pre-training plus fine-tuning paradigm that\nis effective and tunes only extremely light parameters. The soft prompts\ninclude continuous input embeddings across an encoder and a decoder to fit the\nstructure of the generation models. Importantly, a novel inner-prompt placed in\nthe text is introduced to capture document-level information. The aim is to\ndevote attention to understanding the document that better prompts the model to\ngenerate document-related content. The first step in the summarization\nprocedure is to conduct prompt pre-training with self-supervised pseudo-data.\nThis teaches the model basic summarizing capabilities. The model is then\nfine-tuned with few-shot examples. Experimental results on the CNN/DailyMail\nand XSum datasets show that our method, with only 0.1% of the parameters,\noutperforms full-model tuning where all model parameters are tuned. It also\nsurpasses Prompt Tuning by a large margin and delivers competitive results\nagainst Prefix-Tuning with 3% of the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaochen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Multi-Granularity Hierarchical Features for Relation Extraction. (arXiv:2204.04437v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04437","description":"<p>Relation extraction is a key task in Natural Language Processing (NLP), which\naims to extract relations between entity pairs from given texts. Recently,\nrelation extraction (RE) has achieved remarkable progress with the development\nof deep neural networks. Most existing research focuses on constructing\nexplicit structured features using external knowledge such as knowledge graph\nand dependency tree. In this paper, we propose a novel method to extract\nmulti-granularity features based solely on the original input sentences. We\nshow that effective structured features can be attained even without external\nknowledge. Three kinds of features based on the input sentences are fully\nexploited, which are in entity mention level, segment level, and sentence\nlevel. All the three are jointly and hierarchically modeled. We evaluate our\nmethod on three public benchmarks: SemEval 2010 Task 8, Tacred, and Tacred\nRevisited. To verify the effectiveness, we apply our method to different\nencoders such as LSTM and BERT. Experimental results show that our method\nsignificantly outperforms existing state-of-the-art models that even use\nexternal knowledge. Extensive analyses demonstrate that the performance of our\nmodel is contributed by the capture of multi-granularity features and the model\nof their hierarchical structure. Code and data are available at\n\\url{https://github.com/xnliang98/sms}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinnian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding, Detecting, and Separating Out-of-Distribution Samples and Adversarial Samples in Text Classification. (arXiv:2204.04458v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04458","description":"<p>In this paper, we study the differences and commonalities between\nstatistically out-of-distribution (OOD) samples and adversarial (Adv) samples,\nboth of which hurting a text classification model's performance. We conduct\nanalyses to compare the two types of anomalies (OOD and Adv samples) with the\nin-distribution (ID) ones from three aspects: the input features, the hidden\nrepresentations in each layer of the model, and the output probability\ndistributions of the classifier. We find that OOD samples expose their\naberration starting from the first layer, while the abnormalities of Adv\nsamples do not emerge until the deeper layers of the model. We also illustrate\nthat the models' output probabilities for Adv samples tend to be more\nunconfident. Based on our observations, we propose a simple method to separate\nID, OOD, and Adv samples using the hidden representations and output\nprobabilities of the model. On multiple combinations of ID, OOD datasets, and\nAdv attacks, our proposed method shows exceptional results on distinguishing\nID, OOD, and Adv samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoundationLayerNorm: Scaling BERT and GPT to 1,000 Layers. (arXiv:2204.04477v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04477","description":"<p>The mainstream BERT/GPT model contains only 10 to 20 layers, and there is\nlittle literature to discuss the training of deep BERT/GPT. This paper proposes\na simple yet effective method to stabilize BERT and GPT training. We\nsuccessfully scale up BERT and GPT to 1,000 layers, which is an order of\nmagnitude deeper than previous BERT and GPT. The proposed method\nFoundationLayerNormalization enables efficient training of deep neural networks\nand is validated at the 1000-layer scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dezhou Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KUCST@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text. (arXiv:2204.04481v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04481","description":"<p>In this paper we present our approach for detecting signs of depression from\nsocial media text. Our model relies on word unigrams, part-of-speech tags,\nreadabilitiy measures and the use of first, second or third person and the\nnumber of words. Our best model obtained a macro F1-score of 0.439 and ranked\n25th, out of 31 teams. We further take advantage of the interpretability of the\nLogistic Regression model and we make an attempt to interpret the model\ncoefficients with the hope that these will be useful for further research on\nthe topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agirrezabal_M/0/1/0/all/0/1\">Manex Agirrezabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amann_J/0/1/0/all/0/1\">Janek Amann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uninformative Input Features and Counterfactual Invariance: Two Perspectives on Spurious Correlations in Natural Language. (arXiv:2204.04487v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04487","description":"<p>Spurious correlations are a threat to the trustworthiness of natural language\nprocessing systems, motivating research into methods for identifying and\neliminating them. Gardner et al (2021) argue that the compositional nature of\nlanguage implies that \\emph{all} correlations between labels and individual\ninput features are spurious. This paper analyzes this proposal in the context\nof a toy example, demonstrating three distinct conditions that can give rise to\nfeature-label correlations in a simple PCFG. Linking the toy example to a\nstructured causal model shows that (1) feature-label correlations can arise\neven when the label is invariant to interventions on the feature, and (2)\nfeature-label correlations may be absent even when the label is sensitive to\ninterventions on the feature. Because input features will be individually\ncorrelated with labels in all but very rare circumstances, domain knowledge\nmust be applied to identify spurious correlations that pose genuine robustness\nthreats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDPG: An Instance-Dependent Prompt Generation Method. (arXiv:2204.04497v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04497","description":"<p>Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a\ntask-specific prompt in each input instance during the model training stage. It\nfreezes the pre-trained language model and only optimizes a few task-specific\nprompts. In this paper, we propose a conditional prompt generation method to\ngenerate prompts for each input instance, referred to as the Instance-Dependent\nPrompt Generation (IDPG). Unlike traditional prompt tuning methods that use a\nfixed prompt, IDPG introduces a lightweight and trainable component to generate\nprompts based on each input sentence. Extensive experiments on ten natural\nlanguage understanding (NLU) tasks show that the proposed strategy consistently\noutperforms various prompt tuning baselines and is on par with other efficient\ntransfer learning methods such as Compacter while tuning far fewer model\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vydiswaran_V/0/1/0/all/0/1\">V.G.Vinod Vydiswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TANet: Thread-Aware Pretraining for Abstractive Conversational Summarization. (arXiv:2204.04504v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04504","description":"<p>Although pre-trained language models (PLMs) have achieved great success and\nbecome a milestone in NLP, abstractive conversational summarization remains a\nchallenging but less studied task. The difficulty lies in two aspects. One is\nthe lack of large-scale conversational summary data. Another is that applying\nthe existing pre-trained models to this task is tricky because of the\nstructural dependence within the conversation and its informal expression, etc.\nIn this work, we first build a large-scale (11M) pretraining dataset called\nRCS, based on the multi-person discussions in the Reddit community. We then\npresent TANet, a thread-aware Transformer-based network. Unlike the existing\npre-trained models that treat a conversation as a sequence of sentences, we\nargue that the inherent contextual dependency among the utterances plays an\nessential role in understanding the entire conversation and thus propose two\nnew techniques to incorporate the structural information into our model. The\nfirst is thread-aware attention which is computed by taking into account the\ncontextual dependency within utterances. Second, we apply thread prediction\nloss to predict the relations between utterances. We evaluate our model on four\ndatasets of real conversations, covering types of meeting transcripts,\ncustomer-service records, and forum threads. Experimental results demonstrate\nthat TANET achieves a new state-of-the-art in terms of both automatic\nevaluation and human judgment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ze Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhoujin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking for Public Health Surveillance tasks on Social Media with a Domain-Specific Pretrained Language Model. (arXiv:2204.04521v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04521","description":"<p>A user-generated text on social media enables health workers to keep track of\ninformation, identify possible outbreaks, forecast disease trends, monitor\nemergency cases, and ascertain disease awareness and response to official\nhealth correspondence. This exchange of health information on social media has\nbeen regarded as an attempt to enhance public health surveillance (PHS).\nDespite its potential, the technology is still in its early stages and is not\nready for widespread application. Advancements in pretrained language models\n(PLMs) have facilitated the development of several domain-specific PLMs and a\nvariety of downstream applications. However, there are no PLMs for social media\ntasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,\nto identify tasks related to public health surveillance on social media. We\ncompared and benchmarked the performance of PHS-BERT on 25 datasets from\ndifferent social medial platforms related to 7 different PHS tasks. Compared\nwith existing PLMs that are mainly evaluated on limited tasks, PHS-BERT\nachieved state-of-the-art performance on all 25 tested datasets, showing that\nour PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT\navailable, we aim to facilitate the community to reduce the computational cost\nand introduce new baselines for future works across various PHS-related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1\">Usman Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byoung Chan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khushi_M/0/1/0/all/0/1\">Matloob Khushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1\">Adam G. Dunn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending the Scope of Out-of-Domain: Examining QA models in multiple subdomains. (arXiv:2204.04534v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04534","description":"<p>Past works that investigate out-of-domain performance of QA systems have\nmainly focused on general domains (e.g. news domain, wikipedia domain),\nunderestimating the importance of subdomains defined by the internal\ncharacteristics of QA datasets. In this paper, we extend the scope of\n\"out-of-domain\" by splitting QA examples into different subdomains according to\ntheir several internal characteristics including question type, text length,\nanswer position. We then examine the performance of QA systems trained on the\ndata from different subdomains. Experimental results show that the performance\nof QA systems can be significantly reduced when the train data and test data\ncome from different subdomains. These results question the generalizability of\ncurrent QA systems in multiple subdomains, suggesting the need to combat the\nbias introduced by the internal characteristics of QA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KOBEST: Korean Balanced Evaluation of Significant Tasks. (arXiv:2204.04541v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04541","description":"<p>A well-formulated benchmark plays a critical role in spurring advancements in\nthe natural language processing (NLP) field, as it allows objective and precise\nevaluation of diverse models. As modern language models (LMs) have become more\nelaborate and sophisticated, more difficult benchmarks that require linguistic\nknowledge and reasoning have been proposed. However, most of these benchmarks\nonly support English, and great effort is necessary to construct benchmarks for\nother low resource languages. To this end, we propose a new benchmark named\nKorean balanced evaluation of significant tasks (KoBEST), which consists of\nfive Korean-language downstream tasks. Professional Korean linguists designed\nthe tasks that require advanced Korean linguistic knowledge. Moreover, our data\nis purely annotated by humans and thoroughly reviewed to guarantee high data\nquality. We also provide baseline models and human performance results. Our\ndataset is available on the Huggingface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dohyeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1\">Deuk Sin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Eric Davis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Extraction of Pathologies from C-Spine Radiology Reports using Multi-Task Learning. (arXiv:2204.04544v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04544","description":"<p>Pretrained Transformer based models finetuned on domain specific corpora have\nchanged the landscape of NLP. Generally, if one has multiple tasks on a given\ndataset, one may finetune different models or use task specific adapters. In\nthis work, we show that a multi-task model can beat or achieve the performance\nof multiple BERT-based models finetuned on various tasks and various task\nspecific adapter augmented BERT-based models. We validate our method on our\ninternal radiologist's report dataset on cervical spine. We hypothesize that\nthe tasks are semantically close and related and thus multitask learners are\npowerful classifiers. Our work opens the scope of using our method to\nradiologist's reports on various body parts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sehanobish_A/0/1/0/all/0/1\">Arijit Sehanobish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Nathaniel Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daga_I/0/1/0/all/0/1\">Ishita Daga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawar_J/0/1/0/all/0/1\">Jayashri Pawar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_D/0/1/0/all/0/1\">Danielle Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anasuya Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_M/0/1/0/all/0/1\">Murray Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_R/0/1/0/all/0/1\">Richard Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odry_B/0/1/0/all/0/1\">Benjamin Odry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vianu_R/0/1/0/all/0/1\">Ron Vianu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-Examining Human Annotations for Interpretable NLP. (arXiv:2204.04580v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04580","description":"<p>Explanation methods in Interpretable NLP often explain the model's decision\nby extracting evidence (rationale) from the input texts supporting the\ndecision. Benchmark datasets for rationales have been released to evaluate how\ngood the rationale is. The ground truth rationales in these datasets are often\nhuman annotations obtained via crowd-sourced websites. Valuable as these\ndatasets are, the details on how those human annotations are obtained are often\nnot clearly specified. We conduct comprehensive controlled experiments using\ncrowd-sourced websites on two widely used datasets in Interpretable NLP to\nunderstand how those unsaid details can affect the annotation results.\nSpecifically, we compare the annotation results obtained from recruiting\nworkers satisfying different levels of qualification. We also provide\nhigh-quality workers with different instructions for completing the same\nunderlying tasks. Our results reveal that the annotation quality is highly\nsubject to the workers' qualification, and workers can be guided to provide\ncertain annotations by the instructions. We further show that specific\nexplanation methods perform better when evaluated using the ground truth\nrationales obtained by particular instructions. Based on these observations, we\nhighlight the importance of providing complete details of the annotation\nprocess and call for careful interpretation of any experiment results obtained\nusing those annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering. (arXiv:2204.04581v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04581","description":"<p>Retrieval augmented language models have recently become the standard for\nknowledge intensive tasks. Rather than relying purely on latent semantics\nwithin the parameters of large neural models, these methods enlist a\nsemi-parametric memory to encode an index of knowledge for the model to\nretrieve over. Most prior work has employed text passages as the unit of\nknowledge, which has high coverage at the cost of interpretability,\ncontrollability, and efficiency. The opposite properties arise in other methods\nwhich have instead relied on knowledge base (KB) facts. At the same time, more\nrecent work has demonstrated the effectiveness of storing and retrieving from\nan index of Q-A pairs derived from text \\citep{lewis2021paq}. This approach\nyields a high coverage knowledge representation that maintains KB-like\nproperties due to its representations being more atomic units of information.\nIn this work we push this line of research further by proposing a\nquestion-answer augmented encoder-decoder model and accompanying pretraining\nstrategy. This yields an end-to-end system that not only outperforms prior QA\nretrieval methods on single-hop QA tasks but also enables compositional\nreasoning, as demonstrated by strong performance on two multi-hop QA datasets.\nTogether, these methods improve the ability to interpret and control the model\nwhile narrowing the performance gap with passage retrieval systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks. (arXiv:2204.04596v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04596","description":"<p>Parameter-efficient tuning aims to distill knowledge for downstream tasks by\noptimizing a few introduced parameters while freezing the pretrained language\nmodels (PLMs). Continuous prompt tuning which prepends a few trainable vectors\nto the embeddings of input is one of these methods and has drawn much attention\ndue to its effectiveness and efficiency. This family of methods can be\nillustrated as exerting nonlinear transformations of hidden states inside PLMs.\nHowever, a natural question is ignored: can the hidden states be directly used\nfor classification without changing them? In this paper, we aim to answer this\nquestion by proposing a simple tuning method which only introduces three\ntrainable vectors. Firstly, we integrate all layers hidden states using the\nintroduced vectors. And then, we input the integrated hidden state(s) to a\ntask-specific linear classifier to predict categories. This scheme is similar\nto the way ELMo utilises hidden states except that they feed the hidden states\nto LSTM-based models. Although our proposed tuning scheme is simple, it\nachieves comparable performance with prompt tuning methods like P-tuning and\nP-tuning v2, verifying that original hidden states do contain useful\ninformation for classification tasks. Moreover, our method has an advantage\nover prompt tuning in terms of time and the number of parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decay No More: A Persistent Twitter Dataset for Learning Social Meaning. (arXiv:2204.04611v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04611","description":"<p>With the proliferation of social media, many studies resort to social media\nto construct datasets for developing social meaning understanding systems. For\nthe popular case of Twitter, most researchers distribute tweet IDs without the\nactual text contents due to the data distribution policy of the platform. One\nissue is that the posts become increasingly inaccessible over time, which leads\nto unfair comparisons and a temporal bias in social media research. To\nalleviate this challenge of data decay, we leverage a paraphrase model to\npropose a new persistent English Twitter dataset for social meaning (PTSM).\nPTSM consists of $17$ social meaning datasets in $10$ categories of tasks. We\nexperiment with two SOTA pre-trained language models and show that our PTSM can\nsubstitute the actual tweets with paraphrases with marginal performance loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ME-GCN: Multi-dimensional Edge-Embedded Graph Convolutional Networks for Semi-supervised Text Classification. (arXiv:2204.04618v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04618","description":"<p>Compared to sequential learning models, graph-based neural networks exhibit\nexcellent ability in capturing global information and have been used for\nsemi-supervised learning tasks. Most Graph Convolutional Networks are designed\nwith the single-dimensional edge feature and failed to utilise the rich edge\ninformation about graphs. This paper introduces the ME-GCN (Multi-dimensional\nEdge-enhanced Graph Convolutional Networks) for semi-supervised text\nclassification. A text graph for an entire corpus is firstly constructed to\ndescribe the undirected and multi-dimensional relationship of word-to-word,\ndocument-document, and word-to-document. The graph is initialised with\ncorpus-trained multi-dimensional word and document node representation, and the\nrelations are represented according to the distance of those words/documents\nnodes. Then, the generated graph is trained with ME-GCN, which considers the\nedge features as multi-stream signals, and each stream performs a separate\ngraph convolutional operation. Our ME-GCN can integrate a rich source of graph\nedge information of the entire text corpus. The results have demonstrated that\nour proposed model has significantly outperformed the state-of-the-art methods\nacross eight benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kunze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Framework for Fast Automated Phonological Reconstruction Using Trimmed Alignments and Sound Correspondence Patterns. (arXiv:2204.04619v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04619","description":"<p>Computational approaches in historical linguistics have been increasingly\napplied during the past decade and many new methods that implement parts of the\ntraditional comparative method have been proposed. Despite these increased\nefforts, there are not many easy-to-use and fast approaches for the task of\nphonological reconstruction. Here we present a new framework that combines\nstate-of-the-art techniques for automated sequence comparison with novel\ntechniques for phonetic alignment analysis and sound correspondence pattern\ndetection to allow for the supervised reconstruction of word forms in ancestral\nlanguages. We test the method on a new dataset covering six groups from three\ndifferent language families. The results show that our method yields promising\nresults while at the same time being not only fast but also easy to apply and\nexpand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+List_J/0/1/0/all/0/1\">Johann-Mattis List</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forkel_R/0/1/0/all/0/1\">Robert Forkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_N/0/1/0/all/0/1\">Nathan W. Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours of Psycholinguistic Features. (arXiv:2204.04629v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04629","description":"<p>Research at the intersection of personality psychology, computer science, and\nlinguistics has recently focused increasingly on modeling and predicting\npersonality from language use. We report two major improvements in predicting\npersonality traits from text data: (1) to our knowledge, the most comprehensive\nset of theory-based psycholinguistic features and (2) hybrid models that\nintegrate a pre-trained Transformer Language Model BERT and Bidirectional Long\nShort-Term Memory (BLSTM) networks trained on within-text distributions ('text\ncontours') of psycholinguistic features. We experiment with BLSTM models (with\nand without Attention) and with two techniques for applying pre-trained\nlanguage representations from the transformer model - 'feature-based' and\n'fine-tuning'. We evaluate the performance of the models we built on two\nbenchmark datasets that target the two dominant theoretical models of\npersonality: the Big Five Essay dataset and the MBTI Kaggle dataset. Our\nresults are encouraging as our models outperform existing work on the same\ndatasets. More specifically, our models achieve improvement in classification\naccuracy by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset. In\naddition, we perform ablation experiments to quantify the impact of different\ncategories of psycholinguistic features in the respective personality\nprediction models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kerz_E/0/1/0/all/0/1\">Elma Kerz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanwar_S/0/1/0/all/0/1\">Sourabh Zanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiechmann_D/0/1/0/all/0/1\">Daniel Wiechmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"That Is a Suspicious Reaction!\": Interpreting Logits Variation to Detect NLP Adversarial Attacks. (arXiv:2204.04636v1 [cs.AI])","link":"http://arxiv.org/abs/2204.04636","description":"<p>Adversarial attacks are a major challenge faced by current machine learning\nresearch. These purposely crafted inputs fool even the most advanced models,\nprecluding their deployment in safety-critical applications. Extensive research\nin computer vision has been carried to develop reliable defense strategies.\nHowever, the same issue remains less explored in natural language processing.\nOur work presents a model-agnostic detector of adversarial text examples. The\napproach identifies patterns in the logits of the target classifier when\nperturbing the input text. The proposed detector improves the current\nstate-of-the-art performance in recognizing adversarial inputs and exhibits\nstrong generalization capabilities across different NLP models, datasets, and\nword-level attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosca_E/0/1/0/all/0/1\">Edoardo Mosca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shreyash Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rando_Ramirez_J/0/1/0/all/0/1\">Javier Rando-Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniDU: Towards A Unified Generative Dialogue Understanding Framework. (arXiv:2204.04637v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04637","description":"<p>With the development of pre-trained language models, remarkable success has\nbeen witnessed in dialogue understanding (DU) direction. However, the current\nDU approaches just employ an individual model for each DU task, independently,\nwithout considering the shared knowledge across different DU tasks. In this\npaper, we investigate a unified generative dialogue understanding framework,\nnamely UniDU, to achieve information exchange among DU tasks. Specifically, we\nreformulate the DU tasks into unified generative paradigm. In addition, to\nconsider different training data for each task, we further introduce\nmodel-agnostic training strategy to optimize unified model in a balanced\nmanner. We conduct the experiments on ten dialogue understanding datasets,\nwhich span five fundamental tasks: dialogue summary, dialogue completion, slot\nfilling, intent detection and dialogue state tracking. The proposed UniDU\nframework outperforms task-specific well-designed methods on all 5 tasks. We\nfurther conduct comprehensive analysis experiments to study the effect factors.\nThe experimental results also show that the proposed method obtains promising\nperformance on unseen dialogue domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuncong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Su Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Complexity Randomized Self-attention Mechanism. (arXiv:2204.04667v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04667","description":"<p>Recently, random feature attentions (RFAs) are proposed to approximate the\nsoftmax attention in linear time and space complexity by linearizing the\nexponential kernel. In this paper, we first propose a novel perspective to\nunderstand the bias in such approximation by recasting RFAs as self-normalized\nimportance samplers. This perspective further sheds light on an \\emph{unbiased}\nestimator for the whole softmax attention, called randomized attention (RA). RA\nconstructs positive random features via query-specific distributions and enjoys\ngreatly improved approximation fidelity, albeit exhibiting quadratic\ncomplexity. By combining the expressiveness in RA and the efficiency in RFA, we\ndevelop a novel linear complexity self-attention mechanism called linear\nrandomized attention (LARA). Extensive experiments across various domains\ndemonstrate that RA and LARA significantly improve the performance of RFAs by a\nsubstantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Biomedical Factoid Question Answering. (arXiv:2204.04711v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04711","description":"<p>We study the effect of seven data augmentation (da) methods in factoid\nquestion answering, focusing on the biomedical domain, where obtaining training\ninstances is particularly difficult. We experiment with data from the BioASQ\nchallenge, which we augment with training instances obtained from an artificial\nbiomedical machine reading comprehension dataset, or via back-translation,\ninformation retrieval, word substitution based on word2vec embeddings, or\nmasked language modeling, question generation, or extending the given passage\nwith additional context. We show that da can lead to very significant\nperformance gains, even when using large pre-trained Transformers, contributing\nto a broader discussion of if/when da benefits large pre-trained models. One of\nthe simplest da methods, word2vec-based word substitution, performed best and\nis recommended. We release our artificial training instances and code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pappas_D/0/1/0/all/0/1\">Dimitris Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malakasiotis_P/0/1/0/all/0/1\">Prodromos Malakasiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Model Jitter: Stable Re-training of Semantic Parsers in Production Environments. (arXiv:2204.04735v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04735","description":"<p>Retraining modern deep learning systems can lead to variations in model\nperformance even when trained using the same data and hyper-parameters by\nsimply using different random seeds. We call this phenomenon model jitter. This\nissue is often exacerbated in production settings, where models are retrained\non noisy data. In this work we tackle the problem of stable retraining with a\nfocus on conversational semantic parsers. We first quantify the model jitter\nproblem by introducing the model agreement metric and showing the variation\nwith dataset noise and model sizes. We then demonstrate the effectiveness of\nvarious jitter reduction techniques such as ensembling and distillation.\nLastly, we discuss practical trade-offs between such techniques and show that\nco-distillation provides a sweet spot in terms of jitter reduction for semantic\nparsing systems with only a modest increase in resource usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hidey_C/0/1/0/all/0/1\">Christopher Hidey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking Character: Are Subwords Good Enough for MRLs After All?. (arXiv:2204.04748v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04748","description":"<p>Large pretrained language models (PLMs) typically tokenize the input string\ninto contiguous subwords before any pretraining or inference. However, previous\nstudies have claimed that this form of subword tokenization is inadequate for\nprocessing morphologically-rich languages (MRLs). We revisit this hypothesis by\npretraining a BERT-style masked language model over character sequences instead\nof word-pieces. We compare the resulting model, dubbed TavBERT, against\ncontemporary PLMs based on subwords for three highly complex and ambiguous MRLs\n(Hebrew, Turkish, and Arabic), testing them on both morphological and semantic\ntasks. Our results show, for all tested languages, that while TavBERT obtains\nmild improvements on surface-level tasks \\`a la POS tagging and full\nmorphological disambiguation, subword-based PLMs achieve significantly higher\nperformance on semantic tasks, such as named entity recognition and extractive\nquestion answering. These results showcase and (re)confirm the potential of\nsubword tokenization as a reasonable modeling assumption for many languages,\nincluding MRLs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keren_O/0/1/0/all/0/1\">Omri Keren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avinari_T/0/1/0/all/0/1\">Tal Avinari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts. (arXiv:2204.04775v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04775","description":"<p>Despite the advances in digital healthcare systems offering curated\nstructured knowledge, much of the critical information still lies in large\nvolumes of unlabeled and unstructured clinical texts. These texts, which often\ncontain protected health information (PHI), are exposed to information\nextraction tools for downstream applications, risking patient identification.\nExisting works in de-identification rely on using large-scale annotated corpora\nin English, which often are not suitable in real-world multilingual settings.\nPre-trained language models (LM) have shown great potential for cross-lingual\ntransfer in low-resource settings. In this work, we empirically show the\nfew-shot cross-lingual transfer property of LMs for named entity recognition\n(NER) and apply it to solve a low-resource and real-world challenge of\ncode-mixed (Spanish-Catalan) clinical notes de-identification in the stroke\ndomain. We annotate a gold evaluation dataset to assess few-shot setting\nperformance where we only use a few hundred labeled examples for training. Our\nmodel improves the zero-shot F1-score from 73.7% to 91.2% on the gold\nevaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019)\nfrom the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingual\ntarget corpus. When generalized to an out-of-sample test set, the best model\nachieves a human-evaluation F1-score of 97.2%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amin_S/0/1/0/all/0/1\">Saadullah Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_N/0/1/0/all/0/1\">Noon Pokaratsiri Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wixted_M/0/1/0/all/0/1\">Morgan Kelly Wixted</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Rudolph_A/0/1/0/all/0/1\">Alejandro Garc&#xed;a-Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Costa_C/0/1/0/all/0/1\">Catalina Mart&#xed;nez-Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">G&#xfc;nter Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedDistant19: A Challenging Benchmark for Distantly Supervised Biomedical Relation Extraction. (arXiv:2204.04779v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04779","description":"<p>Relation Extraction in the biomedical domain is challenging due to the lack\nof labeled data and high annotation costs, needing domain experts. Distant\nsupervision is commonly used as a way to tackle the scarcity of annotated data\nby automatically pairing knowledge graph relationships with raw texts.\nDistantly Supervised Biomedical Relation Extraction (Bio-DSRE) models can\nseemingly produce very accurate results in several benchmarks. However, given\nthe challenging nature of the task, we set out to investigate the validity of\nsuch impressive results. We probed the datasets used by Amin et al. (2020) and\nHogan et al. (2021) and found a significant overlap between training and\nevaluation relationships that, once resolved, reduced the accuracy of the\nmodels by up to 71%. Furthermore, we noticed several inconsistencies with the\ndata construction process, such as creating negative samples and improper\nhandling of redundant relationships. We mitigate these issues and present\nMedDistant19, a new benchmark dataset obtained by aligning the MEDLINE\nabstracts with the widely used SNOMED Clinical Terms (SNOMED CT) knowledge\nbase. We experimented with several state-of-the-art models achieving an AUC of\n55.4% and 49.8% at sentence- and bag-level, showing that there is still plenty\nof room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amin_S/0/1/0/all/0/1\">Saadullah Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">David Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">G&#xfc;nter Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake news detection using parallel BERT deep neural networks. (arXiv:2204.04793v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04793","description":"<p>Fake news is a growing challenge for social networks and media. Detection of\nfake news always has been a problem for many years, but after the evolution of\nsocial networks and increasing speed of news dissemination in recent years has\nbeen considered again. There are several approaches to solving this problem,\none of which is to detect fake news based on its text style using deep neural\nnetworks. In recent years, one of the most used forms of deep neural networks\nfor natural language processing is transfer learning with transformers. BERT is\none of the most promising transformers who outperforms other models in many NLP\nbenchmarks. This article, we introduce MWPBert, which uses two parallel BERT\nnetworks to perform veracity detection on full-text news articles. One of the\nBERT networks encodes news headline, and another encodes news body. Since the\ninput length of the BERT network is limited and constant and the news body is\nusually a long text, we cannot fed the whole news text into the BERT.\nTherefore, using the MaxWorth algorithm, we selected the part of the news text\nthat is more valuable for fact-checking, and fed it into the BERT network.\nFinally, we encode the output of the two BERT networks to an output network to\nclassify the news. The experiment results showed that the proposed model\noutperformed previous models in terms of accuracy and other performance\nmeasures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farokhian_M/0/1/0/all/0/1\">Mahmood Farokhian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafe_V/0/1/0/all/0/1\">Vahid Rafe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veisi_H/0/1/0/all/0/1\">Hadi Veisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning. (arXiv:2204.04813v1 [cs.CL])","link":"http://arxiv.org/abs/2204.04813","description":"<p>Pre-trained sequence-to-sequence language models have led to widespread\nsuccess in many natural language generation tasks. However, there has been\nrelatively less work on analyzing their ability to generate structured outputs\nsuch as graphs. Unlike natural language, graphs have distinct structural and\nsemantic properties in the context of a downstream NLP task, e.g., generating a\ngraph that is connected and acyclic can be attributed to its structural\nconstraints, while the semantics of a graph can refer to how meaningfully an\nedge represents the relation between two node concepts. In this work, we study\npre-trained language models that generate explanation graphs in an end-to-end\nmanner and analyze their ability to learn the structural constraints and\nsemantics of such graphs. We first show that with limited supervision,\npre-trained language models often generate graphs that either violate these\nconstraints or are semantically incoherent. Since curating large amount of\nhuman-annotated graphs is expensive and tedious, we propose simple yet\neffective ways of graph perturbations via node and edge edit operations that\nlead to structurally and semantically positive and negative graphs. Next, we\nleverage these graphs in different contrastive learning models with Max-Margin\nand InfoNCE losses. Our methods lead to significant improvements in both\nstructural and semantic accuracy of explanation graphs and also generalize to\nother similar graph generation tasks. Lastly, we show that human errors are the\nbest negatives for contrastive learning and also that automatically generating\nmore such human-like negative graphs can lead to further improvements. Our code\nand models are publicly available at https://github.com/swarnaHub/ExplagraphGen\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Swarnadeep Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Prateek Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Represent Programs with Heterogeneous Graphs. (arXiv:2012.04188v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2012.04188","description":"<p>Program source code contains complex structure information, which can be\nrepresented in structured data forms like trees or graphs. To acquire the\nstructural information in source code, most existing researches use abstract\nsyntax trees (AST). A group of works add additional edges to ASTs to convert\nsource code into graphs and use graph neural networks to learn representations\nfor program graphs. Although these works provide additional control or data\nflow information to ASTs for downstream tasks, they neglect an important aspect\nof structure information in AST itself: the different types of nodes and edges.\nIn ASTs, different nodes contain different kinds of information like variables\nor control flow, and the relation between a node and all its children can also\nbe different.\n</p>\n<p>To address the information of node and edge types, we bring the idea of\nheterogeneous graphs to learning on source code and present a new formula of\nbuilding heterogeneous program graphs from ASTs with additional type\ninformation for nodes and edges. We use the ASDL grammar of programming\nlanguage to define the node and edge types of program graphs. Then we use\nheterogeneous graph neural networks to learn on these graphs. We evaluate our\napproach on two tasks: code comment generation and method naming. Both tasks\nrequire reasoning on the semantics of complete code snippets. Experiment\nresults show that our approach outperforms baseline models, including\nhomogeneous graph-based models, showing that leveraging the type information of\nnodes and edges in program graphs can help in learning program semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kechi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum Learning: A Survey. (arXiv:2101.10382v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.10382","description":"<p>Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soviany_P/0/1/0/all/0/1\">Petru Soviany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1\">Paolo Rota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters. (arXiv:2105.06232v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06232","description":"<p>To diversify and enrich generated dialogue responses, knowledge-grounded\ndialogue has been investigated in recent years. The existing methods tackle the\nknowledge grounding challenge by retrieving the relevant sentences over a large\ncorpus and augmenting the dialogues with explicit extra information. Despite\ntheir success, however, the existing works have drawbacks on the inference\nefficiency. This paper proposes KnowExpert, an end-to-end framework to bypass\nthe explicit retrieval process and inject knowledge into the pre-trained\nlanguage models with lightweight adapters and adapt to the knowledge-grounded\ndialogue task. To the best of our knowledge, this is the first attempt to\ntackle this challenge without retrieval in this task under an open-domain\nchit-chat scenario. The experimental results show that KknowExpert performs\ncomparably with some retrieval-based baselines while being time-efficient in\ninference, demonstrating the potential of our proposed direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bangla Natural Language Processing: A Comprehensive Analysis of Classical, Machine Learning, and Deep Learning Based Methods. (arXiv:2105.14875v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14875","description":"<p>The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1\">Ovishake Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1\">Mohtasim Fuad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">MD. Nazrul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbi_J/0/1/0/all/0/1\">Jakaria Rabbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masud_M/0/1/0/all/0/1\">Mehedi Masud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">MD. Kamrul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awal_M/0/1/0/all/0/1\">Md. Abdul Awal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fime_A/0/1/0/all/0/1\">Awal Ahmed Fime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuad_M/0/1/0/all/0/1\">Md. Tahmid Hasan Fuad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikder_D/0/1/0/all/0/1\">Delowar Sikder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iftee_M/0/1/0/all/0/1\">MD. Akil Raihan Iftee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saturated Transformers are Constant-Depth Threshold Circuits. (arXiv:2106.16213v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.16213","description":"<p>Transformers have become a standard neural network architecture for many NLP\nproblems, motivating theoretical analysis of their power in terms of formal\nlanguages. Recent work has shown that transformers with hard attention are\nquite limited in power (Hahn, 2020), as they can be simulated by constant-depth\nAND/OR circuits (Hao et al. 2021). However, hard attention is a strong\nassumption, which may complicate the relevance of these results in practice. In\nthis work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We first show that\nsaturated transformers transcend the known limitations of hard-attention\ntransformers. We then prove saturated transformers with floating-point values\ncan be simulated by constant-depth threshold circuits, giving the class\n$\\mathsf{TC}^0$ as an upper bound on the formal languages they recognize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning. (arXiv:2108.00356v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00356","description":"<p>Masked language models (MLMs) are pre-trained with a denoising objective that\nis in a mismatch with the objective of downstream fine-tuning. We propose\npragmatic masking and surrogate fine-tuning as two complementing strategies\nthat exploit social cues to drive pre-trained representations toward a broad\nset of concepts useful for a wide class of social meaning tasks. We test our\nmodels on $15$ different Twitter datasets for social meaning detection. Our\nmethods achieve $2.34\\%$ $F_1$ over a competitive baseline, while outperforming\ndomain-specific language models pre-trained on large datasets. Our methods also\nexcel in few-shot learning: with only $5\\%$ of training data (severely\nfew-shot), our methods enable an impressive $68.54\\%$ average $F_1$. The\nmethods are also language agnostic, as we show in a zero-shot setting involving\nsix datasets from three different languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMChat: Multi-Modal Chat Dataset on Social Media. (arXiv:2108.07154v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07154","description":"<p>Incorporating multi-modal contexts in conversation is an important step for\ndeveloping more engaging dialogue systems. In this work, we explore this\ndirection by introducing MMChat: a large scale Chinese multi-modal dialogue\ncorpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous\ncorpora that are crowd-sourced or collected from fictitious movies, MMChat\ncontains image-grounded dialogues collected from real conversations on social\nmedia, in which the sparsity issue is observed. Specifically, image-initiated\ndialogues in common communications may deviate to some non-image-grounded\ntopics as the conversation proceeds. To better investigate this issue, we\nmanually annotate 100K dialogues from MMChat and further filter the corpus\naccordingly, which yields MMChat-hf. We develop a benchmark model to address\nthe sparsity issue in dialogue generation tasks by adapting the attention\nrouting mechanism on image features. Experiments demonstrate the usefulness of\nincorporating image features and the effectiveness in handling the sparsity of\nimage features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation. (arXiv:2108.11626v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11626","description":"<p>As the use of interactive machines grow, the task of Emotion Recognition in\nConversation (ERC) became more important. If the machine-generated sentences\nreflect emotion, more human-like sympathetic conversations are possible. Since\nemotion recognition in conversation is inaccurate if the previous utterances\nare not taken into account, many studies reflect the dialogue context to\nimprove the performances. Many recent approaches show performance improvement\nby combining knowledge into modules learned from external structured data.\nHowever, structured data is difficult to access in non-English languages,\nmaking it difficult to extend to other languages. Therefore, we extract the\npre-trained memory using the pre-trained language model as an extractor of\nexternal knowledge. We introduce CoMPM, which combines the speaker's\npre-trained memory with the context model, and find that the pre-trained memory\nsignificantly improves the performance of the context model. CoMPM achieves the\nfirst or second performance on all data and is state-of-the-art among systems\nthat do not leverage structured data. In addition, our method shows that it can\nbe extended to other languages because structured knowledge is not required,\nunlike previous methods. Our code is available on\ngithub~\\footnote{https://github.com/rungjoo/CoMPM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Pursuit of Designing Multi-modal Transformer for Video Grounding. (arXiv:2109.06085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06085","description":"<p>Video grounding aims to localize the temporal segment corresponding to a\nsentence query from an untrimmed video. Almost all existing video grounding\nmethods fall into two frameworks: 1) Top-down model: It predefines a set of\nsegment candidates and then conducts segment classification and regression. 2)\nBottom-up model: It directly predicts frame-wise probabilities of the\nreferential segment boundaries. However, all these methods are not end-to-end,\ni.e., they always rely on some time-consuming post-processing steps to refine\npredictions. To this end, we reformulate video grounding as a set prediction\ntask and propose a novel end-to-end multi-modal Transformer model, dubbed as\nGTR. Specifically, GTR has two encoders for video and language encoding, and a\ncross-modal decoder for grounding prediction. To facilitate the end-to-end\ntraining, we use a Cubic Embedding layer to transform the raw videos into a set\nof visual tokens. To better fuse these two modalities in the decoder, we design\na new Multi-head Cross-Modal Attention. The whole GTR is optimized via a\nMany-to-One matching loss. Furthermore, we conduct comprehensive studies to\ninvestigate different model design choices. Extensive results on three\nbenchmarks have validated the superiority of GTR. All three typical GTR\nvariants achieve record-breaking performance on all datasets and metrics, with\nseveral times faster inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slot Filling for Biomedical Information Extraction. (arXiv:2109.08564v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08564","description":"<p>Information Extraction (IE) from text refers to the task of extracting\nstructured knowledge from unstructured text. The task typically consists of a\nseries of sub-tasks such as Named Entity Recognition and Relation Extraction.\nSourcing entity and relation type specific training data is a major bottleneck\nin domains with limited resources such as biomedicine. In this work we present\na slot filling approach to the task of biomedical IE, effectively replacing the\nneed for entity and relation-specific training data, allowing us to deal with\nzero-shot settings. We follow the recently proposed paradigm of coupling a\nTranformer-based bi-encoder, Dense Passage Retrieval, with a Transformer-based\nreading comprehension model to extract relations from biomedical text. We\nassemble a biomedical slot filling dataset for both retrieval and reading\ncomprehension and conduct a series of experiments demonstrating that our\napproach outperforms a number of simpler baselines. We also evaluate our\napproach end-to-end for standard as well as zero-shot settings. Our work\nprovides a fresh perspective on how to solve biomedical IE tasks, in the\nabsence of relevant training data. Our code, models and datasets are available\nat https://github.com/ypapanik/biomedical-slot-filling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papanikolaou_Y/0/1/0/all/0/1\">Yannis Papanikolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_M/0/1/0/all/0/1\">Marlene Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grace_J/0/1/0/all/0/1\">Justin Grace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_F/0/1/0/all/0/1\">Francine Bennett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-based NP Enrichment. (arXiv:2109.12085v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12085","description":"<p>Understanding the relations between entities denoted by NPs in a text is a\ncritical part of human-like natural language understanding. However, only a\nfraction of such relations is covered by standard NLP tasks and benchmarks\nnowadays. In this work, we propose a novel task termed text-based NP enrichment\n(TNE), in which we aim to enrich each NP in a text with all the\npreposition-mediated relations -- either explicit or implicit -- that hold\nbetween it and other NPs in the text. The relations are represented as\ntriplets, each denoted by two NPs related via a preposition. Humans recover\nsuch relations seamlessly, while current state-of-the-art models struggle with\nthem due to the implicit nature of the problem. We build the first large-scale\ndataset for the problem, provide the formal framing and scope of annotation,\nanalyze the data, and report the results of fine-tuned language models on the\ntask, demonstrating the challenge it poses to current technology. A webpage\nwith a data-exploration UI, a demo, and links to the code, models, and\nleaderboard, to foster further research into this challenging problem can be\nfound at: yanaiela.github.io/TNE/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basmov_V/0/1/0/all/0/1\">Victoria Basmov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v9 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient NLP: A Standard Evaluation and A Strong Baseline. (arXiv:2110.07038v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07038","description":"<p>Supersized pre-trained language models have pushed the accuracy of various\nnatural language processing (NLP) tasks to a new state-of-the-art (SOTA).\nRather than pursuing the reachless SOTA accuracy, more and more researchers\nstart paying attention on model efficiency and usability. Different from\naccuracy, the metric for efficiency varies across different studies, making\nthem hard to be fairly compared. To that end, this work presents ELUE\n(Efficient Language Understanding Evaluation), a standard evaluation, and a\npublic leaderboard for efficient NLP models. ELUE is dedicated to depict the\nPareto Frontier for various language understanding tasks, such that it can tell\nwhether and how much a method achieves Pareto improvement. Along with the\nbenchmark, we also release a strong baseline, ElasticBERT, which allows BERT to\nexit at any layer in both static and dynamic ways. We demonstrate the\nElasticBERT, despite its simplicity, outperforms or performs on par with SOTA\ncompressed and early exiting models. With ElasticBERT, the proposed ELUE has a\nstrong Pareto Frontier and makes a better evaluation for efficient NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiawen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingling Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MReD: A Meta-Review Dataset for Structure-Controllable Text Generation. (arXiv:2110.07474v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07474","description":"<p>When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited. A typical example is\nwhen using CNN/Daily Mail dataset for controllable text summarization, there is\nno guided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and the control signal to guide\nthe generation, which can only be built with a deep understanding of the domain\nknowledge. Motivated by this vision, our paper introduces a new text generation\ndataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its\n45k meta-review sentences are manually annotated with one of the 9 carefully\ndefined categories, including abstract, strength, decision, etc. We present\nexperimental results on start-of-the-art summarization models, and propose\nmethods for structure-controlled generation with both extractive and\nabstractive models using our annotated data. By exploring various settings and\nanalyzing the model behavior with respect to the control signal, we demonstrate\nthe challenges of our proposed task and the values of our dataset MReD.\nMeanwhile, MReD also allows us to have a better understanding of the\nmeta-review domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Compositional Generalization with Self-Training for Data-to-Text Generation. (arXiv:2110.08467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08467","description":"<p>Data-to-text generation focuses on generating fluent natural language\nresponses from structured meaning representations (MRs). Such representations\nare compositional and it is costly to collect responses for all possible\ncombinations of atomic meaning schemata, thereby necessitating few-shot\ngeneralization to novel MRs. In this work, we systematically study the\ncompositional generalization of the state-of-the-art T5 models in few-shot\ndata-to-text tasks. We show that T5 models fail to generalize to unseen MRs,\nand we propose a template-based input representation that considerably improves\nthe model's generalization capability. To further improve the model's\nperformance, we propose an approach based on self-training using fine-tuned\nBLEURT for pseudo response selection. On the commonly-used SGD and Weather\nbenchmarks, the proposed self-training approach improves tree accuracy by 46%+\nand reduces the slot error rates by 73%+ over the strong T5 baselines in\nfew-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sanket Vaibhav Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FPM: A Collection of Large-scale Foundation Pre-trained Language Models. (arXiv:2111.04909v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.04909","description":"<p>Large-scale Transformer models have significantly promoted the recent\ndevelopment of natural language processing applications. However, little effort\nhas been made to unify the effective models. In this paper, driven by providing\na new set of baseline models in the future, we adopt various novel transformer\narchitectures and launch a model set with the help of recent mainstream\ntechnologies. We focus the discussions on optimizing the depth of the networks\nbased on the existing powerful encode-decoder structures. We show that by\nproperly avoiding training defects such as non-convergence and degradation,\nscaling up off-the-shelf transformer architectures consistently delivers better\nperformance. To stimulate future research on large-scale language model\npretraining, we present extensive results and detailed discussions on network\nperformance improvements with respect to the network depth and confirm the\nexistence of the optimal number of layers under specific tasks. To the best of\nour knowledge, we provide the largest Chinese generative model and the largest\nChinese encoding model. The BERT language models we trained on English datasets\ndeliver a 14.45% higher F1 score than the Turing-NLR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dezhou Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Enactivist account of Mind Reading in Natural Language Understanding. (arXiv:2111.06179v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06179","description":"<p>In this paper we apply our understanding of the radical enactivist agenda to\nthe classic AI-hard problem of Natural Language Understanding. When Turing\ndevised his famous test the assumption was that a computer could use language\nand the challenge would be to mimic human intelligence. It turned out playing\nchess and formal logic were easy compared to understanding what people say. The\ntechniques of good old-fashioned AI (GOFAI) assume symbolic representation is\nthe core of reasoning and by that paradigm human communication consists of\ntransferring representations from one mind to another. However, one finds that\nrepresentations appear in another's mind, without appearing in the intermediary\nlanguage. People communicate by mind reading it seems. Systems with speech\ninterfaces such as Alexa and Siri are of course common, but they are limited.\nRather than adding mind reading skills, we introduced a \"cheat\" that enabled\nour systems to fake it. The cheat is simple and only slightly interesting to\ncomputer scientists and not at all interesting to philosophers. However,\nreading about the enactivist idea that we \"directly perceive\" the intentions of\nothers, our cheat took on a new light and in this paper look again at how\nnatural language understanding might actually work between humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Peter Wallis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity. (arXiv:2111.08366v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08366","description":"<p>We present a new scientific document similarity model based on matching\nfine-grained aspects of texts. To train our model, we exploit a\nnaturally-occurring source of supervision: sentences in the full-text of papers\nthat cite multiple papers together (co-citations). Such co-citations not only\nreflect close paper relatedness, but also provide textual descriptions of how\nthe co-cited papers are related. This novel form of textual supervision is used\nfor learning to match aspects across papers. We develop multi-vector\nrepresentations where vectors correspond to sentence-level aspects of\ndocuments, and present two methods for aspect matching: (1) A fast method that\nonly matches single aspects, and (2) a method that makes sparse multiple\nmatches with an Optimal Transport mechanism that computes an Earth Mover's\nDistance between aspects. Our approach improves performance on document\nsimilarity tasks in four datasets. Further, our fast single-match method\nachieves competitive results, paving the way for applying fine-grained\nsimilarity to large scientific corpora. Code, data, and models to be at:\nhttps://github.com/allenai/aspire\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation. (arXiv:2201.01845v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01845","description":"<p>Common designs of model evaluation typically focus on monolingual settings,\nwhere different models are compared according to their performance on a single\ndata set that is assumed to be representative of all possible data for the task\nat hand. While this may be reasonable for a large data set, this assumption is\ndifficult to maintain in low-resource scenarios, where artifacts of the data\ncollection can yield data sets that are outliers, potentially making\nconclusions about model performance coincidental. To address these concerns, we\ninvestigate model generalizability in crosslinguistic low-resource scenarios.\nUsing morphological segmentation as the test case, we compare three broad\nclasses of models with different parameterizations, taking data from 11\nlanguages across 6 language families. In each experimental setting, we evaluate\nall models on a first data set, then examine their performance consistency when\nintroducing new randomly sampled data sets with the same size and when applying\nthe trained models to unseen test sets of varying sizes. The results\ndemonstrate that the extent of model generalization depends on the\ncharacteristics of the data set, and does not necessarily rely heavily on the\ndata set size. Among the characteristics that we studied, the ratio of morpheme\noverlap and that of the average number of morphemes per word between the\ntraining and test sets are the two most prominent factors. Our findings suggest\nthat future work should adopt random sampling to construct data sets with\ndifferent sizes in order to make more responsible claims about model\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zoey Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudhommeaux_E/0/1/0/all/0/1\">Emily Prud&#x27;hommeaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Resources in the Tamasheq Language. (arXiv:2201.05051v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05051","description":"<p>In this paper we present two datasets for Tamasheq, a developing language\nmainly spoken in Mali and Niger. These two datasets were made available for the\nIWSLT 2022 low-resource speech translation track, and they consist of\ncollections of radio recordings from daily broadcast news in Niger (Studio\nKalangou) and Mali (Studio Tamani). We share (i) a massive amount of unlabeled\naudio data (671 hours) in five languages: French from Niger, Fulfulde, Hausa,\nTamasheq and Zarma, and (ii) a smaller 17 hours parallel corpus of audio\nrecordings in Tamasheq, with utterance-level translations in the French\nlanguage. All this data is shared under the Creative Commons BY-NC-ND 3.0\nlicense. We hope these resources will inspire the speech community to develop\nand benchmark models using the Tamasheq language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1\">Marcely Zanon Boito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bougares_F/0/1/0/all/0/1\">Fethi Bougares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbier_F/0/1/0/all/0/1\">Florentin Barbier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gahbiche_S/0/1/0/all/0/1\">Souhir Gahbiche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouvier_M/0/1/0/all/0/1\">Mickael Rouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.01993","description":"<p>Visual question answering is the task of answering questions about images. We\nintroduce the VizWiz-VQA-Grounding dataset, the first dataset that visually\ngrounds answers to visual questions asked by people with visual impairments. We\nanalyze our dataset and compare it with five VQA-Grounding datasets to\ndemonstrate what makes it similar and different. We then evaluate the SOTA VQA\nand VQA-Grounding models and demonstrate that current SOTA algorithms often\nfail to identify the correct visual evidence where the answer is located. These\nmodels regularly struggle when the visual evidence occupies a small fraction of\nthe image, for images that are higher quality, as well as for visual questions\nthat require skills in text recognition. The dataset, evaluation server, and\nleaderboard all can be found at the following link:\nhttps://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_S/0/1/0/all/0/1\">Samreen Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Logic Analogy Learning. (arXiv:2202.02436v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02436","description":"<p>Letter-string analogy is an important analogy learning task which seems to be\neasy for humans but very challenging for machines. The main idea behind current\napproaches to solving letter-string analogies is to design heuristic rules for\nextracting analogy structures and constructing analogy mappings. However, one\nkey problem is that it is difficult to build a comprehensive and exhaustive set\nof analogy structures which can fully describe the subtlety of analogies. This\nproblem makes current approaches unable to handle complicated letter-string\nanalogy problems. In this paper, we propose Neural logic analogy learning\n(Noan), which is a dynamic neural architecture driven by differentiable logic\nreasoning to solve analogy problems. Each analogy problem is converted into\nlogical expressions consisting of logical variables and basic logical\noperations (AND, OR, and NOT). More specifically, Noan learns the logical\nvariables as vector embeddings and learns each logical operation as a neural\nmodule. In this way, the model builds computational graph integrating neural\nnetwork with logical reasoning to capture the internal logical structure of the\ninput letter strings. The analogy learning problem then becomes a True/False\nevaluation problem of the logical expressions. Experiments show that our\nmachine learning-based Noan approach outperforms state-of-the-art approaches on\nstandard letter-string analogy benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yujia Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?. (arXiv:2203.08850v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08850","description":"<p>What can pre-trained multilingual sequence-to-sequence models like mBART\ncontribute to translating low-resource languages? We conduct a thorough\nempirical experiment in 10 languages to ascertain this, considering five\nfactors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning\ndata, (3) the amount of pre-training data in the model, (4) the impact of\ndomain mismatch, and (5) language typology. In addition to yielding several\nheuristics, the experiments form a framework for evaluating the data\nsensitivities of machine translation systems. While mBART is robust to domain\ndifferences, its translations for unseen and typologically distant languages\nremain below 3.0 BLEU. In answer to our title's question, mBART is not a\nlow-resource panacea; we therefore encourage shifting the emphasis from new\nmodels to new data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">En-Shiun Annie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thillainathan_S/0/1/0/all/0/1\">Sarubi Thillainathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Shravan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruisi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_A/0/1/0/all/0/1\">Arya D. McCarthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation. (arXiv:2203.09553v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.09553","description":"<p>Federated Learning (FL) on knowledge graphs (KGs) has yet to be as well\nstudied as other domains, such as computer vision and natural language\nprocessing. A recent study FedE first proposes an FL framework that shares\nentity embeddings of KGs across all clients. However, compared with model\nsharing in vanilla FL, entity embedding sharing from FedE would incur severe\nprivacy leakage. Specifically, the known entity embedding can be used to infer\nwhether a specific relation between two entities exists in a private client. In\nthis paper, we first develop a novel attack that aims to recover the original\ndata based on embedding information, which is further used to evaluate the\nvulnerabilities of FedE. Furthermore, we propose a Federated learning paradigm\nwith privacy-preserving Relation embedding aggregation (FedR) to tackle the\nprivacy issue in FedE. Compared to entity embedding sharing, relation embedding\nsharing policy can significantly reduce the communication cost due to its\nsmaller size of queries. We conduct extensive experiments to evaluate FedR with\nfive different embedding learning models and three benchmark KG datasets.\nCompared to FedE, FedR achieves similar utility and significant (nearly 2X)\nimprovements in both privacy and efficiency on link prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"x-enVENT: A Corpus of Event Descriptions with Experiencer-specific Emotion and Appraisal Annotations. (arXiv:2203.10909v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10909","description":"<p>Emotion classification is often formulated as the task to categorize texts\ninto a predefined set of emotion classes. So far, this task has been the\nrecognition of the emotion of writers and readers, as well as that of entities\nmentioned in the text. We argue that a classification setup for emotion\nanalysis should be performed in an integrated manner, including the different\nsemantic roles that participate in an emotion episode. Based on appraisal\ntheories in psychology, which treat emotions as reactions to events, we compile\nan English corpus of written event descriptions. The descriptions depict\nemotion-eliciting circumstances, and they contain mentions of people who\nresponded emotionally. We annotate all experiencers, including the original\nauthor, with the emotions they likely felt. In addition, we link them to the\nevent they found salient (which can be different for different experiencers in\na text) by annotating event properties, or appraisals (e.g., the perceived\nevent undesirability, the uncertainty of its outcome). Our analysis reveals\npatterns in the co-occurrence of people's emotions in interaction. Hence, this\nrichly-annotated resource provides useful data to study emotions and event\nevaluations from the perspective of different roles, and it enables the\ndevelopment of experiencer-specific emotion and appraisal classification\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegge_M/0/1/0/all/0/1\">Maximilian Wegge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer based ensemble for emotion detection. (arXiv:2203.11899v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11899","description":"<p>Detecting emotions in languages is important to accomplish a complete\ninteraction between humans and machines. This paper describes our contribution\nto the WASSA 2022 shared task which handles this crucial task of emotion\ndetection. We have to identify the following emotions: sadness, surprise,\nneutral, anger, fear, disgust, joy based on a given essay text. We are using an\nensemble of ELECTRA and BERT models to tackle this problem achieving an F1\nscore of $62.76\\%$. Our codebase (https://bit.ly/WASSA_shared_task) and our\nWandB project (https://wandb.ai/acl_wassa_pictxmanipal/acl_wassa) is publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1\">Shantanu Patankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1\">Sahil Khose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirtane_N/0/1/0/all/0/1\">Neeraja Kirtane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"bitsa_nlp@LT-EDI-ACL2022: Leveraging Pretrained Language Models for Detecting Homophobia and Transphobia in Social Media Comments. (arXiv:2203.14267v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14267","description":"<p>Online social networks are ubiquitous and user-friendly. Nevertheless, it is\nvital to detect and moderate offensive content to maintain decency and empathy.\nHowever, mining social media texts is a complex task since users don't adhere\nto any fixed patterns. Comments can be written in any combination of languages\nand many of them may be low-resource.\n</p>\n<p>In this paper, we present our system for the LT-EDI shared task on detecting\nhomophobia and transphobia in social media comments. We experiment with a\nnumber of monolingual and multilingual transformer based models such as mBERT\nalong with a data augmentation technique for tackling class imbalance. Such\npretrained large models have recently shown tremendous success on a variety of\nbenchmark tasks in natural language processing. We observe their performance on\na carefully annotated, real life dataset of YouTube comments in English as well\nas Tamil.\n</p>\n<p>Our submission achieved ranks 9, 6 and 3 with a macro-averaged F1-score of\n0.42, 0.64 and 0.58 in the English, Tamil and Tamil-English subtasks\nrespectively. The code for the system has been open sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_V/0/1/0/all/0/1\">Vitthal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Poonam Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A sequence-to-sequence approach for document-level relation extraction. (arXiv:2204.01098v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.01098","description":"<p>Motivated by the fact that many relations cross the sentence boundary, there\nhas been increasing interest in document-level relation extraction (DocRE).\nDocRE requires integrating information within and across sentences, capturing\ncomplex interactions between mentions of entities. Most existing methods are\npipeline-based, requiring entities as input. However, jointly learning to\nextract entities and relations can improve performance and be more efficient\ndue to shared parameters and training steps. In this paper, we develop a\nsequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE\n(entity extraction, coreference resolution and relation extraction) end-to-end,\nreplacing a pipeline of task-specific components. Using a simple strategy we\ncall entity hinting, we compare our approach to existing pipeline-based methods\non several popular biomedical datasets, in some cases exceeding their\nperformance. We also report the first end-to-end results on these datasets for\nfuture comparison. Finally, we demonstrate that, under our model, an end-to-end\napproach outperforms a pipeline-based approach. Our code, data and trained\nmodels are available at {\\url{https://github.com/johngiorgi/seq2rel}}. An\nonline demo is available at\n{\\url{https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1\">John Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bader_G/0/1/0/all/0/1\">Gary D. Bader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FashionCLIP: Connecting Language and Images for Product Representations. (arXiv:2204.03972v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2204.03972","description":"<p>The steady rise of online shopping goes hand in hand with the development of\nincreasingly complex ML and NLP models. While most use cases are cast as\nspecialized supervised learning problems, we argue that practitioners would\ngreatly benefit from more transferable representations of products. In this\nwork, we build on recent developments in contrastive learning to train\nFashionCLIP, a CLIP-like model for the fashion industry. We showcase its\ncapabilities for retrieval, classification and grounding, and release our model\nand code to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1\">Patrick John Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_A/0/1/0/all/0/1\">Ana Rita Magalh&#xe3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_D/0/1/0/all/0/1\">Diogo Goncalves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1\">Ciro Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Template-free Prompt Tuning for Few-shot NER. (arXiv:2109.13532v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2109.13532","description":"<p>Prompt-based methods have been successfully applied in sentence-level\nfew-shot learning tasks, mostly owing to the sophisticated design of templates\nand label words. However, when applied to token-level labeling tasks such as\nNER, it would be time-consuming to enumerate the template queries over all\npotential entity spans. In this work, we propose a more elegant method to\nreformulate NER tasks as LM problems without any templates. Specifically, we\ndiscard the template construction process while maintaining the word prediction\nparadigm of pre-training models to predict a class-related pivot word (or label\nword) at the entity position. Meanwhile, we also explore principled ways to\nautomatically search for appropriate label words that the pre-trained models\ncan easily adapt to. While avoiding complicated template-based process, the\nproposed LM objective also reduces the gap between different objectives used in\npre-training and fine-tuning, thus it can better benefit the few-shot\nperformance. Experimental results demonstrate the effectiveness of the proposed\nmethod over bert-tagger and template-based method under few-shot setting.\nMoreover, the decoding speed of the proposed method is up to 1930.12 times\nfaster than the template-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiding Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Intelligent Sight and Sound: A Chronic Cancer Pain Dataset. (arXiv:2204.04214v1 [eess.IV])","link":"http://arxiv.org/abs/2204.04214","description":"<p>Cancer patients experience high rates of chronic pain throughout the\ntreatment process. Assessing pain for this patient population is a vital\ncomponent of psychological and functional well-being, as it can cause a rapid\ndeterioration of quality of life. Existing work in facial pain detection often\nhave deficiencies in labeling or methodology that prevent them from being\nclinically relevant. This paper introduces the first chronic cancer pain\ndataset, collected as part of the Intelligent Sight and Sound (ISS) clinical\ntrial, guided by clinicians to help ensure that model findings yield clinically\nrelevant results. The data collected to date consists of 29 patients, 509\nsmartphone videos, 189,999 frames, and self-reported affective and activity\npain scores adopted from the Brief Pain Inventory (BPI). Using static images\nand multi-modal data to predict self-reported pain levels, early models show\nsignificant gaps between current methods available to predict pain today, with\nroom for improvement. Due to the especially sensitive nature of the inherent\nPersonally Identifiable Information (PII) of facial images, the dataset will be\nreleased under the guidance and control of the National Institutes of Health\n(NIH).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ordun_C/0/1/0/all/0/1\">Catherine Ordun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cha_A/0/1/0/all/0/1\">Alexandra N. Cha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaskin_B/0/1/0/all/0/1\">Byron Gaskin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hanson_A/0/1/0/all/0/1\">Alex Hanson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rule_M/0/1/0/all/0/1\">Mason Rule</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Purushotham_S/0/1/0/all/0/1\">Sanjay Purushotham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulley_J/0/1/0/all/0/1\">James L. Gulley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Free Quantization with Accurate Activation Clipping and Adaptive Batch Normalization. (arXiv:2204.04215v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04215","description":"<p>Data-free quantization is a task that compresses the neural network to low\nbit-width without access to original training data. Most existing data-free\nquantization methods cause severe performance degradation due to inaccurate\nactivation clipping range and quantization error, especially for low bit-width.\nIn this paper, we present a simple yet effective data-free quantization method\nwith accurate activation clipping and adaptive batch normalization. Accurate\nactivation clipping (AAC) improves the model accuracy by exploiting accurate\nactivation information from the full-precision model. Adaptive batch\nnormalization firstly proposes to address the quantization error from\ndistribution changes by updating the batch normalization layer adaptively.\nExtensive experiments demonstrate that the proposed data-free quantization\nmethod can yield surprisingly performance, achieving 64.33% top-1 accuracy of\nResNet18 on ImageNet dataset, with 3.7% absolute improvement outperforming the\nexisting state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yefei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luoming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Trajectory-Aware Transformer for Video Super-Resolution. (arXiv:2204.04216v1 [eess.IV])","link":"http://arxiv.org/abs/2204.04216","description":"<p>Video super-resolution (VSR) aims to restore a sequence of high-resolution\n(HR) frames from their low-resolution (LR) counterparts. Although some progress\nhas been made, there are grand challenges to effectively utilize temporal\ndependency in entire video sequences. Existing approaches usually align and\naggregate video frames from limited adjacent frames (e.g., 5 or 7 frames),\nwhich prevents these approaches from satisfactory results. In this paper, we\ntake one step further to enable effective spatio-temporal learning in videos.\nWe propose a novel Trajectory-aware Transformer for Video Super-Resolution\n(TTVSR). In particular, we formulate video frames into several pre-aligned\ntrajectories which consist of continuous visual tokens. For a query token,\nself-attention is only learned on relevant visual tokens along spatio-temporal\ntrajectories. Compared with vanilla vision Transformers, such a design\nsignificantly reduces the computational cost and enables Transformers to model\nlong-range features. We further propose a cross-scale feature tokenization\nmodule to overcome scale-changing problems that often occur in long-range\nvideos. Experimental results demonstrate the superiority of the proposed TTVSR\nover state-of-the-art models, by extensive quantitative and qualitative\nevaluations in four widely-used video super-resolution benchmarks. Both code\nand pre-trained models can be downloaded at\nhttps://github.com/researchmm/TTVSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chengxu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-enhanced Adversarial Semi-supervised Semantic Segmentation Network for Pulmonary Embolism Annotation. (arXiv:2204.04217v1 [eess.IV])","link":"http://arxiv.org/abs/2204.04217","description":"<p>This study established a feature-enhanced adversarial semi-supervised\nsemantic segmentation model to automatically annotate pulmonary embolism lesion\nareas in computed tomography pulmonary angiogram (CTPA) images. In current\nstudies, all of the PE CTPA image segmentation methods are trained by\nsupervised learning. However, the supervised learning models need to be\nretrained and the images need to be relabeled when the CTPA images come from\ndifferent hospitals. This study proposed a semi-supervised learning method to\nmake the model applicable to different datasets by adding a small amount of\nunlabeled images. By training the model with both labeled and unlabeled images,\nthe accuracy of unlabeled images can be improved and the labeling cost can be\nreduced. Our semi-supervised segmentation model includes a segmentation network\nand a discriminator network. We added feature information generated from the\nencoder of segmentation network to the discriminator so that it can learn the\nsimilarity between predicted mask and ground truth mask. This HRNet-based\narchitecture can maintain a higher resolution for convolutional operations so\nthe prediction of small PE lesion areas can be improved. We used the labeled\nopen-source dataset and the unlabeled National Cheng Kung University Hospital\n(NCKUH) (IRB number: B-ER-108-380) dataset to train the semi-supervised\nlearning model, and the resulting mean intersection over union (mIOU), dice\nscore, and sensitivity achieved 0.3510, 0.4854, and 0.4253, respectively on the\nNCKUH dataset. Then, we fine-tuned and tested the model with a small amount of\nunlabeled PE CTPA images from China Medical University Hospital (CMUH) (IRB\nnumber: CMUH110-REC3-173) dataset. Comparing the results of our semi-supervised\nmodel with the supervised model, the mIOU, dice score, and sensitivity improved\nfrom 0.2344, 0.3325, and 0.3151 to 0.3721, 0.5113, and 0.4967, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cheng_T/0/1/0/all/0/1\">Ting-Wei Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_J/0/1/0/all/0/1\">Jerry Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Ching-Chun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chin Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun-Chien Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution. (arXiv:2204.04218v1 [eess.IV])","link":"http://arxiv.org/abs/2204.04218","description":"<p>Super-resolving medical images can help physicians in providing more accurate\ndiagnostics. In many situations, computed tomography (CT) or magnetic resonance\nimaging (MRI) techniques output several scans (modes) during a single\ninvestigation, which can jointly be used (in a multimodal fashion) to further\nboost the quality of super-resolution results. To this end, we propose a novel\nmultimodal multi-head convolutional attention module to super-resolve CT and\nMRI scans. Our attention module uses the convolution operation to perform joint\nspatial-channel attention on multiple concatenated input tensors, where the\nkernel (receptive field) size controls the reduction rate of the spatial\nattention and the number of convolutional filters controls the reduction rate\nof the channel attention, respectively. We introduce multiple attention heads,\neach head having a distinct receptive field size corresponding to a particular\nreduction rate for the spatial attention. We integrate our multimodal\nmulti-head convolutional attention (MMHCA) into two deep neural architectures\nfor super-resolution and conduct experiments on three data sets. Our empirical\nresults show the superiority of our attention module over the state-of-the-art\nattention mechanisms used in super-resolution. Moreover, we conduct an ablation\nstudy to assess the impact of the components involved in our attention module,\ne.g. the number of inputs or the number of heads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miron_A/0/1/0/all/0/1\">Andreea-Iuliana Miron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Savencu_O/0/1/0/all/0/1\">Olivian Savencu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verga_N/0/1/0/all/0/1\">Nicolae Verga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Reliable and Explainable AI Model for Solid Pulmonary Nodule Diagnosis. (arXiv:2204.04219v1 [eess.IV])","link":"http://arxiv.org/abs/2204.04219","description":"<p>Lung cancer has the highest mortality rate of deadly cancers in the world.\nEarly detection is essential to treatment of lung cancer. However, detection\nand accurate diagnosis of pulmonary nodules depend heavily on the experiences\nof radiologists and can be a heavy workload for them. Computer-aided diagnosis\n(CAD) systems have been developed to assist radiologists in nodule detection\nand diagnosis, greatly easing the workload while increasing diagnosis accuracy.\nRecent development of deep learning, greatly improved the performance of CAD\nsystems. However, lack of model reliability and interpretability remains a\nmajor obstacle for its large-scale clinical application. In this work, we\nproposed a multi-task explainable deep-learning model for pulmonary nodule\ndiagnosis. Our neural model can not only predict lesion malignancy but also\nidentify relevant manifestations. Further, the location of each manifestation\ncan also be visualized for visual interpretability. Our proposed neural model\nachieved a test AUC of 0.992 on LIDC public dataset and a test AUC of 0.923 on\nour in-house dataset. Moreover, our experimental results proved that by\nincorporating manifestation identification tasks into the multi-task model, the\naccuracy of the malignancy classification can also be improved. This multi-task\nexplainable model may provide a scheme for better interaction with the\nradiologists in a clinical environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chengxiu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_M/0/1/0/all/0/1\">Mei Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Based American Sign Language Classification Approach via Deep Learning. (arXiv:2204.04235v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04235","description":"<p>Hearing-impaired is the disability of partial or total hearing loss that\ncauses a significant problem for communication with other people in society.\nAmerican Sign Language (ASL) is one of the sign languages that most commonly\nused language used by Hearing impaired communities to communicate with each\nother. In this paper, we proposed a simple deep learning model that aims to\nclassify the American Sign Language letters as a step in a path for removing\ncommunication barriers that are related to disabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_N/0/1/0/all/0/1\">Nelly Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ElSayed_Z/0/1/0/all/0/1\">Zag ElSayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1\">Anthony S. Maida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChildCI Framework: Analysis of Motor and Cognitive Development in Children-Computer Interaction for Age Detection. (arXiv:2204.04236v1 [cs.HC])","link":"http://arxiv.org/abs/2204.04236","description":"<p>This article presents a comprehensive analysis of the different tests\nproposed in the recent ChildCI framework, proving its potential for generating\na better understanding of children's neuromotor and cognitive development along\ntime, as well as their possible application in other research areas such as\ne-Health and e-Learning. In particular, we propose a set of over 100 global\nfeatures related to motor and cognitive aspects of the children interaction\nwith mobile devices, some of them collected and adapted from the literature.\nFurthermore, we analyse the robustness and discriminative power of the proposed\nfeature set including experimental results for the task of children age group\ndetection based on their motor and cognitive behaviors. Two different scenarios\nare considered in this study: i) single-test scenario, and ii) multiple-test\nscenario. Results over 93% accuracy are achieved using the publicly available\nChildCIdb_v1 database (over 400 children from 18 months to 8 years old),\nproving the high correlation of children's age with the way they interact with\nmobile devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_J/0/1/0/all/0/1\">Juan Carlos Ruiz-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herreros_Rodriguez_J/0/1/0/all/0/1\">Jaime Herreros-Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elastic shape analysis of surfaces with second-order Sobolev metrics: a comprehensive numerical framework. (arXiv:2204.04238v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04238","description":"<p>This paper introduces a set of numerical methods for Riemannian shape\nanalysis of 3D surfaces within the setting of invariant (elastic) second-order\nSobolev metrics. More specifically, we address the computation of geodesics and\ngeodesic distances between parametrized or unparametrized immersed surfaces\nrepresented as 3D meshes. Building on this, we develop tools for the\nstatistical shape analysis of sets of surfaces, including methods for\nestimating Karcher means and performing tangent PCA on shape populations, and\nfor computing parallel transport along paths of surfaces. Our proposed approach\nfundamentally relies on a relaxed variational formulation for the geodesic\nmatching problem via the use of varifold fidelity terms, which enable us to\nenforce reparametrization independence when computing geodesics between\nunparametrized surfaces, while also yielding versatile algorithms that allow us\nto compare surfaces with varying sampling or mesh structures. Importantly, we\ndemonstrate how our relaxed variational framework can be extended to tackle\npartially observed data. The different benefits of our numerical pipeline are\nillustrated over various examples, synthetic and real.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartman_E/0/1/0/all/0/1\">Emmanuel Hartman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukurdeep_Y/0/1/0/all/0/1\">Yashil Sukurdeep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klassen_E/0/1/0/all/0/1\">Eric Klassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charon_N/0/1/0/all/0/1\">Nicolas Charon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1\">Martin Bauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Influence of Receptive Field and Network Complexity in Neural-Network-Guided TEM Image Analysis. (arXiv:2204.04250v1 [cond-mat.mtrl-sci])","link":"http://arxiv.org/abs/2204.04250","description":"<p>Trained neural networks are promising tools to analyze the ever-increasing\namount of scientific image data, but it is unclear how to best customize these\nnetworks for the unique features in transmission electron micrographs. Here, we\nsystematically examine how neural network architecture choices affect how\nneural networks segment, or pixel-wise separate, crystalline nanoparticles from\namorphous background in transmission electron microscopy (TEM) images. We focus\non decoupling the influence of receptive field, or the area of the input image\nthat contributes to the output decision, from network complexity, which\ndictates the number of trainable parameters. We find that for low-resolution\nTEM images which rely on amplitude contrast to distinguish nanoparticles from\nbackground, the receptive field does not significantly influence segmentation\nperformance. On the other hand, for high-resolution TEM images which rely on a\ncombination of amplitude and phase contrast changes to identify nanoparticles,\nreceptive field is a key parameter for increased performance, especially in\nimages with minimal amplitude contrast. Our results provide insight and\nguidance as to how to adapt neural networks for applications with TEM datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Sytwu_K/0/1/0/all/0/1\">Katherine Sytwu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Groschner_C/0/1/0/all/0/1\">Catherine Groschner</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Scott_M/0/1/0/all/0/1\">Mary C. Scott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Improving Cross-dataset Generalization of Deepfake Detectors. (arXiv:2204.04285v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04285","description":"<p>Facial manipulation by deep fake has caused major security risks and raised\nsevere societal concerns. As a countermeasure, a number of deep fake detection\nmethods have been proposed recently. Most of them model deep fake detection as\na binary classification problem using a backbone convolutional neural network\n(CNN) architecture pretrained for the task. These CNN-based methods have\ndemonstrated very high efficacy in deep fake detection with the Area under the\nCurve (AUC) as high as 0.99. However, the performance of these methods degrades\nsignificantly when evaluated across datasets. In this paper, we formulate deep\nfake detection as a hybrid combination of supervised and reinforcement learning\n(RL) to improve its cross-dataset generalization performance. The proposed\nmethod chooses the top-k augmentations for each test sample by an RL agent in\nan image-specific manner. The classification scores, obtained using CNN, of all\nthe augmentations of each test image are averaged together for final real or\nfake classification. Through extensive experimental validation, we demonstrate\nthe superiority of our method over existing published research in cross-dataset\ngeneralization of deep fake detectors, thus obtaining state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nadimpalli_A/0/1/0/all/0/1\">Aakash Varma Nadimpalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rattani_A/0/1/0/all/0/1\">Ajita Rattani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to modulate random weights can induce task-specific contexts for economical meta and continual learning. (arXiv:2204.04297v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04297","description":"<p>Neural networks are vulnerable to catastrophic forgetting when data\ndistributions are non-stationary during continual online learning; learning of\na later task often leads to forgetting of an earlier task. One solution\napproach is model-agnostic continual meta-learning, whereby both task-specific\nand meta parameters are trained. Here, we depart from this view and introduce a\nnovel neural-network architecture inspired by neuromodulation in biological\nnervous systems. Neuromodulation is the biological mechanism that dynamically\ncontrols and fine-tunes synaptic dynamics to complement the behavioral context\nin real-time, which has received limited attention in machine learning. We\nintroduce a single-hidden-layer network that learns only a relatively small\ncontext vector per task (task-specific parameters) that neuromodulates\nunchanging, randomized weights (meta parameters) that transform the input. We\nshow that when task boundaries are available, this approach can eliminate\ncatastrophic forgetting entirely while also drastically reducing the number of\nlearnable parameters relative to other context-vector-based approaches.\nFurthermore, by combining this model with a simple meta-learning approach for\ninferring task identity, we demonstrate that the model can be generalized into\na framework to perform continual learning without knowledge of task boundaries.\nFinally, we showcase the framework in a supervised continual online learning\nscenario and discuss the implications of the proposed formalism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jinyung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenting across places: The need for fair transfer learning with satellite imagery. (arXiv:2204.04358v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04358","description":"<p>The increasing availability of high-resolution satellite imagery has enabled\nthe use of machine learning to support land-cover measurement and inform\npolicy-making. However, labelling satellite images is expensive and is\navailable for only some locations. This prompts the use of transfer learning to\nadapt models from data-rich locations to others. Given the potential for\nhigh-impact applications of satellite imagery across geographies, a systematic\nassessment of transfer learning implications is warranted. In this work, we\nconsider the task of land-cover segmentation and study the fairness\nimplications of transferring models across locations. We leverage a large\nsatellite image segmentation benchmark with 5987 images from 18 districts (9\nurban and 9 rural). Via fairness metrics we quantify disparities in model\nperformance along two axes -- across urban-rural locations and across\nland-cover classes. Findings show that state-of-the-art models have better\noverall accuracy in rural areas compared to urban areas, through unsupervised\ndomain adaptation methods transfer learning better to urban versus rural areas\nand enlarge fairness gaps. In analysis of reasons for these findings, we show\nthat raw satellite images are overall more dissimilar between source and target\ndistricts for rural than for urban locations. This work highlights the need to\nconduct fairness analysis for satellite imagery segmentation models and\nmotivates the development of methods for fair transfer learning in order not to\nintroduce disparities between places, particularly urban and rural locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Harvineet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chok_L/0/1/0/all/0/1\">Lazarus Chok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chunara_R/0/1/0/all/0/1\">Rumi Chunara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention guided global enhancement and local refinement network for semantic segmentation. (arXiv:2204.04363v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04363","description":"<p>The encoder-decoder architecture is widely used as a lightweight semantic\nsegmentation network. However, it struggles with a limited performance compared\nto a well-designed Dilated-FCN model for two major problems. First, commonly\nused upsampling methods in the decoder such as interpolation and deconvolution\nsuffer from a local receptive field, unable to encode global contexts. Second,\nlow-level features may bring noises to the network decoder through skip\nconnections for the inadequacy of semantic concepts in early encoder layers. To\ntackle these challenges, a Global Enhancement Method is proposed to aggregate\nglobal information from high-level feature maps and adaptively distribute them\nto different decoder layers, alleviating the shortage of global contexts in the\nupsampling process. Besides, a Local Refinement Module is developed by\nutilizing the decoder features as the semantic guidance to refine the noisy\nencoder features before the fusion of these two (the decoder features and the\nencoder features). Then, the two methods are integrated into a Context Fusion\nBlock, and based on that, a novel Attention guided Global enhancement and Local\nrefinement Network (AGLN) is elaborately designed. Extensive experiments on\nPASCAL Context, ADE20K, and PASCAL VOC 2012 datasets have demonstrated the\neffectiveness of the proposed approach. In particular, with a vanilla\nResNet-101 backbone, AGLN achieves the state-of-the-art result (56.23% mean\nIoU) on the PASCAL Context dataset. The code is available at\nhttps://github.com/zhasen1996/AGLN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Meng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel Pruning In Quantization-aware Training: An Adaptive Projection-gradient Descent-shrinkage-splitting Method. (arXiv:2204.04375v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04375","description":"<p>We propose an adaptive projection-gradient descent-shrinkage-splitting method\n(APGDSSM) to integrate penalty based channel pruning into quantization-aware\ntraining (QAT). APGDSSM concurrently searches weights in both the quantized\nsubspace and the sparse subspace. APGDSSM uses shrinkage operator and a\nsplitting technique to create sparse weights, as well as the Group Lasso\npenalty to push the weight sparsity into channel sparsity. In addition, we\npropose a novel complementary transformed l1 penalty to stabilize the training\nfor extreme compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robotic Surgery Remote Mentoring via AR with 3D Scene Streaming and Hand Interaction. (arXiv:2204.04377v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04377","description":"<p>With the growing popularity of robotic surgery, education becomes\nincreasingly important and urgently needed for the sake of patient safety.\nHowever, experienced surgeons have limited accessibility due to their busy\nclinical schedule or working in a distant city, thus can hardly provide\nsufficient education resources for novices. Remote mentoring, as an effective\nway, can help solve this problem, but traditional methods are limited to plain\ntext, audio, or 2D video, which are not intuitive nor vivid. Augmented reality\n(AR), a thriving technique being widely used for various education scenarios,\nis promising to offer new possibilities of visual experience and interactive\nteaching. In this paper, we propose a novel AR-based robotic surgery remote\nmentoring system with efficient 3D scene visualization and natural 3D hand\ninteraction. Using a head-mounted display (i.e., HoloLens), the mentor can\nremotely monitor the procedure streamed from the trainee's operation side. The\nmentor can also provide feedback directly with hand gestures, which is in-turn\ntransmitted to the trainee and viewed in surgical console as guidance. We\ncomprehensively validate the system on both real surgery stereo videos and\nex-vivo scenarios of common robotic training tasks (i.e., peg-transfer and\nsuturing). Promising results are demonstrated regarding the fidelity of\nstreamed scene visualization, the accuracy of feedback with hand interaction,\nand the low-latency of each component in the entire remote mentoring system.\nThis work showcases the feasibility of leveraging AR technology for reliable,\nflexible and low-cost solutions to robotic surgical education, and holds great\npotential for clinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengkun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond 3DMM: Learning to Capture High-fidelity 3D Face Shape. (arXiv:2204.04379v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04379","description":"<p>3D Morphable Model (3DMM) fitting has widely benefited face analysis due to\nits strong 3D priori. However, previous reconstructed 3D faces suffer from\ndegraded visual verisimilitude due to the loss of fine-grained geometry, which\nis attributed to insufficient ground-truth 3D shapes, unreliable training\nstrategies and limited representation power of 3DMM. To alleviate this issue,\nthis paper proposes a complete solution to capture the personalized shape so\nthat the reconstructed shape looks identical to the corresponding person.\nSpecifically, given a 2D image as the input, we virtually render the image in\nseveral calibrated views to normalize pose variations while preserving the\noriginal image geometry. A many-to-one hourglass network serves as the\nencode-decoder to fuse multiview features and generate vertex displacements as\nthe fine-grained geometry. Besides, the neural network is trained by directly\noptimizing the visual effect, where two 3D shapes are compared by measuring the\nsimilarity between the multiview images rendered from the shapes. Finally, we\npropose to generate the ground-truth 3D shapes by registering RGB-D images\nfollowed by pose and shape augmentation, providing sufficient data for network\ntraining. Experiments on several challenging protocols demonstrate the superior\nreconstruction accuracy of our proposal on the face shape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A dataset of ant colonies motion trajectories in indoor and outdoor scenes for social cluster behavior study. (arXiv:2204.04380v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04380","description":"<p>Motion and interaction of social insects (such as ants) have been studied by\nmany researchers to understand the clustering mechanism. Most studies in the\nfield of ant behavior have only focused on indoor environments, while outdoor\nenvironments are still underexplored. In this paper, we collect 10 videos of\nant colonies from different indoor and outdoor scenes. And we develop an image\nsequence marking software named VisualMarkData, which enables us to provide\nannotations of ants in the video. In all 5354 frames, the location information\nand the identification number of each ant are recorded for a total of 712 ants\nand 114112 annotations. Moreover, we provide visual analysis tools to assess\nand validate the technical quality and reproducibility of our data. It is hoped\nthat this dataset will contribute to a deeper exploration on the behavior of\nthe ant colony.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Meihong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaoyan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaoyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shihui Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Unsupervised Domain Adaptation for Face Recognition. (arXiv:2204.04382v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04382","description":"<p>Given labeled data in a source domain, unsupervised domain adaptation has\nbeen widely adopted to generalize models for unlabeled data in a target domain,\nwhose data distributions are different. However, existing works are\ninapplicable to face recognition under privacy constraints because they require\nsharing of sensitive face images between domains. To address this problem, we\npropose federated unsupervised domain adaptation for face recognition, FedFR.\nFedFR jointly optimizes clustering-based domain adaptation and federated\nlearning to elevate performance on the target domain. Specifically, for\nunlabeled data in the target domain, we enhance a clustering algorithm with\ndistance constrain to improve the quality of predicted pseudo labels. Besides,\nwe propose a new domain constraint loss (DCL) to regularize source domain\ntraining in federated learning. Extensive experiments on a newly constructed\nbenchmark demonstrate that FedFR outperforms the baseline and classic methods\non the target domain by 3% to 14% on different evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_X/0/1/0/all/0/1\">Xin Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuesen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Two Dimensions of Worst-case Training and the Integrated Effect for Out-of-domain Generalization. (arXiv:2204.04384v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04384","description":"<p>Training with an emphasis on \"hard-to-learn\" components of the data has been\nproven as an effective method to improve the generalization of machine learning\nmodels, especially in the settings where robustness (e.g., generalization\nacross distributions) is valued. Existing literature discussing this\n\"hard-to-learn\" concept are mainly expanded either along the dimension of the\nsamples or the dimension of the features. In this paper, we aim to introduce a\nsimple view merging these two dimensions, leading to a new, simple yet\neffective, heuristic to train machine learning models by emphasizing the\nworst-cases on both the sample and the feature dimensions. We name our method\nW2D following the concept of \"Worst-case along Two Dimensions\". We validate the\nidea and demonstrate its empirical strength over standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divergence-aware Federated Self-Supervised Learning. (arXiv:2204.04385v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04385","description":"<p>Self-supervised learning (SSL) is capable of learning remarkable\nrepresentations from centrally available data. Recent works further implement\nfederated learning with SSL to learn from rapidly growing decentralized\nunlabeled images (e.g., from cameras and phones), often resulted from privacy\nconstraints. Extensive attention has been paid to SSL approaches based on\nSiamese networks. However, such an effort has not yet revealed deep insights\ninto various fundamental building blocks for the federated self-supervised\nlearning (FedSSL) architecture. We aim to fill in this gap via in-depth\nempirical study and propose a new method to tackle the non-independently and\nidentically distributed (non-IID) data problem of decentralized data. Firstly,\nwe introduce a generalized FedSSL framework that embraces existing SSL methods\nbased on Siamese networks and presents flexibility catering to future methods.\nIn this framework, a server coordinates multiple clients to conduct SSL\ntraining and periodically updates local models of clients with the aggregated\nglobal model. Using the framework, our study uncovers unique insights of\nFedSSL: 1) stop-gradient operation, previously reported to be essential, is not\nalways necessary in FedSSL; 2) retaining local knowledge of clients in FedSSL\nis particularly beneficial for non-IID data. Inspired by the insights, we then\npropose a new approach for model update, Federated Divergence-aware Exponential\nMoving Average update (FedEMA). FedEMA updates local models of clients\nadaptively using EMA of the global model, where the decay rate is dynamically\nmeasured by model divergence. Extensive experiments demonstrate that FedEMA\noutperforms existing methods by 3-4% on linear evaluation. We hope that this\nwork will provide useful insights for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Stage Approach Toward Hyperspectral Image Super-Resolution. (arXiv:2204.04387v1 [eess.IV])","link":"http://arxiv.org/abs/2204.04387","description":"<p>Hyperspectral image produces high spectral resolution at the sacrifice of\nspatial resolution. Without reducing the spectral resolution, improving the\nresolution in the spatial domain is a very challenging problem. Motivated by\nthe discovery that hyperspectral image exhibits high similarity between\nadjacent bands in a large spectral range, in this paper, we explore a new\nstructure for hyperspectral image super-resolution (DualSR), leading to a\ndual-stage design, i.e., coarse stage and fine stage. In coarse stage, five\nbands with high similarity in a certain spectral range are divided into three\ngroups, and the current band is guided to study the potential knowledge. Under\nthe action of alternative spectral fusion mechanism, the coarse SR image is\nsuper-resolved in band-by-band. In order to build model from a global\nperspective, an enhanced back-projection method via spectral angle constraint\nis developed in fine stage to learn the content of spatial-spectral\nconsistency, dramatically improving the performance gain. Extensive experiments\ndemonstrate the effectiveness of the proposed coarse stage and fine stage.\nBesides, our network produces state-of-the-art results against existing works\nin terms of spatial reconstruction and spectral fidelity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xiuping Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E^2TAD: An Energy-Efficient Tracking-based Action Detector. (arXiv:2204.04416v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04416","description":"<p>Video action detection (spatio-temporal action localization) is usually the\nstarting point for human-centric intelligent analysis of videos nowadays. It\nhas high practical impacts for many applications across robotics, security,\nhealthcare, etc. The two-stage paradigm of Faster R-CNN inspires a standard\nparadigm of video action detection in object detection, i.e., firstly\ngenerating person proposals and then classifying their actions. However, none\nof the existing solutions could provide fine-grained action detection to the\n\"who-when-where-what\" level. This paper presents a tracking-based solution to\naccurately and efficiently localize predefined key actions spatially (by\npredicting the associated target IDs and locations) and temporally (by\npredicting the time in exact frame indices). This solution won first place in\nthe UAV-Video Track of 2021 Low-Power Computer Vision Challenge (LPCVC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hao-Yu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Siqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1\">Taiyu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_P/0/1/0/all/0/1\">Pengcheng Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping Temporary Slums from Satellite Imagery using a Semi-Supervised Approach. (arXiv:2204.04419v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04419","description":"<p>One billion people worldwide are estimated to be living in slums, and\ndocumenting and analyzing these regions is a challenging task. As compared to\nregular slums; the small, scattered and temporary nature of temporary slums\nmakes data collection and labeling tedious and time-consuming. To tackle this\nchallenging problem of temporary slums detection, we present a semi-supervised\ndeep learning segmentation-based approach; with the strategy to detect initial\nseed images in the zero-labeled data settings. A small set of seed samples (32\nin our case) are automatically discovered by analyzing the temporal changes,\nwhich are manually labeled to train a segmentation and representation learning\nmodule. The segmentation module gathers high dimensional image representations,\nand the representation learning module transforms image representations into\nembedding vectors. After that, a scoring module uses the embedding vectors to\nsample images from a large pool of unlabeled images and generates pseudo-labels\nfor the sampled images. These sampled images with their pseudo-labels are added\nto the training set to update the segmentation and representation learning\nmodules iteratively. To analyze the effectiveness of our technique, we\nconstruct a large geographically marked dataset of temporary slums. This\ndataset constitutes more than 200 potential temporary slum locations (2.28\nsquare kilometers) found by sieving sixty-eight thousand images from 12\nmetropolitan cities of Pakistan covering 8000 square kilometers. Furthermore,\nour proposed method outperforms several competitive semi-supervised semantic\nsegmentation baselines on a similar setting. The code and the dataset will be\nmade publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehman_M/0/1/0/all/0/1\">M. Fasi ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_I/0/1/0/all/0/1\">Izza Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultani_W/0/1/0/all/0/1\">Waqas Sultani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbiased Directed Object Attention Graph for Object Navigation. (arXiv:2204.04421v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04421","description":"<p>Object navigation tasks require agents to locate specific objects in unknown\nenvironments based on visual information. Previously, graph convolutions were\nused to implicitly explore the relationships between objects. However, due to\ndifferences in visibility among objects, it is easy to generate biases in\nobject attention. Thus, in this paper, we propose a directed object attention\n(DOA) graph to guide the agent in explicitly learning the attention\nrelationships between objects, thereby reducing the object attention bias. In\nparticular, we use the DOA graph to perform unbiased adaptive object attention\n(UAOA) on the object features and unbiased adaptive image attention (UAIA) on\nthe raw images, respectively. To distinguish features in different branches, a\nconcise adaptive branch energy distribution (ABED) method is proposed. We\nassess our methods on the AI2-Thor dataset. Compared with the state-of-the-art\n(SOTA) method, our method reports 7.4%, 8.1% and 17.6% increase in success rate\n(SR), success weighted by path length (SPL) and success weighted by action\nefficiency (SAE), respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_R/0/1/0/all/0/1\">Ronghao Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhuofan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liuyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zongtao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengju Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Differential Filters for Fast and Communication-Efficient Federated Learning. (arXiv:2204.04424v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04424","description":"<p>Federated learning (FL) scenarios inherently generate a large communication\noverhead by frequently transmitting neural network updates between clients and\nserver. To minimize the communication cost, introducing sparsity in conjunction\nwith differential updates is a commonly used technique. However, sparse model\nupdates can slow down convergence speed or unintentionally skip certain update\naspects, e.g., learned features, if error accumulation is not properly\naddressed. In this work, we propose a new scaling method operating at the\ngranularity of convolutional filters which 1) compensates for highly sparse\nupdates in FL processes, 2) adapts the local models to new data domains by\nenhancing some features in the filter space while diminishing others and 3)\nmotivates extra sparsity in updates and thus achieves higher compression\nratios, i.e., savings in the overall data transfer. Compared to unscaled\nupdates and previous work, experimental results on different computer vision\ntasks (Pascal VOC, CIFAR10, Chest X-Ray) and neural networks (ResNets,\nMobileNets, VGGs) in uni-, bidirectional and partial update FL settings show\nthat the proposed method improves the performance of the central server model\nwhile converging faster and reducing the total amount of transmitted data by up\nto 377 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Becking_D/0/1/0/all/0/1\">Daniel Becking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoffer_H/0/1/0/all/0/1\">Heiner Kirchhoffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tech_G/0/1/0/all/0/1\">Gerhard Tech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haase_P/0/1/0/all/0/1\">Paul Haase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Karsten M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_H/0/1/0/all/0/1\">Heiko Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ManiTrans: Entity-Level Text-Guided Image Manipulation via Token-wise Semantic Alignment and Generation. (arXiv:2204.04428v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04428","description":"<p>Existing text-guided image manipulation methods aim to modify the appearance\nof the image or to edit a few objects in a virtual or simple scenario, which is\nfar from practical application. In this work, we study a novel task on\ntext-guided image manipulation on the entity level in the real world. The task\nimposes three basic requirements, (1) to edit the entity consistent with the\ntext descriptions, (2) to preserve the text-irrelevant regions, and (3) to\nmerge the manipulated entity into the image naturally. To this end, we propose\na new transformer-based framework based on the two-stage image synthesis\nmethod, namely \\textbf{ManiTrans}, which can not only edit the appearance of\nentities but also generate new entities corresponding to the text guidance. Our\nframework incorporates a semantic alignment module to locate the image regions\nto be manipulated, and a semantic loss to help align the relationship between\nthe vision and language. We conduct extensive experiments on the real datasets,\nCUB, Oxford, and COCO datasets to verify that our method can distinguish the\nrelevant and irrelevant regions and achieve more precise and flexible\nmanipulation compared with baseline methods. The project homepage is\n\\url{https://jawang19.github.io/manitrans}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guansong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSTR-Net: High Spatio-Temporal Resolution Video Generation For Wide Area Surveillance. (arXiv:2204.04435v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04435","description":"<p>Wide area surveillance has many applications and tracking of objects under\nobservation is an important task, which often needs high spatio-temporal\nresolution (HSTR) video for better precision. This paper presents the usage of\nmultiple video feeds for the generation of HSTR video as an extension of\nreference based super resolution (RefSR). One feed captures video at high\nspatial resolution with low frame rate (HSLF) while the other captures low\nspatial resolution and high frame rate (LSHF) video simultaneously for the same\nscene. The main purpose is to create an HSTR video from the fusion of HSLF and\nLSHF videos. In this paper we propose an end-to-end trainable deep network that\nperforms optical flow estimation and frame reconstruction by combining inputs\nfrom both video feeds. The proposed architecture provides significant\nimprovement over existing video frame interpolation and RefSR techniques in\nterms of objective PSNR and SSIM metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suluhan_H/0/1/0/all/0/1\">H. Umut Suluhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ates_H/0/1/0/all/0/1\">Hasan F. Ates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunturk_B/0/1/0/all/0/1\">Bahadir K. Gunturk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided deep learning by subaperture decomposition: ocean patterns from SAR imagery. (arXiv:2204.04438v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04438","description":"<p>Spaceborne synthetic aperture radar can provide meters scale images of the\nocean surface roughness day or night in nearly all weather conditions. This\nmakes it a unique asset for many geophysical applications. Sentinel 1 SAR wave\nmode vignettes have made possible to capture many important oceanic and\natmospheric phenomena since 2014. However, considering the amount of data\nprovided, expanding applications requires a strategy to automatically process\nand extract geophysical parameters. In this study, we propose to apply\nsubaperture decomposition as a preprocessing stage for SAR deep learning\nmodels. Our data centring approach surpassed the baseline by 0.7, obtaining\nstate of the art on the TenGeoPSARwv data set. In addition, we empirically\nshowed that subaperture decomposition could bring additional information over\nthe original vignette, by rising the number of clusters for an unsupervised\nsegmentation method. Overall, we encourage the development of data centring\napproaches, showing that, data preprocessing could bring significant\nperformance improvements over existing deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anghel_A/0/1/0/all/0/1\">Andrei Anghel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datcu_M/0/1/0/all/0/1\">Mihai Datcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapron_B/0/1/0/all/0/1\">Bertrand Chapron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise-based Enhancement for Foveated Rendering. (arXiv:2204.04455v1 [cs.GR])","link":"http://arxiv.org/abs/2204.04455","description":"<p>Human visual sensitivity to spatial details declines towards the periphery.\nNovel image synthesis techniques, so-called foveated rendering, exploit this\nobservation and reduce the spatial resolution of synthesized images for the\nperiphery, avoiding the synthesis of high-spatial-frequency details that are\ncostly to generate but not perceived by a viewer. However, contemporary\ntechniques do not make a clear distinction between the range of spatial\nfrequencies that must be reproduced and those that can be omitted. For a given\neccentricity, there is a range of frequencies that are detectable but not\nresolvable. While the accurate reproduction of these frequencies is not\nrequired, an observer can detect their absence if completely omitted. We use\nthis observation to improve the performance of existing foveated rendering\ntechniques. We demonstrate that this specific range of frequencies can be\nefficiently replaced with procedural noise whose parameters are carefully tuned\nto image content and human perception. Consequently, these frequencies do not\nhave to be synthesized during rendering, allowing more aggressive foveation,\nand they can be replaced by noise generated in a less expensive post-processing\nstep, leading to improved performance of the rendering system. Our main\ncontribution is a perceptually-inspired technique for deriving the parameters\nof the noise required for the enhancement and its calibration. The method\noperates on rendering output and runs at rates exceeding 200FPS at 4K\nresolution, making it suitable for integration with real-time foveated\nrendering systems for VR and AR devices. We validate our results and compare\nthem to the existing contrast enhancement technique in user experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tariq_T/0/1/0/all/0/1\">Taimoor Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tursun_C/0/1/0/all/0/1\">Cara Tursun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Didyk_P/0/1/0/all/0/1\">Piotr Didyk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refining time-space traffic diagrams: A multiple linear regression model. (arXiv:2204.04457v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04457","description":"<p>A time-space traffic (TS) diagram that presents traffic states in time-space\ncells with colors is one of the most important traffic analysis and\nvisualization tools. Despite its importance for transportation research and\nengineering, most TS diagrams that have already existed or are being produced\nare too coarse to exhibit detailed traffic dynamics due to the limitation of\nthe current information technology and traffic infrastructure investment. To\nincrease the resolution of a TS diagram and make it present more traffic\ndetails, this paper introduces a TS diagram refinement problem and proposes a\nmultiple linear regression-based model to solve the problem. Two tests, which\nattempt to increase the resolution of a TS diagram for 4 and 16 times,\nrespectively, are carried out to evaluate the performance of the proposed\nmodel. The data collected from different time, different location and even\ndifferent country is involved to thoroughly evaluate the accuracy and\ntransferability of the proposed model. The strict tests with diverse data show\nthat the proposed model, although it is simple in form, is able to refine a TS\ndiagram with a promising accuracy and reliable transferability. The proposed\nrefinement model will \"save\" those widely-existing TS diagrams from their\nblurry \"faces\" and make it possible to learn more traffic details from those TS\ndiagrams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengbing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A3CLNN: Spatial, Spectral and Multiscale Attention ConvLSTM Neural Network for Multisource Remote Sensing Data Classification. (arXiv:2204.04462v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04462","description":"<p>The problem of effectively exploiting the information multiple data sources\nhas become a relevant but challenging research topic in remote sensing. In this\npaper, we propose a new approach to exploit the complementarity of two data\nsources: hyperspectral images (HSIs) and light detection and ranging (LiDAR)\ndata. Specifically, we develop a new dual-channel spatial, spectral and\nmultiscale attention convolutional long short-term memory neural network\n(called dual-channel A3CLNN) for feature extraction and classification of\nmultisource remote sensing data. Spatial, spectral and multiscale attention\nmechanisms are first designed for HSI and LiDAR data in order to learn\nspectral- and spatial-enhanced feature representations, and to represent\nmultiscale information for different classes. In the designed fusion network, a\nnovel composite attention learning mechanism (combined with a three-level\nfusion strategy) is used to fully integrate the features in these two data\nsources. Finally, inspired by the idea of transfer learning, a novel stepwise\ntraining strategy is designed to yield a final classification result. Our\nexperimental results, conducted on several multisource remote sensing data\nsets, demonstrate that the newly proposed dual-channel A3CLNN exhibits better\nfeature representation ability (leading to more competitive classification\nperformance) than other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Heng-Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wen-Shuai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1\">Antonio Plaza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultrasound Signal Processing: From Models to Deep Learning. (arXiv:2204.04466v1 [eess.SP])","link":"http://arxiv.org/abs/2204.04466","description":"<p>Medical ultrasound imaging relies heavily on high-quality signal processing\nalgorithms to provide reliable and interpretable image reconstructions.\nHand-crafted reconstruction methods, often based on approximations of the\nunderlying measurement model, are useful in practice, but notoriously fall\nbehind in terms of image quality. More sophisticated solutions, based on\nstatistical modelling, careful parameter tuning, or through increased model\ncomplexity, can be sensitive to different environments. Recently, deep learning\nbased methods have gained popularity, which are optimized in a data-driven\nfashion. These model-agnostic methods often rely on generic model structures,\nand require vast training data to converge to a robust solution. A relatively\nnew paradigm combines the power of the two: leveraging data-driven deep\nlearning, as well as exploiting domain knowledge. These model-based solutions\nyield high robustness, and require less trainable parameters and training data\nthan conventional neural networks. In this work we provide an overview of these\nmethods from the recent literature, and discuss a wide variety of ultrasound\napplications. We aim to inspire the reader to further research in this area,\nand to address the opportunities within the field of ultrasound signal\nprocessing. We conclude with a future perspective on these model-based deep\nlearning techniques for medical ultrasound applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luijten_B/0/1/0/all/0/1\">Ben Luijten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chennakeshava_N/0/1/0/all/0/1\">Nishith Chennakeshava</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mischi_M/0/1/0/all/0/1\">Massimo Mischi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sloun_R/0/1/0/all/0/1\">Ruud J.G. van Sloun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S4OD: Semi-Supervised learning for Single-Stage Object Detection. (arXiv:2204.04492v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04492","description":"<p>Single-stage detectors suffer from extreme foreground-background class\nimbalance, while two-stage detectors do not. Therefore, in semi-supervised\nobject detection, two-stage detectors can deliver remarkable performance by\nonly selecting high-quality pseudo labels based on classification scores.\nHowever, directly applying this strategy to single-stage detectors would\naggravate the class imbalance with fewer positive samples. Thus, single-stage\ndetectors have to consider both quality and quantity of pseudo labels\nsimultaneously. In this paper, we design a dynamic self-adaptive threshold\n(DSAT) strategy in classification branch, which can automatically select pseudo\nlabels to achieve an optimal trade-off between quality and quantity. Besides,\nto assess the regression quality of pseudo labels in single-stage detectors, we\npropose a module to compute the regression uncertainty of boxes based on\nNon-Maximum Suppression. By leveraging only 10% labeled data from COCO, our\nmethod achieves 35.0% AP on anchor-free detector (FCOS) and 32.9% on\nanchor-based detector (RetinaNet).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yueming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaolin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_T/0/1/0/all/0/1\">Tengfei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Runbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_H/0/1/0/all/0/1\">Hua Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoshan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepLIIF: An Online Platform for Quantification of Clinical Pathology Slides. (arXiv:2204.04494v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04494","description":"<p>In the clinic, resected tissue samples are stained with Hematoxylin-and-Eosin\n(H&amp;E) and/or Immunhistochemistry (IHC) stains and presented to the pathologists\non glass slides or as digital scans for diagnosis and assessment of disease\nprogression. Cell-level quantification, e.g. in IHC protein expression scoring,\ncan be extremely inefficient and subjective. We present DeepLIIF\n(https://deepliif.org), a first free online platform for efficient and\nreproducible IHC scoring. DeepLIIF outperforms current state-of-the-art\napproaches (relying on manual error-prone annotations) by virtually restaining\nclinical IHC slides with more informative multiplex immunofluorescence\nstaining. Our DeepLIIF cloud-native platform supports (1) more than 150\nproprietary/non-proprietary input formats via the Bio-Formats standard, (2)\ninteractive adjustment, visualization, and downloading of the IHC\nquantification results and the accompanying restained images, (3) consumption\nof an exposed workflow API programmatically or through interactive plugins for\nopen source whole slide image viewers such as QuPath/ImageJ, and (4) auto\nscaling to efficiently scale GPU resources based on user demand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghahremani_P/0/1/0/all/0/1\">Parmida Ghahremani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marino_J/0/1/0/all/0/1\">Joseph Marino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_R/0/1/0/all/0/1\">Ricardo Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Exploitation of Deepfake Model Recognition. (arXiv:2204.04513v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04513","description":"<p>Despite recent advances in Generative Adversarial Networks (GANs), with\nspecial focus to the Deepfake phenomenon there is no a clear understanding\nneither in terms of explainability nor of recognition of the involved models.\nIn particular, the recognition of a specific GAN model that generated the\ndeepfake image compared to many other possible models created by the same\ngenerative architecture (e.g. StyleGAN) is a task not yet completely addressed\nin the state-of-the-art. In this work, a robust processing pipeline to evaluate\nthe possibility to point-out analytic fingerprints for Deepfake model\nrecognition is presented. After exploiting the latent space of 50 slightly\ndifferent models through an in-depth analysis on the generated images, a proper\nencoder was trained to discriminate among these models obtaining a\nclassification accuracy of over 96%. Once demonstrated the possibility to\ndiscriminate extremely similar images, a dedicated metric exploiting the\ninsights discovered in the latent space was introduced. By achieving a final\naccuracy of more than 94% for the Model Recognition task on images generated by\nmodels not employed in the training phase, this study takes an important step\nin countering the Deepfake phenomenon introducing a sort of signature in some\nsense similar to those employed in the multimedia forensics field (e.g. for\ncamera source identification task, image ballistics task, etc).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guarnera_L/0/1/0/all/0/1\">Luca Guarnera</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Giudice_O/0/1/0/all/0/1\">Oliver Giudice</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Niessner</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1\">Sebastiano Battiato</a> (1) ((1) University of Catania, (2) Applied Research Team, IT dept., Banca d&#x27;Italia, Italy, (3) Technical University of Munich, Germany)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Informed Deep Learning Models Enable High-Confidence Predictions for Digital Histopathology. (arXiv:2204.04516v1 [q-bio.QM])","link":"http://arxiv.org/abs/2204.04516","description":"<p>A model's ability to express its own predictive uncertainty is an essential\nattribute for maintaining clinical user confidence as computational biomarkers\nare deployed into real-world medical settings. In the domain of cancer digital\nhistopathology, we describe a novel, clinically-oriented approach to\nuncertainty quantification (UQ) for whole-slide images, estimating uncertainty\nusing dropout and calculating thresholds on training data to establish cutoffs\nfor low- and high-confidence predictions. We train models to identify lung\nadenocarcinoma vs. squamous cell carcinoma and show that high-confidence\npredictions outperform predictions without UQ, in both cross-validation and\ntesting on two large external datasets spanning multiple institutions. Our\ntesting strategy closely approximates real-world application, with predictions\ngenerated on unsupervised, unannotated slides using predetermined thresholds.\nFurthermore, we show that UQ thresholding remains reliable in the setting of\ndomain shift, with accurate high-confidence predictions of adenocarcinoma vs.\nsquamous cell carcinoma for out-of-distribution, non-lung cancer cohorts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Dolezal_J/0/1/0/all/0/1\">James M Dolezal</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Srisuwananukorn_A/0/1/0/all/0/1\">Andrew Srisuwananukorn</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Karpeyev_D/0/1/0/all/0/1\">Dmitry Karpeyev</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ramesh_S/0/1/0/all/0/1\">Siddhi Ramesh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kochanny_S/0/1/0/all/0/1\">Sara Kochanny</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cody_B/0/1/0/all/0/1\">Brittany Cody</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mansfield_A/0/1/0/all/0/1\">Aaron Mansfield</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rakshit_S/0/1/0/all/0/1\">Sagar Rakshit</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bansa_R/0/1/0/all/0/1\">Radhika Bansa</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bois_M/0/1/0/all/0/1\">Melanie Bois</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bungum_A/0/1/0/all/0/1\">Aaron O Bungum</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Schulte_J/0/1/0/all/0/1\">Jefree J Schulte</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Vokes_E/0/1/0/all/0/1\">Everett E Vokes</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Garassino_M/0/1/0/all/0/1\">Marina Chiara Garassino</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Husain_A/0/1/0/all/0/1\">Aliya N Husain</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pearson_A/0/1/0/all/0/1\">Alexander T Pearson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Free Black-Box Watermark and Ownership Proof for Image Classification Neural Networks. (arXiv:2204.04522v1 [cs.CR])","link":"http://arxiv.org/abs/2204.04522","description":"<p>Watermarking has become a plausible candidate for ownership verification and\nintellectual property protection of deep neural networks. Regarding image\nclassification neural networks, current watermarking schemes uniformly resort\nto backdoor triggers. However, injecting a backdoor into a neural network\nrequires knowledge of the training dataset, which is usually unavailable in the\nreal-world commercialization. Meanwhile, established watermarking schemes\noversight the potential damage of exposed evidence during ownership\nverification and the watermarking algorithms themselves. Those concerns decline\ncurrent watermarking schemes from industrial applications. To confront these\nchallenges, we propose a knowledge-free black-box watermarking scheme for image\nclassification neural networks. The image generator obtained from a data-free\ndistillation process is leveraged to stabilize the network's performance during\nthe backdoor injection. A delicate encoding and verification protocol is\ndesigned to ensure the scheme's security against knowledgable adversaries. We\nalso give a pioneering analysis of the capacity of the watermarking scheme.\nExperiment results proved the functionality-preserving capability and security\nof the proposed watermarking scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fangqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shilin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Labeling Refinement for Robust Representation Learning with Bootstrap Your Own Latent. (arXiv:2204.04545v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04545","description":"<p>In this work, we have worked towards two major goals. Firstly, we have\ninvestigated the importance of Batch Normalisation (BN) layers in a\nnon-contrastive representation learning framework called Bootstrap Your Own\nLatent (BYOL). We conducted several experiments to conclude that BN layers are\nnot necessary for representation learning in BYOL. Moreover, BYOL only learns\nfrom the positive pairs of images but ignores other semantically similar images\nin the same input batch. For the second goal, we have introduced two new loss\nfunctions to determine the semantically similar pairs in the same input batch\nof images and reduce the distance between their representations. These loss\nfunctions are Cross-Cosine Similarity Loss (CCSL) and Cross-Sigmoid Similarity\nLoss (CSSL). Using the proposed loss functions, we are able to surpass the\nperformance of Vanilla BYOL (71.04%) by training the BYOL framework using CCSL\nloss (76.87%) on the STL10 dataset. BYOL trained using CSSL loss performs\ncomparably with Vanilla BYOL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddhant Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Dhruval Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive search area for fast motion estimation. (arXiv:2204.04546v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04546","description":"<p>This paper suggests a new method for determining the search area for a motion\nestimation algorithm based on block matching. The search area is adaptively\nfound in the proposed method for each frame block. This search area is similar\nto that of the full search (FS) algorithm but smaller for most blocks of a\nframe. Therefore, the proposed algorithm is analogous to FS in terms of\nregularity but has much less computational complexity. The temporal and spatial\ncorrelations among the motion vectors of blocks are used to find the search\narea. The matched block is chosen from a rectangular area that the prediction\nvectors set out. Simulation results indicate that the speed of the proposed\nalgorithm is at least seven times better than the FS algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soroushmehr_S/0/1/0/all/0/1\">S.M.Reza Soroushmehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirani_S/0/1/0/all/0/1\">Shahram Shirani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Transformer for Nursing Activity Recognition. (arXiv:2204.04564v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04564","description":"<p>In an aging population, elderly patient safety is a primary concern at\nhospitals and nursing homes, which demands for increased nurse care. By\nperforming nurse activity recognition, we can not only make sure that all\npatients get an equal desired care, but it can also free nurses from manual\ndocumentation of activities they perform, leading to a fair and safe place of\ncare for the elderly. In this work, we present a multimodal transformer-based\nnetwork, which extracts features from skeletal joints and acceleration data,\nand fuses them to perform nurse activity recognition. Our method achieves\nstate-of-the-art performance of 81.8% accuracy on the benchmark dataset\navailable for nurse activity recognition from the Nurse Care Activity\nRecognition Challenge. We perform ablation studies to show that our fusion\nmodel is better than single modality transformer variants (using only\nacceleration or skeleton joints data). Our solution also outperforms\nstate-of-the-art ST-GCN, GRU and other classical hand-crafted-feature-based\nclassifier solutions by a margin of 1.6%, on the NCRC dataset. Code is\navailable at \\url{https://github.com/Momilijaz96/MMT_for_NCRC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ijaz_M/0/1/0/all/0/1\">Momal Ijaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_R/0/1/0/all/0/1\">Renato Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot Classification. (arXiv:2204.04567v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04567","description":"<p>Few-shot classification is a challenging problem as only very few training\nexamples are given for each new task. One of the effective research lines to\naddress this challenge focuses on learning deep representations driven by a\nsimilarity measure between a query image and few support images of some class.\nStatistically, this amounts to measure the dependency of image features, viewed\nas random vectors in a high-dimensional embedding space. Previous methods\neither only use marginal distributions without considering joint distributions,\nsuffering from limited representation capability, or are computationally\nexpensive though harnessing joint distributions. In this paper, we propose a\ndeep Brownian Distance Covariance (DeepBDC) method for few-shot classification.\nThe central idea of DeepBDC is to learn image representations by measuring the\ndiscrepancy between joint characteristic functions of embedded features and\nproduct of the marginals. As the BDC metric is decoupled, we formulate it as a\nhighly modular and efficient layer. Furthermore, we instantiate DeepBDC in two\ndifferent few-shot classification frameworks. We make experiments on six\nstandard few-shot image benchmarks, covering general object recognition,\nfine-grained categorization and cross-domain classification. Extensive\nevaluations show our DeepBDC significantly outperforms the counterparts, while\nestablishing new state-of-the-art results. The source code is available at\n<a href=\"http://www.peihuali.org/DeepBDC\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangtao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_F/0/1/0/all/0/1\">Fei Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiaming Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peihua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Cross-Modal Representation Learning with Progressive Self-Distillation. (arXiv:2204.04588v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04588","description":"<p>The learning objective of vision-language approach of CLIP does not\neffectively account for the noisy many-to-many correspondences found in\nweb-harvested image captioning datasets, which contributes to its compute and\ndata inefficiency. To address this challenge, we introduce a novel training\nframework based on cross-modal contrastive learning that uses progressive\nself-distillation and soft image-text alignments to more efficiently learn\nrobust representations from noisy data. Our model distills its own knowledge to\ndynamically generate soft-alignment targets for a subset of images and captions\nin every minibatch, which are then used to update its parameters. Extensive\nevaluation across 14 benchmark datasets shows that our method consistently\noutperforms its CLIP counterpart in multiple settings, including: (a) zero-shot\nclassification, (b) linear probe transfer, and (c) image-text retrieval,\nwithout incurring added computational cost. Analysis using an ImageNet-based\nrobustness test-bed reveals that our method offers better effective robustness\nto natural distribution shifts compared to both ImageNet-trained models and\nCLIP itself. Lastly, pretraining with datasets spanning two orders of magnitude\nin size shows that our improvements over CLIP tend to scale with number of\ntraining examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shixing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamid_R/0/1/0/all/0/1\">Raffay Hamid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic Filter Attention. (arXiv:2204.04601v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04601","description":"<p>Interpretability is an important property for visual models as it helps\nresearchers and users understand the internal mechanism of a complex model.\nHowever, generating semantic explanations about the learned representation is\nchallenging without direct supervision to produce such explanations. We propose\na general framework, Latent Visual Semantic Explainer (LaViSE), to teach any\nexisting convolutional neural network to generate text descriptions about its\nown latent representations at the filter level. Our method constructs a mapping\nbetween the visual and semantic spaces using generic image datasets, using\nimages and category names. It then transfers the mapping to the target domain\nwhich does not have semantic labels. The proposed framework employs a modular\nstructure and enables to analyze any trained network whether or not its\noriginal training data is available. We show that our method can generate novel\ndescriptions for learned filters beyond the set of categories defined in the\ntraining dataset and perform an extensive evaluation on multiple datasets. We\nalso demonstrate a novel application of our method for unsupervised dataset\nbias analysis which allows us to automatically discover hidden biases in\ndatasets or compare different subsets without using additional labels. The\ndataset and code are made public to facilitate further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungbae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Video Representation Learning with Motion-Contrastive Perception. (arXiv:2204.04607v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04607","description":"<p>Visual-only self-supervised learning has achieved significant improvement in\nvideo representation learning. Existing related methods encourage models to\nlearn video representations by utilizing contrastive learning or designing\nspecific pretext tasks. However, some models are likely to focus on the\nbackground, which is unimportant for learning video representations. To\nalleviate this problem, we propose a new view called long-range residual frame\nto obtain more motion-specific information. Based on this, we propose the\nMotion-Contrastive Perception Network (MCPNet), which consists of two branches,\nnamely, Motion Information Perception (MIP) and Contrastive Instance Perception\n(CIP), to learn generic video representations by focusing on the changing areas\nin videos. Specifically, the MIP branch aims to learn fine-grained motion\nfeatures, and the CIP branch performs contrastive learning to learn overall\nsemantics information for each instance. Experiments on two benchmark datasets\nUCF-101 and HMDB-51 show that our method outperforms current state-of-the-art\nvisual-only self-supervised approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Ying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuejie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui-Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Pixel-Level Distinctions for Video Highlight Detection. (arXiv:2204.04615v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04615","description":"<p>The goal of video highlight detection is to select the most attractive\nsegments from a long video to depict the most interesting parts of the video.\nExisting methods typically focus on modeling relationship between different\nvideo segments in order to learning a model that can assign highlight scores to\nthese segments; however, these approaches do not explicitly consider the\ncontextual dependency within individual segments. To this end, we propose to\nlearn pixel-level distinctions to improve the video highlight detection. This\npixel-level distinction indicates whether or not each pixel in one video\nbelongs to an interesting section. The advantages of modeling such fine-level\ndistinctions are two-fold. First, it allows us to exploit the temporal and\nspatial relations of the content in one video, since the distinction of a pixel\nin one frame is highly dependent on both the content before this frame and the\ncontent around this pixel in this frame. Second, learning the pixel-level\ndistinction also gives a good explanation to the video highlight task regarding\nwhat contents in a highlight segment will be attractive to people. We design an\nencoder-decoder network to estimate the pixel-level distinction, in which we\nleverage the 3D convolutional neural networks to exploit the temporal context\ninformation, and further take advantage of the visual saliency to model the\nspatial distinction. State-of-the-art performance on three public benchmarks\nclearly validates the effectiveness of our framework for video highlight\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fanyue Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Biao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tiezheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lixin Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Principal Curve-Based Classifiers and Similarity-Based Selective Sampling in Time-Series. (arXiv:2204.04620v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04620","description":"<p>Considering the concept of time-dilation, there exist some major issues with\nrecurrent neural Architectures. Any variation in time spans between input data\npoints causes performance attenuation in recurrent neural network\narchitectures. Principal curve-based classifiers have the ability of handling\nany kind of variation in time spans. In other words, principal curve-based\nclassifiers preserve the relativity of time while neural network architecture\nviolates this property of time. On the other hand, considering the labeling\ncosts and problems in online monitoring devices, there should be an algorithm\nthat finds the data points which knowing their labels will cause in better\nperformance of the classifier. Current selective sampling algorithms have lack\nof reliability due to the randomness of the proposed algorithms. This paper\nproposes a classifier and also a deterministic selective sampling algorithm\nwith the same computational steps, both by use of principal curve as their\nbuilding block in model definition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hakimzadeh_A/0/1/0/all/0/1\">Aref Hakimzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziarati_K/0/1/0/all/0/1\">Koorush Ziarati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taheri_M/0/1/0/all/0/1\">Mohammad Taheri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Manga Character Re-identification via Face-body and Spatial-temporal Associated Clustering. (arXiv:2204.04621v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04621","description":"<p>In the past few years, there has been a dramatic growth in e-manga\n(electronic Japanese-style comics). Faced with the booming demand for manga\nresearch and the large amount of unlabeled manga data, we raised a new task,\ncalled unsupervised manga character re-identification. However, the artistic\nexpression and stylistic limitations of manga pose many challenges to the\nre-identification problem. Inspired by the idea that some content-related\nfeatures may help clustering, we propose a Face-body and Spatial-temporal\nAssociated Clustering method (FSAC). In the face-body combination module, a\nface-body graph is constructed to solve problems such as exaggeration and\ndeformation in artistic creation by using the integrity of the image. In the\nspatial-temporal relationship correction module, we analyze the appearance\nfeatures of characters and design a temporal-spatial-related triplet loss to\nfine-tune the clustering. Extensive experiments on a manga book dataset with\n109 volumes validate the superiority of our method in unsupervised manga\ncharacter re-identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stripformer: Strip Transformer for Fast Image Deblurring. (arXiv:2204.04627v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04627","description":"<p>Images taken in dynamic scenes may contain unwanted motion blur, which\nsignificantly degrades visual quality. Such blur causes short- and long-range\nregion-specific smoothing artifacts that are often directional and non-uniform,\nwhich is difficult to be removed. Inspired by the current success of\ntransformers on computer vision and image processing tasks, we develop,\nStripformer, a transformer-based architecture that constructs intra- and\ninter-strip tokens to reweight image features in the horizontal and vertical\ndirections to catch blurred patterns with different orientations. It stacks\ninterlaced intra-strip and inter-strip attention layers to reveal blur\nmagnitudes. In addition to detecting region-specific blurred patterns of\nvarious orientations and magnitudes, Stripformer is also a token-efficient and\nparameter-efficient transformer model, demanding much less memory usage and\ncomputation cost than the vanilla transformer but works better without relying\non tremendous training data. Experimental results show that Stripformer\nperforms favorably against state-of-the-art models in dynamic scene deblurring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_F/0/1/0/all/0/1\">Fu-Jen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan-Tsung Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1\">Chung-Chi Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intersection Prediction from Single 360{\\deg} Image via Deep Detection of Possible Direction of Travel. (arXiv:2204.04634v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04634","description":"<p>Movie-Map, an interactive first-person-view map that engages the user in a\nsimulated walking experience, comprises short 360{\\deg} video segments\nseparated by traffic intersections that are seamlessly connected according to\nthe viewer's direction of travel. However, in wide urban-scale areas with\nnumerous intersecting roads, manual intersection segmentation requires\nsignificant human effort. Therefore, automatic identification of intersections\nfrom 360{\\deg} videos is an important problem for scaling up Movie-Map. In this\npaper, we propose a novel method that identifies an intersection from\nindividual frames in 360{\\deg} videos. Instead of formulating the intersection\nidentification as a standard binary classification task with a 360{\\deg} image\nas input, we identify an intersection based on the number of the possible\ndirections of travel (PDoT) in perspective images projected in eight directions\nfrom a single 360{\\deg} image detected by the neural network for handling\nvarious types of intersections. We constructed a large-scale 360{\\deg} Image\nIntersection Identification (iii360) dataset for training and evaluation where\n360{\\deg} videos were collected from various areas such as school campus,\ndowntown, suburb, and china town and demonstrate that our PDoT-based method\nachieves 88\\% accuracy, which is significantly better than that achieved by the\ndirect naive binary classification based method. The source codes and a partial\ndataset will be shared in the community after the paper is published.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_N/0/1/0/all/0/1\">Naoki Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikehata_S/0/1/0/all/0/1\">Satoshi Ikehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1\">Kiyoharu Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConsInstancy: Learning Instance Representations for Semi-Supervised Panoptic Segmentation of Concrete Aggregate Particles. (arXiv:2204.04635v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04635","description":"<p>We present a semi-supervised method for panoptic segmentation based on\nConsInstancy regularisation, a novel strategy for semi-supervised learning. It\nleverages completely unlabelled data by enforcing consistency between predicted\ninstance representations and semantic segmentations during training in order to\nimprove the segmentation performance. To this end, we also propose new types of\ninstance representations that can be predicted by one simple forward path\nthrough a fully convolutional network (FCN), delivering a convenient and\nsimple-to-train framework for panoptic segmentation. More specifically, we\npropose the prediction of a three-dimensional instance orientation map as\nintermediate representation and two complementary distance transform maps as\nfinal representation, providing unique instance representations for a panoptic\nsegmentation. We test our method on two challenging data sets of both, hardened\nand fresh concrete, the latter being proposed by the authors in this paper\ndemonstrating the effectiveness of our approach, outperforming the results\nachieved by state-of-the-art methods for semi-supervised segmentation. In\nparticular, we are able to show that by leveraging completely unlabeled data in\nour semi-supervised approach the achieved overall accuracy (OA) is increased by\nup to 5% compared to an entirely supervised training using only labeled data.\nFurthermore, we exceed the OA achieved by state-of-the-art semi-supervised\nmethods by up to 1.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coenen_M/0/1/0/all/0/1\">Max Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schack_T/0/1/0/all/0/1\">Tobias Schack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_D/0/1/0/all/0/1\">Dries Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heipke_C/0/1/0/all/0/1\">Christian Heipke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haist_M/0/1/0/all/0/1\">Michael Haist</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Unmixing of Hyperspectral Images Based on Block Sparse Structure. (arXiv:2204.04638v1 [eess.IV])","link":"http://arxiv.org/abs/2204.04638","description":"<p>Spectral unmixing (SU) of hyperspectral images (HSIs) is one of the important\nareas in remote sensing (RS) that needs to be carefully addressed in different\nRS applications. Despite the high spectral resolution of the hyperspectral\ndata, the relatively low spatial resolution of the sensors may lead to mixture\nof different pure materials within the image pixels. In this case, the spectrum\nof a given pixel recorded by the sensor can be a combination of multiple\nspectra each belonging to a unique material in that pixel. Spectral unmixing is\nthen used as a technique to extract the spectral characteristics of the\ndifferent materials within the mixed pixels and to recover the spectrum of each\npure spectral signature, called endmember. Block-sparsity exists in\nhyperspectral images as a result of spectral similarity between neighboring\npixels. In block-sparse signals, the nonzero samples occur in clusters and the\npattern of the clusters is often supposed to be unavailable as prior\ninformation. This paper presents an innovative spectral unmixing approach for\nHSIs based on block-sparse structure and sparse Bayesian learning (SBL)\nstrategy. To evaluate the performance of the proposed SU algorithm, it is\ntested on both synthetic and real hyperspectral data and the quantitative\nresults are compared to those of other state-of-the-art methods in terms of\nabundance angel distance (AAD) and mean square error (MSE). The achieved\nresults show the superiority of the proposed algorithm over the other competing\nmethods by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Azarang_S/0/1/0/all/0/1\">Seyed Hossein Mosavi Azarang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajabi_R/0/1/0/all/0/1\">Roozbeh Rajabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zayyani_H/0/1/0/all/0/1\">Hadi Zayyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zehtabian_A/0/1/0/all/0/1\">Amin Zehtabian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counting in the 2020s: Binned Representations and Inclusive Performance Measures for Deep Crowd Counting Approaches. (arXiv:2204.04653v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04653","description":"<p>The data distribution in popular crowd counting datasets is typically heavy\ntailed and discontinuous. This skew affects all stages within the pipelines of\ndeep crowd counting approaches. Specifically, the approaches exhibit\nunacceptably large standard deviation wrt statistical measures (MSE, MAE). To\naddress such concerns in a holistic manner, we make two fundamental\ncontributions. Firstly, we modify the training pipeline to accommodate the\nknowledge of dataset skew. To enable principled and balanced minibatch\nsampling, we propose a novel smoothed Bayesian binning approach. More\nspecifically, we propose a novel cost function which can be readily\nincorporated into existing crowd counting deep networks to encourage bin-aware\noptimization. As the second contribution, we introduce additional performance\nmeasures which are more inclusive and throw light on various comparative\nperformance aspects of the deep networks. We also show that our binning-based\nmodifications retain their superiority wrt the newly proposed performance\nmeasures. Overall, our contributions enable a practically useful and\ndetail-oriented characterization of performance for crowd counting approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shivapuja_S/0/1/0/all/0/1\">Sravya Vardhani Shivapuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopinath_A/0/1/0/all/0/1\">Ashwin Gopinath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ayush Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fashionformer: A simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition. (arXiv:2204.04654v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04654","description":"<p>Human fashion understanding is one important computer vision task since it\nhas the comprehensive information that can be used for real-world applications.\nIn this work, we focus on joint human fashion segmentation and attribute\nrecognition. Contrary to the previous works that separately model each task as\na multi-head prediction problem, our insight is to bridge these two tasks with\none unified model via vision transformer modeling to benefit each task. In\nparticular, we introduce the object query for segmentation and the attribute\nquery for attribute prediction. Both queries and their corresponding features\ncan be linked via mask prediction. Then we adopt a two-stream query learning\nframework to learn the decoupled query representations. For attribute stream,\nwe design a novel Multi-Layer Rendering module to explore more fine-grained\nfeatures. The decoder design shares the same spirits with DETR, thus we name\nthe proposed method Fahsionformer. Extensive experiments on three human fashion\ndatasets including Fashionpedia, ModaNet and Deepfashion illustrate the\neffectiveness of our approach. In particular, our method with the same backbone\nachieve relative 10% improvements than previous works in case of \\textit{a\njoint metric ( AP$^{\\text{mask}}_{\\text{IoU+F}_1}$) for both segmentation and\nattribute recognition}. To the best of our knowledge, we are the first unified\nend-to-end vision transformer framework for human fashion analysis. We hope\nthis simple yet effective method can serve as a new flexible baseline for\nfashion analysis. Code will be available at\nhttps://github.com/xushilin1/FashionFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic-PartFormer: Learning a Unified Model for Panoptic Part Segmentation. (arXiv:2204.04655v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04655","description":"<p>Panoptic Part Segmentation (PPS) aims to unify panoptic segmentation and part\nsegmentation into one task. Previous work mainly utilizes separated approaches\nto handle thing, stuff, and part predictions individually without performing\nany shared computation and task association. In this work, we aim to unify\nthese tasks at the architectural level, designing the first end-to-end unified\nmethod named Panoptic-PartFormer. In particular, motivated by the recent\nprogress in Vision Transformer, we model things, stuff, and part as object\nqueries and directly learn to optimize the all three predictions as unified\nmask prediction and classification problem. We design a decoupled decoder to\ngenerate part feature and thing/stuff feature respectively. Then we propose to\nutilize all the queries and corresponding features to perform reasoning jointly\nand iteratively. The final mask can be obtained via inner product between\nqueries and the corresponding features. The extensive ablation studies and\nanalysis prove the effectiveness of our framework. Our Panoptic-PartFormer\nachieves the new state-of-the-art results on both Cityscapes PPS and Pascal\nContext PPS datasets with at least 70% GFlops and 50% parameters decrease. In\nparticular, we get 3.4% relative improvements with ResNet50 backbone and 10%\nimprovements after adopting Swin Transformer on Pascal Context PPS dataset. To\nthe best of our knowledge, we are the first to solve the PPS problem via\n\\textit{a unified and end-to-end transformer model. Given its effectiveness and\nconceptual simplicity, we hope our Panoptic-PartFormer can serve as a good\nbaseline and aid future unified research for PPS. Our code and models will be\navailable at https://github.com/lxtGH/Panoptic-PartFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yibo Yang.Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation. (arXiv:2204.04656v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04656","description":"<p>This paper presents Video K-Net, a simple, strong, and unified framework for\nfully end-to-end video panoptic segmentation. The method is built upon K-Net, a\nmethod that unifies image segmentation via a group of learnable kernels. We\nobserve that these learnable kernels from K-Net, which encode object\nappearances and contexts, can naturally associate identical instances across\nvideo frames. Motivated by this observation, Video K-Net learns to\nsimultaneously segment and track \"things\" and \"stuff\" in a video with simple\nkernel-based appearance modeling and cross-temporal kernel interaction. Despite\nthe simplicity, it achieves state-of-the-art video panoptic segmentation\nresults on Citscapes-VPS and KITTI-STEP without bells and whistles. In\nparticular on KITTI-STEP, the simple method can boost almost 12\\% relative\nimprovements over previous methods. We also validate its generalization on\nvideo semantic segmentation, where we boost various baselines by 2\\% on the\nVSPW dataset. Moreover, we extend K-Net into clip-level video framework for\nvideo instance segmentation where we obtain 40.5\\% for ResNet50 backbone and\n51.5\\% mAP for Swin-base on YouTube-2019 validation set. We hope this simple\nyet effective method can serve as a new flexible baseline in video\nsegmentation. Both code and models are released at\nhttps://github.com/lxtGH/Video-K-Net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOSTER: Feature Boosting and Compression for Class-Incremental Learning. (arXiv:2204.04662v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04662","description":"<p>The ability to learn new concepts continually is necessary in this\never-changing world. However, deep neural networks suffer from catastrophic\nforgetting when learning new categories. Many works have been proposed to\nalleviate this phenomenon, whereas most of them either fall into the\nstability-plasticity dilemma or take too much computation or storage overhead.\nInspired by the gradient boosting algorithm to gradually fit the residuals\nbetween the target and the current approximation function, we propose a novel\ntwo-stage learning paradigm FOSTER, empowering the model to learn new\ncategories adaptively. Specifically, we first dynamically expand new modules to\nfit the residuals of the target and the original model. Next, we remove\nredundant parameters and feature dimensions through an effective distillation\nstrategy to maintain the single backbone model. We validate our method FOSTER\non CIFAR-100, ImageNet-100/1000 under different settings. Experimental results\nshow that our method achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fu-Yun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Out-of-Distribution Detection in Classifier Based on PEDCC-Loss. (arXiv:2204.04665v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04665","description":"<p>Deep neural networks suffer from the overconfidence issue in the open world,\nmeaning that classifiers could yield confident, incorrect predictions for\nout-of-distribution (OOD) samples. Thus, it is an urgent and challenging task\nto detect these samples drawn far away from training distribution based on the\nsecurity considerations of artificial intelligence. Many current methods based\non neural networks mainly rely on complex processing strategies, such as\ntemperature scaling and input preprocessing, to obtain satisfactory results. In\nthis paper, we propose an effective algorithm for detecting out-of-distribution\nexamples utilizing PEDCC-Loss. We mathematically analyze the nature of the\nconfidence score output by the PEDCC (Predefined Evenly-Distribution Class\nCentroids) classifier, and then construct a more effective scoring function to\ndistinguish in-distribution (ID) and out-of-distribution. In this method, there\nis no need to preprocess the input samples and the computational burden of the\nalgorithm is reduced. Experiments demonstrate that our method can achieve\nbetter OOD detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiuyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guohui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yingying Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Complexity Randomized Self-attention Mechanism. (arXiv:2204.04667v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04667","description":"<p>Recently, random feature attentions (RFAs) are proposed to approximate the\nsoftmax attention in linear time and space complexity by linearizing the\nexponential kernel. In this paper, we first propose a novel perspective to\nunderstand the bias in such approximation by recasting RFAs as self-normalized\nimportance samplers. This perspective further sheds light on an \\emph{unbiased}\nestimator for the whole softmax attention, called randomized attention (RA). RA\nconstructs positive random features via query-specific distributions and enjoys\ngreatly improved approximation fidelity, albeit exhibiting quadratic\ncomplexity. By combining the expressiveness in RA and the efficiency in RFA, we\ndevelop a novel linear complexity self-attention mechanism called linear\nrandomized attention (LARA). Extensive experiments across various domains\ndemonstrate that RA and LARA significantly improve the performance of RFAs by a\nsubstantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAN: Noise-Aware NeRFs for Burst-Denoising. (arXiv:2204.04668v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04668","description":"<p>Burst denoising is now more relevant than ever, as computational photography\nhelps overcome sensitivity issues inherent in mobile phones and small cameras.\nA major challenge in burst-denoising is in coping with pixel misalignment,\nwhich was so far handled with rather simplistic assumptions of simple motion,\nor the ability to align in pre-processing. Such assumptions are not realistic\nin the presence of large motion and high levels of noise. We show that Neural\nRadiance Fields (NeRFs), originally suggested for physics-based novel-view\nrendering, can serve as a powerful framework for burst denoising. NeRFs have an\ninherent capability of handling noise as they integrate information from\nmultiple images, but they are limited in doing so, mainly since they build on\npixel-wise operations which are suitable to ideal imaging conditions. Our\napproach, termed NAN, leverages inter-view and spatial information in NeRFs to\nbetter deal with noise. It achieves state-of-the-art results in burst denoising\nand is especially successful in coping with large movement and occlusions,\nunder very high levels of noise. With the rapid advances in accelerating NeRFs,\nit could provide a powerful platform for denoising in challenging environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pearl_N/0/1/0/all/0/1\">Naama Pearl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treibitz_T/0/1/0/all/0/1\">Tali Treibitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korman_S/0/1/0/all/0/1\">Simon Korman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is my Driver Observation Model Overconfident? Input-guided Calibration Networks for Reliable and Interpretable Confidence Estimates. (arXiv:2204.04674v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04674","description":"<p>Driver observation models are rarely deployed under perfect conditions. In\npractice, illumination, camera placement and type differ from the ones present\nduring training and unforeseen behaviours may occur at any time. While\nobserving the human behind the steering wheel leads to more intuitive\nhuman-vehicle-interaction and safer driving, it requires recognition algorithms\nwhich do not only predict the correct driver state, but also determine their\nprediction quality through realistic and interpretable confidence measures.\nReliable uncertainty estimates are crucial for building trust and are a serious\nobstacle for deploying activity recognition networks in real driving systems.\nIn this work, we for the first time examine how well the confidence values of\nmodern driver observation models indeed match the probability of the correct\noutcome and show that raw neural network-based approaches tend to significantly\noverestimate their prediction quality. To correct this misalignment between the\nconfidence values and the actual uncertainty, we consider two strategies.\nFirst, we enhance two activity recognition models often used for driver\nobservation with temperature scaling-an off-the-shelf method for confidence\ncalibration in image classification. Then, we introduce Calibrated Action\nRecognition with Input Guidance (CARING)-a novel approach leveraging an\nadditional neural network to learn scaling the confidences depending on the\nvideo representation. Extensive experiments on the Drive&amp;Act dataset\ndemonstrate that both strategies drastically improve the quality of model\nconfidences, while our CARING model out-performs both, the original\narchitectures and their temperature scaling enhancement, leading to best\nuncertainty estimates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1\">David Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koulakis_M/0/1/0/all/0/1\">Marios Koulakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1\">Manuel Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Baselines for Image Restoration. (arXiv:2204.04676v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04676","description":"<p>Although there have been significant advances in the field of image\nrestoration recently, the system complexity of the state-of-the-art (SOTA)\nmethods is increasing as well, which may hinder the convenient analysis and\ncomparison of methods. In this paper, we propose a simple baseline that exceeds\nthe SOTA methods and is computationally efficient. To further simplify the\nbaseline, we reveal that the nonlinear activation functions, e.g. Sigmoid,\nReLU, GELU, Softmax, etc. are not necessary: they could be replaced by\nmultiplication or removed. Thus, we derive a Nonlinear Activation Free Network,\nnamely NAFNet, from the baseline. SOTA results are achieved on various\nchallenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring),\nexceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs;\n40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28\ndB with less than half of its computational costs. The code and the pretrained\nmodels will be released at https://github.com/megvii-research/NAFNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedCorr: Multi-Stage Federated Learning for Label Noise Correction. (arXiv:2204.04677v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04677","description":"<p>Federated learning (FL) is a privacy-preserving distributed learning paradigm\nthat enables clients to jointly train a global model. In real-world FL\nimplementations, client data could have label noise, and different clients\ncould have vastly different label noise levels. Although there exist methods in\ncentralized learning for tackling label noise, such methods do not perform well\non heterogeneous label noise in FL settings, due to the typically smaller sizes\nof client datasets and data privacy requirements in FL. In this paper, we\npropose $\\texttt{FedCorr}$, a general multi-stage framework to tackle\nheterogeneous label noise in FL, without making any assumptions on the noise\nmodels of local clients, while still maintaining client data privacy. In\nparticular, (1) $\\texttt{FedCorr}$ dynamically identifies noisy clients by\nexploiting the dimensionalities of the model prediction subspaces independently\nmeasured on all clients, and then identifies incorrect labels on noisy clients\nbased on per-sample losses. To deal with data heterogeneity and to increase\ntraining stability, we propose an adaptive local proximal regularization term\nthat is based on estimated local noise levels. (2) We further finetune the\nglobal model on identified clean clients and correct the noisy labels for the\nremaining noisy clients after finetuning. (3) Finally, we apply the usual\ntraining on all clients to make full use of all local data. Experiments\nconducted on CIFAR-10/100 with federated synthetic label noise, and on a\nreal-world noisy dataset, Clothing1M, demonstrate that $\\texttt{FedCorr}$ is\nrobust to label noise and substantially outperforms the state-of-the-art\nmethods at multiple noise levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1\">Tony Q.S. Quek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_K/0/1/0/all/0/1\">Kai Fong Ernest Chong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale Invariant Semantic Segmentation with RGB-D Fusion. (arXiv:2204.04679v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04679","description":"<p>In this paper, we propose a neural network architecture for scale-invariant\nsemantic segmentation using RGB-D images. We utilize depth information as an\nadditional modality apart from color images only. Especially in an outdoor\nscene which consists of different scale objects due to the distance of the\nobjects from the camera. The near distance objects consist of significantly\nmore pixels than the far ones. We propose to incorporate depth information to\nthe RGB data for pixel-wise semantic segmentation to address the different\nscale objects in an outdoor scene. We adapt to a well-known\nDeepLab-v2(ResNet-101) model as our RGB baseline. Depth images are passed\nseparately as an additional input with a distinct branch. The intermediate\nfeature maps of both color and depth image branch are fused using a novel\nfusion block. Our model is compact and can be easily applied to the other RGB\nmodel. We perform extensive qualitative and quantitative evaluation on a\nchallenging dataset Cityscapes. The results obtained are comparable to the\nstate-of-the-art. Additionally, we evaluated our model on a self-recorded real\ndataset. For the shake of extended evaluation of a driving scene with ground\ntruth we generated a synthetic dataset using popular vehicle simulation project\nCARLA. The results obtained from the real and synthetic dataset shows the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Mohammad Dawud Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husada_A/0/1/0/all/0/1\">Alwi Husada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning with Multi-Structure Commonsense Knowledge in Visual Dialog. (arXiv:2204.04680v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04680","description":"<p>Visual Dialog requires an agent to engage in a conversation with humans\ngrounded in an image. Many studies on Visual Dialog focus on the understanding\nof the dialog history or the content of an image, while a considerable amount\nof commonsense-required questions are ignored. Handling these scenarios depends\non logical reasoning that requires commonsense priors. How to capture relevant\ncommonsense knowledge complementary to the history and the image remains a key\nchallenge. In this paper, we propose a novel model by Reasoning with\nMulti-structure Commonsense Knowledge (RMK). In our model, the external\nknowledge is represented with sentence-level facts and graph-level facts, to\nproperly suit the scenario of the composite of dialog history and image. On top\nof these multi-structure representations, our model can capture relevant\nknowledge and incorporate them into the vision and semantic features, via\ngraph-based interaction and transformer-based fusion. Experimental results and\nanalysis on VisDial v1.0 and VisDialCK datasets show that our proposed model\neffectively outperforms comparative methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shunyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoze Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zequn Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_T/0/1/0/all/0/1\">Tao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zengchang Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing the Robustness, Efficiency, and Diversity of Differentiable Architecture Search. (arXiv:2204.04681v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04681","description":"<p>Differentiable architecture search (DARTS) has attracted much attention due\nto its simplicity and significant improvement in efficiency. However, the\nexcessive accumulation of the skip connection makes it suffer from long-term\nweak stability and low robustness. Many works attempt to restrict the\naccumulation of skip connections by indicators or manual design, however, these\nmethods are susceptible to thresholds and human priors. In this work, we\nsuggest a more subtle and direct approach that removes skip connections from\nthe operation space. Then, by introducing an adaptive channel allocation\nstrategy, we redesign the DARTS framework to automatically refill the skip\nconnections in the evaluation stage, resolving the performance degradation\ncaused by the absence of skip connections. Our method, dubbed\nAdaptive-Channel-Allocation-DARTS (ACA-DRATS), could eliminate the\ninconsistency in operation strength and significantly expand the architecture\ndiversity. We continue to explore smaller search space under our framework, and\noffer a direct search on the entire ImageNet dataset. Experiments show that\nACA-DRATS improves the search stability and significantly speeds up DARTS by\nmore than ten times while yielding higher accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1\">Jia Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coreset of Hyperspectral Images on Small Quantum Computer. (arXiv:2204.04691v1 [quant-ph])","link":"http://arxiv.org/abs/2204.04691","description":"<p>Machine Learning (ML) techniques are employed to analyze and process big\nRemote Sensing (RS) data, and one well-known ML technique is a Support Vector\nMachine (SVM). An SVM is a quadratic programming (QP) problem, and a D-Wave\nquantum annealer (D-Wave QA) promises to solve this QP problem more efficiently\nthan a conventional computer. However, the D-Wave QA cannot solve directly the\nSVM due to its very few input qubits. Hence, we use a coreset (\"core of a\ndataset\") of given EO data for training an SVM on this small D-Wave QA. The\ncoreset is a small, representative weighted subset of an original dataset, and\nany training models generate competitive classes by using the coreset in\ncontrast to by using its original dataset. We measured the closeness between an\noriginal dataset and its coreset by employing a Kullback-Leibler (KL)\ndivergence measure. Moreover, we trained the SVM on the coreset data by using\nboth a D-Wave QA and a conventional method. We conclude that the coreset\ncharacterizes the original dataset with very small KL divergence measure. In\naddition, we present our KL divergence results for demonstrating the closeness\nbetween our original data and its coreset. As practical RS data, we use\nHyperspectral Image (HSI) of Indian Pine, USA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Otgonbaatar_S/0/1/0/all/0/1\">Soronzonbold Otgonbaatar</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Datcu_M/0/1/0/all/0/1\">Mihai Datcu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO). (arXiv:2204.04704v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04704","description":"<p>Automation of feature analysis in the dynamic image frame dataset deals with\ncomplexity of intensity mapping with normal and abnormal class. The\nthreshold-based data clustering and feature analysis requires iterative model\nto learn the component of image frame in multi-pattern for different image\nframe data type. This paper proposed a novel model of feature analysis method\nwith the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature\nvectors that are optimized by Grey Wolf Optimization (GWO) algorithm.\nInitially, the image frame gets normalized by applying median filter to the\nimage frame that reduce the noise and apply smoothening on it. From that, the\nedge information represents the boundary region of bright spot in the image\nframe. Neural network-based image frame classification performs repeated\nlearning of the feature with minimum training of dataset to cluster the image\nframe pixels. Features of the filtered image frame was analyzed in different\npattern of feature extraction model based on the convoluted model of wavelet\ntransformation method. These features represent the different class of image\nframe in spatial and textural pattern of it. Convolutional Neural Network (CNN)\nclassifier supports to analyze the features and classify the action label for\nthe image frame dataset. This process enhances the classification with minimum\nnumber of training dataset. The performance of this proposed method can be\nvalidated by comparing with traditional state-of-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jamshed_A/0/1/0/all/0/1\">Aatif Jamshed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallick_B/0/1/0/all/0/1\">Bhawna Mallick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharti_R/0/1/0/all/0/1\">Rajendra Kumar Bharti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Networks for Image Augmentation in Agriculture: A Systematic Review. (arXiv:2204.04707v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04707","description":"<p>In agricultural image analysis, optimal model performance is keenly pursued\nfor better fulfilling visual recognition tasks (e.g., image classification,\nsegmentation, object detection and localization), in the presence of challenges\nwith biological variability and unstructured environments. Large-scale,\nbalanced and ground-truthed image datasets, however, are often difficult to\nobtain to fuel the development of advanced, high-performance models. As\nartificial intelligence through deep learning is impacting analysis and\nmodeling of agricultural images, data augmentation plays a crucial role in\nboosting model performance while reducing manual efforts for data preparation,\nby algorithmically expanding training datasets. Beyond traditional data\naugmentation techniques, generative adversarial network (GAN) invented in 2014\nin the computer vision community, provides a suite of novel approaches that can\nlearn good data representations and generate highly realistic samples. Since\n2017, there has been a growth of research into GANs for image augmentation or\nsynthesis in agriculture for improved model performance. This paper presents an\noverview of the evolution of GAN architectures followed by a systematic review\nof their application to agriculture\n(https://github.com/Derekabc/GANs-Agriculture), involving various vision tasks\nfor plant health, weeds, fruits, aquaculture, animal farming, plant phenotyping\nas well as postharvest detection of fruit defects. Challenges and opportunities\nof GANs are discussed for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olaniyi_E/0/1/0/all/0/1\">Ebenezer Olaniyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuzhen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanbo Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Harmonization by Matching Regional References. (arXiv:2204.04715v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04715","description":"<p>To achieve visual consistency in composite images, recent image harmonization\nmethods typically summarize the appearance pattern of global background and\napply it to the global foreground without location discrepancy. However, for a\nreal image, the appearances (illumination, color temperature, saturation, hue,\ntexture, etc) of different regions can vary significantly. So previous methods,\nwhich transfer the appearance globally, are not optimal. Trying to solve this\nissue, we firstly match the contents between the foreground and background and\nthen adaptively adjust every foreground location according to the appearance of\nits content-related background regions. Further, we design a residual\nreconstruction strategy, that uses the predicted residual to adjust the\nappearance, and the composite foreground to reserve the image details.\nExtensive experiments demonstrate the effectiveness of our method. The source\ncode will be available publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Ziyue Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruiqi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zhi Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chun-Le Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOV: The Original Vision Model for Optical Remote Sensing Image Understanding via Self-supervised Learning. (arXiv:2204.04716v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04716","description":"<p>Do we on the right way for remote sensing image understanding (RSIU) by\ntraining models via supervised data-dependent and task-dependent way, instead\nof human vision in a label-free and task-independent way? We argue that a more\ndesirable RSIU model should be trained with intrinsic structure from data\nrather that extrinsic human labels to realize generalizability across a wide\nrange of RSIU tasks. According to this hypothesis, we proposed \\textbf{T}he\n\\textbf{O}riginal \\textbf{V}ision model (TOV) in remote sensing filed. Trained\nby massive unlabeled optical data along a human-like self-supervised learning\n(SSL) path that is from general knowledge to specialized knowledge, TOV model\ncan be easily adapted to various RSIU tasks, including scene classification,\nobject detection, and semantic segmentation, and outperforms dominant ImageNet\nsupervised pretrained method as well as two recently proposed SSL pretrained\nmethods on majority of 12 publicly available benchmarks. Moreover, we analyze\nthe influences of two key factors on the performance of building TOV model for\nRSIU, including the influence of using different data sampling methods and the\nselection of learning paths during self-supervised optimization. We believe\nthat a general model which is trained by a label-free and task-independent way\nmay be the next paradigm for RSIU and hope the insights distilled from this\nstudy can help to foster the development of an original vision model for RSIU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chao Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qia_J/0/1/0/all/0/1\">Ji Qia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weipeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Non-rigid Structure-from-Motion: A Sequence-to-Sequence Translation Perspective. (arXiv:2204.04730v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04730","description":"<p>Directly regressing the non-rigid shape and camera pose from the individual\n2D frame is ill-suited to the Non-Rigid Structure-from-Motion (NRSfM) problem.\nThis frame-by-frame 3D reconstruction pipeline overlooks the inherent\nspatial-temporal nature of NRSfM, i.e., reconstructing the whole 3D sequence\nfrom the input 2D sequence. In this paper, we propose to model deep NRSfM from\na sequence-to-sequence translation perspective, where the input 2D frame\nsequence is taken as a whole to reconstruct the deforming 3D non-rigid shape\nsequence. First, we apply a shape-motion predictor to estimate the initial\nnon-rigid shape and camera motion from a single frame. Then we propose a\ncontext modeling module to model camera motions and complex non-rigid shapes.\nTo tackle the difficulty in enforcing the global structure constraint within\nthe deep framework, we propose to impose the union-of-subspace structure by\nreplacing the self-expressiveness layer with multi-head attention and delayed\nregularizers, which enables end-to-end batch-wise training. Experimental\nresults across different datasets such as Human3.6M, CMU Mocap and InterHand\nprove the superiority of our framework. The code will be made publicly\navailable\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hui Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiawei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Analysis of Decision-Level Fusion for Multimodal Driver Behaviour Understanding. (arXiv:2204.04734v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04734","description":"<p>Visual recognition inside the vehicle cabin leads to safer driving and more\nintuitive human-vehicle interaction but such systems face substantial obstacles\nas they need to capture different granularities of driver behaviour while\ndealing with highly limited body visibility and changing illumination.\nMultimodal recognition mitigates a number of such issues: prediction outcomes\nof different sensors complement each other due to different modality-specific\nstrengths and weaknesses. While several late fusion methods have been\nconsidered in previously published frameworks, they constantly feature\ndifferent architecture backbones and building blocks making it very hard to\nisolate the role of the chosen late fusion strategy itself. This paper presents\nan empirical evaluation of different paradigms for decision-level late fusion\nin video-based driver observation. We compare seven different mechanisms for\njoining the results of single-modal classifiers which have been both popular,\n(e.g. score averaging) and not yet considered (e.g. rank-level fusion) in the\ncontext of driver observation evaluating them based on different criteria and\nbenchmark settings. This is the first systematic study of strategies for fusing\noutcomes of multimodal predictors inside the vehicles, conducted with the goal\nto provide guidance for fusion scheme selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinov_Z/0/1/0/all/0/1\">Zdravko Marinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1\">David Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CholecTriplet2021: A benchmark challenge for surgical action triplet recognition. (arXiv:2204.04746v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04746","description":"<p>Context-aware decision support in the operating room can foster surgical\nsafety and efficiency by leveraging real-time feedback from surgical workflow\nanalysis. Most existing works recognize surgical activities at a coarse-grained\nlevel, such as phases, steps or events, leaving out fine-grained interaction\ndetails about the surgical activity; yet those are needed for more helpful AI\nassistance in the operating room. Recognizing surgical actions as triplets of\n&lt;instrument, verb, target&gt; combination delivers comprehensive details about the\nactivities taking place in surgical videos. This paper presents\nCholecTriplet2021: an endoscopic vision challenge organized at MICCAI 2021 for\nthe recognition of surgical action triplets in laparoscopic videos. The\nchallenge granted private access to the large-scale CholecT50 dataset, which is\nannotated with action triplet information. In this paper, we present the\nchallenge setup and assessment of the state-of-the-art deep learning methods\nproposed by the participants during the challenge. A total of 4 baseline\nmethods from the challenge organizers and 19 new deep learning algorithms by\ncompeting teams are presented to recognize surgical action triplets directly\nfrom surgical videos, achieving mean average precision (mAP) ranging from 4.2%\nto 38.1%. This study also analyzes the significance of the results obtained by\nthe presented approaches, performs a thorough methodological comparison between\nthem, in-depth result analysis, and proposes a novel ensemble method for\nenhanced recognition. Our analysis shows that surgical workflow analysis is not\nyet solved, and also highlights interesting directions for future research on\nfine-grained surgical activity recognition which is of utmost importance for\nthe development of AI in surgery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nwoye_C/0/1/0/all/0/1\">Chinedu Innocent Nwoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alapatt_D/0/1/0/all/0/1\">Deepak Alapatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardazaryan_A/0/1/0/all/0/1\">Armine Vardazaryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fangfang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1\">Fucang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuxuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Derong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoyan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xiaotian Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getty_N/0/1/0/all/0/1\">Neil Getty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Matilla_R/0/1/0/all/0/1\">Ricardo Sanchez-Matilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robu_M/0/1/0/all/0/1\">Maria Robu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huabin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiacheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bokai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerats_B/0/1/0/all/0/1\">Beerend Gerats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviteja_S/0/1/0/all/0/1\">Sista Raviteja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathish_R/0/1/0/all/0/1\">Rachana Sathish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Rong Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondo_S/0/1/0/all/0/1\">Satoshi Kondo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_W/0/1/0/all/0/1\">Winnie Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbing_J/0/1/0/all/0/1\">Julian Ronald Abbing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarhan_M/0/1/0/all/0/1\">Mohammad Hasan Sarhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenstedt_S/0/1/0/all/0/1\">Sebastian Bodenstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhasker_N/0/1/0/all/0/1\">Nithya Bhasker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_B/0/1/0/all/0/1\">Bruno Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_H/0/1/0/all/0/1\">Helena R. Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_L/0/1/0/all/0/1\">Li Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaida_F/0/1/0/all/0/1\">Finn Gaida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czempiel_T/0/1/0/all/0/1\">Tobias Czempiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilaca_J/0/1/0/all/0/1\">Jo&#xe3;o L. Vila&#xe7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morais_P/0/1/0/all/0/1\">Pedro Morais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_J/0/1/0/all/0/1\">Jaime Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egging_R/0/1/0/all/0/1\">Ruby Mae Egging</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijma_I/0/1/0/all/0/1\">Inge Nicole Wijma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_G/0/1/0/all/0/1\">Guibin Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Velmurugan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheet_D/0/1/0/all/0/1\">Debdoot Sheet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luengo_I/0/1/0/all/0/1\">Imanol Luengo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuanbo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuai Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aschenbrenner_J/0/1/0/all/0/1\">Jakob-Anton Aschenbrenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_N/0/1/0/all/0/1\">Nicolas Elini van der Kar</a>, et al. (10 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Cross-view Image Retrieval: Highly Accurate Vehicle Localization Using Satellite Image. (arXiv:2204.04752v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04752","description":"<p>This paper addresses the problem of vehicle-mounted camera localization by\nmatching a ground-level image with an overhead-view satellite map. Existing\nmethods often treat this problem as cross-view image retrieval, and use learned\ndeep features to match the ground-level query image to a partition (eg, a small\npatch) of the satellite map. By these methods, the localization accuracy is\nlimited by the partitioning density of the satellite map (often in the order of\ntens meters). Departing from the conventional wisdom of image retrieval, this\npaper presents a novel solution that can achieve highly-accurate localization.\nThe key idea is to formulate the task as pose estimation and solve it by\nneural-net based optimization. Specifically, we design a two-branch {CNN} to\nextract robust features from the ground and satellite images, respectively. To\nbridge the vast cross-view domain gap, we resort to a Geometry Projection\nmodule that projects features from the satellite map to the ground-view, based\non a relative camera pose. Aiming to minimize the differences between the\nprojected features and the observed features, we employ a differentiable\nLevenberg-Marquardt ({LM}) module to search for the optimal camera pose\niteratively. The entire pipeline is differentiable and runs end-to-end.\nExtensive experiments on standard autonomous vehicle localization datasets have\nconfirmed the superiority of the proposed method. Notably, e.g., starting from\na coarse estimate of camera location within a wide region of 40m x 40m, with an\n80% likelihood our method quickly reduces the lateral location error to be\nwithin 5m on a new KITTI cross-view dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yujiao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DILEMMA: Self-Supervised Shape and Texture Learning with Transformers. (arXiv:2204.04788v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04788","description":"<p>There is a growing belief that deep neural networks with a shape bias may\nexhibit better generalization capabilities than models with a texture bias,\nbecause shape is a more reliable indicator of the object category. However, we\nshow experimentally that existing measures of shape bias are not stable\npredictors of generalization and argue that shape discrimination should not\ncome at the expense of texture discrimination. Thus, we propose a pseudo-task\nto explicitly boost both shape and texture discriminability in models trained\nvia self-supervised learning. For this purpose, we train a ViT to detect which\ninput token has been combined with an incorrect positional embedding. To retain\ntexture discrimination, the ViT is also trained as in MoCo with a\nstudent-teacher architecture and a contrastive loss over an extra learnable\nclass token. We call our method DILEMMA, which stands for Detection of\nIncorrect Location EMbeddings with MAsked inputs. We evaluate our method\nthrough fine-tuning on several datasets and show that it outperforms MoCoV3 and\nDINO. Moreover, we show that when downstream tasks are strongly reliant on\nshape (such as in the YOGA-82 pose dataset), our pre-trained features yield a\nsignificant gain over prior work. Code will be released upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sameni_S/0/1/0/all/0/1\">Sepehr Sameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenni_S/0/1/0/all/0/1\">Simon Jenni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOS! Self-supervised Learning Over Sets Of Handled Objects In Egocentric Action Recognition. (arXiv:2204.04796v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04796","description":"<p>Learning an egocentric action recognition model from video data is\nchallenging due to distractors (e.g., irrelevant objects) in the background.\nFurther integrating object information into an action model is hence\nbeneficial. Existing methods often leverage a generic object detector to\nidentify and represent the objects in the scene. However, several important\nissues remain. Object class annotations of good quality for the target domain\n(dataset) are still required for learning good object representation. Besides,\nprevious methods deeply couple the existing action models and need to retrain\nthem jointly with object representation, leading to costly and inflexible\nintegration. To overcome both limitations, we introduce Self-Supervised\nLearning Over Sets (SOS), an approach to pre-train a generic Objects In Contact\n(OIC) representation model from video object regions detected by an\noff-the-shelf hand-object contact detector. Instead of augmenting object\nregions individually as in conventional self-supervised learning, we view the\naction process as a means of natural data transformations with unique\nspatio-temporal continuity and exploit the inherent relationships among\nper-video object sets. Extensive experiments on two datasets, EPIC-KITCHENS-100\nand EGTEA, show that our OIC significantly boosts the performance of multiple\nstate-of-the-art video classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Escorcia_V/0/1/0/all/0/1\">Victor Escorcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1\">Ricardo Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Brais Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning. (arXiv:2204.04799v1 [cs.LG])","link":"http://arxiv.org/abs/2204.04799","description":"<p>Continual learning aims to enable a single model to learn a sequence of tasks\nwithout catastrophic forgetting. Top-performing methods usually require a\nrehearsal buffer to store past pristine examples for experience replay, which,\nhowever, limits their practical value due to privacy and memory constraints. In\nthis work, we present a simple yet effective framework, DualPrompt, which\nlearns a tiny set of parameters, called prompts, to properly instruct a\npre-trained model to learn tasks arriving sequentially without buffering past\nexamples. DualPrompt presents a novel approach to attach complementary prompts\nto the pre-trained backbone, and then formulates the objective as learning\ntask-invariant and task-specific \"instructions\". With extensive experimental\nvalidation, DualPrompt consistently sets state-of-the-art performance under the\nchallenging class-incremental setting. In particular, DualPrompt outperforms\nrecent advanced continual learning methods with relatively large buffer sizes.\nWe also introduce a more challenging benchmark, Split ImageNet-R, to help\ngeneralize rehearsal-free continual learning research. Source code is available\nat https://github.com/google-research/l2p.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaoqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1\">Jennifer Dy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OutfitTransformer: Learning Outfit Representations for Fashion Recommendation. (arXiv:2204.04812v1 [cs.CV])","link":"http://arxiv.org/abs/2204.04812","description":"<p>Learning an effective outfit-level representation is critical for predicting\nthe compatibility of items in an outfit, and retrieving complementary items for\na partial outfit. We present a framework, OutfitTransformer, that uses the\nproposed task-specific tokens and leverages the self-attention mechanism to\nlearn effective outfit-level representations encoding the compatibility\nrelationships between all items in the entire outfit for addressing both\ncompatibility prediction and complementary item retrieval tasks. For\ncompatibility prediction, we design an outfit token to capture a global outfit\nrepresentation and train the framework using a classification loss. For\ncomplementary item retrieval, we design a target item token that additionally\ntakes the target item specification (in the form of a category or text\ndescription) into consideration. We train our framework using a proposed\nset-wise outfit ranking loss to generate a target item embedding given an\noutfit, and a target item specification as inputs. The generated target item\nembedding is then used to retrieve compatible items that match the rest of the\noutfit. Additionally, we adopt a pre-training approach and a curriculum\nlearning strategy to improve retrieval performance. Since our framework learns\nat an outfit-level, it allows us to learn a single embedding capturing\nhigher-order relations among multiple items in the outfit more effectively than\npairwise methods. Experiments demonstrate that our approach outperforms\nstate-of-the-art methods on compatibility prediction, fill-in-the-blank, and\ncomplementary item retrieval tasks. We further validate the quality of our\nretrieval results with a user study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1\">Rohan Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodla_N/0/1/0/all/0/1\">Navaneeth Bodla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasileva_M/0/1/0/all/0/1\">Mariya Vasileva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beniwal_A/0/1/0/all/0/1\">Anurag Beniwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Alan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medioni_G/0/1/0/all/0/1\">Gerard Medioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Spectral Feature Extraction via Deep ConvLSTM Neural Networks for Hyperspectral Image Classification. (arXiv:1905.03577v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1905.03577","description":"<p>In recent years, deep learning has presented a great advance in hyperspectral\nimage (HSI) classification. Particularly, long short-term memory (LSTM), as a\nspecial deep learning structure, has shown great ability in modeling long-term\ndependencies in the time dimension of video or the spectral dimension of HSIs.\nHowever, the loss of spatial information makes it quite difficult to obtain the\nbetter performance. In order to address this problem, two novel deep models are\nproposed to extract more discriminative spatial-spectral features by exploiting\nthe Convolutional LSTM (ConvLSTM). By taking the data patch in a local sliding\nwindow as the input of each memory cell band by band, the 2-D extended\narchitecture of LSTM is considered for building the spatial-spectral ConvLSTM\n2-D Neural Network (SSCL2DNN) to model long-range dependencies in the spectral\ndomain. To better preserve the intrinsic structure information of the\nhyperspectral data, the spatial-spectral ConvLSTM 3-D Neural Network (SSCL3DNN)\nis proposed by extending LSTM to 3-D version for further improving the\nclassification performance. The experiments, conducted on three commonly used\nHSI data sets, demonstrate that the proposed deep models have certain\ncompetitive advantages and can provide better classification performance than\nother state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wen-Shuai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Heng-Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harmonic Convolutional Networks based on Discrete Cosine Transform. (arXiv:2001.06570v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.06570","description":"<p>Convolutional neural networks (CNNs) learn filters in order to capture local\ncorrelation patterns in feature space. We propose to learn these filters as\ncombinations of preset spectral filters defined by the Discrete Cosine\nTransform (DCT). Our proposed DCT-based harmonic blocks replace conventional\nconvolutional layers to produce partially or fully harmonic versions of new or\nexisting CNN architectures. Using DCT energy compaction properties, we\ndemonstrate how the harmonic networks can be efficiently compressed by\ntruncating high-frequency information in harmonic blocks thanks to the\nredundancies in the spectral domain. We report extensive experimental\nvalidation demonstrating benefits of the introduction of harmonic blocks into\nstate-of-the-art CNN models in image classification, object detection and\nsemantic segmentation applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulicny_M/0/1/0/all/0/1\">Matej Ulicny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krylov_V/0/1/0/all/0/1\">Vladimir A. Krylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahyot_R/0/1/0/all/0/1\">Rozenn Dahyot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Affect Transfer Learning for Behavior Prediction in an Intelligent Tutoring System. (arXiv:2002.05242v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.05242","description":"<p>In this work, we propose a video-based transfer learning approach for\npredicting problem outcomes of students working with an intelligent tutoring\nsystem (ITS). By analyzing a student's face and gestures, our method predicts\nthe outcome of a student answering a problem in an ITS from a video feed. Our\nwork is motivated by the reasoning that the ability to predict such outcomes\nenables tutoring systems to adjust interventions, such as hints and\nencouragement, and to ultimately yield improved student learning. We collected\na large labeled dataset of student interactions with an intelligent online math\ntutor consisting of 68 sessions, where 54 individual students solved 2,749\nproblems. The dataset is public and available at\nhttps://www.cs.bu.edu/faculty/betke/research/learning/ . Working with this\ndataset, our transfer-learning challenge was to design a representation in the\nsource domain of pictures obtained \"in the wild\" for the task of facial\nexpression analysis, and transferring this learned representation to the task\nof human behavior prediction in the domain of webcam videos of students in a\nclassroom environment. We developed a novel facial affect representation and a\nuser-personalized training scheme that unlocks the potential of this\nrepresentation. We designed several variants of a recurrent neural network that\nmodels the temporal structure of video sequences of students solving math\nproblems. Our final model, named ATL-BP for Affect Transfer Learning for\nBehavior Prediction, achieves a relative increase in mean F-score of 50% over\nthe state-of-the-art method on this new dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1\">Nataniel Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allessio_D/0/1/0/all/0/1\">Danielle A. Allessio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalal_M/0/1/0/all/0/1\">Mona Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Ajjen Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_T/0/1/0/all/0/1\">Thomas Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_J/0/1/0/all/0/1\">John J. Magee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob R. Whitehill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1\">Vitaly Ablavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arroyo_I/0/1/0/all/0/1\">Ivon Arroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woolf_B/0/1/0/all/0/1\">Beverly P. Woolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betke_M/0/1/0/all/0/1\">Margrit Betke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness of Deep Sensor Fusion Models. (arXiv:2006.13192v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.13192","description":"<p>We experimentally study the robustness of deep camera-LiDAR fusion\narchitectures for 2D object detection in autonomous driving. First, we find\nthat the fusion model is usually both more accurate, and more robust against\nsingle-source attacks than single-sensor deep neural networks. Furthermore, we\nshow that without adversarial training, early fusion is more robust than late\nfusion, whereas the two perform similarly after adversarial training. However,\nwe note that single-channel adversarial training of deep fusion is often\ndetrimental even to robustness. Moreover, we observe cross-channel\nexternalities, where single-channel adversarial training reduces robustness to\nattacks on the other channel. Additionally, we observe that the choice of\nadversarial model in adversarial training is critical: using attacks restricted\nto cars' bounding boxes is more effective in adversarial training and exhibits\nless significant cross-channel externalities. Finally, we find that\njoint-channel adversarial training helps mitigate many of the issues above, but\ndoes not significantly boost adversarial robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1\">Ayan Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1\">Yevgeniy Vorobeychik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural networks with late-phase weights. (arXiv:2007.12927v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.12927","description":"<p>The largely successful method of training neural networks is to learn their\nweights using some variant of stochastic gradient descent (SGD). Here, we show\nthat the solutions found by SGD can be further improved by ensembling a subset\nof the weights in late stages of learning. At the end of learning, we obtain\nback a single model by taking a spatial average in weight space. To avoid\nincurring increased computational costs, we investigate a family of\nlow-dimensional late-phase weight models which interact multiplicatively with\nthe remaining parameters. Our results show that augmenting standard models with\nlate-phase weights improves generalization in established benchmarks such as\nCIFAR-10/100, ImageNet and enwik8. These findings are complemented with a\ntheoretical analysis of a noisy quadratic problem which provides a simplified\npicture of the late phases of neural network learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oswald_J/0/1/0/all/0/1\">Johannes von Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1\">Seijin Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meulemans_A/0/1/0/all/0/1\">Alexander Meulemans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henning_C/0/1/0/all/0/1\">Christian Henning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1\">Benjamin F. Grewe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1\">Jo&#xe3;o Sacramento</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularizing Attention Networks for Anomaly Detection in Visual Question Answering. (arXiv:2009.10054v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.10054","description":"<p>For stability and reliability of real-world applications, the robustness of\nDNNs in unimodal tasks has been evaluated. However, few studies consider\nabnormal situations that a visual question answering (VQA) model might\nencounter at test time after deployment in the real-world. In this study, we\nevaluate the robustness of state-of-the-art VQA models to five different\nanomalies, including worst-case scenarios, the most frequent scenarios, and the\ncurrent limitation of VQA models. Different from the results in unimodal tasks,\nthe maximum confidence of answers in VQA models cannot detect anomalous inputs,\nand post-training of the outputs, such as outlier exposure, is ineffective for\nVQA models. Thus, we propose an attention-based method, which uses confidence\nof reasoning between input images and questions and shows much more promising\nresults than the previous methods in unimodal tasks. In addition, we show that\na maximum entropy regularization of attention networks can significantly\nimprove the attention-based anomaly detection of the VQA models. Thanks to the\nsimplicity, attention-based anomaly detection and the regularization are\nmodel-agnostic methods, which can be used for various cross-modal attentions in\nthe state-of-the-art VQA models. The results imply that cross-modal attention\nin VQA is important to improve not only VQA accuracy, but also the robustness\nto various anomalies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Doyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheon_Y/0/1/0/all/0/1\">Yeongjae Cheon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wook-Shin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems. (arXiv:2011.12945v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.12945","description":"<p>In real-world classification tasks, each class often comprises multiple\nfiner-grained \"subclasses.\" As the subclass labels are frequently unavailable,\nmodels trained using only the coarser-grained class labels often exhibit highly\nvariable performance across different subclasses. This phenomenon, known as\nhidden stratification, has important consequences for models deployed in\nsafety-critical applications such as medicine. We propose GEORGE, a method to\nboth measure and mitigate hidden stratification even when subclass labels are\nunknown. We first observe that unlabeled subclasses are often separable in the\nfeature space of deep neural networks, and exploit this fact to estimate\nsubclass labels for the training data via clustering techniques. We then use\nthese approximate subclass labels as a form of noisy supervision in a\ndistributionally robust optimization objective. We theoretically characterize\nthe performance of GEORGE in terms of the worst-case generalization error\nacross any subclass. We empirically validate GEORGE on a mix of real-world and\nbenchmark image classification datasets, and show that our approach boosts\nworst-case subclass accuracy by up to 22 percentage points compared to standard\ntraining techniques, without requiring any prior information about the\nsubclasses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sohoni_N/0/1/0/all/0/1\">Nimit S. Sohoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunnmon_J/0/1/0/all/0/1\">Jared A. Dunnmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angus_G/0/1/0/all/0/1\">Geoffrey Angus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1\">Albert Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum Learning: A Survey. (arXiv:2101.10382v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.10382","description":"<p>Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soviany_P/0/1/0/all/0/1\">Petru Soviany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1\">Paolo Rota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MSR-Video to Text Dataset with Clean Annotations. (arXiv:2102.06448v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06448","description":"<p>Video captioning automatically generates short descriptions of the video\ncontent, usually in form of a single sentence. Many methods have been proposed\nfor solving this task. A large dataset called MSR Video to Text (MSR-VTT) is\noften used as the benchmark dataset for testing the performance of the methods.\nHowever, we found that the human annotations, i.e., the descriptions of video\ncontents in the dataset are quite noisy, e.g., there are many duplicate\ncaptions and many captions contain grammatical problems. These problems may\npose difficulties to video captioning models for learning underlying patterns.\nWe cleaned the MSR-VTT annotations by removing these problems, then tested\nseveral typical video captioning models on the cleaned dataset. Experimental\nresults showed that data cleaning boosted the performances of the models\nmeasured by popular quantitative metrics. We recruited subjects to evaluate the\nresults of a model trained on the original and cleaned datasets. The human\nbehavior experiment demonstrated that trained on the cleaned dataset, the model\ngenerated captions that were more coherent and more relevant to the contents of\nthe video clips.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianmin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frintrop_S/0/1/0/all/0/1\">Simone Frintrop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAE Approximation Error: ELBO and Exponential Families. (arXiv:2102.09310v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.09310","description":"<p>The importance of Variational Autoencoders reaches far beyond standalone\ngenerative models -- the approach is also used for learning latent\nrepresentations and can be generalized to semi-supervised learning. This\nrequires a thorough analysis of their commonly known shortcomings: posterior\ncollapse and approximation errors. This paper analyzes VAE approximation errors\ncaused by the combination of the ELBO objective and encoder models from\nconditional exponential families, including, but not limited to, commonly used\nconditionally independent discrete and continuous models. We characterize\nsubclasses of generative models consistent with these encoder families. We show\nthat the ELBO optimizer is pulled away from the likelihood optimizer towards\nthe consistent subset and study this effect experimentally. Importantly, this\nsubset can not be enlarged, and the respective error cannot be decreased, by\nconsidering deeper encoder/decoder networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhovtsov_A/0/1/0/all/0/1\">Alexander Shekhovtsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlesinger_D/0/1/0/all/0/1\">Dmitrij Schlesinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flach_B/0/1/0/all/0/1\">Boris Flach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeply Unsupervised Patch Re-Identification for Pre-training Object Detectors. (arXiv:2103.04814v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04814","description":"<p>Unsupervised pre-training aims at learning transferable features that are\nbeneficial for downstream tasks. However, most state-of-the-art unsupervised\nmethods concentrate on learning global representations for image-level\nclassification tasks instead of discriminative local region representations,\nwhich limits their transferability to region-level downstream tasks, such as\nobject detection. To improve the transferability of pre-trained features to\nobject detection, we present Deeply Unsupervised Patch Re-ID (DUPR), a simple\nyet effective method for unsupervised visual representation learning. The patch\nRe-ID task treats individual patch as a pseudo-identity and contrastively\nlearns its correspondence in two views, enabling us to obtain discriminative\nlocal features for object detection. Then the proposed patch Re-ID is performed\nin a deeply unsupervised manner, appealing to object detection, which usually\nrequires multilevel feature maps. Extensive experiments demonstrate that DUPR\noutperforms state-of-the-art unsupervised pre-trainings and even the ImageNet\nsupervised pre-training on various downstream tasks related to object\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenhan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Believe The HiPe: Hierarchical Perturbation for Fast, Robust, and Model-Agnostic Saliency Mapping. (arXiv:2103.05108v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.05108","description":"<p>Understanding the predictions made by Artificial Intelligence (AI) systems is\nbecoming more and more important as deep learning models are used for\nincreasingly complex and high-stakes tasks. Saliency mapping -- a popular\nvisual attribution method -- is one important tool for this, but existing\nformulations are limited by either computational cost or architectural\nconstraints. We therefore propose Hierarchical Perturbation, a very fast and\ncompletely model-agnostic method for interpreting model predictions with robust\nsaliency maps. Using standard benchmarks and datasets, we show that our\nsaliency maps are of competitive or superior quality to those generated by\nexisting model-agnostic methods -- and are over 20 times faster to compute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning. (arXiv:2103.09504v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.09504","description":"<p>The predictive learning of spatiotemporal sequences aims to generate future\nimages by learning from the historical context, where the visual dynamics are\nbelieved to have modular structures that can be learned with compositional\nsubsystems. This paper models these structures by presenting PredRNN, a new\nrecurrent network, in which a pair of memory cells are explicitly decoupled,\noperate in nearly independent transition manners, and finally form unified\nrepresentations of the complex environment. Concretely, besides the original\nmemory cell of LSTM, this network is featured by a zigzag memory flow that\npropagates in both bottom-up and top-down directions across all layers,\nenabling the learned visual dynamics at different levels of RNNs to\ncommunicate. It also leverages a memory decoupling loss to keep the memory\ncells from learning redundant features. We further propose a new curriculum\nlearning strategy to force PredRNN to learn long-term dynamics from context\nframes, which can be generalized to most sequence-to-sequence models. We\nprovide detailed ablation studies to verify the effectiveness of each\ncomponent. Our approach is shown to obtain highly competitive results on five\ndatasets for both action-free and action-conditioned predictive learning\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haixu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianjin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency-based Active Learning for Object Detection. (arXiv:2103.10374v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10374","description":"<p>Active learning aims to improve the performance of task model by selecting\nthe most informative samples with a limited budget. Unlike most recent works\nthat focused on applying active learning for image classification, we propose\nan effective Consistency-based Active Learning method for object Detection\n(CALD), which fully explores the consistency between original and augmented\ndata. CALD has three appealing benefits. (i) CALD is systematically designed by\ninvestigating the weaknesses of existing active learning methods, which do not\ntake the unique challenges of object detection into account. (ii) CALD unifies\nbox regression and classification with a single metric, which is not concerned\nby active learning methods for classification. CALD also focuses on the most\ninformative local region rather than the whole image, which is beneficial for\nobject detection. (iii) CALD not only gauges individual information for sample\nselection, but also leverages mutual information to encourage a balanced data\ndistribution. Extensive experiments show that CALD significantly outperforms\nexisting state-of-the-art task-agnostic and detection-specific active learning\nmethods on general object detection datasets. Based on the Faster R-CNN\ndetector, CALD consistently surpasses the baseline method (random selection) by\n2.9/2.8/0.8 mAP on average on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO.\nCode is available at \\url{https://github.com/we1pingyu/CALD}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weiping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Taojiannan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A State-of-the-art Survey of Object Detection Techniques in Microorganism Image Analysis: From Classical Methods to Deep Learning Approaches. (arXiv:2105.03148v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03148","description":"<p>Microorganisms play a vital role in human life. Therefore, microorganism\ndetection is of great significance to human beings. However, the traditional\nmanual microscopic detection methods have the disadvantages of long detection\ncycle, low detection accuracy in large orders, and great difficulty in\ndetecting uncommon microorganisms. Therefore, it is meaningful to apply\ncomputer image analysis technology to the field of microorganism detection.\nComputer image analysis can realize high-precision and high-efficiency\ndetection of microorganisms. In this review, first,we analyse the existing\nmicroorganism detection methods in chronological order, from traditional image\nprocessing and traditional machine learning to deep learning methods. Then, we\nanalyze and summarize these existing methods and introduce some potential\nmethods, including visual transformers. In the end, the future development\ndirection and challenges of microorganism detection are discussed. In general,\nwe have summarized 142 related technical papers from 1985 to the present. This\nreview will help researchers have a more comprehensive understanding of the\ndevelopment process, research status, and future trends in the field of\nmicroorganism detection and provide a reference for researchers in other\nfields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shuojia Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent advances and clinical applications of deep learning in medical image analysis. (arXiv:2105.13381v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13381","description":"<p>Deep learning has received extensive research interest in developing new\nmedical image processing algorithms, and deep learning based models have been\nremarkably successful in a variety of medical imaging tasks to support disease\ndetection and diagnosis. Despite the success, the further improvement of deep\nlearning models in medical image analysis is majorly bottlenecked by the lack\nof large-sized and well-annotated datasets. In the past five years, many\nstudies have focused on addressing this challenge. In this paper, we reviewed\nand summarized these recent studies to provide a comprehensive overview of\napplying deep learning methods in various medical image analysis tasks.\nEspecially, we emphasize the latest progress and contributions of\nstate-of-the-art unsupervised and semi-supervised deep learning in medical\nimage analysis, which are summarized based on different application scenarios,\nincluding classification, segmentation, detection, and image registration. We\nalso discuss the major technical challenges and suggest the possible solutions\nin future research efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Ximin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_K/0/1/0/all/0/1\">Kar-Ming Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_T/0/1/0/all/0/1\">Theresa C. Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1\">Kathleen Moore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannel_R/0/1/0/all/0/1\">Robert S. Mannel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yuchen Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Guided Segmentation Framework for Semi-supervised Video Instance Segmentation. (arXiv:2106.03330v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03330","description":"<p>In this paper, we propose Contextual Guided Segmentation (CGS) framework for\nvideo instance segmentation in three passes. In the first pass, i.e., preview\nsegmentation, we propose Instance Re-Identification Flow to estimate main\nproperties of each instance (i.e., human/non-human, rigid/deformable,\nknown/unknown category) by propagating its preview mask to other frames. In the\nsecond pass, i.e., contextual segmentation, we introduce multiple contextual\nsegmentation schemes. For human instance, we develop skeleton-guided\nsegmentation in a frame along with object flow to correct and refine the result\nacross frames. For non-human instance, if the instance has a wide variation in\nappearance and belongs to known categories (which can be inferred from the\ninitial mask), we adopt instance segmentation. If the non-human instance is\nnearly rigid, we train FCNs on synthesized images from the first frame of a\nvideo sequence. In the final pass, i.e., guided segmentation, we develop a\nnovel fined-grained segmentation method on non-rectangular regions of interest\n(ROIs). The natural-shaped ROI is generated by applying guided attention from\nthe neighbor frames of the current one to reduce the ambiguity in the\nsegmentation of different overlapping instances. Forward mask propagation is\nfollowed by backward mask propagation to further restore missing instance\nfragments due to re-appeared instances, fast motion, occlusion, or heavy\ndeformation. Finally, instances in each frame are merged based on their depth\nvalues, together with human and non-human object interaction and rare instance\npriority. Experiments conducted on the DAVIS Test-Challenge dataset demonstrate\nthe effectiveness of our proposed framework. We achieved the 3rd consistently\nin the DAVIS Challenges 2017-2019 with 75.4%, 72.4%, and 78.4% in terms of\nglobal score, region similarity, and contour accuracy, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam V. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulated Adversarial Testing of Face Recognition Models. (arXiv:2106.04569v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04569","description":"<p>Most machine learning models are validated and tested on fixed datasets. This\ncan give an incomplete picture of the capabilities and weaknesses of the model.\nSuch weaknesses can be revealed at test time in the real world. The risks\ninvolved in such failures can be loss of profits, loss of time or even loss of\nlife in certain critical applications. In order to alleviate this issue,\nsimulators can be controlled in a fine-grained manner using interpretable\nparameters to explore the semantic image manifold. In this work, we propose a\nframework for learning how to test machine learning algorithms using simulators\nin an adversarial manner in order to find weaknesses in the model before\ndeploying it in critical scenarios. We apply this method in a face recognition\nsetup. We show that certain weaknesses of models trained on real data can be\ndiscovered using simulated samples. Using our proposed method, we can find\nadversarial synthetic faces that fool contemporary face recognition models.\nThis demonstrates the fact that these models have weaknesses that are not\nmeasured by commonly used validation datasets. We hypothesize that this type of\nadversarial examples are not isolated, but usually lie in connected spaces in\nthe latent space of the simulator. We present a method to find these\nadversarial regions as opposed to the typical adversarial points found in the\nadversarial example literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1\">Nataniel Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Weichao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1\">Sarah Adel Bargal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective. (arXiv:2106.07115v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07115","description":"<p>Multiple views of data, both naturally acquired (e.g., image and audio) and\nartificially produced (e.g., via adding different noise to data samples), have\nproven useful in enhancing representation learning. Natural views are often\nhandled by multiview analysis tools, e.g., (deep) canonical correlation\nanalysis [(D)CCA], while the artificial ones are frequently used in\nself-supervised learning (SSL) paradigms, e.g., BYOL and Barlow Twins. Both\ntypes of approaches often involve learning neural feature extractors such that\nthe embeddings of data exhibit high cross-view correlations. Although\nintuitive, the effectiveness of correlation-based neural embedding is mostly\nempirically validated.\n</p>\n<p>This work aims to understand latent correlation maximization-based deep\nmultiview learning from a latent component identification viewpoint. An\nintuitive generative model of multiview data is adopted, where the views are\ndifferent nonlinear mixtures of shared and private components. Since the shared\ncomponents are view/distortion-invariant, representing the data using such\ncomponents is believed to reveal the identity of the samples effectively and\nrobustly. Under this model, latent correlation maximization is shown to\nguarantee the extraction of the shared components across views (up to certain\nambiguities). In addition, it is further shown that the private information in\neach view can be provably disentangled from the shared using proper\nregularization design. A finite sample analysis, which has been rare in\nnonlinear mixture identifiability study, is also presented. The theoretical\nresults and newly designed regularization are tested on a series of tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Songtao Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAR-Net: Shape Alignment and Recovery Network for Category-level 6D Object Pose and Size Estimation. (arXiv:2106.14193v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14193","description":"<p>Given a single scene image, this paper proposes a method of Category-level 6D\nObject Pose and Size Estimation (COPSE) from the point cloud of the target\nobject, without external real pose-annotated training data. Specifically,\nbeyond the visual cues in RGB images, we rely on the shape information\npredominately from the depth (D) channel. The key idea is to explore the shape\nalignment of each instance against its corresponding category-level template\nshape, and the symmetric correspondence of each object category for estimating\na coarse 3D object shape. Our framework deforms the point cloud of the\ncategory-level template shape to align the observed instance point cloud for\nimplicitly representing its 3D rotation. Then we model the symmetric\ncorrespondence by predicting symmetric point cloud from the partially observed\npoint cloud. The concatenation of the observed point cloud and symmetric one\nreconstructs a coarse object shape, thus facilitating object center (3D\ntranslation) and 3D size estimation. Extensive experiments on the\ncategory-level NOCS benchmark demonstrate that our lightweight model still\ncompetes with state-of-the-art approaches that require labeled real-world\nimages. We also deploy our approach to a physical Baxter robot to perform\ngrasping tasks on unseen but category-known instances, and the results further\nvalidate the efficacy of our proposed model. Code and pre-trained models are\navailable on the project webpage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1\">Chilam Cheang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14836","description":"<p>Representations of the world environment play a crucial role in artificial\nintelligence. It is often inefficient to conduct reasoning and inference\ndirectly in the space of raw sensory representations, such as pixel values of\nimages. Representation learning allows us to automatically discover suitable\nrepresentations from raw sensory data. For example, given raw sensory data, a\ndeep neural network learns nonlinear representations at its hidden layers,\nwhich are subsequently used for classification (or regression) at its output\nlayer. This happens implicitly during training through minimizing a supervised\nor unsupervised loss in common practical regimes of deep learning, unlike the\nneural tangent kernel (NTK) regime. In this paper, we study the dynamics of\nsuch implicit nonlinear representation learning, which is beyond the NTK\nregime. We identify a pair of a new assumption and a novel condition, called\nthe common model structure assumption and the data-architecture alignment\ncondition. Under the common model structure assumption, the data-architecture\nalignment condition is shown to be sufficient for the global convergence and\nnecessary for the global optimality. Moreover, our theory explains how and when\nincreasing the network size does and does not improve the training behaviors in\nthe practical regime. Our results provide practical guidance for designing a\nmodel structure: e.g., the common model structure assumption can be used as a\njustification for using a particular model structure instead of others. We also\nderive a new training framework based on the theory. The proposed framework is\nempirically shown to maintain competitive (practical) test performances while\nproviding global convergence guarantees for deep residual neural networks with\nconvolutions, skip connections, and batch normalization with standard benchmark\ndatasets, including CIFAR-10, CIFAR-100, and SVHN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08031","description":"<p>The human driver is no longer the only one concerned with the complexity of\nthe driving scenarios. Autonomous vehicles (AV) are similarly becoming involved\nin the process. Nowadays, the development of AVs in urban places raises\nessential safety concerns for vulnerable road users (VRUs) such as pedestrians.\nTherefore, to make the roads safer, it is critical to classify and predict the\npedestrians' future behavior. In this paper, we present a framework based on\nmultiple variations of the Transformer models able to infer predict the\npedestrian street-crossing decision-making based on the dynamics of its\ninitiated trajectory. We showed that using solely bounding boxes as input\nfeatures can outperform the previous state-of-the-art results by reaching a\nprediction accuracy of 91\\% and an F1-score of 0.83 on the PIE dataset. In\naddition, we introduced a large-size simulated dataset (CP2A) using CARLA for\naction prediction. Our model has similarly reached high accuracy (91\\%) and\nF1-score (0.91) on this dataset. Interestingly, we showed that pre-training our\nTransformer model on the CP2A dataset and then fine-tuning it on the PIE\ndataset is beneficial for the action prediction task. Finally, our model's\nresults are successfully supported by the \"human attention to bounding boxes\"\nexperiment which we created to test humans ability for pedestrian action\nprediction without the need for environmental context. The code for the dataset\nand the models is available at:\nhttps://github.com/linaashaji/Action_Anticipation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1\">Lina Achaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1\">Thibault Fouqueray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1\">Francois Aioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1\">Francois Charpillet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Gap between Image Pixels and Semantics via Supervision: A Survey. (arXiv:2107.13757v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13757","description":"<p>The fact that there exists a gap between low-level features and semantic\nmeanings of images, called the semantic gap, is known for decades. Resolution\nof the semantic gap is a long standing problem. The semantic gap problem is\nreviewed and a survey on recent efforts in bridging the gap is made in this\nwork. Most importantly, we claim that the semantic gap is primarily bridged\nthrough supervised learning today. Experiences are drawn from two application\ndomains to illustrate this point: 1) object detection and 2) metric learning\nfor content-based image retrieval (CBIR). To begin with, this paper offers a\nhistorical retrospective on supervision, makes a gradual transition to the\nmodern data-driven methodology and introduces commonly used datasets. Then, it\nsummarizes various supervision methods to bridge the semantic gap in the\ncontext of object detection and metric learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SphereFace2: Binary Classification is All You Need for Deep Face Recognition. (arXiv:2108.01513v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01513","description":"<p>State-of-the-art deep face recognition methods are mostly trained with a\nsoftmax-based multi-class classification framework. Despite being popular and\neffective, these methods still have a few shortcomings that limit empirical\nperformance. In this paper, we start by identifying the discrepancy between\ntraining and evaluation in the existing multi-class classification framework\nand then discuss the potential limitations caused by the \"competitive\" nature\nof softmax normalization. Motivated by these limitations, we propose a novel\nbinary classification training framework, termed SphereFace2. In contrast to\nexisting methods, SphereFace2 circumvents the softmax normalization, as well as\nthe corresponding closed-set assumption. This effectively bridges the gap\nbetween training and evaluation, enabling the representations to be improved\nindividually by each binary classification task. Besides designing a specific\nwell-performing loss function, we summarize a few general principles for this\n\"one-vs-all\" binary classification framework so that it can outperform current\ncompetitive methods. Our experiments on popular benchmarks demonstrate that\nSphereFace2 can consistently outperform state-of-the-art deep face recognition\nmethods. The code has been made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NODEO: A Neural Ordinary Differential Equation Based Optimization Framework for Deformable Image Registration. (arXiv:2108.03443v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03443","description":"<p>Deformable image registration (DIR), aiming to find spatial correspondence\nbetween images, is one of the most critical problems in the domain of medical\nimage analysis. In this paper, we present a novel, generic, and accurate\ndiffeomorphic image registration framework that utilizes neural ordinary\ndifferential equations (NODEs). We model each voxel as a moving particle and\nconsider the set of all voxels in a 3D image as a high-dimensional dynamical\nsystem whose trajectory determines the targeted deformation field. Our method\nleverages deep neural networks for their expressive power in modeling dynamical\nsystems, and simultaneously optimizes for a dynamical system between the image\npairs and the corresponding transformation. Our formulation allows various\nconstraints to be imposed along the transformation to maintain desired\nregularities. Our experiment results show that our method outperforms the\nbenchmarks under various metrics. Additionally, we demonstrate the feasibility\nto expand our framework to register multiple image sets using a unified form of\ntransformation,which could possibly serve a wider range of applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z. Jiahao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiancong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yushkevich_P/0/1/0/all/0/1\">Paul A. Yushkevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1\">M. Ani Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">James C. Gee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Self-Distillation Embedded Supervised Affinity Attention Model for Few-Shot Segmentation. (arXiv:2108.06600v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06600","description":"<p>Few-shot semantic segmentation is a challenging task of predicting object\ncategories in pixel-wise with only few annotated samples. However, existing\napproaches still face two main challenges. First, huge feature distinction\nbetween support and query images causes knowledge transferring barrier, which\nharms the segmentation performance. Second, few support samples cause\nunrepresentative of support features, hardly to guide high-quality query\nsegmentation. To deal with the above two issues, we propose self-distillation\nembedded supervised affinity attention model (SD-AANet) to improve the\nperformance of few-shot segmentation task. Specifically, the self-distillation\nguided prototype module (SDPM) extracts intrinsic prototype by\nself-distillation between support and query to capture representative features.\nThe supervised affinity attention module (SAAM) adopts support ground truth to\nguide the production of high quality query attention map, which can learn\naffinity information to focus on whole area of query target. Extensive\nexperiments prove that our SD-AANet significantly improves the performance\ncomparing with existing methods. Comprehensive ablation experiments and\nvisualization studies also show the significant effect of SDPM and SAAM for\nfew-shot segmentation task. On benchmark datasets, PASCAL-5i and COCO-20i, our\nproposed SD-AANet both achieve state-of-the-art results. Our code is available\non https://github.com/cv516Buaa/SD-AANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Binghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lijiang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMChat: Multi-Modal Chat Dataset on Social Media. (arXiv:2108.07154v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07154","description":"<p>Incorporating multi-modal contexts in conversation is an important step for\ndeveloping more engaging dialogue systems. In this work, we explore this\ndirection by introducing MMChat: a large scale Chinese multi-modal dialogue\ncorpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous\ncorpora that are crowd-sourced or collected from fictitious movies, MMChat\ncontains image-grounded dialogues collected from real conversations on social\nmedia, in which the sparsity issue is observed. Specifically, image-initiated\ndialogues in common communications may deviate to some non-image-grounded\ntopics as the conversation proceeds. To better investigate this issue, we\nmanually annotate 100K dialogues from MMChat and further filter the corpus\naccordingly, which yields MMChat-hf. We develop a benchmark model to address\nthe sparsity issue in dialogue generation tasks by adapting the attention\nrouting mechanism on image features. Experiments demonstrate the usefulness of\nincorporating image features and the effectiveness in handling the sparsity of\nimage features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatchCleanser: Certifiably Robust Defense against Adversarial Patches for Any Image Classifier. (arXiv:2108.09135v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09135","description":"<p>The adversarial patch attack against image classification models aims to\ninject adversarially crafted pixels within a restricted image region (i.e., a\npatch) for inducing model misclassification. This attack can be realized in the\nphysical world by printing and attaching the patch to the victim object; thus,\nit imposes a real-world threat to computer vision systems. To counter this\nthreat, we design PatchCleanser as a certifiably robust defense against\nadversarial patches. In PatchCleanser, we perform two rounds of pixel masking\non the input image to neutralize the effect of the adversarial patch. This\nimage-space operation makes PatchCleanser compatible with any state-of-the-art\nimage classifier for achieving high accuracy. Furthermore, we can prove that\nPatchCleanser will always predict the correct class labels on certain images\nagainst any adaptive white-box attacker within our threat model, achieving\ncertified robustness. We extensively evaluate PatchCleanser on the ImageNet,\nImageNette, CIFAR-10, CIFAR-100, SVHN, and Flowers-102 datasets and demonstrate\nthat our defense achieves similar clean accuracy as state-of-the-art\nclassification models and also significantly improves certified robustness from\nprior works. Remarkably, PatchCleanser achieves 83.9% top-1 clean accuracy and\n62.1% top-1 certified robust accuracy against a 2%-pixel square patch anywhere\non the image for the 1000-class ImageNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Misalignment Problem in Dense Object Detection. (arXiv:2108.12176v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12176","description":"<p>Object detection aims to localize and classify the objects in a given image,\nand these two tasks are sensitive to different object regions. Therefore, some\nlocations predict high-quality bounding boxes but low classification scores,\nand some locations are quite the opposite. A misalignment exists between the\ntwo tasks, and their features are spatially entangled. In order to solve the\nmisalignment problem, we propose a plug-in Spatial-disentangled and\nTask-aligned operator (SALT). By predicting two task-aware point sets that are\nlocated in each task's sensitive regions, SALT can reassign features from those\nregions and align them to the corresponding anchor point. Therefore, features\nfor the two tasks are spatially aligned and disentangled. To minimize the\ndifference between the two regression stages, we propose a Self-distillation\nregression (SDR) loss that can transfer knowledge from the refined regression\nresults to the coarse regression results. On the basis of SALT and SDR loss, we\npropose SALT-Net, which explicitly exploits task-aligned point-set features for\naccurate detection results. Extensive experiments on the MS-COCO dataset show\nthat our proposed methods can consistently boost different state-of-the-art\ndense detectors by $\\sim$2 AP. Notably, SALT-Net with Res2Net-101-DCN backbone\nachieves 53.8 AP on the MS-COCO test-dev.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Pursuit of Designing Multi-modal Transformer for Video Grounding. (arXiv:2109.06085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06085","description":"<p>Video grounding aims to localize the temporal segment corresponding to a\nsentence query from an untrimmed video. Almost all existing video grounding\nmethods fall into two frameworks: 1) Top-down model: It predefines a set of\nsegment candidates and then conducts segment classification and regression. 2)\nBottom-up model: It directly predicts frame-wise probabilities of the\nreferential segment boundaries. However, all these methods are not end-to-end,\ni.e., they always rely on some time-consuming post-processing steps to refine\npredictions. To this end, we reformulate video grounding as a set prediction\ntask and propose a novel end-to-end multi-modal Transformer model, dubbed as\nGTR. Specifically, GTR has two encoders for video and language encoding, and a\ncross-modal decoder for grounding prediction. To facilitate the end-to-end\ntraining, we use a Cubic Embedding layer to transform the raw videos into a set\nof visual tokens. To better fuse these two modalities in the decoder, we design\na new Multi-head Cross-Modal Attention. The whole GTR is optimized via a\nMany-to-One matching loss. Furthermore, we conduct comprehensive studies to\ninvestigate different model design choices. Extensive results on three\nbenchmarks have validated the superiority of GTR. All three typical GTR\nvariants achieve record-breaking performance on all datasets and metrics, with\nseveral times faster inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design. (arXiv:2110.03659v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03659","description":"<p>An agent's functionality is largely determined by its design, i.e., skeletal\nstructure and joint attributes (e.g., length, size, strength). However, finding\nthe optimal agent design for a given function is extremely challenging since\nthe problem is inherently combinatorial and the design space is prohibitively\nlarge. Additionally, it can be costly to evaluate each candidate design which\nrequires solving for its optimal controller. To tackle these problems, our key\nidea is to incorporate the design procedure of an agent into its\ndecision-making process. Specifically, we learn a conditional policy that, in\nan episode, first applies a sequence of transform actions to modify an agent's\nskeletal structure and joint attributes, and then applies control actions under\nthe new design. To handle a variable number of joints across designs, we use a\ngraph-based policy where each graph node represents a joint and uses message\npassing with its neighbors to output joint-specific actions. Using policy\ngradient methods, our approach enables joint optimization of agent design and\ncontrol as well as experience sharing across different designs, which improves\nsample efficiency substantially. Experiments show that our approach,\nTransform2Act, outperforms prior methods significantly in terms of convergence\nspeed and final performance. Notably, Transform2Act can automatically discover\nplausible designs similar to giraffes, squids, and spiders. Code and videos are\navailable at https://sites.google.com/view/transform2act.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengyi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arch-Net: Model Distillation for Architecture Agnostic Model Deployment. (arXiv:2111.01135v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.01135","description":"<p>Vast requirement of computation power of Deep Neural Networks is a major\nhurdle to their real world applications. Many recent Application Specific\nIntegrated Circuit (ASIC) chips feature dedicated hardware support for Neural\nNetwork Acceleration. However, as ASICs take multiple years to develop, they\nare inevitably out-paced by the latest development in Neural Architecture\nResearch. For example, Transformer Networks do not have native support on many\npopular chips, and hence are difficult to deploy. In this paper, we propose\nArch-Net, a family of Neural Networks made up of only operators efficiently\nsupported across most architectures of ASICs. When a Arch-Net is produced, less\ncommon network constructs, like Layer Normalization and Embedding Layers, are\neliminated in a progressive manner through label-free Blockwise Model\nDistillation, while performing sub-eight bit quantization at the same time to\nmaximize performance. Empirical results on machine translation and image\nclassification tasks confirm that we can transform latest developed Neural\nArchitectures into fast running and as-accurate Arch-Net, ready for deployment\non multiple mass-produced ASIC chips. The code will be available at\nhttps://github.com/megvii-research/Arch-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weixin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zipeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shuangkang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Song Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LTD: Low Temperature Distillation for Robust Adversarial Training. (arXiv:2111.02331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.02331","description":"<p>Adversarial training has been widely used to enhance the robustness of the\nneural network models against adversarial attacks. However, there still a\nnotable gap between the nature accuracy and the robust accuracy. We found one\nof the reasons is the commonly used labels, one-hot vectors, hinder the\nlearning process for image recognition. In this paper, we proposed a method,\ncalled Low Temperature Distillation (LTD), which is based on the knowledge\ndistillation framework to generate the desired soft labels. Unlike the previous\nwork, LTD uses relatively low temperature in the teacher model, and employs\ndifferent, but fixed, temperatures for the teacher model and the student model.\nMoreover, we have investigated the methods to synergize the use of nature data\nand adversarial ones in LTD. Experimental results show that without extra\nunlabeled data, the proposed method combined with the previous work can achieve\n57.72\\% and 30.36\\% robust accuracy on CIFAR-10 and CIFAR-100 dataset\nrespectively, which is about 1.21\\% improvement of the state-of-the-art methods\nin average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Erh-Chung Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Che-Rung Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMU: smooth activation function for deep networks using smoothing maximum technique. (arXiv:2111.04682v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.04682","description":"<p>Deep learning researchers have a keen interest in proposing two new novel\nactivation functions which can boost network performance. A good choice of\nactivation function can have significant consequences in improving network\nperformance. A handcrafted activation is the most common choice in neural\nnetwork models. ReLU is the most common choice in the deep learning community\ndue to its simplicity though ReLU has some serious drawbacks. In this paper, we\nhave proposed a new novel activation function based on approximation of known\nactivation functions like Leaky ReLU, and we call this function Smooth Maximum\nUnit (SMU). Replacing ReLU by SMU, we have got 6.22% improvement in the\nCIFAR100 dataset with the ShuffleNet V2 model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Shilpak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FabricFlowNet: Bimanual Cloth Manipulation with a Flow-based Policy. (arXiv:2111.05623v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.05623","description":"<p>We address the problem of goal-directed cloth manipulation, a challenging\ntask due to the deformability of cloth. Our insight is that optical flow, a\ntechnique normally used for motion estimation in video, can also provide an\neffective representation for corresponding cloth poses across observation and\ngoal images. We introduce FabricFlowNet (FFN), a cloth manipulation policy that\nleverages flow as both an input and as an action representation to improve\nperformance. FabricFlowNet also elegantly switches between bimanual and\nsingle-arm actions based on the desired goal. We show that FabricFlowNet\nsignificantly outperforms state-of-the-art model-free and model-based cloth\nmanipulation policies that take image input. We also present real-world\nexperiments on a bimanual system, demonstrating effective sim-to-real transfer.\nFinally, we show that our method generalizes when trained on a single square\ncloth to other cloth shapes, such as T-shirts and rectangular cloths. Video and\nother supplementary materials are available at:\nhttps://sites.google.com/view/fabricflownet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1\">Thomas Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajracharya_S/0/1/0/all/0/1\">Sujay Bajracharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1\">Khush Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swin Transformer V2: Scaling Up Capacity and Resolution. (arXiv:2111.09883v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09883","description":"<p>Large-scale NLP models have been shown to significantly improve the\nperformance on language tasks with no signs of saturation. They also\ndemonstrate amazing few-shot capabilities like that of human beings. This paper\naims to explore large-scale models in computer vision. We tackle three major\nissues in training and application of large vision models, including training\ninstability, resolution gaps between pre-training and fine-tuning, and hunger\non labelled data. Three main techniques are proposed: 1) a residual-post-norm\nmethod combined with cosine attention to improve training stability; 2) A\nlog-spaced continuous position bias method to effectively transfer models\npre-trained using low-resolution images to downstream tasks with\nhigh-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to\nreduce the needs of vast labeled images. Through these techniques, this paper\nsuccessfully trained a 3 billion-parameter Swin Transformer V2 model, which is\nthe largest dense vision model to date, and makes it capable of training with\nimages of up to 1,536$\\times$1,536 resolution. It set new performance records\non 4 representative vision tasks, including ImageNet-V2 image classification,\nCOCO object detection, ADE20K semantic segmentation, and Kinetics-400 video\naction classification. Also note our training is much more efficient than that\nin Google's billion-level visual models, which consumes 40 times less labelled\ndata and 40 times less training time. Code is available at\n\\url{https://github.com/microsoft/Swin-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuliang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1\">Jia Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferability Estimation using Bhattacharyya Class Separability. (arXiv:2111.12780v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12780","description":"<p>Transfer learning has become a popular method for leveraging pre-trained\nmodels in computer vision. However, without performing computationally\nexpensive fine-tuning, it is difficult to quantify which pre-trained source\nmodels are suitable for a specific target task, or, conversely, to which tasks\na pre-trained source model can be easily adapted to. In this work, we propose\nGaussian Bhattacharyya Coefficient (GBC), a novel method for quantifying\ntransferability between a source model and a target dataset. In a first step we\nembed all target images in the feature space defined by the source model, and\nrepresent them with per-class Gaussians. Then, we estimate their pairwise class\nseparability using the Bhattacharyya coefficient, yielding a simple and\neffective measure of how well the source model transfers to the target task. We\nevaluate GBC on image classification tasks in the context of dataset and\narchitecture selection. Further, we also perform experiments on the more\ncomplex semantic segmentation transferability estimation task. We demonstrate\nthat GBC outperforms state-of-the-art transferability metrics on most\nevaluation criteria in the semantic segmentation settings, matches the\nperformance of top methods for dataset transferability in image classification,\nand performs best on architecture selection problems for image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandy_M/0/1/0/all/0/1\">Michal P&#xe1;ndy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agostinelli_A/0/1/0/all/0/1\">Andrea Agostinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uijlings_J/0/1/0/all/0/1\">Jasper Uijlings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1\">Thomas Mensink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Low-Cost and Efficient Malaria Detection. (arXiv:2111.13656v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13656","description":"<p>Malaria, a fatal but curable disease claims hundreds of thousands of lives\nevery year. Early and correct diagnosis is vital to avoid health complexities,\nhowever, it depends upon the availability of costly microscopes and trained\nexperts to analyze blood-smear slides. Deep learning-based methods have the\npotential to not only decrease the burden of experts but also improve\ndiagnostic accuracy on low-cost microscopes. However, this is hampered by the\nabsence of a reasonable size dataset. One of the most challenging aspects is\nthe reluctance of the experts to annotate the dataset at low magnification on\nlow-cost microscopes. We present a dataset to further the research on malaria\nmicroscopy over the low-cost microscopes at low magnification. Our large-scale\ndataset consists of images of blood-smear slides from several malaria-infected\npatients, collected through microscopes at two different cost spectrums and\nmultiple magnifications. Malarial cells are annotated for the localization and\nlife-stage classification task on the images collected through the high-cost\nmicroscope at high magnification. We design a mechanism to transfer these\nannotations from the high-cost microscope at high magnification to the low-cost\nmicroscope, at multiple magnifications. Multiple object detectors and domain\nadaptation methods are presented as the baselines. Furthermore, a partially\nsupervised domain adaptation method is introduced to adapt the object-detector\nto work on the images collected from the low-cost microscope. The dataset will\nbe made publicly available after publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sultani_W/0/1/0/all/0/1\">Waqas Sultani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawaz_W/0/1/0/all/0/1\">Wajahat Nawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Syed Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danish_M/0/1/0/all/0/1\">Muhammad Sohail Danish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadia_A/0/1/0/all/0/1\">Asma Saadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"n-CPS: Generalising Cross Pseudo Supervision to n Networks for Semi-Supervised Semantic Segmentation. (arXiv:2112.07528v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07528","description":"<p>We present n-CPS - a generalisation of the recent state-of-the-art cross\npseudo supervision (CPS) approach for the task of semi-supervised semantic\nsegmentation. In n-CPS, there are n simultaneously trained subnetworks that\nlearn from each other through one-hot encoding perturbation and consistency\nregularisation. We also show that ensembling techniques applied to subnetworks\noutputs can significantly improve the performance. To the best of our\nknowledge, n-CPS paired with CutMix outperforms CPS and sets the new\nstate-of-the-art for Pascal VOC 2012 with (1/16, 1/8, 1/4, and 1/2 supervised\nregimes) and Cityscapes (1/16 supervised).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Filipiak_D/0/1/0/all/0/1\">Dominik Filipiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tempczyk_P/0/1/0/all/0/1\">Piotr Tempczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cygan_M/0/1/0/all/0/1\">Marek Cygan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Distribution Patterns and Stress Potential Signs of Clownfish in Recirculating Aquaculture Systems. (arXiv:2112.14513v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14513","description":"<p>Monitoring and detecting fish behaviors provide essential information on fish\nwelfare and contribute to achieving intelligent production in global\naquaculture. This work proposes an efficient approach to analyze the spatial\ndistribution status and motion patterns of juvenile clownfish\n\\textit{(Amphiprion bicinctus)} maintained in aquaria at three stocking\ndensities (1, 5, and 10 individuals/aquarium). The estimated displacement is\nthe key factor in assessing the dispersion and velocity to express the\nclownfish's spatial distribution and movement behavior in a recirculating\naquaculture system. Indeed, we aim at computing the velocity, magnitude, and\nturning angle using an optical flow method to assist aquaculturists in\nefficiently monitoring and identifying fish behavior. We test the system design\non a database containing two days of video streams of juvenile clownfish\nmaintained in aquaria. The proposed displacement estimation reveals good\nperformance in measuring clownfish's motion and dispersion characteristics\nleading to assess the potential signs of stress behaviors. Furthermore, we\ndemonstrate the effectiveness of the proposed technique for quantifying\nvariation in clownfish activity levels between recordings taken in the morning\nand afternoon at different stocking densities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aljehani_F/0/1/0/all/0/1\">Fahad Aljehani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+NDoye_I/0/1/0/all/0/1\">Ibrahima N&#x27;Doye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justo_M/0/1/0/all/0/1\">Micaela S. Justo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majoris_J/0/1/0/all/0/1\">John E. Majoris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berumen_M/0/1/0/all/0/1\">Michael L. Berumen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleg_Kirati_T/0/1/0/all/0/1\">Taous-Meriem Laleg-Kirati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering. (arXiv:2112.15399v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15399","description":"<p>We present an information-theoretic regularization technique for few-shot\nnovel view synthesis based on neural implicit representation. The proposed\napproach minimizes potential reconstruction inconsistency that happens due to\ninsufficient viewpoints by imposing the entropy constraint of the density in\neach ray. In addition, to alleviate the potential degenerate issue when all\ntraining images are acquired from almost redundant viewpoints, we further\nincorporate the spatially smoothness constraint into the estimated images by\nrestricting information gains from a pair of rays with slightly different\nviewpoints. The main idea of our algorithm is to make reconstructed scenes\ncompact along individual rays and consistent across rays in the neighborhood.\nThe proposed regularizers can be plugged into most of existing neural volume\nrendering techniques based on NeRF in a straightforward way. Despite its\nsimplicity, we achieve consistently improved performance compared to existing\nneural view synthesis methods by large margins on multiple standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Mijeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seonguk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Grounded Visual Embeddings for Zero-Shot Learning. (arXiv:2201.00577v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00577","description":"<p>Zero-shot learning methods rely on fixed visual and semantic embeddings,\nextracted from independent vision and language models, both pre-trained for\nother large-scale tasks. This is a weakness of current zero-shot learning\nframeworks as such disjoint embeddings fail to adequately associate visual and\ntextual information to their shared semantic content. Therefore, we propose to\nlearn semantically grounded and enriched visual information by computing a\njoint image and text model with a two-stream network on a proxy task. To\nimprove this alignment between image and textual representations, provided by\nattributes, we leverage ancillary captions to provide grounded semantic\ninformation. Our method, dubbed joint embeddings for zero-shot learning is\nevaluated on several benchmark datasets, improving the performance of existing\nstate-of-the-art methods in both standard ($+1.6$\\% on aPY, $+2.6\\%$ on FLO)\nand generalized ($+2.1\\%$ on AWA$2$, $+2.2\\%$ on CUB) zero-shot recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nawaz_S/0/1/0/all/0/1\">Shah Nawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavazza_J/0/1/0/all/0/1\">Jacopo Cavazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An unambiguous cloudiness index for nonwovens. (arXiv:2201.02011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02011","description":"<p>Cloudiness or formation is a concept routinely used in industry to address\ndeviations from homogeneity in nonwovens and papers. Measuring a cloudiness\nindex based on image data is a common task in industrial quality assurance. The\ntwo most popular ways of quantifying cloudiness are based on power spectrum or\ncorrelation function on the one hand or the Laplacian pyramid on the other\nhand. Here, we recall the mathematical basis of the first approach\ncomprehensively, derive a cloudiness index, and demonstrate its practical\nestimation. We prove that the Laplacian pyramid as well as other quantities\ncharacterizing cloudiness like the range of interaction and the intensity of\nsmall-angle scattering are very closely related to the power spectrum. Finally,\nwe show that the power spectrum is easy to be measured image analytically and\ncarries more information than the alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godehardt_M/0/1/0/all/0/1\">Michael Godehardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghiseh_A/0/1/0/all/0/1\">Ali Moghiseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oetjen_C/0/1/0/all/0/1\">Christine Oetjen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohser_J/0/1/0/all/0/1\">Joachim Ohser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schladitz_K/0/1/0/all/0/1\">Katja Schladitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swin Transformer for Fast MRI. (arXiv:2201.03230v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.03230","description":"<p>Magnetic resonance imaging (MRI) is an important non-invasive clinical tool\nthat can produce high-resolution and reproducible images. However, a long\nscanning time is required for high-quality MR images, which leads to exhaustion\nand discomfort of patients, inducing more artefacts due to voluntary movements\nof the patients and involuntary physiological movements. To accelerate the\nscanning process, methods by k-space undersampling and deep learning based\nreconstruction have been popularised. This work introduced SwinMR, a novel Swin\ntransformer based method for fast MRI reconstruction. The whole network\nconsisted of an input module (IM), a feature extraction module (FEM) and an\noutput module (OM). The IM and OM were 2D convolutional layers and the FEM was\ncomposed of a cascaded of residual Swin transformer blocks (RSTBs) and 2D\nconvolutional layers. The RSTB consisted of a series of Swin transformer layers\n(STLs). The shifted windows multi-head self-attention (W-MSA/SW-MSA) of STL was\nperformed in shifted windows rather than the multi-head self-attention (MSA) of\nthe original transformer in the whole image space. A novel multi-channel loss\nwas proposed by using the sensitivity maps, which was proved to reserve more\ntextures and details. We performed a series of comparative studies and ablation\nstudies in the Calgary-Campinas public brain MR dataset and conducted a\ndownstream segmentation experiment in the Multi-modal Brain Tumour Segmentation\nChallenge 2017 dataset. The results demonstrate our SwinMR achieved\nhigh-quality reconstruction compared with other benchmark methods, and it shows\ngreat robustness with different undersampling masks, under noise interruption\nand on different datasets. The code is publicly available at\nhttps://github.com/ayanglab/SwinMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jiahao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yingying Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huanjun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Real Talking Faces via Self-Supervision for Robust Forgery Detection. (arXiv:2201.07131v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07131","description":"<p>One of the most pressing challenges for the detection of face-manipulated\nvideos is generalising to forgery methods not seen during training while\nremaining effective under common corruptions such as compression. In this\npaper, we examine whether we can tackle this issue by harnessing videos of real\ntalking faces, which contain rich information on natural facial appearance and\nbehaviour and are readily available in large quantities online. Our method,\ntermed RealForensics, consists of two stages. First, we exploit the natural\ncorrespondence between the visual and auditory modalities in real videos to\nlearn, in a self-supervised cross-modal manner, temporally dense video\nrepresentations that capture factors such as facial movements, expression, and\nidentity. Second, we use these learned representations as targets to be\npredicted by our forgery detector along with the usual binary forgery\nclassification task; this encourages it to base its real/fake decision on said\nfactors. We show that our method achieves state-of-the-art performance on\ncross-manipulation generalisation and robustness experiments, and examine the\nfactors that contribute to its performance. Our results suggest that leveraging\nnatural and unlabelled videos is a promising direction for the development of\nmore robust face forgery detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haliassos_A/0/1/0/all/0/1\">Alexandros Haliassos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mira_R/0/1/0/all/0/1\">Rodrigo Mira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POTHER: Patch-Voted Deep Learning-based Chest X-ray Bias Analysis for COVID-19 Detection. (arXiv:2201.09360v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.09360","description":"<p>A critical step in the fight against COVID-19, which continues to have a\ncatastrophic impact on peoples lives, is the effective screening of patients\npresented in the clinics with severe COVID-19 symptoms. Chest radiography is\none of the promising screening approaches. Many studies reported detecting\nCOVID-19 in chest X-rays accurately using deep learning. A serious limitation\nof many published approaches is insufficient attention paid to explaining\ndecisions made by deep learning models. Using explainable artificial\nintelligence methods, we demonstrate that model decisions may rely on\nconfounding factors rather than medical pathology. After an analysis of\npotential confounding factors found on chest X-ray images, we propose a novel\nmethod to minimise their negative impact. We show that our proposed method is\nmore robust than previous attempts to counter confounding factors such as ECG\nleads in chest X-rays that often influence model classification decisions. In\naddition to being robust, our method achieves results comparable to the\nstate-of-the-art. The source code and pre-trained weights are publicly\navailable at (https://github.com/tomek1911/POTHER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Szczepanski_T/0/1/0/all/0/1\">Tomasz Szczepa&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTHA: Video Captioning Evaluation Via Transfer-Learned Human Assessment. (arXiv:2201.10243v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10243","description":"<p>Evaluating video captioning systems is a challenging task as there are\nmultiple factors to consider; for instance: the fluency of the caption,\nmultiple actions happening in a single scene, and the human bias of what is\nconsidered important. Most metrics try to measure how similar the system\ngenerated captions are to a single or a set of human-annotated captions. This\npaper presents a new method based on a deep learning model to evaluate these\nsystems. The model is based on BERT, which is a language model that has been\nshown to work well in multiple NLP tasks. The aim is for the model to learn to\nperform an evaluation similar to that of a human. To do so, we use a dataset\nthat contains human evaluations of system generated captions. The dataset\nconsists of the human judgments of the captions produce by the system\nparticipating in various years of the TRECVid video to text task. These\nannotations will be made publicly available. BERTHA obtain favourable results,\noutperforming the commonly used metrics in some setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lebron_L/0/1/0/all/0/1\">Luis Lebron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouramas_K/0/1/0/all/0/1\">Konstantinos Kouramas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Contrastive Learning is Provably (almost) Principal Component Analysis. (arXiv:2201.12680v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12680","description":"<p>We show that Contrastive Learning (CL) under a family of loss functions\n(including InfoNCE) has a game-theoretical formulation, where the \\emph{max\nplayer} finds representation to maximize contrastiveness, and the \\emph{min\nplayer} puts weights on pairs of samples with similar representation. We show\nthat the max player who does \\emph{representation learning} reduces to\nPrincipal Component Analysis for deep linear network, and almost all local\nminima are global, recovering optimal PCA solutions. Experiments show that the\nformulation yields comparable (or better) performance on CIFAR10 and STL-10\nwhen extending beyond InfoNCE, yielding novel contrastive losses. Furthermore,\nwe extend our theoretical analysis to 2-layer ReLU networks, showing its\ndifference from linear ones, and proving that feature composition is preferred\nover picking single dominant feature under strong augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.01993","description":"<p>Visual question answering is the task of answering questions about images. We\nintroduce the VizWiz-VQA-Grounding dataset, the first dataset that visually\ngrounds answers to visual questions asked by people with visual impairments. We\nanalyze our dataset and compare it with five VQA-Grounding datasets to\ndemonstrate what makes it similar and different. We then evaluate the SOTA VQA\nand VQA-Grounding models and demonstrate that current SOTA algorithms often\nfail to identify the correct visual evidence where the answer is located. These\nmodels regularly struggle when the visual evidence occupies a small fraction of\nthe image, for images that are higher quality, as well as for visual questions\nthat require skills in text recognition. The dataset, evaluation server, and\nleaderboard all can be found at the following link:\nhttps://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_S/0/1/0/all/0/1\">Samreen Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSHA: Video Violence Recognition and Localization Using a Semi-Supervised Hard Attention Model. (arXiv:2202.02212v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02212","description":"<p>Current human-based surveillance systems are prone to inadequate availability\nand reliability. Artificial intelligence-based solutions are compelling,\nconsidering their reliability and precision in the face of an increasing\nadaption of surveillance systems. Exceedingly efficient and precise machine\nlearning models are required to effectively utilize the extensive volume of\nhigh-definition surveillance imagery. This study focuses on improving the\naccuracy of the methods and models used in automated surveillance systems to\nrecognize and localize human violence in video footage. The proposed model uses\nan I3D backbone pretrained on the Kinetics dataset and has achieved\nstate-of-the-art accuracy of 90.4% and 98.7% on RWF and Hockey datasets,\nrespectively. The semi-supervised hard attention mechanism has enabled the\nproposed method to fully capture the available information in a high-resolution\nvideo by processing the necessary video regions in great detail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_H/0/1/0/all/0/1\">Hamid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazerfard_E/0/1/0/all/0/1\">Ehsan Nazerfard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Wilderness Using Explainable Machine Learning in Satellite Imagery. (arXiv:2203.00379v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00379","description":"<p>Wilderness areas offer important ecological and social benefits and therefore\nwarrant monitoring and preservation. Yet, the characteristics of wilderness are\nlittle known, making the detection and monitoring of wilderness areas via\nremote sensing techniques a challenging task. We explore the appearance and\ncharacteristics of the vague concept of wilderness via multispectral satellite\nimagery. For this, we apply a novel explainable machine learning technique to a\ndataset consisting of wild and anthropogenic areas in Fennoscandia. With our\ntechnique, we predict continuous, detailed, and high-resolution sensitivity\nmaps of unseen remote sensing data for wild and anthropogenic characteristics.\nOur neural network provides an interpretable activation space in which regions\nare semantically arranged regarding these characteristics and certain land\ncover classes. Interpretability increases confidence in the method and allows\nfor new explanations of the investigated concept. Our model advances\nexplainable machine learning for remote sensing, offers opportunities for\ncomprehensive analyses of existing wilderness, and has practical relevance for\nconservation efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stomberg_T/0/1/0/all/0/1\">Timo T. Stomberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_T/0/1/0/all/0/1\">Taylor Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonhardt_J/0/1/0/all/0/1\">Johannes Leonhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1\">Immanuel Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SingleSketch2Mesh : Generating 3D Mesh model from Sketch. (arXiv:2203.03157v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03157","description":"<p>Sketching is an important activity in any design process. Designers and\nstakeholders share their ideas through hand-drawn sketches. These sketches are\nfurther used to create 3D models. Current methods to generate 3D models from\nsketches are either manual or tightly coupled with 3D modeling platforms.\nTherefore, it requires users to have an experience of sketching on such\nplatform. Moreover, most of the existing approaches are based on geometric\nmanipulation and thus cannot be generalized. We propose a novel AI based\nensemble approach, SingleSketch2Mesh, for generating 3D models from hand-drawn\nsketches. Our approach is based on Generative Networks and Encoder-Decoder\nArchitecture to generate 3D mesh model from a hand-drawn sketch. We evaluate\nour solution with existing solutions. Our approach outperforms existing\napproaches on both - quantitative and qualitative evaluation criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_N/0/1/0/all/0/1\">Nitish Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_D/0/1/0/all/0/1\">Dhornala Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Alpana Dubey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Open-Set Text Recognition via Label-to-Prototype Learning. (arXiv:2203.05179v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05179","description":"<p>Scene text recognition is a popular topic and extensively used in the\nindustry. Although many methods have achieved satisfactory performance for the\nclose-set text recognition challenges, these methods lose feasibility in\nopen-set scenarios, where collecting data or retraining models for novel\ncharacters is too expensive. E.g., annotating samples for foreign languages can\nbe expensive, whereas retraining the model each time a \"novel\" character is\ndiscovered from historical documents also costs time and resources. In this\npaper, we introduce and formulate a new task, i.e., the open-set text\nrecognition task, which demands the capability to spot and cognize novel\ncharacters without retraining. Here, we propose a label-to-prototype learning\nframework that fulfills the new requirements in the proposed task.\nSpecifically, novel characters are mapped to their corresponding prototypes\nwith a Label-to-Prototype Learning module. The module is trained on seen labels\nand holds generalization capability for generating class centers for novel\ncharacters without retraining. The framework also implements rejection\ncapability over out-of-set characters, which allows spotting unknown characters\nduring the evaluation process. Extensive experiments show that our method\nachieves promising performance on a variety of zero-shot, close-set, and\nopen-set text recognition datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hai-Bo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng-Lin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFTR: Depth-supervised Fusion Transformer for Salient Object Detection. (arXiv:2203.06429v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06429","description":"<p>Automated salient object detection (SOD) plays an increasingly crucial role\nin many computer vision applications. By reformulating the depth information as\nsupervision rather than as input, depth-supervised convolutional neural\nnetworks (CNN) have achieved promising results on both RGB and RGB-D SOD\nscenarios with the merits of no requirements for extra depth networks and depth\ninputs in the inference stage. This paper, for the first time, seeks to expand\nthe applicability of depth supervision to the Transformer architecture.\nSpecifically, we develop a Depth-supervised Fusion TRansformer (DFTR), to\nfurther improve the accuracy of both RGB and RGB-D SOD. The proposed DFTR\ninvolves three primary features: 1) DFTR, to the best of our knowledge, is the\nfirst pure Transformer-based model for depth-supervised SOD; 2) A multi-scale\nfeature aggregation (MFA) module is proposed to fully exploit the multi-scale\nfeatures encoded by the Swin Transformer in a coarse-to-fine manner; 3) To\nenable bidirectional information flow across different streams of features, a\nnovel multi-stage feature fusion (MFF) module is further integrated into our\nDFTR with the emphasis on salient regions at different network learning stages.\nWe extensively evaluate the proposed DFTR on ten benchmarking datasets.\nExperimental results show that our DFTR consistently outperforms the existing\nstate-of-the-art methods for both RGB and RGB-D SOD tasks. The code and model\nwill be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Latency Attacks via Sponge Poisoning. (arXiv:2203.08147v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2203.08147","description":"<p>Sponge examples are test-time inputs carefully-optimized to increase energy\nconsumption and latency of neural networks when deployed on hardware\naccelerators. In this work, we demonstrate that sponge attacks can also be\nimplanted at training time, when model training is outsourced to a third party,\nvia an attack that we call sponge poisoning. This attack allows one to increase\nthe energy consumption and latency of machine-learning models indiscriminately\non each test-time input. We present a novel formalization for sponge poisoning,\novercoming the limitations related to the optimization of test-time sponge\nexamples, and show that this attack is possible even if the attacker only\ncontrols a few poisoning samples and model updates. Our extensive experimental\nanalysis, involving two deep learning architectures and three datasets, shows\nthat sponge poisoning can almost completely vanish the effect of such hardware\naccelerators. Finally, we analyze activations of the resulting sponge models,\nidentifying the module components that are more sensitive to this\nvulnerability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cina_A/0/1/0/all/0/1\">Antonio Emanuele Cin&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelillo_M/0/1/0/all/0/1\">Marcello Pelillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.09268","description":"<p>We present PROSUB: PROgressive SUBsampling, a deep learning based, automated\nmethodology that subsamples an oversampled data set (e.g. multi-channeled 3D\nimages) with minimal loss of information. We build upon a recent dual-network\napproach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI\nmeasurement sampling-reconstruction challenge, but suffers from deep learning\ntraining instability, by subsampling with a hard decision boundary. PROSUB uses\nthe paradigm of recursive feature elimination (RFE) and progressively\nsubsamples measurements during deep learning training, improving optimization\nstability. PROSUB also integrates a neural architecture search (NAS) paradigm,\nallowing the network architecture hyperparameters to respond to the subsampling\nprocess. We show PROSUB outperforms the winner of the MUDI MICCAI challenge,\nproducing large improvements &gt;18% MSE on the MUDI challenge sub-tasks and\nqualitative improvements on downstream processes useful for clinical\napplications. We also show the benefits of incorporating NAS and analyze the\neffect of PROSUB's components. As our method generalizes to other problems\nbeyond MRI measurement selection-reconstruction, our code is\nhttps://github.com/sbb-gh/PROSUB\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Blumberg_S/0/1/0/all/0/1\">Stefano B. Blumberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Hongxiang Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grussu_F/0/1/0/all/0/1\">Francesco Grussu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yukun Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Figini_M/0/1/0/all/0/1\">Matteo Figini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization. (arXiv:2203.12870v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12870","description":"<p>6-DoF object pose estimation from a monocular image is challenging, and a\npost-refinement procedure is generally needed for high-precision estimation. In\nthis paper, we propose a framework based on a recurrent neural network (RNN)\nfor object pose refinement, which is robust to erroneous initial poses and\nocclusions. During the recurrent iterations, object pose refinement is\nformulated as a non-linear least squares problem based on the estimated\ncorrespondence field (between a rendered image and the observed image). The\nproblem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm\nenabling end-to-end training. The correspondence field estimation and pose\nrefinement are conducted alternatively in each iteration to recover the object\nposes. Furthermore, to improve the robustness to occlusion, we introduce a\nconsistency-check mechanism based on the learned descriptors of the 3D model\nand observed 2D images, which downweights the unreliable correspondences during\npose optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and\nYCB-Video datasets validate the effectiveness of our method and demonstrate\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kwan-Yee Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction. (arXiv:2203.12997v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12997","description":"<p>Dimensionality reduction is crucial both for visualization and preprocessing\nhigh dimensional data for machine learning. We introduce a novel method based\non a hierarchy built on 1-nearest neighbor graphs in the original space which\nis used to preserve the grouping properties of the data distribution on\nmultiple levels. The core of the proposal is an optimization-free projection\nthat is competitive with the latest versions of t-SNE and UMAP in performance\nand visualization quality while being an order of magnitude faster in run-time.\nFurthermore, its interpretable mechanics, the ability to project new data, and\nthe natural separation of data clusters in visualizations make it a general\npurpose unsupervised dimension reduction technique. In the paper, we argue\nabout the soundness of the proposed method and evaluate it on a diverse\ncollection of datasets with sizes varying from 1K to 11M samples and dimensions\nfrom 28 to 16K. We perform comparisons with other state-of-the-art methods on\nmultiple metrics and target dimensions highlighting its efficiency and\nperformance. Code is available at https://github.com/koulakis/h-nne\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarfraz_M/0/1/0/all/0/1\">M. Saquib Sarfraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koulakis_M/0/1/0/all/0/1\">Marios Koulakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Playing Lottery Tickets in Style Transfer Models. (arXiv:2203.13802v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13802","description":"<p>Style transfer has achieved great success and attracted a wide range of\nattention from both academic and industrial communities due to its flexible\napplication scenarios. However, the dependence on a pretty large VGG-based\nautoencoder leads to existing style transfer models having high parameter\ncomplexities, which limits their applications on resource-constrained devices.\nCompared with many other tasks, the compression of style transfer models has\nbeen less explored. Recently, the lottery ticket hypothesis (LTH) has shown\ngreat potential in finding extremely sparse matching subnetworks which can\nachieve on par or even better performance than the original full networks when\ntrained in isolation. In this work, we for the first time perform an empirical\nstudy to verify whether such trainable matching subnetworks also exist in style\ntransfer models. Specifically, we take two most popular style transfer models,\ni.e., AdaIN and SANet, as the main testbeds, which represent global and local\ntransformation based style transfer methods respectively. We carry out\nextensive experiments and comprehensive analysis, and draw the following\nconclusions. (1) Compared with fixing the VGG encoder, style transfer models\ncan benefit more from training the whole network together. (2) Using iterative\nmagnitude pruning, we find the matching subnetworks at 89.2% sparsity in AdaIN\nand 73.7% sparsity in SANet, which demonstrates that style transfer models can\nplay lottery tickets too. (3) The feature transformation module should also be\npruned to obtain a much sparser model without affecting the existence and\nquality of the matching subnetworks. (4) Besides AdaIN and SANet, other models\nsuch as LST, MANet, AdaAttN and MCCNet can also play lottery tickets, which\nshows that LTH can be generalized to various style transfer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_M/0/1/0/all/0/1\">Meihao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAEDID: Patch Autoencoder Based Deep Image Decomposition For Pixel-level Defective Region Segmentation. (arXiv:2203.14457v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14457","description":"<p>Unsupervised pixel-level defective region segmentation is an important task\nin image-based anomaly detection for various industrial applications. The\nstate-of-the-art methods have their own advantages and limitations:\nmatrix-decomposition-based methods are robust to noise but lack complex\nbackground image modeling capability; representation-based methods are good at\ndefective region localization but lack accuracy in defective region shape\ncontour extraction; reconstruction-based methods detected defective region\nmatch well with the ground truth defective region shape contour but are noisy.\nTo combine the best of both worlds, we present an unsupervised patch\nautoencoder based deep image decomposition (PAEDID) method for defective region\nsegmentation. In the training stage, we learn the common background as a deep\nimage prior by a patch autoencoder (PAE) network. In the inference stage, we\nformulate anomaly detection as an image decomposition problem with the deep\nimage prior and domain-specific regularizations. By adopting the proposed\napproach, the defective regions in the image can be accurately extracted in an\nunsupervised fashion. We demonstrate the effectiveness of the PAEDID method in\nsimulation studies and an industrial dataset in the case study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_S/0/1/0/all/0/1\">Shancong Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianjun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1\">Jiulong Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Driven, Soft Alignment of Functional Data Using Shapes and Landmarks. (arXiv:2203.14810v2 [stat.ME] UPDATED)","link":"http://arxiv.org/abs/2203.14810","description":"<p>Alignment or registration of functions is a fundamental problem in\nstatistical analysis of functions and shapes. While there are several\napproaches available, a more recent approach based on Fisher-Rao metric and\nsquare-root velocity functions (SRVFs) has been shown to have good performance.\nHowever, this SRVF method has two limitations: (1) it is susceptible to over\nalignment, i.e., alignment of noise as well as the signal, and (2) in case\nthere is additional information in form of landmarks, the original formulation\ndoes not prescribe a way to incorporate that information. In this paper we\npropose an extension that allows for incorporation of landmark information to\nseek a compromise between matching curves and landmarks. This results in a soft\nlandmark alignment that pushes landmarks closer, without requiring their exact\noverlays to finds a compromise between contributions from functions and\nlandmarks. The proposed method is demonstrated to be superior in certain\npractical scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyang Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Srivastava_A/0/1/0/all/0/1\">Anuj Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A systematic review and meta-analysis of Digital Elevation Model (DEM) fusion: pre-processing, methods and applications. (arXiv:2203.15026v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15026","description":"<p>The remote sensing community has identified data fusion as one of the key\nchallenging topics of the 21st century. The subject of image fusion in\ntwo-dimensional (2D) space has been covered in several published reviews.\nHowever, the special case of 2.5D/3D Digital Elevation Model (DEM) fusion has\nnot been addressed till date. DEM fusion is a key application of data fusion in\nremote sensing. It takes advantage of the complementary characteristics of\nmulti-source DEMs to deliver a more complete, accurate and reliable elevation\ndataset. Although several methods for fusing DEMs have been developed, the\nabsence of a well-rounded review has limited their proliferation among\nresearchers and end-users. It is often required to combine knowledge from\nmultiple studies to inform a holistic perspective and guide further research.\nIn response, this paper provides a systematic review of DEM fusion: the\npre-processing workflow, methods and applications, enhanced with a\nmeta-analysis. Through the discussion and comparative analysis, unresolved\nchallenges and open issues were identified, and future directions for research\nwere proposed. This review is a timely solution and an invaluable source of\ninformation for researchers within the fields of remote sensing and spatial\ninformation science, and the data fusion community at large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okolie_C/0/1/0/all/0/1\">Chukwuma Okolie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smit_J/0/1/0/all/0/1\">Julian Smit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-N-Out Generative Learning for Dense Unsupervised Video Segmentation. (arXiv:2203.15312v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15312","description":"<p>In this paper, we focus on the unsupervised learning for Video Object\nSegmentation (VOS) which learns visual correspondence (i.e., similarity between\npixel-level features) from unlabeled videos. Previous methods are mainly based\non the contrastive learning paradigm, which optimize either in image level or\npixel level. Image-level optimization (e.g., the spatially pooled feature of\nResNet) learns robust high-level semantics but is sub-optimal since the\npixel-level features are optimized implicitly. By contrast, pixel-level\noptimization is more explicit, however, it is sensitive to the visual quality\nof training data and is not robust to object deformation. To complementarily\nperform these two levels of optimization in a unified framework, we propose the\nIn-aNd-Out (INO) generative learning from a purely generative perspective with\nthe help of naturally designed class tokens and patch tokens in Vision\nTransformer (ViT). Specifically, for image-level optimization, we force the\nout-view imagination from local to global views on class tokens, which helps\ncapturing high-level semantics, and we name it as out-generative learning. As\nto pixel-level optimization, we perform in-view masked image modeling on patch\ntokens, which recovers the corrupted parts of an image via inferring its\nfine-grained structure, and we term it as in-generative learning. To better\ndiscover the temporal information, we additionally force the inter-frame\nconsistency from both feature level and affinity matrix level. Extensive\nexperiments on DAVIS-2017 val and YouTube-VOS 2018 val show that our INO\noutperforms previous state-of-the-art methods by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiling Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15331","description":"<p>Currently, many theoretical as well as practically relevant questions towards\nthe transferability and robustness of Convolutional Neural Networks (CNNs)\nremain unsolved. While ongoing research efforts are engaging these problems\nfrom various angles, in most computer vision related cases these approaches can\nbe generalized to investigations of the effects of distribution shifts in image\ndata. In this context, we propose to study the shifts in the learned weights of\ntrained CNN models. Here we focus on the properties of the distributions of\ndominantly used 3x3 convolution filter kernels. We collected and publicly\nprovide a dataset with over 1.4 billion filters from hundreds of trained CNNs,\nusing a wide range of datasets, architectures, and vision tasks. In a first use\ncase of the proposed dataset, we can show highly relevant properties of many\npublicly available pre-trained models for practical applications: I) We analyze\ndistribution shifts (or the lack thereof) between trained filters along\ndifferent axes of meta-parameters, like visual category of the dataset, task,\narchitecture, or layer depth. Based on these results, we conclude that model\npre-training can succeed on arbitrary datasets if they meet size and variance\nconditions. II) We show that many pre-trained models contain degenerated\nfilters which make them less robust and less suitable for fine-tuning on target\napplications.\n</p>\n<p>Data &amp; Project website: https://github.com/paulgavrikov/cnn-filter-db\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gavrikov_P/0/1/0/all/0/1\">Paul Gavrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mc-BEiT: Multi-choice Discretization for Image BERT Pre-training. (arXiv:2203.15371v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15371","description":"<p>Image BERT pre-training with masked image modeling (MIM) becomes a popular\npractice to cope with self-supervised representation learning. A seminal work,\nBEiT, casts MIM as a classification task with a visual vocabulary, tokenizing\nthe continuous visual signals into discrete vision tokens using a pre-learned\ndVAE. Despite a feasible solution, the improper discretization hinders further\nimprovements of image pre-training. Since image discretization has no\nground-truth answers, we believe that the masked patch should not be assigned\nwith a unique token id even if a better tokenizer can be obtained. In this\nwork, we introduce an improved BERT-style image pre-training method, namely\nmc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice\ntraining objectives. Specifically, the multi-choice supervision for the masked\nimage patches is formed by the soft probability vectors of the discrete token\nids, which are predicted by the off-the-shelf image tokenizer and further\nrefined by high-level inter-patch perceptions resorting to the observation that\nsimilar patches should share their choices. Extensive experiments on\nclassification, segmentation, and detection tasks demonstrate the superiority\nof our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning\naccuracy on ImageNet-1K classification, 50.8% mIOU on ADE20K semantic\nsegmentation, 51.2% AP^b and 44.3% AP^m of object detection and instance\nsegmentation on COCO, outperforming the competitive counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zixuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smooth Robust Tensor Completion for Background/Foreground Separation with Missing Pixels: Novel Algorithm with Convergence Guarantee. (arXiv:2203.16328v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16328","description":"<p>The objective of this study is to address the problem of\nbackground/foreground separation with missing pixels by combining the video\nacquisition, video recovery, background/foreground separation into a single\nframework. To achieve this, a smooth robust tensor completion (SRTC) model is\nproposed to recover the data and decompose it into the static background and\nsmooth foreground, respectively. Specifically, the static background is modeled\nby the low-rank tucker decomposition and the smooth foreground (moving objects)\nis modeled by the spatiotemporal continuity, which is enforced by the total\nvariation regularization. An efficient algorithm based on tensor proximal\nalternating minimization (tenPAM) is implemented to solve the proposed model\nwith global convergence guarantee under very mild conditions. Extensive\nexperiments on real data demonstrate that the proposed method significantly\noutperforms the state-of-the-art approaches for background/foreground\nseparation with missing pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bo Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weijun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenyu Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEB+: A benchmark for generic event boundary captioning, grounding and text-based retrieval. (arXiv:2204.00486v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00486","description":"<p>Cognitive science has shown that humans perceive videos in terms of events\nseparated by state changes of dominant subjects. State changes trigger new\nevents and are one of the most useful among the large amount of redundant\ninformation perceived. However, previous research focuses on the overall\nunderstanding of segments without evaluating the fine-grained status changes\ninside. In this paper, we introduce a new dataset called Kinetic-GEBC (Generic\nEvent Boundary Captioning). The dataset consists of over 170k boundaries\nassociated with captions describing status changes in the generic events in 12K\nvideos. Upon this new dataset, we propose three tasks supporting the\ndevelopment of a more fine-grained, robust, and human-like understanding of\nvideos through status changes. We evaluate many representative baselines in our\ndataset, where we also design a new TPD (Temporal-based Pairwise Difference)\nModeling method for current state-of-the-art backbones and achieve significant\nperformance improvements. Besides, the results show there are still formidable\nchallenges for current methods in the utilization of different granularities,\nrepresentation of visual difference, and the accurate localization of status\nchanges. Further analysis shows that our dataset can drive developing more\npowerful methods to understand status changes and thus improve video level\ncomprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How stable are Transferability Metrics evaluations?. (arXiv:2204.01403v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01403","description":"<p>Transferability metrics is a maturing field with increasing interest, which\naims at providing heuristics for selecting the most suitable source models to\ntransfer to a given target dataset, without fine-tuning them all. However,\nexisting works rely on custom experimental setups which differ across papers,\nleading to inconsistent conclusions about which transferability metrics work\nbest. In this paper we conduct a large-scale study by systematically\nconstructing a broad range of 715k experimental setup variations. We discover\nthat even small variations to an experimental setup lead to different\nconclusions about the superiority of a transferability metric over another.\nThen we propose better evaluations by aggregating across many experiments,\nenabling to reach more stable conclusions. As a result, we reveal the\nsuperiority of LogME at selecting good source datasets to transfer from in a\nsemantic segmentation scenario, NLEEP at selecting good source architectures in\nan image classification scenario, and GBC at determining which target task\nbenefits most from a given source model. Yet, no single transferability metric\nworks best in all scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agostinelli_A/0/1/0/all/0/1\">Andrea Agostinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandy_M/0/1/0/all/0/1\">Michal P&#xe1;ndy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uijlings_J/0/1/0/all/0/1\">Jasper Uijlings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1\">Thomas Mensink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSGN++: Exploiting Visual-Spatial Relation for Stereo-based 3D Detectors. (arXiv:2204.03039v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03039","description":"<p>Camera-based 3D object detectors are welcome due to their wider deployment\nand lower price than LiDAR sensors. We revisit the prior stereo modeling DSGN\nabout the stereo volume constructions for representing both 3D geometry and\nsemantics. We polish the stereo modeling and propose our approach, DSGN++,\naiming for improving information flow throughout the 2D-to-3D pipeline in the\nfollowing three main aspects. First, to effectively lift the 2D information to\nstereo volume, we propose depth-wise plane sweeping (DPS) that allows denser\nconnections and extracts depth-guided features. Second, for better grasping\ndifferently spaced features, we present a novel stereo volume -- Dual-view\nStereo Volume (DSV) that integrates front-view and top-view features and\nreconstructs sub-voxel depth in the camera frustum. Third, as the foreground\nregion becomes less dominant in 3D space, we firstly propose a multi-modal data\nediting strategy -- Stereo-LiDAR Copy-Paste, which ensures cross-modal\nalignment and improves data efficiency. Without bells and whistles, extensive\nexperiments in various modality setups on the popular KITTI benchmark show that\nour method consistently outperforms other camera-based 3D detectors for all\ncategories. Code will be released at https://github.com/chenyilun95/DSGN2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Prototype Prompt-tuning with Pre-trained Representation for Class Incremental Learning. (arXiv:2204.03410v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03410","description":"<p>Class incremental learning has attracted much attention, but most existing\nworks still continually fine-tune the representation model, resulting in much\ncatastrophic forgetting. Instead of struggling to fight against such forgetting\nby replaying or distillation like most of the existing methods, we take the\npre-train-and-prompt-tuning paradigm to sequentially learn new visual concepts\nbased on a fixed semantic rich pre-trained representation model by incremental\nprototype prompt-tuning (IPP), which substantially reduces the catastrophic\nforgetting. In addition, an example prototype classification is proposed to\ncompensate for semantic drift, the problem caused by learning bias at different\nphases. Extensive experiments conducted on the three incremental learning\nbenchmarks demonstrate that our method consistently outperforms other\nstate-of-the-art methods with a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jieren Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianhua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haojian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunkuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effects of Regularization and Data Augmentation are Class Dependent. (arXiv:2204.03632v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.03632","description":"<p>Regularization is a fundamental technique to prevent over-fitting and to\nimprove generalization performances by constraining a model's complexity.\nCurrent Deep Networks heavily rely on regularizers such as Data-Augmentation\n(DA) or weight-decay, and employ structural risk minimization, i.e.\ncross-validation, to select the optimal regularization hyper-parameters. In\nthis study, we demonstrate that techniques such as DA or weight decay produce a\nmodel with a reduced complexity that is unfair across classes. The optimal\namount of DA or weight decay found from cross-validation leads to disastrous\nmodel performances on some classes e.g. on Imagenet with a resnet50, the \"barn\nspider\" classification test accuracy falls from $68\\%$ to $46\\%$ only by\nintroducing random crop DA during training. Even more surprising, such\nperformance drop also appears when introducing uninformative regularization\ntechniques such as weight decay. Those results demonstrate that our search for\never increasing generalization performance -- averaged over all classes and\nsamples -- has left us with models and regularizers that silently sacrifice\nperformances on some classes. This scenario can become dangerous when deploying\na model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on\nINaturalist sees its performances fall from $70\\%$ to $30\\%$ on class \\#8889\nwhen introducing random crop DA during the Imagenet pre-training phase. Those\nresults demonstrate that designing novel regularizers without class-dependent\nbias remains an open research question.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bottou_L/0/1/0/all/0/1\">Leon Bottou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAD-3DHeads: A Large-scale Dense, Accurate and Diverse Dataset for 3D Head Alignment from a Single Image. (arXiv:2204.03688v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03688","description":"<p>We present DAD-3DHeads, a dense and diverse large-scale dataset, and a robust\nmodel for 3D Dense Head Alignment in the wild. It contains annotations of over\n3.5K landmarks that accurately represent 3D head shape compared to the\nground-truth scans. The data-driven model, DAD-3DNet, trained on our dataset,\nlearns shape, expression, and pose parameters, and performs 3D reconstruction\nof a FLAME mesh. The model also incorporates a landmark prediction branch to\ntake advantage of rich supervision and co-training of multiple related tasks.\nExperimentally, DAD-3DNet outperforms or is comparable to the state-of-the-art\nmodels in (i) 3D Head Pose Estimation on AFLW2000-3D and BIWI, (ii) 3D Face\nShape Reconstruction on NoW and Feng, and (iii) 3D Dense Head Alignment and 3D\nLandmarks Estimation on DAD-3DHeads dataset. Finally, the diversity of\nDAD-3DHeads in camera angles, facial expressions, and occlusions enables a\nbenchmark to study in-the-wild generalization and robustness to distribution\nshifts. The dataset webpage is https://p.farm/research/dad-3dheads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martyniuk_T/0/1/0/all/0/1\">Tetiana Martyniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupyn_O/0/1/0/all/0/1\">Orest Kupyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurlyak_Y/0/1/0/all/0/1\">Yana Kurlyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krashenyi_I/0/1/0/all/0/1\">Igor Krashenyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Ji&#x159;i Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharmanska_V/0/1/0/all/0/1\">Viktoriia Sharmanska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}