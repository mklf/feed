{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Text Simplification for Comprehension-based Question-Answering. (arXiv:2109.13984v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13984","description":"<p>Text simplification is the process of splitting and rephrasing a sentence to\na sequence of sentences making it easier to read and understand while\npreserving the content and approximating the original meaning. Text\nsimplification has been exploited in NLP applications like machine translation,\nsummarization, semantic role labeling, and information extraction, opening a\nbroad avenue for its exploitation in comprehension-based question-answering\ndownstream tasks. In this work, we investigate the effect of text\nsimplification in the task of question-answering using a comprehension context.\nWe release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset.\n</p>\n<p>Firstly, we outline each step in the dataset creation pipeline, including\nstyle transfer, thresholding of sentences showing correct transfer, and offset\nfinding for each answer. Secondly, we verify the quality of the transferred\nsentences through various methodologies involving both automated and human\nevaluation. Thirdly, we benchmark the newly created corpus and perform an\nablation study for examining the effect of the simplification process in the\nSQuAD-based question answering task. Our experiments show that simplification\nleads to up to 2.04% and 1.74% increase in Exact Match and F1, respectively.\nFinally, we conclude with an analysis of the transfer process, investigating\nthe types of edits made by the model, and the effect of sentence length on the\ntransfer model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dadu_T/0/1/0/all/0/1\">Tanvi Dadu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pant_K/0/1/0/all/0/1\">Kartikey Pant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagar_S/0/1/0/all/0/1\">Seema Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbhuiya_F/0/1/0/all/0/1\">Ferdous Ahmed Barbhuiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_K/0/1/0/all/0/1\">Kuntal Dey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations. (arXiv:2109.14017v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14017","description":"<p>Recent research has adopted a new experimental field centered around the\nconcept of text perturbations which has revealed that shuffled word order has\nlittle to no impact on the downstream performance of Transformer-based language\nmodels across many NLP tasks. These findings contradict the common\nunderstanding of how the models encode hierarchical and structural information\nand even question if the word order is modeled with position embeddings. To\nthis end, this paper proposes nine probing datasets organized by the type of\n\\emph{controllable} text perturbation for three Indo-European languages with a\nvarying degree of word order flexibility: English, Swedish and Russian. Based\non the probing analysis of the M-BERT and M-BART models, we report that the\nsyntactic sensitivity depends on the language and model pre-training\nobjectives. We also find that the sensitivity grows across layers together with\nthe increase of the perturbation granularity. Last but not least, we show that\nthe models barely use the positional information to induce syntactic trees from\ntheir intermediate self-attention and contextualized representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taktasheva_E/0/1/0/all/0/1\">Ekaterina Taktasheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marked Attribute Bias in Natural Language Inference. (arXiv:2109.14039v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14039","description":"<p>Reporting and providing test sets for harmful bias in NLP applications is\nessential for building a robust understanding of the current problem. We\npresent a new observation of gender bias in a downstream NLP application:\nmarked attribute bias in natural language inference. Bias in downstream\napplications can stem from training data, word embeddings, or be amplified by\nthe model in use. However, focusing on biased word embeddings is potentially\nthe most impactful first step due to their universal nature. Here we seek to\nunderstand how the intrinsic properties of word embeddings contribute to this\nobserved marked attribute effect, and whether current post-processing methods\naddress the bias successfully. An investigation of the current debiasing\nlandscape reveals two open problems: none of the current debiased embeddings\nmitigate the marked attribute error, and none of the intrinsic bias measures\nare predictive of the marked attribute effect. By noticing that a new type of\nintrinsic bias measure correlates meaningfully with the marked attribute\neffect, we propose a new postprocessing debiasing scheme for static word\nembeddings. The proposed method applied to existing embeddings achieves new\nbest results on the marked attribute bias test set. See\nhttps://github.com/hillary-dawkins/MAB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dawkins_H/0/1/0/all/0/1\">Hillary Dawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution. (arXiv:2109.14047v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14047","description":"<p>We observe an instance of gender-induced bias in a downstream application,\ndespite the absence of explicit gender words in the test cases. We provide a\ntest set, SoWinoBias, for the purpose of measuring such latent gender bias in\ncoreference resolution systems. We evaluate the performance of current\ndebiasing methods on the SoWinoBias test set, especially in reference to the\nmethod's design and altered embedding space properties. See\nhttps://github.com/hillarydawkins/SoWinoBias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dawkins_H/0/1/0/all/0/1\">Hillary Dawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Summaries for Scientific Paper Review. (arXiv:2109.14059v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14059","description":"<p>The review process is essential to ensure the quality of publications.\nRecently, the increase of submissions for top venues in machine learning and\nNLP has caused a problem of excessive burden on reviewers and has often caused\nconcerns regarding how this may not only overload reviewers, but also may\naffect the quality of the reviews. An automatic system for assisting with the\nreviewing process could be a solution for ameliorating the problem. In this\npaper, we explore automatic review summary generation for scientific papers. We\nposit that neural language models have the potential to be valuable candidates\nfor this task. In order to test this hypothesis, we release a new dataset of\nscientific papers and their reviews, collected from papers published in the\nNeurIPS conference from 2013 to 2020. We evaluate state of the art neural\nsummarization models, present initial results on the feasibility of automatic\nreview summary generation, and propose directions for the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uban_A/0/1/0/all/0/1\">Ana Sabina Uban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAFT: A Real-World Few-Shot Text Classification Benchmark. (arXiv:2109.14076v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14076","description":"<p>Large pre-trained language models have shown promise for few-shot learning,\ncompleting text-based tasks given only a few task-specific examples. Will\nmodels soon solve classification tasks that have so far been reserved for human\nresearch assistants? Existing benchmarks are not designed to measure progress\nin applied settings, and so don't directly answer this question. The RAFT\nbenchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring\ntasks and uses an evaluation setup that mirrors deployment. Baseline\nevaluations on RAFT reveal areas current techniques struggle with: reasoning\nover long texts and tasks with many classes. Human baselines show that some\nclassification tasks are difficult for non-expert humans, reflecting that\nreal-world value sometimes depends on domain expertise. Yet even non-expert\nhuman baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets\nand leaderboard will track which model improvements translate into real-world\nbenefits at https://raft.elicit.org .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alex_N/0/1/0/all/0/1\">Neel Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lifland_E/0/1/0/all/0/1\">Eli Lifland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhishek Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maham_P/0/1/0/all/0/1\">Pegah Maham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_C/0/1/0/all/0/1\">C. Jess Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hine_E/0/1/0/all/0/1\">Emmie Hine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashurst_C/0/1/0/all/0/1\">Carolyn Ashurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedille_P/0/1/0/all/0/1\">Paul Sedille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlier_A/0/1/0/all/0/1\">Alexis Carlier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noetel_M/0/1/0/all/0/1\">Michael Noetel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuhlmuller_A/0/1/0/all/0/1\">Andreas Stuhlm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. (arXiv:2109.14084v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14084","description":"<p>We present VideoCLIP, a contrastive approach to pre-train a unified model for\nzero-shot video and text understanding, without using any labels on downstream\ntasks. VideoCLIP trains a transformer for video and text by contrasting\ntemporally overlapping positive video-text pairs with hard negatives from\nnearest neighbor retrieval. Our experiments on a diverse series of downstream\ntasks, including sequence-level text-video retrieval, VideoQA, token-level\naction localization, and action segmentation reveal state-of-the-art\nperformance, surpassing prior work, and in some cases even outperforming\nsupervised approaches. Code is made available at\nhttps://github.com/pytorch/fairseq/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_F/0/1/0/all/0/1\">Florian Metze Luke Zettlemoyer Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Video-Language Segmentation. (arXiv:2109.14131v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14131","description":"<p>We focus on the problem of segmenting a certain object referred by a natural\nlanguage sentence in video content, at the core of formulating a pinpoint\nvision-language relation. While existing attempts mainly construct such\nrelation in an implicit way, i.e., grid-level multi-modal feature fusion, it\nhas been proven problematic to distinguish semantically similar objects under\nthis paradigm. In this work, we propose to interwind the visual and linguistic\nmodalities in an explicit way via the contrastive learning objective, which\ndirectly aligns the referred object and the language description and separates\nthe unreferred content apart across frames. Moreover, to remedy for the\ndegradation problem, we present two complementary hard instance mining\nstrategies, i.e., Language-relevant Channel Filter and Relative Hard Instance\nConstruction. They encourage the network to exclude visual-distinguishable\nfeature and to focus on easy-confused objects during the contrastive training.\nExtensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB\nSentences, quantitatively demonstrate the state-of-the-arts performance of our\nmethod and qualitatively show the more accurate distinguishment between\nsemantically similar objects over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Dialogue State Tracking by Joint Slot Modeling. (arXiv:2109.14144v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14144","description":"<p>Dialogue state tracking models play an important role in a task-oriented\ndialogue system. However, most of them model the slot types conditionally\nindependently given the input. We discover that it may cause the model to be\nconfused by slot types that share the same data type. To mitigate this issue,\nwe propose TripPy-MRF and TripPy-LSTM that models the slots jointly. Our\nresults show that they are able to alleviate the confusion mentioned above, and\nthey push the state-of-the-art on dataset MultiWoZ 2.1 from 58.7 to 61.3. Our\nimplementation is available at https://github.com/CTinRay/Trippy-Joint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1\">Ting-Rui Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Arabic Diacritization by Learning to Diacritize and Translate. (arXiv:2109.14150v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14150","description":"<p>We propose a novel multitask learning method for diacritization which trains\na model to both diacritize and translate. Our method addresses data sparsity by\nexploiting large, readily available bitext corpora. Furthermore, translation\nrequires implicit linguistic and semantic knowledge, which is helpful for\nresolving ambiguities in the diacritization task. We apply our method to the\nPenn Arabic Treebank and report a new state-of-the-art word error rate of\n4.79%. We also conduct manual and automatic analysis to better understand our\nmethod and highlight some of the remaining challenges in diacritization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">Brian Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshehri_A/0/1/0/all/0/1\">Ali Alshehri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflexivity in Issues of Scale and Representation in a Digital Humanities Project. (arXiv:2109.14184v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14184","description":"<p>In this paper, we explore issues that we have encountered in developing a\npipeline that combines natural language processing with data analysis and\nvisualization techniques. The characteristics of the corpus - being comprised\nof diaries of a single person spanning several decades - present both\nconceptual challenges in terms of issues of representation, and affordances as\na source for historical research. We consider these issues in a team context\nwith a particular focus on the generation and interpretation of visualizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Annie T. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_C/0/1/0/all/0/1\">Camille Lyans Cole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context based Roman-Urdu to Urdu Script Transliteration System. (arXiv:2109.14197v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14197","description":"<p>Now a day computer is necessary for human being and it is very useful in many\nfields like search engine, text processing, short messaging services, voice\nchatting and text recognition. Since last many years there are many tools and\ntechniques that have been developed to support the writing of language script.\nMost of the Asian languages like Arabic, Urdu, Persian, Chains and Korean are\nwritten in Roman alphabets. Roman alphabets are the most commonly used for\ntransliteration of languages, which have non-Latin scripts. For writing Urdu\ncharacters as an input, there are many layouts which are already exist. Mostly\nUrdu speaker prefer to use Roman-Urdu for different applications, because\nmostly user is not familiar with Urdu language keyboard. The objective of this\nwork is to improve the context base transliteration of Roman-Urdu to Urdu\nscript. In this paper, we propose an algorithm which effectively solve the\ntransliteration issues. The algorithm work like, convert the encoding roman\nwords into the words in the standard Urdu script and match it with the lexicon.\nIf match found, then display the word in the text editor. The highest frequency\nwords are displayed if more than one match found in the lexicon. Display the\nfirst encoded and converted instance and set it to the default if there is not\na single instance of the match is found and then adjust the given ambiguous\nword to their desire location according to their context. The outcome of this\nalgorithm proved the efficiency and significance as compare to other models and\nalgorithms which work for transliteration of Raman-Urdu to Urdu on context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shakeel_H/0/1/0/all/0/1\">H Muhammad Shakeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Rashid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waheed_M/0/1/0/all/0/1\">Muhammad Waheed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who says like a style of Vitamin: Towards Syntax-Aware DialogueSummarization using Multi-task Learning. (arXiv:2109.14199v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14199","description":"<p>Abstractive dialogue summarization is a challenging task for several reasons.\nFirst, most of the important pieces of information in a conversation are\nscattered across utterances through multi-party interactions with different\ntextual styles. Second, dialogues are often informal structures, wherein\ndifferent individuals express personal perspectives, unlike text summarization,\ntasks that usually target formal documents such as news articles. To address\nthese issues, we focused on the association between utterances from individual\nspeakers and unique syntactic structures. Speakers have unique textual styles\nthat can contain linguistic information, such as voiceprint. Therefore, we\nconstructed a syntax-aware model by leveraging linguistic information (i.e.,\nPOS tagging), which alleviates the above issues by inherently distinguishing\nsentences uttered from individual speakers. We employed multi-task learning of\nboth syntax-aware information and dialogue summarization. To the best of our\nknowledge, our approach is the first method to apply multi-task learning to the\ndialogue summarization task. Experiments on a SAMSum corpus (a large-scale\ndialogue summarization corpus) demonstrated that our method improved upon the\nvanilla model. We further analyze the costs and benefits of our approach\nrelative to baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kisu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation. (arXiv:2109.14200v1 [eess.AS])","link":"http://arxiv.org/abs/2109.14200","description":"<p>Decades of research has studied how language learning infants learn to\ndiscriminate speech sounds, segment words, and associate words with their\nmeanings. While gradual development of such capabilities is unquestionable, the\nexact nature of these skills and the underlying mental representations yet\nremains unclear. In parallel, computational studies have shown that basic\ncomprehension of speech can be achieved by statistical learning between speech\nand concurrent referentially ambiguous visual input. These models can operate\nwithout prior linguistic knowledge such as representations of linguistic units,\nand without learning mechanisms specifically targeted at such units. This has\nraised the question of to what extent knowledge of linguistic units, such as\nphone(me)s, syllables, and words, could actually emerge as latent\nrepresentations supporting the translation between speech and representations\nin other modalities, and without the units being proximal learning targets for\nthe learner. In this study, we formulate this idea as the so-called latent\nlanguage hypothesis (LLH), connecting linguistic representation learning to\ngeneral predictive processing within and across sensory modalities. We review\nthe extent that the audiovisual aspect of LLH is supported by the existing\ncomputational studies. We then explore LLH further in extensive learning\nsimulations with different neural network models for audiovisual\ncross-situational learning, and comparing learning from both synthetic and real\nspeech data. We investigate whether the latent representations learned by the\nnetworks reflect phonetic, syllabic, or lexical structure of input speech by\nutilizing an array of complementary evaluation metrics related to linguistic\nselectivity and temporal characteristics of the representations. As a result,\nwe find that representations associated...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khorrami_K/0/1/0/all/0/1\">Khazar Khorrami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLEU, METEOR, BERTScore: Evaluation of Metrics Performance in Assessing Critical Translation Errors in Sentiment-oriented Text. (arXiv:2109.14250v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14250","description":"<p>Social media companies as well as authorities make extensive use of\nartificial intelligence (AI) tools to monitor postings of hate speech,\ncelebrations of violence or profanity. Since AI software requires massive\nvolumes of data to train computers, Machine Translation (MT) of the online\ncontent is commonly used to process posts written in several languages and\nhence augment the data needed for training. However, MT mistakes are a regular\noccurrence when translating sentiment-oriented user-generated content (UGC),\nespecially when a low-resource language is involved. The adequacy of the whole\nprocess relies on the assumption that the evaluation metrics used give a\nreliable indication of the quality of the translation. In this paper, we assess\nthe ability of automatic quality metrics to detect critical machine translation\nerrors which can cause serious misunderstanding of the affect message. We\ncompare the performance of three canonical metrics on meaningless translations\nwhere the semantic content is seriously impaired as compared to meaningful\ntranslations with a critical error which exclusively distorts the sentiment of\nthe source text. We conclude that there is a need for fine-tuning of automatic\nmetrics to make them more robust in detecting sentiment critical errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Orasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Character Tagger for Short Text Spelling Error Correction. (arXiv:2109.14259v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14259","description":"<p>State-of-the-art approaches to spelling error correction problem include\nTransformer-based Seq2Seq models, which require large training sets and suffer\nfrom slow inference time; and sequence labeling models based on Transformer\nencoders like BERT, which involve token-level label space and therefore a large\npre-defined vocabulary dictionary. In this paper we present a Hierarchical\nCharacter Tagger model, or HCTagger, for short text spelling error correction.\nWe use a pre-trained language model at the character level as a text encoder,\nand then predict character-level edits to transform the original text into its\nerror-free form with a much smaller label space. For decoding, we propose a\nhierarchical multi-task approach to alleviate the issue of long-tail label\ndistribution without introducing extra model parameters. Experiments on two\npublic misspelling correction datasets demonstrate that HCTagger is an accurate\nand much faster approach than many existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mengyi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Call Larisa Ivanovna: Code-Switching Fools Multilingual NLU Models. (arXiv:2109.14350v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14350","description":"<p>Practical needs of developing task-oriented dialogue assistants require the\nability to understand many languages. Novel benchmarks for multilingual natural\nlanguage understanding (NLU) include monolingual sentences in several\nlanguages, annotated with intents and slots. In such setup models for\ncross-lingual transfer show remarkable performance in joint intent recognition\nand slot filling. However, existing benchmarks lack of code-switched\nutterances, which are difficult to gather and label due to complexity in the\ngrammatical structure. The evaluation of NLU models seems biased and limited,\nsince code-switching is being left out of scope.\n</p>\n<p>Our work adopts recognized methods to generate plausible and\nnaturally-sounding code-switched utterances and uses them to create a synthetic\ncode-switched test set. Based on experiments, we report that the\nstate-of-the-art NLU models are unable to handle code-switching. At worst, the\nperformance, evaluated by semantic accuracy, drops as low as 15\\% from 80\\%\nacross languages. Further we show, that pre-training on synthetic code-mixed\ndata helps to maintain performance on the proposed test set at a comparable\nlevel with monolingual data. Finally, we analyze different language pairs and\nshow that the closer the languages are, the better the NLU model handles their\nalternation. This is in line with the common understanding of how multilingual\nmodels conduct transferring between languages\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Birshert_A/0/1/0/all/0/1\">Alexey Birshert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Fact Linking. (arXiv:2109.14364v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14364","description":"<p>Knowledge-intensive NLP tasks can benefit from linking natural language text\nwith facts from a Knowledge Graph (KG). Although facts themselves are\nlanguage-agnostic, the fact labels (i.e., language-specific representation of\nthe fact) in the KG are often present only in a few languages. This makes it\nchallenging to link KG facts to sentences in languages other than the limited\nset of languages. To address this problem, we introduce the task of\nMultilingual Fact Linking (MFL) where the goal is to link fact expressed in a\nsentence to corresponding fact in the KG, even when the fact label in the KG is\nnot available in the language of the sentence. To facilitate research in this\narea, we present a new evaluation dataset, IndicLink. This dataset contains\n11,293 linked WikiData facts and 6,429 sentences spanning English and six\nIndian languages. We propose a Retrieval+Generation model, ReFCoG, that can\nscale to millions of KG facts by combining Dual Encoder based retrieval with a\nSeq2Seq based generation model which is constrained to output only valid KG\nfacts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in\nPrecision@1. In spite of this gain, the model achieves an overall score of\n52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink\ndata are available at https://github.com/SaiKeshav/mfl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolluru_K/0/1/0/all/0/1\">Keshav Kolluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezk_M/0/1/0/all/0/1\">Martin Rezk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdinSaar@WMT21: North-Germanic Low-Resource Multilingual NMT. (arXiv:2109.14368v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14368","description":"<p>We describe the EdinSaar submission to the shared task of Multilingual\nLow-Resource Translation for North Germanic Languages at the Sixth Conference\non Machine Translation (WMT2021). We submit multilingual translation models for\ntranslations to/from Icelandic (is), Norwegian-Bokmal (nb), and Swedish (sv).\nWe employ various experimental approaches, including multilingual pre-training,\nback-translation, fine-tuning, and ensembling. In most translation directions,\nour models outperform other submitted systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tchistiakova_S/0/1/0/all/0/1\">Svetlana Tchistiakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_K/0/1/0/all/0/1\">Koel Dutta Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sourav Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDGAR-CORPUS: Billions of Tokens Make The World Go Round. (arXiv:2109.14394v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14394","description":"<p>We release EDGAR-CORPUS, a novel corpus comprising annual reports from all\nthe publicly traded companies in the US spanning a period of more than 25\nyears. To the best of our knowledge, EDGAR-CORPUSis the largest financial NLP\ncorpus available to date. All the reports are downloaded, split into their\ncorresponding items (sections), and provided in a clean, easy-to-use JSON\nformat. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC\nembeddings for the financial domain. We employ these embeddings in a battery of\nfinancial NLP tasks and showcase their superiority over generic GloVe\nembeddings and other existing financial word embeddings. We also open-source\nEDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future\nannual reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_L/0/1/0/all/0/1\">Lefteris Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malakasiotis_P/0/1/0/all/0/1\">Prodromos Malakasiotis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StoryDB: Broad Multi-language Narrative Dataset. (arXiv:2109.14396v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14396","description":"<p>This paper presents StoryDB - a broad multi-language dataset of narratives.\nStoryDB is a corpus of texts that includes stories in 42 different languages.\nEvery language includes 500+ stories. Some of the languages include more than\n20 000 stories. Every story is indexed across languages and labeled with tags\nsuch as a genre or a topic. The corpus shows rich topical and language\nvariation and can serve as a resource for the study of the role of narrative in\nnatural language processing across various languages including low resource\nones. We also demonstrate how the dataset could be used to benchmark three\nmodern multilanguage models, namely, mDistillBERT, mBERT, and XLM-RoBERTa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samenko_I/0/1/0/all/0/1\">Igor Samenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiQUE: Biquaternionic Embeddings of Knowledge Graphs. (arXiv:2109.14401v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14401","description":"<p>Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge\ngraphs (KGs). Existing KGE models rely on geometric operations to model\nrelational patterns. Euclidean (circular) rotation is useful for modeling\npatterns such as symmetry, but cannot represent hierarchical semantics. In\ncontrast, hyperbolic models are effective at modeling hierarchical relations,\nbut do not perform as well on patterns on which circular rotation excels. It is\ncrucial for KGE models to unify multiple geometric transformations so as to\nfully cover the multifarious relations in KGs. To do so, we propose BiQUE, a\nnovel model that employs biquaternions to integrate multiple geometric\ntransformations, viz., scaling, translation, Euclidean rotation, and hyperbolic\nrotation. BiQUE makes the best trade-offs among geometric operators during\ntraining, picking the best one (or their best combination) for each relation.\nExperiments on five datasets show BiQUE's effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kok_S/0/1/0/all/0/1\">Stanley Kok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14420","description":"<p>Error correction is widely used in automatic speech recognition (ASR) to\npost-process the generated sentence, and can further reduce the word error rate\n(WER). Although multiple candidates are generated by an ASR system through beam\nsearch, current error correction approaches can only correct one sentence at a\ntime, failing to leverage the voting effect from multiple candidates to better\ndetect and correct error tokens. In this work, we propose FastCorrect 2, an\nerror correction model that takes multiple ASR candidates as input for better\ncorrection accuracy. FastCorrect 2 adopts non-autoregressive generation for\nfast inference, which consists of an encoder that processes multiple source\nsentences and a decoder that generates the target sentence in parallel from the\nadjusted source sentence, where the adjustment is based on the predicted\nduration of each source token. However, there are some issues when handling\nmultiple source sentences. First, it is non-trivial to leverage the voting\neffect from multiple source sentences since they usually vary in length. Thus,\nwe propose a novel alignment algorithm to maximize the degree of token\nalignment among multiple sentences in terms of token and pronunciation\nsimilarity. Second, the decoder can only take one adjusted source sentence as\ninput, while there are multiple source sentences. Thus, we develop a candidate\npredictor to detect the most suitable candidate for the decoder. Experiments on\nour inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce\nthe WER over the previous correction model with single candidate by 3.2% and\n2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR\nerror correction. FastCorrect 2 achieves better performance than the cascaded\nre-scoring and correction pipeline and can serve as a unified post-processing\nmodule for ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the Arabic Sentiment Analysis 2021 Competition at KAUST. (arXiv:2109.14456v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14456","description":"<p>This paper provides an overview of the Arabic Sentiment Analysis Challenge\norganized by King Abdullah University of Science and Technology (KAUST). The\ntask in this challenge is to develop machine learning models to classify a\ngiven tweet into one of the three categories Positive, Negative, or Neutral.\nFrom our recently released ASAD dataset, we provide the competitors with 55K\ntweets for training, and 20K tweets for validation, based on which the\nperformance of participating teams are ranked on a leaderboard,\nhttps://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust. The competition\nreceived in total 1247 submissions from 74 teams (99 team members). The final\nwinners are determined by another private set of 20K tweets that have the same\ndistribution as the training and validation set. In this paper, we present the\nmain findings in the competition and summarize the methods and tools used by\nthe top ranked teams. The full dataset of 100K labeled tweets is also released\nfor public usage, at\nhttps://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust/data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alamro_H/0/1/0/all/0/1\">Hind Alamro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshehri_M/0/1/0/all/0/1\">Manal Alshehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alharbi_B/0/1/0/all/0/1\">Basma Alharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khayyat_Z/0/1/0/all/0/1\">Zuhair Khayyat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalkatawi_M/0/1/0/all/0/1\">Manal Kalkatawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaber_I/0/1/0/all/0/1\">Inji Ibrahim Jaber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition. (arXiv:2004.10663v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.10663","description":"<p>We present a fast and scalable architecture called Explicit Modular\nDecomposition (EMD), in which we incorporate both classification-based and\nextraction-based methods and design four modules (for classification and\nsequence labelling) to jointly extract dialogue states. Experimental results\nbased on the MultiWoz 2.0 dataset validates the superiority of our proposed\nmodel in terms of both complexity and scalability when compared to the\nstate-of-the-art methods, especially in the scenario of multi-domain dialogues\nentangled with many turns of utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems. (arXiv:2010.05740v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.05740","description":"<p>Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue\nsystems is challenging, since it requires to properly represent the entity of\nKB, which is associated with its KB context and dialogue context. The existing\nworks represent the entity with only perceiving a part of its KB context, which\ncan lead to the less effective representation due to the information loss, and\nadversely favor KB reasoning and response generation. To tackle this issue, we\nexplore to fully contextualize the entity representation by dynamically\nperceiving all the relevant entities} and dialogue history. To achieve this, we\npropose a COntext-aware Memory Enhanced Transformer framework (COMET), which\ntreats the KB as a sequence and leverages a novel Memory Mask to enforce the\nentity to only focus on its relevant entities and dialogue history, while\navoiding the distraction from the irrelevant entities. Through extensive\nexperiments, we show that our COMET framework can achieve superior performance\nover the state of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1\">Yanjie Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunxu Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence Patterns for Explaining Information Flow in BERT. (arXiv:2011.00740v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.00740","description":"<p>While attention is all you need may be proving true, we do not know why:\nattention-based transformer models such as BERT are superior but how\ninformation flows from input tokens to output predictions are unclear. We\nintroduce influence patterns, abstractions of sets of paths through a\ntransformer model. Patterns quantify and localize the flow of information to\npaths passing through a sequence of model nodes. Experimentally, we find that\nsignificant portion of information flow in BERT goes through skip connections\ninstead of attention heads. We further show that consistency of patterns across\ninstances is an indicator of BERT's performance. Finally, We demonstrate that\npatterns account for far more model performance than previous attention-based\nand layer-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kaiji Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardziel_P/0/1/0/all/0/1\">Piotr Mardziel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Anupam Datta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing and Recomposing Event Structure. (arXiv:2103.10387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.10387","description":"<p>We present an event structure classification empirically derived from\ninferential properties annotated on sentence- and document-level Universal\nDecompositional Semantics (UDS) graphs. We induce this classification jointly\nwith semantic role, entity, and event-event relation classifications using a\ndocument-level generative model structured by these graphs. To support this\ninduction, we augment existing annotations found in the UDS1.0 dataset, which\ncovers the entirety of the English Web Treebank, with an array of inferential\nproperties capturing fine-grained aspects of the temporal and aspectual\nstructure of events. The resulting dataset (available at decomp.io) is the\nlargest annotation of event structure and (partial) event coreference to date.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1\">William Gantt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_L/0/1/0/all/0/1\">Lelia Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09996","description":"<p>We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training. Code is\nmade available at https://github.com/pytorch/fairseq/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1\">Prahal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1\">Masoumeh Aminzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Various Tokenizers for Arabic Text Classification. (arXiv:2106.07540v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07540","description":"<p>The first step in any NLP pipeline is to split the text into individual\ntokens. The most obvious and straightforward approach is to use words as\ntokens. However, given a large text corpus, representing all the words is not\nefficient in terms of vocabulary size. In the literature, many tokenization\nalgorithms have emerged to tackle this problem by creating subwords which in\nturn limits the vocabulary size in a given text corpus. Most tokenization\ntechniques are language-agnostic i.e they don't incorporate the linguistic\nfeatures of a given language. Not to mention the difficulty of evaluating such\ntechniques in practice. In this paper, we introduce three new tokenization\nalgorithms for Arabic and compare them to three other baselines using\nunsupervised evaluations. In addition to that, we compare all the six\nalgorithms by evaluating them on three supervised classification tasks which\nare sentiment analysis, news classification and poetry classification using six\npublicly available datasets. Our experiments show that none of the tokenization\ntechnique is the best choice overall and that the performance of a given\ntokenization algorithm depends on the size of the dataset, type of the task,\nand the amount of morphology that exists in the dataset. However, some\ntokenization techniques are better overall as compared to others on various\ntext classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_shaibani_M/0/1/0/all/0/1\">Maged S. Al-shaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaleb_M/0/1/0/all/0/1\">Mustafa Ghaleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Irfan Ahmad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark. (arXiv:2107.07498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07498","description":"<p>Pretrained Language Models (PLMs) have achieved tremendous success in natural\nlanguage understanding tasks. While different learning schemes -- fine-tuning,\nzero-shot, and few-shot learning -- have been widely explored and compared for\nlanguages such as English, there is comparatively little work in Chinese to\nfairly and comprehensively evaluate and compare these methods and thus hinders\ncumulative progress. In this paper, we introduce the Chinese Few-shot Learning\nEvaluation Benchmark (FewCLUE), the first comprehensive few-shot evaluation\nbenchmark in Chinese. It includes nine tasks, ranging from single-sentence and\nsentence-pair classification tasks to machine reading comprehension tasks. We\nsystematically evaluate five state-of-the-art (SOTA) few-shot learning methods\n(including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their\nperformance with fine-tuning and zero-shot learning schemes on the newly\nconstructed FewCLUE benchmark. Experimental results reveal that: 1) The effect\nof different few-shot learning methods is sensitive to the pre-trained model to\nwhich the methods are applied; 2) PET and P-tuning achieve the best overall\nperformance with RoBERTa and ERNIE respectively. Our benchmark is used in the\nfew-shot learning contest of NLPCC 2021. In addition, we provide a\nuser-friendly toolkit, as well as an online leaderboard to help facilitate\nfurther progress on Chinese few-shot learning. We provide a baseline\nperformance on different learning methods, a reference for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaojing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenyang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hai_H/0/1/0/all/0/1\">Hu Hai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Vaccine and Social Media: Exploring Emotions and Discussions on Twitter. (arXiv:2108.04816v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2108.04816","description":"<p>The understanding of the public response to COVID-19 vaccines is the key\nsuccess factor to control the COVID-19 pandemic. To understand the public\nresponse, there is a need to explore public opinion. Traditional surveys are\nexpensive and time-consuming, address limited health topics, and obtain\nsmall-scale data. Twitter can provide a great opportunity to understand public\nopinion regarding COVID-19 vaccines. The current study proposes an approach\nusing computational and human coding methods to collect and analyze a large\nnumber of tweets to provide a wider perspective on the COVID-19 vaccine. This\nstudy identifies the sentiment of tweets using a machine learning rule-based\napproach, discovers major topics, explores temporal trend and compares topics\nof negative and non-negative tweets using statistical tests, and discloses top\ntopics of tweets having negative and non-negative sentiment. Our findings show\nthat the negative sentiment regarding the COVID-19 vaccine had a decreasing\ntrend between November 2020 and February 2021. We found Twitter users have\ndiscussed a wide range of topics from vaccination sites to the 2020 U.S.\nelection between November 2020 and February 2021. The findings show that there\nwas a significant difference between tweets having negative and non-negative\nsentiment regarding the weight of most topics. Our results also indicate that\nthe negative and non-negative tweets had different topic priorities and\nfocuses. This research illustrates that Twitter data can be used to explore\npublic opinion regarding the COVID-19 vaccine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1\">Amir Karami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Michael Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1\">Bailey Goldschmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1\">Hannah R. Boyajieff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafabadi_M/0/1/0/all/0/1\">Mahdi M. Najafabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Adversarial Fine-Tuning Benefit BERT?. (arXiv:2108.13602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13602","description":"<p>Adversarial training (AT) is one of the most reliable methods for defending\nagainst adversarial attacks in machine learning. Variants of this method have\nbeen used as regularization mechanisms to achieve SOTA results on NLP\nbenchmarks, and they have been found to be useful for transfer learning and\ncontinual learning. We search for the reasons for the effectiveness of AT by\ncontrasting vanilla and adversarially fine-tuned BERT models. We identify\npartial preservation of BERT's syntactic abilities during fine-tuning as the\nkey to the success of AT. We observe that adversarially fine-tuned models\nremain more faithful to BERT's language modeling behavior and are more\nsensitive to the word order. As concrete examples of syntactic abilities, an\nadversarially fine-tuned model could have an advantage of up to 38% on anaphora\nagreement and up to 11% on dependency parsing. Our analysis demonstrates that\nvanilla fine-tuning oversimplifies the sentence representation by focusing\nheavily on a small subset of words. AT, however, moderates the effect of these\ninfluential words and encourages representational diversity. This allows for a\nmore hierarchical representation of a sentence and leads to the mitigation of\nBERT's loss of syntactic abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_J/0/1/0/all/0/1\">Javid Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13897","description":"<p>The MS MARCO ranking dataset has been widely used for training deep learning\nmodels for IR tasks, achieving considerable effectiveness on diverse zero-shot\nscenarios. However, this type of resource is scarce in other languages than\nEnglish. In this work we present mMARCO, a multilingual version of the MS MARCO\npassage ranking dataset comprising 8 languages that was created using machine\ntranslation. We evaluated mMARCO by fine-tuning mono and multilingual\nre-ranking models on it. Experimental results demonstrate that multilingual\nmodels fine-tuned on our translated dataset achieve superior effectiveness than\nmodels fine-tuned on the original English version alone. Also, our distilled\nmultilingual re-ranker is competitive with non-distilled models while having\n5.4 times fewer parameters. The translated datasets as well as fine-tuned\nmodels are available at https://github.com/unicamp-dl/mMARCO.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Henrique Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campiotti_I/0/1/0/all/0/1\">Israel Campiotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeronymo_V/0/1/0/all/0/1\">Vitor Jeronymo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patterns of Lexical Ambiguity in Contextualised Language Models. (arXiv:2109.13032v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13032","description":"<p>One of the central aspects of contextualised language models is that they\nshould be able to distinguish the meaning of lexically ambiguous words by their\ncontexts. In this paper we investigate the extent to which the contextualised\nembeddings of word forms that display multiplicity of sense reflect traditional\ndistinctions of polysemy and homonymy. To this end, we introduce an extended,\nhuman-annotated dataset of graded word sense similarity and co-predication\nacceptability, and evaluate how well the similarity of embeddings predicts\nsimilarity in meaning. Both types of human judgements indicate that the\nsimilarity of polysemic interpretations falls in a continuum between identity\nof meaning and homonymy. However, we also observe significant differences\nwithin the similarity ratings of polysemes, forming consistent patterns for\ndifferent types of polysemic sense alternation. Our dataset thus appears to\ncapture a substantial part of the complexity of lexical ambiguity, and can\nprovide a realistic test bed for contextualised embeddings. Among the tested\nmodels, BERT Large shows the strongest correlation with the collected word\nsense similarity ratings, but struggles to consistently replicate the observed\nsimilarity patterns. When clustering ambiguous word forms based on their\nembeddings, the model displays high confidence in discerning homonyms and some\ntypes of polysemic alternations, but consistently fails for others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haber_J/0/1/0/all/0/1\">Janosch Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesio_M/0/1/0/all/0/1\">Massimo Poesio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Fine-tuning Vision Transformers for the Prediction of State Variables in Ising Models. (arXiv:2109.13925v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13925","description":"<p>Transformers are state-of-the-art deep learning models that are composed of\nstacked attention and point-wise, fully connected layers designed for handling\nsequential data. Transformers are not only ubiquitous throughout Natural\nLanguage Processing (NLP), but, recently, they have inspired a new wave of\nComputer Vision (CV) applications research. In this work, a Vision Transformer\n(ViT) is applied to predict the state variables of 2-dimensional Ising model\nsimulations. Our experiments show that ViT outperform state-of-the-art\nConvolutional Neural Networks (CNN) when using a small number of microstate\nimages from the Ising model corresponding to various boundary conditions and\ntemperatures. This work opens the possibility of applying ViT to other\nsimulations, and raises interesting research directions on how attention maps\ncan learn about the underlying physics governing different phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kara_O/0/1/0/all/0/1\">Onur Kara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sehanobish_A/0/1/0/all/0/1\">Arijit Sehanobish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corzo_H/0/1/0/all/0/1\">Hector H Corzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All-Around Real Label Supervision: Cyclic Prototype Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.13930v1 [eess.IV])","link":"http://arxiv.org/abs/2109.13930","description":"<p>Semi-supervised learning has substantially advanced medical image\nsegmentation since it alleviates the heavy burden of acquiring the costly\nexpert-examined annotations. Especially, the consistency-based approaches have\nattracted more attention for their superior performance, wherein the real\nlabels are only utilized to supervise their paired images via supervised loss\nwhile the unlabeled images are exploited by enforcing the perturbation-based\n\\textit{\"unsupervised\"} consistency without explicit guidance from those real\nlabels. However, intuitively, the expert-examined real labels contain more\nreliable supervision signals. Observing this, we ask an unexplored but\ninteresting question: can we exploit the unlabeled data via explicit real label\nsupervision for semi-supervised training? To this end, we discard the previous\nperturbation-based consistency but absorb the essence of non-parametric\nprototype learning. Based on the prototypical network, we then propose a novel\ncyclic prototype consistency learning (CPCL) framework, which is constructed by\na labeled-to-unlabeled (L2U) prototypical forward process and an\nunlabeled-to-labeled (U2L) backward process. Such two processes synergistically\nenhance the segmentation network by encouraging more discriminative and compact\nfeatures. In this way, our framework turns previous \\textit{\"unsupervised\"}\nconsistency into new \\textit{\"supervised\"} consistency, obtaining the\n\\textit{\"all-around real label supervision\"} property of our method. Extensive\nexperiments on brain tumor segmentation from MRI and kidney segmentation from\nCT images show that our CPCL can effectively exploit the unlabeled data and\noutperform other state-of-the-art semi-supervised medical image segmentation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_R/0/1/0/all/0/1\">Raymond Kai-yu Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A framework for quantitative analysis of Computed Tomography images of viral pneumonitis: radiomic features in COVID and non-COVID patients. (arXiv:2109.13931v1 [physics.med-ph])","link":"http://arxiv.org/abs/2109.13931","description":"<p>Purpose: to optimize a pipeline of clinical data gathering and CT images\nprocessing implemented during the COVID-19 pandemic crisis and to develop\nartificial intelligence model for different of viral pneumonia. Methods: 1028\nchest CT image of patients with positive swab were segmented automatically for\nlung extraction. A Gaussian model developed in Python language was applied to\ncalculate quantitative metrics (QM) describing well-aerated and ill portions of\nthe lungs from the histogram distribution of lung CT numbers in both lungs of\neach image and in four geometrical subdivision. Furthermore, radiomic features\n(RF) of first and second order were extracted from bilateral lungs using\nPyRadiomic tools. QM and RF were used to develop 4 different Multi-Layer\nPerceptron (MLP) classifier to discriminate images of patients with COVID\n(n=646) and non-COVID (n=382) viral pneumonia. Results: The Gaussian model\napplied to lung CT histogram correctly described healthy parenchyma 94% of the\npatients. The resulting accuracy of the models for COVID diagnosis were in the\nrange 0.76-0.87, as the integral of the receiver operating curve. The best\ndiagnostic performances were associated to the model based on RF of first and\nsecond order, with 21 relevant features after LASSO regression and an accuracy\nof 0.81$\\pm$0.02 after 4-fold cross validation Conclusions: Despite these\nresults were obtained with CT images from a single center, a platform for\nextracting useful quantitative metrics from CT images was developed and\noptimized. Four artificial intelligence-based models for classifying patients\nwith COVID and non-COVID viral pneumonia were developed and compared showing\noverall good diagnostic performances\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Zorzi_G/0/1/0/all/0/1\">Giulia Zorzi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Berta_L/0/1/0/all/0/1\">Luca Berta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Carrazza_S/0/1/0/all/0/1\">Stefano Carrazza</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Torresin_A/0/1/0/all/0/1\">Alberto Torresin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-shot Key Information Extraction from Document with Deep Partial Graph Matching. (arXiv:2109.13967v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13967","description":"<p>Automating the Key Information Extraction (KIE) from documents improves\nefficiency, productivity, and security in many industrial scenarios such as\nrapid indexing and archiving. Many existing supervised learning methods for the\nKIE task need to feed a large number of labeled samples and learn separate\nmodels for different types of documents. However, collecting and labeling a\nlarge dataset is time-consuming and is not a user-friendly requirement for many\ncloud platforms. To overcome these challenges, we propose a deep end-to-end\ntrainable network for one-shot KIE using partial graph matching. Contrary to\nprevious methods that the learning of similarity and solving are optimized\nseparately, our method enables the learning of the two processes in an\nend-to-end framework. Existing one-shot KIE methods are either template or\nsimple attention-based learning approach that struggle to handle texts that are\nshifted beyond their desired positions caused by printers, as illustrated in\nFig.1. To solve this problem, we add one-to-(at most)-one constraint such that\nwe will find the globally optimized solution even if some texts are drifted.\nFurther, we design a multimodal context ensemble block to boost the performance\nthrough fusing features of spatial, textual, and aspect representations. To\npromote research of KIE, we collected and annotated a one-shot document KIE\ndataset named DKIE with diverse types of images. The DKIE dataset consists of\n2.5K document images captured by mobile phones in natural scenes, and it is the\nlargest available one-shot KIE dataset up to now. The results of experiments on\nDKIE show that our method achieved state-of-the-art performance compared with\nrecent one-shot and supervised learning approaches. The dataset and proposed\none-shot KIE model will be released soo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Minghong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liangwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_L/0/1/0/all/0/1\">Liansheng Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competence-Aware Path Planning via Introspective Perception. (arXiv:2109.13974v1 [cs.RO])","link":"http://arxiv.org/abs/2109.13974","description":"<p>Robots deployed in the real world over extended periods of time need to\nreason about unexpected failures, learn to predict them, and to proactively\ntake actions to avoid future failures. Existing approaches for competence-aware\nplanning are either model-based, requiring explicit enumeration of known\nfailure modes, or purely statistical, using state- and location-specific\nfailure statistics to infer competence. We instead propose a structured\nmodel-free approach to competence-aware planning by reasoning about plan\nexecution failures due to errors in perception, without requiring a-priori\nenumeration of failure modes or requiring location-specific failure statistics.\nWe introduce competence-aware path planning via introspective perception\n(CPIP), a Bayesian framework to iteratively learn and exploit task-level\ncompetence in novel deployment environments. CPIP factorizes the\ncompetence-aware planning problem into two components. First, perception errors\nare learned in a model-free and location-agnostic setting via introspective\nperception prior to deployment in novel environments. Second, during actual\ndeployments, the prediction of task-level failures is learned in a\ncontext-aware setting. Experiments in a simulation show that the proposed CPIP\napproach outperforms the frequentist baseline in multiple mobile robot tasks,\nand is further validated via real robot experiments in an environment with\nperceptually challenging obstacles and terrain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabiee_S/0/1/0/all/0/1\">Sadegh Rabiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basich_C/0/1/0/all/0/1\">Connor Basich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_K/0/1/0/all/0/1\">Kyle Hollins Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilberstein_S/0/1/0/all/0/1\">Shlomo Zilberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_J/0/1/0/all/0/1\">Joydeep Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Y-GAN: Learning Dual Data Representations for Efficient Anomaly Detection. (arXiv:2109.14020v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14020","description":"<p>We propose a novel reconstruction-based model for anomaly detection, called\nY-GAN. The model consists of a Y-shaped auto-encoder and represents images in\ntwo separate latent spaces. The first captures meaningful image semantics, key\nfor representing (normal) training data, whereas the second encodes low-level\nresidual image characteristics. To ensure the dual representations encode\nmutually exclusive information, a disentanglement procedure is designed around\na latent (proxy) classifier. Additionally, a novel consistency loss is proposed\nto prevent information leakage between the latent spaces. The model is trained\nin a one-class learning setting using normal training data only. Due to the\nseparation of semantically-relevant and residual information, Y-GAN is able to\nderive informative data representations that allow for efficient anomaly\ndetection across a diverse set of anomaly detection tasks. The model is\nevaluated in comprehensive experiments with several recent anomaly detection\nmodels using four popular datasets, i.e., MNIST, FMNIST and CIFAR10, and\nPlantVillage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovska_M/0/1/0/all/0/1\">Marija Ivanovska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1\">Vitomir &#x160;truc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Unrolled Recovery in Sparse Biological Imaging. (arXiv:2109.14025v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14025","description":"<p>Deep algorithm unrolling has emerged as a powerful model-based approach to\ndevelop deep architectures that combine the interpretability of iterative\nalgorithms with the performance gains of supervised deep learning, especially\nin cases of sparse optimization. This framework is well-suited to applications\nin biological imaging, where physics-based models exist to describe the\nmeasurement process and the information to be recovered is often highly\nstructured. Here, we review the method of deep unrolling, and show how it\nimproves source localization in several biological imaging settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahel_Y/0/1/0/all/0/1\">Yair Ben Sahel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_J/0/1/0/all/0/1\">John P. Bryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cleary_B/0/1/0/all/0/1\">Brian Cleary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhi_S/0/1/0/all/0/1\">Samouil L. Farhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoPhaseNN: Unsupervised Physics-aware Deep Learning of 3D Nanoscale Coherent Imaging. (arXiv:2109.14053v1 [physics.app-ph])","link":"http://arxiv.org/abs/2109.14053","description":"<p>The problem of phase retrieval, or the algorithmic recovery of lost phase\ninformation from measured intensity alone, underlies various imaging methods\nfrom astronomy to nanoscale imaging. Traditional methods of phase retrieval are\niterative in nature, and are therefore computationally expensive and time\nconsuming. More recently, deep learning (DL) models have been developed to\neither provide learned priors to iterative phase retrieval or in some cases\ncompletely replace phase retrieval with networks that learn to recover the lost\nphase information from measured intensity alone. However, such models require\nvast amounts of labeled data, which can only be obtained through simulation or\nperforming computationally prohibitive phase retrieval on hundreds of or even\nthousands of experimental datasets. Using a 3D nanoscale X-ray imaging modality\n(Bragg Coherent Diffraction Imaging or BCDI) as a representative technique, we\ndemonstrate AutoPhaseNN, a DL-based approach which learns to solve the phase\nproblem without labeled data. By incorporating the physics of the imaging\ntechnique into the DL model during training, AutoPhaseNN learns to invert 3D\nBCDI data from reciprocal space to real space in a single shot without ever\nbeing shown real space images. Once trained, AutoPhaseNN is about one hundred\ntimes faster than traditional iterative phase retrieval methods while providing\ncomparable image quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chan_H/0/1/0/all/0/1\">Henry Chan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sankaranarayanan_S/0/1/0/all/0/1\">Subramanian Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Balaprakash_P/0/1/0/all/0/1\">Prasanna Balaprakash</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Harder_R/0/1/0/all/0/1\">Ross J. Harder</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cherukara_M/0/1/0/all/0/1\">Mathew J. Cherukara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. (arXiv:2109.14084v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14084","description":"<p>We present VideoCLIP, a contrastive approach to pre-train a unified model for\nzero-shot video and text understanding, without using any labels on downstream\ntasks. VideoCLIP trains a transformer for video and text by contrasting\ntemporally overlapping positive video-text pairs with hard negatives from\nnearest neighbor retrieval. Our experiments on a diverse series of downstream\ntasks, including sequence-level text-video retrieval, VideoQA, token-level\naction localization, and action segmentation reveal state-of-the-art\nperformance, surpassing prior work, and in some cases even outperforming\nsupervised approaches. Code is made available at\nhttps://github.com/pytorch/fairseq/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_F/0/1/0/all/0/1\">Florian Metze Luke Zettlemoyer Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually Grounded Concept Composition. (arXiv:2109.14115v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14115","description":"<p>We investigate ways to compose complex concepts in texts from primitive ones\nwhile grounding them in images. We propose Concept and Relation Graph (CRG),\nwhich builds on top of constituency analysis and consists of recursively\ncombined concepts with predicate functions. Meanwhile, we propose a concept\ncomposition neural network called Composer to leverage the CRG for visually\ngrounded concept learning. Specifically, we learn the grounding of both\nprimitive and all composed concepts by aligning them to images and show that\nlearning to compose leads to more robust grounding results, measured in\ntext-to-image matching accuracy. Notably, our model can model grounded concepts\nforming at both the finer-grained sentence level and the coarser-grained\nintermediate level (or word-level). Composer leads to pronounced improvement in\nmatching accuracy when the evaluation data has significant compound divergence\nfrom the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of atlas-based and neural-network-based semantic segmentation for DENSE MRI images. (arXiv:2109.14116v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14116","description":"<p>Two segmentation methods, one atlas-based and one neural-network-based, were\ncompared to see how well they can each automatically segment the brain stem and\ncerebellum in Displacement Encoding with Stimulated Echoes Magnetic Resonance\nImaging (DENSE-MRI) data. The segmentation is a pre-requisite for estimating\nthe average displacements in these regions, which have recently been proposed\nas biomarkers in the diagnosis of Chiari Malformation type I (CMI). In\nnumerical experiments, the segmentations of both methods were similar to manual\nsegmentations provided by trained experts. It was found that, overall, the\nneural-network-based method alone produced more accurate segmentations than the\natlas-based method did alone, but that a combination of the two methods -- in\nwhich the atlas-based method is used for the segmentation of the brain stem and\nthe neural-network is used for the segmentation of the cerebellum -- may be the\nmost successful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Buser_E/0/1/0/all/0/1\">Elle Buser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hart_E/0/1/0/all/0/1\">Emma Hart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huenemann_B/0/1/0/all/0/1\">Ben Huenemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Learning on a Sequence of Imbalanced Domains with Difficulty Awareness. (arXiv:2109.14120v1 [cs.LG])","link":"http://arxiv.org/abs/2109.14120","description":"<p>Recognizing new objects by learning from a few labeled examples in an\nevolving environment is crucial to obtain excellent generalization ability for\nreal-world machine learning systems. A typical setting across current meta\nlearning algorithms assumes a stationary task distribution during meta\ntraining. In this paper, we explore a more practical and challenging setting\nwhere task distribution changes over time with domain shift. Particularly, we\nconsider realistic scenarios where task distribution is highly imbalanced with\ndomain labels unavailable in nature. We propose a kernel-based method for\ndomain change detection and a difficulty-aware memory management mechanism that\njointly considers the imbalanced domain size and domain importance to learn\nacross domains continuously. Furthermore, we introduce an efficient adaptive\ntask sampling method during meta training, which significantly reduces task\ngradient variance with theoretical guarantees. Finally, we propose a\nchallenging benchmark with imbalanced domain sequences and varied domain\ndifficulty. We have performed extensive evaluations on the proposed benchmark,\ndemonstrating the effectiveness of our method. We made our code publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_T/0/1/0/all/0/1\">Tiehang Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Le Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suo_Q/0/1/0/all/0/1\">Qiuling Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Dense Crowd Trajectory Forecasting. (arXiv:2109.14128v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14128","description":"<p>Accurate, long-term forecasting of human pedestrian trajectories in highly\ndynamic and interactive scenes is a long-standing challenge. Recent advances in\nusing data-driven approaches have achieved significant improvements in terms of\nprediction accuracy. However, the lack of group-aware analysis has limited the\nperformance of forecasting models. This is especially apparent in highly\npopulated scenes, where pedestrians are moving in groups and the interactions\nbetween groups are extremely complex and dynamic. In this paper, we present\nGrouptron, a multi-scale dynamic forecasting framework that leverages\npedestrian group detection and utilizes individual-level, group-level, and\nscene-level information for better understanding and representation of the\nscenes. Our approach employs spatio-temporal clustering algorithms to identify\npedestrian groups, creates spatio-temporal graphs at the individual, group, and\nscene levels. It then uses graph neural networks to encode dynamics at\ndifferent scales and incorporates encoding across different scales for\ntrajectory prediction. We carried out extensive comparisons and ablation\nexperiments to demonstrate the effectiveness of our approach. Our method\nachieves 9.3% decrease in final displacement error (FDE) compared with\nstate-of-the-art methods on ETH/UCY benchmark datasets, and 16.1% decrease in\nFDE in more crowded scenes where extensive human group interactions are more\nfrequently present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Rui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhuo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Video-Language Segmentation. (arXiv:2109.14131v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14131","description":"<p>We focus on the problem of segmenting a certain object referred by a natural\nlanguage sentence in video content, at the core of formulating a pinpoint\nvision-language relation. While existing attempts mainly construct such\nrelation in an implicit way, i.e., grid-level multi-modal feature fusion, it\nhas been proven problematic to distinguish semantically similar objects under\nthis paradigm. In this work, we propose to interwind the visual and linguistic\nmodalities in an explicit way via the contrastive learning objective, which\ndirectly aligns the referred object and the language description and separates\nthe unreferred content apart across frames. Moreover, to remedy for the\ndegradation problem, we present two complementary hard instance mining\nstrategies, i.e., Language-relevant Channel Filter and Relative Hard Instance\nConstruction. They encourage the network to exclude visual-distinguishable\nfeature and to focus on easy-confused objects during the contrastive training.\nExtensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB\nSentences, quantitatively demonstrate the state-of-the-arts performance of our\nmethod and qualitatively show the more accurate distinguishment between\nsemantically similar objects over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Zero-Shot Learning with DNA as Side Information. (arXiv:2109.14133v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14133","description":"<p>Fine-grained zero-shot learning task requires some form of side-information\nto transfer discriminative information from seen to unseen classes. As manually\nannotated visual attributes are extremely costly and often impractical to\nobtain for a large number of classes, in this study we use DNA as side\ninformation for the first time for fine-grained zero-shot classification of\nspecies. Mitochondrial DNA plays an important role as a genetic marker in\nevolutionary biology and has been used to achieve near-perfect accuracy in the\nspecies classification of living organisms. We implement a simple hierarchical\nBayesian model that uses DNA information to establish the hierarchy in the\nimage space and employs local priors to define surrogate classes for unseen\nones. On the benchmark CUB dataset, we show that DNA can be equally promising\nyet in general a more accessible alternative than word vectors as a side\ninformation. This is especially important as obtaining robust word\nrepresentations for fine-grained species names is not a practicable goal when\ninformation about these species in free-form text is limited. On a newly\ncompiled fine-grained insect dataset that uses DNA information from over a\nthousand species, we show that the Bayesian approach outperforms\nstate-of-the-art by a wide margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badirli_S/0/1/0/all/0/1\">Sarkhan Badirli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohler_G/0/1/0/all/0/1\">George Mohler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_C/0/1/0/all/0/1\">Christine Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dundar_M/0/1/0/all/0/1\">Murat Dundar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Xception with Dual Attention Mechanism and Feature Fusion for Face Forgery Detection. (arXiv:2109.14136v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14136","description":"<p>With the rapid development of deep learning technology, more and more face\nforgeries by deepfake are widely spread on social media, causing serious social\nconcern. Face forgery detection has become a research hotspot in recent years,\nand many related methods have been proposed until now. For those images with\nlow quality and/or diverse sources, however, the detection performances of\nexisting methods are still far from satisfactory. In this paper, we propose an\nimproved Xception with dual attention mechanism and feature fusion for face\nforgery detection. Different from the middle flow in original Xception model,\nwe try to catch different high-semantic features of the face images using\ndifferent levels of convolution, and introduce the convolutional block\nattention module and feature fusion to refine and reorganize those\nhigh-semantic features. In the exit flow, we employ the self-attention\nmechanism and depthwise separable convolution to learn the global information\nand local information of the fused features separately to improve the\nclassification the ability of the proposed model. Experimental results\nevaluated on three Deepfake datasets demonstrate that the proposed method\noutperforms Xception as well as other related methods both in effectiveness and\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weiqi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kangkang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minglin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Entangled Visual Semantic Transformer for Image Captioning. (arXiv:2109.14137v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14137","description":"<p>Recent advancements of image captioning have featured Visual-Semantic Fusion\nor Geometry-Aid attention refinement. However, those fusion-based models, they\nare still criticized for the lack of geometry information for inter and intra\nattention refinement. On the other side, models based on Geometry-Aid attention\nstill suffer from the modality gap between visual and semantic information. In\nthis paper, we introduce a novel Geometry-Entangled Visual Semantic Transformer\n(GEVST) network to realize the complementary advantages of Visual-Semantic\nFusion and Geometry-Aid attention refinement. Concretely, a Dense-Cap model\nproposes some dense captions with corresponding geometry information at first.\nThen, to empower GEVST with the ability to bridge the modality gap among visual\nand semantic information, we build four parallel transformer encoders VV(Pure\nVisual), VS(Semantic fused to Visual), SV(Visual fused to Semantic), SS(Pure\nSemantic) for final caption generation. Both visual and semantic geometry\nfeatures are used in the Fusion module and also the Self-Attention module for\nbetter attention measurement. To validate our model, we conduct extensive\nexperiments on the MS-COCO dataset, the experimental results show that our\nGEVST model can obtain promising performance gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Ling Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feida Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-frame Joint Enhancement for Early Interlaced Videos. (arXiv:2109.14151v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14151","description":"<p>Early interlaced videos usually contain multiple and interlacing and complex\ncompression artifacts, which significantly reduce the visual quality. Although\nthe high-definition reconstruction technology for early videos has made great\nprogress in recent years, related research on deinterlacing is still lacking.\nTraditional methods mainly focus on simple interlacing mechanism, and cannot\ndeal with the complex artifacts in real-world early videos. Recent interlaced\nvideo reconstruction deep deinterlacing models only focus on single frame,\nwhile neglecting important temporal information. Therefore, this paper proposes\na multiframe deinterlacing network joint enhancement network for early\ninterlaced videos that consists of three modules, i.e., spatial vertical\ninterpolation module, temporal alignment and fusion module, and final\nrefinement module. The proposed method can effectively remove the complex\nartifacts in early videos by using temporal redundancy of multi-fields.\nExperimental results demonstrate that the proposed method can recover high\nquality results for both synthetic dataset and real-world early interlaced\nvideos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1\">Yanbo Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Ronggang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoping Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Dynamic Contrast and Probability Distillation for Unsupervised Person Re-Id. (arXiv:2109.14157v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14157","description":"<p>Unsupervised person re-identification (Re-Id) has attracted increasing\nattention due to its practical application in the read-world video surveillance\nsystem. The traditional unsupervised Re-Id are mostly based on the method\nalternating between clustering and fine-tuning with the classification or\nmetric learning objectives on the grouped clusters. However, since person Re-Id\nis an open-set problem, the clustering based methods often leave out lots of\noutlier instances or group the instances into the wrong clusters, thus they can\nnot make full use of the training samples as a whole. To solve these problems,\nwe present the hybrid dynamic cluster contrast and probability distillation\nalgorithm. It formulates the unsupervised Re-Id problem into an unified\nlocal-to-global dynamic contrastive learning and self-supervised probability\ndistillation framework. Specifically, the proposed method can make the utmost\nof the self-supervised signals of all the clustered and un-clustered instances,\nfrom both the instances' self-contrastive level and the probability\ndistillation respective, in the memory-based non-parametric manner. Besides,\nthe proposed hybrid local-to-global contrastive learning can take full\nadvantage of the informative and valuable training examples for effective and\nrobust training. Extensive experiment results show that the proposed method\nachieves superior performances to state-of-the-art methods, under both the\npurely unsupervised and unsupervised domain adaptation experiment settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">De Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Communications With AI Tasks. (arXiv:2109.14170v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14170","description":"<p>A radical paradigm shift of wireless networks from ``connected things'' to\n``connected intelligence'' undergoes, which coincides with the Shanno and\nWeaver's envisions: Communications will transform from the technical level to\nthe semantic level. This article proposes a semantic communication method with\nartificial intelligence tasks (SC-AIT). First, the architecture of SC-AIT is\nelaborated. Then, based on the proposed architecture, we implement SC-AIT for a\nimage classifications task. A prototype of SC-AIT is also established for\nsurface defect detection, is conducted. Experimental results show that SC-AIT\nhas much lower bandwidth requirements, and can achieve more than $40\\%$\nclassification accuracy gains compared with the communications at the technical\nlevel. Future trends and key challenges for semantic communications are also\nidentified.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Caili Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangfang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lunan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qizheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiujiu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Segmentation of Radiation-Induced Pulmonary Fibrosis from Lung CT Scans with Multi-Scale Guided Dense Attention. (arXiv:2109.14172v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14172","description":"<p>Computed Tomography (CT) plays an important role in monitoring\nradiation-induced Pulmonary Fibrosis (PF), where accurate segmentation of the\nPF lesions is highly desired for diagnosis and treatment follow-up. However,\nthe task is challenged by ambiguous boundary, irregular shape, various position\nand size of the lesions, as well as the difficulty in acquiring a large set of\nannotated volumetric images for training. To overcome these problems, we\npropose a novel convolutional neural network called PF-Net and incorporate it\ninto a semi-supervised learning framework based on Iterative Confidence-based\nRefinement And Weighting of pseudo Labels (I-CRAWL). Our PF-Net combines 2D and\n3D convolutions to deal with CT volumes with large inter-slice spacing, and\nuses multi-scale guided dense attention to segment complex PF lesions. For\nsemi-supervised learning, our I-CRAWL employs pixel-level uncertainty-based\nconfidence-aware refinement to improve the accuracy of pseudo labels of\nunannotated images, and uses image-level uncertainty for confidence-based image\nweighting to suppress low-quality pseudo labels in an iterative training\nprocess. Extensive experiments with CT scans of Rhesus Macaques with\nradiation-induced PF showed that: 1) PF-Net achieved higher segmentation\naccuracy than existing 2D, 3D and 2.5D neural networks, and 2) I-CRAWL\noutperformed state-of-the-art semi-supervised learning methods for the PF\nlesion segmentation task. Our method has a potential to improve the diagnosis\nof PF and clinical assessment of side effects of radiotherapy for lung cancers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_S/0/1/0/all/0/1\">Shuwei Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lasio_G/0/1/0/all/0/1\">Giovanni Lasio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_B/0/1/0/all/0/1\">Baoshe Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yi_B/0/1/0/all/0/1\">Byong Yi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Macvittie_T/0/1/0/all/0/1\">Thomas J. Macvittie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1\">Jinghao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSAMT: Time-Series-Analysis-based Motion Transfer among Multiple Cameras. (arXiv:2109.14174v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14174","description":"<p>Along with advances in optical sensors is the common practice of building an\nimaging system with heterogeneous cameras. While high-resolution (HR) videos\nacquisition and analysis are benefited from hybrid sensors, the intrinsic\ncharacteristics of multiple cameras lead to an interesting motion transfer\nproblem. Unfortunately, most of the existing methods provide no theoretical\nanalysis and require intensive training data. In this paper, we propose an\nalgorithm using time series analysis for motion transfer among multiple\ncameras. Specifically, we firstly identify seasonality in motion data and then\nbuild an addictive time series model to extract patterns that could be\ntransferred across cameras. Our approach has a complete and clear mathematical\nformulation, thus being efficient and interpretable. Through quantitative\nevaluations on real-world data, we demonstrate the effectiveness of our method.\nFurthermore, our motion transfer algorithm could combine with and facilitate\ndownstream tasks, e.g., enhancing pose estimation on LR videos with inherent\npatterns extracted from HR ones. Code is available at\nhttps://github.com/IndigoPurple/TSAMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongrui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays. (arXiv:2109.14187v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14187","description":"<p>Deep learning has shown recent success in classifying anomalies in chest\nx-rays, but datasets are still small compared to natural image datasets.\nSupervision of abnormality localization has been shown to improve trained\nmodels, partially compensating for dataset sizes. However, explicitly labeling\nthese anomalies requires an expert and is very time-consuming. We propose a\nmethod for collecting implicit localization data using an eye tracker to\ncapture gaze locations and a microphone to capture a dictation of a report,\nimitating the setup of a reading room, and potentially scalable for large\ndatasets. The resulting REFLACX (Reports and Eye-Tracking Data for Localization\nof Abnormalities in Chest X-rays) dataset was labeled by five radiologists and\ncontains 3,032 synchronized sets of eye-tracking data and timestamped report\ntranscriptions. We also provide bounding boxes around lungs and heart and\nvalidation labels consisting of ellipses localizing abnormalities and\nimage-level labels. Furthermore, a small subset of the data contains readings\nfrom all radiologists, allowing for the calculation of inter-rater scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lanfredi_R/0/1/0/all/0/1\">Ricardo Bigolin Lanfredi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auffermann_W/0/1/0/all/0/1\">William F. Auffermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jessica Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_P/0/1/0/all/0/1\">Phuong-Anh T. Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drew_T/0/1/0/all/0/1\">Trafton Drew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroeder_J/0/1/0/all/0/1\">Joyce D. Schroeder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation. (arXiv:2109.14196v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14196","description":"<p>Domain generalization for semantic segmentation is highly demanded in real\napplications, where a trained model is expected to work well in previously\nunseen domains. One challenge lies in the lack of data which could cover the\ndiverse distributions of the possible unseen domains for training. In this\npaper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme,\nwhich is the first to exploit the diversity of web-crawled images for\ngeneralizable semantic segmentation. To explore and exploit the real-world data\ndistributions, we collect a web-crawled dataset which presents large diversity\nin terms of weather conditions, sites, lighting, camera styles, etc. We also\npresent a method which injects the style representation of the web-crawled data\ninto the source domain on-the-fly during training, which enables the network to\nexperience images of diverse styles with reliable labels for effective\ntraining. Moreover, we use the web-crawled dataset with predicted pseudo labels\nfor training to further enhance the capability of the network. Extensive\nexperiments demonstrate that our method clearly outperforms existing domain\ngeneralization techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Namyup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_T/0/1/0/all/0/1\">Taeyoung Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation. (arXiv:2109.14200v1 [eess.AS])","link":"http://arxiv.org/abs/2109.14200","description":"<p>Decades of research has studied how language learning infants learn to\ndiscriminate speech sounds, segment words, and associate words with their\nmeanings. While gradual development of such capabilities is unquestionable, the\nexact nature of these skills and the underlying mental representations yet\nremains unclear. In parallel, computational studies have shown that basic\ncomprehension of speech can be achieved by statistical learning between speech\nand concurrent referentially ambiguous visual input. These models can operate\nwithout prior linguistic knowledge such as representations of linguistic units,\nand without learning mechanisms specifically targeted at such units. This has\nraised the question of to what extent knowledge of linguistic units, such as\nphone(me)s, syllables, and words, could actually emerge as latent\nrepresentations supporting the translation between speech and representations\nin other modalities, and without the units being proximal learning targets for\nthe learner. In this study, we formulate this idea as the so-called latent\nlanguage hypothesis (LLH), connecting linguistic representation learning to\ngeneral predictive processing within and across sensory modalities. We review\nthe extent that the audiovisual aspect of LLH is supported by the existing\ncomputational studies. We then explore LLH further in extensive learning\nsimulations with different neural network models for audiovisual\ncross-situational learning, and comparing learning from both synthetic and real\nspeech data. We investigate whether the latent representations learned by the\nnetworks reflect phonetic, syllabic, or lexical structure of input speech by\nutilizing an array of complementary evaluation metrics related to linguistic\nselectivity and temporal characteristics of the representations. As a result,\nwe find that representations associated...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khorrami_K/0/1/0/all/0/1\">Khazar Khorrami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity-Expression Ambiguity in 3D Morphable Face Models. (arXiv:2109.14203v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14203","description":"<p>3D Morphable Models are a class of generative models commonly used to model\nfaces. They are typically applied to ill-posed problems such as 3D\nreconstruction from 2D data. Several ambiguities in this problem's image\nformation process have been studied explicitly. We demonstrate that\nnon-orthogonality of the variation in identity and expression can cause\nidentity-expression ambiguity in 3D Morphable Models, and that in practice\nexpression and identity are far from orthogonal and can explain each other\nsurprisingly well. Whilst previously reported ambiguities only arise in an\ninverse rendering setting, identity-expression ambiguity emerges in the 3D\nshape generation process itself. We demonstrate this effect with 3D shapes\ndirectly as well as through an inverse rendering task, and use two popular\nmodels built from high quality 3D scans as well as a model built from a large\ncollection of 2D images and videos. We explore this issue's implications for\ninverse rendering and observe that it cannot be resolved by a purely\nstatistical prior on identity and expression deformations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1\">Bernhard Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutherland_S/0/1/0/all/0/1\">Skylar Sutherland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medin_S/0/1/0/all/0/1\">Safa C. Medin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Brightness Agnostic Adversarial Examples Against Face Recognition Systems. (arXiv:2109.14205v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14205","description":"<p>This paper introduces a novel adversarial example generation method against\nface recognition systems (FRSs). An adversarial example (AX) is an image with\ndeliberately crafted noise to cause incorrect predictions by a target system.\nThe AXs generated from our method remain robust under real-world brightness\nchanges. Our method performs non-linear brightness transformations while\nleveraging the concept of curriculum learning during the attack generation\nprocedure. We demonstrate that our method outperforms conventional techniques\nfrom comprehensive experimental investigations in the digital and physical\nworld. Furthermore, this method enables practical risk assessment of FRSs\nagainst brightness agnostic AXs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Inderjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momiyama_S/0/1/0/all/0/1\">Satoru Momiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakizaki_K/0/1/0/all/0/1\">Kazuya Kakizaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_T/0/1/0/all/0/1\">Toshinori Araki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation in Semantic Segmentation Based on Pixel Alignment and Self-Training. (arXiv:2109.14219v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14219","description":"<p>This paper proposes an unsupervised cross-modality domain adaptation approach\nbased on pixel alignment and self-training. Pixel alignment transfers ceT1\nscans to hrT2 modality, helping to reduce domain shift in the training\nsegmentation model. Self-training adapts the decision boundary of the\nsegmentation network to fit the distribution of hrT2 scans. Experiment results\nshow that PAST has outperformed the non-UDA baseline significantly, and it\nreceived rank-2 on CrossMoDA validation phase Leaderboard with a mean Dice\nscore of 0.8395.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hexin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multipath CNN with alpha matte inference for knee tissue segmentation from MRI. (arXiv:2109.14249v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14249","description":"<p>Precise segmentation of knee tissues from magnetic resonance imaging (MRI) is\ncritical in quantitative imaging and diagnosis. Convolutional neural networks\n(CNNs), which are state of the art, have limitations owing to the lack of\nimage-specific adaptation, such as low tissue contrasts and structural\ninhomogeneities, thereby leading to incomplete segmentation results. This paper\npresents a deep learning based automatic segmentation framework for knee tissue\nsegmentation. A novel multipath CNN-based method is proposed, which consists of\nan encoder decoder-based segmentation network in combination with a low rank\ntensor-reconstructed segmentation network. Low rank reconstruction in MRI\ntensor sub-blocks is introduced to exploit the structural and morphological\nvariations in knee tissues. To further improve the segmentation from CNNs,\ntrimap generation, which effectively utilizes superimposed regions, is proposed\nfor defining high, medium and low confidence regions from the multipath CNNs.\nThe secondary path with low rank reconstructed input mitigates the conditions\nin which the primary segmentation network can potentially fail and overlook the\nboundary regions. The outcome of the segmentation is solved as an alpha matting\nproblem by blending the trimap with the source input. Experiments on\nOsteoarthritis Initiative (OAI) datasets and a self prepared scan validate the\neffectiveness of the proposed method. We specifically demonstrate the\napplication of the proposed method in a cartilage segmentation based thickness\nmap for diagnosis purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Sheheryar Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azam_B/0/1/0/all/0/1\">Basim Azam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1\">Yongcheng Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Weitian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v1 [cs.LG])","link":"http://arxiv.org/abs/2109.14251","description":"<p>Accurate inference of fine-grained traffic flow from coarse-grained one is an\nemerging yet crucial problem, which can help greatly reduce the number of\ntraffic monitoring sensors for cost savings. In this work, we notice that\ntraffic flow has a high correlation with road network, which was either\ncompletely ignored or simply treated as an external factor in previous works.\nTo facilitate this problem, we propose a novel Road-Aware Traffic Flow\nMagnifier (RATFM) that explicitly exploits the prior knowledge of road networks\nto fully learn the road-aware spatial distribution of fine-grained traffic\nflow. Specifically, a multi-directional 1D convolutional layer is first\nintroduced to extract the semantic feature of the road network. Subsequently,\nwe incorporate the road network feature and coarse-grained flow feature to\nregularize the short-range spatial distribution modeling of road-relative\ntraffic flow. Furthermore, we take the road network feature as a query to\ncapture the long-range spatial distribution of traffic flow with a transformer\narchitecture. Benefiting from the road-aware inference mechanism, our method\ncan generate high-quality fine-grained traffic flow maps. Extensive experiments\non three real-world datasets show that the proposed RATFM outperforms\nstate-of-the-art models under various scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing Counterfactual Generators using Deep Model Inversion. (arXiv:2109.14274v1 [cs.LG])","link":"http://arxiv.org/abs/2109.14274","description":"<p>Explanation techniques that synthesize small, interpretable changes to a\ngiven image while producing desired changes in the model prediction have become\npopular for introspecting black-box models. Commonly referred to as\ncounterfactuals, the synthesized explanations are required to contain\ndiscernible changes (for easy interpretability) while also being realistic\n(consistency to the data manifold). In this paper, we focus on the case where\nwe have access only to the trained deep classifier and not the actual training\ndata. While the problem of inverting deep models to synthesize images from the\ntraining distribution has been explored, our goal is to develop a deep\ninversion approach to generate counterfactual explanations for a given query\nimage. Despite their effectiveness in conditional image synthesis, we show that\nexisting deep inversion methods are insufficient for producing meaningful\ncounterfactuals. We propose DISC (Deep Inversion for Synthesizing\nCounterfactuals) that improves upon deep inversion by utilizing (a) stronger\nimage priors, (b) incorporating a novel manifold consistency objective and (c)\nadopting a progressive optimization strategy. We find that, in addition to\nproducing visually meaningful explanations, the counterfactuals from DISC are\neffective at learning classifier decision boundaries and are robust to unknown\ntest-time corruptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J. Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanaswamy_V/0/1/0/all/0/1\">Vivek Narayanaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_D/0/1/0/all/0/1\">Deepta Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jason Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanias_A/0/1/0/all/0/1\">Andreas Spanias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localizing Objects with Self-Supervised Transformers and no Labels. (arXiv:2109.14279v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14279","description":"<p>Localizing objects in image collections without supervision can help to avoid\nexpensive annotation campaigns. We propose a simple approach to this problem,\nthat leverages the activation features of a vision transformer pre-trained in a\nself-supervised manner. Our method, LOST, does not require any external object\nproposal nor any exploration of the image collection; it operates on a single\nimage. Yet, we outperform state-of-the-art object discovery methods by up to 8\nCorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic\ndetector on the discovered objects boosts results by another 7 points.\nMoreover, we show promising results on the unsupervised object discovery task.\nThe code to reproduce our results can be found at\nhttps://github.com/valeoai/LOST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simeoni_O/0/1/0/all/0/1\">Oriane Sim&#xe9;oni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1\">Gilles Puy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1\">Huy V. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roburin_S/0/1/0/all/0/1\">Simon Roburin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1\">Spyros Gidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for 3D Medical Image Analysis using 3D SimCLR and Monte Carlo Dropout. (arXiv:2109.14288v1 [cs.LG])","link":"http://arxiv.org/abs/2109.14288","description":"<p>Self-supervised learning methods can be used to learn meaningful\nrepresentations from unlabeled data that can be transferred to supervised\ndownstream tasks to reduce the need for labeled data. In this paper, we propose\na 3D self-supervised method that is based on the contrastive (SimCLR) method.\nAdditionally, we show that employing Bayesian neural networks (with Monte-Carlo\nDropout) during the inference phase can further enhance the results on the\ndownstream tasks. We showcase our models on two medical imaging segmentation\ntasks: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation\nfrom 3D CT. Our experimental results demonstrate the benefits of our proposed\nmethods in both downstream data-efficiency and performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_Y/0/1/0/all/0/1\">Yamen Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taleb_A/0/1/0/all/0/1\">Aiham Taleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1\">Marina M. -C. H&#xf6;hne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippert_C/0/1/0/all/0/1\">Christoph Lippert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three-Stream 3D/1D CNN for Fine-Grained Action Classification and Segmentation in Table Tennis. (arXiv:2109.14306v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14306","description":"<p>This paper proposes a fusion method of modalities extracted from video\nthrough a three-stream network with spatio-temporal and temporal convolutions\nfor fine-grained action classification in sport. It is applied to TTStroke-21\ndataset which consists of untrimmed videos of table tennis games. The goal is\nto detect and classify table tennis strokes in the videos, the first step of a\nbigger scheme aiming at giving feedback to the players for improving their\nperformance. The three modalities are raw RGB data, the computed optical flow\nand the estimated pose of the player. The network consists of three branches\nwith attention blocks. Features are fused at the latest stage of the network\nusing bilinear layers. Compared to previous approaches, the use of three\nmodalities allows faster convergence and better performances on both tasks:\nclassification of strokes with known temporal boundaries and joint segmentation\nand classification. The pose is also further investigated in order to offer\nricher feedback to the athletes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_P/0/1/0/all/0/1\">Pierre-Etienne Martin</a> (MPI-EVA), <a href=\"http://arxiv.org/find/cs/1/au:+Benois_Pineau_J/0/1/0/all/0/1\">Jenny Benois-Pineau</a> (UB), <a href=\"http://arxiv.org/find/cs/1/au:+Peteri_R/0/1/0/all/0/1\">Renaud P&#xe9;teri</a> (MIA), <a href=\"http://arxiv.org/find/cs/1/au:+Morlier_J/0/1/0/all/0/1\">Julien Morlier</a> (UB)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution. (arXiv:2109.14335v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14335","description":"<p>Single-image super-resolution (SISR) is an important task in image\nprocessing, which aims to enhance the resolution of imaging systems. Recently,\nSISR has made a huge leap and has achieved promising results with the help of\ndeep learning (DL). In this survey, we give an overview of DL-based SISR\nmethods and group them according to their targets, such as reconstruction\nefficiency, reconstruction accuracy, and perceptual accuracy. Specifically, we\nfirst introduce the problem definition, research background, and the\nsignificance of SISR. Secondly, we introduce some related works, including\nbenchmark datasets, upsampling methods, optimization objectives, and image\nquality assessment methods. Thirdly, we provide a detailed investigation of\nSISR and give some domain-specific applications of it. Fourthly, we present the\nreconstruction results of some classic SISR methods to intuitively know their\nperformance. Finally, we discuss some issues that still exist in SISR and\nsummarize some new trends and future directions. This is an exhaustive survey\nof SISR, which can help researchers better understand SISR and inspire more\nexciting research in this field. An investigation project for SISR is provided\nin https://github.com/CV-JunchengLi/SISR-Survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pei_Z/0/1/0/all/0/1\">Zehua Pei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_T/0/1/0/all/0/1\">Tieyong Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infrared Small-Dim Target Detection with Transformer under Complex Backgrounds. (arXiv:2109.14379v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14379","description":"<p>The infrared small-dim target detection is one of the key techniques in the\ninfrared search and tracking system. Since the local regions which similar to\ninfrared small-dim targets spread over the whole background, exploring the\ninteraction information amongst image features in large-range dependencies to\nmine the difference between the target and background is crucial for robust\ndetection. However, existing deep learning-based methods are limited by the\nlocality of convolutional neural networks, which impairs the ability to capture\nlarge-range dependencies. To this end, we propose a new infrared small-dim\ntarget detection method with the transformer. We adopt the self-attention\nmechanism of the transformer to learn the interaction information of image\nfeatures in a larger range. Additionally, we design a feature enhancement\nmodule to learn more features of small-dim targets. After that, we adopt a\ndecoder with the U-Net-like skip connection operation to get the detection\nresult. Extensive experiments on two public datasets show the obvious\nsuperiority of the proposed method over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangcen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chenqiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UFO-ViT: High Performance Linear Vision Transformer without Softmax. (arXiv:2109.14382v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14382","description":"<p>Vision transformers have become one of the most important models for computer\nvision tasks. While they outperform earlier convolutional networks, the\ncomplexity quadratic to $N$ is one of the major drawbacks when using\ntraditional self-attention algorithms. Here we propose the UFO-ViT(Unit Force\nOperated Vision Trnasformer), novel method to reduce the computations of\nself-attention by eliminating some non-linearity. Modifying few of lines from\nself-attention, UFO-ViT achieves linear complexity without the degradation of\nperformance. The proposed models outperform most transformer-based models on\nimage classification and dense prediction tasks through most capacity regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jeong-geun Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Knitworks: Patched Neural Implicit Representation Networks. (arXiv:2109.14406v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14406","description":"<p>Coordinate-based Multilayer Perceptron (MLP) networks, despite being capable\nof learning neural implicit representations, are not performant for internal\nimage synthesis applications. Convolutional Neural Networks (CNNs) are\ntypically used instead for a variety of internal generative tasks, at the cost\nof a larger model. We propose Neural Knitwork, an architecture for neural\nimplicit representation learning of natural images that achieves image\nsynthesis by optimizing the distribution of image patches in an adversarial\nmanner and by enforcing consistency between the patch predictions. To the best\nof our knowledge, this is the first implementation of a coordinate-based MLP\ntailored for synthesis tasks such as image inpainting, super-resolution, and\ndenoising. We demonstrate the utility of the proposed technique by training on\nthese three tasks. The results show that modeling natural images using patches,\nrather than pixels, produces results of higher fidelity. The resulting model\nrequires 80% fewer parameters than alternative CNN-based solutions while\nachieving comparable performance and training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Czerkawski_M/0/1/0/all/0/1\">Mikolaj Czerkawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardona_J/0/1/0/all/0/1\">Javier Cardona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkinson_R/0/1/0/all/0/1\">Robert Atkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michie_C/0/1/0/all/0/1\">Craig Michie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonovic_I/0/1/0/all/0/1\">Ivan Andonovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemente_C/0/1/0/all/0/1\">Carmine Clemente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tachtatzis_C/0/1/0/all/0/1\">Christos Tachtatzis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-loss ensemble deep learning for chest X-ray classification. (arXiv:2109.14433v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14433","description":"<p>Class imbalance is common in medical image classification tasks, where the\nnumber of abnormal samples is fewer than the number of normal samples. The\ndifficulty of imbalanced classification is compounded by other issues such as\nthe size and distribution of the dataset. Reliable training of deep neural\nnetworks continues to be a major challenge in such class-imbalanced conditions.\nThe loss function used to train the deep neural networks highly impact the\nperformance of both balanced and imbalanced tasks. Currently, the cross-entropy\nloss remains the de-facto loss function for balanced and imbalanced\nclassification tasks. This loss, however, asserts equal learning to all\nclasses, leading to the classification of most samples as the majority normal\nclass. To provide a critical analysis of different loss functions and identify\nthose suitable for class-imbalanced classification, we benchmark various\nstate-of-the-art loss functions and propose novel loss functions to train a DL\nmodel and analyze its performance in a multiclass classification setting that\nclassifies pediatric chest X-rays as showing normal lungs, bacterial pneumonia,\nor viral pneumonia manifestations. We also construct prediction-level and\nmodel-level ensembles of the models that are trained with various loss\nfunctions to improve classification performance. We performed localization\nstudies to interpret model behavior to ensure that the individual models and\ntheir ensembles precisely learned the regions of interest showing disease\nmanifestations to classify the chest X-rays to their respective categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rajaraman_S/0/1/0/all/0/1\">Sivaramakrishnan Rajaraman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antani_S/0/1/0/all/0/1\">Sameer Antani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Loss for All: Deep Hashing with a Single Cosine Similarity based Learning Objective. (arXiv:2109.14449v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14449","description":"<p>A deep hashing model typically has two main learning objectives: to make the\nlearned binary hash codes discriminative and to minimize a quantization error.\nWith further constraints such as bit balance and code orthogonality, it is not\nuncommon for existing models to employ a large number (&gt;4) of losses. This\nleads to difficulties in model training and subsequently impedes their\neffectiveness. In this work, we propose a novel deep hashing model with only a\nsingle learning objective. Specifically, we show that maximizing the cosine\nsimilarity between the continuous codes and their corresponding binary\northogonal codes can ensure both hash code discriminativeness and quantization\nerror minimization. Further, with this learning objective, code balancing can\nbe achieved by simply using a Batch Normalization (BN) layer and multi-label\nclassification is also straightforward with label smoothing. The result is an\none-loss deep hashing model that removes all the hassles of tuning the weights\nof various losses. Importantly, extensive experiments show that our model is\nhighly effective, outperforming the state-of-the-art multi-loss hashing models\non three large-scale instance retrieval benchmarks, often by significant\nmargins. Code is available at https://github.com/kamwoh/orthohash\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoe_J/0/1/0/all/0/1\">Jiun Tian Hoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1\">Kam Woh Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Programmable Spectral Filter Arrays for Hyperspectral Imaging. (arXiv:2109.14450v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14450","description":"<p>Modulating the spectral dimension of light has numerous applications in\ncomputational imaging. While there are many techniques for achieving this,\nthere are few, if any, for implementing a spatially-varying and programmable\nspectral filter. This paper provides an optical design for implementing such a\ncapability. Our key insight is that spatially-varying spectral modulation can\nbe implemented using a liquid crystal spatial light modulator since it provides\nan array of liquid crystal cells, each of which can be purposed to act as a\nprogrammable spectral filter array. Relying on this insight, we provide an\noptical schematic and an associated lab prototype for realizing the capability,\nas well as address the associated challenges at implementation using optical\nand computational innovations. We show a number of unique operating points with\nour prototype including single- and multi-image hyperspectral imaging, as well\nas its application in material identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aswin C. Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragadam_V/0/1/0/all/0/1\">Vishwanath Saragadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rengarajan_V/0/1/0/all/0/1\">Vijay Rengarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadano_R/0/1/0/all/0/1\">Ryuichi Tadano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1\">Tuo Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyaizu_H/0/1/0/all/0/1\">Hideki Oyaizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murayama_J/0/1/0/all/0/1\">Jun Murayama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCTrans: Simplifying and Improving Crowd Counting with Transformer. (arXiv:2109.14483v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14483","description":"<p>Most recent methods used for crowd counting are based on the convolutional\nneural network (CNN), which has a strong ability to extract local features. But\nCNN inherently fails in modeling the global context due to the limited\nreceptive fields. However, the transformer can model the global context easily.\nIn this paper, we propose a simple approach called CCTrans to simplify the\ndesign pipeline. Specifically, we utilize a pyramid vision transformer backbone\nto capture the global crowd information, a pyramid feature aggregation (PFA)\nmodel to combine low-level and high-level features, an efficient regression\nhead with multi-scale dilated convolution (MDC) to predict density maps.\nBesides, we tailor the loss functions for our pipeline. Without bells and\nwhistles, extensive experiments demonstrate that our method achieves new\nstate-of-the-art results on several benchmarks both in weakly and\nfully-supervised crowd counting. Moreover, we currently rank No.1 on the\nleaderboard of NWPU-Crowd. Our code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongpeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Probabilistic Image Colorization. (arXiv:2109.14518v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14518","description":"<p>We propose Generative Probabilistic Image Colorization, a diffusion-based\ngenerative process that trains a sequence of probabilistic models to reverse\neach step of noise corruption. Given a line-drawing image as input, our method\nsuggests multiple candidate colorized images. Therefore, our method accounts\nfor the ill-posed nature of the colorization problem. We conducted\ncomprehensive experiments investigating the colorization of line-drawing\nimages, report the influence of a score-based MCMC approach that corrects the\nmarginal distribution of estimated samples, and further compare different\ncombinations of models and the similarity of their generated images. Despite\nusing only a relatively small training dataset, we experimentally develop a\nmethod to generate multiple diverse colorization candidates which avoids mode\ncollapse and does not require any additional constraints, losses, or\nre-training with alternative training conditions. Our proposed approach\nperformed well not only on color-conditional image generation tasks using\nbiased initial values, but also on some practical image completion and\ninpainting tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Furusawa_C/0/1/0/all/0/1\">Chie Furusawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitaoka_S/0/1/0/all/0/1\">Shinya Kitaoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Michael Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odagiri_Y/0/1/0/all/0/1\">Yuri Odagiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detailed Region-Adaptive Normalization for Heavy Makeup Transfer. (arXiv:2109.14525v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14525","description":"<p>In recent years, facial makeup transfer has attracted growing attention due\nto its efficiency and flexibility in transferring makeup styles between\ndifferent faces. Although recent works have achieved realistic results, most of\nthem fail to handle heavy makeup styles with multiple colors and subtle\ndetails. Hence we propose a novel GAN model to handle heavy makeup transfer,\nwhile maintaining the robustness to different poses and expressions. Firstly, a\nMakeup Multi-Extraction Network is introduced to learn region-wise makeup\nfeatures from multiple layers. Then, a key transferring module called Detailed\nRegion-Adaptive Normalization is proposed to fuse different levels of makeup\nstyles in an adaptive way, making great improvement to the quality of heavy\nmakeup transfer. With the outputs from the two components, Makeup Transfer\nNetwork is used to perform makeup transfer. To evaluate the efficacy of our\nproposed method, we collected a new makeup dataset containing a wide range of\nheavy styles. Experiments show that our method achieves state-of-the-art\nresults both on light and heavy makeup styles, and is robust to different poses\nand expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yueming Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peibin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingna Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization. (arXiv:2109.14549v1 [cs.RO])","link":"http://arxiv.org/abs/2109.14549","description":"<p>Developing robust vision-guided controllers for quadrupedal robots in complex\nenvironments, with various obstacles, dynamical surroundings and uneven\nterrains, is very challenging. While Reinforcement Learning (RL) provides a\npromising paradigm for agile locomotion skills with vision inputs in\nsimulation, it is still very challenging to deploy the RL policy in the real\nworld. Our key insight is that aside from the discrepancy in the domain gap, in\nvisual appearance between the simulation and the real world, the latency from\nthe control pipeline is also a major cause of difficulty. In this paper, we\npropose Multi-Modal Delay Randomization (MMDR) to address this issue when\ntraining RL agents. Specifically, we simulate the latency of real hardware by\nusing past observations, sampled with randomized periods, for both\nproprioception and vision. We train the RL policy for end-to-end control in a\nphysical simulator without any predefined controller or reference motion, and\ndirectly deploy it on the real A1 quadruped robot running in the wild. We\nevaluate our method in different outdoor environments with complex terrains and\nobstacles. We demonstrate the robot can smoothly maneuver at a high speed,\navoid the obstacles, and show significant improvement over the baselines. Our\nproject page with videos is at https://mehooz.github.io/mmdr-wild/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imai_C/0/1/0/all/0/1\">Chieko Sarah Imai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kierebinski_M/0/1/0/all/0/1\">Marcin Kierebinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Temporal Ensembling for Learning with Noisy Labels. (arXiv:2109.14563v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14563","description":"<p>Successful training of deep neural networks with noisy labels is an essential\ncapability as most real-world datasets contain some amount of mislabeled data.\nLeft unmitigated, label noise can sharply degrade typical supervised learning\napproaches. In this paper, we present robust temporal ensembling (RTE), which\ncombines robust loss with semi-supervised regularization methods to achieve\nnoise-robust learning. We demonstrate that RTE achieves state-of-the-art\nperformance across the CIFAR-10, CIFAR-100, ImageNet, WebVision, and Food-101N\ndatasets, while forgoing the recent trend of label filtering and/or fixing.\nFinally, we show that RTE also retains competitive corruption robustness to\nunforeseen input noise using CIFAR-10-C, obtaining a mean corruption error\n(mCE) of 13.50% even in the presence of an 80% noise ratio, versus 26.9% mCE\nwith standard methods on clean data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1\">Abel Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schifferer_B/0/1/0/all/0/1\">Benedikt Schifferer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DiPietro_R/0/1/0/all/0/1\">Robert DiPietro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Flexible Blind JPEG Artifacts Removal. (arXiv:2109.14573v1 [eess.IV])","link":"http://arxiv.org/abs/2109.14573","description":"<p>Training a single deep blind model to handle different quality factors for\nJPEG image artifacts removal has been attracting considerable attention due to\nits convenience for practical usage. However, existing deep blind methods\nusually directly reconstruct the image without predicting the quality factor,\nthus lacking the flexibility to control the output as the non-blind methods. To\nremedy this problem, in this paper, we propose a flexible blind convolutional\nneural network, namely FBCNN, that can predict the adjustable quality factor to\ncontrol the trade-off between artifacts removal and details preservation.\nSpecifically, FBCNN decouples the quality factor from the JPEG image via a\ndecoupler module and then embeds the predicted quality factor into the\nsubsequent reconstructor module through a quality factor attention block for\nflexible control. Besides, we find existing methods are prone to fail on\nnon-aligned double JPEG images even with only a one-pixel shift, and we thus\npropose a double JPEG degradation model to augment the training data. Extensive\nexperiments on single JPEG images, more general double JPEG images, and\nreal-world JPEG images demonstrate that our proposed FBCNN achieves favorable\nperformance against state-of-the-art methods in terms of both quantitative\nmetrics and visual quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jiaxi Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposable-Net: Scalable Low-Rank Compression for Neural Networks. (arXiv:1910.13141v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1910.13141","description":"<p>Compressing DNNs is important for the real-world applications operating on\nresource-constrained devices. However, we typically observe drastic performance\ndeterioration when changing model size after training is completed. Therefore,\nretraining is required to resume the performance of the compressed models\nsuitable for different devices. In this paper, we propose Decomposable-Net (the\nnetwork decomposable in any size), which allows flexible changes to model size\nwithout retraining. We decompose weight matrices in the DNNs via singular value\ndecomposition and adjust ranks according to the target model size. Unlike the\nexisting low-rank compression methods that specialize the model to a fixed\nsize, we propose a novel backpropagation scheme that jointly minimizes losses\nfor both of full- and low-rank networks. This enables not only to maintain the\nperformance of a full-rank network {\\it without retraining} but also to improve\nlow-rank networks in multiple sizes. Additionally, we introduce a simple\ncriterion for rank selection that effectively suppresses approximation error.\nIn experiments on the ImageNet classification task, Decomposable-Net yields\nsuperior accuracy in a wide range of model sizes. In particular,\nDecomposable-Net achieves the top-1 accuracy of $73.2\\%$ with $0.27\\times$MACs\nwith ResNet-50, compared to Tucker decomposition ($67.4\\% / 0.30\\times$),\nTrained Rank Pruning ($70.6\\% / 0.28\\times$), and universally slimmable\nnetworks ($71.4\\% / 0.26\\times$).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yaguchi_A/0/1/0/all/0/1\">Atsushi Yaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitta_S/0/1/0/all/0/1\">Shuhei Nitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakata_Y/0/1/0/all/0/1\">Yukinobu Sakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanizawa_A/0/1/0/all/0/1\">Akiyuki Tanizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Degenerative Adversarial NeuroImage Nets for Brain Scan Simulations: Application in Ageing and Dementia. (arXiv:1912.01526v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1912.01526","description":"<p>Accurate and realistic simulation of high-dimensional medical images has\nbecome an important research area relevant to many AI-enabled healthcare\napplications. However, current state-of-the-art approaches lack the ability to\nproduce satisfactory high-resolution and accurate subject-specific images. In\nthis work, we present a deep learning framework, namely 4D-Degenerative\nAdversarial NeuroImage Net (4D-DANI-Net), to generate high-resolution,\nlongitudinal MRI scans that mimic subject-specific neurodegeneration in ageing\nand dementia. 4D-DANI-Net is a modular framework based on adversarial training\nand a set of novel spatiotemporal, biologically-informed constraints. To ensure\nefficient training and overcome memory limitations affecting such\nhigh-dimensional problems, we rely on three key technological advances: i) a\nnew 3D training consistency mechanism called Profile Weight Functions (PWFs),\nii) a 3D super-resolution module and iii) a transfer learning strategy to\nfine-tune the system for a given individual. To evaluate our approach, we\ntrained the framework on 9852 T1-weighted MRI scans from 876 participants in\nthe Alzheimer's Disease Neuroimaging Initiative dataset and held out a separate\ntest set of 1283 MRI scans from 170 participants for quantitative and\nqualitative assessment of the personalised time series of synthetic images. We\nperformed three evaluations: i) image quality assessment; ii) quantifying the\naccuracy of regional brain volumes over and above benchmark models; and iii)\nquantifying visual perception of the synthetic images by medical experts.\nOverall, both quantitative and qualitative results show that 4D-DANI-Net\nproduces realistic, low-artefact, personalised time series of synthetic T1 MRI\nthat outperforms benchmark models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ravi_D/0/1/0/all/0/1\">Daniele Ravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumberg_S/0/1/0/all/0/1\">Stefano B. Blumberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ingala_S/0/1/0/all/0/1\">Silvia Ingala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barkhof_F/0/1/0/all/0/1\">Frederik Barkhof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oxtoby_N/0/1/0/all/0/1\">Neil P. Oxtoby</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LGVTON: A Landmark Guided Approach to Virtual Try-On. (arXiv:2004.00562v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.00562","description":"<p>In this paper, we propose a Landmark Guided Virtual Try-On (LGVTON) method\nfor clothes, which aims to solve the problem of clothing trials on e-commerce\nwebsites. Given the images of two people: a person and a model, it generates a\nrendition of the person wearing the clothes of the model. This is useful\nconsidering the fact that on most e-commerce websites images of only clothes\nare not usually available. We follow a three-stage approach to achieve our\nobjective. In the first stage, LGVTON warps the clothes of the model using a\nThin-Plate Spline (TPS) based transformation to fit the person. Unlike previous\nTPS-based methods, we use the landmarks (of human and clothes) to compute the\nTPS transformation. This enables the warping to work independently of the\ncomplex patterns, such as stripes, florals, and textures, present on the\nclothes. However, this computed warp may not always be very precise. We,\ntherefore, further refine it in the subsequent stages with the help of a mask\ngenerator (Stage 2) and an image synthesizer (Stage 3) modules. The mask\ngenerator improves the fit of the warped clothes, and the image synthesizer\nensures a realistic output. To tackle the problem of lack of paired training\ndata, we resort to a self-supervised training strategy. Here paired data refers\nto the image pair of model and person wearing the same cloth. We compare LGVTON\nwith four existing methods on two popular fashion datasets namely MPV and\nDeepFashion using two performance measures, FID (Fr\\'echet Inception Distance)\nand SSIM (Structural Similarity Index). The proposed method in most cases\noutperforms the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Debapriya Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santra_S/0/1/0/all/0/1\">Sanchayan Santra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_B/0/1/0/all/0/1\">Bhabatosh Chanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Background-Aware Correlation Filters: Adaptive Context Modeling by Hand-Crafted and Deep RGB Features for Visual Tracking. (arXiv:2004.02932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.02932","description":"<p>In recent years, the background-aware correlation filters have achie-ved a\nlot of research interest in the visual target tracking. However, these methods\ncannot suitably model the target appearance due to the exploitation of\nhand-crafted features. On the other hand, the recent deep learning-based visual\ntracking methods have provided a competitive performance along with extensive\ncomputations. In this paper, an adaptive background-aware correlation\nfilter-based tracker is proposed that effectively models the target appearance\nby using either the histogram of oriented gradients (HOG) or convolutional\nneural network (CNN) feature maps. The proposed method exploits the fast 2D\nnon-maximum suppression (NMS) algorithm and the semantic information comparison\nto detect challenging situations. When the HOG-based response map is not\nreliable, or the context region has a low semantic similarity with prior\nregions, the proposed method constructs the CNN context model to improve the\ntarget region estimation. Furthermore, the rejection option allows the proposed\nmethod to update the CNN context model only on valid regions. Comprehensive\nexperimental results demonstrate that the proposed adaptive method clearly\noutperforms the accuracy and robustness of visual target tracking compared to\nthe state-of-the-art methods on the OTB-50, OTB-100, TC-128, UAV-123, and\nVOT-2015 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1\">Seyed Mojtaba Marvasti-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanei_Yakhdan_H/0/1/0/all/0/1\">Hossein Ghanei-Yakhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobFR: Benchmarking Adversarial Robustness on Face Recognition. (arXiv:2007.04118v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.04118","description":"<p>Face recognition (FR) has recently made substantial progress and achieved\nhigh accuracy on standard benchmarks. However, it has raised security concerns\nin enormous FR applications because deep CNNs are unusually vulnerable to\nadversarial examples, and it is still lack of a comprehensive robustness\nevaluation before a FR model is deployed in safety-critical scenarios. To\nfacilitate a better understanding of the adversarial vulnerability on FR, we\ndevelop an adversarial robustness evaluation library on FR named\n\\textbf{RobFR}, which serves as a reference for evaluating the robustness of\ndownstream tasks. Specifically, RobFR involves 15 popular naturally trained FR\nmodels, 9 models with representative defense mechanisms and 2 commercial FR API\nservices, to perform the robustness evaluation by using various adversarial\nattacks as an important surrogate. The evaluations are conducted under diverse\nadversarial settings in terms of dodging and impersonation, $\\ell_2$ and\n$\\ell_\\infty$, as well as white-box and black-box attacks. We further propose a\nlandmark-guided cutout (LGC) attack method to improve the transferability of\nadversarial examples for black-box attacks by considering the special\ncharacteristics of FR. Based on large-scale evaluations, the commercial FR API\nservices fail to exhibit acceptable performance on robustness evaluation, and\nwe also draw several important conclusions for understanding the adversarial\nrobustness of FR models and providing insights for the design of robust FR\nmodels. RobFR is open-source and maintains all extendable modules, i.e.,\n\\emph{Datasets}, \\emph{FR Models}, \\emph{Attacks\\&amp;Defenses}, and\n\\emph{Evaluations} at\n\\url{https://github.com/ShawnXYang/Face-Robustness-Benchmark}, which will be\ncontinuously updated to promote future research on robust FR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingcheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenjian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"flexgrid2vec: Learning Efficient Visual Representations Vectors. (arXiv:2007.15444v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.15444","description":"<p>We propose flexgrid2vec, a novel approach for image representation learning.\nExisting visual representation methods suffer from several issues, including\nthe need for highly intensive computation, the risk of losing in-depth\nstructural information and the specificity of the method to certain shapes or\nobjects. flexgrid2vec converts an image to a low-dimensional feature vector. We\nrepresent each image with a graph of flexible, unique node locations and edge\ndistances. flexgrid2vec is a multi-channel GCN that learns features of the most\nrepresentative image patches. We have investigated both spectral and\nnon-spectral implementations of the GCN node-embedding. Specifically, we have\nimplemented flexgrid2vec based on different node-aggregation methods, such as\nvector summation, concatenation and normalisation with eigenvector centrality.\nWe compare the performance of flexgrid2vec with a set of state-of-the-art\nvisual representation learning models on binary and multi-class image\nclassification tasks. Although we utilise imbalanced, low-size and\nlow-resolution datasets, flexgrid2vec shows stable and outstanding results\nagainst well-known base classifiers. flexgrid2vec achieves 96.23% on CIFAR-10,\n83.05% on CIFAR-100, 94.50% on STL-10, 98.8% on ASIRRA and 89.69% on the COCO\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Ali Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Du Yong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automatic System to Monitor the Physical Distance and Face Mask Wearing of Construction Workers in COVID-19 Pandemic. (arXiv:2101.01373v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.01373","description":"<p>The COVID-19 pandemic has caused many shutdowns in different industries\naround the world. Sectors such as infrastructure construction and maintenance\nprojects have not been suspended due to their significant effect on people's\nroutine life. In such projects, workers work close together that makes a high\nrisk of infection. The World Health Organization recommends wearing a face mask\nand practicing physical distancing to mitigate the virus's spread. This paper\ndeveloped a computer vision system to automatically detect the violation of\nface mask wearing and physical distancing among construction workers to assure\ntheir safety on infrastructure projects during the pandemic. For the face mask\ndetection, the paper collected and annotated 1,000 images, including different\ntypes of face mask wearing, and added them to a pre-existing face mask dataset\nto develop a dataset of 1,853 images. Then trained and tested multiple\nTensorflow state-of-the-art object detection models on the face mask dataset\nand chose the Faster R-CNN Inception ResNet V2 network that yielded the\naccuracy of 99.8%. For physical distance detection, the paper employed the\nFaster R-CNN Inception V2 to detect people. A transformation matrix was used to\neliminate the camera angle's effect on the object distances on the image. The\nEuclidian distance used the pixels of the transformed image to compute the\nactual distance between people. A threshold of six feet was considered to\ncapture physical distance violation. The paper also used transfer learning for\ntraining the model. The final model was applied on four videos of road\nmaintenance projects in Houston, TX, that effectively detected the face mask\nand physical distance. We recommend that construction owners use the proposed\nsystem to enhance construction workers' safety in the pandemic situation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razavi_M/0/1/0/all/0/1\">Moein Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_H/0/1/0/all/0/1\">Hamed Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janfaza_V/0/1/0/all/0/1\">Vahid Janfaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_B/0/1/0/all/0/1\">Benyamin Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_E/0/1/0/all/0/1\">Ehsan Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-Guided Black-box Adversarial Attacks with Large-Scale Multiobjective Evolutionary Optimization. (arXiv:2101.07512v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07512","description":"<p>Fooling deep neural networks (DNNs) with the black-box optimization has\nbecome a popular adversarial attack fashion, as the structural prior knowledge\nof DNNs is always unknown. Nevertheless, recent black-box adversarial attacks\nmay struggle to balance their attack ability and visual quality of the\ngenerated adversarial examples (AEs) in tackling high-resolution images. In\nthis paper, we propose an attention-guided black-box adversarial attack based\non the large-scale multiobjective evolutionary optimization, termed as LMOA. By\nconsidering the spatial semantic information of images, we firstly take\nadvantage of the attention map to determine the perturbed pixels. Instead of\nattacking the entire image, reducing the perturbed pixels with the attention\nmechanism can help to avoid the notorious curse of dimensionality and thereby\nimproves the performance of attacking. Secondly, a large-scale multiobjective\nevolutionary algorithm is employed to traverse the reduced pixels in the\nsalient region. Benefiting from its characteristics, the generated AEs have the\npotential to fool target DNNs while being imperceptible by the human vision.\nExtensive experimental results have verified the effectiveness of the proposed\nLMOA on the ImageNet dataset. More importantly, it is more competitive to\ngenerate high-resolution AEs with better visual quality compared with the\nexisting black-box adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaoxia Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yang Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNETR: Transformers for 3D Medical Image Segmentation. (arXiv:2103.10504v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.10504","description":"<p>Fully Convolutional Neural Networks (FCNNs) with contracting and expanding\npaths have shown prominence for the majority of medical image segmentation\napplications since the past decade. In FCNNs, the encoder plays an integral\nrole by learning both global and local features and contextual representations\nwhich can be utilized for semantic output prediction by the decoder. Despite\ntheir success, the locality of convolutional layers in FCNNs, limits the\ncapability of learning long-range spatial dependencies. Inspired by the recent\nsuccess of transformers for Natural Language Processing (NLP) in long-range\nsequence learning, we reformulate the task of volumetric (3D) medical image\nsegmentation as a sequence-to-sequence prediction problem. We introduce a novel\narchitecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer\nas the encoder to learn sequence representations of the input volume and\neffectively capture the global multi-scale information, while also following\nthe successful \"U-shaped\" network design for the encoder and decoder. The\ntransformer encoder is directly connected to a decoder via skip connections at\ndifferent resolutions to compute the final semantic segmentation output. We\nhave validated the performance of our method on the Multi Atlas Labeling Beyond\nThe Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical\nSegmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation\ntasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV\nleaderboard. Code: https://monai.io/research/unetr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nath_V/0/1/0/all/0/1\">Vishwesh Nath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1\">Andriy Myronenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett Landman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Multi-Adaptive-Resolution-Surfel 6D LiDAR Odometry using Continuous-time Trajectory Optimization. (arXiv:2105.02010v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.02010","description":"<p>Simultaneous Localization and Mapping (SLAM) is an essential capability for\nautonomous robots, but due to high data rates of 3D LiDARs real-time SLAM is\nchallenging. We propose a real-time method for 6D LiDAR odometry. Our approach\ncombines a continuous-time B-Spline trajectory representation with a Gaussian\nMixture Model (GMM) formulation to jointly align local multi-resolution surfel\nmaps. Sparse voxel grids and permutohedral lattices ensure fast access to map\nsurfels, and an adaptive resolution selection scheme effectively speeds up\nregistration. A thorough experimental evaluation shows the performance of our\napproach on multiple datasets and during real-robot experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quenzel_J/0/1/0/all/0/1\">Jan Quenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09996","description":"<p>We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training. Code is\nmade available at https://github.com/pytorch/fairseq/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1\">Prahal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1\">Masoumeh Aminzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Seamless and High-Performance Out-of-Distribution Detection Approach Simply Replacing the SoftMax Loss. (arXiv:2105.14399v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14399","description":"<p>Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Besides being competitive with or\noutperforming all major current approaches, our solution avoids all their\ncurrent limitations in addition to being much easier to use, as just a simple\nloss replacement for training the neural network is required. Code available at\nhttps://github.com/dlmacedo/entropic-out-of-distribution-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Contextual Knowledge to Visual Features for Fine Art Classification. (arXiv:2105.15028v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.15028","description":"<p>Automatic art analysis has seen an ever-increasing interest from the pattern\nrecognition and computer vision community. However, most of the current work is\nmainly based solely on digitized artwork images, sometimes supplemented with\nsome metadata and textual comments. A knowledge graph that integrates a rich\nbody of information about artworks, artists, painting schools, etc., in a\nunified structured framework can provide a valuable resource for more powerful\ninformation retrieval and knowledge discovery tools in the artistic domain. To\nthis end, this paper presents ArtGraph: an artistic knowledge graph based on\nWikiArt and DBpedia. The graph, implemented in Neo4j, already provides\nknowledge discovery capabilities without having to train a learning system. In\naddition, the embeddings extracted from the graph are used to inject\n\"contextual\" knowledge into a deep learning model to improve the accuracy of\nartwork attribute prediction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castellano_G/0/1/0/all/0/1\">Giovanna Castellano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sansaro_G/0/1/0/all/0/1\">Giovanni Sansaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vessio_G/0/1/0/all/0/1\">Gennaro Vessio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Positional Contrastive Learning for Volumetric Medical Image Segmentation. (arXiv:2106.09157v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09157","description":"<p>The success of deep learning heavily depends on the availability of large\nlabeled training sets. However, it is hard to get large labeled datasets in\nmedical image domain because of the strict privacy concern and costly labeling\nefforts. Contrastive learning, an unsupervised learning technique, has been\nproved powerful in learning image-level representations from unlabeled data.\nThe learned encoder can then be transferred or fine-tuned to improve the\nperformance of downstream tasks with limited labels. A critical step in\ncontrastive learning is the generation of contrastive data pairs, which is\nrelatively simple for natural image classification but quite challenging for\nmedical image segmentation due to the existence of the same tissue or organ\nacross the dataset. As a result, when applied to medical image segmentation,\nmost state-of-the-art contrastive learning frameworks inevitably introduce a\nlot of false-negative pairs and result in degraded segmentation quality. To\naddress this issue, we propose a novel positional contrastive learning (PCL)\nframework to generate contrastive data pairs by leveraging the position\ninformation in volumetric medical images. Experimental results on CT and MRI\ndatasets demonstrate that the proposed PCL method can substantially improve the\nsegmentation performance compared to existing methods in both semi-supervised\nsetting and transfer learning setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yawen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinrong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haiyun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Meiping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jian Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingtong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToAlign: Task-oriented Alignment for Unsupervised Domain Adaptation. (arXiv:2106.10812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10812","description":"<p>Unsupervised domain adaptive classification intends to improve\ntheclassification performance on unlabeled target domain. To alleviate the\nadverse effect of domain shift, many approaches align the source and target\ndomains in the feature space. However, a feature is usually taken as a whole\nfor alignment without explicitly making domain alignment proactively serve the\nclassification task, leading to sub-optimal solution. What sub-feature should\nbe aligned for better adaptation is under-explored. In this paper, we propose\nan effective Task-oriented Alignment (ToAlign) for unsupervised domain\nadaptation (UDA). We study what features should be aligned across domains and\npropose to make the domain alignment proactively serve classification by\nperforming feature decomposition and alignment under the guidance of the prior\nknowledge induced from the classification taskitself. Particularly, we\nexplicitly decompose a feature in the source domain intoa\ntask-related/discriminative feature that should be aligned, and a\ntask-irrelevant feature that should be avoided/ignored, based on the\nclassification meta-knowledge. Extensive experimental results on various\nbenchmarks (e.g., Office-Home, Visda-2017, and DomainNet) under different\ndomain adaptation settings demonstrate theeffectiveness of ToAlign which helps\nachieve the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11930","description":"<p>In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features, and\nthe knowledge transfer between tasks. This is especially important when tasks\ncontain limited amount of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1\">Albin Soutif--Cormerais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost Van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GTNet:Guided Transformer Network for Detecting Human-Object Interactions. (arXiv:2108.00596v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00596","description":"<p>The human-object interaction (HOI) detection task refers to localizing\nhumans, localizing objects, and predicting the interactions between each\nhuman-object pair. HOI is considered one of the fundamental steps in truly\nunderstanding complex visual scenes. For detecting HOI, it is important to\nutilize relative spatial configurations and object semantics to find salient\nspatial regions of images that highlight the interactions between human object\npairs. This issue is addressed by the novel self-attention based guided\ntransformer network, GTNet. GTNet encodes this spatial contextual information\nin human and object visual features via self-attention while achieving state of\nthe art results on both the V-COCO and HICO-DET datasets. Code will be made\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iftekhar_A/0/1/0/all/0/1\">A S M Iftekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Satish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McEver_R/0/1/0/all/0/1\">R. Austin McEver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1\">B.S. Manjunath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Cut by Watching Movies. (arXiv:2108.04294v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04294","description":"<p>Video content creation keeps growing at an incredible pace; yet, creating\nengaging stories remains challenging and requires non-trivial video editing\nexpertise. Many video editing components are astonishingly hard to automate\nprimarily due to the lack of raw video materials. This paper focuses on a new\ntask for computational video editing, namely the task of raking cut\nplausibility. Our key idea is to leverage content that has already been edited\nto learn fine-grained audiovisual patterns that trigger cuts. To do this, we\nfirst collected a data source of more than 10K videos, from which we extract\nmore than 255K cuts. We devise a model that learns to discriminate between real\nand artificial cuts via contrastive learning. We set up a new task and a set of\nbaselines to benchmark video cut generation. We observe that our proposed model\noutperforms the baselines by large margins. To demonstrate our model in\nreal-world applications, we conduct human studies in a collection of unedited\nvideos. The results show that our model does a better job at cutting than\nrandom and alternative baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Information Theory-inspired Strategy for Automatic Network Pruning. (arXiv:2108.08532v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08532","description":"<p>Despite superior performance on many computer vision tasks, deep convolution\nneural networks are well known to be compressed on devices that have resource\nconstraints. Most existing network pruning methods require laborious human\nefforts and prohibitive computation resources, especially when the constraints\nare changed. This practically limits the application of model compression when\nthe model needs to be deployed on a wide range of devices. Besides, existing\nmethods are still challenged by the missing theoretical guidance. In this paper\nwe propose an information theory-inspired strategy for automatic model\ncompression. The principle behind our method is the information bottleneck\ntheory, i.e., the hidden representation should compress information with each\nother. We thus introduce the normalized Hilbert-Schmidt Independence Criterion\n(nHSIC) on network activations as a stable and generalized indicator of layer\nimportance. When a certain resource constraint is given, we integrate the HSIC\nindicator with the constraint to transform the architecture search problem into\na linear programming problem with quadratic constraints. Such a problem is\neasily solved by a convex optimization method with a few seconds. We also\nprovide a rigorous proof to reveal that optimizing the normalized HSIC\nsimultaneously minimizes the mutual information between different layers.\nWithout any search process, our method achieves better compression tradeoffs\ncomparing to the state-of-the-art compression algorithms. For instance, with\nResNet-50, we achieve a 45.3%-FLOPs reduction, with a 75.75 top-1 accuracy on\nImageNet. Codes are avaliable at\nhttps://github.com/MAC-AutoML/ITPruner/tree/master.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiawu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1\">Teng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Objective for Novel Class Discovery. (arXiv:2108.08536v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08536","description":"<p>In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims\nat inferring novel object categories in an unlabeled set by leveraging from\nprior knowledge of a labeled set containing different, but related classes.\nExisting approaches tackle this problem by considering multiple objective\nfunctions, usually involving specialized loss terms for the labeled and the\nunlabeled samples respectively, and often requiring auxiliary regularization\nterms. In this paper, we depart from this traditional scheme and introduce a\nUNified Objective function (UNO) for discovering novel classes, with the\nexplicit purpose of favoring synergy between supervised and unsupervised\nlearning. Using a multi-view self-labeling strategy, we generate pseudo-labels\nthat can be treated homogeneously with ground truth labels. This leads to a\nsingle classification objective operating on both known and unknown classes.\nDespite its simplicity, UNO outperforms the state of the art by a significant\nmargin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The\nproject page is available at: https://ncd-uno.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"External Knowledge enabled Text Visual Question Answering. (arXiv:2108.09717v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09717","description":"<p>The open-ended question answering task of Text-VQA requires reading and\nreasoning about local, often previously unseen, scene-text content of an image\nto generate answers. In this work, we propose the generalized use of external\nknowledge to augment our understanding of the said scene-text. We design a\nframework to extract, validate, and reason with knowledge using a standard\nmultimodal transformer for vision language understanding tasks. Through\nempirical evidence and qualitative results, we demonstrate how external\nknowledge can highlight instance-only cues and thus help deal with training\ndata bias, improve answer entity type correctness, and detect multiword named\nentities. We generate results comparable to the state-of-the-art on two\npublicly available datasets, under the constraints of similar upstream OCR\nsystems and training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arka Ujjal Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harit_G/0/1/0/all/0/1\">Gaurav Harit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 2nd Anti-UAV Workshop & Challenge: Methods and Results. (arXiv:2108.09909v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09909","description":"<p>The 2nd Anti-UAV Workshop \\&amp; Challenge aims to encourage research in\ndeveloping novel and accurate methods for multi-scale object tracking. The\nAnti-UAV dataset used for the Anti-UAV Challenge has been publicly released.\nThere are two subsets in the dataset, $i.e.$, the test-dev subset and\ntest-challenge subset. Both subsets consist of 140 thermal infrared video\nsequences, spanning multiple occurrences of multi-scale UAVs. Around 24\nparticipating teams from the globe competed in the 2nd Anti-UAV Challenge. In\nthis paper, we provide a brief summary of the 2nd Anti-UAV Workshop \\&amp;\nChallenge including brief introductions to the top three methods.The submission\nleaderboard will be reopened for researchers that are interested in the\nAnti-UAV challenge. The benchmark dataset and other information can be found\nat: https://anti-uav.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Nana Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_T/0/1/0/all/0/1\">Ting Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yafeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiming Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stain-Robust Mitotic Figure Detection for the Mitosis Domain Generalization Challenge. (arXiv:2109.00853v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00853","description":"<p>The detection of mitotic figures from different scanners/sites remains an\nimportant topic of research, owing to its potential in assisting clinicians\nwith tumour grading. The MItosis DOmain Generalization (MIDOG) challenge aims\nto test the robustness of detection models on unseen data from multiple\nscanners for this task. We present a short summary of the approach employed by\nthe TIA Centre team to address this challenge. Our approach is based on a\nhybrid detection model, where mitotic candidates are segmented on stain\nnormalised images, before being refined by a deep learning classifier.\nCross-validation on the training images achieved the F1-score of 0.786 and\n0.765 on the preliminary test set, demonstrating the generalizability of our\nmodel to unseen data from new scanners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shephard_A/0/1/0/all/0/1\">Adam Shephard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajeddin_N/0/1/0/all/0/1\">Neda Zamani Tajeddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashir_R/0/1/0/all/0/1\">R.M. Saad Bashir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilal_M/0/1/0/all/0/1\">Mohsin Bilal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurram_S/0/1/0/all/0/1\">Syed Ali Khurram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Cascade R-CNN for MItosis DOmain Generalization (MIDOG) Challenge. (arXiv:2109.00965v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.00965","description":"<p>We present a summary of the domain adaptive cascade R-CNN method for mitosis\ndetection of digital histopathology images. By comprehensive data augmentation\nand adapting existing popular detection architecture, our proposed method has\nachieved an F1 score of 0.7500 on the preliminary test set in MItosis DOmain\nGeneralization (MIDOG) Challenge at MICCAI 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Long_X/0/1/0/all/0/1\">Xi Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1\">Ying Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mu_X/0/1/0/all/0/1\">Xiao Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1\">Lian Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingxin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Mitosis Detection Using a Cascade Mask-RCNN Approach With Domain-Specific Residual Cycle-GAN Data Augmentation. (arXiv:2109.01878v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01878","description":"<p>For the MIDOG mitosis detection challenge, we created a cascade algorithm\nconsisting of a Mask-RCNN detector, followed by a classification ensemble\nconsisting of ResNet50 and DenseNet201 to refine detected mitotic candidates.\nThe MIDOG training data consists of 200 frames originating from four scanners,\nthree of which are annotated for mitotic instances with centroid annotations.\nOur main algorithmic choices are as follows: first, to enhance the\ngeneralizability of our detector and classification networks, we use a\nstate-of-the-art residual Cycle-GAN to transform each scanner domain to every\nother scanner domain. During training, we then randomly load, for each image,\none of the four domains. In this way, our networks can learn from the fourth\nnon-annotated scanner domain even if we don't have annotations for it. Second,\nfor training the detector network, rather than using centroid-based fixed-size\nbounding boxes, we create mitosis-specific bounding boxes. We do this by\nmanually annotating a small selection of mitoses, training a Mask-RCNN on this\nsmall dataset, and applying it to the rest of the data to obtain full\nannotations. We trained the follow-up classification ensemble using only the\nchallenge-provided positive and hard-negative examples. On the preliminary test\nset, the algorithm scores an F1 score of 0.7578, putting us as the second-place\nteam on the leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_G/0/1/0/all/0/1\">Gauthier Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dedieu_J/0/1/0/all/0/1\">Jules Dedieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertrand_C/0/1/0/all/0/1\">Capucine Bertrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshayedi_A/0/1/0/all/0/1\">Alireza Moshayedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mammadov_A/0/1/0/all/0/1\">Ali Mammadov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petit_S/0/1/0/all/0/1\">St&#xe9;phanie Petit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadj_S/0/1/0/all/0/1\">Saima Ben Hadj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fick_R/0/1/0/all/0/1\">Rutger H.J. Fick</a> (Tribvn Healthcare)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differential Diagnosis of Frontotemporal Dementia and Alzheimer's Disease using Generative Adversarial Network. (arXiv:2109.05627v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.05627","description":"<p>Frontotemporal dementia and Alzheimer's disease are two common forms of\ndementia and are easily misdiagnosed as each other due to their similar pattern\nof clinical symptoms. Differentiating between the two dementia types is crucial\nfor determining disease-specific intervention and treatment. Recent development\nof Deep-learning-based approaches in the field of medical image computing are\ndelivering some of the best performance for many binary classification tasks,\nalthough its application in differential diagnosis, such as neuroimage-based\ndifferentiation for multiple types of dementia, has not been explored. In this\nstudy, a novel framework was proposed by using the Generative Adversarial\nNetwork technique to distinguish FTD, AD and normal control subjects, using\nvolumetric features extracted at coarse-to-fine structural scales from Magnetic\nResonance Imaging scans. Experiments of 10-folds cross-validation on 1,954\nimages achieved high accuracy. With the proposed framework, we have\ndemonstrated that the combination of multi-scale structural features and\nsynthetic data augmentation based on generative adversarial network can improve\nthe performance of challenging tasks such as differentiating Dementia\nsub-types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Popuri_K/0/1/0/all/0/1\">Karteek Popuri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beg_M/0/1/0/all/0/1\">Mirza Faisal Beg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation. (arXiv:2109.07407v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07407","description":"<p>The success of deep learning methods in medical image segmentation tasks\nheavily depends on a large amount of labeled data to supervise the training. On\nthe other hand, the annotation of biomedical images requires domain knowledge\nand can be laborious. Recently, contrastive learning has demonstrated great\npotential in learning latent representation of images even without any label.\nExisting works have explored its application to biomedical image segmentation\nwhere only a small portion of data is labeled, through a pre-training phase\nbased on self-supervised contrastive learning without using any labels followed\nby a supervised fine-tuning phase on the labeled portion of data only. In this\npaper, we establish that by including the limited label in formation in the\npre-training phase, it is possible to boost the performance of contrastive\nlearning. We propose a supervised local contrastive loss that leverages limited\npixel-wise annotation to force pixels with the same label to gather around in\nthe embedding space. Such loss needs pixel-wise computation which can be\nexpensive for large images, and we further propose two strategies, downsampling\nand block division, to address the issue. We evaluate our methods on two public\nbiomedical image datasets of different modalities. With different amounts of\nlabeled data, our methods consistently outperform the state-of-the-art\ncontrast-based methods and other semi-supervised learning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinrong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mass Segmentation in Automated 3-D Breast Ultrasound Using Dual-Path U-net. (arXiv:2109.08330v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.08330","description":"<p>Automated 3-D breast ultrasound (ABUS) is a newfound system for breast\nscreening that has been proposed as a supplementary modality to mammography for\nbreast cancer detection. While ABUS has better performance in dense breasts,\nreading ABUS images is exhausting and time-consuming. So, a computer-aided\ndetection system is necessary for interpretation of these images. Mass\nsegmentation plays a vital role in the computer-aided detection systems and it\naffects the overall performance. Mass segmentation is a challenging task\nbecause of the large variety in size, shape, and texture of masses. Moreover,\nan imbalanced dataset makes segmentation harder. A novel mass segmentation\napproach based on deep learning is introduced in this paper. The deep network\nthat is used in this study for image segmentation is inspired by U-net, which\nhas been used broadly for dense segmentation in recent years. The system's\nperformance was determined using a dataset of 50 masses including 38 malign and\n12 benign lesions. The proposed segmentation method attained a mean Dice of\n0.82 which outperformed a two-stage supervised edge-based method with a mean\nDice of 0.74 and an adaptive region growing method with a mean Dice of 0.65.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fayyaz_H/0/1/0/all/0/1\">Hamed Fayyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kozegar_E/0/1/0/all/0/1\">Ehsan Kozegar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1\">Tao Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soryani_M/0/1/0/all/0/1\">Mohsen Soryani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. (arXiv:2109.09658v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09658","description":"<p>The recent advancements in artificial intelligence (AI) combined with the\nextensive amount of data generated by today's clinical systems, has led to the\ndevelopment of imaging AI solutions across the whole value chain of medical\nimaging, including image reconstruction, medical image segmentation,\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\nfuture potential of AI in medical imaging, many stakeholders are concerned of\nthe potential risks and ethical implications of imaging AI solutions, which are\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\nin critical clinical applications. Despite these concerns and risks, there are\ncurrently no concrete guidelines and best practices for guiding future AI\ndevelopments in medical imaging towards increased trust, safety and adoption.\nTo bridge this gap, this paper introduces a careful selection of guiding\nprinciples drawn from the accumulated experiences, consensus, and best\npractices from five large European projects on AI in Health Imaging. These\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\nand (vi) Explainability. In a step-by-step approach, these guidelines are\nfurther translated into a framework of concrete recommendations for specifying,\ndeveloping, evaluating, and deploying technically, clinically and ethically\ntrustworthy AI solutions into clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallin_C/0/1/0/all/0/1\">Catherine Gallin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazrak_N/0/1/0/all/0/1\">Noussair Lazrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakou_G/0/1/0/all/0/1\">Gianna Tsakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ausso_S/0/1/0/all/0/1\">Susanna Auss&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberich_L/0/1/0/all/0/1\">Leonor Cerd&#xe1; Alberich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marias_K/0/1/0/all/0/1\">Kostas Marias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiknakis_M/0/1/0/all/0/1\">Manolis Tsiknakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colantonio_S/0/1/0/all/0/1\">Sara Colantonio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papanikolaou_N/0/1/0/all/0/1\">Nickolas Papanikolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salahuddin_Z/0/1/0/all/0/1\">Zohaib Salahuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_H/0/1/0/all/0/1\">Henry C Woodruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambin_P/0/1/0/all/0/1\">Philippe Lambin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marti_Bonmati_L/0/1/0/all/0/1\">Luis Mart&#xed;-Bonmat&#xed;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressive Visual Representations. (arXiv:2109.12909v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.12909","description":"<p>Learning effective visual representations that generalize well without human\nsupervision is a fundamental problem in order to apply Machine Learning to a\nwide variety of tasks. Recently, two families of self-supervised methods,\ncontrastive learning and latent bootstrapping, exemplified by SimCLR and BYOL\nrespectively, have made significant progress. In this work, we hypothesize that\nadding explicit information compression to these algorithms yields better and\nmore robust representations. We verify this by developing SimCLR and BYOL\nformulations compatible with the Conditional Entropy Bottleneck (CEB)\nobjective, allowing us to both measure and control the amount of compression in\nthe learned representation, and observe their impact on downstream tasks.\nFurthermore, we explore the relationship between Lipschitz continuity and\ncompression, showing a tractable lower bound on the Lipschitz constant of the\nencoders we learn. As Lipschitz continuity is closely related to robustness,\nthis provides a new explanation for why compressed models are more robust. Our\nexperiments confirm that adding compression to SimCLR and BYOL significantly\nimproves linear evaluation accuracies and model robustness across a wide range\nof domain shifts. In particular, the compressed version of BYOL achieves 76.0%\nTop-1 linear evaluation accuracy on ImageNet with ResNet-50, and 78.8% with\nResNet-50 2x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuang-Huei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guadarrama_S/0/1/0/all/0/1\">Sergio Guadarrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John Canny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1\">Ian Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12925","description":"<p>Due to its role in many computer vision tasks, image matching has been\nsubjected to an active investigation by researchers, which has lead to better\nand more discriminant feature descriptors and to more robust matching\nstrategies, also thanks to the advent of the deep learning and the increased\ncomputational power of the modern hardware. Despite of these achievements, the\nkeypoint extraction process at the base of the image matching pipeline has not\nseen equivalent progresses. This paper presents Harrisz$^{+}$, an upgrade to\nthe HarrisZ corner detector, optimized to synergically take advance of the\nrecent improvements of the other steps of the image matching pipeline.\nHarrisz$^{+}$ does not only consists of a tuning of the setup parameters, but\nintroduces further refinements to the selection criteria delineated by HarrisZ,\nso providing more, yet discriminative, keypoints, which are better distributed\non the image and with higher localization accuracy. The image matching pipeline\nincluding Harrisz$^{+}$, together with the other modern components, obtained in\ndifferent recent matching benchmarks state-of-the-art results among the classic\nimage matching pipelines, closely following results of the more recent fully\ndeep end-to-end trainable approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1\">Fabio Bellavia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_D/0/1/0/all/0/1\">Dmytro Mishkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Contrastive Learning Approach to Auroral Identification and Classification. (arXiv:2109.13899v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.13899","description":"<p>Unsupervised learning algorithms are beginning to achieve accuracies\ncomparable to their supervised counterparts on benchmark computer vision tasks,\nbut their utility for practical applications has not yet been demonstrated. In\nthis work, we present a novel application of unsupervised learning to the task\nof auroral image classification. Specifically, we modify and adapt the Simple\nframework for Contrastive Learning of Representations (SimCLR) algorithm to\nlearn representations of auroral images in a recently released auroral image\ndataset constructed using image data from Time History of Events and Macroscale\nInteractions during Substorms (THEMIS) all-sky imagers. We demonstrate that (a)\nsimple linear classifiers fit to the learned representations of the images\nachieve state-of-the-art classification performance, improving the\nclassification accuracy by almost 10 percentage points over the current\nbenchmark; and (b) the learned representations naturally cluster into more\nclusters than exist manually assigned categories, suggesting that existing\ncategorizations are overly coarse and may obscure important connections between\nauroral types, near-earth solar wind conditions, and geomagnetic disturbances\nat the earth's surface. Moreover, our model is much lighter than the previous\nbenchmark on this dataset, requiring in the area of fewer than 25\\% of the\nnumber of parameters. Our approach exceeds an established threshold for\noperational purposes, demonstrating readiness for deployment and utilization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Jeremiah W. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hari_S/0/1/0/all/0/1\">Swathi Hari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hampton_D/0/1/0/all/0/1\">Donald Hampton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Connor_H/0/1/0/all/0/1\">Hyunju K. Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keesee_A/0/1/0/all/0/1\">Amy Keesee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDC-Net+: Enhanced Probabilistic Dense Correspondence Network. (arXiv:2109.13912v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.13912","description":"<p>Establishing robust and accurate correspondences between a pair of images is\na long-standing computer vision problem with numerous applications. While\nclassically dominated by sparse methods, emerging dense approaches offer a\ncompelling alternative paradigm that avoids the keypoint detection step.\nHowever, dense flow estimation is often inaccurate in the case of large\ndisplacements, occlusions, or homogeneous regions. In order to apply dense\nmethods to real-world applications, such as pose estimation, image\nmanipulation, or 3D reconstruction, it is therefore crucial to estimate the\nconfidence of the predicted matches.\n</p>\n<p>We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+,\ncapable of estimating accurate dense correspondences along with a reliable\nconfidence map. We develop a flexible probabilistic approach that jointly\nlearns the flow prediction and its uncertainty. In particular, we parametrize\nthe predictive distribution as a constrained mixture model, ensuring better\nmodelling of both accurate flow predictions and outliers. Moreover, we develop\nan architecture and an enhanced training strategy tailored for robust and\ngeneralizable uncertainty prediction in the context of self-supervised\ntraining. Our approach obtains state-of-the-art results on multiple challenging\ngeometric matching and optical flow datasets. We further validate the\nusefulness of our probabilistic confidence estimation for the tasks of pose\nestimation, 3D reconstruction, image-based localization, and image retrieval.\nCode and models are available at https://github.com/PruneTruong/DenseMatching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1\">Prune Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}