{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Privacy enabled Financial Text Classification using Differential Privacy and Federated Learning. (arXiv:2110.01643v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01643","description":"<p>Privacy is important considering the financial Domain as such data is highly\nconfidential and sensitive. Natural Language Processing (NLP) techniques can be\napplied for text classification and entity detection purposes in financial\ndomains such as customer feedback sentiment analysis, invoice entity detection,\ncategorisation of financial documents by type etc. Due to the sensitive nature\nof such data, privacy measures need to be taken for handling and training large\nmodels with such data. In this work, we propose a contextualized transformer\n(BERT and RoBERTa) based text classification model integrated with privacy\nfeatures such as Differential Privacy (DP) and Federated Learning (FL). We\npresent how to privately train NLP models and desirable privacy-utility\ntradeoffs and evaluate them on the Financial Phrase Bank dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1\">Zumrut Muftuoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rerunning OCR -- A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01661","description":"<p>Iterating with new and improved OCR solutions enforces decisions to be taken\nwhen it comes to targeting the right reprocessing candidates. This especially\napplies when the underlying data collection is of considerable size and rather\ndiverse in terms of fonts, languages, periods of publication and consequently\nOCR quality. This article captures the efforts of the National Library of\nLuxembourg to support those exact decisions. They are crucial in order to\nguarantee low computational overhead and reduced quality degradation risks,\ncombined with a more quantifiable OCR improvement. In particular, this work\nexplains the methodology of the library with respect to text block level\nquality assessment. As an extension of this technique, another contribution\ncomes in the form of a regression model that takes the enhancement potential of\na new OCR engine into account. They both mark promising approaches, especially\nfor cultural institutions dealing with historic data of lower quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. (arXiv:2110.01691v1 [cs.HC])","link":"http://arxiv.org/abs/2110.01691","description":"<p>Although large language models (LLMs) have demonstrated impressive potential\non simple tasks, their breadth of scope, lack of transparency, and insufficient\ncontrollability can make them less effective when assisting humans on more\ncomplex tasks. In response, we introduce the concept of Chaining LLM steps\ntogether, where the output of one step becomes the input for the next, thus\naggregating the gains per step. We first define a set of LLM primitive\noperations useful for Chain construction, then present an interactive system\nwhere users can modify these Chains, along with their intermediate results, in\na modular way. In a 20-person user study, we found that Chaining not only\nimproved the quality of task outcomes, but also significantly enhanced system\ntransparency, controllability, and sense of collaboration. Additionally, we saw\nthat users developed new ways of interacting with LLMs through Chains: they\nleveraged sub-tasks to calibrate model expectations, compared and contrasted\nalternative strategies by observing parallel downstream effects, and debugged\nunexpected model outputs by \"unit-testing\" sub-components of a Chain. In two\ncase studies, we further explore how LLM Chains may be used in future\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Carrie J. Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoEfication: Conditional Computation of Transformer Models for Efficient Inference. (arXiv:2110.01786v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01786","description":"<p>Transformer-based pre-trained language models can achieve superior\nperformance on most NLP tasks due to large parameter capacity, but also lead to\nhuge computation cost. Fortunately, we find by empirical study that, most\ninputs only activate a tiny ratio of neurons during inference. Hence, we\nexplore to accelerate large-model inference by conditional computation based on\nthe sparse activation phenomenon. We propose to transform a large model into\nits mixture-of-experts (MoE) version with equal model size, namely MoEfication.\nModel MoEfication consists of two steps: (1) splitting the parameters of\nfeed-forward neural networks (FFNs) into multiple parts as experts, and (2)\nbuilding expert routers to decide which experts will be used for each input. To\nfurther improve the performance of MoEfied models, we can also fine-tune the\nmodels on downstream tasks, namely parameter calibration. Experimental results\nshow that the MoEfied models can significantly reduce computation cost, e.g.,\nonly activating 20% FFN parameters of a 700-million-parameter model without\nperformance degradation on several downstream tasks including text\nclassification and reading comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts. (arXiv:2110.01799v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01799","description":"<p>Reviewing contracts is a time-consuming procedure that incurs large expenses\nto companies and social inequality to those who cannot afford it. In this work,\nwe propose \"document-level natural language inference (NLI) for contracts\", a\nnovel, real-world application of NLI that addresses such problems. In this\ntask, a system is given a set of hypotheses (such as \"Some obligations of\nAgreement may survive termination.\") and a contract, and it is asked to\nclassify whether each hypothesis is \"entailed by\", \"contradicting to\" or \"not\nmentioned by\" (neutral to) the contract as well as identifying \"evidence\" for\nthe decision as spans in the contract. We annotated and release the largest\ncorpus to date consisting of 607 annotated contracts. We then show that\nexisting models fail badly on our task and introduce a strong baseline, which\n(1) models evidence identification as multi-label classification over spans\ninstead of trying to predict start and end tokens, and (2) employs more\nsophisticated context segmentation for dealing with long documents. We also\nshow that linguistic characteristics of contracts, such as negations by\nexceptions, are contributing to the difficulty of this task and that there is\nmuch room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koreeda_Y/0/1/0/all/0/1\">Yuta Koreeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey On Neural Word Embeddings. (arXiv:2110.01804v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01804","description":"<p>Understanding human language has been a sub-challenge on the way of\nintelligent machines. The study of meaning in natural language processing (NLP)\nrelies on the distributional hypothesis where language elements get meaning\nfrom the words that co-occur within contexts. The revolutionary idea of\ndistributed representation for a concept is close to the working of a human\nmind in that the meaning of a word is spread across several neurons, and a loss\nof activation will only slightly affect the memory retrieval process.\n</p>\n<p>Neural word embeddings transformed the whole field of NLP by introducing\nsubstantial improvements in all NLP tasks. In this survey, we provide a\ncomprehensive literature review on neural word embeddings. We give theoretical\nfoundations and describe existing work by an interplay between word embeddings\nand language modelling. We provide broad coverage on neural word embeddings,\nincluding early word embeddings, embeddings targeting specific semantic\nrelations, sense embeddings, morpheme embeddings, and finally, contextual\nrepresentations. Finally, we describe benchmark datasets in word embeddings'\nperformance evaluation and downstream tasks along with the performance results\nof/due to word embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sezerer_E/0/1/0/all/0/1\">Erhan Sezerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekir_S/0/1/0/all/0/1\">Selma Tekir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation. (arXiv:2110.01811v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01811","description":"<p>Pre-training (PT) and back-translation (BT) are two simple and powerful\nmethods to utilize monolingual data for improving the model performance of\nneural machine translation (NMT). This paper takes the first step to\ninvestigate the complementarity between PT and BT. We introduce two probing\ntasks for PT and BT respectively and find that PT mainly contributes to the\nencoder module while BT brings more benefits to the decoder. Experimental\nresults show that PT and BT are nicely complementary to each other,\nestablishing state-of-the-art performances on the WMT16 English-Romanian and\nEnglish-Russian benchmarks. Through extensive analyses on sentence originality\nand word frequency, we also demonstrate that combining Tagged BT with PT is\nmore helpful to their complementarity, leading to better translation quality.\nSource code is freely available at https://github.com/SunbowLiu/PTvsBT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truth-Conditional Captioning of Time Series Data. (arXiv:2110.01839v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01839","description":"<p>In this paper, we explore the task of automatically generating natural\nlanguage descriptions of salient patterns in a time series, such as stock\nprices of a company over a week. A model for this task should be able to\nextract high-level patterns such as presence of a peak or a dip. While typical\ncontemporary neural models with attention mechanisms can generate fluent output\ndescriptions for this task, they often generate factually incorrect\ndescriptions. We propose a computational model with a truth-conditional\narchitecture which first runs small learned programs on the input time series,\nthen identifies the programs/patterns which hold true for the given input, and\nfinally conditions on only the chosen valid program (rather than the input time\nseries) to generate the output text description. A program in our model is\nconstructed from modules, which are small neural networks that are designed to\ncapture numerical patterns and temporal information. The modules are shared\nacross multiple programs, enabling compositionality as well as efficient\nlearning of module parameters. The modules, as well as the composition of the\nmodules, are unobserved in data, and we learn them in an end-to-end fashion\nwith the only training signal coming from the accompanying natural language\ntext descriptions. We find that the proposed model is able to generate\nhigh-precision captions even though we consider a small and simple space of\nmodule types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation Approaches in Natural Language Processing: A Survey. (arXiv:2110.01852v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01852","description":"<p>As an effective strategy, data augmentation (DA) alleviates data scarcity\nscenarios where deep learning techniques may fail. It is widely applied in\ncomputer vision then introduced to natural language processing and achieves\nimprovements in many tasks. One of the main focuses of the DA methods is to\nimprove the diversity of training data, thereby helping the model to better\ngeneralize to unseen testing data. In this survey, we frame DA methods into\nthree categories based on the diversity of augmented data, including\nparaphrasing, noising, and sampling. Our paper sets out to analyze DA methods\nin detail according to the above categories. Further, we also introduce their\napplications in NLP tasks as well as the challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR Rescoring and Confidence Estimation with ELECTRA. (arXiv:2110.01857v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01857","description":"<p>In automatic speech recognition (ASR) rescoring, the hypothesis with the\nfewest errors should be selected from the n-best list using a language model\n(LM). However, LMs are usually trained to maximize the likelihood of correct\nword sequences, not to detect ASR errors. We propose an ASR rescoring method\nfor directly detecting errors with ELECTRA, which is originally a pre-training\nmethod for NLP tasks. ELECTRA is pre-trained to predict whether each word is\nreplaced by BERT or not, which can simulate ASR error detection on large text\ncorpora. To make this pre-training closer to ASR error detection, we further\npropose an extended version of ELECTRA called phone-attentive ELECTRA\n(P-ELECTRA). In the pre-training of P-ELECTRA, each word is replaced by a\nphone-to-word conversion model, which leverages phone information to generate\nacoustically similar words. Since our rescoring method is optimized for\ndetecting errors, it can also be used for word-level confidence estimation.\nExperimental evaluations on the Librispeech and TED-LIUM2 corpora show that our\nrescoring method with ELECTRA is competitive with conventional rescoring\nmethods with faster inference. ELECTRA also performs better in confidence\nestimation than BERT because it can learn to detect inappropriate words not\nonly in fine-tuning but also in pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futami_H/0/1/0/all/0/1\">Hayato Futami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimura_M/0/1/0/all/0/1\">Masato Mimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakai_S/0/1/0/all/0/1\">Shinsuke Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Impact of Pre-trained Language Models on Dialog Evaluation. (arXiv:2110.01895v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01895","description":"<p>Recently, there is a surge of interest in applying pre-trained language\nmodels (Pr-LM) in automatic open-domain dialog evaluation. Pr-LMs offer a\npromising direction for addressing the multi-domain evaluation challenge. Yet,\nthe impact of different Pr-LMs on the performance of automatic metrics is not\nwell-understood. This paper examines 8 different Pr-LMs and studies their\nimpact on three typical automatic dialog evaluation metrics across three\ndifferent dialog evaluation benchmarks. Specifically, we analyze how the choice\nof Pr-LMs affects the performance of automatic metrics. Extensive correlation\nanalyses on each of the metrics are performed to assess the effects of\ndifferent Pr-LMs along various axes, including pre-training objectives, dialog\nevaluation criteria, model size, and cross-dataset robustness. This study\nserves as the first comprehensive assessment of the effects of different Pr-LMs\non automatic dialog evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrichs_T/0/1/0/all/0/1\">Thomas Friedrichs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT. (arXiv:2110.01900v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01900","description":"<p>Self-supervised speech representation learning methods like wav2vec 2.0 and\nHidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and\noffer good representations for numerous speech processing tasks. Despite the\nsuccess of these methods, they require large memory and high pre-training\ncosts, making them inaccessible for researchers in academia and small\ncompanies. Therefore, this paper introduces DistilHuBERT, a novel multi-task\nlearning framework to distill hidden representations from a HuBERT model\ndirectly. This method reduces HuBERT's size by 75% and 73% faster while\nretaining most performance in ten different tasks. Moreover, DistilHuBERT\nrequired little training time and data, opening the possibilities of\npre-training personal and on-device SSL models for speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sicilian Translator: A Recipe for Low-Resource NMT. (arXiv:2110.01938v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01938","description":"<p>With 17,000 pairs of Sicilian-English translated sentences, Arba Sicula\ndeveloped the first neural machine translator for the Sicilian language. Using\nsmall subword vocabularies, we trained small Transformer models with high\ndropout parameters and achieved BLEU scores in the upper 20s. Then we\nsupplemented our dataset with backtranslation and multilingual translation and\npushed our scores into the mid 30s. We also attribute our success to\nincorporating theoretical information in our dataset. Prior to training, we\nbiased the subword vocabulary towards the desinences one finds in a textbook.\nAnd we included textbook exercises in our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wdowiak_E/0/1/0/all/0/1\">Eryk Wdowiak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AraCOVID19-SSD: Arabic COVID-19 Sentiment and Sarcasm Detection Dataset. (arXiv:2110.01948v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01948","description":"<p>Coronavirus disease (COVID-19) is an infectious respiratory disease that was\nfirst discovered in late December 2019, in Wuhan, China, and then spread\nworldwide causing a lot of panic and death. Users of social networking sites\nsuch as Facebook and Twitter have been focused on reading, publishing, and\nsharing novelties, tweets, and articles regarding the newly emerging pandemic.\nA lot of these users often employ sarcasm to convey their intended meaning in a\nhumorous, funny, and indirect way making it hard for computer-based\napplications to automatically understand and identify their goal and the harm\nlevel that they can inflect. Motivated by the emerging need for annotated\ndatasets that tackle these kinds of problems in the context of COVID-19, this\npaper builds and releases AraCOVID19-SSD a manually annotated Arabic COVID-19\nsarcasm and sentiment detection dataset containing 5,162 tweets. To confirm the\npractical utility of the built dataset, it has been carefully analyzed and\ntested using several classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ameur_M/0/1/0/all/0/1\">Mohamed Seghir Hadj Ameur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliane_H/0/1/0/all/0/1\">Hassina Aliane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Objective Few-shot Learning for Fair Classification. (arXiv:2110.01951v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01951","description":"<p>In this paper, we propose a general framework for mitigating the disparities\nof the predicted classes with respect to secondary attributes within the data\n(e.g., race, gender etc.). Our proposed method involves learning a\nmulti-objective function that in addition to learning the primary objective of\npredicting the primary class labels from the data, also employs a\nclustering-based heuristic to minimize the disparities of the class label\ndistribution with respect to the cluster memberships, with the assumption that\neach cluster should ideally map to a distinct combination of attribute values.\nExperiments demonstrate effective mitigation of cognitive biases on a benchmark\ndataset without the use of annotations of secondary attribute values (the\nzero-shot case) or with the use of a small number of attribute value\nannotations (the few-shot case).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1\">Ishani Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1\">Procheta Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_D/0/1/0/all/0/1\">Debasis Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Transition System for End-to-End Opinion Role Labeling. (arXiv:2110.02001v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02001","description":"<p>Unified opinion role labeling (ORL) aims to detect all possible opinion\nstructures of `opinion-holder-target' in one shot, given a text. The existing\ntransition-based unified method, unfortunately, is subject to longer opinion\nterms and fails to solve the term overlap issue. Current top performance has\nbeen achieved by employing the span-based graph model, which however still\nsuffers from both high model complexity and insufficient interaction among\nopinions and roles. In this work, we investigate a novel solution by revisiting\nthe transition architecture, and augment it with a pointer network (PointNet).\nThe framework parses out all opinion structures in linear-time complexity,\nmeanwhile breaks through the limitation of any length of terms with PointNet.\nTo achieve the explicit opinion-role interactions, we further propose a unified\ndependency-opinion graph (UDOG), co-modeling the syntactic dependency structure\nand the partial opinion-role structure. We then devise a relation-centered\ngraph aggregator (RCGA) to encode the multi-relational UDOG, where the\nresulting high-order representations are used to promote the predictions in the\nvanilla transition system. Our model achieves new state-of-the-art results on\nthe MPQA benchmark. Analyses further demonstrate the superiority of our methods\non both efficacy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoodChem: A food-chemical relation extraction model. (arXiv:2110.02019v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02019","description":"<p>In this paper, we present FoodChem, a new Relation Extraction (RE) model for\nidentifying chemicals present in the composition of food entities, based on\ntextual information provided in biomedical peer-reviewed scientific literature.\nThe RE task is treated as a binary classification problem, aimed at identifying\nwhether the contains relation exists between a food-chemical entity pair. This\nis accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models.\nFor evaluation purposes, a novel dataset with annotated contains relations in\nfood-chemical entity pairs is generated, in a golden and silver version. The\nmodels are integrated into a voting scheme in order to produce the silver\nversion of the dataset which we use for augmenting the individual models, while\nthe manually annotated golden version is used for their evaluation. Out of the\nthree evaluated models, the BioBERT model achieves the best results, with a\nmacro averaged F1 score of 0.902 in the unbalanced augmentation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cenikj_G/0/1/0/all/0/1\">Gjorgjina Cenikj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seljak_B/0/1/0/all/0/1\">Barbara Korou&#x161;i&#x107; Seljak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eftimov_T/0/1/0/all/0/1\">Tome Eftimov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings. (arXiv:2110.02030v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02030","description":"<p>Semantic sentence embeddings are usually supervisedly built minimizing\ndistances between pairs of embeddings of sentences labelled as semantically\nsimilar by annotators. Since big labelled datasets are rare, in particular for\nnon-English languages, and expensive, recent studies focus on unsupervised\napproaches that require not-paired input sentences. We instead propose a\nlanguage-independent approach to build large datasets of pairs of informal\ntexts weakly similar, without manual human effort, exploiting Twitter's\nintrinsic powerful signals of relatedness: replies and quotes of tweets. We use\nthe collected pairs to train a Transformer model with triplet-like structures,\nand we test the generated embeddings on Twitter NLP similarity tasks (PIT and\nTURL) and STSb. We also introduce four new sentence ranking evaluation\nbenchmarks of informal texts, carefully extracted from the initial collections\nof tweets, proving not only that our best model learns classical Semantic\nTextual Similarity, but also excels on tasks where pairs of sentences are not\nexact paraphrases. Ablation studies reveal how increasing the corpus size\ninfluences positively the results, even at 2M samples, suggesting that bigger\ncollections of Tweets still do not contain redundant information about semantic\nsimilarities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giovanni_M/0/1/0/all/0/1\">Marco Di Giovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brambilla_M/0/1/0/all/0/1\">Marco Brambilla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions. (arXiv:2110.02035v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02035","description":"<p>In this paper we introduce the Food Drinks and groceries Images Multi Lingual\n(FooDI-ML) dataset. This dataset contains over 1.5M unique images and over 9.5M\nstore names, product names descriptions, and collection sections gathered from\nthe Glovo application. The data made available corresponds to food, drinks and\ngroceries products from 37 countries in Europe, the Middle East, Africa and\nLatin America. The dataset comprehends 33 languages, including 870K samples of\nlanguages of countries from Eastern Europe and Western Asia such as Ukrainian\nand Kazakh, which have been so far underrepresented in publicly available\nvisio-linguistic datasets. The dataset also includes widely spoken languages\nsuch as Spanish and English. To assist further research, we include a benchmark\nover the text-image retrieval task using ADAPT, a SotA existing technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olondriz_D/0/1/0/all/0/1\">David Amat Ol&#xf3;ndriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puigdevall_P/0/1/0/all/0/1\">Pon&#xe7; Palau Puigdevall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palau_A/0/1/0/all/0/1\">Adri&#xe0; Salvador Palau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ur-iw-hnt at GermEval 2021: An Ensembling Strategy with Multiple BERT Models. (arXiv:2110.02042v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02042","description":"<p>This paper describes our approach (ur-iw-hnt) for the Shared Task of\nGermEval2021 to identify toxic, engaging, and fact-claiming comments. We\nsubmitted three runs using an ensembling strategy by majority (hard) voting\nwith multiple different BERT models of three different types: German-based,\nTwitter-based, and multilingual models. All ensemble models outperform single\nmodels, while BERTweet is the winner of all individual models in every subtask.\nTwitter-based models perform better than GermanBERT models, and multilingual\nmodels perform worse but by a small margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hoai Nam Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1\">Udo Kruschwitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TENT: Text Classification Based on ENcoding Tree Learning. (arXiv:2110.02047v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02047","description":"<p>Text classification is a primary task in natural language processing (NLP).\nRecently, graph neural networks (GNNs) have developed rapidly and been applied\nto text classification tasks. Although more complex models tend to achieve\nbetter performance, research highly depends on the computing power of the\ndevice used. In this article, we propose TENT (https://github.com/Daisean/TENT)\nto obtain better text classification performance and reduce the reliance on\ncomputing power. Specifically, we first establish a dependency analysis graph\nfor each text and then convert each graph into its corresponding encoding tree.\nThe representation of the entire graph is obtained by updating the\nrepresentation of the non-leaf nodes in the encoding tree. Experimental results\nshow that our method outperforms other baselines on several datasets while\nhaving a simple structure and few parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">He Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Multi-lingual Tasks -- a Survey. (arXiv:2110.02052v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02052","description":"<p>These days different platforms such as social media provide their clients\nfrom different backgrounds and languages the possibility to connect and\nexchange information. It is not surprising anymore to see comments from\ndifferent languages in posts published by international celebrities or data\nproviders. In this era, understanding cross languages content and\nmultilingualism in natural language processing (NLP) are hot topics, and\nmultiple efforts have tried to leverage existing technologies in NLP to tackle\nthis challenging research problem. In this survey, we provide a comprehensive\noverview of the existing literature with a focus on transfer learning\ntechniques in multilingual tasks. We also identify potential opportunities for\nfurther research in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jafari_A/0/1/0/all/0/1\">Amir Reza Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidary_B/0/1/0/all/0/1\">Behnam Heidary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahbakhsh_R/0/1/0/all/0/1\">Reza Farahbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mostafa Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalili_M/0/1/0/all/0/1\">Mahdi Jalili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoiER: An Approach for Training more Reliable Fine-TunedDownstream Task Models. (arXiv:2110.02054v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02054","description":"<p>The recent development in pretrained language models trained in a\nself-supervised fashion, such as BERT, is driving rapid progress in the field\nof NLP. However, their brilliant performance is based on leveraging syntactic\nartifacts of the training data rather than fully understanding the intrinsic\nmeaning of language. The excessive exploitation of spurious artifacts causes a\nproblematic issue: The distribution collapse problem, which is the phenomenon\nthat the model fine-tuned on downstream tasks is unable to distinguish\nout-of-distribution (OOD) sentences while producing a high confidence score. In\nthis paper, we argue that distribution collapse is a prevalent issue in\npretrained language models and propose noise entropy regularisation (NoiER) as\nan efficient learning paradigm that solves the problem without auxiliary models\nand additional~data. The proposed approach improved traditional OOD detection\nevaluation metrics by 55% on average compared to the original fine-tuned\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Training Resources Insufficient? Predict First Then Explain!. (arXiv:2110.02056v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02056","description":"<p>Natural language free-text explanation generation is an efficient approach to\ntrain explainable language processing models for\ncommonsense-knowledge-requiring tasks. The most predominant form of these\nmodels is the explain-then-predict (EtP) structure, which first generates\nexplanations and uses them for making decisions. The performance of EtP models\nis highly dependent on that of the explainer by the nature of their structure.\nTherefore, large-sized explanation data are required to train a good explainer\nmodel. However, annotating explanations is expensive. Also, recent works reveal\nthat free-text explanations might not convey sufficient information for\ndecision making. These facts cast doubts on the effectiveness of EtP models. In\nthis paper, we argue that the predict-then-explain (PtE) architecture is a more\nefficient approach in terms of the modelling perspective. Our main contribution\nis twofold. First, we show that the PtE structure is the most data-efficient\napproach when explanation data are lacking. Second, we reveal that the PtE\nstructure is always more training-efficient than the EtP structure. We also\nprovide experimental results that confirm the theoretical advantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Prediction in NLP -- A survey. (arXiv:2110.02057v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02057","description":"<p>Over the last several years, the field of Structured prediction in NLP has\nhad seen huge advancements with sophisticated probabilistic graphical models,\nenergy-based networks, and its combination with deep learning-based approaches.\nThis survey provides a brief of major techniques in structured prediction and\nits applications in the NLP domains like parsing, sequence labeling, text\ngeneration, and sequence to sequence tasks. We also deep-dived into\nenergy-based and attention-based techniques in structured prediction,\nidentified some relevant open issues and gaps in the current state-of-the-art\nresearch, and have come up with some detailed ideas for future research in\nthese fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_C/0/1/0/all/0/1\">Chauhan Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biyani_N/0/1/0/all/0/1\">Naman Biyani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthar_N/0/1/0/all/0/1\">Nirmal P. Suthar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Prashant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1\">Priyanshu Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactively Generating Explanations for Transformer-based Language Models. (arXiv:2110.02058v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02058","description":"<p>Transformer language models are state-of-the-art in a multitude of NLP tasks.\nDespite these successes, their opaqueness remains problematic. Recent methods\naiming to provide interpretability and explainability to black-box models\nprimarily focus on post-hoc explanations of (sometimes spurious) input-output\ncorrelations. Instead, we emphasize using prototype networks directly\nincorporated into the model architecture and hence explain the reasoning\nprocess behind the network's decisions. Moreover, while our architecture\nperforms on par with several language models, it enables one to learn from user\ninteractions. This not only offers a better understanding of language models,\nbut uses human capabilities to incorporate knowledge outside of the rigid range\nof purely data-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1\">Felix Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tauchmann_C/0/1/0/all/0/1\">Christopher Tauchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Relational Graph based Heterogeneous Multi-Task Learning in Community Question Answering. (arXiv:2110.02059v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02059","description":"<p>Various data mining tasks have been proposed to study Community Question\nAnswering (CQA) platforms like Stack Overflow. The relatedness between some of\nthese tasks provides useful learning signals to each other via Multi-Task\nLearning (MTL). However, due to the high heterogeneity of these tasks, few\nexisting works manage to jointly solve them in a unified framework. To tackle\nthis challenge, we develop a multi-relational graph based MTL model called\nHeterogeneous Multi-Task Graph Isomorphism Network (HMTGIN) which efficiently\nsolves heterogeneous CQA tasks. In each training forward pass, HMTGIN embeds\nthe input CQA forum graph by an extension of Graph Isomorphism Network and skip\nconnections. The embeddings are then shared across all task-specific output\nlayers to compute respective losses. Moreover, two cross-task constraints based\non the domain knowledge about tasks' relationships are used to regularize the\njoint learning. In the evaluation, the embeddings are shared among different\ntask-specific output layers to make corresponding predictions. To the best of\nour knowledge, HMTGIN is the first MTL model capable of tackling CQA tasks from\nthe aspect of multi-relational graphs. To evaluate HMTGIN's effectiveness, we\nbuild a novel large-scale multi-relational graph CQA dataset with over two\nmillion nodes from Stack Overflow. Extensive experiments show that: $(1)$\nHMTGIN is superior to all baselines on five tasks; $(2)$ The proposed MTL\nstrategy and cross-task constraints have substantial advantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zizheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_H/0/1/0/all/0/1\">Haowen Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Ngo-Yin Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaxin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junpeng Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teach Me What to Say and I Will Learn What to Pick: Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models. (arXiv:2110.02067v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02067","description":"<p>Knowledge Grounded Conversation Models (KGCM) are usually based on a\nselection/retrieval module and a generation module, trained separately or\nsimultaneously, with or without having access to a gold knowledge option. With\nthe introduction of large pre-trained generative models, the selection and\ngeneration part have become more and more entangled, shifting the focus towards\nenhancing knowledge incorporation (from multiple sources) instead of trying to\npick the best knowledge option. These approaches however depend on knowledge\nlabels and/or a separate dense retriever for their best performance. In this\nwork we study the unsupervised selection abilities of pre-trained generative\nmodels (e.g. BART) and show that by adding a score-and-aggregate module between\nencoder and decoder, they are capable of learning to pick the proper knowledge\nthrough minimising the language modelling loss (i.e. without having access to\nknowledge labels). Trained as such, our model - K-Mine - shows competitive\nselection and generation performance against models that benefit from knowledge\nlabels and/or separate dense retriever.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1\">Ehsan Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1\">Maxime De Bruyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1\">Jeska Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1\">Walter Daelemans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPAD: An Optimized Policy-based Active Learning Framework for Document Content Analysis. (arXiv:2110.02069v1 [cs.IR])","link":"http://arxiv.org/abs/2110.02069","description":"<p>Documents are central to many business systems, and include forms, reports,\ncontracts, invoices or purchase orders. The information in documents is\ntypically in natural language, but can be organized in various layouts and\nformats. There have been recent spurt of interest in understanding document\ncontent with novel deep learning architectures. However, document understanding\ntasks need dense information annotations, which are costly to scale and\ngeneralize. Several active learning techniques have been proposed to reduce the\noverall budget of annotation while maintaining the performance of the\nunderlying deep learning model. However, most of these techniques work only for\nclassification problems. But content detection is a more complex task, and has\nbeen scarcely explored in active learning literature. In this paper, we propose\n\\textit{OPAD}, a novel framework using reinforcement policy for active learning\nin content detection tasks for documents. The proposed framework learns the\nacquisition function to decide the samples to be selected while optimizing\nperformance metrics that the tasks typically have. Furthermore, we extend to\nweak labelling scenarios to further reduce the cost of annotation\nsignificantly. We propose novel rewards to account for class imbalance and user\nfeedback in the annotation interface, to improve the active learning method. We\nshow superior performance of the proposed \\textit{OPAD} framework for active\nlearning for various tasks related to document understanding like layout\nparsing, object detection and named entity recognition. Ablation studies for\nhuman feedback and class imbalance rewards are presented, along with a\ncomparison of annotation times for different approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Sumit Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guda_B/0/1/0/all/0/1\">Bhanu Prakash Reddy Guda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaubey_A/0/1/0/all/0/1\">Ashutosh Chaubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_I/0/1/0/all/0/1\">Ishan Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Avanish Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaRLE: Natural Language Models using Reinforcement Learning with Emotion Feedback. (arXiv:2110.02148v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02148","description":"<p>Current research in dialogue systems is focused on conversational assistants\nworking on short conversations in either task-oriented or open domain settings.\nIn this paper, we focus on improving task-based conversational assistants\nonline, primarily those working on document-type conversations (e.g., emails)\nwhose contents may or may not be completely related to the assistant's task. We\npropose \"NARLE\" a deep reinforcement learning (RL) framework for improving the\nnatural language understanding (NLU) component of dialogue systems online\nwithout the need to collect human labels for customer data. The proposed\nsolution associates user emotion with the assistant's action and uses that to\nimprove NLU models using policy gradients. For two intent classification\nproblems, we empirically show that using reinforcement learning to fine tune\nthe pre-trained supervised learning models improves performance up to 43%.\nFurthermore, we demonstrate the robustness of the method to partial and noisy\nimplicit feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ruijie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1\">Soham Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greer_J/0/1/0/all/0/1\">Jeremiah Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Charles Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Impact of COVID-19 on Economy from the Perspective of Users Reviews. (arXiv:2110.02198v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02198","description":"<p>One of the most important incidents in the world in 2020 is the outbreak of\nthe Coronavirus. Users on social networks publish a large number of comments\nabout this event. These comments contain important hidden information of public\nopinion regarding this pandemic. In this research, a large number of\nCoronavirus-related tweets are considered and analyzed using natural language\nprocessing and information retrieval science. Initially, the location of the\ntweets is determined using a dictionary prepared through the Geo-Names\ngeographic database, which contains detailed and complete information of places\nsuch as city names, streets, and postal codes. Then, using a large dictionary\nprepared from the terms of economics, related tweets are extracted and\nsentiments corresponded to tweets are analyzed with the help of the RoBERTa\nlanguage-based model, which has high accuracy and good performance. Finally,\nthe frequency chart of tweets related to the economy and their sentiment scores\n(positive and negative tweets) is plotted over time for the entire world and\nthe top 10 economies. From the analysis of the charts, we learn that the reason\nfor publishing economic tweets is not only the increase in the number of people\ninfected with the Coronavirus but also imposed restrictions and lockdowns in\ncountries. The consequences of these restrictions include the loss of millions\nof jobs and the economic downturn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salmani_F/0/1/0/all/0/1\">Fatemeh Salmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_Nejad_H/0/1/0/all/0/1\">Hamed Vahdat-Nejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiabadi_H/0/1/0/all/0/1\">Hamideh Hajiabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Psuedolabels for training Sentiment Classifiers makes the model generalize better across datasets. (arXiv:2110.02200v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02200","description":"<p>The problem statement addressed in this work is : For a public sentiment\nclassification API, how can we set up a classifier that works well on different\ntypes of data, having limited ability to annotate data from across domains. We\nshow that given a large amount of unannotated data from across different\ndomains and pseudolabels on this dataset generated by a classifier trained on a\nsmall annotated dataset from one domain, we can train a sentiment classifier\nthat generalizes better across different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_N/0/1/0/all/0/1\">Natesh Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Muktabh Mayank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sense-Specific Static Embeddings using Contextualised Word Embeddings as a Proxy. (arXiv:2110.02204v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02204","description":"<p>Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Waypoint Models for Instruction-guided Navigation in Continuous Environments. (arXiv:2110.02207v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02207","description":"<p>Little inquiry has explicitly addressed the role of action spaces in\nlanguage-guided visual navigation -- either in terms of its effect on\nnavigation success or the efficiency with which a robotic agent could execute\nthe resulting trajectory. Building on the recently released VLN-CE setting for\ninstruction following in continuous environments, we develop a class of\nlanguage-conditioned waypoint prediction networks to examine this question. We\nvary the expressivity of these models to explore a spectrum between low-level\nactions and continuous waypoint prediction. We measure task performance and\nestimated execution time on a profiled LoCoBot robot. We find more expressive\nmodels result in simpler, faster to execute trajectories, but lower-level\nactions can achieve better navigation metrics by approximating shortest paths\nbetter. Further, our models outperform prior work in VLN-CE and set a new\nstate-of-the-art on the public leaderboard -- increasing success rate by 4%\nwith our best model on this challenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1\">Jacob Krantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Legal Approach to Hate Speech: Operationalizing the EU's Legal Framework against the Expression of Hatred as an NLP Task. (arXiv:2004.03422v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.03422","description":"<p>We propose a 'legal approach' to hate speech detection by operationalization\nof the decision as to whether a post is subject to criminal law into an NLP\ntask. Comparing existing regulatory regimes for hate speech, we base our\ninvestigation on the European Union's framework as it provides a widely\napplicable legal minimum standard. Accurately judging whether a post is\npunishable or not usually requires legal training. We show that, by breaking\nthe legal assessment down into a series of simpler sub-decisions, even\nlaypersons can annotate consistently. Based on a newly annotated dataset, our\nexperiments show that directly learning an automated model of punishable\ncontent is challenging. However, learning the two sub-tasks of `target group'\nand `targeting conduct' instead of an end-to-end approach to punishability\nyields better results. Overall, our method also provides decisions that are\nmore transparent than those of end-to-end models, which is a crucial point in\nlegal decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zufall_F/0/1/0/all/0/1\">Frederike Zufall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamacher_M/0/1/0/all/0/1\">Marius Hamacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloppenborg_K/0/1/0/all/0/1\">Katharina Kloppenborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zesch_T/0/1/0/all/0/1\">Torsten Zesch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriched Pre-trained Transformers for Joint Slot Filling and Intent Detection. (arXiv:2004.14848v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.14848","description":"<p>Detecting the user's intent and finding the corresponding slots among the\nutterance's words are important tasks in natural language understanding. Their\ninterconnected nature makes their joint modeling a standard part of training\nsuch models. Moreover, data scarceness and specialized vocabularies pose\nadditional challenges. Recently, the advances in pre-trained language models,\nnamely contextualized models such as ELMo and BERT have revolutionized the\nfield by tapping the potential of training very large models with just a few\nsteps of fine-tuning on a task-specific dataset. Here, we leverage such models,\nnamely BERT and RoBERTa, and we design a novel architecture on top of them.\nMoreover, we propose an intent pooling attention mechanism, and we reinforce\nthe slot filling task by fusing intent distributions, word features, and token\nrepresentations. The experimental results on standard datasets show that our\nmodel outperforms both the current non-BERT state of the art as well as some\nstronger BERT-based baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset for Automatic Summarization of Russian News. (arXiv:2006.11063v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.11063","description":"<p>Automatic text summarization has been studied in a variety of domains and\nlanguages. However, this does not hold for the Russian language. To overcome\nthis issue, we present Gazeta, the first dataset for summarization of Russian\nnews. We describe the properties of this dataset and benchmark several\nextractive and abstractive models. We demonstrate that the dataset is a valid\ntask for methods of text summarization for Russian. Additionally, we prove the\npretrained mBART model to be useful for Russian text summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1\">Ilya Gusev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Book Success Prediction with Pretrained Sentence Embeddings and Readability Scores. (arXiv:2007.11073v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.11073","description":"<p>Predicting the potential success of a book in advance is vital in many\napplications. This could help both publishers and readers in their\ndecision-making process whether or not a book is worth publishing and reading,\nrespectively. In this paper, we propose a model that leverages pretrained\nsentence embeddings along with various readability scores for book success\nprediction. Unlike previous methods, the proposed method requires no\ncount-based, lexical, or syntactic features. Instead, we use a convolutional\nneural network over pretrained sentence embeddings and leverage different\nreadability scores through a simple concatenation operation. Our proposed model\noutperforms strong baselines for this task by as large as 6.4\\% F1-score\npoints. Moreover, our experiments show that according to our model, only the\nfirst 1K sentences are good enough to predict the potential success of books.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Aminul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The optimality of syntactic dependency distances. (arXiv:2007.15342v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.15342","description":"<p>It is often stated that human languages, as other biological systems, are\nshaped by cost-cutting pressures but, to what extent? Attempts to quantify the\ndegree of optimality of languages by means of an optimality score have been\nscarce and focused mostly on English. Here we recast the problem of the\noptimality of the word order of a sentence as an optimization problem on a\nspatial network where the vertices are words, arcs indicate syntactic\ndependencies and the space is defined by the linear order of the words in the\nsentence. We introduce a new score to quantify the cognitive pressure to reduce\nthe distance between linked words in a sentence. The analysis of sentences from\n93 languages representing 19 linguistic families reveals that half of languages\nare optimized to a 70% or more. The score indicates that distances are not\nsignificantly reduced in a few languages and confirms two theoretical\npredictions, i.e. that longer sentences are more optimized and that distances\nare more likely to be longer than expected by chance in short sentences. We\npresent a new hierarchical ranking of languages by their degree of\noptimization. The new score has implications for various fields of language\nresearch (dependency linguistics, typology, historical linguistics, clinical\nlinguistics and cognitive science). Finally, the principles behind the design\nof the score have implications for network science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Quality Assessment of Cognitive Behavioral Therapy Sessions Through Highly Contextualized Language Representations. (arXiv:2102.11573v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.11573","description":"<p>During a psychotherapy session, the counselor typically adopts techniques\nwhich are codified along specific dimensions (e.g., 'displays warmth and\nconfidence', or 'attempts to set up collaboration') to facilitate the\nevaluation of the session. Those constructs, traditionally scored by trained\nhuman raters, reflect the complex nature of psychotherapy and highly depend on\nthe context of the interaction. Recent advances in deep contextualized language\nmodels offer an avenue for accurate in-domain linguistic representations which\ncan lead to robust recognition and scoring of such psychotherapy-relevant\nbehavioral constructs, and support quality assurance and supervision. In this\nwork, we propose a BERT-based model for automatic behavioral scoring of a\nspecific type of psychotherapy, called Cognitive Behavioral Therapy (CBT),\nwhere prior work is limited to frequency-based language features and/or short\ntext excerpts which do not capture the unique elements involved in a\nspontaneous long conversational interaction. The model focuses on the\nclassification of therapy sessions with respect to the overall score achieved\non the widely-used Cognitive Therapy Rating Scale (CTRS), but is trained in a\nmulti-task manner in order to achieve higher interpretability. BERT-based\nrepresentations are further augmented with available therapy metadata,\nproviding relevant non-linguistic context and leading to consistent performance\nimprovements. We train and evaluate our models on a set of 1,118 real-world\ntherapy sessions, recorded and automatically transcribed. Our best model\nachieves an F1 score equal to 72.61% on the binary classification task of low\nvs. high total CTRS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Flemotomos_N/0/1/0/all/0/1\">Nikolaos Flemotomos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_V/0/1/0/all/0/1\">Victor R. Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuohao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creed_T/0/1/0/all/0/1\">Torrey A. Creed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkins_D/0/1/0/all/0/1\">David C. Atkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability. (arXiv:2103.07162v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07162","description":"<p>This paper investigates whether the power of the models pre-trained on text\ndata, such as BERT, can be transferred to general token sequence classification\napplications. To verify pre-trained models' transferability, we test the\npre-trained models on text classification tasks with meanings of tokens\nmismatches, and real-world non-text token sequence classification data,\nincluding amino acid, DNA, and music. We find that even on non-text data, the\nmodels pre-trained on text converge faster, perform better than the randomly\ninitialized models, and only slightly worse than the models using task-specific\nknowledge. We also find that the representations of the text and non-text\npre-trained models share non-trivial similarities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kao_W/0/1/0/all/0/1\">Wei-Tsung Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integer-only Zero-shot Quantization for Efficient Speech Recognition. (arXiv:2103.16827v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2103.16827","description":"<p>End-to-end neural network models achieve improved performance on various\nautomatic speech recognition (ASR) tasks. However, these models perform poorly\non edge hardware due to large memory and computation requirements. While\nquantizing model weights and/or activations to low-precision can be a promising\nsolution, previous research on quantizing ASR models is limited. In particular,\nthe previous approaches use floating-point arithmetic during inference and thus\nthey cannot fully exploit efficient integer processing units. Moreover, they\nrequire training/validation data during quantization, which may not be\navailable due to security/privacy concerns. To address these limitations, we\npropose an integer-only, zero shot quantization scheme for ASR models. In\nparticular, we generate synthetic data whose runtime statistics resemble the\nreal data, and we use it to calibrate models during quantization. We apply our\nmethod to quantize QuartzNet, Jasper, and Conformer and show negligible WER\nchange as compared to the full-precision baseline models, even without using\nany training data. Moreover, we achieve up to 2.35x speedup on a T4 GPU and 4x\ncompression rate, with a modest WER degradation of &lt;1% with INT8 quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_N/0/1/0/all/0/1\">Nicholas Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Patrick Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nrusimha_A/0/1/0/all/0/1\">Anirudda Nrusimha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_B/0/1/0/all/0/1\">Bohan Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_T/0/1/0/all/0/1\">Tianren Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Architectures and Training for Raw Waveform Feature Extraction in ASR. (arXiv:2104.04298v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2104.04298","description":"<p>With the success of neural network based modeling in automatic speech\nrecognition (ASR), many studies investigated acoustic modeling and learning of\nfeature extractors directly based on the raw waveform. Recently, one line of\nresearch has focused on unsupervised pre-training of feature extractors on\naudio-only data to improve downstream ASR performance. In this work, we\ninvestigate the usefulness of one of these front-end frameworks, namely\nwav2vec, in a setting without additional untranscribed data for hybrid ASR\nsystems. We compare this framework both to the manually defined standard\nGammatone feature set, as well as to features extracted as part of the acoustic\nmodel of an ASR system trained supervised. We study the benefits of using the\npre-trained feature extractor and explore how to additionally exploit an\nexisting acoustic model trained with different features. Finally, we\nsystematically examine combinations of the described features in order to\nfurther advance the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vieting_P/0/1/0/all/0/1\">Peter Vieting</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Michel_W/0/1/0/all/0/1\">Wilfried Michel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Dialog Generation with Fine-Grained Intents. (arXiv:2105.06829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06829","description":"<p>Empathetic dialog generation aims at generating coherent responses following\nprevious dialog turns and, more importantly, showing a sense of caring and a\ndesire to help. Existing models either rely on pre-defined emotion labels to\nguide the response generation, or use deterministic rules to decide the emotion\nof the response. With the advent of advanced language models, it is possible to\nlearn subtle interactions directly from the dataset, providing that the emotion\ncategories offer sufficient nuances and other non-emotional but emotional\nregulating intents are included. In this paper, we describe how to incorporate\na taxonomy of 32 emotion categories and 8 additional emotion regulating intents\nto succeed the task of empathetic response generation. To facilitate the\ntraining, we also curated a large-scale emotional dialog dataset from movie\nsubtitles. Through a carefully designed crowdsourcing experiment, we evaluated\nand demonstrated how our model produces more empathetic dialogs compared with\nits baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yubo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence. (arXiv:2107.02173v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.02173","description":"<p>Topic model evaluation, like evaluation of other unsupervised methods, can be\ncontentious. However, the field has coalesced around automated estimates of\ntopic coherence, which rely on the frequency of word co-occurrences in a\nreference corpus. Recent models relying on neural components surpass classical\ntopic models according to these metrics. At the same time, unlike classical\nmodels, the practice of neural topic model evaluation suffers from a validation\ngap: automatic coherence for neural models has not been validated using human\nexperimentation. In addition, as we show via a meta-analysis of topic modeling\nliterature, there is a substantial standardization gap in the use of automated\ntopic modeling benchmarks. We address both the standardization gap and the\nvalidation gap. Using two of the most widely used topic model evaluation\ndatasets, we assess a dominant classical model and two state-of-the-art neural\nmodels in a systematic, clearly documented, reproducible way. We use automatic\ncoherence along with the two most widely accepted human judgment tasks, namely,\ntopic rating and word intrusion. Automated evaluation will declare one model\nsignificantly different from another when corresponding human evaluations do\nnot, calling into question the validity of fully automatic evaluations\nindependent of human judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoyle_A/0/1/0/all/0/1\">Alexander Hoyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Pranav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peskov_D/0/1/0/all/0/1\">Denis Peskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hian_Cheong_A/0/1/0/all/0/1\">Andrew Hian-Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnik_P/0/1/0/all/0/1\">Philip Resnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuned Language Models Are Zero-Shot Learners. (arXiv:2109.01652v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01652","description":"<p>This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially boosts zero-shot performance on unseen tasks.\n</p>\n<p>We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof tasks and model scale are key components to the success of instruction\ntuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Y. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12584","description":"<p>In recent times, there has been definitive progress in the field of NLP, with\nits applications growing as the utility of our language models increases with\nadvances in their performance. However, these models require a large amount of\ncomputational power and data to train, consequently leading to large carbon\nfootprints. Therefore, is it imperative that we study the carbon efficiency and\nlook for alternatives to reduce the overall environmental impact of training\nmodels, in particular large language models. In our work, we assess the\nperformance of models for machine translation, across multiple language pairs\nto assess the difference in computational power required to train these models\nfor each of these language pairs and examine the various components of these\nmodels to analyze aspects of our pipeline that can be optimized to reduce these\ncarbon emissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_M/0/1/0/all/0/1\">Mirza Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surana_P/0/1/0/all/0/1\">Praatibh Surana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFAQ: a Multilingual FAQ Dataset. (arXiv:2109.12870v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12870","description":"<p>In this paper, we present the first multilingual FAQ dataset publicly\navailable. We collected around 6M FAQ pairs from the web, in 21 different\nlanguages. Although this is significantly larger than existing FAQ retrieval\ndatasets, it comes with its own challenges: duplication of content and uneven\ndistribution of topics. We adopt a similar setup as Dense Passage Retrieval\n(DPR) and test various bi-encoders on this dataset. Our experiments reveal that\na multilingual model based on XLM-RoBERTa achieves the best results, except for\nEnglish. Lower resources languages seem to learn from one another as a\nmultilingual model achieves a higher MRR than language-specific ones. Our\nqualitative analysis reveals the brittleness of the model on simple word\nchanges. We publicly release our dataset, model and training script.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1\">Maxime De Bruyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1\">Ehsan Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1\">Jeska Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1\">Walter Daelemans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems. (arXiv:2109.14895v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14895","description":"<p>In translating text where sentiment is the main message, human translators\ngive particular attention to sentiment-carrying words. The reason is that an\nincorrect translation of such words would miss the fundamental aspect of the\nsource text, i.e. the author's sentiment. In the online world, MT systems are\nextensively used to translate User-Generated Content (UGC) such as reviews,\ntweets, and social media posts, where the main message is often the author's\npositive or negative attitude towards the topic of the text. It is important in\nsuch scenarios to accurately measure how far an MT system can be a reliable\nreal-life utility in transferring the correct affect message. This paper\ntackles an under-recognised problem in the field of machine translation\nevaluation which is judging to what extent automatic metrics concur with the\ngold standard of human evaluation for a correct translation of sentiment. We\nevaluate the efficacy of conventional quality metrics in spotting a\nmistranslation of sentiment, especially when it is the sole error in the MT\noutput. We propose a numerical `sentiment-closeness' measure appropriate for\nassessing the accuracy of a translated affect message in UGC text by an MT\nsystem. We will show that incorporating this sentiment-aware measure can\nsignificantly enhance the correlation of some available quality metrics with\nthe human judgement of an accurate translation of sentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Orasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Emad Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantawy_A/0/1/0/all/0/1\">Ashraf Tantawy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional generalization in semantic parsing with pretrained transformers. (arXiv:2109.15101v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.15101","description":"<p>Large-scale pretraining instills large amounts of knowledge in deep neural\nnetworks. This, in turn, improves the generalization behavior of these models\nin downstream tasks. What exactly are the limits to the generalization benefits\nof large-scale pretraining? Here, we report observations from some simple\nexperiments aimed at addressing this question in the context of two semantic\nparsing tasks involving natural language, SCAN and COGS. We show that language\nmodels pretrained exclusively with non-English corpora, or even with\nprogramming language corpora, significantly improve out-of-distribution\ngeneralization in these benchmarks, compared with models trained from scratch,\neven though both benchmarks are English-based. This demonstrates the\nsurprisingly broad transferability of pretrained representations and knowledge.\nPretraining with a large-scale protein sequence prediction task, on the other\nhand, mostly deteriorates the generalization performance in SCAN and COGS,\nsuggesting that pretrained representations do not transfer universally and that\nthere are constraints on the similarity between the pretraining and downstream\ndomains for successful transfer. Finally, we show that larger models are harder\nto train from scratch and their generalization accuracy is lower when trained\nup to convergence on the relatively small SCAN and COGS datasets, but the\nbenefits of large-scale pretraining become much clearer with larger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1\">A. Emin Orhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts. (arXiv:2110.01159v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01159","description":"<p>Recent models in developing summarization systems consist of millions of\nparameters and the model performance is highly dependent on the abundance of\ntraining data. While most existing summarization corpora contain data in the\norder of thousands to one million, generation of large-scale summarization\ndatasets in order of couple of millions is yet to be explored. Practically,\nmore data is better at generalizing the training patterns to unseen data. In\nthis paper, we introduce TLDR9+ -- a large-scale summarization dataset --\ncontaining over 9 million training instances extracted from Reddit discussion\nforum (https://github.com/sajastu/reddit_collector). This dataset is\nspecifically gathered to perform extreme summarization (i.e., generating\none-sentence summary in high compression and abstraction) and is more than\ntwice larger than the previously proposed dataset. We go one step further and\nwith the help of human annotations, we distill a more fine-grained dataset by\nsampling High-Quality instances from TLDR9+ and call it TLDRHQ dataset. We\nfurther pinpoint different state-of-the-art summarization models on our\nproposed datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sotudeh_S/0/1/0/all/0/1\">Sajad Sotudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deilamsalehy_H/0/1/0/all/0/1\">Hanieh Deilamsalehy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1\">Nazli Goharian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LawSum: A weakly supervised approach for Indian Legal Document Summarization. (arXiv:2110.01188v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01188","description":"<p>Unlike the courts in western countries, public records of Indian judiciary\nare completely unstructured and noisy. No large scale publicly available\nannotated datasets of Indian legal documents exist till date. This limits the\nscope for legal analytics research. In this work, we propose a new dataset\nconsisting of over 10,000 judgements delivered by the supreme court of India\nand their corresponding hand written summaries. The proposed dataset is\npre-processed by normalising common legal abbreviations, handling spelling\nvariations in named entities, handling bad punctuations and accurate sentence\ntokenization. Each sentence is tagged with their rhetorical roles. We also\nannotate each judgement with several attributes like date, names of the\nplaintiffs, defendants and the people representing them, judges who delivered\nthe judgement, acts/statutes that are cited and the most common citations used\nto refer the judgement. Further, we propose an automatic labelling technique\nfor identifying sentences which have summary worthy information. We demonstrate\nthat this auto labeled data can be used effectively to train a weakly\nsupervised sentence extractor with high accuracy. Some possible applications of\nthis dataset besides legal document summarization can be in retrieval, citation\nanalysis and prediction of decisions by a particular judge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_V/0/1/0/all/0/1\">Vedant Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_V/0/1/0/all/0/1\">Vidit Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metha_P/0/1/0/all/0/1\">Parth Metha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_N/0/1/0/all/0/1\">Namita Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorized Neural Transducer for Efficient Language Model Adaptation. (arXiv:2110.01500v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01500","description":"<p>In recent years, end-to-end (E2E) based automatic speech recognition (ASR)\nsystems have achieved great success due to their simplicity and promising\nperformance. Neural Transducer based models are increasingly popular in\nstreaming E2E based ASR systems and have been reported to outperform the\ntraditional hybrid system in some scenarios. However, the joint optimization of\nacoustic model, lexicon and language model in neural Transducer also brings\nabout challenges to utilize pure text for language model adaptation. This\ndrawback might prevent their potential applications in practice. In order to\naddress this issue, in this paper, we propose a novel model, factorized neural\nTransducer, by factorizing the blank and vocabulary prediction, and adopting a\nstandalone language model for the vocabulary prediction. It is expected that\nthis factorization can transfer the improvement of the standalone language\nmodel to the Transducer for speech recognition, which allows various language\nmodel adaptation techniques to be applied. We demonstrate that the proposed\nfactorized neural Transducer yields 15% to 20% WER improvements when\nout-of-domain text data is used for language model adaptation, at the cost of a\nminor degradation in WER on a general test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Sarangarajan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v2 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2110.00165","description":"<p>Self- and semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1\">Ananya Misra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddhartha_N/0/1/0/all/0/1\">Nikhil Siddhartha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CCS-GAN: COVID-19 CT-scan classification with very few positive training images. (arXiv:2110.01605v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01605","description":"<p>We present a novel algorithm that is able to classify COVID-19 pneumonia from\nCT Scan slices using a very small sample of training images exhibiting COVID-19\npneumonia in tandem with a larger number of normal images. This algorithm is\nable to achieve high classification accuracy using as few as 10 positive\ntraining slices (from 10 positive cases), which to the best of our knowledge is\none order of magnitude fewer than the next closest published work at the time\nof writing. Deep learning with extremely small positive training volumes is a\nvery difficult problem and has been an important topic during the COVID-19\npandemic, because for quite some time it was difficult to obtain large volumes\nof COVID-19 positive images for training. Algorithms that can learn to screen\nfor diseases using few examples are an important area of research. We present\nthe Cycle Consistent Segmentation Generative Adversarial Network (CCS-GAN).\nCCS-GAN combines style transfer with pulmonary segmentation and relevant\ntransfer learning from negative images in order to create a larger volume of\nsynthetic positive images for the purposes of improving diagnostic\nclassification performance. The performance of a VGG-19 classifier plus CCS-GAN\nwas trained using a small sample of positive image slices ranging from at most\n50 down to as few as 10 COVID-19 positive CT-scan images. CCS-GAN achieves high\naccuracy with few positive images and thereby greatly reduces the barrier of\nacquiring large training volumes in order to train a diagnostic classifier for\nCOVID-19.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Menon_S/0/1/0/all/0/1\">Sumeet Menon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mangalagiri_J/0/1/0/all/0/1\">Jayalakshmi Mangalagiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galita_J/0/1/0/all/0/1\">Josh Galita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morris_M/0/1/0/all/0/1\">Michael Morris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saboury_B/0/1/0/all/0/1\">Babak Saboury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yesha_Y/0/1/0/all/0/1\">Yaacov Yesha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yesha_Y/0/1/0/all/0/1\">Yelena Yesha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gangopadhyay_A/0/1/0/all/0/1\">Aryya Gangopadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chapman_D/0/1/0/all/0/1\">David Chapman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breast Cancer Diagnosis in Two-View Mammography Using End-to-End Trained EfficientNet-Based Convolutional Network. (arXiv:2110.01606v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01606","description":"<p>Some recent studies have described deep convolutional neural networks to\ndiagnose breast cancer in mammograms with similar or even superior performance\nto that of human experts. Shen et al. (2019) present one of the best techniques\nthat consists of two transfer learnings. The first uses a model trained on\nnatural images to create a \"patch classifier\" that categorizes small subimages.\nThe second uses the patch classifier to scan the whole mammogram and create the\n\"single-view whole-image classifier\". We propose to make a third transfer\nlearning to obtain a \"two-view classifier\" to use the two mammographic views:\nbilateral craniocaudal and mediolateral oblique. We use modern EfficientNet as\nthe basis of our model. We \"end-to-end\" train the entire system using CBIS-DDSM\ndataset. To ensure statistical robustness, we test our system twice using: (a)\n5-fold cross validation; and (b) the original training/test division of the\ndataset. Our technique reached an AUC of 0.934 using 5-fold cross validation\n(sensitivity and specificity are 85.13% at the equal error rate of ROC). Using\nthe original dataset division, our technique achieved an AUC of 0.8483, the\nlargest AUC reported for this problem, as far as we know.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Petrini_D/0/1/0/all/0/1\">Daniel G.P. Petrini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shimizu_C/0/1/0/all/0/1\">Carlos Shimizu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roela_R/0/1/0/all/0/1\">Rosimeire A. Roela</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valente_G/0/1/0/all/0/1\">Gabriel V. Valente</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Folgueira_M/0/1/0/all/0/1\">Maria A.A.K. Folgueira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hae Yong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Out-of-the-Box Frameworks for Unpaired Image Translation and Image Segmentation for the crossMoDA Challenge. (arXiv:2110.01607v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01607","description":"<p>The purpose of this study is to apply and evaluate out-of-the-box deep\nlearning frameworks for the crossMoDA challenge. We use the CUT model for\ndomain adaptation from contrast-enhanced T1 MR to high-resolution T2 MR. As\ndata augmentation, we generated additional images with vestibular schwannomas\nwith lower signal intensity. For the segmentation task, we use the nnU-Net\nframework. Our final submission achieved a mean Dice score of 0.8299 (0.0465)\nin the validation phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1\">Jae Won Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Experimental Evaluation on Deepfake Detection using Deep Face Recognition. (arXiv:2110.01640v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01640","description":"<p>Significant advances in deep learning have obtained hallmark accuracy rates\nfor various computer vision applications. However, advances in deep generative\nmodels have also led to the generation of very realistic fake content, also\nknown as deepfakes, causing a threat to privacy, democracy, and national\nsecurity. Most of the current deepfake detection methods are deemed as a binary\nclassification problem in distinguishing authentic images or videos from fake\nones using two-class convolutional neural networks (CNNs). These methods are\nbased on detecting visual artifacts, temporal or color inconsistencies produced\nby deep generative models. However, these methods require a large amount of\nreal and fake data for model training and their performance drops significantly\nin cross dataset evaluation with samples generated using advanced deepfake\ngeneration techniques. In this paper, we thoroughly evaluate the efficacy of\ndeep face recognition in identifying deepfakes, using different loss functions\nand deepfake generation techniques. Experimental investigations on challenging\nCeleb-DF and FaceForensics++ deepfake datasets suggest the efficacy of deep\nface recognition in identifying deepfakes over two-class CNNs and the ocular\nmodality. Reported results suggest a maximum Area Under Curve (AUC) of 0.98 and\nan Equal Error Rate (EER) of 7.1% in detecting deepfakes using face recognition\non the Celeb-DF dataset. This EER is lower by 16.6% compared to the EER\nobtained for the two-class CNN and the ocular modality on the Celeb-DF dataset.\nFurther on the FaceForensics++ dataset, an AUC of 0.99 and EER of 2.04% were\nobtained. The use of biometric facial recognition technology has the advantage\nof bypassing the need for a large amount of fake data for model training and\nobtaining better generalizability to evolving deepfake creation techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Sreeraj Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadimpalli_A/0/1/0/all/0/1\">Aakash Varma Nadimpalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rattani_A/0/1/0/all/0/1\">Ajita Rattani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Fairness of Ocular Biometrics Among Young, Middle-Aged, and Older Adults. (arXiv:2110.01641v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01641","description":"<p>A number of studies suggest bias of the face biometrics, i.e., face\nrecognition and soft-biometric estimation methods, across gender, race, and age\ngroups. There is a recent urge to investigate the bias of different biometric\nmodalities toward the deployment of fair and trustworthy biometric solutions.\nOcular biometrics has obtained increased attention from academia and industry\ndue to its high accuracy, security, privacy, and ease of use in mobile devices.\nA recent study in $2020$ also suggested the fairness of ocular-based user\nrecognition across males and females. This paper aims to evaluate the fairness\nof ocular biometrics in the visible spectrum among age groups; young, middle,\nand older adults. Thanks to the availability of the latest large-scale 2020\nUFPR ocular biometric dataset, with subjects acquired in the age range 18 - 79\nyears, to facilitate this study. Experimental results suggest the overall\nequivalent performance of ocular biometrics across gender and age groups in\nuser verification and gender classification. Performance difference for older\nadults at lower false match rate and young adults was noted at user\nverification and age classification, respectively. This could be attributed to\ninherent characteristics of the biometric data from these age groups impacting\nspecific applications, which suggest a need for advancement in sensor\ntechnology and software solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1\">Anoop Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almadan_A/0/1/0/all/0/1\">Ali Almadan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rattani_A/0/1/0/all/0/1\">Ajita Rattani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel-Level Bijective Matching for Video Object Segmentation. (arXiv:2110.01644v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01644","description":"<p>Semi-supervised video object segmentation (VOS) aims to track the designated\nobjects present in the initial frame of a video at the pixel level. To fully\nexploit the appearance information of an object, pixel-level feature matching\nis widely used in VOS. Conventional feature matching runs in a surjective\nmanner, i.e., only the best matches from the query frame to the reference frame\nare considered. Each location in the query frame refers to the optimal location\nin the reference frame regardless of how often each reference frame location is\nreferenced. This works well in most cases and is robust against rapid\nappearance variations, but may cause critical errors when the query frame\ncontains background distractors that look similar to the target object. To\nmitigate this concern, we introduce a bijective matching mechanism to find the\nbest matches from the query frame to the reference frame and vice versa. Before\nfinding the best matches for the query frame pixels, the optimal matches for\nthe reference frame pixels are first considered to prevent each reference frame\npixel from being overly referenced. As this mechanism operates in a strict\nmanner, i.e., pixels are connected if and only if they are the sure matches for\neach other, it can effectively eliminate background distractors. In addition,\nwe propose a mask embedding module to improve the existing mask propagation\nmethod. By embedding multiple historic masks with coordinate information, it\ncan effectively capture the position information of a target object.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Suhwan Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heansung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minjung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Sungjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VTAMIQ: Transformers for Attention Modulated Image Quality Assessment. (arXiv:2110.01655v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01655","description":"<p>Following the major successes of self-attention and Transformers for image\nanalysis, we investigate the use of such attention mechanisms in the context of\nImage Quality Assessment (IQA) and propose a novel full-reference IQA method,\nVision Transformer for Attention Modulated Image Quality (VTAMIQ). Our method\nachieves competitive or state-of-the-art performance on the existing IQA\ndatasets and significantly outperforms previous metrics in cross-database\nevaluations. Most patch-wise IQA methods treat each patch independently; this\npartially discards global information and limits the ability to model\nlong-distance interactions. We avoid this problem altogether by employing a\ntransformer to encode a sequence of patches as a single global representation,\nwhich by design considers interdependencies between patches. We rely on various\nattention mechanisms -- first with self-attention within the Transformer, and\nsecond with channel attention within our difference modulation network --\nspecifically to reveal and enhance the more salient features throughout our\narchitecture. With large-scale pre-training for both classification and IQA\ntasks, VTAMIQ generalizes well to unseen sets of images and distortions,\nfurther demonstrating the strength of transformer-based networks for vision\nmodelling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chubarau_A/0/1/0/all/0/1\">Andrei Chubarau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">James Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDR-cGAN: Single LDR to HDR Image Translation using Conditional GAN. (arXiv:2110.01660v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01660","description":"<p>The prime goal of digital imaging techniques is to reproduce the realistic\nappearance of a scene. Low Dynamic Range (LDR) cameras are incapable of\nrepresenting the wide dynamic range of the real-world scene. The captured\nimages turn out to be either too dark (underexposed) or too bright\n(overexposed). Specifically, saturation in overexposed regions makes the task\nof reconstructing a High Dynamic Range (HDR) image from single LDR image\nchallenging. In this paper, we propose a deep learning based approach to\nrecover details in the saturated areas while reconstructing the HDR image. We\nformulate this problem as an image-to-image (I2I) translation task. To this\nend, we present a novel conditional GAN (cGAN) based framework trained in an\nend-to-end fashion over the HDR-REAL and HDR-SYNTH datasets. Our framework uses\nan overexposed mask obtained from a pre-trained segmentation model to\nfacilitate the hallucination task of adding details in the saturated regions.\nWe demonstrate the effectiveness of the proposed method by performing an\nextensive quantitative and qualitative comparison with several state-of-the-art\nsingle-image HDR reconstruction techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raipurkar_P/0/1/0/all/0/1\">Prarabdh Raipurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_R/0/1/0/all/0/1\">Rohil Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Approach Protecting Privacy in Camera-Based Critical Applications. (arXiv:2110.01676v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01676","description":"<p>Many critical applications rely on cameras to capture video footage for\nanalytical purposes. This has led to concerns about these cameras accidentally\ncapturing more information than is necessary. In this paper, we propose a deep\nlearning approach towards protecting privacy in camera-based systems. Instead\nof specifying specific objects (e.g. faces) are privacy sensitive, our\ntechnique distinguishes between salient (visually prominent) and non-salient\nobjects based on the intuition that the latter is unlikely to be needed by the\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramajayam_G/0/1/0/all/0/1\">Gautham Ramajayam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chiu C. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lannan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How You Move Your Head Tells What You Do: Self-supervised Video Representation Learning with Egocentric Cameras and IMU Sensors. (arXiv:2110.01680v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01680","description":"<p>Understanding users' activities from head-mounted cameras is a fundamental\ntask for Augmented and Virtual Reality (AR/VR) applications. A typical approach\nis to train a classifier in a supervised manner using data labeled by humans.\nThis approach has limitations due to the expensive annotation cost and the\nclosed coverage of activity labels. A potential way to address these\nlimitations is to use self-supervised learning (SSL). Instead of relying on\nhuman annotations, SSL leverages intrinsic properties of data to learn\nrepresentations. We are particularly interested in learning egocentric video\nrepresentations benefiting from the head-motion generated by users' daily\nactivities, which can be easily obtained from IMU sensors embedded in AR/VR\ndevices. Towards this goal, we propose a simple but effective approach to learn\nvideo representation by learning to tell the corresponding pairs of video clip\nand head-motion. We demonstrate the effectiveness of our learned representation\nfor recognizing egocentric activities of people and dogs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsutsui_S/0/1/0/all/0/1\">Satoshi Tsutsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1\">Ruta Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridgeway_K/0/1/0/all/0/1\">Karl Ridgeway</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let there be a clock on the beach: Reducing Object Hallucination in Image Captioning. (arXiv:2110.01705v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01705","description":"<p>Explaining an image with missing or non-existent objects is known as object\nbias (hallucination) in image captioning. This behaviour is quite common in the\nstate-of-the-art captioning models which is not desirable by humans. To\ndecrease the object hallucination in captioning, we propose three simple yet\nefficient training augmentation method for sentences which requires no new\ntraining data or increase in the model size. By extensive analysis, we show\nthat the proposed methods can significantly diminish our models' object bias on\nhallucination metrics. Moreover, we experimentally demonstrate that our methods\ndecrease the dependency on the visual features. All of our code, configuration\nfiles and model weights will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdjointBackMapV2: Precise Reconstruction of Arbitrary CNN Unit's Activation via Adjoint Operators. (arXiv:2110.01736v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01736","description":"<p>Adjoint operators have been found to be effective in the exploration of CNN's\ninner workings [1]. However, the previous no-bias assumption restricted its\ngeneralization. We overcome the restriction via embedding input images into an\nextended normed space that includes bias in all CNN layers as part of the\nextended input space and propose an adjoint-operator-based algorithm that maps\nhigh-level weights back to the extended input space for reconstructing an\neffective hypersurface. Such hypersurface can be computed for an arbitrary unit\nin the CNN, and we prove that this reconstructed hypersurface, when multiplied\nby the original input (through an inner product), will precisely replicate the\noutput value of each unit. We show experimental results based on the CIFAR-10\ndataset that the proposed approach achieves near $0$ reconstruction error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1\">Qing Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_Y/0/1/0/all/0/1\">Yoonsuck Choe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Integrated System for Mobile Image-Based Dietary Assessment. (arXiv:2110.01754v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01754","description":"<p>Accurate assessment of dietary intake requires improved tools to overcome\nlimitations of current methods including user burden and measurement error.\nEmerging technologies such as image-based approaches using advanced machine\nlearning techniques coupled with widely available mobile devices present new\nopportunities to improve the accuracy of dietary assessment that is\ncost-effective, convenient and timely. However, the quality and quantity of\ndatasets are essential for achieving good performance for automated image\nanalysis. Building a large image dataset with high quality groundtruth\nannotation is a challenging problem, especially for food images as the\nassociated nutrition information needs to be provided or verified by trained\ndietitians with domain knowledge. In this paper, we present the design and\ndevelopment of a mobile, image-based dietary assessment system to capture and\nanalyze dietary intake, which has been deployed in both controlled-feeding and\ncommunity-dwelling dietary studies. Our system is capable of collecting high\nquality food images in naturalistic settings and provides groundtruth\nannotations for developing new computational approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zeman Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yue Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Runyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">Janine Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerr_D/0/1/0/all/0/1\">Deborah Kerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boushey_C/0/1/0/all/0/1\">Carol Boushey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bottom-up Hierarchical Classification Using Confusion-based Logit Compression. (arXiv:2110.01756v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01756","description":"<p>In this work, we propose a method to efficiently compute label posteriors of\na base flat classifier in the presence of few validation examples within a\nbottom-up hierarchical inference framework. A stand-alone validation set (not\nused to train the base classifier) is preferred for posterior estimation to\navoid overfitting the base classifier, however a small validation set limits\nthe number of features one can effectively use. We propose a simple, yet\nrobust, logit vector compression approach based on generalized logits and label\nconfusions for the task of label posterior estimation within the context of\nhierarchical classification. Extensive comparative experiments with other\ncompression techniques are provided across multiple sized validation sets, and\na comparison with related hierarchical classification approaches is also\nconducted. The proposed approach mitigates the problem of not having enough\nvalidation examples for reliable posterior estimation while maintaining strong\nhierarchical classification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jim Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilin_R/0/1/0/all/0/1\">Roman Ilin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantified Facial Expressiveness for Affective Behavior Analytics. (arXiv:2110.01758v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01758","description":"<p>The quantified measurement of facial expressiveness is crucial to analyze\nhuman affective behavior at scale. Unfortunately, methods for expressiveness\nquantification at the video frame-level are largely unexplored, unlike the\nstudy of discrete expression. In this work, we propose an algorithm that\nquantifies facial expressiveness using a bounded, continuous expressiveness\nscore using multimodal facial features, such as action units (AUs), landmarks,\nhead pose, and gaze. The proposed algorithm more heavily weights AUs with high\nintensities and large temporal changes. The proposed algorithm can compute the\nexpressiveness in terms of discrete expression, and can be used to perform\ntasks including facial behavior tracking and subjectivity quantification in\ncontext. Our results on benchmark datasets show the proposed algorithm is\neffective in terms of capturing temporal changes and expressiveness, measuring\nsubjective differences in context, and extracting useful insight.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1\">Md Taufeeq Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canavan_S/0/1/0/all/0/1\">Shaun Canavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proxy-bridged Image Reconstruction Network for Anomaly Detection in Medical Images. (arXiv:2110.01761v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01761","description":"<p>Anomaly detection in medical images refers to the identification of abnormal\nimages with only normal images in the training set. Most existing methods solve\nthis problem with a self-reconstruction framework, which tends to learn an\nidentity mapping and reduces the sensitivity to anomalies. To mitigate this\nproblem, in this paper, we propose a novel Proxy-bridged Image Reconstruction\nNetwork (ProxyAno) for anomaly detection in medical images. Specifically, we\nuse an intermediate proxy to bridge the input image and the reconstructed\nimage. We study different proxy types, and we find that the superpixel-image\n(SI) is the best one. We set all pixels' intensities within each superpixel as\ntheir average intensity, and denote this image as SI. The proposed ProxyAno\nconsists of two modules, a Proxy Extraction Module and an Image Reconstruction\nModule. In the Proxy Extraction Module, a memory is introduced to memorize the\nfeature correspondence for normal image to its corresponding SI, while the\nmemorized correspondence does not apply to the abnormal images, which leads to\nthe information loss for abnormal image and facilitates the anomaly detection.\nIn the Image Reconstruction Module, we map an SI to its reconstructed image.\nFurther, we crop a patch from the image and paste it on the normal SI to mimic\nthe anomalies, and enforce the network to reconstruct the normal image even\nwith the pseudo abnormal SI. In this way, our network enlarges the\nreconstruction error for anomalies. Extensive experiments on brain MR images,\nretinal OCT images and retinal fundus images verify the effectiveness of our\nmethod for both image-level and pixel-level anomaly detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1\">Kang Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_W/0/1/0/all/0/1\">Weixin Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengxin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jianlong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Procedure Planning in Instructional Videosvia Contextual Modeling and Model-based Policy Learning. (arXiv:2110.01770v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01770","description":"<p>Learning new skills by observing humans' behaviors is an essential capability\nof AI. In this work, we leverage instructional videos to study humans'\ndecision-making processes, focusing on learning a model to plan goal-directed\nactions in real-life videos. In contrast to conventional action recognition,\ngoal-directed actions are based on expectations of their outcomes requiring\ncausal knowledge of potential consequences of actions. Thus, integrating the\nenvironment structure with goals is critical for solving this task. Previous\nworks learn a single world model will fail to distinguish various tasks,\nresulting in an ambiguous latent space; planning through it will gradually\nneglect the desired outcomes since the global information of the future goal\ndegrades quickly as the procedure evolves. We address these limitations with a\nnew formulation of procedure planning and propose novel algorithms to model\nhuman behaviors through Bayesian Inference and model-based Imitation Learning.\nExperiments conducted on real-world instructional videos show that our method\ncan achieve state-of-the-art performance in reaching the indicated goals.\nFurthermore, the learned contextual information presents interesting features\nfor planning in a latent space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jing Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HighlightMe: Detecting Highlights from Human-Centric Videos. (arXiv:2110.01774v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01774","description":"<p>We present a domain- and user-preference-agnostic approach to detect\nhighlightable excerpts from human-centric videos. Our method works on the\ngraph-based representation of multiple observable human-centric modalities in\nthe videos, such as poses and faces. We use an autoencoder network equipped\nwith spatial-temporal graph convolutions to detect human activities and\ninteractions based on these modalities. We train our network to map the\nactivity- and interaction-based latent structural representations of the\ndifferent modalities to per-frame highlight scores based on the\nrepresentativeness of the frames. We use these scores to compute which frames\nto highlight and stitch contiguous frames to produce the excerpts. We train our\nnetwork on the large-scale AVA-Kinetics action dataset and evaluate it on four\nbenchmark video highlight datasets: DSH, TVSum, PHD2, and SumMe. We observe a\n4-12% improvement in the mean average precision of matching the human-annotated\nhighlights over state-of-the-art methods in these datasets, without requiring\nany user-provided preferences or dataset-specific fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrangeli_S/0/1/0/all/0/1\">Stefano Petrangeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Viswanathan Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Instance Segmentation with High-Resolution Automotive Radar. (arXiv:2110.01775v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01775","description":"<p>Automotive radar has been widely used in the modern advanced driver\nassistance systems (ADAS) and autonomous driving system as it provides reliable\nenvironmental perception in all-weather conditions with affordable cost.\nHowever, automotive radar usually only plays as an auxiliary sensor since it\nhardly supplies semantic and geometry information due to the sparsity of radar\ndetection points. Nonetheless, as development of high-resolution automotive\nradar in recent years, more advanced perception functionality like instance\nsegmentation which has only been well explored using Lidar point clouds,\nbecomes possible by using automotive radar. Its data comes with rich contexts\nsuch as Radar Cross Section (RCS) and micro-doppler effects which may\npotentially be pertinent, and sometimes can even provide detection when the\nfield of view is completely obscured. Therefore, the effective utilization of\nradar detection points data is an integral part of automotive perception. The\noutcome from instance segmentation could be seen as comparable result of\nclustering, and could be potentially used as the input of tracker for tracking\nthe targets. In this paper, we propose two efficient methods for instance\nsegmentation with radar detection points, one is implemented in an end-to-end\ndeep learning driven fashion using PointNet++ framework, and the other is based\non clustering of the radar detection points with semantic information. Both\napproaches can be further improved by implementing visual multi-layer\nperceptron (MLP). The effectiveness of the proposed methods is verified using\nexperimental results on the recent RadarScenes dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weiyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Liping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaPix: Domain Transfer for Semantic Segmentation by Meta Pixel Weighting. (arXiv:2110.01777v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01777","description":"<p>Training a deep neural model for semantic segmentation requires collecting a\nlarge amount of pixel-level labeled data. To alleviate the data scarcity\nproblem presented in the real world, one could utilize synthetic data whose\nlabel is easy to obtain. Previous work has shown that the performance of a\nsemantic segmentation model can be improved by training jointly with real and\nsynthetic examples with a proper weighting on the synthetic data. Such\nweighting was learned by a heuristic to maximize the similarity between\nsynthetic and real examples. In our work, we instead learn a pixel-level\nweighting of the synthetic data by meta-learning, i.e., the learning of\nweighting should only be minimizing the loss on the target task. We achieve\nthis by gradient-on-gradient technique to propagate the target loss back into\nthe parameters of the weighting model. The experiments show that our method\nwith only one single meta module can outperform a complicated combination of an\nadversarial feature alignment, a reconstruction loss, plus a hierarchical\nheuristic weighting at pixel, region and image levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1\">Yiren Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chongyang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Subspace analysing for Semi-Supervised multi-label classification of Diabetic Foot Ulcer. (arXiv:2110.01795v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01795","description":"<p>Diabetes is a global raising pandemic. Diabetes patients are at risk of\ndeveloping foot ulcer that usually leads to limb amputation. In order to\ndevelop a self monitoring mobile application, in this work, we propose a novel\ndeep subspace analysis pipeline for semi-supervised diabetic foot ulcer\nmulit-label classification. To avoid any chance of over-fitting, unlike recent\nstate of the art deep semi-supervised methods, the proposed pipeline dose not\ninclude any data augmentation. Whereas, after extracting deep features, in\norder to make the representation shift invariant, we employ variety of data\naugmentation methods on each image and generate an image-sets, which is then\nmapped into a linear subspace. Moreover, the proposed pipeline reduces the cost\nof retraining when more new unlabelled data become available. Thus, the first\nstage of the pipeline employs the concept of transfer learning for feature\nextraction purpose through modifying and retraining a deep convolutional\nnetwork architect known as Xception. Then, the output of a mid-layer is\nextracted to generate an image set representer of any given image with help of\ndata augmentation methods. At this stage, each image is transferred to a linear\nsubspace which is a point on a Grassmann Manifold topological space. Hence, to\nperform analyse them, the geometry of such manifold must be considered. As\nsuch, each labelled image is represented as a vector of distances to number of\nunlabelled images using geodesic distance on Grassmann manifold. Finally,\nRandom Forest is trained for multi-label classification of diabetic foot ulcer\nimages. The method is then evaluated on the blind test set provided by DFU2021\ncompetition, and the result considerable improvement compared to using\nclassical transfer learning with data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alavi_A/0/1/0/all/0/1\">Azadeh Alavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Perceptually Optimized Block Motion Estimates for Video Compression. (arXiv:2110.01805v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01805","description":"<p>Block based motion estimation is integral to inter prediction processes\nperformed in hybrid video codecs. Prevalent block matching based methods that\nare used to compute block motion vectors (MVs) rely on computationally\nintensive search procedures. They also suffer from the aperture problem, which\ncan worsen as the block size is reduced. Moreover, the block matching criteria\nused in typical codecs do not account for the resulting levels of perceptual\nquality of the motion compensated pictures that are created upon decoding.\nTowards achieving the elusive goal of perceptually optimized motion estimation,\nwe propose a search-free block motion estimation framework using a multi-stage\nconvolutional neural network, which is able to conduct motion estimation on\nmultiple block sizes simultaneously, using a triplet of frames as input. This\ncomposite block translation network (CBT-Net) is trained in a self-supervised\nmanner on a large database that we created from publicly available uncompressed\nvideo content. We deploy the multi-scale structural similarity (MS-SSIM) loss\nfunction to optimize the perceptual quality of the motion compensated predicted\nframes. Our experimental results highlight the computational efficiency of our\nproposed model relative to conventional block matching based motion estimation\nalgorithms, for comparable prediction errors. Further, when used to perform\ninter prediction in AV1, the MV predictions of the perceptually optimized model\nresult in average Bjontegaard-delta rate (BD-rate) improvements of -1.70% and\n-1.52% with respect to the MS-SSIM and Video Multi-Method Assessment Fusion\n(VMAF) quality metrics, respectively as compared to the block matching based\nmotion estimation system employed in the SVT-AV1 encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Paul_S/0/1/0/all/0/1\">Somdyuti Paul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norkin_A/0/1/0/all/0/1\">Andrey Norkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DA-DRN: Degradation-Aware Deep Retinex Network for Low-Light Image Enhancement. (arXiv:2110.01809v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01809","description":"<p>Images obtained in real-world low-light conditions are not only low in\nbrightness, but they also suffer from many other types of degradation, such as\ncolor distortion, unknown noise, detail loss and halo artifacts. In this paper,\nwe propose a Degradation-Aware Deep Retinex Network (denoted as DA-DRN) for\nlow-light image enhancement and tackle the above degradation. Based on Retinex\nTheory, the decomposition net in our model can decompose low-light images into\nreflectance and illumination maps and deal with the degradation in the\nreflectance during the decomposition phase directly. We propose a\nDegradation-Aware Module (DA Module) which can guide the training process of\nthe decomposer and enable the decomposer to be a restorer during the training\nphase without additional computational cost in the test phase. DA Module can\nachieve the purpose of noise removal while preserving detail information into\nthe illumination map as well as tackle color distortion and halo artifacts. We\nintroduce Perceptual Loss to train the enhancement network to generate the\nbrightness-improved illumination maps which are more consistent with human\nvisual perception. We train and evaluate the performance of our proposed model\nover the LOL real-world and LOL synthetic datasets, and we also test our model\nover several other frequently used datasets without Ground-Truth (LIME, DICM,\nMEF and NPE datasets). We conduct extensive experiments to demonstrate that our\napproach achieves a promising effect with good rubustness and generalization\nand outperforms many other state-of-the-art methods qualitatively and\nquantitatively. Our method only takes 7 ms to process an image with 600x400\nresolution on a TITAN Xp GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1\">Xinxu Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xianshi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Cheng Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yanlin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_K/0/1/0/all/0/1\">Kaifu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yongjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UHP-SOT: An Unsupervised High-Performance Single Object Tracker. (arXiv:2110.01812v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01812","description":"<p>An unsupervised online object tracking method that exploits both foreground\nand background correlations is proposed and named UHP-SOT (Unsupervised\nHigh-Performance Single Object Tracker) in this work. UHP-SOT consists of three\nmodules: 1) appearance model update, 2) background motion modeling, and 3)\ntrajectory-based box prediction. A state-of-the-art discriminative correlation\nfilters (DCF) based tracker is adopted by UHP-SOT as the first module. We point\nout shortcomings of using the first module alone such as failure in recovering\nfrom tracking loss and inflexibility in object box adaptation and then propose\nthe second and third modules to overcome them. Both are novel in single object\ntracking (SOT). We test UHP-SOT on two popular object tracking benchmarks,\nTB-50 and TB-100, and show that it outperforms all previous unsupervised SOT\nmethods, achieves a performance comparable with the best supervised\ndeep-learning-based SOT methods, and operates at a fast speed (i.e. 22.7-32.0\nFPS on a CPU).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Suya You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borel_Donohue_C/0/1/0/all/0/1\">Christoph C. Borel-Donohue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations. (arXiv:2110.01823v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01823","description":"<p>When compared to the image classification models, black-box adversarial\nattacks against video classification models have been largely understudied.\nThis could be possible because, with video, the temporal dimension poses\nsignificant additional challenges in gradient estimation. Query-efficient\nblack-box attacks rely on effectively estimated gradients towards maximizing\nthe probability of misclassifying the target video. In this work, we\ndemonstrate that such effective gradients can be searched for by parameterizing\nthe temporal structure of the search space with geometric transformations.\nSpecifically, we design a novel iterative algorithm Geometric TRAnsformed\nPerturbations (GEO-TRAP), for attacking video classification models. GEO-TRAP\nemploys standard geometric transformation operations to reduce the search space\nfor effective gradients into searching for a small group of parameters that\ndefine these operations. This group of parameters describes the geometric\nprogression of gradients, resulting in a reduced and structured search space.\nOur algorithm inherently leads to successful perturbations with surprisingly\nfew queries. For example, adversarial examples generated from GEO-TRAP have\nbetter attack success rates with ~73.55% fewer queries compared to the\nstate-of-the-art method for video adversarial attacks on the widely used Jester\ndataset. Overall, our algorithm exposes vulnerabilities of diverse video\nclassification models and achieves new state-of-the-art results under black-box\nsettings on two large datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shitong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1\">Srikanth Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep reinforcement learning for guidewire navigation in coronary artery phantom. (arXiv:2110.01840v1 [cs.RO])","link":"http://arxiv.org/abs/2110.01840","description":"<p>In percutaneous intervention for treatment of coronary plaques, guidewire\nnavigation is a primary procedure for stent delivery. Steering a flexible\nguidewire within coronary arteries requires considerable training, and the\nnon-linearity between the control operation and the movement of the guidewire\nmakes precise manipulation difficult. Here, we introduce a deep reinforcement\nlearning(RL) framework for autonomous guidewire navigation in a robot-assisted\ncoronary intervention. Using Rainbow, a segment-wise learning approach is\napplied to determine how best to accelerate training using human demonstrations\nwith deep Q-learning from demonstrations (DQfD), transfer learning, and weight\ninitialization. `State' for RL is customized as a focus window near the\nguidewire tip, and subgoals are placed to mitigate a sparse reward problem. The\nRL agent improves performance, eventually enabling the guidewire to reach all\nvalid targets in `stable' phase. Our framework opens anew direction in the\nautomation of robot-assisted intervention, providing guidance on RL in physical\nspaces involving mechanical fatigue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kweon_J/0/1/0/all/0/1\">Jihoon Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyunghwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chaehyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Hwi Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyoseok Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeeone Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_I/0/1/0/all/0/1\">Inwook Back</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Jae-Hyung Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_Y/0/1/0/all/0/1\">Youngjin Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaesoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Hak Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypernetworks for Continual Semi-Supervised Learning. (arXiv:2110.01856v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01856","description":"<p>Learning from data sequentially arriving, possibly in a non i.i.d. way, with\nchanging task distribution over time is called continual learning. Much of the\nwork thus far in continual learning focuses on supervised learning and some\nrecent works on unsupervised learning. In many domains, each task contains a\nmix of labelled (typically very few) and unlabelled (typically plenty) training\nexamples, which necessitates a semi-supervised learning approach. To address\nthis in a continual learning setting, we propose a framework for\nsemi-supervised continual learning called Meta-Consolidation for Continual\nSemi-Supervised Learning (MCSSL). Our framework has a hypernetwork that learns\nthe meta-distribution that generates the weights of a semi-supervised auxiliary\nclassifier generative adversarial network $(\\textit{Semi-ACGAN})$ as the base\nnetwork. We consolidate the knowledge of sequential tasks in the hypernetwork,\nand the base network learns the semi-supervised learning task. Further, we\npresent $\\textit{Semi-Split CIFAR-10}$, a new benchmark for continual\nsemi-supervised learning, obtained by modifying the $\\textit{Split CIFAR-10}$\ndataset, in which the tasks with labelled and unlabelled data arrive\nsequentially. Our proposed model yields significant improvements in the\ncontinual semi-supervised learning setting. We compare the performance of\nseveral existing continual learning approaches on the proposed continual\nsemi-supervised learning benchmark of the Semi-Split CIFAR-10 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brahma_D/0/1/0/all/0/1\">Dhanajit Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vinay Kumar Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency Aware Face Hallucination Generative Adversarial Network with Semantic Structural Constraint. (arXiv:2110.01880v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01880","description":"<p>In this paper, we address the issue of face hallucination. Most current face\nhallucination methods rely on two-dimensional facial priors to generate high\nresolution face images from low resolution face images. These methods are only\ncapable of assimilating global information into the generated image. Still\nthere exist some inherent problems in these methods; such as, local features,\nsubtle structural details and missing depth information in final output image.\nPresent work proposes a Generative Adversarial Network (GAN) based novel\nprogressive Face Hallucination (FH) network to address these issues present\namong current methods. The generator of the proposed model comprises of FH\nnetwork and two sub-networks, assisting FH network to generate high resolution\nimages. The first sub-network leverages on explicitly adding high frequency\ncomponents into the model. To explicitly encode the high frequency components,\nan auto encoder is proposed to generate high resolution coefficients of\nDiscrete Cosine Transform (DCT). To add three dimensional parametric\ninformation into the network, second sub-network is proposed. This network uses\na shape model of 3D Morphable Models (3DMM) to add structural constraint to the\nFH network. Extensive experimentation results in the paper shows that the\nproposed model outperforms the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shailza Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1\">Abhinav Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vinay Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"De-rendering Stylized Texts. (arXiv:2110.01890v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01890","description":"<p>Editing raster text is a promising but challenging task. We propose to apply\ntext vectorization for the task of raster text editing in display media, such\nas posters, web pages, or advertisements. In our approach, instead of applying\nimage transformation or generation in the raster domain, we learn a text\nvectorization model to parse all the rendering parameters including text,\nlocation, size, font, style, effects, and hidden background, then utilize those\nparameters for reconstruction and any editing task. Our text vectorization\ntakes advantage of differentiable text rendering to accurately reproduce the\ninput raster text in a resolution-free parametric format. We show in the\nexperiments that our approach can successfully parse text, styling, and\nbackground information in the unified model, and produces artifact-free text\nediting compared to a raster baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shimoda_W/0/1/0/all/0/1\">Wataru Shimoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_D/0/1/0/all/0/1\">Daichi Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_K/0/1/0/all/0/1\">Kota Yamaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RapidAI4EO: A Corpus for Higher Spatial and Temporal Reasoning. (arXiv:2110.01919v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01919","description":"<p>Under the sponsorship of the European Union Horizon 2020 program, RapidAI4EO\nwill establish the foundations for the next generation of Copernicus Land\nMonitoring Service (CLMS) products. The project aims to provide intensified\nmonitoring of Land Use (LU), Land Cover (LC), and LU change at a much higher\nlevel of detail and temporal cadence than it is possible today. Focus is on\ndisentangling phenology from structural change and in providing critical\ntraining data to drive advancement in the Copernicus community and ecosystem\nwell beyond the lifetime of this project. To this end we are creating the\ndensest spatiotemporal training sets ever by fusing open satellite data with\nPlanet imagery at as many as 500,000 patch locations over Europe and delivering\nhigh resolution daily time series at all locations. We plan to open source\nthese datasets for the benefit of the entire remote sensing community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_G/0/1/0/all/0/1\">Giovanni Marchisio</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Helber_P/0/1/0/all/0/1\">Patrick Helber</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Bischke_B/0/1/0/all/0/1\">Benjamin Bischke</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Davis_T/0/1/0/all/0/1\">Timothy Davis</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Senaras_C/0/1/0/all/0/1\">Caglar Senaras</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zanaga_D/0/1/0/all/0/1\">Daniele Zanaga</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Kerchove_R/0/1/0/all/0/1\">Ruben Van De Kerchove</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Wania_A/0/1/0/all/0/1\">Annett Wania</a> (2) ((1) Planet Labs Inc., USA, (2) Planet Labs GmbH, Germany, (3) Vision Impulse GmbH and DFKI, Germany, (4) VITO NV, Belgium)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-based Human Detection for UAVs in Search and Rescue. (arXiv:2110.01930v1 [cs.RO])","link":"http://arxiv.org/abs/2110.01930","description":"<p>The use of Unmanned Aerial Vehicles (UAVs) as a substitute for ordinary\nvehicles in applications of search and rescue is being studied all over the\nworld due to its flexible mobility and less obstruction, including two main\ntasks: search and rescue. This paper proposes an approach for the first task of\nsearching and detecting victims using a type of convolutional neural network\ntechnique, the Single Shot Detector (SSD) model, with the Quadcopter hardware\nplatform, a type of UAVs. The model used in the research is a pre-trained model\nand is applied to test on a Raspberry Pi model B, which is attached on a\nQuadcopter, while a single camera is equipped at the bottom of the Quadcopter\nto look from above for search and detection. The Quadcopter in this research is\na DIY hardware model that uses accelerometer and gyroscope sensors and\nultrasonic sensor as the essential components for balancing control, however,\nthese sensors are susceptible to noise caused by the driving forces on the\nmodel, such as the vibration of the motors, therefore, the issues about the PID\ncontroller, noise processing for the sensors are also mentioned in the paper.\nExperimental results proved that the Quadcopter is able to stably flight and\nthe SSD model works well on the Raspberry Pi model B with a processing speed of\n3 fps and produces the best detection results at the distance of 1 to 20 meters\nto objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mesvan_N/0/1/0/all/0/1\">Nikite Mesvan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor-free Oriented Proposal Generator for Object Detection. (arXiv:2110.01931v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01931","description":"<p>Oriented object detection is a practical and challenging task in remote\nsensing image interpretation. Nowadays, oriented detectors mostly use\nhorizontal boxes as intermedium to derive oriented boxes from them. However,\nthe horizontal boxes are inclined to get a small Intersection-over-Unions\n(IoUs) with ground truths, which may have some undesirable effects, such as\nintroducing redundant noise, mismatching with ground truths, detracting from\nthe robustness of detectors, etc. In this paper, we propose a novel Anchor-free\nOriented Proposal Generator (AOPG) that abandons the horizontal boxes-related\noperations from the network architecture. AOPG first produces coarse oriented\nboxes by Coarse Location Module (CLM) in an anchor-free manner and then refines\nthem into high-quality oriented proposals. After AOPG, we apply a Fast R-CNN\nhead to produce the final detection results. Furthermore, the shortage of\nlarge-scale datasets is also a hindrance to the development of oriented object\ndetection. To alleviate the data insufficiency, we release a new dataset on the\nbasis of our DIOR dataset and name it DIOR-R. Massive experiments demonstrate\nthe effectiveness of AOPG. Particularly, without bells and whistles, we achieve\nthe highest accuracy of 64.41$\\%$, 75.24$\\%$ and 96.22$\\%$ mAP on the DIOR-R,\nDOTA and HRSC2016 datasets respectively. Code and models are available at\nhttps://github.com/jbwang1997/AOPG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xingxing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Chunbo Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yanqing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Double Encoder-Decoder Networks for Gastrointestinal Polyp Segmentation. (arXiv:2110.01939v1 [eess.IV])","link":"http://arxiv.org/abs/2110.01939","description":"<p>Polyps represent an early sign of the development of Colorectal Cancer. The\nstandard procedure for their detection consists of colonoscopic examination of\nthe gastrointestinal tract. However, the wide range of polyp shapes and visual\nappearances, as well as the reduced quality of this image modality, turn their\nautomatic identification and segmentation with computational tools into a\nchallenging computer vision task. In this work, we present a new strategy for\nthe delineation of gastrointestinal polyps from endoscopic images based on a\ndirect extension of common encoder-decoder networks for semantic segmentation.\nIn our approach, two pretrained encoder-decoder networks are sequentially\nstacked: the second network takes as input the concatenation of the original\nframe and the initial prediction generated by the first network, which acts as\nan attention mechanism enabling the second network to focus on interesting\nareas within the image, thereby improving the quality of its predictions.\nQuantitative evaluation carried out on several polyp segmentation databases\nshows that double encoder-decoder networks clearly outperform their single\nencoder-decoder counterparts in all cases. In addition, our best double\nencoder-decoder combination attains excellent segmentation accuracy and reaches\nstate-of-the-art performance results in all the considered datasets, with a\nremarkable boost of accuracy on images extracted from datasets not used for\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballester_M/0/1/0/all/0/1\">Miguel A. Gonz&#xe1;lez Ballester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution Mismatch Correction for Improved Robustness in Deep Neural Networks. (arXiv:2110.01955v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01955","description":"<p>Deep neural networks rely heavily on normalization methods to improve their\nperformance and learning behavior. Although normalization methods spurred the\ndevelopment of increasingly deep and efficient architectures, they also\nincrease the vulnerability with respect to noise and input corruptions. In most\napplications, however, noise is ubiquitous and diverse; this can often lead to\ncomplete failure of machine learning systems as they fail to cope with\nmismatches between the input distribution during training- and test-time. The\nmost common normalization method, batch normalization, reduces the distribution\nshift during training but is agnostic to changes in the input distribution\nduring test time. This makes batch normalization prone to performance\ndegradation whenever noise is present during test-time. Sample-based\nnormalization methods can correct linear transformations of the activation\ndistribution but cannot mitigate changes in the distribution shape; this makes\nthe network vulnerable to distribution changes that cannot be reflected in the\nnormalization parameters. We propose an unsupervised non-parametric\ndistribution correction method that adapts the activation distribution of each\nlayer. This reduces the mismatch between the training and test-time\ndistribution by minimizing the 1-D Wasserstein distance. In our experiments, we\nempirically show that the proposed method effectively reduces the impact of\nintense image corruptions and thus improves the classification performance\nwithout the need for retraining or fine-tuning the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_A/0/1/0/all/0/1\">Alexander Fuchs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_C/0/1/0/all/0/1\">Christian Knoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pernkopf_F/0/1/0/all/0/1\">Franz Pernkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Bird's-Eye-View Traffic Scene Understanding from Onboard Images. (arXiv:2110.01997v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01997","description":"<p>Autonomous navigation requires structured representation of the road network\nand instance-wise identification of the other traffic agents. Since the traffic\nscene is defined on the ground plane, this corresponds to scene understanding\nin the bird's-eye-view (BEV). However, the onboard cameras of autonomous cars\nare customarily mounted horizontally for a better view of the surrounding,\nmaking this task very challenging. In this work, we study the problem of\nextracting a directed graph representing the local road network in BEV\ncoordinates, from a single onboard camera image. Moreover, we show that the\nmethod can be extended to detect dynamic objects on the BEV plane. The\nsemantics, locations, and orientations of the detected objects together with\nthe road graph facilitates a comprehensive understanding of the scene. Such\nunderstanding becomes fundamental for the downstream tasks, such as path\nplanning and navigation. We validate our approach against powerful baselines\nand show that our network achieves superior performance. We also demonstrate\nthe effects of various design choices through ablation studies. Code:\nhttps://github.com/ybarancan/STSU\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Can_Y/0/1/0/all/0/1\">Yigit Baran Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions. (arXiv:2110.02035v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02035","description":"<p>In this paper we introduce the Food Drinks and groceries Images Multi Lingual\n(FooDI-ML) dataset. This dataset contains over 1.5M unique images and over 9.5M\nstore names, product names descriptions, and collection sections gathered from\nthe Glovo application. The data made available corresponds to food, drinks and\ngroceries products from 37 countries in Europe, the Middle East, Africa and\nLatin America. The dataset comprehends 33 languages, including 870K samples of\nlanguages of countries from Eastern Europe and Western Asia such as Ukrainian\nand Kazakh, which have been so far underrepresented in publicly available\nvisio-linguistic datasets. The dataset also includes widely spoken languages\nsuch as Spanish and English. To assist further research, we include a benchmark\nover the text-image retrieval task using ADAPT, a SotA existing technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olondriz_D/0/1/0/all/0/1\">David Amat Ol&#xf3;ndriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puigdevall_P/0/1/0/all/0/1\">Pon&#xe7; Palau Puigdevall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palau_A/0/1/0/all/0/1\">Adri&#xe0; Salvador Palau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Object Tracking with Deep Learning Ensemble for Unmanned Aerial System Applications. (arXiv:2110.02044v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02044","description":"<p>Multi-object tracking (MOT) is a crucial component of situational awareness\nin military defense applications. With the growing use of unmanned aerial\nsystems (UASs), MOT methods for aerial surveillance is in high demand.\nApplication of MOT in UAS presents specific challenges such as moving sensor,\nchanging zoom levels, dynamic background, illumination changes, obscurations\nand small objects. In this work, we present a robust object tracking\narchitecture aimed to accommodate for the noise in real-time situations. We\npropose a kinematic prediction model, called Deep Extended Kalman Filter\n(DeepEKF), in which a sequence-to-sequence architecture is used to predict\nentity trajectories in latent space. DeepEKF utilizes a learned image embedding\nalong with an attention mechanism trained to weight the importance of areas in\nan image to predict future states. For the visual scoring, we experiment with\ndifferent similarity measures to calculate distance based on entity\nappearances, including a convolutional neural network (CNN) encoder,\npre-trained using Siamese networks. In initial evaluation experiments, we show\nthat our method, combining scoring structure of the kinematic and visual models\nwithin a MHT framework, has improved performance especially in edge cases where\nentity motion is unpredictable, or the data presents frames with significant\ngaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanlin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ide_J/0/1/0/all/0/1\">Jaime Ide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izadi_D/0/1/0/all/0/1\">Daniel Izadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banger_S/0/1/0/all/0/1\">Sean Banger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_T/0/1/0/all/0/1\">Thayne Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceresani_R/0/1/0/all/0/1\">Ryan Ceresani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spagnuolo_D/0/1/0/all/0/1\">Dylan Spagnuolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guagliano_C/0/1/0/all/0/1\">Christopher Guagliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_H/0/1/0/all/0/1\">Henry Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twedt_J/0/1/0/all/0/1\">Jason Twedt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Context Awareness for Unsupervised Change Detection in Optical Satellite Images. (arXiv:2110.02068v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02068","description":"<p>Detecting changes on the ground in multitemporal Earth observation data is\none of the key problems in remote sensing. In this paper, we introduce Sibling\nRegression for Optical Change detection (SiROC), an unsupervised method for\nchange detection in optical satellite images with medium and high resolution.\nSiROC is a spatial context-based method that models a pixel as a linear\ncombination of its distant neighbors. It uses this model to analyze differences\nin the pixel and its spatial context-based predictions in subsequent time\nperiods for change detection. We combine this spatial context-based change\ndetection with ensembling over mutually exclusive neighborhoods and\ntransitioning from pixel to object-level changes with morphological operations.\nSiROC achieves competitive performance for change detection with\nmedium-resolution Sentinel-2 and high-resolution Planetscope imagery on four\ndatasets. Besides accurate predictions without the need for training, SiROC\nalso provides a well-calibrated uncertainty of its predictions. This makes the\nmethod especially useful in conjunction with deep-learning based methods for\napplications such as pseudo-labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kondmann_L/0/1/0/all/0/1\">Lukas Kondmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toker_A/0/1/0/all/0/1\">Aysim Toker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Methodology to Identify Cognition Gaps in Visual Recognition Applications Based on Convolutional Neural Networks. (arXiv:2110.02080v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02080","description":"<p>Developing consistently well performing visual recognition applications based\non convolutional neural networks, e.g. for autonomous driving, is very\nchallenging. One of the obstacles during the development is the opaqueness of\ntheir cognitive behaviour. A considerable amount of literature has been\npublished which describes irrational behaviour of trained CNNs showcasing gaps\nin their cognition. In this paper, a methodology is presented that creates\nworstcase images using image augmentation techniques. If the CNN's cognitive\nperformance on such images is weak while the augmentation techniques are\nsupposedly harmless, a potential gap in the cognition has been found. The\npresented worst-case image generator is using adversarial search approaches to\nefficiently identify the most challenging image. This is evaluated with the\nwell-known AlexNet CNN using images depicting a typical driving scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vietz_H/0/1/0/all/0/1\">Hannes Vietz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauch_T/0/1/0/all/0/1\">Tristan Rauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locklin_A/0/1/0/all/0/1\">Andreas L&#xf6;cklin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jazdi_N/0/1/0/all/0/1\">Nasser Jazdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1\">Michael Weyrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Limits of Large Scale Pre-training. (arXiv:2110.02095v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02095","description":"<p>Recent developments in large-scale machine learning suggest that by scaling\nup data, model size and training time properly, one might observe that\nimprovements in pre-training would transfer favorably to most downstream tasks.\nIn this work, we systematically study this phenomena and establish that, as we\nincrease the upstream accuracy, the performance of downstream tasks saturates.\nIn particular, we investigate more than 4800 experiments on Vision\nTransformers, MLP-Mixers and ResNets with number of parameters ranging from ten\nmillion to ten billion, trained on the largest scale of available image data\n(JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition\ntasks. We propose a model for downstream performance that reflects the\nsaturation phenomena and captures the nonlinear relationship in performance of\nupstream and downstream tasks. Delving deeper to understand the reasons that\ngive rise to these phenomena, we show that the saturation behavior we observe\nis closely related to the way that representations evolve through the layers of\nthe models. We showcase an even more extreme scenario where performance on\nupstream and downstream are at odds with each other. That is, to have a better\ndownstream performance, we need to hurt upstream accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1\">Hanie Sedghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Generative Style Transfer for One-Shot Medical Image Segmentation. (arXiv:2110.02117v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02117","description":"<p>In medical image segmentation, supervised deep networks' success comes at the\ncost of requiring abundant labeled data. While asking domain experts to\nannotate only one or a few of the cohort's images is feasible, annotating all\navailable images is impractical. This issue is further exacerbated when\npre-trained deep networks are exposed to a new image dataset from an unfamiliar\ndistribution. Using available open-source data for ad-hoc transfer learning or\nhand-tuned techniques for data augmentation only provides suboptimal solutions.\nMotivated by atlas-based segmentation, we propose a novel volumetric\nself-supervised learning for data augmentation capable of synthesizing\nvolumetric image-segmentation pairs via learning transformations from a single\nlabeled atlas to the unlabeled data. Our work's central tenet benefits from a\ncombined view of one-shot generative learning and the proposed self-supervised\ntraining strategy that cluster unlabeled volumetric images with similar styles\ntogether. Unlike previous methods, our method does not require input volumes at\ninference time to synthesize new images. Instead, it can generate diversified\nvolumetric image-segmentation pairs from a prior distribution given a single or\nmulti-site dataset. Augmented data generated by our method used to train the\nsegmentation network provide significant improvements over state-of-the-art\ndeep one-shot learning methods on the task of brain MRI segmentation. Ablation\nstudies further exemplified that the proposed appearance model and joint\ntraining are crucial to synthesize realistic examples compared to existing\nmedical registration methods. The code, data, and models are available at\nhttps://github.com/devavratTomar/SST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomar_D/0/1/0/all/0/1\">Devavrat Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1\">Behzad Bozorgtabar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lortkipanidze_M/0/1/0/all/0/1\">Manana Lortkipanidze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vray_G/0/1/0/all/0/1\">Guillaume Vray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rad_M/0/1/0/all/0/1\">Mohammad Saeed Rad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1\">Jean-Philippe Thiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Modelling Across Time of Human Actions and Interactions. (arXiv:2110.02120v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02120","description":"<p>This thesis focuses on video understanding for human action and interaction\nrecognition. We start by identifying the main challenges related to action\nrecognition from videos and review how they have been addressed by current\nmethods.\n</p>\n<p>Based on these challenges, and by focusing on the temporal aspect of actions,\nwe argue that current fixed-sized spatio-temporal kernels in 3D convolutional\nneural networks (CNNs) can be improved to better deal with temporal variations\nin the input. Our contributions are based on the enlargement of the\nconvolutional receptive fields through the introduction of spatio-temporal\nsize-varying segments of videos, as well as the discovery of the local feature\nrelevance over the entire video sequence. The resulting extracted features\nencapsulate information that includes the importance of local features across\nmultiple temporal durations, as well as the entire video sequence.\n</p>\n<p>Subsequently, we study how we can better handle variations between classes of\nactions, by enhancing their feature differences over different layers of the\narchitecture. The hierarchical extraction of features models variations of\nrelatively similar classes the same as very dissimilar classes. Therefore,\ndistinctions between similar classes are less likely to be modelled. The\nproposed approach regularises feature maps by amplifying features that\ncorrespond to the class of the video that is processed. We move away from\nclass-agnostic networks and make early predictions based on feature\namplification mechanism.\n</p>\n<p>The proposed approaches are evaluated on several benchmark action recognition\ndatasets and show competitive results. In terms of performance, we compete with\nthe state-of-the-art while being more efficient in terms of GFLOPs.\n</p>\n<p>Finally, we present a human-understandable approach aimed at providing visual\nexplanations for features learned over spatio-temporal networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stergiou_A/0/1/0/all/0/1\">Alexandros Stergiou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\textit{FacialFilmroll}$: High-resolution multi-shot video editing. (arXiv:2110.02124v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02124","description":"<p>We present $\\textit{FacialFilmroll}$, a solution for spatially and temporally\nconsistent editing of faces in one or multiple shots. We build upon unwrap\nmosaic [Rav-Acha et al. 2008] by specializing it to faces. We leverage recent\ntechniques to fit a 3D face model on monocular videos to (i) improve the\nquality of the mosaic for edition and (ii) permit the automatic transfer of\nedits from one shot to other shots of the same actor. We explain how\n$\\textit{FacialFilmroll}$ is integrated in post-production facility. Finally,\nwe present video editing results using $\\textit{FacialFilmroll}$ on high\nresolution videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1\">Bharath Bhushan Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jolly_E/0/1/0/all/0/1\">Emmanuel Jolly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1\">Gilles Puy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_P/0/1/0/all/0/1\">Philippe Henri Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thebault_C/0/1/0/all/0/1\">C&#xe9;dric Th&#xe9;bault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Junghyun Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christensen_T/0/1/0/all/0/1\">Tim Christensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghezzo_P/0/1/0/all/0/1\">Paul Ghezzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine learning attack on copy detection patterns: are 1x1 patterns cloneable?. (arXiv:2110.02176v1 [cs.CR])","link":"http://arxiv.org/abs/2110.02176","description":"<p>Nowadays, the modern economy critically requires reliable yet cheap\nprotection solutions against product counterfeiting for the mass market. Copy\ndetection patterns (CDP) are considered as such solution in several\napplications. It is assumed that being printed at the maximum achievable limit\nof a printing resolution of an industrial printer with the smallest symbol size\n1x1 elements, the CDP cannot be copied with sufficient accuracy and thus are\nunclonable. In this paper, we challenge this hypothesis and consider a copy\nattack against the CDP based on machine learning. The experimental based on\nsamples produced on two industrial printers demonstrate that simple detection\nmetrics used in the CDP authentication cannot reliably distinguish the original\nCDP from their fakes. Thus, the paper calls for a need of careful\nreconsideration of CDP cloneability and search for new authentication\ntechniques and CDP optimization because of the current attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaban_R/0/1/0/all/0/1\">Roman Chaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taran_O/0/1/0/all/0/1\">Olga Taran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutt_J/0/1/0/all/0/1\">Joakim Tutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holotyak_T/0/1/0/all/0/1\">Taras Holotyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonev_S/0/1/0/all/0/1\">Slavi Bonev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1\">Slava Voloshynovskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer. (arXiv:2110.02178v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02178","description":"<p>Light-weight convolutional neural networks (CNNs) are the de-facto for mobile\nvision tasks. Their spatial inductive biases allow them to learn\nrepresentations with fewer parameters across different vision tasks. However,\nthese networks are spatially local. To learn global representations,\nself-attention-based vision trans-formers (ViTs) have been adopted. Unlike\nCNNs, ViTs are heavy-weight. In this paper, we ask the following question: is\nit possible to combine the strengths of CNNs and ViTs to build a light-weight\nand low latency network for mobile vision tasks? Towards this end, we introduce\nMobileViT, a light-weight and general-purpose vision transformer for mobile\ndevices. MobileViT presents a different perspective for the global processing\nof information with transformers, i.e., transformers as convolutions. Our\nresults show that MobileViT significantly outperforms CNN- and ViT-based\nnetworks across different tasks and datasets. On the ImageNet-1k dataset,\nMobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters,\nwhich is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT\n(ViT-based) for a similar number of parameters. On the MS-COCO object detection\ntask, MobileViT is 5.7% more accurate than Mo-bileNetv3 for a similar number of\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sachin Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning U-Net Deep Learning for Lung Ultrasound Segmentation. (arXiv:2110.02196v1 [eess.IV])","link":"http://arxiv.org/abs/2110.02196","description":"<p>Transfer learning (TL) for medical image segmentation helps deep learning\nmodels achieve more accurate performances when there are scarce medical images.\nThis study focuses on completing segmentation of the ribs from lung ultrasound\nimages and finding the best TL technique with U-Net, a convolutional neural\nnetwork for precise and fast image segmentation. Two approaches of TL were\nused, using a pre-trained VGG16 model to build the U-Net (V-Unet) and\npre-training U-Net network with grayscale natural salient object dataset\n(X-Unet). Visual results and dice coefficients (DICE) of the models were\ncompared. X-Unet showed more accurate and artifact-free visual performances on\nthe actual mask prediction, despite its lower DICE than V-Unet. A\npartial-frozen network fine-tuning (FT) technique was also applied to X-Unet to\ncompare results between different FT strategies, which FT all layers slightly\noutperformed freezing part of the network. The effect of dataset sizes was also\nevaluated, showing the importance of the combination between TL and data\naugmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cheng_D/0/1/0/all/0/1\">Dorothy Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_E/0/1/0/all/0/1\">Edmund Y. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\Delta$-UQ: Accurate Uncertainty Quantification via Anchor Marginalization. (arXiv:2110.02197v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02197","description":"<p>We present $\\Delta$-UQ -- a novel, general-purpose uncertainty estimator\nusing the concept of anchoring in predictive models. Anchoring works by first\ntransforming the input into a tuple consisting of an anchor point drawn from a\nprior distribution, and a combination of the input sample with the anchor using\na pretext encoding scheme. This encoding is such that the original input can be\nperfectly recovered from the tuple -- regardless of the choice of the anchor.\nTherefore, any predictive model should be able to predict the target response\nfrom the tuple alone (since it implicitly represents the input). Moreover, by\nvarying the anchors for a fixed sample, we can estimate uncertainty in the\nprediction even using only a single predictive model. We find this uncertainty\nis deeply connected to improper sampling of the input data, and inherent noise,\nenabling us to estimate the total uncertainty in any system. With extensive\nempirical studies on a variety of use-cases, we demonstrate that $\\Delta$-UQ\noutperforms several competitive baselines. Specifically, we study model\nfitting, sequential model optimization, model based inversion in the regression\nsetting and out of distribution detection, &amp; calibration under distribution\nshifts for classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J. Thiagarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Waypoint Models for Instruction-guided Navigation in Continuous Environments. (arXiv:2110.02207v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02207","description":"<p>Little inquiry has explicitly addressed the role of action spaces in\nlanguage-guided visual navigation -- either in terms of its effect on\nnavigation success or the efficiency with which a robotic agent could execute\nthe resulting trajectory. Building on the recently released VLN-CE setting for\ninstruction following in continuous environments, we develop a class of\nlanguage-conditioned waypoint prediction networks to examine this question. We\nvary the expressivity of these models to explore a spectrum between low-level\nactions and continuous waypoint prediction. We measure task performance and\nestimated execution time on a profiled LoCoBot robot. We find more expressive\nmodels result in simpler, faster to execute trajectories, but lower-level\nactions can achieve better navigation metrics by approximating shortest paths\nbetter. Further, our models outperform prior work in VLN-CE and set a new\nstate-of-the-art on the public leaderboard -- increasing success rate by 4%\nwith our best model on this challenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1\">Jacob Krantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mix3D: Out-of-Context Data Augmentation for 3D Scenes. (arXiv:2110.02210v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02210","description":"<p>We present Mix3D, a data augmentation technique for segmenting large-scale 3D\nscenes. Since scene context helps reasoning about object semantics, current\nworks focus on models with large capacity and receptive fields that can fully\ncapture the global context of an input 3D scene. However, strong contextual\npriors can have detrimental implications like mistaking a pedestrian crossing\nthe street for a car. In this work, we focus on the importance of balancing\nglobal scene context and local geometry, with the goal of generalizing beyond\nthe contextual priors in the training set. In particular, we propose a \"mixing\"\ntechnique which creates new training samples by combining two augmented scenes.\nBy doing so, object instances are implicitly placed into novel out-of-context\nenvironments and therefore making it harder for models to rely on scene context\nalone, and instead infer semantics from local structure as well. We perform\ndetailed analysis to understand the importance of global context, local\nstructures and the effect of mixing scenes. In experiments, we show that models\ntrained with Mix3D profit from a significant performance boost on indoor\n(ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially\nused with any existing method, e.g., trained with Mix3D, MinkowskiNet\noutperforms all prior state-of-the-art methods by a significant margin on the\nScanNet test benchmark 78.1 mIoU. Code is available at:\nhttps://nekrasov.dev/mix3d/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nekrasov_A/0/1/0/all/0/1\">Alexey Nekrasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schult_J/0/1/0/all/0/1\">Jonas Schult</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1\">Bastian Leibe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1\">Francis Engelmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Normalized Diversification. (arXiv:1904.03608v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.03608","description":"<p>Generating diverse yet specific data is the goal of the generative\nadversarial network (GAN), but it suffers from the problem of mode collapse. We\nintroduce the concept of normalized diversity which force the model to preserve\nthe normalized pairwise distance between the sparse samples from a latent\nparametric distribution and their corresponding high-dimensional outputs. The\nnormalized diversification aims to unfold the manifold of unknown topology and\nnon-uniform distribution, which leads to safe interpolation between valid\nlatent variables. By alternating the maximization over the pairwise distance\nand updating the total distance (normalizer), we encourage the model to\nactively explore in the high-dimensional output space. We demonstrate that by\ncombining the normalized diversity loss and the adversarial loss, we generate\ndiverse data without suffering from mode collapsing. Experimental results show\nthat our method achieves consistent improvement on unsupervised image\ngeneration, conditional image generation and hand pose estimation over strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wangni_J/0/1/0/all/0/1\">Jianqiao Wangni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianbo Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive LiDAR Sampling and Depth Completion using Ensemble Variance. (arXiv:2007.13834v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.13834","description":"<p>This work considers the problem of depth completion, with or without image\ndata, where an algorithm may measure the depth of a prescribed limited number\nof pixels. The algorithmic challenge is to choose pixel positions strategically\nand dynamically to maximally reduce overall depth estimation error. This\nsetting is realized in daytime or nighttime depth completion for autonomous\nvehicles with a programmable LiDAR. Our method uses an ensemble of predictors\nto define a sampling probability over pixels. This probability is proportional\nto the variance of the predictions of ensemble members, thus highlighting\npixels that are difficult to predict. By additionally proceeding in several\nprediction phases, we effectively reduce redundant sampling of similar pixels.\nOur ensemble-based method may be implemented using any depth-completion\nlearning algorithm, such as a state-of-the-art neural network, treated as a\nblack box. In particular, we also present a simple and effective Random\nForest-based algorithm, and similarly use its internal ensemble in our design.\nWe conduct experiments on the KITTI dataset, using the neural network algorithm\nof Ma et al. and our Random Forest based learner for implementing our method.\nThe accuracy of both implementations exceeds the state of the art. Compared\nwith a random or grid sampling pattern, our method allows a reduction by a\nfactor of 4-10 in the number of measurements required to attain the same\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gofer_E/0/1/0/all/0/1\">Eyal Gofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praisler_S/0/1/0/all/0/1\">Shachar Praisler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilboa_G/0/1/0/all/0/1\">Guy Gilboa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Neural Networks via Orthogonal Diversity. (arXiv:2010.12190v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.12190","description":"<p>Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the\nimages generated by adversarial attacks, which raises researches on the\nadversarial robustness of DNNs. A series of methods represented by the\n\\textit{adversarial training} and its variants have proved the most practical\ntechniques in enhancing the DNN robustness. Generally, adversarial training\nfocuses on enriching the training data by involving perturbed data into clean\ndata. Despite of the efficiency on defending specific attacks, adversarial\ntraining essentially benefits from the data augmentation, but does not\ncontribute to the robustness of DNN itself, and usually suffers accuracy drop\non clean data as well as inefficiency on unknown attacks. Towards the\nrobustness of DNN itself, we propose a novel defense that aims at augmenting\nthe model in order to learn features adaptive to diverse inputs, including\nadversarial examples. Specifically, we introduce multiple paths to augment the\nnetwork, and impose orthogonality constraint on these paths. In addition, a\nmargin-maximization loss is designed to further boost DIversity via\nOrthogonality (DIO). Extensive empirical results on various data sets,\narchitectures, and attacks demonstrate the robustness of DIO: it does not need\nany adversarial example and yet achieves greater robustness compared with\nstate-of-the-art adversarial training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1\">Qinghua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingwen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1\">Feipeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity-Based Clustering for Enhancing Image Classification Architectures. (arXiv:2011.04728v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.04728","description":"<p>Convolutional networks are at the center of best in class computer vision\napplications for a wide assortment of undertakings. Since 2014, profound amount\nof work began to make better convolutional architectures, yielding generous\nadditions in different benchmarks. Albeit expanded model size and computational\ncost will, in general, mean prompt quality increases for most undertakings but,\nthe architectures now need to have some additional information to increase the\nperformance. We show empirical evidence that with the amalgamation of\ncontent-based image similarity and deep learning models, we can provide the\nflow of information which can be used in making clustered learning possible. We\nshow how parallel training of sub-dataset clusters not only reduces the cost of\ncomputation but also increases the benchmark accuracies by 5-11 percent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Dishant Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using Accelerated Neuroevolution with Weight Transfer. (arXiv:2011.08446v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08446","description":"<p>Neural architecture search has proven to be highly effective in the design of\nefficient convolutional neural networks that are better suited for mobile\ndeployment than hand-designed networks. Hypothesizing that neural architecture\nsearch holds great potential for human pose estimation, we explore the\napplication of neuroevolution, a form of neural architecture search inspired by\nbiological evolution, in the design of 2D human pose networks for the first\ntime. Additionally, we propose a new weight transfer scheme that enables us to\naccelerate neuroevolution in a flexible manner. Our method produces network\ndesigns that are more efficient and more accurate than state-of-the-art\nhand-designed networks. In fact, the generated networks process images at\nhigher resolutions using less computation than previous hand-designed networks\nat lower resolutions, allowing us to push the boundaries of 2D human pose\nestimation. Our base network designed via neuroevolution, which we refer to as\nEvoPose2D-S, achieves comparable accuracy to SimpleBaseline while being 50%\nfaster and 12.7x smaller in terms of file size. Our largest network,\nEvoPose2D-L, achieves new state-of-the-art accuracy on the Microsoft COCO\nKeypoints benchmark, is 4.3x smaller than its nearest competitor, and has\nsimilar inference speed. The code is publicly available at\nhttps://github.com/wmcnally/evopose2d.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McNally_W/0/1/0/all/0/1\">William McNally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McPhee_J/0/1/0/all/0/1\">John McPhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Monocular Depth Reconstruction of Non-Rigid Scenes. (arXiv:2012.15680v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.15680","description":"<p>Monocular depth reconstruction of complex and dynamic scenes is a highly\nchallenging problem. While for rigid scenes learning-based methods have been\noffering promising results even in unsupervised cases, there exists little to\nno literature addressing the same for dynamic and deformable scenes. In this\nwork, we present an unsupervised monocular framework for dense depth estimation\nof dynamic scenes, which jointly reconstructs rigid and non-rigid parts without\nexplicitly modelling the camera motion. Using dense correspondences, we derive\na training objective that aims to opportunistically preserve pairwise distances\nbetween reconstructed 3D points. In this process, the dense depth map is\nlearned implicitly using the as-rigid-as-possible hypothesis. Our method\nprovides promising results, demonstrating its capability of reconstructing 3D\nfrom challenging videos of non-rigid scenes. Furthermore, the proposed method\nalso provides unsupervised motion segmentation results as an auxiliary output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takmaz_A/0/1/0/all/0/1\">Ay&#xe7;a Takmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAAS: Multi-modal Assignation for Active Speaker Detection. (arXiv:2101.03682v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.03682","description":"<p>Active speaker detection requires a solid integration of multi-modal cues.\nWhile individual modalities can approximate a solution, accurate predictions\ncan only be achieved by explicitly fusing the audio and visual features and\nmodeling their temporal progression. Despite its inherent muti-modal nature,\ncurrent methods still focus on modeling and fusing short-term audiovisual\nfeatures for individual speakers, often at frame level. In this paper we\npresent a novel approach to active speaker detection that directly addresses\nthe multi-modal nature of the problem, and provides a straightforward strategy\nwhere independent visual features from potential speakers in the scene are\nassigned to a previously detected speech event. Our experiments show that, an\nsmall graph data structure built from a single frame, allows to approximate an\ninstantaneous audio-visual assignment problem. Moreover, the temporal extension\nof this initial graph achieves a new state-of-the-art on the AVA-ActiveSpeaker\ndataset with a mAP of 88.8\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leon_Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n-Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.10423","description":"<p>Online continual learning for image classification studies the problem of\nlearning to classify images from an online stream of data and tasks, where\ntasks may include new classes (class incremental) or data nonstationarity\n(domain incremental). One of the key challenges of continual learning is to\navoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence\nof more recent tasks. Over the past few years, many methods and tricks have\nbeen introduced to address this problem, but many have not been fairly and\nsystematically compared under a variety of realistic and practical settings. To\nbetter understand the relative advantages of various approaches and the\nsettings where they work best, this survey aims to (1) compare state-of-the-art\nmethods such as MIR, iCARL, and GDumb and determine which works best at\ndifferent experimental settings; (2) determine if the best class incremental\nmethods are also competitive in domain incremental setting; (3) evaluate the\nperformance of 7 simple but effective trick such as \"review\" trick and nearest\nclass mean (NCM) classifier to assess their relative impact. Regarding (1), we\nobserve iCaRL remains competitive when the memory buffer is small; GDumb\noutperforms many recently proposed methods in medium-size datasets and MIR\nperforms the best in larger-scale datasets. For (2), we note that GDumb\nperforms quite poorly while MIR -- already competitive for (1) -- is also\nstrongly competitive in this very different but important setting. Overall,\nthis allows us to conclude that MIR is overall a strong and versatile method\nacross a wide variety of settings. For (3), we find that all 7 tricks are\nbeneficial, and when augmented with the \"review\" trick and NCM classifier, MIR\nproduces performance levels that bring online continual learning much closer to\nits ultimate goal of matching offline training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1\">Zheda Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jihwan Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quispe_D/0/1/0/all/0/1\">David Quispe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Damage detection using in-domain and cross-domain transfer learning. (arXiv:2102.03858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03858","description":"<p>We investigate the capabilities of transfer learning in the area of\nstructural health monitoring. In particular, we are interested in damage\ndetection for concrete structures. Typical image datasets for such problems are\nrelatively small, calling for the transfer of learned representation from a\nrelated large-scale dataset. Past efforts of damage detection using images have\nmainly considered cross-domain transfer learning approaches using pre-trained\nIMAGENET models that are subsequently fine-tuned for the target task. However,\nthere are rising concerns about the generalizability of IMAGENET\nrepresentations for specific target domains, such as for visual inspection and\nmedical imaging. We, therefore, evaluate a combination of in-domain and\ncross-domain transfer learning strategies for damage detection in bridges. We\nperform comprehensive comparisons to study the impact of cross-domain and\nin-domain transfer, with various initialization strategies, using six publicly\navailable visual inspection datasets. The pre-trained models are also evaluated\nfor their ability to cope with the extremely low-data regime. We show that the\ncombination of cross-domain and in-domain transfer persistently shows superior\nperformance specially with tiny datasets. Likewise, we also provide visual\nexplanations of predictive models to enable algorithmic transparency and\nprovide insights to experts about the intrinsic decision logic of typically\nblack-box deep models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bukhsh_Z/0/1/0/all/0/1\">Zaharah A. Bukhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_N/0/1/0/all/0/1\">Nils Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1\">Aaqib Saeed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-Hairstyle: A Large-scale Korean Hairstyle Dataset for Virtual Hair Editing and Hairstyle Classification. (arXiv:2102.06288v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06288","description":"<p>The hair and beauty industry is a fast-growing industry. This led to the\ndevelopment of various applications, such as virtual hair dyeing or hairstyle\ntransfer, to satisfy the customer's needs. Although several hairstyle datasets\nare available for these applications, they often consist of a relatively small\nnumber of images with low resolution, thus limiting their performance on\nhigh-quality hair editing. In response, we introduce a novel large-scale Korean\nhairstyle dataset, K-hairstyle, containing 500,000 high-resolution images. In\naddition, K-hairstyle includes various hair attributes annotated by Korean\nexpert hairstylists as well as hair segmentation masks. We validate the\neffectiveness of our dataset via several applications, such as hair dyeing,\nhairstyle transfer, and hairstyle classification. K-hairstyle is publicly\navailable at https://psh01087.github.io/K-Hairstyle/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1\">Chaeyeon Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Gyojung Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_K/0/1/0/all/0/1\">Keonmin Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_W/0/1/0/all/0/1\">Wonzo Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaesung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Bio-Inspired Texture Descriptor based on Biodiversity and Taxonomic Measures. (arXiv:2102.06997v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06997","description":"<p>Texture can be defined as the change of image intensity that forms repetitive\npatterns, resulting from physical properties of the object's roughness or\ndifferences in a reflection on the surface. Considering that texture forms a\ncomplex system of patterns in a non-deterministic way, biodiversity concepts\ncan help texture characterization in images. This paper proposes a novel\napproach capable of quantifying such a complex system of diverse patterns\nthrough species diversity and richness and taxonomic distinctiveness. The\nproposed approach considers each image channel as a species ecosystem and\ncomputes species diversity and richness measures as well as taxonomic measures\nto describe the texture. The proposed approach takes advantage of ecological\npatterns' invariance characteristics to build a permutation, rotation, and\ntranslation invariant descriptor. Experimental results on three datasets of\nnatural texture images and two datasets of histopathological images have shown\nthat the proposed texture descriptor has advantages over several texture\ndescriptors and deep methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ataky_S/0/1/0/all/0/1\">Steve Tsham Mpinda Ataky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerich_A/0/1/0/all/0/1\">Alessandro Lameiras Koerich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VA-RED$^2$: Video Adaptive Redundancy Reduction. (arXiv:2102.07887v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.07887","description":"<p>Performing inference on deep learning models for videos remains a challenge\ndue to the large amount of computational resources required to achieve robust\nrecognition. An inherent property of real-world videos is the high correlation\nof information across frames which can translate into redundancy in either\ntemporal or spatial feature maps of the models, or both. The type of redundant\nfeatures depends on the dynamics and type of events in the video: static videos\nhave more temporal redundancy while videos focusing on objects tend to have\nmore channel redundancy. Here we present a redundancy reduction framework,\ntermed VA-RED$^2$, which is input-dependent. Specifically, our VA-RED$^2$\nframework uses an input-dependent policy to decide how many features need to be\ncomputed for temporal and channel dimensions. To keep the capacity of the\noriginal model, after fully computing the necessary features, we reconstruct\nthe remaining redundant features from those using cheap linear operations. We\nlearn the adaptive policy jointly with the network weights in a differentiable\nway with a shared-weight mechanism, making it highly efficient. Extensive\nexperiments on multiple video datasets and different visual tasks show that our\nframework achieves $20\\% - 40\\%$ reduction in computation (FLOPs) when compared\nto state-of-the-art methods without any performance loss. Project page:\n<a href=\"http://people.csail.mit.edu/bpan/va-red/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bowen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosco_C/0/1/0/all/0/1\">Camilo Fosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yue Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrivateMail: Supervised Manifold Learning of Deep Features With Differential Privacy for Image Retrieval. (arXiv:2102.10802v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.10802","description":"<p>Differential Privacy offers strong guarantees such as immutable privacy under\npost processing. Thus it is often looked to as a solution to learning on\nscattered and isolated data. This work focuses on supervised manifold learning,\na paradigm that can generate fine-tuned manifolds for a target use case. Our\ncontributions are two fold. 1) We present a novel differentially private method\n\\textit{PrivateMail} for supervised manifold learning, the first of its kind to\nour knowledge. 2) We provide a novel private geometric embedding scheme for our\nexperimental use case. We experiment on private \"content based image retrieval\"\n- embedding and querying the nearest neighbors of images in a private manner -\nand show extensive privacy-utility tradeoff results, as well as the\ncomputational efficiency and practicality of our methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vepakomma_P/0/1/0/all/0/1\">Praneeth Vepakomma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balla_J/0/1/0/all/0/1\">Julia Balla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards clinically applicable automated aneurysm detection in TOF-MRA: weak labels, anatomical knowledge, and open data. (arXiv:2103.06168v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.06168","description":"<p>Purpose: 1) Develop a deep learning algorithm for brain aneurysm detection\nexploiting weak labels and prior anatomical knowledge. 2) Describe and release\nthe largest Time-Of-Flight Magnetic Resonance Angiography (TOF-MRA) dataset to\nthe community.\n</p>\n<p>Materials and Methods: In this retrospective study we retrieved TOF-MRA\nimages of 284 subjects (170 females) scanned between 2010 and 2015. Out of\nthese, 157 are patients with a total of 198 aneurysms, while 127 are controls.\nWe used spherical weak labels as detection ground truth, thus making data\nannotation, a major bottleneck for medical AI, noticeably faster. Since\naneurysms mainly occur in specific locations, we built our deep neural network\nleveraging the anatomy of the brain vasculature. To assess model robustness, we\nparticipated in the first public challenge for TOF-MRA data (93 patients, 20\ncontrols, 125 aneurysms). We stratified results according to aneurysm\nrisk-of-rupture, location, and size.\n</p>\n<p>Results: Our network achieves a sensitivity of 80% on the in-house data, with\nFalse Positive (FP) rate of 1.2 per patient. On the public challenge data,\nsensitivity was 68% (FP rate = 2.5), ranking 4th/16 on the open leaderboard. We\nfound no significant difference in sensitivity between risk groups (p = 0.75),\nlocations (p = 0.72), or sizes (p = 0.15).\n</p>\n<p>Conclusion: Competitive results can be obtained using fast weak labels and\nanatomical knowledge for automated aneurysm detection. Our open-source code and\nopen access dataset can foster reproducibility, and bring us closer to clinical\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Noto_T/0/1/0/all/0/1\">Tommaso Di Noto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marie_G/0/1/0/all/0/1\">Guillaume Marie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tourbier_S/0/1/0/all/0/1\">Sebastien Tourbier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aleman_Gomez_Y/0/1/0/all/0/1\">Yasser Alem&#xe1;n-G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esteban_O/0/1/0/all/0/1\">Oscar Esteban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saliou_G/0/1/0/all/0/1\">Guillaume Saliou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cuadra_M/0/1/0/all/0/1\">Meritxell Bach Cuadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hagmann_P/0/1/0/all/0/1\">Patric Hagmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Richiardi_J/0/1/0/all/0/1\">Jonas Richiardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The impact of data volume on performance of deep learning based building rooftop extraction using very high spatial resolution aerial images. (arXiv:2103.09300v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09300","description":"<p>Building rooftop data are of importance in several urban applications and in\nnatural disaster management. In contrast to traditional surveying and mapping,\nby using high spatial resolution aerial images, deep learning-based building\nrooftops extraction methods are efficient and accurate. Although more training\ndata is preferred in deep learning-based tasks, the effect of data volume on\nbuilding extraction models is underexplored. Therefore, the paper explores the\nimpact of data volume on the performance of building rooftop extraction from\nvery-high-spatial-resolution (VHSR) images using deep learning-based methods.\nTo do so, we manually labelled 0.12m spatial resolution aerial images and\nperform a comparative analysis of models trained on datasets of different sizes\nusing popular deep learning architectures for segmentation tasks, including\nFully Convolutional Networks (FCN)-8s, U-Net and DeepLabv3+. The experiments\nshowed that with more training data, algorithms converged faster and achieved\nhigher accuracy, while better algorithms were able to better mitigate the lack\nof training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongjie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Ke Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuwei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zijian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiutong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatholahi_S/0/1/0/all/0/1\">Sarah Narges Fatholahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrosians_H/0/1/0/all/0/1\">Hasti Andon Petrosians</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bingxu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_L/0/1/0/all/0/1\">Liyuan Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhehan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongzhang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kyle Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linlin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning on fundus images detects glaucoma beyond the optic disc. (arXiv:2103.11895v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.11895","description":"<p>Although unprecedented sensitivity and specificity values are reported,\nrecent glaucoma detection deep learning models lack in decision transparency.\nHere, we propose a methodology that advances explainable deep learning in the\nfield of glaucoma detection and vertical cup-disc ratio (VCDR), an important\nrisk factor. We trained and evaluated deep learning models using fundus images\nthat underwent a certain cropping policy. We defined the crop radius as a\npercentage of image size, centered on the optic nerve head (ONH), with an\nequidistant spaced range from 10%-60% (ONH crop policy). The inverse of the\ncropping mask was also applied (periphery crop policy). Trained models using\noriginal images resulted in an area under the curve (AUC) of 0.94 [95% CI:\n0.92-0.96] for glaucoma detection, and a coefficient of determination (R^2)\nequal to 77% [95% CI: 0.77-0.79] for VCDR estimation. Models that were trained\non images with absence of the ONH are still able to obtain significant\nperformance (0.88 [95% CI: 0.85-0.90] AUC for glaucoma detection and 37% [95%\nCI: 0.35-0.40] R^2 score for VCDR estimation in the most extreme setup of 60%\nONH crop). Our findings provide the first irrefutable evidence that deep\nlearning can detect glaucoma from fundus image regions outside the ONH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hemelings_R/0/1/0/all/0/1\">Ruben Hemelings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elen_B/0/1/0/all/0/1\">Bart Elen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barbosa_Breda_J/0/1/0/all/0/1\">Jo&#xe3;o Barbosa-Breda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew B. Blaschko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boever_P/0/1/0/all/0/1\">Patrick De Boever</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stalmans_I/0/1/0/all/0/1\">Ingeborg Stalmans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Light Transport Matrices for Non-Line-Of-Sight Imaging. (arXiv:2103.12622v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.12622","description":"<p>The light transport matrix (LTM) is an instrumental tool in line-of-sight\n(LOS) imaging, describing how light interacts with the scene and enabling\napplications such as relighting or separation of illumination components. We\nintroduce a framework to estimate the LTM of non-line-of-sight (NLOS)\nscenarios, coupling recent virtual forward light propagation models for NLOS\nimaging with the LOS light transport equation. We design computational\nprojector-camera setups, and use these virtual imaging systems to estimate the\ntransport matrix of hidden scenes. We introduce the specific illumination\nfunctions to compute the different elements of the matrix, overcoming the\nchallenging wide-aperture conditions of NLOS setups. Our NLOS light transport\nmatrix allows us to (re)illuminate specific locations of a hidden scene, and\nseparate direct, first-order indirect, and higher-order indirect illumination\nof complex cluttered hidden scenes, similar to existing LOS techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Marco_J/0/1/0/all/0/1\">Julio Marco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jarabo_A/0/1/0/all/0/1\">Adrian Jarabo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nam_J/0/1/0/all/0/1\">Ji Hyun Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaochun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cosculluela_M/0/1/0/all/0/1\">Miguel &#xc1;ngel Cosculluela</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Velten_A/0/1/0/all/0/1\">Andreas Velten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_D/0/1/0/all/0/1\">Diego Gutierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLinO: Generic Single Shot Polyline Detection in Real Time. (arXiv:2103.14420v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14420","description":"<p>The detection of polylines is usually either bound to branchless polylines or\nformulated in a recurrent way, prohibiting their use in real-time systems.\n</p>\n<p>We propose an approach that builds upon the idea of single shot object\ndetection. Reformulating the problem of polyline detection as a bottom-up\ncomposition of small line segments allows to detect bounded, dashed and\ncontinuous polylines with a single head. This has several major advantages over\nprevious methods. Not only is the method at 187 fps more than suited for\nreal-time applications with virtually any restriction on the shapes of the\ndetected polylines. By predicting multiple line segments for each cell, even\nbranching or crossing polylines can be detected.\n</p>\n<p>We evaluate our approach on three different applications for road marking,\nlane border and center line detection. Hereby, we demonstrate the ability to\ngeneralize to different domains as well as both implicit and explicit polyline\ndetection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1\">Annika Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skudlik_P/0/1/0/all/0/1\">Philipp Skudlik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauls_J/0/1/0/all/0/1\">Jan-Hendrik Pauls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Broaden Your Views for Self-Supervised Video Learning. (arXiv:2103.16559v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16559","description":"<p>Most successful self-supervised learning methods are trained to align the\nrepresentations of two independent views from the data. State-of-the-art\nmethods in video are inspired by image techniques, where these two views are\nsimilarly extracted by cropping and augmenting the resulting crop. However,\nthese methods miss a crucial element in the video domain: time. We introduce\nBraVe, a self-supervised learning framework for video. In BraVe, one of the\nviews has access to a narrow temporal window of the video while the other view\nhas a broad access to the video content. Our models learn to generalise from\nthe narrow view to the general content of the video. Furthermore, BraVe\nprocesses the views with different backbones, enabling the use of alternative\naugmentations or modalities into the broad view such as optical flow, randomly\nconvolved RGB frames, audio or their combinations. We demonstrate that BraVe\nachieves state-of-the-art results in self-supervised representation learning on\nstandard video and audio classification benchmarks including UCF101, HMDB51,\nKinetics, ESC-50 and AudioSet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Recasens_A/0/1/0/all/0/1\">Adri&#xe0; Recasens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luc_P/0/1/0/all/0/1\">Pauline Luc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Luyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemsley_R/0/1/0/all/0/1\">Ross Hemsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1\">Florian Strub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tallec_C/0/1/0/all/0/1\">Corentin Tallec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1\">Viorica Patraucean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altche_F/0/1/0/all/0/1\">Florent Altch&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1\">Michal Valko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grill_J/0/1/0/all/0/1\">Jean-Bastien Grill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1\">A&#xe4;ron van den Oord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Discriminator Adversarial Distillation for Data-free Model Compression. (arXiv:2104.05382v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05382","description":"<p>Knowledge distillation has been widely used to produce portable and efficient\nneural networks which can be well applied on edge devices for computer vision\ntasks. However, almost all top-performing knowledge distillation methods need\nto access the original training data, which usually has a huge size and is\noften unavailable. To tackle this problem, we propose a novel data-free\napproach in this paper, named Dual Discriminator Adversarial Distillation\n(DDAD) to distill a neural network without any training data or meta-data. To\nbe specific, we use a generator to create samples through dual discriminator\nadversarial distillation, which mimics the original training data. The\ngenerator not only uses the pre-trained teacher's intrinsic statistics in\nexisting batch normalization layers but also obtains the maximum discrepancy\nfrom the student model. Then the generated samples are used to train the\ncompact student network under the supervision of the teacher. The proposed\nmethod obtains an efficient student network which closely approximates its\nteacher network, despite using no original training data. Extensive experiments\nare conducted to to demonstrate the effectiveness of the proposed approach on\nCIFAR-10, CIFAR-100 and Caltech101 datasets for classification tasks. Moreover,\nwe extend our method to semantic segmentation tasks on several public datasets\nsuch as CamVid and NYUv2. All experiments show that our method outperforms all\nbaselines for data-free knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haoran Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 detection using deep convolutional neural networks and binary-differential-algorithm-based feature selection on X-ray images. (arXiv:2104.07279v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.07279","description":"<p>The new Coronavirus is spreading rapidly, and it has taken the lives of many\npeople so far. The virus has destructive effects on the human lung, and early\ndetection is very important. Deep Convolution neural networks are such powerful\ntools in classifying images. Therefore, in this paper, a hybrid approach based\non a deep network is presented. Feature vectors were extracted by applying a\ndeep convolution neural network on the images, and useful features were\nselected by the binary differential meta-heuristic algorithm. These optimized\nfeatures were given to the SVM classifier. A database consisting of three\ncategories of images such as COVID-19, pneumonia, and healthy included in 1092\nX-ray samples was considered. The proposed method achieved an accuracy of\n99.43%, a sensitivity of 99.16%, and a specificity of 99.57%. Our results\ndemonstrate that the suggested approach is better than recent studies on\nCOVID-19 detection with X-ray images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Iraji_M/0/1/0/all/0/1\">Mohammad Saber Iraji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanha_J/0/1/0/all/0/1\">Jafar Tanha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TricubeNet: 2D Kernel-Based Object Representation for Weakly-Occluded Oriented Object Detection. (arXiv:2104.11435v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11435","description":"<p>We present a novel approach for oriented object detection, named TricubeNet,\nwhich localizes oriented objects using visual cues ($i.e.,$ heatmap) instead of\noriented box offsets regression. We represent each object as a 2D Tricube\nkernel and extract bounding boxes using simple image-processing algorithms. Our\napproach is able to (1) obtain well-arranged boxes from visual cues, (2) solve\nthe angle discontinuity problem, and (3) can save computational complexity due\nto our anchor-free modeling. To further boost the performance, we propose some\neffective techniques for size-invariant loss, reducing false detections,\nextracting rotation-invariant features, and heatmap refinement. To demonstrate\nthe effectiveness of our TricubeNet, we experiment on various tasks for\nweakly-occluded oriented object detection: detection in an aerial image,\ndensely packed object image, and text image. The extensive experimental results\nshow that our TricubeNet is quite effective for oriented object detection. Code\nis available at https://github.com/qjadud1994/TricubeNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Janghyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-shot Compositional Data Generation for Low Resource Handwritten Text Recognition. (arXiv:2105.05300v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05300","description":"<p>Low resource Handwritten Text Recognition (HTR) is a hard problem due to the\nscarce annotated data and the very limited linguistic information (dictionaries\nand language models). For example, in the case of historical ciphered\nmanuscripts, which are usually written with invented alphabets to hide the\nmessage contents. Thus, in this paper we address this problem through a data\ngeneration technique based on Bayesian Program Learning (BPL). Contrary to\ntraditional generation approaches, which require a huge amount of annotated\nimages, our method is able to generate human-like handwriting using only one\nsample of each symbol in the alphabet. After generating symbols, we create\nsynthetic lines to train state-of-the-art HTR architectures in a segmentation\nfree fashion. Quantitative and qualitative analyses were carried out and\nconfirm the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sounak Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fit4CAD: A point cloud benchmark for fitting simple geometric primitives in CAD objects. (arXiv:2105.06858v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2105.06858","description":"<p>We propose Fit4CAD, a benchmark for the evaluation and comparison of methods\nfor fitting simple geometric primitives in point clouds representing CAD\nobjects. This benchmark is meant to help both method developers and those who\nwant to identify the best performing tools. The Fit4CAD dataset is composed by\n225 high quality point clouds, each of which has been obtained by sampling a\nCAD object. The way these elements were created by using existing platforms and\ndatasets makes the benchmark easily expandable. The dataset is already split\ninto a training set and a test set. To assess performance and accuracy of the\ndifferent primitive fitting methods, various measures are defined. To\ndemonstrate the effective use of Fit4CAD, we have tested it on two methods\nbelonging to two different categories of approaches to the primitive fitting\nproblem: a clustering method based on a primitive growing framework and a\nparametric method based on the Hough transform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romanengo_C/0/1/0/all/0/1\">Chiara Romanengo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffo_A/0/1/0/all/0/1\">Andrea Raffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_Y/0/1/0/all/0/1\">Yifan Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_N/0/1/0/all/0/1\">Nabil Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcidieno_B/0/1/0/all/0/1\">Bianca Falcidieno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoMate: A Dataset and Learning Approach for Automatic Mating of CAD Assemblies. (arXiv:2105.12238v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.12238","description":"<p>Assembly modeling is a core task of computer aided design (CAD), comprising\naround one third of the work in a CAD workflow. Optimizing this process\ntherefore represents a huge opportunity in the design of a CAD system, but\ncurrent research of assembly based modeling is not directly applicable to\nmodern CAD systems because it eschews the dominant data structure of modern\nCAD: parametric boundary representations (BREPs). CAD assembly modeling defines\nassemblies as a system of pairwise constraints, called mates, between parts,\nwhich are defined relative to BREP topology rather than in world coordinates\ncommon to existing work. We propose SB-GCN, a representation learning scheme on\nBREPs that retains the topological structure of parts, and use these learned\nrepresentations to predict CAD type mates. To train our system, we compiled the\nfirst large scale dataset of BREP CAD assemblies, which we are releasing along\nwith benchmark mate prediction tasks. Finally, we demonstrate the compatibility\nof our model with an existing commercial CAD system by building a tool that\nassists users in mate creation by suggesting mate completions, with 72.2%\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_B/0/1/0/all/0/1\">Benjamin Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildreth_D/0/1/0/all/0/1\">Dalton Hildreth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Duowen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baran_I/0/1/0/all/0/1\">Ilya Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir G. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Adriana Schulz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fidelity Estimation Improves Noisy-Image Classification With Pretrained Networks. (arXiv:2106.00673v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00673","description":"<p>Image classification has significantly improved using deep learning. This is\nmainly due to convolutional neural networks (CNNs) that are capable of learning\nrich feature extractors from large datasets. However, most deep learning\nclassification methods are trained on clean images and are not robust when\nhandling noisy ones, even if a restoration preprocessing step is applied. While\nnovel methods address this problem, they rely on modified feature extractors\nand thus necessitate retraining. We instead propose a method that can be\napplied on a $pretrained$ classifier. Our method exploits a fidelity map\nestimate that is fused into the internal representations of the feature\nextractor, thereby guiding the attention of the network and making it more\nrobust to noisy data. We improve the noisy-image classification (NIC) results\nby significantly large margins, especially at high noise levels, and come close\nto the fully retrained approaches. Furthermore, as proof of concept, we show\nthat when using our oracle fidelity map we even outperform the fully retrained\nmethods, whether trained on noisy or restored images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaoyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1\">Deblina Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helou_M/0/1/0/all/0/1\">Majed El Helou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digital Taxonomist: Identifying Plant Species in Community Scientists' Photographs. (arXiv:2106.03774v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03774","description":"<p>Automatic identification of plant specimens from amateur photographs could\nimprove species range maps, thus supporting ecosystems research as well as\nconservation efforts. However, classifying plant specimens based on image data\nalone is challenging: some species exhibit large variations in visual\nappearance, while at the same time different species are often visually\nsimilar; additionally, species observations follow a highly imbalanced,\nlong-tailed distribution due to differences in abundance as well as observer\nbiases. On the other hand, most species observations are accompanied by side\ninformation about the spatial, temporal and ecological context. Moreover,\nbiological species are not an unordered list of classes but embedded in a\nhierarchical taxonomic structure. We propose a multimodal deep learning model\nthat takes into account these additional cues in a unified framework. Our\nDigital Taxonomist is able to identify plant species in photographs better than\na classifier trained on the image content alone, the performance gained is over\n6 percent points in terms of accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lutio_R/0/1/0/all/0/1\">Riccardo de Lutio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Y/0/1/0/all/0/1\">Yihang She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_S/0/1/0/all/0/1\">Stefania Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_P/0/1/0/all/0/1\">Philipp Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan D. Wegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multi-stage GAN for multi-organ chest X-ray image generation and segmentation. (arXiv:2106.05132v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.05132","description":"<p>Multi-organ segmentation of X-ray images is of fundamental importance for\ncomputer aided diagnosis systems. However, the most advanced semantic\nsegmentation methods rely on deep learning and require a huge amount of labeled\nimages, which are rarely available due to both the high cost of human resources\nand the time required for labeling. In this paper, we present a novel\nmulti-stage generation algorithm based on Generative Adversarial Networks\n(GANs) that can produce synthetic images along with their semantic labels and\ncan be used for data augmentation. The main feature of the method is that,\nunlike other approaches, generation occurs in several stages, which simplifies\nthe procedure and allows it to be used on very small datasets. The method has\nbeen evaluated on the segmentation of chest radiographic images, showing\npromising results. The multistage approach achieves state-of-the-art and, when\nvery few images are used to train the GANs, outperforms the corresponding\nsingle-stage approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ciano_G/0/1/0/all/0/1\">Giorgio Ciano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andreini_P/0/1/0/all/0/1\">Paolo Andreini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazzierli_T/0/1/0/all/0/1\">Tommaso Mazzierli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bianchini_M/0/1/0/all/0/1\">Monica Bianchini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scarselli_F/0/1/0/all/0/1\">Franco Scarselli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Resolution Continuous Normalizing Flows. (arXiv:2106.08462v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08462","description":"<p>Recent work has shown that Neural Ordinary Differential Equations (ODEs) can\nserve as generative models of images using the perspective of Continuous\nNormalizing Flows (CNFs). Such models offer exact likelihood calculation, and\ninvertible generation/density estimation. In this work we introduce a\nMulti-Resolution variant of such models (MRCNF), by characterizing the\nconditional distribution over the additional information required to generate a\nfine image that is consistent with the coarse image. We introduce a\ntransformation between resolutions that allows for no change in the log\nlikelihood. We show that this approach yields comparable likelihood values for\nvarious image datasets, with improved performance at higher resolutions, with\nfewer parameters, using only 1 GPU. Further, we examine the out-of-distribution\nproperties of (Multi-Resolution) Continuous Normalizing Flows, and find that\nthey are similar to those of other likelihood-based generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finlay_C/0/1/0/all/0/1\">Chris Finlay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1\">Adam Oberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?. (arXiv:2106.11297v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11297","description":"<p>In this paper, we introduce a novel visual representation learning which\nrelies on a handful of adaptively learned tokens, and which is applicable to\nboth image and video understanding tasks. Instead of relying on hand-designed\nsplitting strategies to obtain visual tokens and processing a large number of\ndensely sampled patches for attention, our approach learns to mine important\ntokens in visual data. This results in efficiently and effectively finding a\nfew important visual tokens and enables modeling of pairwise attention between\nsuch tokens, over a longer temporal horizon for videos, or the spatial content\nin images. Our experiments demonstrate strong performance on several\nchallenging benchmarks for both image and video recognition tasks. Importantly,\ndue to our tokens being adaptive, we accomplish competitive results at\nsignificantly reduced compute amount. We obtain comparable results to the\nstate-of-the-arts on ImageNet while being computationally more efficient. We\nestablish new state-of-the-arts on multiple video datasets, including\nKinetics-400, Kinetics-600, Charades, and AViD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Technical Document Classification. (arXiv:2106.14269v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14269","description":"<p>In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have only focused on processing text for\nclassification, whereas technical documents often contain multimodal\ninformation. This paper presents a novel multimodal deep learning architecture,\nTechDoc, for technical document classification, which utilizes three types of\ninformation, including natural language texts and descriptive images within\ndocuments and the associations among the documents. The architecture\nsynthesizes the convolutional neural network, recurrent neural network, and\ngraph neural network through an integrated multimodal training process. We\napplied the architecture to a large multimodal technical document database and\ntrained the model for classifying documents based on the hierarchical\nInternational Patent Classification system. Our results show that TechDoc\npresents a greater classification accuracy than the unimodal methods and other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Mammographic Image Classification using Case-Based Reasoning and Deep Learning. (arXiv:2107.05605v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05605","description":"<p>When we deploy machine learning models in high-stakes medical settings, we\nmust ensure these models make accurate predictions that are consistent with\nknown medical science. Inherently interpretable networks address this need by\nexplaining the rationale behind each decision while maintaining equal or higher\naccuracy compared to black-box models. In this work, we present a novel\ninterpretable neural network algorithm that uses case-based reasoning for\nmammography. Designed to aid a radiologist in their decisions, our network\npresents both a prediction of malignancy and an explanation of that prediction\nusing known medical features. In order to yield helpful explanations, the\nnetwork is designed to mimic the reasoning processes of a radiologist: our\nnetwork first detects the clinically relevant semantic features of each image\nby comparing each new image with a learned set of prototypical image parts from\nthe training images, then uses those clinical features to predict malignancy.\nCompared to other methods, our model detects clinical features (mass margins)\nwith equal or higher accuracy, provides a more detailed explanation of its\nprediction, and is better able to differentiate the classification-relevant\nparts of the image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barnett_A/0/1/0/all/0/1\">Alina Jade Barnett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_F/0/1/0/all/0/1\">Fides Regina Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chaofan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yinhao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1\">Joseph Y. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1\">Cynthia Rudin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Urban Driving by Imitating a Reinforcement Learning Coach. (arXiv:2108.08265v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08265","description":"<p>End-to-end approaches to autonomous driving commonly rely on expert\ndemonstrations. Although humans are good drivers, they are not good coaches for\nend-to-end algorithms that demand dense on-policy supervision. On the contrary,\nautomated experts that leverage privileged information can efficiently generate\nlarge scale on-policy and off-policy demonstrations. However, existing\nautomated experts for urban driving make heavy use of hand-crafted rules and\nperform suboptimally even on driving simulators, where ground-truth information\nis available. To address these issues, we train a reinforcement learning expert\nthat maps bird's-eye view images to continuous low-level actions. While setting\na new performance upper-bound on CARLA, our expert is also a better coach that\nprovides informative supervision signals for imitation learning agents to learn\nfrom. Supervised by our reinforcement learning coach, a baseline end-to-end\nagent with monocular camera-input achieves expert-level performance. Our\nend-to-end agent achieves a 78% success rate while generalizing to a new town\nand new weather on the NoCrash-dense benchmark and state-of-the-art performance\non the challenging public routes of the CARLA LeaderBoard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhejun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Spacecraft Relative Navigation Methods: A Survey. (arXiv:2108.08876v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.08876","description":"<p>Autonomous spacecraft relative navigation technology has been planned for and\napplied to many famous space missions. The development of on-board electronics\nsystems has enabled the use of vision-based and LiDAR-based methods to achieve\nbetter performances. Meanwhile, deep learning has reached great success in\ndifferent areas, especially in computer vision, which has also attracted the\nattention of space researchers. However, spacecraft navigation differs from\nground tasks due to high reliability requirements but lack of large datasets.\nThis survey aims to systematically investigate the current deep learning-based\nautonomous spacecraft relative navigation methods, focusing on concrete orbital\napplications such as spacecraft rendezvous and landing on small bodies or the\nMoon. The fundamental characteristics, primary motivations, and contributions\nof deep learning-based relative navigation algorithms are first summarised from\nthree perspectives of spacecraft rendezvous, asteroid exploration, and terrain\nnavigation. Furthermore, popular visual tracking benchmarks and their\nrespective properties are compared and summarised. Finally, potential\napplications are discussed, along with expected impediments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jianing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rondao_D/0/1/0/all/0/1\">Duarte Rondao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1\">Nabil Aouf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep few-shot learning for bi-temporal building change detection. (arXiv:2108.11262v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11262","description":"<p>In real-world applications (e.g., change detection), annotating images is\nvery expensive. To build effective deep learning models in these applications,\ndeep few-shot learning methods have been developed and prove to be a robust\napproach in small training data. The analysis of building change detection from\nhigh spatial resolution remote sensing observations is important research in\nphotogrammetry, computer vision, and remote sensing nowadays, which can be\nwidely used in a variety of real-world applications, such as map updating. As\nmanual high resolution image interpretation is expensive and time-consuming,\nbuilding change detection methods are of high interest. The interest in\ndeveloping building change detection approaches from optical remote sensing\nimages is rapidly increasing due to larger coverages, and lower costs of\noptical images. In this study, we focus on building change detection analysis\non a small set of building change from different regions that sit in several\ncities. In this paper, a new deep few-shot learning method is proposed for\nbuilding change detection using Monte Carlo dropout and remote sensing\nobservations. The setup is based on a small dataset, including bitemporal\noptical images labeled for building change detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khoshboresh_Masouleh_M/0/1/0/all/0/1\">Mehdi Khoshboresh-Masouleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_Hosseini_R/0/1/0/all/0/1\">Reza Shah-Hosseini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo. (arXiv:2109.01129v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01129","description":"<p>In this work, we present a new multi-view depth estimation method that\nutilizes both conventional reconstruction and learning-based priors over the\nrecently proposed neural radiance fields (NeRF). Unlike existing neural network\nbased optimization method that relies on estimated correspondences, our method\ndirectly optimizes over implicit volumes, eliminating the challenging step of\nmatching pixels in indoor scenes. The key to our approach is to utilize the\nlearning-based priors to guide the optimization process of NeRF. Our system\nfirstly adapts a monocular depth network over the target scene by finetuning on\nits sparse SfM+MVS reconstruction from COLMAP. Then, we show that the\nshape-radiance ambiguity of NeRF still exists in indoor environments and\npropose to address the issue by employing the adapted depth priors to monitor\nthe sampling process of volume rendering. Finally, a per-pixel confidence map\nacquired by error computation on the rendered image can be used to further\nimprove the depth quality. Experiments show that our proposed framework\nsignificantly outperforms state-of-the-art methods on indoor scenes, with\nsurprising findings presented on the effectiveness of correspondence-based\noptimization and NeRF-based optimization over the adapted depth priors. In\naddition, we show that the guided optimization scheme does not sacrifice the\noriginal synthesis capability of neural radiance fields, improving the\nrendering quality on both seen and novel views. Code is available at\nhttps://github.com/weiyithu/NerfingMVS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets. (arXiv:2109.03229v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03229","description":"<p>Many existing works have made great strides towards reducing racial bias in\nface recognition. However, most of these methods attempt to rectify bias that\nmanifests in models during training instead of directly addressing a major\nsource of the bias, the dataset itself. Exceptions to this are\nBUPT-Balancedface/RFW and Fairface, but these works assume that primarily\ntraining on a single race or not racially balancing the dataset are inherently\ndisadvantageous. We demonstrate that these assumptions are not necessarily\nvalid. In our experiments, training on only African faces induced less bias\nthan training on a balanced distribution of faces and distributions skewed to\ninclude more African faces produced more equitable models. We additionally\nnotice that adding more images of existing identities to a dataset in place of\nadding new identities can lead to accuracy boosts across racial categories. Our\ncode is available at\nhttps://github.com/j-alex-hanson/rethinking-race-face-datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1\">Matthew Gwilliam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Srinidhi Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinubu_L/0/1/0/all/0/1\">Lade Tinubu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanson_A/0/1/0/all/0/1\">Alex Hanson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigation of condominium building collapse in Surfside, Florida: A video feature tracking approach. (arXiv:2109.06629v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06629","description":"<p>On June 24, 2021, a 12-story condominium building (Champlain Towers South) in\nSurfside, Florida partially collapsed, resulting in one of the deadliest\nbuilding collapses in United States history with 98 people confirmed deceased.\nIn this work, we analyze the collapse event using a video clip that is publicly\navailable from social media. In our analysis, we apply computer vision\nalgorithms to corroborate new information from the video clip that may not be\nreadily interpreted by human eyes. By comparing the differential features\nagainst different video frames, our proposed method is used to quantify the\nfalling structural components by mapping the directions and magnitudes of their\nmovements. We demonstrate the potential of this video processing methodology in\ninvestigations of catastrophic structural failures and hope our approach may\nserve as a basis for further investigations into structure collapse events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangxiong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyl_D/0/1/0/all/0/1\">Danny Smyl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing Counterfactual Generators using Deep Model Inversion. (arXiv:2109.14274v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.14274","description":"<p>Explanation techniques that synthesize small, interpretable changes to a\ngiven image while producing desired changes in the model prediction have become\npopular for introspecting black-box models. Commonly referred to as\ncounterfactuals, the synthesized explanations are required to contain\ndiscernible changes (for easy interpretability) while also being realistic\n(consistency to the data manifold). In this paper, we focus on the case where\nwe have access only to the trained deep classifier and not the actual training\ndata. While the problem of inverting deep models to synthesize images from the\ntraining distribution has been explored, our goal is to develop a deep\ninversion approach to generate counterfactual explanations for a given query\nimage. Despite their effectiveness in conditional image synthesis, we show that\nexisting deep inversion methods are insufficient for producing meaningful\ncounterfactuals. We propose DISC (Deep Inversion for Synthesizing\nCounterfactuals) that improves upon deep inversion by utilizing (a) stronger\nimage priors, (b) incorporating a novel manifold consistency objective and (c)\nadopting a progressive optimization strategy. We find that, in addition to\nproducing visually meaningful explanations, the counterfactuals from DISC are\neffective at learning classifier decision boundaries and are robust to unknown\ntest-time corruptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J. Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanaswamy_V/0/1/0/all/0/1\">Vivek Narayanaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_D/0/1/0/all/0/1\">Deepta Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jason Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanias_A/0/1/0/all/0/1\">Andreas Spanias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake It Till You Make It: Face analysis in the wild using synthetic data alone. (arXiv:2109.15102v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15102","description":"<p>We demonstrate that it is possible to perform face-related computer vision in\nthe wild using synthetic data alone. The community has long enjoyed the\nbenefits of synthesizing training data with graphics, but the domain gap\nbetween real and synthetic data has remained a problem, especially for human\nfaces. Researchers have tried to bridge this gap with data mixing, domain\nadaptation, and domain-adversarial training, but we show that it is possible to\nsynthesize data with minimal domain gap, so that models trained on synthetic\ndata generalize to real in-the-wild datasets. We describe how to combine a\nprocedurally-generated parametric 3D face model with a comprehensive library of\nhand-crafted assets to render training images with unprecedented realism and\ndiversity. We train machine learning systems for face-related tasks such as\nlandmark localization and face parsing, showing that synthetic data can both\nmatch real data in accuracy as well as open up new approaches where manual\nlabelling would be impossible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wood_E/0/1/0/all/0/1\">Erroll Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltrusaitis_T/0/1/0/all/0/1\">Tadas Baltru&#x161;aitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_C/0/1/0/all/0/1\">Charlie Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziadzio_S/0/1/0/all/0/1\">Sebastian Dziadzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Matthew Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Estellers_V/0/1/0/all/0/1\">Virginia Estellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cashman_T/0/1/0/all/0/1\">Thomas J. Cashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shotton_J/0/1/0/all/0/1\">Jamie Shotton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity-Disentangled Neural Deformation Model for Dynamic Meshes. (arXiv:2109.15299v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15299","description":"<p>Neural shape models can represent complex 3D shapes with a compact latent\nspace. When applied to dynamically deforming shapes such as the human hands,\nhowever, they would need to preserve temporal coherence of the deformation as\nwell as the intrinsic identity of the subject. These properties are difficult\nto regularize with manually designed loss functions. In this paper, we learn a\nneural deformation model that disentangles the identity-induced shape\nvariations from pose-dependent deformations using implicit neural functions. We\nperform template-free unsupervised learning on 3D scans without explicit mesh\ncorrespondence or semantic correspondences of shapes across subjects. We can\nthen apply the learned model to reconstruct partial dynamic 4D scans of novel\nsubjects performing unseen actions. We propose two methods to integrate global\npose alignment with our neural deformation model. Experiments demonstrate the\nefficacy of our method in the disentanglement of identities and pose. Our\nmethod also outperforms traditional skeleton-driven models in reconstructing\nsurface details such as palm prints or tendons without limitations from a fixed\ntemplate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Binbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lingni Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yuting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_T/0/1/0/all/0/1\">Tanner Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twigg_C/0/1/0/all/0/1\">Christopher D. Twigg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovegrove_S/0/1/0/all/0/1\">Steven Lovegrove</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Spiking Variational Autoencoder. (arXiv:2110.00375v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2110.00375","description":"<p>Spiking neural networks (SNNs) can be run on neuromorphic devices with\nultra-high speed and ultra-low energy consumption because of their binary and\nevent-driven nature. Therefore, SNNs are expected to have various applications,\nincluding as generative models being running on edge devices to create\nhigh-quality images. In this study, we build a variational autoencoder (VAE)\nwith SNN to enable image generation. VAE is known for its stability among\ngenerative models; recently, its quality advanced. In vanilla VAE, the latent\nspace is represented as a normal distribution, and floating-point calculations\nare required in sampling. However, this is not possible in SNNs because all\nfeatures must be binary time series data. Therefore, we constructed the latent\nspace with an autoregressive SNN model, and randomly selected samples from its\noutput to sample the latent variables. This allows the latent variables to\nfollow the Bernoulli process and allows variational learning. Thus, we build\nthe Fully Spiking Variational Autoencoder where all modules are constructed\nwith SNN. To the best of our knowledge, we are the first to build a VAE only\nwith SNN layers. We experimented with several datasets, and confirmed that it\ncan generate images with the same or better quality compared to conventional\nANNs. The code is available at https://github.com/kamata1729/FullySpikingVAE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamata_H/0/1/0/all/0/1\">Hiromichi Kamata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Unfolding Total Variation Network for Low-Light Image Enhancement. (arXiv:2110.00984v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.00984","description":"<p>Real-world low-light images suffer from two main degradations, namely,\ninevitable noise and poor visibility. Since the noise exhibits different\nlevels, its estimation has been implemented in recent works when enhancing\nlow-light images from raw Bayer space. When it comes to sRGB color space, the\nnoise estimation becomes more complicated due to the effect of the image\nprocessing pipeline. Nevertheless, most existing enhancing algorithms in sRGB\nspace only focus on the low visibility problem or suppress the noise under a\nhypothetical noise level, leading them impractical due to the lack of\nrobustness. To address this issue,we propose an adaptive unfolding total\nvariation network (UTVNet), which approximates the noise level from the real\nsRGB low-light image by learning the balancing parameter in the model-based\ndenoising method with total variation regularization. Meanwhile, we learn the\nnoise level map by unrolling the corresponding minimization process for\nproviding the inferences of smoothness and fidelity constraints. Guided by the\nnoise level map, our UTVNet can recover finer details and is more capable to\nsuppress noise in real captured low-light scenes. Extensive experiments on\nreal-world low-light images clearly demonstrate the superior performance of\nUTVNet over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanjun Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_D/0/1/0/all/0/1\">Daming Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1\">Wentian Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Contrastive Replay: Revisiting the Nearest Class Mean Classifier in Online Class-Incremental Continual Learning. (arXiv:2103.13885v3 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2103.13885","description":"<p>Online class-incremental continual learning (CL) studies the problem of\nlearning new classes continually from an online non-stationary data stream,\nintending to adapt to new data while mitigating catastrophic forgetting. While\nmemory replay has shown promising results, the recency bias in online learning\ncaused by the commonly used Softmax classifier remains an unsolved challenge.\nAlthough the Nearest-Class-Mean (NCM) classifier is significantly undervalued\nin the CL community, we demonstrate that it is a simple yet effective\nsubstitute for the Softmax classifier. It addresses the recency bias and avoids\nstructural changes in the fully-connected layer for new classes. Moreover, we\nobserve considerable and consistent performance gains when replacing the\nSoftmax classifier with the NCM classifier for several state-of-the-art replay\nmethods. To leverage the NCM classifier more effectively, data embeddings\nbelonging to the same class should be clustered and well-separated from those\nwith a different class label. To this end, we contribute Supervised Contrastive\nReplay (SCR), which explicitly encourages samples from the same class to\ncluster tightly in embedding space while pushing those of different classes\nfurther apart during replay-based training. Overall, we observe that our\nproposed SCR substantially reduces catastrophic forgetting and outperforms\nstate-of-the-art CL methods by a significant margin on a variety of datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1\">Zheda Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}