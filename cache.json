{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Text Transformations in Contrastive Self-Supervised Learning: A Review. (arXiv:2203.12000v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12000","description":"<p>Contrastive self-supervised learning has become a prominent technique in\nrepresentation learning. The main step in these methods is to contrast\nsemantically similar and dissimilar pairs of samples. However, in the domain of\nNatural Language, the augmentation methods used in creating similar pairs with\nregard to contrastive learning assumptions are challenging. This is because,\neven simply modifying a word in the input might change the semantic meaning of\nthe sentence, and hence, would violate the distributional hypothesis. In this\nreview paper, we formalize the contrastive learning framework in the domain of\nnatural language processing. We emphasize the considerations that need to be\naddressed in the data transformation step and review the state-of-the-art\nmethods and evaluations for contrastive representation learning in NLP.\nFinally, we describe some challenges and potential directions for learning\nbetter text representations using contrastive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Amrita Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karami_M/0/1/0/all/0/1\">Mansooreh Karami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Robust Spoken Language Understanding by Cross Attention between Phoneme Sequence and ASR Hypothesis. (arXiv:2203.12067v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12067","description":"<p>Building Spoken Language Understanding (SLU) robust to Automatic Speech\nRecognition (ASR) errors is an essential issue for various voice-enabled\nvirtual assistants. Considering that most ASR errors are caused by phonetic\nconfusion between similar-sounding expressions, intuitively, leveraging the\nphoneme sequence of speech can complement ASR hypothesis and enhance the\nrobustness of SLU. This paper proposes a novel model with Cross Attention for\nSLU (denoted as CASLU). The cross attention block is devised to catch the\nfine-grained interactions between phoneme and word embeddings in order to make\nthe joint representations catch the phonetic and semantic features of input\nsimultaneously and for overcoming the ASR errors in downstream natural language\nunderstanding (NLU) tasks. Extensive experiments are conducted on three\ndatasets, showing the effectiveness and competitiveness of our approach.\nAdditionally, We also validate the universality of CASLU and prove its\ncomplementarity when combining with other robust SLU techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zexun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Y/0/1/0/all/0/1\">Yuquan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Mingchao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Learning and Improving the Search Objective for Unsupervised Paraphrasing. (arXiv:2203.12106v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12106","description":"<p>Research in unsupervised text generation has been gaining attention over the\nyears. One recent approach is local search towards a heuristically defined\nobjective, which specifies language fluency, semantic meanings, and other\ntask-specific attributes. Search in the sentence space is realized by\nword-level edit operations including insertion, replacement, and deletion.\nHowever, such objective function is manually designed with multiple components.\nAlthough previous work has shown maximizing this objective yields good\nperformance in terms of true measure of success (i.e. BLEU and iBLEU), the\nobjective landscape is considered to be non-smooth with significant noises,\nposing challenges for optimization. In this dissertation, we address the\nresearch problem of smoothing the noise in the heuristic search objective by\nlearning to model the search dynamics. Then, the learned model is combined with\nthe original objective function to guide the search in a bootstrapping fashion.\nExperimental results show that the learned models combined with the original\nsearch objective can indeed provide a smoothing effect, improving the search\nperformance by a small margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weikai Steven Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALT: um software para an\\'alise de legibilidade de textos em L\\'ingua Portuguesa. (arXiv:2203.12135v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12135","description":"<p>In the initial stage of human life, communication, seen as a process of\nsocial interaction, was always the best way to reach consensus between the\nparties. Understanding and credibility in this process are essential for the\nmutual agreement to be validated. But, how to do it so that this communication\nreaches the great mass? This is the main challenge when what is sought is the\ndissemination of information and its approval. In this context, this study\npresents the ALT software, developed from original readability metrics adapted\nto the Portuguese language, available on the web, to reduce communication\ndifficulties. The development of the software was motivated by the theory of\ncommunicative action of Habermas, which uses a multidisciplinary style to\nmeasure the credibility of the discourse in the communication channels used to\nbuild and maintain a safe and healthy relationship with the public.\n</p>\n<p>--\n</p>\n<p>No est\\'agio inicial da vida humana a comunica\\c{c}\\~ao, vista como um\nprocesso de intera\\c{c}\\~ao social, foi sempre o melhor caminho para o consenso\nentre as partes. O entendimento e a credibilidade nesse processo s\\~ao\nfundamentais para que o acordo m\\'utuo seja validado. Mas, como faz\\^e-lo de\nforma que essa comunica\\c{c}\\~ao alcance a grande massa? Esse \\'e o principal\ndesafio quando o que se busca \\'e a difus\\~ao da informa\\c{c}\\~ao e a sua\naprova\\c{c}\\~ao. Nesse contexto, este estudo apresenta o software ALT,\ndesenvolvido a partir de m\\'etricas de legibilidade originais adaptadas para a\nL\\'ingua Portuguesa, dispon\\'ivel na web, para reduzir as dificuldades na\ncomunica\\c{c}\\~ao. O desenvolvimento do software foi motivado pela teoria do\nagir comunicativo de Habermas, que faz uso de um estilo multidisciplinar para\nmedir a credibilidade do discurso nos canais de comunica\\c{c}\\~ao utilizados\npara construir e manter uma rela\\c{c}\\~ao segura e saud\\'avel com o p\\'ublico.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreno_G/0/1/0/all/0/1\">Gleice Carvalho de Lima Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1\">Marco P. M. de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_N/0/1/0/all/0/1\">Nelson Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_A/0/1/0/all/0/1\">Adriana Kroenke Hein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Memorization in NLP. (arXiv:2203.12171v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12171","description":"<p>A recent study by Feldman (2020) proposed a long-tail theory to explain the\nmemorization behavior of deep learning models. However, memorization has not\nbeen empirically verified in the context of NLP, a gap addressed by this work.\nIn this paper, we use three different NLP tasks to check if the long-tail\ntheory holds. Our experiments demonstrate that top-ranked memorized training\ninstances are likely atypical, and removing the top-memorized training\ninstances leads to a more serious drop in test accuracy compared with removing\ntraining instances randomly. Furthermore, we develop an attribution method to\nbetter understand why a training instance is memorized. We empirically show\nthat our memorization attribution method is faithful, and share our interesting\nfinding that the top-memorized parts of a training instance tend to be features\nnegatively correlated with the class label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaosen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Theoretically Grounded Benchmark for Evaluating Machine Commonsense. (arXiv:2203.12184v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12184","description":"<p>Programming machines with commonsense reasoning (CSR) abilities is a\nlongstanding challenge in the Artificial Intelligence community. Current CSR\nbenchmarks use multiple-choice (and in relatively fewer cases, generative)\nquestion-answering instances to evaluate machine commonsense. Recent progress\nin transformer-based language representation models suggest that considerable\nprogress has been made on existing benchmarks. However, although tens of CSR\nbenchmarks currently exist, and are growing, it is not evident that the full\nsuite of commonsense capabilities have been systematically evaluated.\nFurthermore, there are doubts about whether language models are 'fitting' to a\nbenchmark dataset's training partition by picking up on subtle, but normatively\nirrelevant (at least for CSR), statistical features to achieve good performance\non the testing partition. To address these challenges, we propose a benchmark\ncalled Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based\non discriminative question answering, but with questions designed to evaluate\ndiverse aspects of commonsense, such as space, time, and world states. TG-CSR\nis based on a subset of commonsense categories first proposed as a viable\ntheory of commonsense by Gordon and Hobbs. The benchmark is also designed to be\nfew-shot (and in the future, zero-shot), with only a few training and\nvalidation examples provided. This report discusses the structure and\nconstruction of the benchmark. Preliminary results suggest that the benchmark\nis challenging even for advanced language representation models designed for\ndiscriminative CSR question answering tasks.\n</p>\n<p>Benchmark access and leaderboard:\nhttps://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website:\nhttps://usc-isi-i2.github.io/TGCSR/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_H/0/1/0/all/0/1\">Henrique Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Ke Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulvehill_A/0/1/0/all/0/1\">Alice M. Mulvehill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razeghi_Y/0/1/0/all/0/1\">Yasaman Razeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_D/0/1/0/all/0/1\">Deborah L. McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1\">Mayank Kejriwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AbductionRules: Training Transformers to Explain Unexpected Inputs. (arXiv:2203.12186v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12186","description":"<p>Transformers have recently been shown to be capable of reliably performing\nlogical reasoning over facts and rules expressed in natural language, but\nabductive reasoning - inference to the best explanation of an unexpected\nobservation - has been underexplored despite significant applications to\nscientific discovery, common-sense reasoning, and model interpretability.\n</p>\n<p>We present AbductionRules, a group of natural language datasets designed to\ntrain and test generalisable abduction over natural-language knowledge bases.\nWe use these datasets to finetune pretrained Transformers and discuss their\nperformance, finding that our models learned generalisable abductive techniques\nbut also learned to exploit the structure of our data. Finally, we discuss the\nviability of this approach to abductive reasoning and ways in which it may be\nimproved in future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Young_N/0/1/0/all/0/1\">Nathan Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qiming Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensemann_J/0/1/0/all/0/1\">Joshua Bensemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Converse -- A Tree-Based Modular Task-Oriented Dialogue System. (arXiv:2203.12187v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12187","description":"<p>Creating a system that can have meaningful conversations with humans to help\naccomplish tasks is one of the ultimate goals of Artificial Intelligence (AI).\nIt has defined the meaning of AI since the beginning. A lot has been\naccomplished in this area recently, with voice assistant products entering our\ndaily lives and chat bot systems becoming commonplace in customer service. At\nfirst glance there seems to be no shortage of options for dialogue systems.\nHowever, the frequently deployed dialogue systems today seem to all struggle\nwith a critical weakness - they are hard to build and harder to maintain. At\nthe core of the struggle is the need to script every single turn of\ninteractions between the bot and the human user. This makes the dialogue\nsystems more difficult to maintain as the tasks become more complex and more\ntasks are added to the system. In this paper, we propose Converse, a flexible\ntree-based modular task-oriented dialogue system. Converse uses an and-or tree\nstructure to represent tasks and offers powerful multi-task dialogue\nmanagement. Converse supports task dependency and task switching, which are\nunique features compared to other open-source dialogue frameworks. At the same\ntime, Converse aims to make the bot building process easy and simple, for both\nprofessional and non-professional software developers. The code is available at\nhttps://github.com/salesforce/Converse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1\">Angela S. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feihong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Young Mo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Michael Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1\">Richard Socher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Expressive Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis. (arXiv:2203.12201v1 [cs.SD])","link":"http://arxiv.org/abs/2203.12201","description":"<p>Previous works on expressive speech synthesis mainly focus on current\nsentence. The context in adjacent sentences is neglected, resulting in\ninflexible speaking style for the same text, which lacks speech variations. In\nthis paper, we propose a hierarchical framework to model speaking style from\ncontext. A hierarchical context encoder is proposed to explore a wider range of\ncontextual information considering structural relationship in context,\nincluding inter-phrase and inter-sentence relations. Moreover, to encourage\nthis encoder to learn style representation better, we introduce a novel\ntraining strategy with knowledge distillation, which provides the target for\nencoder training. Both objective and subjective evaluations on a Mandarin\nlecture dataset demonstrate that the proposed method can significantly improve\nthe naturalness and expressiveness of the synthesized speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Shiyin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Vectorized Lexical Constraints for Neural Machine Translation. (arXiv:2203.12210v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12210","description":"<p>Lexically constrained neural machine translation (NMT), which controls the\ngeneration of NMT models with pre-specified constraints, is important in many\npractical scenarios. Due to the representation gap between discrete constraints\nand continuous vectors in NMT models, most existing works choose to construct\nsynthetic data or modify the decoding algorithm to impose lexical constraints,\ntreating the NMT model as a black box. In this work, we propose to open this\nblack box by directly integrating the constraints into NMT models.\nSpecifically, we vectorize source and target constraints into continuous keys\nand values, which can be utilized by the attention modules of NMT models. The\nproposed integration method is based on the assumption that the correspondence\nbetween keys and values in attention modules is naturally suitable for modeling\nconstraint pairs. Experimental results show that our method consistently\noutperforms several representative baselines on four language pairs,\ndemonstrating the superiority of integrating vectorized lexical constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions. (arXiv:2203.12235v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12235","description":"<p>The syntactic categories of categorial grammar formalisms are structured\nunits made of smaller, indivisible primitives, bound together by the underlying\ngrammar's category formation rules. In the trending approach of constructive\nsupertagging, neural models are increasingly made aware of the internal\ncategory structure, which in turn enables them to more reliably predict rare\nand out-of-vocabulary categories, with significant implications for grammars\npreviously deemed too complex to find practical use. In this work, we revisit\nconstructive supertagging from a graph-theoretic perspective, and propose a\nframework based on heterogeneous dynamic graph convolutions aimed at exploiting\nthe distinctive structure of a supertagger's output space. We test our approach\non a number of categorial grammar datasets spanning different languages and\ngrammar formalisms, achieving substantial improvements over previous state of\nthe art scores. Code will be made available at\nhttps://github.com/konstantinosKokos/dynamic-graph-supertagging\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moortgat_M/0/1/0/all/0/1\">Michael Moortgat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Named Entity Recognition with Self-describing Networks. (arXiv:2203.12252v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12252","description":"<p>Few-shot NER needs to effectively capture information from limited instances\nand transfer useful knowledge from external resources. In this paper, we\npropose a self-describing mechanism for few-shot NER, which can effectively\nleverage illustrative instances and precisely transfer knowledge from external\nresources by describing both entity types and mentions using a universal\nconcept set. Specifically, we design Self-describing Networks (SDNet), a\nSeq2Seq generation model which can universally describe mentions using\nconcepts, automatically map novel entity types to concepts, and adaptively\nrecognize entities on-demand. We pre-train SDNet with large-scale corpus, and\nconduct experiments on 8 benchmarks from different domains. Experiments show\nthat SDNet achieves competitive performances on all benchmarks and achieves the\nnew state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chat-Capsule: A Hierarchical Capsule for Dialog-level Emotion Analysis. (arXiv:2203.12254v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12254","description":"<p>Many studies on dialog emotion analysis focus on utterance-level emotion\nonly. These models hence are not optimized for dialog-level emotion detection,\ni.e. to predict the emotion category of a dialog as a whole. More importantly,\nthese models cannot benefit from the context provided by the whole dialog. In\nreal-world applications, annotations to dialog could fine-grained, including\nboth utterance-level tags (e.g. speaker type, intent category, and emotion\ncategory), and dialog-level tags (e.g. user satisfaction, and emotion curve\ncategory). In this paper, we propose a Context-based Hierarchical Attention\nCapsule~(Chat-Capsule) model, which models both utterance-level and\ndialog-level emotions and their interrelations. On a dialog dataset collected\nfrom customer support of an e-commerce platform, our model is also able to\npredict user satisfaction and emotion curve category. Emotion curve refers to\nthe change of emotions along the development of a conversation. Experiments\nshow that the proposed Chat-Capsule outperform state-of-the-art baselines on\nboth benchmark dataset and proprietary dataset. Source code will be released\nupon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks. (arXiv:2203.12257v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12257","description":"<p>Traditionally, a debate usually requires a manual preparation process,\nincluding reading plenty of articles, selecting the claims, identifying the\nstances of the claims, seeking the evidence for the claims, etc. As the AI\ndebate attracts more attention these years, it is worth exploring the methods\nto automate the tedious process involved in the debating system. In this work,\nwe introduce a comprehensive and large dataset named IAM, which can be applied\nto a series of argument mining tasks, including claim extraction, stance\nclassification, evidence extraction, etc. Our dataset is collected from over 1k\narticles related to 123 topics. Near 70k sentences in the dataset are fully\nannotated based on their argument properties (e.g., claims, stances, evidence,\netc.). We further propose two new integrated argument mining tasks associated\nwith the debate preparation process: (1) claim extraction with stance\nclassification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a\npipeline approach and an end-to-end method for each integrated task separately.\nPromising experimental results are reported to show the values and challenges\nof our proposed tasks, and motivate future research on argument mining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruidan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View. (arXiv:2203.12258v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12258","description":"<p>Prompt-based probing has been widely used in evaluating the abilities of\npretrained language models (PLMs). Unfortunately, recent studies have\ndiscovered such an evaluation may be inaccurate, inconsistent and unreliable.\nFurthermore, the lack of understanding its inner workings, combined with its\nwide applicability, has the potential to lead to unforeseen risks for\nevaluating and applying PLMs in real-world applications. To discover,\nunderstand and quantify the risks, this paper investigates the prompt-based\nprobing from a causal view, highlights three critical biases which could induce\nbiased results and conclusions, and proposes to conduct debiasing via causal\nintervention. This paper provides valuable insights for the design of unbiased\ndatasets, better probing frameworks and more reliable evaluations of pretrained\nlanguage models. Furthermore, our conclusions also echo that we need to rethink\nthe criteria for identifying better pretrained language models. We openly\nreleased the source code and data at https://github.com/c-box/causalEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Boxi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECO v1: Towards Event-Centric Opinion Mining. (arXiv:2203.12264v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12264","description":"<p>Events are considered as the fundamental building blocks of the world. Mining\nevent-centric opinions can benefit decision making, people communication, and\nsocial good. Unfortunately, there is little literature addressing event-centric\nopinion mining, although which significantly diverges from the well-studied\nentity-centric opinion mining in connotation, structure, and expression. In\nthis paper, we propose and formulate the task of event-centric opinion mining\nbased on event-argument structure and expression categorizing theory. We also\nbenchmark this task by constructing a pioneer corpus and designing a two-step\nbenchmark framework. Experiment results show that event-centric opinion mining\nis feasible and challenging, and the proposed task, dataset, and baselines are\nbeneficial for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruoxi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Meng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingfei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training to Match for Unified Low-shot Relation Extraction. (arXiv:2203.12274v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12274","description":"<p>Low-shot relation extraction~(RE) aims to recognize novel relations with very\nfew or even no samples, which is critical in real scenario application.\nFew-shot and zero-shot RE are two representative low-shot RE tasks, which seem\nto be with similar target but require totally different underlying abilities.\nIn this paper, we propose Multi-Choice Matching Networks to unify low-shot\nrelation extraction. To fill in the gap between zero-shot and few-shot RE, we\npropose the triplet-paraphrase meta-training, which leverages triplet\nparaphrase to pre-train zero-shot label matching ability and uses meta-learning\nparadigm to learn few-shot instance summarizing ability. Experimental results\non three different low-shot RE tasks show that the proposed method outperforms\nstrong baselines by a large margin, and achieve the best performance on\nfew-shot RE leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Boxi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through Regularized Self-Attention. (arXiv:2203.12276v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12276","description":"<p>Sparse Transformer has recently attracted a lot of attention since the\nability for reducing the quadratic dependency on the sequence length. We argue\nthat two factors, information bottleneck sensitivity and inconsistency between\ndifferent attention topologies, could affect the performance of the Sparse\nTransformer. This paper proposes a well-designed model named ERNIE-Sparse. It\nconsists of two distinctive parts: (i) Hierarchical Sparse Transformer (HST) to\nsequentially unify local and global information. (ii) Self-Attention\nRegularization (SAR) method, a novel regularization designed to minimize the\ndistance for transformers with different attention topologies. To evaluate the\neffectiveness of ERNIE-Sparse, we perform extensive evaluations. Firstly, we\nperform experiments on a multi-modal long sequence modeling task benchmark,\nLong Range Arena (LRA). Experimental results demonstrate that ERNIE-Sparse\nsignificantly outperforms a variety of strong baseline methods including the\ndense attention and other efficient sparse attention methods and achieves\nimprovements by 2.77% (57.78% vs. 55.01%). Secondly, to further show the\neffectiveness of our method, we pretrain ERNIE-Sparse and verified it on 3 text\nclassification and 2 QA downstream tasks, achieve improvements on\nclassification benchmark by 0.83% (92.46% vs. 91.63%), on QA benchmark by 3.24%\n(74.67% vs. 71.43%). Experimental results continue to demonstrate its superior\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuxiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhida Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Structure Generation for Universal Information Extraction. (arXiv:2203.12277v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12277","description":"<p>Information extraction suffers from its varying targets, heterogeneous\nstructures, and demand-specific schemas. In this paper, we propose a unified\ntext-to-structure generation framework, namely UIE, which can universally model\ndifferent IE tasks, adaptively generate targeted structures, and\ncollaboratively learn general IE abilities from different knowledge sources.\nSpecifically, UIE uniformly encodes different extraction structures via a\nstructured extraction language, adaptively generates target extractions via a\nschema-based prompt mechanism - structural schema instructor, and captures the\ncommon IE abilities via a large-scale pre-trained text-to-structure model.\nExperiments show that UIE achieved the state-of-the-art performance on 4 IE\ntasks, 13 datasets, and on all supervised, low-resource, and few-shot settings\nfor a wide range of entity, relation, event and sentiment extraction tasks and\ntheir unification. These results verified the effectiveness, universality, and\ntransferability of UIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaojie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Input-specific Attention Subnetworks for Adversarial Detection. (arXiv:2203.12298v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12298","description":"<p>Self-attention heads are characteristic of Transformer models and have been\nwell studied for interpretability and pruning. In this work, we demonstrate an\naltogether different utility of attention heads, namely for adversarial\ndetection. Specifically, we propose a method to construct input-specific\nattention subnetworks (IAS) from which we extract three features to\ndiscriminate between authentic and adversarial inputs. The resultant detector\nsignificantly improves (by over 7.5%) the state-of-the-art adversarial\ndetection accuracy for the BERT encoder on 10 NLU datasets with 11 different\nadversarial attack types. We also demonstrate that our method (a) is more\naccurate for larger models which are likely to have more spurious correlations\nand thus vulnerable to adversarial attack, and (b) performs well even with\nmodest training sets of adversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biju_E/0/1/0/all/0/1\">Emil Biju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Anirudh Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Fast Polarity Labelling of Massive Data Streams. (arXiv:2203.12368v1 [cs.DB])","link":"http://arxiv.org/abs/2203.12368","description":"<p>Many of the existing sentiment analysis techniques are based on supervised\nlearning, and they demand the availability of valuable training datasets to\ntrain their models. When dataset freshness is critical, the annotating of high\nspeed unlabelled data streams becomes critical but remains an open problem. In\nthis paper, we propose PLStream, a novel Apache Flink-based framework for fast\npolarity labelling of massive data streams, like Twitter tweets or online\nproduct reviews. We address the associated implementation challenges and\npropose a list of techniques including both algorithmic improvements and system\noptimizations. A thorough empirical validation with two real-world workloads\ndemonstrates that PLStream is able to generate high quality labels (almost 80%\naccuracy) in the presence of high-speed continuous unlabelled data streams\n(almost 16,000 tuples/sec) without any manual efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huilin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Mian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey On Semantic Steganography Systems. (arXiv:2203.12425v1 [cs.CR])","link":"http://arxiv.org/abs/2203.12425","description":"<p>Steganography is the practice of concealing a message within some other\ncarrier or cover message. It is used to allow the sending of hidden information\nthrough communication channels where third parties would only be aware of the\nexplicit information in the carrier message. With the growth of internet\nsurveillance and the increased need for secret communication, steganography\nsystems continue to find new applications. In semantic steganography, the\nredundancies in the semantics of a language are used to send a text\nsteganographic message. In this article we go over the concepts behind semantic\nsteganography and propose a hierarchy for classifying systems within the\ncontext of text steganography and steganography in general. After laying this\ngroundwork we list systems for semantic steganography that have been published\nin the past and review their properties. Finally, we comment on and briefly\ncompare the described systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Figueira_J/0/1/0/all/0/1\">Jo&#xe3;o Figueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The VoicePrivacy 2022 Challenge Evaluation Plan. (arXiv:2203.12468v1 [eess.AS])","link":"http://arxiv.org/abs/2203.12468","description":"<p>For new participants - Executive summary: (1) The task is to develop a voice\nanonymization system for speech data which conceals the speaker's voice\nidentity while protecting linguistic content, paralinguistic attributes,\nintelligibility and naturalness. (2) Training, development and evaluation\ndatasets are provided in addition to 3 different baseline anonymization\nsystems, evaluation scripts, and metrics. Participants apply their developed\nanonymization systems, run evaluation scripts and submit objective evaluation\nresults and anonymized speech data to the organizers. (3) Results will be\npresented at a workshop held in conjunction with INTERSPEECH 2022 to which all\nparticipants are invited to present their challenge systems and to submit\nadditional workshop papers.\n</p>\n<p>For readers familiar with the VoicePrivacy Challenge - Changes w.r.t. 2020:\n(1) A stronger, semi-informed attack model in the form of an automatic speaker\nverification (ASV) system trained on anonymized (per-utterance) speech data.\n(2) Complementary metrics comprising the equal error rate (EER) as a privacy\nmetric, the word error rate (WER) as a primary utility metric, and the pitch\ncorrelation and gain of voice distinctiveness as secondary utility metrics. (3)\nA new ranking policy based upon a set of minimum target privacy requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_X/0/1/0/all/0/1\">Xiaoxiao Miao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nourtel_H/0/1/0/all/0/1\">Hubert Nourtel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Champion_P/0/1/0/all/0/1\">Pierre Champion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonastre_J/0/1/0/all/0/1\">Jean Fran&#xe7;ois Bonastre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based Pre-trained Model for Personality and Interpersonal Reactivity Prediction. (arXiv:2203.12481v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12481","description":"<p>This paper describes the LingJing team's method to the Workshop on\nComputational Approaches to Subjectivity, Sentiment &amp; Social Media Analysis\n(WASSA) 2022 shared task on Personality Prediction (PER) and Reactivity Index\nPrediction (IRI). In this paper, we adopt the prompt-based method with the\npre-trained language model to accomplish these tasks. Specifically, the prompt\nis designed to provide the extra knowledge for enhancing the pre-trained model.\nData augmentation and model ensemble are adopted for obtaining better results.\nExtensive experiments are performed, which shows the effectiveness of the\nproposed method. On the final submission, our system achieves a Pearson\nCorrelation Coefficient of 0.2301 and 0.2546 on Track 3 and Track 4\nrespectively. We ranked Top-1 on both sub-tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qiya Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fuyan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Context-Aware Feature Fusion Framework for Punctuation Restoration. (arXiv:2203.12487v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12487","description":"<p>To accomplish the punctuation restoration task, most existing approaches\nfocused on leveraging extra information (e.g., part-of-speech tags) or\naddressing the class imbalance problem. Recent works have widely applied the\ntransformer-based language models and significantly improved their\neffectiveness. To the best of our knowledge, an inherent issue has remained\nneglected: the attention of individual heads in the transformer will be diluted\nor powerless while feeding the long non-punctuation utterances. Since those\nprevious contexts, not the followings, are comparatively more valuable to the\ncurrent position, it's hard to achieve a good balance by independent attention.\nIn this paper, we propose a novel Feature Fusion framework based on two-type\nAttentions (FFA) to alleviate the shortage. It introduces a two-stream\narchitecture. One module involves interaction between attention heads to\nencourage the communication, and another masked attention module captures the\ndependent feature representation. Then, it aggregates two feature embeddings to\nfuse information and enhances context-awareness. The experiments on the popular\nbenchmark dataset IWSLT demonstrate that our approach is effective. Without\nadditional data, it obtains comparable performance to the current\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangjun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kebin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Cross-Lingual Summarization. (arXiv:2203.12515v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12515","description":"<p>Cross-lingual summarization is the task of generating a summary in one\nlanguage (e.g., English) for the given document(s) in a different language\n(e.g., Chinese). Under the globalization background, this task has attracted\nincreasing attention of the computational linguistics community. Nevertheless,\nthere still remains a lack of comprehensive review for this task. Therefore, we\npresent the first systematic critical review on the datasets, approaches and\nchallenges in this field. Specifically, we carefully organize existing datasets\nand approaches according to different construction methods and solution\nparadigms, respectively. For each type of datasets or approaches, we thoroughly\nintroduce and summarize previous efforts and further compare them with each\nother to provide deeper analyses. In the end, we also discuss promising\ndirections and offer our thoughts to facilitate future research. This survey is\nfor both beginners and experts in cross-lingual summarization, and we hope it\nwill serve as a starting point as well as a source of new ideas for researchers\nand engineers interested in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational historical linguistics and language diversity in South Asia. (arXiv:2203.12524v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12524","description":"<p>South Asia is home to a plethora of languages, many of which severely lack\naccess to new language technologies. This linguistic diversity also results in\na research environment conducive to the study of comparative, contact, and\nhistorical linguistics -- fields which necessitate the gathering of extensive\ndata from many languages. We claim that data scatteredness (rather than\nscarcity) is the primary obstacle in the development of South Asian language\ntechnology, and suggest that the study of language history is uniquely aligned\nwith surmounting this obstacle. We review recent developments in and at the\nintersection of South Asian NLP and historical-comparative linguistics,\ndescribing our and others' current efforts in this area. We also offer new\nstrategies towards breaking the data barrier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Aryaman Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farris_A/0/1/0/all/0/1\">Adam Farris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Samopriya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolichala_S/0/1/0/all/0/1\">Suresh Kolichala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection. (arXiv:2203.12536v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12536","description":"<p>Hate speech classifiers exhibit substantial performance degradation when\nevaluated on datasets different from the source. This is due to learning\nspurious correlations between words that are not necessarily relevant to\nhateful language, and hate speech labels from the training corpus. Previous\nwork has attempted to mitigate this problem by regularizing specific terms from\npre-defined static dictionaries. While this has been demonstrated to improve\nthe generalizability of classifiers, the coverage of such methods is limited\nand the dictionaries require regular manual updates from human experts. In this\npaper, we propose to automatically identify and reduce spurious correlations\nusing attribution methods with dynamic refinement of the list of terms that\nneed to be regularized during training. Our approach is flexible and improves\nthe cross-corpora performance over previous work independently and in\ncombination with pre-defined dictionaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_T/0/1/0/all/0/1\">Tulika Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Illina_I/0/1/0/all/0/1\">Irina Illina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fohr_D/0/1/0/all/0/1\">Dominique Fohr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal. (arXiv:2203.12574v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12574","description":"<p>Language models excel at generating coherent text, and model compression\ntechniques such as knowledge distillation have enabled their use in\nresource-constrained settings. However, these models can be biased in multiple\nways, including the unfounded association of male and female genders with\ngender-neutral professions. Therefore, knowledge distillation without any\nfairness constraints may preserve or exaggerate the teacher model's biases onto\nthe distilled model. To this end, we present a novel approach to mitigate\ngender disparity in text generation by learning a fair model during knowledge\ndistillation. We propose two modifications to the base knowledge distillation\nbased on counterfactual role reversal$\\unicode{x2014}$modifying teacher\nprobabilities and augmenting the training set. We evaluate gender polarity\nacross professions in open-ended text generated from the resulting distilled\nand finetuned GPT$\\unicode{x2012}$2 models and demonstrate a substantial\nreduction in gender disparity with only a minor compromise in utility. Finally,\nwe observe that language models that reduce gender polarity in language\ngeneration do not improve embedding fairness or downstream classification\nfairness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1\">Umang Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Apurv Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1\">Yada Pruksachatkun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1\">Satyapriya Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tokenization Repair in the Presence of Spelling Errors. (arXiv:2010.07878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.07878","description":"<p>We consider the following tokenization repair problem: Given a natural\nlanguage text with any combination of missing or spurious spaces, correct\nthese. Spelling errors can be present, but it's not part of the problem to\ncorrect them. For example, given: \"Tispa per isabout token izaionrep air\",\ncompute \"Tis paper is about tokenizaion repair\". We identify three key\ningredients of high-quality tokenization repair, all missing from previous\nwork: deep language models with a bidirectional component, training the models\non text with spelling errors, and making use of the space information already\npresent. Our methods also improve existing spell checkers by fixing not only\nmore tokenization errors but also more spelling errors: once it is clear which\ncharacters form a word, it is much easier for them to figure out the correct\nword. We provide six benchmarks that cover three use cases (OCR errors, text\nextraction from PDF, human errors) and the cases of partially correct space\ninformation and all spaces missing. We evaluate our methods against the best\nexisting methods and a non-trivial baseline. We provide full reproducibility\nunder https://ad.cs.uni-freiburg.de/publications .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bast_H/0/1/0/all/0/1\">Hannah Bast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertel_M/0/1/0/all/0/1\">Matthias Hertel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M. Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing. (arXiv:2104.04736v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04736","description":"<p>Meta-learning, or learning to learn, is a technique that can help to overcome\nresource scarcity in cross-lingual NLP problems, by enabling fast adaptation to\nnew tasks. We apply model-agnostic meta-learning (MAML) to the task of\ncross-lingual dependency parsing. We train our model on a diverse set of\nlanguages to learn a parameter initialization that can adapt quickly to new\nlanguages. We find that meta-learning with pre-training can significantly\nimprove upon the performance of language transfer and standard supervised\nlearning baselines for a variety of unseen, typologically diverse, and\nlow-resource languages, in a few-shot learning setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Langedijk_A/0/1/0/all/0/1\">Anna Langedijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1\">Verna Dankers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bos_S/0/1/0/all/0/1\">Sander Bos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guevara_B/0/1/0/all/0/1\">Bryan Cardenas Guevara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Packed Levitated Marker for Entity and Relation Extraction. (arXiv:2109.06067v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06067","description":"<p>Recent entity and relation extraction works focus on investigating how to\nobtain a better span representation from the pre-trained encoder. However, a\nmajor limitation of existing works is that they ignore the interrelation\nbetween spans (pairs). In this work, we propose a novel span representation\napproach, named Packed Levitated Markers (PL-Marker), to consider the\ninterrelation between the spans (pairs) by strategically packing the markers in\nthe encoder. In particular, we propose a neighborhood-oriented packing\nstrategy, which considers the neighbor spans integrally to better model the\nentity boundary information. Furthermore, for those more complicated span pair\nclassification tasks, we design a subject-oriented packing strategy, which\npacks each subject and all its objects to model the interrelation between the\nsame-subject span pairs. The experimental results show that, with the enhanced\nmarker feature, our model advances baselines on six NER benchmarks, and obtains\na 4.1%-4.3% strict relation F1 improvement with higher speed over previous\nstate-of-the-art models on ACE04 and ACE05.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical prosody modeling and control in non-autoregressive parallel neural TTS. (arXiv:2110.02952v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.02952","description":"<p>Neural text-to-speech (TTS) synthesis can generate speech that is\nindistinguishable from natural speech. However, the synthetic speech often\nrepresents the average prosodic style of the database instead of having more\nversatile prosodic variation. Moreover, many models lack the ability to control\nthe output prosody, which does not allow for different styles for the same text\ninput. In this work, we train a non-autoregressive parallel neural TTS\nfront-end model hierarchically conditioned on both coarse and fine-grained\nacoustic speech features to learn a latent prosody space with intuitive and\nmeaningful dimensions. Experiments show that a non-autoregressive TTS model\nhierarchically conditioned on utterance-wise pitch, pitch range, duration,\nenergy, and spectral tilt can effectively control each prosodic dimension,\ngenerate a wide variety of speaking styles, and provide word-wise emphasis\ncontrol, while maintaining equal or better quality to the baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Raitio_T/0/1/0/all/0/1\">Tuomo Raitio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangchuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seshadri_S/0/1/0/all/0/1\">Shreyas Seshadri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. (arXiv:2110.07577v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07577","description":"<p>Recent parameter-efficient language model tuning (PELT) methods manage to\nmatch the performance of fine-tuning with much fewer trainable parameters and\nperform especially well when training data is limited. However, different PELT\nmethods may perform rather differently on the same task, making it nontrivial\nto select the most appropriate method for a specific task, especially\nconsidering the fast-growing number of new PELT methods and tasks. In light of\nmodel diversity and the difficulty of model selection, we propose a unified\nframework, UniPELT, which incorporates different PELT methods as submodules and\nlearns to activate the ones that best suit the current data or task setup via\ngating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1~4%\ngains compared to the best individual PELT method that it incorporates and even\noutperforms fine-tuning under different setups. Moreover, UniPELT generally\nsurpasses the upper bound that takes the best performance of all its submodules\nused individually on each task, indicating that a mixture of multiple PELT\nmethods may be inherently more effective than single methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almahairi_A/0/1/0/all/0/1\">Amjad Almahairi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models. (arXiv:2110.08527v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08527","description":"<p>Recent work has shown pre-trained language models capture social biases from\nthe large amounts of text they are trained on. This has attracted attention to\ndeveloping techniques that mitigate such biases. In this work, we perform an\nempirical survey of five recently proposed bias mitigation techniques:\nCounterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace\nProjection, Self-Debias, and SentenceDebias. We quantify the effectiveness of\neach technique using three intrinsic bias benchmarks while also measuring the\nimpact of these techniques on a model's language modeling ability, as well as\nits performance on downstream NLU tasks. We experimentally find that: (1)\nSelf-Debias is the strongest debiasing technique, obtaining improved scores on\nall bias benchmarks; (2) Current debiasing techniques perform less consistently\nwhen mitigating non-gender biases; And (3) improvements on bias benchmarks such\nas StereoSet and CrowS-Pairs by using debiasing strategies are often\naccompanied by a decrease in language modeling ability, making it difficult to\ndetermine whether the bias mitigation was effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1\">Nicholas Meade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_Dayan_E/0/1/0/all/0/1\">Elinor Poole-Dayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triples-to-Text Generation with Reinforcement Learning Based Graph-augmented Neural Networks. (arXiv:2111.10545v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10545","description":"<p>Considering a collection of RDF triples, the RDF-to-text generation task aims\nto generate a text description. Most previous methods solve this task using a\nsequence-to-sequence model or using a graph-based model to encode RDF triples\nand to generate a text sequence. Nevertheless, these approaches fail to clearly\nmodel the local and global structural information between and within RDF\ntriples. Moreover, the previous methods also face the non-negligible problem of\nlow faithfulness of the generated text, which seriously affects the overall\nperformance of these models. To solve these problems, we propose a model\ncombining two new graph-augmented structural neural encoders to jointly learn\nboth local and global structural information in the input RDF triples. To\nfurther improve text faithfulness, we innovatively introduce a reinforcement\nlearning (RL) reward based on information extraction (IE). We first extract\ntriples from the generated text using a pretrained IE model and regard the\ncorrect number of the extracted triples as the additional RL reward.\nExperimental results on two benchmark datasets demonstrate that our proposed\nmodel outperforms the state-of-the-art baselines, and the additional\nreinforcement learning reward does help to improve the faithfulness of the\ngenerated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hanning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Po Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v10 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Bounded Context-Free-Grammar via LSTM and the Transformer:Difference and Explanations. (arXiv:2112.09174v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09174","description":"<p>Long Short-Term Memory (LSTM) and Transformers are two popular neural\narchitectures used for natural language processing tasks. Theoretical results\nshow that both are Turing-complete and can represent any context-free language\n(CFL).In practice, it is often observed that Transformer models have better\nrepresentation power than LSTM. But the reason is barely understood. We study\nsuch practical differences between LSTM and Transformer and propose an\nexplanation based on their latent space decomposition patterns. To achieve this\ngoal, we introduce an oracle training paradigm, which forces the decomposition\nof the latent representation of LSTM and the Transformer and supervises with\nthe transitions of the Pushdown Automaton (PDA) of the corresponding CFL. With\nthe forced decomposition, we show that the performance upper bounds of LSTM and\nTransformer in learning CFL are close: both of them can simulate a stack and\nperform stack operation along with state transitions. However, the absence of\nforced decomposition leads to the failure of LSTM models to capture the stack\nand stack operations, while having a marginal impact on the Transformer model.\nLastly, we connect the experiment on the prototypical PDA to a real-world\nparsing task to re-verify the conclusions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Sicun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jishen Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better. (arXiv:2202.12024v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12024","description":"<p>Effectively finetuning pretrained language models (PLMs) is critical for\ntheir success in downstream tasks. However, PLMs may have risks in overfitting\nthe pretraining tasks and data, which usually have gap with the target\ndownstream tasks. Such gap may be difficult for existing PLM finetuning methods\nto overcome and lead to suboptimal performance. In this paper, we propose a\nvery simple yet effective method named NoisyTune to help better finetune PLMs\non downstream tasks by adding some noise to the parameters of PLMs before\nfine-tuning. More specifically, we propose a matrix-wise perturbing method\nwhich adds different uniform noises to different parameter matrices based on\ntheir standard deviations. In this way, the varied characteristics of different\ntypes of parameters in PLMs can be considered. Extensive experiments on both\nGLUE English benchmark and XTREME multilingual benchmark show NoisyTune can\nconsistently empower the finetuning of different PLMs on different downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models. (arXiv:2202.13392v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13392","description":"<p>Pre-trained language models (PLMs) cannot well recall rich factual knowledge\nof entities exhibited in large-scale corpora, especially those rare entities.\nIn this paper, we propose to build a simple but effective Pluggable Entity\nLookup Table (PELT) on demand by aggregating the entity's output\nrepresentations of multiple occurrences in the corpora. PELT can be compatibly\nplugged as inputs to infuse supplemental entity knowledge into PLMs. Compared\nto previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation\nwith capability of acquiring knowledge from out-of-domain corpora for domain\nadaptation scenario. The experiments on knowledge-related tasks demonstrate\nthat our method, PELT, can flexibly and effectively transfer entity knowledge\nfrom related corpora into PLMs with different architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages. (arXiv:2203.01976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01976","description":"<p>Pre-trained multilingual language models such as mBERT and XLM-R have\ndemonstrated great potential for zero-shot cross-lingual transfer to low\nweb-resource languages (LRL). However, due to limited model capacity, the large\ndifference in the sizes of available monolingual corpora between high\nweb-resource languages (HRL) and LRLs does not provide enough scope of\nco-embedding the LRL with the HRL, thereby affecting downstream task\nperformance of LRLs. In this paper, we argue that relatedness among languages\nin a language family along the dimension of lexical overlap may be leveraged to\novercome some of the corpora limitations of LRLs. We propose Overlap BPE\n(OBPE), a simple yet effective modification to the BPE vocabulary generation\nalgorithm which enhances overlap across related languages. Through extensive\nexperiments on multiple NLP tasks and datasets, we observe that OBPE generates\na vocabulary that increases the representation of LRLs via tokens shared with\nHRLs. This results in improved zero-shot transfer from related HRLs to LRLs\nwithout reducing HRL representation and accuracy. Unlike previous studies that\ndismissed the importance of token-overlap, we show that in the low-resource\nrelated language setting, token overlap matters. Synthetically reducing the\noverlap to zero can cause as much as a four-fold drop in zero-shot transfer\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1\">Vaidehi Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification. (arXiv:2203.03825v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03825","description":"<p>Hierarchical text classification is a challenging subtask of multi-label\nclassification due to its complex label hierarchy. Existing methods encode text\nand label hierarchy separately and mix their representations for\nclassification, where the hierarchy remains unchanged for all input text.\nInstead of modeling them separately, in this work, we propose Hierarchy-guided\nContrastive Learning (HGCLR) to directly embed the hierarchy into a text\nencoder. During training, HGCLR constructs positive samples for input text\nunder the guidance of the label hierarchy. By pulling together the input text\nand its positive sample, the text encoder can learn to generate the\nhierarchy-aware text representation independently. Therefore, after training,\nthe HGCLR enhanced text encoder can dispense with the redundant hierarchy.\nExtensive experiments on three benchmark datasets verify the effectiveness of\nHGCLR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lianzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms other\nstate-of-the-art (SOTA) methods by 28.36 in mIOU score with a large margin,\nwhich demonstrates the effectiveness of visual prompt and the text span\npredictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Word Translation via Two-Stage Contrastive Learning. (arXiv:2203.08307v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08307","description":"<p>Word translation or bilingual lexicon induction (BLI) is a key cross-lingual\ntask, aiming to bridge the lexical gap between different languages. In this\nwork, we propose a robust and effective two-stage contrastive learning\nframework for the BLI task. At Stage C1, we propose to refine standard\ncross-lingual linear maps between static word embeddings (WEs) via a\ncontrastive learning objective; we also show how to integrate it into the\nself-learning procedure for even more refined cross-lingual maps. In Stage C2,\nwe conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word\ntranslation capability. We also show that static WEs induced from the\n`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments\non standard BLI datasets for diverse languages and different experimental\nsetups demonstrate substantial gains achieved by our framework. While the BLI\nmethod from Stage C1 already yields substantial gains over all state-of-the-art\nBLI methods in our comparison, even stronger improvements are met with the full\ntwo-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28\nlanguage pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaoyiran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation. (arXiv:2203.08394v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08394","description":"<p>Back-translation is a critical component of Unsupervised Neural Machine\nTranslation (UNMT), which generates pseudo parallel data from target\nmonolingual data. A UNMT model is trained on the pseudo parallel data with\ntranslated source, and translates natural source sentences in inference. The\nsource discrepancy between training and inference hinders the translation\nperformance of UNMT models. By carefully designing experiments, we identify two\nrepresentative characteristics of the data gap in source: (1) style gap (i.e.,\ntranslated vs. natural text style) that leads to poor generalization\ncapability; (2) content gap that induces the model to produce hallucination\ncontent biased towards the target language. To narrow the data gap, we propose\nan online self-training approach, which simultaneously uses the pseudo parallel\ndata {natural source, translated target} to mimic the inference scenario.\nExperimental results on several widely-used language pairs show that our\napproach outperforms two strong baselines (XLM and MASS) by remedying the style\nand content gaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework. (arXiv:2203.09053v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09053","description":"<p>Simultaneous machine translation (SiMT) starts translating while receiving\nthe streaming source inputs, and hence the source sentence is always incomplete\nduring translating. Different from the full-sentence MT using the conventional\nseq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,\nwhich forces each target word to only align with a partial source prefix to\nadapt to the incomplete source in streaming inputs. However, the source words\nin the front positions are always illusoryly considered more important since\nthey appear in more prefixes, resulting in position bias, which makes the model\npay more attention on the front source positions in testing. In this paper, we\nfirst analyze the phenomenon of position bias in SiMT, and develop a\nLength-Aware Framework to reduce the position bias by bridging the structural\ngap between SiMT and full-sentence MT. Specifically, given the streaming\ninputs, we first predict the full-sentence length and then fill the future\nsource position with positional encoding, thereby turning the streaming inputs\ninto a pseudo full-sentence. The proposed framework can be integrated into most\nexisting SiMT methods to further improve performance. Experiments on two\nrepresentative SiMT methods, including the state-of-the-art adaptive policy,\nshow that our method successfully reduces the position bias and thereby\nachieves better SiMT performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRS: Combining Generation and Revision in Unsupervised Sentence Simplification. (arXiv:2203.09742v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09742","description":"<p>We propose GRS: an unsupervised approach to sentence simplification that\ncombines text generation and text revision. We start with an iterative\nframework in which an input sentence is revised using explicit edit operations,\nand add paraphrasing as a new edit operation. This allows us to combine the\nadvantages of generative and revision-based approaches: paraphrasing captures\ncomplex edit operations, and the use of explicit edit operations in an\niterative manner provides controllability and interpretability. We demonstrate\nthese advantages of GRS compared to existing methods on the Newsela and ASSET\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehghan_M/0/1/0/all/0/1\">Mohammad Dehghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1\">Lukasz Golab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CM-GAN: Image Inpainting with Cascaded Modulation GAN and Object-Aware Training. (arXiv:2203.11947v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11947","description":"<p>Recent image inpainting methods have made great progress but often struggle\nto generate plausible image structures when dealing with large holes in complex\nimages. This is partially due to the lack of effective network structures that\ncan capture both the long-range dependency and high-level semantics of an\nimage. To address these problems, we propose cascaded modulation GAN (CM-GAN),\na new network design consisting of an encoder with Fourier convolution blocks\nthat extract multi-scale feature representations from the input image with\nholes and a StyleGAN-like decoder with a novel cascaded global-spatial\nmodulation block at each scale level. In each decoder block, global modulation\nis first applied to perform coarse semantic-aware structure synthesis, then\nspatial modulation is applied on the output of global modulation to further\nadjust the feature map in a spatially adaptive fashion. In addition, we design\nan object-aware training scheme to prevent the network from hallucinating new\nobjects inside holes, fulfilling the needs of object removal tasks in\nreal-world scenarios. Extensive experiments are conducted to show that our\nmethod significantly outperforms existing methods in both quantitative and\nqualitative evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_C/0/1/0/all/0/1\">Connelly Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirghodsi_S/0/1/0/all/0/1\">Sohrab Amirghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Patch-to-Cluster Attention in Vision Transformer. (arXiv:2203.11987v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11987","description":"<p>The Vision Transformer (ViT) model is built on the assumption of treating\nimage patches as \"visual tokens\" and learning patch-to-patch attention. The\npatch embedding based tokenizer is a workaround in practice and has a semantic\ngap with respect to its counterpart, the textual tokenizer. The patch-to-patch\nattention suffers from the quadratic complexity issue, and also makes it\nnon-trivial to explain learned ViT models. To address these issues in ViT\nmodels, this paper proposes to learn patch-to-cluster attention (PaCa) based\nViT models. Queries in our PaCaViT are based on patches, while keys and values\nare based on clustering (with a predefined small number of clusters). The\nclusters are learned end-to-end, leading to better tokenizers and realizing\njoint clustering-for-attention and attention-for-clustering when deployed in\nViT models. The quadratic complexity is relaxed to linear complexity. Also,\ndirectly visualizing the learned clusters can reveal how a trained ViT model\nlearns to perform a task (e.g., object detection). In experiments, the proposed\nPaCa-ViT is tested on CIFAR-100 and ImageNet-1000 image classification, and\nMS-COCO object detection and instance segmentation. Compared with prior arts,\nit obtains better performance in classification and comparable performance in\ndetection and segmentation. It is significantly more efficient in COCO due to\nthe linear complexity. The learned clusters are also semantically meaningful\nand shed light on designing more discriminative yet interpretable ViT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grainger_R/0/1/0/all/0/1\">Ryan Grainger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paniagua_T/0/1/0/all/0/1\">Thomas Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework. (arXiv:2203.11991v1 [cs.CV])","link":"http://arxiv.org/abs/2203.11991","description":"<p>The current popular two-stream, two-stage tracking framework extracts the\ntemplate and the search region features separately and then performs relation\nmodeling, thus the extracted features lack the awareness of the target and have\nlimited target-background discriminability. To tackle the above issue, we\npropose a novel one-stream tracking (OSTrack) framework that unifies feature\nlearning and relation modeling by bridging the template-search image pairs with\nbidirectional information flows. In this way, discriminative target-oriented\nfeatures can be dynamically extracted by mutual guidance. Since no extra heavy\nrelation modeling module is needed and the implementation is highly\nparallelized, the proposed tracker runs at a fast speed. To further improve the\ninference efficiency, an in-network candidate early elimination module is\nproposed based on the strong similarity prior calculated in the one-stream\nframework. As a unified framework, OSTrack achieves state-of-the-art\nperformance on multiple benchmarks, in particular, it shows impressive results\non the one-shot tracking benchmark GOT-10k, i.e., achieving 73.7% AO, improving\nthe existing best result (SwinTrack) by 4.3%. Besides, our method maintains a\ngood performance-speed trade-off and shows faster convergence. The code and\nmodels will be available at https://github.com/botaoye/OSTrack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1\">Botao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Bingpeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Geodesic-Aware Local Features from RGB-D Images. (arXiv:2203.12016v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12016","description":"<p>Most of the existing handcrafted and learning-based local descriptors are\nstill at best approximately invariant to affine image transformations, often\ndisregarding deformable surfaces. In this paper, we take one step further by\nproposing a new approach to compute descriptors from RGB-D images (where RGB\nrefers to the pixel color brightness and D stands for depth information) that\nare invariant to isometric non-rigid deformations, as well as to scale changes\nand rotation. Our proposed description strategies are grounded on the key idea\nof learning feature representations on undistorted local image patches using\nsurface geodesics. We design two complementary local descriptors strategies to\ncompute geodesic-aware features efficiently: one efficient binary descriptor\nbased on handcrafted binary tests (named GeoBit), and one learning-based\ndescriptor (GeoPatch) with convolutional neural networks (CNNs) to compute\nfeatures. In different experiments using real and publicly available RGB-D data\nbenchmarks, they consistently outperforms state-of-the-art handcrafted and\nlearning-based image and RGB-D descriptors in matching scores, as well as in\nobject retrieval and non-rigid surface tracking experiments, with comparable\nprocessing times. We also provide to the community a new dataset with accurate\nmatching annotations of RGB-D images of different objects (shirts, cloths,\npaintings, bags), subjected to strong non-rigid deformations, for evaluation\nbenchmark of deformable surface correspondence algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Potje_G/0/1/0/all/0/1\">Guilherme Potje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1\">Renato Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadar_F/0/1/0/all/0/1\">Felipe Cadar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1\">Erickson R. Nascimento</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v1 [cs.LG])","link":"http://arxiv.org/abs/2203.12023","description":"<p>Many promising applications of supervised machine learning face hurdles in\nthe acquisition of labeled data in sufficient quantity and quality, creating an\nexpensive bottleneck. To overcome such limitations, techniques that do not\ndepend on ground truth labels have been developed, including weak supervision\nand generative modeling. While these techniques would seem to be usable in\nconcert, improving one another, how to build an interface between them is not\nwell-understood. In this work, we propose a model fusing weak supervision and\ngenerative adversarial networks. It captures discrete variables in the data\nalongside the weak supervision derived label estimate. Their alignment allows\nfor better modeling of sample-dependent accuracies of the weak supervision\nsources, improving the unobserved ground truth estimate. It is the first\napproach to enable data augmentation through weakly supervised synthetic images\nand pseudolabels. Additionally, its learned discrete variables can be inspected\nqualitatively. The model outperforms baseline weak supervision label models on\na number of multiclass classification datasets, improves the quality of\ngenerated images, and further improves end-model performance through data\naugmentation with synthetic samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1\">Benedikt Boecking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1\">Nicholas Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1\">Frederic Sala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1\">Artur Dubrawski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervision through Random Segments with Autoregressive Coding (RandSAC). (arXiv:2203.12054v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12054","description":"<p>Inspired by the success of self-supervised autoregressive representation\nlearning in natural language (GPT and its variants), and advances in recent\nvisual architecture design with Vision Transformers (ViTs), in this paper, we\nexplore the effects various design choices have on the success of applying such\ntraining strategies for visual feature learning. Specifically, we introduce a\nnovel strategy that we call Random Segments with Autoregressive Coding\n(RandSAC). In RandSAC, we group patch representations (image tokens) into\nhierarchically arranged segments; within each segment, tokens are predicted in\nparallel, similar to BERT, while across segment predictions are sequential,\nsimilar to GPT. We illustrate that randomized serialization of the segments\nsignificantly improves the performance and results in distribution over\nspatially-long (across-segments) and -short (within-segment) predictions which\nare effective for feature learning. We illustrate the pertinence of these\ndesign choices and explore alternatives on a number of datasets (e.g., CIFAR10,\nImageNet). While our pre-training strategy works with vanilla Transformer, we\nalso propose a conceptually simple, but highly effective, addition to the\ndecoder that allows learnable skip-connections to encoder feature layers, which\nfurther improves the performance. Our final model, trained on ImageNet,\nachieves new state-of-the-art linear probing performance 68.3% among\ncomparative predictive self-supervised learning approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Tianyu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WayFAST: Traversability Predictive Navigation for Field Robots. (arXiv:2203.12071v1 [cs.RO])","link":"http://arxiv.org/abs/2203.12071","description":"<p>We present a self-supervised approach for learning to predict traversable\npaths for wheeled mobile robots that require good traction to navigate. Our\nalgorithm, termed WayFAST (Waypoint Free Autonomous Systems for\nTraversability), uses RGB and depth data, along with navigation experience, to\nautonomously generate traversable paths in outdoor unstructured environments.\nOur key inspiration is that traction can be estimated for rolling robots using\nkinodynamic models. Using traction estimates provided by an online receding\nhorizon estimator, we are able to train a traversability prediction neural\nnetwork in a self-supervised manner, without requiring heuristics utilized by\nprevious methods. We demonstrate the effectiveness of WayFAST through extensive\nfield testing in varying environments, ranging from sandy dry beaches to forest\ncanopies and snow covered grass fields. Our results clearly demonstrate that\nWayFAST can learn to avoid geometric obstacles as well as untraversable\nterrain, such as snow, which would be difficult to avoid with sensors that\nprovide only geometric data, such as LiDAR. Furthermore, we show that our\ntraining pipeline based on online traction estimates is more data-efficient\nthan other heuristic-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gasparino_M/0/1/0/all/0/1\">Mateus Valverde Gasparino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_A/0/1/0/all/0/1\">Arun Narenthiran Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1\">Andres Eduardo Baquero Velasquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higuti_V/0/1/0/all/0/1\">Vitor Akihiro Hisano Higuti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_J/0/1/0/all/0/1\">John Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Huy Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification. (arXiv:2203.12081v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12081","description":"<p>Multiple instance learning (MIL) has been increasingly used in the\nclassification of histopathology whole slide images (WSIs). However, MIL\napproaches for this specific classification problem still face unique\nchallenges, particularly those related to small sample cohorts. In these, there\nare limited number of WSI slides (bags), while the resolution of a single WSI\nis huge, which leads to a large number of patches (instances) cropped from this\nslide. To address this issue, we propose to virtually enlarge the number of\nbags by introducing the concept of pseudo-bags, on which a double-tier MIL\nframework is built to effectively use the intrinsic features. Besides, we also\ncontribute to deriving the instance probability under the framework of\nattention-based MIL, and utilize the derivation to help construct and analyze\nthe proposed framework. The proposed method outperforms other latest methods on\nthe CAMELYON-16 by substantially large margins, and is also better in\nperformance on the TCGA lung cancer dataset. The proposed framework is ready to\nbe extended for wider MIL applications. The code is available at:\nhttps://github.com/hrzhang1123/DTFD-MIL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongrun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanda Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yihong Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coupland_S/0/1/0/all/0/1\">Sarah E. Coupland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yalin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo. (arXiv:2203.12082v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12082","description":"<p>We present a novel framework named PlaneMVS for 3D plane reconstruction from\nmultiple input views with known camera poses. Most previous learning-based\nplane reconstruction methods reconstruct 3D planes from single images, which\nhighly rely on single-view regression and suffer from depth scale ambiguity. In\ncontrast, we reconstruct 3D planes with a multi-view-stereo (MVS) pipeline that\ntakes advantage of multi-view geometry. We decouple plane reconstruction into a\nsemantic plane detection branch and a plane MVS branch. The semantic plane\ndetection branch is based on a single-view plane detection framework but with\ndifferences. The plane MVS branch adopts a set of slanted plane hypotheses to\nreplace conventional depth hypotheses to perform plane sweeping strategy and\nfinally learns pixel-level plane parameters and its planar depth map. We\npresent how the two branches are learned in a balanced way, and propose a\nsoft-pooling loss to associate the outputs of the two branches and make them\nbenefit from each other. Extensive experiments on various indoor datasets show\nthat PlaneMVS significantly outperforms state-of-the-art (SOTA) single-view\nplane reconstruction methods on both plane detection and 3D geometry metrics.\nOur method even outperforms a set of SOTA learning-based MVS methods thanks to\nthe learned plane priors. To the best of our knowledge, this is the first work\non 3D plane reconstruction within an end-to-end MVS framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_N/0/1/0/all/0/1\">Nitin Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Changjiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Portrait Delighting. (arXiv:2203.12088v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12088","description":"<p>We present a deep neural network for removing undesirable shading features\nfrom an unconstrained portrait image, recovering the underlying texture. Our\ntraining scheme incorporates three regularization strategies: masked loss, to\nemphasize high-frequency shading features; soft-shadow loss, which improves\nsensitivity to subtle changes in lighting; and shading-offset estimation, to\nsupervise separation of shading and texture. Our method demonstrates improved\ndelighting quality and generalization when compared with the state-of-the-art.\nWe further demonstrate how our delighting method can enhance the performance of\nlight-sensitive computer vision tasks such as face relighting and semantic\nparsing, allowing them to handle extreme lighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_J/0/1/0/all/0/1\">Joshua Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalmers_A/0/1/0/all/0/1\">Andrew Chalmers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_T/0/1/0/all/0/1\">Taehyun Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FxP-QNet: A Post-Training Quantizer for the Design of Mixed Low-Precision DNNs with Dynamic Fixed-Point Representation. (arXiv:2203.12091v1 [cs.NE])","link":"http://arxiv.org/abs/2203.12091","description":"<p>Deep neural networks (DNNs) have demonstrated their effectiveness in a wide\nrange of computer vision tasks, with the state-of-the-art results obtained\nthrough complex and deep structures that require intensive computation and\nmemory. Now-a-days, efficient model inference is crucial for consumer\napplications on resource-constrained platforms. As a result, there is much\ninterest in the research and development of dedicated deep learning (DL)\nhardware to improve the throughput and energy efficiency of DNNs. Low-precision\nrepresentation of DNN data-structures through quantization would bring great\nbenefits to specialized DL hardware. However, the rigorous quantization leads\nto a severe accuracy drop. As such, quantization opens a large hyper-parameter\nspace at bit-precision levels, the exploration of which is a major challenge.\nIn this paper, we propose a novel framework referred to as the Fixed-Point\nQuantizer of deep neural Networks (FxP-QNet) that flexibly designs a mixed\nlow-precision DNN for integer-arithmetic-only deployment. Specifically, the\nFxP-QNet gradually adapts the quantization level for each data-structure of\neach layer based on the trade-off between the network accuracy and the\nlow-precision requirements. Additionally, it employs post-training\nself-distillation and network prediction error statistics to optimize the\nquantization of floating-point values into fixed-point numbers. Examining\nFxP-QNet on state-of-the-art architectures and the benchmark ImageNet dataset,\nwe empirically demonstrate the effectiveness of FxP-QNet in achieving the\naccuracy-compression trade-off without the need for training. The results show\nthat FxP-QNet-quantized AlexNet, VGG-16, and ResNet-18 reduce the overall\nmemory requirements of their full-precision counterparts by 7.16x, 10.36x, and\n6.44x with less than 0.95%, 0.95%, and 1.99% accuracy drop, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shawahna_A/0/1/0/all/0/1\">Ahmad Shawahna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sait_S/0/1/0/all/0/1\">Sadiq M. Sait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Maleh_A/0/1/0/all/0/1\">Aiman El-Maleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Irfan Ahmad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast on-line signature recognition based on VQ with time modeling. (arXiv:2203.12104v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12104","description":"<p>This paper proposes a multi-section vector quantization approach for on-line\nsignature recognition. We have used the MCYT database, which consists of 330\nusers and 25 skilled forgeries per person performed by 5 different impostors.\nThis database is larger than those typically used in the literature.\nNevertheless, we also provide results from the SVC database.\n</p>\n<p>Our proposed system outperforms the winner of SVC with a reduced\ncomputational requirement, which is around 47 times lower than DTW. In\naddition, our system improves the database storage requirements due to vector\ncompression, and is more privacy-friendly as it is not possible to recover the\noriginal signature using the codebooks. Experimental results with MCYT provide\na 99.76% identification rate and 2.46% EER (skilled forgeries and individual\nthreshold). Experimental results with SVC are 100% of identification rate and\n0% (individual threshold) and 0.31% (general threshold) when using a\ntwo-section VQ approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pascual_Gaspar_J/0/1/0/all/0/1\">Juan-Manuel Pascual-Gaspar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivaracho_C/0/1/0/all/0/1\">Carlos Vivaracho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lymphocyte Classification in Hyperspectral Images of Ovarian Cancer Tissue Biopsy Samples. (arXiv:2203.12112v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12112","description":"<p>Current methods for diagnosing the progression of multiple types of cancer\nwithin patients rely on interpreting stained needle biopsies. This process is\ntime-consuming and susceptible to error throughout the paraffinization,\nHematoxylin and Eosin (H&amp;E) staining, deparaffinization, and annotation stages.\nFourier Transform Infrared (FTIR) imaging has been shown to be a promising\nalternative to staining for appropriately annotating biopsy cores without the\nneed for deparaffinization or H&amp;E staining with the use of Fourier Transform\nInfrared (FTIR) images when combined with machine learning to interpret the\ndense spectral information. We present a machine learning pipeline to segment\nwhite blood cell (lymphocyte) pixels in hyperspectral images of biopsy cores.\nThese cells are clinically important for diagnosis, but some prior work has\nstruggled to incorporate them due to difficulty obtaining precise pixel labels.\nEvaluated methods include Support Vector Machine (SVM), Gaussian Naive Bayes,\nand Multilayer Perceptron (MLP), as well as analyzing the comparatively modern\nconvolutional neural network (CNN).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Paulson_B/0/1/0/all/0/1\">Benjamin Paulson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colwell_T/0/1/0/all/0/1\">Theodore Colwell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bukowski_N/0/1/0/all/0/1\">Natalia Bukowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weller_J/0/1/0/all/0/1\">Joseph Weller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crisler_A/0/1/0/all/0/1\">Andrew Crisler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cisler_J/0/1/0/all/0/1\">John Cisler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drobek_A/0/1/0/all/0/1\">Alexander Drobek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neuwirth_A/0/1/0/all/0/1\">Alexander Neuwirth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOSS: Towards Generalized Open-set Semantic Segmentation. (arXiv:2203.12116v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12116","description":"<p>In this paper, we present and study a new image segmentation task, called\nGeneralized Open-set Semantic Segmentation (GOSS). Previously, with the\nwell-known open-set semantic segmentation (OSS), the intelligent agent only\ndetects the unknown regions without further processing, limiting their\nperception of the environment. It stands to reason that a further analysis of\nthe detected unknown pixels would be beneficial. Therefore, we propose GOSS,\nwhich unifies the abilities of two well-defined segmentation tasks, OSS and\ngeneric segmentation (GS), in a holistic way. Specifically, GOSS classifies\npixels as belonging to known classes, and clusters (or groups) of pixels of\nunknown class are labelled as such. To evaluate this new expanded task, we\nfurther propose a metric which balances the pixel classification and clustering\naspects. Moreover, we build benchmark tests on top of existing datasets and\npropose a simple neural architecture as a baseline, which jointly predicts\npixel classification and clustering under open-set settings. Our experiments on\nmultiple benchmarks demonstrate the effectiveness of our baseline. We believe\nour new GOSS task can produce an expressive image understanding for future\nresearch. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jie Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junlin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Prompt Tuning. (arXiv:2203.12119v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12119","description":"<p>The current modus operandi in adapting pre-trained models involves updating\nall the backbone parameters, ie, full fine-tuning. This paper introduces Visual\nPrompt Tuning (VPT) as an efficient and effective alternative to full\nfine-tuning for large-scale Transformer models in vision. Taking inspiration\nfrom recent advances in efficiently tuning large language models, VPT\nintroduces only a small amount (less than 1% of model parameters) of trainable\nparameters in the input space while keeping the model backbone frozen. Via\nextensive experiments on a wide variety of downstream recognition tasks, we\nshow that VPT achieves significant performance gains compared to other\nparameter efficient tuning protocols. Most importantly, VPT even outperforms\nfull fine-tuning in many cases across model capacities and training data\nscales, while reducing per-task storage cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Luming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bor-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection. (arXiv:2203.12121v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12121","description":"<p>Current polyp detection methods from colonoscopy videos use exclusively\nnormal (i.e., healthy) training images, which i) ignore the importance of\ntemporal information in consecutive video frames, and ii) lack knowledge about\nthe polyps. Consequently, they often have high detection errors, especially on\nchallenging polyp cases (e.g., small, flat, or partially visible polyps). In\nthis work, we formulate polyp detection as a weakly-supervised anomaly\ndetection task that uses video-level labelled training data to detect\nframe-level polyps. In particular, we propose a novel convolutional\ntransformer-based multiple instance learning method designed to identify\nabnormal frames (i.e., frames with polyps) from anomalous videos (i.e., videos\ncontaining at least one frame with polyp). In our method, local and global\ntemporal dependencies are seamlessly captured while we simultaneously optimise\nvideo and snippet-level anomaly scores. A contrastive snippet mining method is\nalso proposed to enable an effective modelling of the challenging polyp cases.\nThe resulting method achieves a detection accuracy that is substantially better\nthan current state-of-the-art approaches on a new large-scale colonoscopy video\ndataset introduced in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W Verjans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel VQ-VAEs for Improved Pixel Art Representation. (arXiv:2203.12130v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12130","description":"<p>Machine learning has had a great deal of success in image processing.\nHowever, the focus of this work has largely been on realistic images, ignoring\nmore niche art styles such as pixel art. Additionally, many traditional machine\nlearning models that focus on groups of pixels do not work well with pixel art,\nwhere individual pixels are important. We propose the Pixel VQ-VAE, a\nspecialized VQ-VAE model that learns representations of pixel art. We show that\nit outperforms other models in both the quality of embeddings as well as\nperformance on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saravanan_A/0/1/0/all/0/1\">Akash Saravanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzdial_M/0/1/0/all/0/1\">Matthew Guzdial</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Hybrid Spine Network for Segmentation of Spine MR Images. (arXiv:2203.12151v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12151","description":"<p>Automatic segmentation of vertebral bodies (VBs) and intervertebral discs\n(IVDs) in 3D magnetic resonance (MR) images is vital in diagnosing and treating\nspinal diseases. However, segmenting the VBs and IVDs simultaneously is not\ntrivial. Moreover, problems exist, including blurry segmentation caused by\nanisotropy resolution, high computational cost, inter-class similarity and\nintra-class variability, and data imbalances. We proposed a two-stage\nalgorithm, named semi-supervised hybrid spine network (SSHSNet), to address\nthese problems by achieving accurate simultaneous VB and IVD segmentation. In\nthe first stage, we constructed a 2D semi-supervised DeepLabv3+ by using cross\npseudo supervision to obtain intra-slice features and coarse segmentation. In\nthe second stage, a 3D full-resolution patch-based DeepLabv3+ was built. This\nmodel can be used to extract inter-slice information and combine the coarse\nsegmentation and intra-slice features provided from the first stage. Moreover,\na cross tri-attention module was applied to compensate for the loss of\ninter-slice and intra-slice information separately generated from 2D and 3D\nnetworks, thereby improving feature representation ability and achieving\nsatisfactory segmentation results. The proposed SSHSNet was validated on a\npublicly available spine MR image dataset, and remarkable segmentation\nperformance was achieved. Moreover, results show that the proposed method has\ngreat potential in dealing with the data imbalance problem. Based on previous\nreports, few studies have incorporated a semi-supervised learning strategy with\na cross attention mechanism for spine segmentation. Therefore, the proposed\nmethod may provide a useful tool for spine segmentation and aid clinically in\nspinal disease diagnoses and treatments. Codes are publicly available at:\nhttps://github.com/Meiyan88/SSHSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1\">Meiyan Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shuoling Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiumei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_H/0/1/0/all/0/1\">Haoran Lai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_Q/0/1/0/all/0/1\">Qianjin Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive Benchmark Datasets for Amharic Scene Text Detection and Recognition. (arXiv:2203.12165v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12165","description":"<p>Ethiopic/Amharic script is one of the oldest African writing systems, which\nserves at least 23 languages (e.g., Amharic, Tigrinya) in East Africa for more\nthan 120 million people. The Amharic writing system, Abugida, has 282\nsyllables, 15 punctuation marks, and 20 numerals. The Amharic syllabic matrix\nis derived from 34 base graphemes/consonants by adding up to 12 appropriate\ndiacritics or vocalic markers to the characters. The syllables with a common\nconsonant or vocalic markers are likely to be visually similar and challenge\ntext recognition tasks. In this work, we presented the first comprehensive\npublic datasets named HUST-ART, HUST-AST, ABE, and Tana for Amharic script\ndetection and recognition in the natural scene. We have also conducted\nextensive experiments to evaluate the performance of the state of art methods\nin detecting and recognizing Amharic scene text on our datasets. The evaluation\nresults demonstrate the robustness of our datasets for benchmarking and its\npotential of promoting the development of robust Amharic script detection and\nrecognition algorithms. Consequently, the outcome will benefit people in East\nAfrica, including diplomats from several countries and international\ncommunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dikubab_W/0/1/0/all/0/1\">Wondimu Dikubab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dingkang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Minghui Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Transformers for Robust Few-shot Cross-domain Face Anti-spoofing. (arXiv:2203.12175v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12175","description":"<p>While recent face anti-spoofing methods perform well under the intra-domain\nsetups, an effective approach needs to account for much larger appearance\nvariations of images acquired in complex scenes with different sensors for\nrobust performance. In this paper, we present adaptive vision transformers\n(ViT) for robust cross-domain face anti-spoofing. Specifically, we adopt ViT as\na backbone to exploit its strength to account for long-range dependencies among\npixels. We further introduce the ensemble adapters module and feature-wise\ntransformation layers in the ViT to adapt to different domains for robust\nperformance with a few samples. Experiments on several benchmark datasets show\nthat the proposed models achieve both robust and competitive performance\nagainst the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hsin-Ping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaojie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wen-Sheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Taihong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jinwei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Motion Deblurring and Frame Interpolation with Events. (arXiv:2203.12178v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12178","description":"<p>Slow shutter speed and long exposure time of frame-based cameras often cause\nvisual blur and loss of inter-frame information, degenerating the overall\nquality of captured videos. To this end, we present a unified framework of\nevent-based motion deblurring and frame interpolation for blurry video\nenhancement, where the extremely low latency of events is leveraged to\nalleviate motion blur and facilitate intermediate frame prediction.\nSpecifically, the mapping relation between blurry frames and sharp latent\nimages is first predicted by a learnable double integral network, and a fusion\nnetwork is then proposed to refine the coarse results via utilizing the\ninformation from consecutive blurry inputs and the concurrent events. By\nexploring the mutual constraints among blurry frames, latent images, and event\nstreams, we further propose a self-supervised learning framework to enable\nnetwork training with real-world blurry videos and events. Extensive\nexperiments demonstrate that our method compares favorably against the\nstate-of-the-art approaches and achieves remarkable performance on both\nsynthetic and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Censor by Noisy Sampling. (arXiv:2203.12192v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12192","description":"<p>Point clouds are an increasingly ubiquitous input modality and the raw signal\ncan be efficiently processed with recent progress in deep learning. This signal\nmay, often inadvertently, capture sensitive information that can leak semantic\nand geometric properties of the scene which the data owner does not want to\nshare. The goal of this work is to protect sensitive information when learning\nfrom point clouds; by censoring the sensitive information before the point\ncloud is released for downstream tasks. Specifically, we focus on preserving\nutility for perception tasks while mitigating attribute leakage attacks. The\nkey motivating insight is to leverage the localized saliency of perception\ntasks on point clouds to provide good privacy-utility trade-offs. We realize\nthis through a mechanism called Censoring by Noisy Sampling (CBNS), which is\ncomposed of two modules: i) Invariant Sampler: a differentiable point-cloud\nsampler which learns to remove points invariant to utility and ii) Noisy\nDistorter: which learns to distort sampled points to decouple the sensitive\ninformation from utility, and mitigate privacy leakage. We validate the\neffectiveness of CBNS through extensive comparisons with state-of-the-art\nbaselines and sensitivity analyses of key design choices. Results show that\nCBNS achieves superior privacy-utility trade-offs on multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ayush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Abhishek Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Vivek Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Robust Scene Flow Estimation via the Alignment of Probability Density Functions. (arXiv:2203.12193v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12193","description":"<p>In this paper, we present a new self-supervised scene flow estimation\napproach for a pair of consecutive point clouds. The key idea of our approach\nis to represent discrete point clouds as continuous probability density\nfunctions using Gaussian mixture models. Scene flow estimation is therefore\nconverted into the problem of recovering motion from the alignment of\nprobability density functions, which we achieve using a closed-form expression\nof the classic Cauchy-Schwarz divergence. Unlike existing\nnearest-neighbor-based approaches that use hard pairwise correspondences, our\nproposed approach establishes soft and implicit point correspondences between\npoint clouds and generates more robust and accurate scene flow in the presence\nof missing correspondences and outliers. Comprehensive experiments show that\nour method makes noticeable gains over the Chamfer Distance and the Earth\nMover's Distance in real-world environments and achieves state-of-the-art\nperformance among self-supervised learning methods on FlyingThings3D and KITTI,\neven outperforming some supervised methods with ground truth annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emami_P/0/1/0/all/0/1\">Patrick Emami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1\">Sanjay Ranka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1\">Anand Rangarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biceph-Net: A robust and lightweight framework for the diagnosis of Alzheimer's disease using 2D-MRI scans and deep similarity learning. (arXiv:2203.12197v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12197","description":"<p>Alzheimer's Disease (AD) is a neurodegenerative disease that is one of the\nsignificant causes of death in the elderly population. Many deep learning\ntechniques have been proposed to diagnose AD using Magnetic Resonance Imaging\n(MRI) scans. Predicting AD using 2D slices extracted from 3D MRI scans is\nchallenging as the inter-slice information gets lost. To this end, we propose a\nnovel and lightweight framework termed 'Biceph-Net' for AD diagnosis using 2D\nMRI scans that model both the intra-slice and inter-slice information.\nBiceph-Net has been experimentally shown to perform similar to other\nSpatio-temporal neural networks while being computationally more efficient.\nBiceph-Net is also superior in performance compared to vanilla 2D convolutional\nneural networks (CNN) for AD diagnosis using 2D MRI slices. Biceph-Net also has\nan inbuilt neighbourhood-based model interpretation feature that can be\nexploited to understand the classification decision taken by the network.\nBiceph-Net experimentally achieves a test accuracy of 100% in the\nclassification of Cognitively Normal (CN) vs AD, 98.16% for Mild Cognitive\nImpairment (MCI) vs AD, and 97.80% for CN vs MCI vs AD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rashid_A/0/1/0/all/0/1\">A. H. Rashid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">A. Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_J/0/1/0/all/0/1\">J. Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanveer_M/0/1/0/all/0/1\">M. Tanveer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Frequency Filtering for Domain Generalization. (arXiv:2203.12198v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12198","description":"<p>Improving the generalization capability of Deep Neural Networks (DNNs) is\ncritical for their practical uses, which has been a longstanding challenge.\nSome theoretical studies have revealed that DNNs have preferences to different\nfrequency components in the learning process and indicated that this may affect\nthe robustness of learned features. In this paper, we propose Deep Frequency\nFiltering (DFF) for learning domain-generalizable features, which is the first\nendeavour to explicitly modulate frequency components of different transfer\ndifficulties across domains during training. To achieve this, we perform Fast\nFourier Transform (FFT) on feature maps at different layers, then adopt a\nlight-weight module to learn the attention masks from frequency representations\nafter FFT to enhance transferable frequency components while suppressing the\ncomponents not conductive to generalization. Further, we empirically compare\ndifferent types of attention for implementing our conceptualized DFF. Extensive\nexperiments demonstrate the effectiveness of the proposed DFF and show that\napplying DFF on a plain baseline outperforms the state-of-the-art methods on\ndifferent domain generalization tasks, including close-set classification and\nopen-set retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shiqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhipeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_P/0/1/0/all/0/1\">Peng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1\">Quanzeng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parulkar_A/0/1/0/all/0/1\">Amey Parulkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navkal_V/0/1/0/all/0/1\">Viraj Navkal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Prediction of Lung Squamous Cell Carcinoma Recurrence With Self-supervised Learning. (arXiv:2203.12204v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12204","description":"<p>Lung squamous cell carcinoma (LSCC) has a high recurrence and metastasis\nrate. Factors influencing recurrence and metastasis are currently unknown and\nthere are no distinct histopathological or morphological features indicating\nthe risks of recurrence and metastasis in LSCC. Our study focuses on the\nrecurrence prediction of LSCC based on H&amp;E-stained histopathological\nwhole-slide images (WSI). Due to the small size of LSCC cohorts in terms of\npatients with available recurrence information, standard end-to-end learning\nwith various convolutional neural networks for this task tends to overfit.\nAlso, the predictions made by these models are hard to interpret.\nHistopathology WSIs are typically very large and are therefore processed as a\nset of smaller tiles. In this work, we propose a novel conditional\nself-supervised learning (SSL) method to learn representations of WSI at the\ntile level first, and leverage clustering algorithms to identify the tiles with\nsimilar histopathological representations. The resulting representations and\nclusters from self-supervision are used as features of a survival model for\nrecurrence prediction at the patient level. Using two publicly available\ndatasets from TCGA and CPTAC, we show that our LSCC recurrence prediction\nsurvival model outperforms both LSCC pathological stage-based approach and\nmachine learning baselines such as multiple instance learning. The proposed\nmethod also enables us to explain the recurrence histopathological risk factors\nvia the derived clusters. This can help pathologists derive new hypotheses\nregarding morphological features associated with LSCC recurrence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Weicheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavian_N/0/1/0/all/0/1\">Narges Razavian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection. (arXiv:2203.12208v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12208","description":"<p>Recent studies in deepfake detection have yielded promising results when the\ntraining and testing face forgeries are from the same dataset. However, the\nproblem remains challenging when one tries to generalize the detector to\nforgeries created by unseen methods in the training dataset. This work\naddresses the generalizable deepfake detection from a simple principle: a\ngeneralizable representation should be sensitive to diverse types of forgeries.\nFollowing this principle, we propose to enrich the \"diversity\" of forgeries by\nsynthesizing augmented forgeries with a pool of forgery configurations and\nstrengthen the \"sensitivity\" to the forgeries by enforcing the model to predict\nthe forgery configurations. To effectively explore the large forgery\naugmentation space, we further propose to use the adversarial training strategy\nto dynamically synthesize the most challenging forgeries to the current model.\nThrough extensive experiments, we show that the proposed strategies are\nsurprisingly effective (see Figure 1), and they could achieve superior\nperformance than the current state-of-the-art methods. Code is available at\n\\url{https://github.com/liangchen527/SLADD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-Driven Deep Learning for Computational Magnetic Resonance Imaging. (arXiv:2203.12215v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12215","description":"<p>Physics-driven deep learning methods have emerged as a powerful tool for\ncomputational magnetic resonance imaging (MRI) problems, pushing reconstruction\nperformance to new limits. This article provides an overview of the recent\ndevelopments in incorporating physics information into learning-based MRI\nreconstruction. We consider inverse problems with both linear and non-linear\nforward models for computational MRI, and review the classical approaches for\nsolving these. We then focus on physics-driven deep learning approaches,\ncovering physics-driven loss functions, plug-and-play methods, generative\nmodels, and unrolled networks. We highlight domain-specific challenges such as\nreal- and complex-valued building blocks of neural networks, and translational\napplications in MRI with linear and non-linear forward models. Finally, we\ndiscuss common issues and open challenges, and draw connections to the\nimportance of physics-driven learning when combined with other downstream tasks\nin the medical imaging pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1\">Thomas K&#xfc;stner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1\">Burhaneddin Yaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengnan Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knoll_F/0/1/0/all/0/1\">Florian Knoll</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akcakaya_M/0/1/0/all/0/1\">Mehmet Ak&#xe7;akaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training-free Transformer Architecture Search. (arXiv:2203.12217v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12217","description":"<p>Recently, Vision Transformer (ViT) has achieved remarkable success in several\ncomputer vision tasks. The progresses are highly relevant to the architecture\ndesign, then it is worthwhile to propose Transformer Architecture Search (TAS)\nto search for better ViTs automatically. However, current TAS methods are\ntime-consuming and existing zero-cost proxies in CNN do not generalize well to\nthe ViT search space according to our experimental observations. In this paper,\nfor the first time, we investigate how to conduct TAS in a training-free manner\nand devise an effective training-free TAS (TF-TAS) scheme. Firstly, we observe\nthat the properties of multi-head self-attention (MSA) and multi-layer\nperceptron (MLP) in ViTs are quite different and that the synaptic diversity of\nMSA affects the performance notably. Secondly, based on the observation, we\ndevise a modular strategy in TF-TAS that evaluates and ranks ViT architectures\nfrom two theoretical perspectives: synaptic diversity and synaptic saliency,\ntermed as DSS-indicator. With DSS-indicator, evaluation results are strongly\ncorrelated with the test accuracies of ViT models. Experimental results\ndemonstrate that our TF-TAS achieves a competitive performance against the\nstate-of-the-art manually or automatically design ViT architectures, and it\npromotes the searching efficiency in ViT search space greatly: from about $24$\nGPU days to less than $0.5$ GPU days. Moreover, the proposed DSS-indicator\noutperforms the existing cutting-edge zero-cost approaches (e.g., TE-score and\nNASWOT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qinqin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiawu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Few-Shot Object Detection via Knowledge Inheritance. (arXiv:2203.12224v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12224","description":"<p>Few-shot object detection (FSOD), which aims at learning a generic detector\nthat can adapt to unseen tasks with scarce training samples, has witnessed\nconsistent improvement recently. However, most existing methods ignore the\nefficiency issues, e.g., high computational complexity and slow adaptation\nspeed. Notably, efficiency has become an increasingly important evaluation\nmetric for few-shot techniques due to an emerging trend toward embedded AI. To\nthis end, we present an efficient pretrain-transfer framework (PTF) baseline\nwith no computational increment, which achieves comparable results with\nprevious state-of-the-art (SOTA) methods. Upon this baseline, we devise an\ninitializer named knowledge inheritance (KI) to reliably initialize the novel\nweights for the box classifier, which effectively facilitates the knowledge\ntransfer process and boosts the adaptation speed. Within the KI initializer, we\npropose an adaptive length re-scaling (ALR) strategy to alleviate the vector\nlength inconsistency between the predicted novel weights and the pretrained\nbase weights. Finally, our approach not only achieves the SOTA results across\nthree public benchmarks, i.e., PASCAL VOC, COCO and LVIS, but also exhibits\nhigh efficiency with 1.8-9.0x faster adaptation speed against the other methods\non COCO/LVIS benchmark during few-shot transfer. To our best knowledge, this is\nthe first work to consider the efficiency problem in FSOD. We hope to motivate\na trend toward powerful yet efficient few-shot technique development. The codes\nare publicly available at https://github.com/Ze-Yang/Efficient-FSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ze Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruibo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Selection by Clustering for Contrastive Learning in Human Activity Recognition. (arXiv:2203.12230v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12230","description":"<p>Contrastive learning has been applied to Human Activity Recognition (HAR)\nbased on sensor data owing to its ability to achieve performance comparable to\nsupervised learning with a large amount of unlabeled data and a small amount of\nlabeled data. The pre-training task for contrastive learning is generally\ninstance discrimination, which specifies that each instance belongs to a single\nclass, but this will consider the same class of samples as negative examples.\nSuch a pre-training task is not conducive to human activity recognition tasks,\nwhich are mainly classification tasks. To address this problem, we follow\nSimCLR to propose a new contrastive learning framework that negative selection\nby clustering in HAR, which is called ClusterCLHAR. Compared with SimCLR, it\nredefines the negative pairs in the contrastive loss function by using\nunsupervised clustering methods to generate soft labels that mask other samples\nof the same cluster to avoid regarding them as negative samples. We evaluate\nClusterCLHAR on three benchmark datasets, USC-HAD, MotionSense, and UCI-HAR,\nusing mean F1-score as the evaluation metric. The experiment results show that\nit outperforms all the state-of-the-art methods applied to HAR in\nself-supervised learning and semi-supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Huansheng Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yaping Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Characteristic Learning Method with Micro-Doppler Signatures for Pedestrian Identification. (arXiv:2203.12236v1 [eess.SP])","link":"http://arxiv.org/abs/2203.12236","description":"<p>The identification of pedestrians using radar micro-Doppler signatures has\nbecome a hot topic in recent years. In this paper, we propose a\nmulti-characteristic learning (MCL) model with clusters to jointly learn\ndiscrepant pedestrian micro-Doppler signatures and fuse the knowledge learned\nfrom each cluster into final decisions. Time-Doppler spectrogram (TDS) and\nsignal statistical features extracted from FMCW radar, as two categories of\nmicro-Doppler signatures, are used in MCL to learn the micro-motion information\ninside pedestrians' free walking patterns. The experimental results show that\nour model achieves a higher accuracy rate and is more stable for pedestrian\nidentification than other studies, which make our model more practical.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Haodong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guangbo Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Method of Data Augmentation to Train a Small Area Fingerprint Recognition Deep Neural Network with a Normal Fingerprint Database. (arXiv:2203.12241v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12241","description":"<p>Fingerprints are popular among the biometric based systems due to ease of\nacquisition, uniqueness and availability. Nowadays it is used in smart phone\nsecurity, digital payment and digital locker. The traditional fingerprint\nmatching methods based on minutiae are mainly applicable for large-area\nfingerprint and the accuracy rate would reduce significantly when dealing with\nsmall-area fingerprint from smart phone. There are many attempts to using deep\nlearning for small-area fingerprint recognition, and there are many successes.\nBut training deep neural network needs a lot of datasets for training. There is\nno well-known dataset for small-area, so we have to make datasets ourselves. In\nthis paper, we propose a method of data augmentation to train a small-area\nfingerprint recognition deep neural network with a normal fingerprint database\n(such as FVC2002) and verify it via tests. The experimental results showed the\nefficiency of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">JuSong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-Equivalent Distillation for Semi-Supervised Object Detection. (arXiv:2203.12244v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12244","description":"<p>Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on\nself-training, i.e., generating hard pseudo-labels by a teacher model on\nunlabeled data as supervisory signals. Although they achieved certain success,\nthe limited labeled data in semi-supervised learning scales up the challenges\nof object detection. We analyze the challenges these methods meet with the\nempirical experiment results. We find that the massive False Negative samples\nand inferior localization precision lack consideration. Besides, the large\nvariance of object sizes and class imbalance (i.e., the extreme ratio between\nbackground and object) hinder the performance of prior arts. Further, we\novercome these challenges by introducing a novel approach, Scale-Equivalent\nDistillation (SED), which is a simple yet effective end-to-end knowledge\ndistillation framework robust to large object size variance and class\nimbalance. SED has several appealing benefits compared to the previous works.\n(1) SED imposes a consistency regularization to handle the large scale variance\nproblem. (2) SED alleviates the noise problem from the False Negative samples\nand inferior localization precision. (3) A re-weighting strategy can implicitly\nscreen the potential foreground regions of the unlabeled data to reduce the\neffect of class imbalance. Extensive experiments show that SED consistently\noutperforms the recent state-of-the-art methods on different datasets with\nsignificant margins. For example, it surpasses the supervised counterpart by\nmore than 10 mAP when using 5% and 10% labeled data on MS-COCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qiushan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yao Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ev-TTA: Test-Time Adaptation for Event-Based Object Recognition. (arXiv:2203.12247v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12247","description":"<p>We introduce Ev-TTA, a simple, effective test-time adaptation algorithm for\nevent-based object recognition. While event cameras are proposed to provide\nmeasurements of scenes with fast motions or drastic illumination changes, many\nexisting event-based recognition algorithms suffer from performance\ndeterioration under extreme conditions due to significant domain shifts. Ev-TTA\nmitigates the severe domain gaps by fine-tuning the pre-trained classifiers\nduring the test phase using loss functions inspired by the spatio-temporal\ncharacteristics of events. Since the event data is a temporal stream of\nmeasurements, our loss function enforces similar predictions for adjacent\nevents to quickly adapt to the changed environment online. Also, we utilize the\nspatial correlations between two polarities of events to handle noise under\nextreme illumination, where different polarities of events exhibit distinctive\nnoise distributions. Ev-TTA demonstrates a large amount of performance gain on\na wide range of event-based object recognition tasks without extensive\nadditional training. Our formulation can be successfully applied regardless of\ninput representations and further extended into regression tasks. We expect\nEv-TTA to provide the key technique to deploy event-based vision algorithms in\nchallenging real-world applications where significant domain shift is\ninevitable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Inwoo Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-Based Dense Reconstruction Pipeline. (arXiv:2203.12270v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12270","description":"<p>Event cameras are a new type of sensors that are different from traditional\ncameras. Each pixel is triggered asynchronously by event. The trigger event is\nthe change of the brightness irradiated on the pixel. If the increment or\ndecrement of brightness is higher than a certain threshold, an event is output.\nCompared with traditional cameras, event cameras have the advantages of high\ndynamic range and no motion blur. Since events are caused by the apparent\nmotion of intensity edges, the majority of 3D reconstructed maps consist only\nof scene edges, i.e., semi-dense maps, which is not enough for some\napplications. In this paper, we propose a pipeline to realize event-based dense\nreconstruction. First, deep learning is used to reconstruct intensity images\nfrom events. And then, structure from motion (SfM) is used to estimate camera\nintrinsic, extrinsic and sparse point cloud. Finally, multi-view stereo (MVS)\nis used to complete dense reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_J/0/1/0/all/0/1\">Jinghong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yongfeng Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAN: a Segmentation-free Document Attention Network for Handwritten Document Recognition. (arXiv:2203.12273v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12273","description":"<p>Unconstrained handwritten document recognition is a challenging computer\nvision task. It is traditionally handled by a two-step approach combining line\nsegmentation followed by text line recognition. For the first time, we propose\nan end-to-end segmentation-free architecture for the task of handwritten\ndocument recognition: the Document Attention Network. In addition to the text\nrecognition, the model is trained to label text parts using begin and end tags\nin an XML-like fashion. This model is made up of an FCN encoder for feature\nextraction and a stack of transformer decoder layers for a recurrent\ntoken-by-token prediction process. It takes whole text documents as input and\nsequentially outputs characters, as well as logical layout tokens. Contrary to\nthe existing segmentation-based approaches, the model is trained without using\nany segmentation label. We achieve competitive results on the READ dataset at\npage level, as well as double-page level with a CER of 3.53% and 3.69%,\nrespectively. We also provide results for the RIMES dataset at page level,\nreaching 4.54% of CER.\n</p>\n<p>We provide all source code and pre-trained model weights at\nhttps://github.com/FactoDeepLearning/DAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coquenet_D/0/1/0/all/0/1\">Denis Coquenet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1\">Cl&#xe9;ment Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paquet_T/0/1/0/all/0/1\">Thierry Paquet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cell segmentation from telecentric bright-field transmitted light microscopic images using a Residual Attention U-Net: a case study on HeLa line. (arXiv:2203.12290v1 [q-bio.QM])","link":"http://arxiv.org/abs/2203.12290","description":"<p>Living cell segmentation from bright-field light microscopic images is\nchallenging due to the image complexity and temporal changes in the living\ncells. Recently developed deep learning (DL)-based methods became popular in\nmedical and microscopic image segmentation tasks due to their success and\npromising outcomes. The main objective of this paper is to develop a deep\nlearning, UNet-based method to segment the living cells of the HeLa line in\nbright-field transmitted light microscopy. To find the most suitable\narchitecture for our datasets, we have proposed a residual attention U-Net and\ncompared it with an attention and a simple U-Net architecture. The attention\nmechanism highlights the remarkable features and suppresses activations in the\nirrelevant image regions. The residual mechanism overcomes with vanishing\ngradient problem. The Mean-IoU score for our datasets reaches 0.9505, 0.9524,\nand 0.9530 for the simple, attention, and residual attention U-Net,\nrespectively. We achieved the most accurate semantic segmentation results in\nthe Mean-IoU and Dice metrics by applying the residual and attention mechanisms\ntogether. The watershed method applied to this best - Residual Attention -\nsemantic segmentation result gave the segmentation with the specific\ninformation for each cell.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ghaznavi_A/0/1/0/all/0/1\">Ali Ghaznavi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rychtarikova_R/0/1/0/all/0/1\">Renata Rychtarikova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saberioon_M/0/1/0/all/0/1\">Mohammadmehdi Saberioon</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stys_D/0/1/0/all/0/1\">Dalibor Stys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lane detection with Position Embedding. (arXiv:2203.12301v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12301","description":"<p>Recently, lane detection has made great progress in autonomous driving. RESA\n(REcurrent Feature-Shift Aggregator) is based on image segmentation. It\npresents a novel module to enrich lane feature after preliminary feature\nextraction with an ordinary CNN. For Tusimple dataset, there is not too\ncomplicated scene and lane has more prominent spatial features. On the basis of\nRESA, we introduce the method of position embedding to enhance the spatial\nfeatures. The experimental results show that this method has achieved the best\naccuracy 96.93% on Tusimple dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiacheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Dezhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaer Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_J/0/1/0/all/0/1\">Jianwei Shuai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Generalized Textured Surface Anomaly Detection. (arXiv:2203.12304v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12304","description":"<p>Anomaly detection aims to identify abnormal data that deviates from the\nnormal ones, while typically requiring a sufficient amount of normal data to\ntrain the model for performing this task. Despite the success of recent anomaly\ndetection methods, performing anomaly detection in an unseen domain remain a\nchallenging task. In this paper, we address the task of domain-generalized\ntextured surface anomaly detection. By observing normal and abnormal surface\ndata across multiple source domains, our model is expected to be generalized to\nan unseen textured surface of interest, in which only a small number of normal\ndata can be observed during testing. Although with only image-level labels\nobserved in the training data, our patch-based meta-learning model exhibits\npromising generalization ability: not only can it generalize to unseen image\ndomains, but it can also localize abnormal regions in the query image. Our\nexperiments verify that our model performs favorably against state-of-the-art\nanomaly detection and domain generalization approaches in various settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shang-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Min Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Ching Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Trista Pei-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Chiang Frank Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised HDR Imaging from Motion and Exposure Cues. (arXiv:2203.12311v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12311","description":"<p>Recent High Dynamic Range (HDR) techniques extend the capabilities of current\ncameras where scenes with a wide range of illumination can not be accurately\ncaptured with a single low-dynamic-range (LDR) image. This is generally\naccomplished by capturing several LDR images with varying exposure values whose\ninformation is then incorporated into a merged HDR image. While such approaches\nwork well for static scenes, dynamic scenes pose several challenges, mostly\nrelated to the difficulty of finding reliable pixel correspondences.\nData-driven approaches tackle the problem by learning an end-to-end mapping\nwith paired LDR-HDR training data, but in practice generating such HDR\nground-truth labels for dynamic scenes is time-consuming and requires complex\nprocedures that assume control of certain dynamic elements of the scene (e.g.\nactor pose) and repeatable lighting conditions (stop-motion capturing). In this\nwork, we propose a novel self-supervised approach for learnable HDR estimation\nthat alleviates the need for HDR ground-truth labels. We propose to leverage\nthe internal statistics of LDR images to create HDR pseudo-labels. We\nseparately exploit static and well-exposed parts of the input images, which in\nconjunction with synthetic illumination clipping and motion augmentation\nprovide high quality training examples. Experimental results show that the HDR\nmodels trained using our proposed self-supervision approach achieve performance\ncompetitive with those trained under full supervision, and are to a large\nextent superior to previous methods that equally do not require any\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nazarczuk_M/0/1/0/all/0/1\">Michal Nazarczuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catley_Chandar_S/0/1/0/all/0/1\">Sibi Catley-Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellitero_E/0/1/0/all/0/1\">Eduardo P&#xe9;rez Pellitero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autofocus for Event Cameras. (arXiv:2203.12321v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12321","description":"<p>Focus control (FC) is crucial for cameras to capture sharp images in\nchallenging real-world scenarios. The autofocus (AF) facilitates the FC by\nautomatically adjusting the focus settings. However, due to the lack of\neffective AF methods for the recently introduced event cameras, their FC still\nrelies on naive AF like manual focus adjustments, leading to poor adaptation in\nchallenging real-world conditions. In particular, the inherent differences\nbetween event and frame data in terms of sensing modality, noise, temporal\nresolutions, etc., bring many challenges in designing an effective AF method\nfor event cameras. To address these challenges, we develop a novel event-based\nautofocus framework consisting of an event-specific focus measure called event\nrate (ER) and a robust search strategy called event-based golden search (EGS).\nTo verify the performance of our method, we have collected an event-based\nautofocus dataset (EAD) containing well-synchronized frames, events, and focal\npositions in a wide variety of challenging scenes with severe lighting and\nmotion conditions. The experiments on this dataset and additional real-world\nscenarios demonstrated the superiority of our method over state-of-the-art\napproaches in terms of efficiency and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shijie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaowei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DR.VIC: Decomposition and Reasoning for Video Individual Counting. (arXiv:2203.12335v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12335","description":"<p>Pedestrian counting is a fundamental tool for understanding pedestrian\npatterns and crowd flow analysis. Existing works (e.g., image-level pedestrian\ncounting, crossline crowd counting et al.) either only focus on the image-level\ncounting or are constrained to the manual annotation of lines. In this work, we\npropose to conduct the pedestrian counting from a new perspective - Video\nIndividual Counting (VIC), which counts the total number of individual\npedestrians in the given video (a person is only counted once). Instead of\nrelying on the Multiple Object Tracking (MOT) techniques, we propose to solve\nthe problem by decomposing all pedestrians into the initial pedestrians who\nexisted in the first frame and the new pedestrians with separate identities in\neach following frame. Then, an end-to-end Decomposition and Reasoning Network\n(DRNet) is designed to predict the initial pedestrian count with the density\nestimation method and reason the new pedestrian's count of each frame with the\ndifferentiable optimal transport. Extensive experiments are conducted on two\ndatasets with congested pedestrians and diverse scenes, demonstrating the\neffectiveness of our method over baselines with great superiority in counting\nthe individual pedestrians. Code: https://github.com/taohan10200/DRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binary Morphological Neural Network. (arXiv:2203.12337v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12337","description":"<p>In the last ten years, Convolutional Neural Networks (CNNs) have formed the\nbasis of deep-learning architectures for most computer vision tasks. However,\nthey are not necessarily optimal. For example, mathematical morphology is known\nto be better suited to deal with binary images. In this work, we create a\nmorphological neural network that handles binary inputs and outputs. We propose\ntheir construction inspired by CNNs to formulate layers adapted to such images\nby replacing convolutions with erosions and dilations. We give explainable\ntheoretical results on whether or not the resulting learned networks are indeed\nmorphological operators. We present promising experimental results designed to\nlearn basic binary operators, and we have made our code publicly available\nonline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aouad_T/0/1/0/all/0/1\">Theodore Aouad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbot_H/0/1/0/all/0/1\">Hugues Talbot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Object Detection for Streaming Perception. (arXiv:2203.12338v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12338","description":"<p>Autonomous driving requires the model to perceive the environment and (re)act\nwithin a low latency for safety. While past works ignore the inevitable changes\nin the environment after processing, streaming perception is proposed to\njointly evaluate the latency and accuracy into a single metric for video online\nperception. In this paper, instead of searching trade-offs between accuracy and\nspeed like previous works, we point out that endowing real-time models with the\nability to predict the future is the key to dealing with this problem. We build\na simple and effective framework for streaming perception. It equips a novel\nDualFlow Perception module (DFP), which includes dynamic and static flows to\ncapture the moving trend and basic detection feature for streaming prediction.\nFurther, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to\ngenerate adaptive weights for objects with different moving speeds. Our simple\nmethod achieves competitive performance on Argoverse-HD dataset and improves\nthe AP by 4.9% compared to the strong baseline, validating its effectiveness.\nOur code will be made available at https://github.com/yancie-yjr/StreamYOLO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin. (arXiv:2203.12341v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12341","description":"<p>Only parts of unlabeled data are selected to train models for most\nsemi-supervised learning methods, whose confidence scores are usually higher\nthan the pre-defined threshold (i.e., the confidence margin). We argue that the\nrecognition performance should be further improved by making full use of all\nunlabeled data. In this paper, we learn an Adaptive Confidence Margin (Ada-CM)\nto fully leverage all unlabeled data for semi-supervised deep facial expression\nrecognition. All unlabeled samples are partitioned into two subsets by\ncomparing their confidence scores with the adaptively learned confidence margin\nat each training epoch: (1) subset I including samples whose confidence scores\nare no lower than the margin; (2) subset II including samples whose confidence\nscores are lower than the margin. For samples in subset I, we constrain their\npredictions to match pseudo labels. Meanwhile, samples in subset II participate\nin the feature-level contrastive objective to learn effective facial expression\nfeatures. We extensively evaluate Ada-CM on four challenging datasets, showing\nthat our method achieves state-of-the-art performance, especially surpassing\nfully-supervised baselines in a semi-supervised manner. Ablation study further\nproves the effectiveness of our method. The source code is available at\nhttps://github.com/hangyu94/Ada-CM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs. (arXiv:2203.12344v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12344","description":"<p>We aim to understand how actions are performed and identify subtle\ndifferences, such as 'fold firmly' vs. 'fold gently'. To this end, we propose a\nmethod which recognizes adverbs across different actions. However, such\nfine-grained annotations are difficult to obtain and their long-tailed nature\nmakes it challenging to recognize adverbs in rare action-adverb compositions.\nOur approach therefore uses semi-supervised learning with multiple adverb\npseudo-labels to leverage videos with only action labels. Combined with\nadaptive thresholding of these pseudo-adverbs we are able to make efficient use\nof the available data while tackling the long-tailed distribution.\nAdditionally, we gather adverb annotations for three existing video retrieval\ndatasets, which allows us to introduce the new tasks of recognizing adverbs in\nunseen action-adverb compositions and unseen domains. Experiments demonstrate\nthe effectiveness of our method, which outperforms prior work in recognizing\nadverbs and semi-supervised works adapted for adverb recognition. We also show\nhow adverbs can relate fine-grained actions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1\">Hazel Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Text Line Detection in Historical Documents: Learning and Evaluation Methods. (arXiv:2203.12346v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12346","description":"<p>Text line segmentation is one of the key steps in historical document\nunderstanding. It is challenging due to the variety of fonts, contents, writing\nstyles and the quality of documents that have degraded through the years.\n</p>\n<p>In this paper, we address the limitations that currently prevent people from\nbuilding line segmentation models with a high generalization capacity. We\npresent a study conducted using three state-of-the-art systems Doc-UFCN,\ndhSegment and ARU-Net and show that it is possible to build generic models\ntrained on a wide variety of historical document datasets that can correctly\nsegment diverse unseen pages. This paper also highlights the importance of the\nannotations used during training: each existing dataset is annotated\ndifferently. We present a unification of the annotations and show its positive\nimpact on the final text recognition results. In this end, we present a\ncomplete evaluation strategy using standard pixel-level metrics, object-level\nones and introducing goal-oriented metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boillet_M/0/1/0/all/0/1\">M&#xe9;lodie Boillet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kermorvant_C/0/1/0/all/0/1\">Christopher Kermorvant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paquet_T/0/1/0/all/0/1\">Thierry Paquet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyper-Spectral Imaging for Overlapping Plastic Flakes Segmentation. (arXiv:2203.12350v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12350","description":"<p>Given the hyper-spectral imaging unique potentials in grasping the polymer\ncharacteristics of different materials, it is commonly used in sorting\nprocedures. In a practical plastic sorting scenario, multiple plastic flakes\nmay overlap which depending on their characteristics, the overlap can be\nreflected in their spectral signature. In this work, we use hyper-spectral\nimaging for the segmentation of three types of plastic flakes and their\npossible overlapping combinations. We propose an intuitive and simple\nmulti-label encoding approach, bitfield encoding, to account for the\noverlapping regions. With our experiments, we show that the bitfield encoding\nimproves over the baseline single-label approach and we further demonstrate its\npotential in predicting multiple labels for overlapping classes even when the\nmodel is only trained with non-overlapping classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_G/0/1/0/all/0/1\">Guillem Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1\">Maya Aghaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkstra_M/0/1/0/all/0/1\">Martin Dijkstra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagarajan_B/0/1/0/all/0/1\">Bhalaji Nagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaarsma_F/0/1/0/all/0/1\">Femke Jaarsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loosdrecht_J/0/1/0/all/0/1\">Jaap van de Loosdrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radeva_P/0/1/0/all/0/1\">Petia Radeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkstra_K/0/1/0/all/0/1\">Klaas Dijkstra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MONAI Label: A framework for AI-assisted Interactive Labeling of 3D Medical Images. (arXiv:2203.12362v1 [cs.HC])","link":"http://arxiv.org/abs/2203.12362","description":"<p>The lack of annotated datasets is a major challenge in training new\ntask-specific supervised AI algorithms as manual annotation is expensive and\ntime-consuming. To address this problem, we present MONAI Label, a free and\nopen-source platform that facilitates the development of AI-based applications\nthat aim at reducing the time required to annotate 3D medical image datasets.\nThrough MONAI Label researchers can develop annotation applications focusing on\ntheir domain of expertise. It allows researchers to readily deploy their apps\nas services, which can be made available to clinicians via their preferred\nuser-interface. Currently, MONAI Label readily supports locally installed\n(3DSlicer) and web-based (OHIF) frontends, and offers two Active learning\nstrategies to facilitate and speed up the training of segmentation algorithms.\nMONAI Label allows researchers to make incremental improvements to their\nlabeling apps by making them available to other researchers and clinicians\nalike. Lastly, MONAI Label provides sample labeling apps, namely DeepEdit and\nDeepGrow, demonstrating dramatically reduced annotation times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Pinto_A/0/1/0/all/0/1\">Andres Diaz-Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alle_S/0/1/0/all/0/1\">Sachidanand Alle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihsani_A/0/1/0/all/0/1\">Alvin Ihsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asad_M/0/1/0/all/0/1\">Muhammad Asad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_V/0/1/0/all/0/1\">Vishwesh Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_P/0/1/0/all/0/1\">Pritesh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1\">Holger R. Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogra_P/0/1/0/all/0/1\">Prerna Dogra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Andrew Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Multimodal Information Fusion for Facial Expression Analysis. (arXiv:2203.12367v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12367","description":"<p>Facial expression analysis has been a crucial research problem in the\ncomputer vision area. With the recent development of deep learning techniques\nand large-scale in-the-wild annotated datasets, facial expression analysis is\nnow aimed at challenges in real world settings. In this paper, we introduce our\nsubmission to CVPR2022 Competition on Affective Behavior Analysis in-the-wild\n(ABAW) that defines four competition tasks, including expression\nclassification, action unit detection, valence-arousal estimation, and a\nmulti-task-learning. The available multimodal information consist of spoken\nwords, speech prosody, and visual expression in videos. Our work proposes four\nunified transformer-based network frameworks to create the fusion of the above\nmultimodal information. The preliminary results on the official Aff-Wild2\ndataset are reported and demonstrate the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_F/0/1/0/all/0/1\">Feng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Bowen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_R/0/1/0/all/0/1\">Rudong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the (Limited) Generalization of MasterFace Attacks and Its Relation to the Capacity of Face Representations. (arXiv:2203.12387v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12387","description":"<p>A MasterFace is a face image that can successfully match against a large\nportion of the population. Since their generation does not require access to\nthe information of the enrolled subjects, MasterFace attacks represent a\npotential security risk for widely-used face recognition systems. Previous\nworks proposed methods for generating such images and demonstrated that these\nattacks can strongly compromise face recognition. However, previous works\nfollowed evaluation settings consisting of older recognition models, limited\ncross-dataset and cross-model evaluations, and the use of low-scale testing\ndata. This makes it hard to state the generalizability of these attacks. In\nthis work, we comprehensively analyse the generalizability of MasterFace\nattacks in empirical and theoretical investigations. The empirical\ninvestigations include the use of six state-of-the-art FR models, cross-dataset\nand cross-model evaluation protocols, and utilizing testing datasets of\nsignificantly higher size and variance. The results indicate a low\ngeneralizability when MasterFaces are training on a different face recognition\nmodel than the one used for testing. In these cases, the attack performance is\nsimilar to zero-effort imposter attacks. In the theoretical investigations, we\ndefine and estimate the face capacity and the maximum MasterFace coverage under\nthe assumption that identities in the face space are well separated. The\ncurrent trend of increasing the fairness and generalizability in face\nrecognition indicates that the vulnerability of future systems might further\ndecrease. We conclude that MasterFaces should not be seen as a threat to face\nrecognition systems but, on the contrary, seen as a tool to understand and\nenhance the robustness of face recognition models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1\">Philipp Terh&#xf6;rst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bierbaum_F/0/1/0/all/0/1\">Florian Bierbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Marco Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-Boost NAS: Utilization-Boosted Differentiable Neural Architecture Search. (arXiv:2203.12412v1 [cs.LG])","link":"http://arxiv.org/abs/2203.12412","description":"<p>Optimizing resource utilization in target platforms is key to achieving high\nperformance during DNN inference. While optimizations have been proposed for\ninference latency, memory footprint, and energy consumption, prior\nhardware-aware neural architecture search (NAS) methods have omitted resource\nutilization, preventing DNNs to take full advantage of the target inference\nplatforms. Modeling resource utilization efficiently and accurately is\nchallenging, especially for widely-used array-based inference accelerators such\nas Google TPU. In this work, we propose a novel hardware-aware NAS framework\nthat does not only optimize for task accuracy and inference latency, but also\nfor resource utilization. We also propose and validate a new computational\nmodel for resource utilization in inference accelerators. By using the proposed\nNAS framework and the proposed resource utilization model, we achieve 2.8 - 4x\nspeedup for DNN inference compared to prior hardware-aware NAS methods while\nattaining similar or improved accuracy in image classification on CIFAR-10 and\nImagenet-100 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuzuguler_A/0/1/0/all/0/1\">Ahmet Caner Y&#xfc;z&#xfc;g&#xfc;ler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitriadis_N/0/1/0/all/0/1\">Nikolaos Dimitriadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attention-based Method for Action Unit Detection at the 3rd ABAW Competition. (arXiv:2203.12428v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12428","description":"<p>Facial Action Coding System is an approach for modeling the complexity of\nhuman emotional expression. Automatic action unit (AU) detection is a crucial\nresearch area in human-computer interaction. This paper describes our\nsubmission to the third Affective Behavior Analysis in-the-wild (ABAW)\ncompetition 2022. We proposed a method for detecting facial action units in the\nvideo. At the first stage, a lightweight CNN-based feature extractor is\nemployed to extract the feature map from each video frame. Then, an attention\nmodule is applied to refine the attention map. The attention encoded vector is\nderived using a weighted sum of the feature map and the attention scores later.\nFinally, the sigmoid function is used at the output layer to make the\nprediction suitable for multi-label AUs detection. We achieved a macro F1 score\nof 0.48 on the ABAW challenge validation set compared to 0.39 from the baseline\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoai_D/0/1/0/all/0/1\">Duy Le Hoai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Eunchae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunbin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sieun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pant_S/0/1/0/all/0/1\">Sudarshan Pant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Guee-Sang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Huyng Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hyung-Jeong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMEMO: Social Memory for Trajectory Forecasting. (arXiv:2203.12446v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12446","description":"<p>Effective modeling of human interactions is of utmost importance when\nforecasting behaviors such as future trajectories. Each individual, with its\nmotion, influences surrounding agents since everyone obeys to social\nnon-written rules such as collision avoidance or group following. In this paper\nwe model such interactions, which constantly evolve through time, by looking at\nthe problem from an algorithmic point of view, i.e. as a data manipulation\ntask. We present a neural network based on an end-to-end trainable working\nmemory, which acts as an external storage where information about each agent\ncan be continuously written, updated and recalled. We show that our method is\ncapable of learning explainable cause-effect relationships between motions of\ndifferent agents, obtaining state-of-the-art results on multiple trajectory\nforecasting datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchetti_F/0/1/0/all/0/1\">Francesco Marchetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becattini_F/0/1/0/all/0/1\">Federico Becattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidenari_L/0/1/0/all/0/1\">Lorenzo Seidenari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels. (arXiv:2203.12454v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12454","description":"<p>The success of deep convolutional neural networks (DCNNs) benefits from high\nvolumes of annotated data. However, annotating medical images is laborious,\nexpensive, and requires human expertise, which induces the label scarcity\nproblem. Especially when encountering the domain shift, the problem becomes\nmore serious. Although deep unsupervised domain adaptation (UDA) can leverage\nwell-established source domain annotations and abundant target domain data to\nfacilitate cross-modality image segmentation and also mitigate the label\npaucity problem on the target domain, the conventional UDA methods suffer from\nsevere performance degradation when source domain annotations are scarce. In\nthis paper, we explore a challenging UDA setting - limited source domain\nannotations. We aim to investigate how to efficiently leverage unlabeled data\nfrom the source and target domains with limited source annotations for\ncross-modality image segmentation. To achieve this, we propose a new\nlabel-efficient UDA framework, termed MT-UDA, in which the student model\ntrained with limited source labels learns from unlabeled data of both domains\nby two teacher models respectively in a semi-supervised manner. More\nspecifically, the student model not only distills the intra-domain semantic\nknowledge by encouraging prediction consistency but also exploits the\ninter-domain anatomical information by enforcing structural consistency.\nConsequently, the student model can effectively integrate the underlying\nknowledge beneath available data resources to mitigate the impact of source\nlabel scarcity and yield improved cross-modality segmentation performance. We\nevaluate our method on MM-WHS 2017 dataset and demonstrate that our approach\noutperforms the state-of-the-art methods by a large margin under the\nsource-label scarcity scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kaixin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shumeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Activation-Based Sampling for Pixel- to Image-Level Aggregation in Weakly-Supervised Segmentation. (arXiv:2203.12459v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12459","description":"<p>Classification networks can be used to localize and segment objects in images\nby means of class activation maps (CAMs). However, without pixel-level\nannotations, they are known to (1) mainly focus on discriminative regions, and\n(2) to produce diffuse CAMs without well-defined prediction contours. In this\nwork, we approach both problems with two contributions for improving CAM\nlearning. First, we incorporate importance sampling based on the class-wise\nprobability mass function induced by the CAMs to produce stochastic image-level\nclass predictions. This results in CAMs which activate over a larger extent of\nthe objects. Second, we formulate a feature similarity loss term which aims to\nmatch the prediction contours with edges in the image. As a third contribution,\nwe conduct experiments on the PASCAL VOC and MS-COCO benchmark datasets to\ndemonstrate that these modifications significantly increase the performance in\nterms of contour accuracy, while being comparable to current state-of-the-art\nmethods in terms of region similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jonnarth_A/0/1/0/all/0/1\">Arvi Jonnarth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yushan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Adapted Random Forest Vision (3DARFV) for Untangling Heterogeneous-Fabric Exceeding Deep Learning Semantic Segmentation Efficiency at the Utmost Accuracy. (arXiv:2203.12469v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12469","description":"<p>Planetary exploration depends heavily on 3D image data to characterize the\nstatic and dynamic properties of the rock and environment. Analyzing 3D images\nrequires many computations, causing efficiency to suffer lengthy processing\ntime alongside large energy consumption. High-Performance Computing (HPC)\nprovides apparent efficiency at the expense of energy consumption. However, for\nremote explorations, the conveyed surveillance and the robotized sensing need\nfaster data analysis with ultimate accuracy to make real-time decisions. In\nsuch environments, access to HPC and energy is limited. Therefore, we realize\nthat reducing the number of computations to optimal and maintaining the desired\naccuracy leads to higher efficiency. This paper demonstrates the semantic\nsegmentation capability of a probabilistic decision tree algorithm, 3D Adapted\nRandom Forest Vision (3DARFV), exceeding deep learning algorithm efficiency at\nthe utmost accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfarisi_O/0/1/0/all/0/1\">Omar Alfarisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aung_Z/0/1/0/all/0/1\">Zeyar Aung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khateeb_A/0/1/0/all/0/1\">Ashraf Al-Khateeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhashmi_H/0/1/0/all/0/1\">Hamed Alhashmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelsalam_M/0/1/0/all/0/1\">Mohamed Abdelsalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alzaabi_S/0/1/0/all/0/1\">Salem Alzaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyazeedi_H/0/1/0/all/0/1\">Haifa Alyazeedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzes_A/0/1/0/all/0/1\">Anthony Tzes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptively Re-weighting Multi-Loss Untrained Transformer for Sparse-View Cone-Beam CT Reconstruction. (arXiv:2203.12476v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12476","description":"<p>Cone-Beam Computed Tomography (CBCT) has been proven useful in diagnosis, but\nhow to shorten scanning time with lower radiation dosage and how to efficiently\nreconstruct 3D image remain as the main issues for clinical practice. The\nrecent development of tomographic image reconstruction on sparse-view\nmeasurements employs deep neural networks in a supervised way to tackle such\nissues, whereas the success of model training requires quantity and quality of\nthe given paired measurements/images. We propose a novel untrained Transformer\nto fit the CBCT inverse solver without training data. It is mainly comprised of\nan untrained 3D Transformer of billions of network weights and a multi-level\nloss function with variable weights. Unlike conventional deep neural networks\n(DNNs), there is no requirement of training steps in our approach. Upon\nobserving the hardship of optimising Transformer, the variable weights within\nthe loss function are designed to automatically update together with the\niteration process, ultimately stabilising its optimisation. We evaluate the\nproposed approach on two publicly available datasets: SPARE and Walnut. The\nresults show a significant performance improvement on image quality metrics\nwith streak artefact reduction in the visualisation. We also provide a clinical\nreport by an experienced radiologist to assess our reconstructed images in a\ndiagnosis point of view. The source code and the optimised models are available\nfrom the corresponding author on request at the moment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Minghui Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yangdi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yingying Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1\">Guangwei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1\">Qingqing Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Hongxiang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Framework to Reconstruct Face under Mask. (arXiv:2203.12482v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12482","description":"<p>While deep learning-based image reconstruction methods have shown significant\nsuccess in removing objects from pictures, they have yet to achieve acceptable\nresults for attributing consistency to gender, ethnicity, expression, and other\ncharacteristics like the topological structure of the face. The purpose of this\nwork is to extract the mask region from a masked image and rebuild the area\nthat has been detected. This problem is complex because (i) it is difficult to\ndetermine the gender of an image hidden behind a mask, which causes the network\nto become confused and reconstruct the male face as a female or vice versa;\n(ii) we may receive images from multiple angles, making it extremely difficult\nto maintain the actual shape, topological structure of the face and a natural\nimage; and (iii) there are problems with various mask forms because, in some\ncases, the area of the mask cannot be anticipated precisely; certain parts of\nthe mask remain on the face after completion. To solve this complex task, we\nsplit the problem into three phases: landmark detection, object detection for\nthe targeted mask area, and inpainting the addressed mask region. To begin, to\nsolve the first problem, we have used gender classification, which detects the\nactual gender behind a mask, then we detect the landmark of the masked facial\nimage. Second, we identified the non-face item, i.e., the mask, and used the\nMask R-CNN network to create the binary mask of the observed mask area.\nThirdly, we developed an inpainting network that uses anticipated landmarks to\ncreate realistic images. To segment the mask, this article uses a mask R-CNN\nand offers a binary segmentation map for identifying the mask area.\nAdditionally, we generated the image utilizing landmarks as structural guidance\nthrough a GAN-based network. The studies presented in this paper use the FFHQ\nand CelebA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modak_G/0/1/0/all/0/1\">Gourango Modak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Shuvra Smaran Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miraj_M/0/1/0/all/0/1\">Md. Ajharul Islam Miraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morol_M/0/1/0/all/0/1\">Md. Kishor Morol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CroMo: Cross-Modal Learning for Monocular Depth Estimation. (arXiv:2203.12485v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12485","description":"<p>Learning-based depth estimation has witnessed recent progress in multiple\ndirections; from self-supervision using monocular video to supervised methods\noffering highest accuracy. Complementary to supervision, further boosts to\nperformance and robustness are gained by combining information from multiple\nsignals. In this paper we systematically investigate key trade-offs associated\nwith sensor and modality design choices as well as related model training\nstrategies. Our study leads us to a new method, capable of connecting\nmodality-specific advantages from polarisation, Time-of-Flight and\nstructured-light inputs. We propose a novel pipeline capable of estimating\ndepth from monocular polarisation for which we evaluate various training\nsignals. The inversion of differentiable analytic models thereby connects scene\ngeometry with polarisation and ToF signals and enables self-supervised and\ncross-modal learning. In the absence of existing multimodal datasets, we\nexamine our approach with a custom-made multi-modal camera rig and collect\nCroMo; the first dataset to consist of synchronized stereo polarisation,\nindirect ToF and structured-light depth, captured at video rates. Extensive\nexperiments on challenging video scenes confirm both qualitative and\nquantitative pipeline advantages where we are able to outperform competitive\nmonocular depth estimation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verdie_Y/0/1/0/all/0/1\">Yannick Verdi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jifei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mas_B/0/1/0/all/0/1\">Barnab&#xe9; Mas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refine-Net: Normal Refinement Neural Network for Noisy Point Clouds. (arXiv:2203.12514v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12514","description":"<p>Point normal, as an intrinsic geometric property of 3D objects, not only\nserves conventional geometric tasks such as surface consolidation and\nreconstruction, but also facilitates cutting-edge learning-based techniques for\nshape analysis and generation. In this paper, we propose a normal refinement\nnetwork, called Refine-Net, to predict accurate normals for noisy point clouds.\nTraditional normal estimation wisdom heavily depends on priors such as surface\nshapes or noise distributions, while learning-based solutions settle for single\ntypes of hand-crafted features. Differently, our network is designed to refine\nthe initial normal of each point by extracting additional information from\nmultiple feature representations. To this end, several feature modules are\ndeveloped and incorporated into Refine-Net by a novel connection module.\nBesides the overall network architecture of Refine-Net, we propose a new\nmulti-scale fitting patch selection scheme for the initial normal estimation,\nby absorbing geometry domain knowledge. Also, Refine-Net is a generic normal\nestimation framework: 1) point normals obtained from other methods can be\nfurther refined, and 2) any feature module related to the surface geometric\nstructures can be potentially integrated into the framework. Qualitative and\nquantitative evaluations demonstrate the clear superiority of Refine-Net over\nthe state-of-the-arts on both synthetic and real-scanned datasets. Our code is\navailable at https://github.com/hrzhou2/refinenet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingkui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label Transformer for Action Unit Detection. (arXiv:2203.12531v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12531","description":"<p>Action Unit (AU) Detection is the branch of affective computing that aims at\nrecognizing unitary facial muscular movements. It is key to unlock unbiaised\ncomputational face representations and has therefore aroused great interest in\nthe past few years. One of main obstacles toward building efficient deep\nlearning based AU detection system facial images database annotated by AU\nexperts. In that extent the ABAW challenge paves the way toward better AU\ndetection as it involves a ~2M frames AU annotated dataset. In this paper, we\npresent our submission to the ABAW3 challenge. In a nutshell, we applied a\nmulti-label detection transformer that leverage multi-head attention to learn\nwhich part of the face image is the most relevant to predict each AU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tallec_G/0/1/0/all/0/1\">Gauthier Tallec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvinec_E/0/1/0/all/0/1\">Edouard Yvinec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1\">Arnaud Dapogny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1\">Kevin Bailly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GriTS: Grid table similarity metric for table structure recognition. (arXiv:2203.12555v1 [cs.LG])","link":"http://arxiv.org/abs/2203.12555","description":"<p>In this paper, we propose a new class of evaluation metric for table\nstructure recognition, grid table similarity (GriTS). Unlike prior metrics,\nGriTS evaluates the correctness of a predicted table directly in its natural\nform as a matrix. To create a similarity measure between matrices, we\ngeneralize the two-dimensional largest common substructure (2D-LCS) problem,\nwhich is NP-hard, to the 2D most similar substructures (2D-MSS) problem and\npropose a polynomial-time heuristic for solving it. We validate empirically\nusing the PubTables-1M dataset that comparison between matrices exhibits more\ndesirable behavior than alternatives for table structure recognition\nevaluation. GriTS also unifies all three subtasks of cell topology recognition,\ncell location recognition, and cell content recognition within the same\nframework, which simplifies the evaluation and enables more meaningful\ncomparisons across different types of structure recognition approaches. Code\nwill be released at https://github.com/microsoft/table-transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smock_B/0/1/0/all/0/1\">Brandon Smock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesala_R/0/1/0/all/0/1\">Rohith Pesala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_R/0/1/0/all/0/1\">Robin Abraham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation. (arXiv:2203.12560v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12560","description":"<p>Earth observation is a fundamental tool for monitoring the evolution of land\nuse in specific areas of interest. Observing and precisely defining change, in\nthis context, requires both time-series data and pixel-wise segmentations. To\nthat end, we propose the DynamicEarthNet dataset that consists of daily,\nmulti-spectral satellite observations of 75 selected areas of interest\ndistributed over the globe with imagery from Planet Labs. These observations\nare paired with pixel-wise monthly semantic segmentation labels of 7 land use\nand land cover (LULC) classes. DynamicEarthNet is the first dataset that\nprovides this unique combination of daily measurements and high-quality labels.\nIn our experiments, we compare several established baselines that either\nutilize the daily observations as additional training data (semi-supervised\nlearning) or multiple observations at once (spatio-temporal learning) as a\npoint of reference for future research. Finally, we propose a new evaluation\nmetric SCS that addresses the specific challenges associated with time-series\nsemantic change segmentation. The data is available at:\nhttps://mediatum.ub.tum.<a href=\"/abs/de/1650201\">de/1650201</a>.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toker_A/0/1/0/all/0/1\">Aysim Toker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondmann_L/0/1/0/all/0/1\">Lukas Kondmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1\">Mark Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenberger_M/0/1/0/all/0/1\">Marvin Eisenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camero_A/0/1/0/all/0/1\">Andr&#xe9;s Camero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingliang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoderlein_A/0/1/0/all/0/1\">Ariadna Pregel Hoderlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senaras_C/0/1/0/all/0/1\">&#xc7;a&#x11f;lar &#x15e;enaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_T/0/1/0/all/0/1\">Timothy Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_G/0/1/0/all/0/1\">Giovanni Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Your \"Attention\" Deserves Attention: A Self-Diversified Multi-Channel Attention for Facial Action Analysis. (arXiv:2203.12570v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12570","description":"<p>Visual attention has been extensively studied for learning fine-grained\nfeatures in both facial expression recognition (FER) and Action Unit (AU)\ndetection. A broad range of previous research has explored how to use attention\nmodules to localize detailed facial parts (e,g. facial action units), learn\ndiscriminative features, and learn inter-class correlation. However, few\nrelated works pay attention to the robustness of the attention module itself.\nThrough experiments, we found neural attention maps initialized with different\nfeature maps yield diverse representations when learning to attend the\nidentical Region of Interest (ROI). In other words, similar to general feature\nlearning, the representational quality of attention maps also greatly affects\nthe performance of a model, which means unconstrained attention learning has\nlots of randomnesses. This uncertainty lets conventional attention learning\nfall into sub-optimal. In this paper, we propose a compact model to enhance the\nrepresentational and focusing power of neural attention maps and learn the\n\"inter-attention\" correlation for refined attention maps, which we term the\n\"Self-Diversified Multi-Channel Attention Network (SMA-Net)\". The proposed\nmethod is evaluated on two benchmark databases (BP4D and DISFA) for AU\ndetection and four databases (CK+, MMI, BU-3DFE, and BP4D+) for facial\nexpression recognition. It achieves superior performance compared to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Geran Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lijun Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuMan: Neural Human Radiance Field from a Single Video. (arXiv:2203.12575v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12575","description":"<p>Photorealistic rendering and reposing of humans is important for enabling\naugmented reality experiences. We propose a novel framework to reconstruct the\nhuman and the scene that can be rendered with novel human poses and views from\njust a single in-the-wild video. Given a video captured by a moving camera, we\ntrain two NeRF models: a human NeRF model and a scene NeRF model. To train\nthese models, we rely on existing methods to estimate the rough geometry of the\nhuman and the scene. Those rough geometry estimates allow us to create a\nwarping field from the observation space to the canonical pose-independent\nspace, where we train the human model in. Our method is able to learn subject\nspecific details, including cloth wrinkles and accessories, from just a 10\nseconds video clip, and to provide high quality renderings of the human under\nnovel poses, from novel views, together with the background.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samei_G/0/1/0/all/0/1\">Golnoosh Samei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1\">Oncel Tuzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1\">Anurag Ranjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R3M: A Universal Visual Representation for Robot Manipulation. (arXiv:2203.12601v1 [cs.RO])","link":"http://arxiv.org/abs/2203.12601","description":"<p>We study how visual representations pre-trained on diverse human video data\ncan enable data-efficient learning of downstream robotic manipulation tasks.\nConcretely, we pre-train a visual representation using the Ego4D human video\ndataset using a combination of time-contrastive learning, video-language\nalignment, and an L1 penalty to encourage sparse and compact representations.\nThe resulting representation, R3M, can be used as a frozen perception module\nfor downstream policy learning. Across a suite of 12 simulated robot\nmanipulation tasks, we find that R3M improves task success by over 20% compared\nto training from scratch and by over 10% compared to state-of-the-art visual\nrepresentations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika\nPanda arm to learn a range of manipulation tasks in a real, cluttered apartment\ngiven just 20 demonstrations. Code and pre-trained models are available at\nhttps://tinyurl.com/robotr3m.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1\">Aravind Rajeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vikash Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training. (arXiv:2203.12602v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12602","description":"<p>Pre-training video transformers on extra large-scale datasets is generally\nrequired to achieve premier performance on relatively small datasets. In this\npaper, we show that video masked autoencoders (VideoMAE) are data-efficient\nlearners for self-supervised video pre-training (SSVP). We are inspired by the\nrecent ImageMAE and propose customized video tube masking and reconstruction.\nThese simple designs turn out to be effective for overcoming information\nleakage caused by the temporal correlation during video reconstruction. We\nobtain three important findings on SSVP: (1) An extremely high proportion of\nmasking ratio (i.e., 90% to 95%) still yields favorable performance of\nVideoMAE. The temporally redundant video content enables higher masking ratio\nthan that of images. (2) VideoMAE achieves impressive results on very small\ndatasets (i.e., around 3k-4k videos) without using any extra data. This is\npartially ascribed to the challenging task of video reconstruction to enforce\nhigh-level structure learning. (3) VideoMAE shows that data quality is more\nimportant than data quantity for SSVP. Domain shift between pre-training and\ntarget datasets are important issues in SSVP. Notably, our VideoMAE with the\nvanilla ViT backbone can achieve 83.9% on Kinects-400, 75.3% on\nSomething-Something V2, 90.8% on UCF101, and 61.1% on HMDB51 without using any\nextra data. Code will be released at https://github.com/MCG-NJU/VideoMAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Fairness of Chest X-ray Classifiers. (arXiv:2203.12609v1 [cs.LG])","link":"http://arxiv.org/abs/2203.12609","description":"<p>Deep learning models have reached or surpassed human-level performance in the\nfield of medical imaging, especially in disease diagnosis using chest x-rays.\nHowever, prior work has found that such classifiers can exhibit biases in the\nform of gaps in predictive performance across protected groups. In this paper,\nwe question whether striving to achieve zero disparities in predictive\nperformance (i.e. group fairness) is the appropriate fairness definition in the\nclinical setting, over minimax fairness, which focuses on maximizing the\nperformance of the worst-case group. We benchmark the performance of nine\nmethods in improving classifier fairness across these two definitions. We find,\nconsistent with prior work on non-clinical data, that methods which strive to\nachieve better worst-group performance do not outperform simple data balancing.\nWe also find that methods which achieve group fairness do so by worsening\nperformance for all groups. In light of these results, we discuss the utility\nof fairness definitions in the clinical setting, advocating for an\ninvestigation of the bias-inducing mechanisms in the underlying data generating\nprocess whenever possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dullerud_N/0/1/0/all/0/1\">Natalie Dullerud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oakden_Rayner_L/0/1/0/all/0/1\">Lauren Oakden-Rayner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfohl_S/0/1/0/all/0/1\">Stephen Robert Pfohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StructToken : Rethinking Semantic Segmentation with Structural Prior. (arXiv:2203.12612v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12612","description":"<p>In this paper, we present structure token (StructToken), a new paradigm for\nsemantic segmentation. From a perspective on semantic segmentation as per-pixel\nclassification, the previous deep learning-based methods learn the per-pixel\nrepresentation first through an encoder and a decoder head and then classify\neach pixel representation to a specific category to obtain the semantic masks.\nDifferently, we propose a structure-aware algorithm that takes structural\ninformation as prior to predict semantic masks directly without per-pixel\nclassification. Specifically, given an input image, the learnable structure\ntoken interacts with the image representations to reason the final semantic\nmasks. Three interaction approaches are explored and the results not only\noutperform the state-of-the-art methods but also contain more structural\ninformation. Experiments are conducted on three widely used datasets including\nADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could\nserve as an alternative for semantic segmentation and inspire future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhanhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Miao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction. (arXiv:2203.12613v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12613","description":"<p>We propose a novel method to reconstruct the 3D shapes of transparent objects\nusing hand-held captured images under natural light conditions. It combines the\nadvantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid\nrepresentation, to simplify the capture setting used in recent contributions.\nAfter obtaining an initial shape through the multi-view silhouettes, we\nintroduce surface-based local MLPs to encode the vertex displacement field\n(VDF) for the reconstruction of surface details. The design of local MLPs\nallows to represent the VDF in a piece-wise manner using two layer MLP\nnetworks, which is beneficial to the optimization algorithm. Defining local\nMLPs on the surface instead of the volume also reduces the searching space.\nSuch a hybrid representation enables us to relax the ray-pixel correspondences\nthat represent the light path constraint to our designed ray-cell\ncorrespondences, which significantly simplifies the implementation of\nsingle-image based environment matting algorithm. We evaluate our\nrepresentation and reconstruction algorithm on several transparent objects with\nground truth models. Our experiments show that our method can produce\nhigh-quality reconstruction results superior to state-of-the-art methods using\na simplified data acquisition setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiamin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wewei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Salient Object Detection with Spectral Cluster Voting. (arXiv:2203.12614v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12614","description":"<p>In this paper, we tackle the challenging task of unsupervised salient object\ndetection (SOD) by leveraging spectral clustering on self-supervised features.\nWe make the following contributions: (i) We revisit spectral clustering and\ndemonstrate its potential to group the pixels of salient objects; (ii) Given\nmask proposals from multiple applications of spectral clustering on image\nfeatures computed from various self-supervised models, e.g., MoCov2, SwAV,\nDINO, we propose a simple but effective winner-takes-all voting mechanism for\nselecting the salient masks, leveraging object priors based on framing and\ndistinctiveness; (iii) Using the selected object segmentation as pseudo\ngroundtruth masks, we train a salient object detector, dubbed SelfMask, which\noutperforms prior approaches on three unsupervised SOD benchmarks. Code is\npublicly available at https://github.com/NoelShin/selfmask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_G/0/1/0/all/0/1\">Gyungin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring Restaurant Styles by Mining Crowd Sourced Photos from User-Review Websites. (arXiv:1611.06301v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1611.06301","description":"<p>When looking for a restaurant online, user uploaded photos often give people\nan immediate and tangible impression about a restaurant. Due to their\ninformativeness, such user contributed photos are leveraged by restaurant\nreview websites to provide their users an intuitive and effective search\nexperience. In this paper, we present a novel approach to inferring restaurant\ntypes or styles (ambiance, dish styles, suitability for different occasions)\nfrom user uploaded photos on user-review websites. To that end, we first\ncollect a novel restaurant photo dataset associating the user contributed\nphotos with the restaurant styles from TripAdvior. We then propose a deep\nmulti-instance multi-label learning (MIML) framework to deal with the unique\nproblem setting of the restaurant style classification task. We employ a\ntwo-step bootstrap strategy to train a multi-label convolutional neural network\n(CNN). The multi-label CNN is then used to compute the confidence scores of\nrestaurant styles for all the images associated with a restaurant. The computed\nconfidence scores are further used to train a final binary classifier for each\nrestaurant style tag. Upon training, the styles of a restaurant can be profiled\nby analyzing restaurant photos with the trained multi-label CNN and SVM models.\nExperimental evaluation has demonstrated that our crowd sourcing-based approach\ncan effectively infer the restaurant style when there are a sufficient number\nof user uploaded photos for a given restaurant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tianran Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on Crowd Videos. (arXiv:1811.11849v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1811.11849","description":"<p>Group-level emotion recognition (ER) is a growing research area as the\ndemands for assessing crowds of all sizes are becoming an interest in both the\nsecurity arena as well as social media. This work extends the earlier ER\ninvestigations, which focused on either group-level ER on single images or\nwithin a video, by fully investigating group-level expression recognition on\ncrowd videos. In this paper, we propose an effective deep feature level fusion\nmechanism to model the spatial-temporal information in the crowd videos. In our\napproach, the fusing process is performed on the deep feature domain by a\ngenerative probabilistic model, Non-Volume Preserving Fusion (NVPF), that\nmodels spatial information relationships. Furthermore, we extend our proposed\nspatial NVPF approach to the spatial-temporal NVPF approach to learn the\ntemporal information between frames. To demonstrate the robustness and\neffectiveness of each component in the proposed approach, three experiments\nwere conducted: (i) evaluation on AffectNet database to benchmark the proposed\nEmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to\nbenchmark the proposed deep feature level fusion mechanism NVPF; and, (iii)\nexamine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos\n(GECV) dataset composed of 627 videos collected from publicly available\nsources. GECV dataset is a collection of videos containing crowds of people.\nEach video is labeled with emotion categories at three levels: individual\nfaces, group of people, and the entire video frame.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quach_K/0/1/0/all/0/1\">Kha Gia Quach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalata_I/0/1/0/all/0/1\">Ibsa Jalata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Completion with Semantic Knowledge and Collaborative Adversarial Learning. (arXiv:1812.03252v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.03252","description":"<p>Unlike a conventional background inpainting approach that infers a missing\narea from image patches similar to the background, face completion requires\nsemantic knowledge about the target object for realistic outputs. Current image\ninpainting approaches utilize generative adversarial networks (GANs) to achieve\nsuch semantic understanding. However, in adversarial learning, the semantic\nknowledge is learned implicitly and hence good semantic understanding is not\nalways guaranteed. In this work, we propose a collaborative adversarial\nlearning approach to face completion to explicitly induce the training process.\nOur method is formulated under a novel generative framework called\ncollaborative GAN (collaGAN), which allows better semantic understanding of a\ntarget object through collaborative learning of multiple tasks including face\ncompletion, landmark detection, and semantic segmentation. Together with the\ncollaGAN, we also introduce an inpainting concentrated scheme such that the\nmodel emphasizes more on inpainting instead of autoencoding. Extensive\nexperiments show that the proposed designs are indeed effective and\ncollaborative adversarial learning provides better feature representations of\nthe faces. In comparison with other generative image inpainting models and\nsingle task learning methods, our solution produces superior performances on\nall tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funka_Lea_G/0/1/0/all/0/1\">Gareth Funka-Lea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Knowledge is Better: Cross-Modality Volume Completion and 3D+2D Segmentation for Intracardiac Echocardiography Contouring. (arXiv:1812.03507v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.03507","description":"<p>Using catheter ablation to treat atrial fibrillation increasingly relies on\nintracardiac echocardiography (ICE) for an anatomical delineation of the left\natrium and the pulmonary veins that enter the atrium. However, it is a\nchallenge to build an automatic contouring algorithm because ICE is noisy and\nprovides only a limited 2D view of the 3D anatomy. This work provides the first\nautomatic solution to segment the left atrium and the pulmonary veins from ICE.\nIn this solution, we demonstrate the benefit of building a cross-modality\nframework that can leverage a database of diagnostic images to supplement the\nless available interventional images. To this end, we develop a novel deep\nneural network approach that uses the (i) 3D geometrical information provided\nby a position sensor embedded in the ICE catheter and the (ii) 3D image\nappearance information from a set of computed tomography cardiac volumes. We\nevaluate the proposed approach over 11,000 ICE images collected from 150\nclinical patients. Experimental results show that our model is significantly\nbetter than a direct 2D image-to-image deep neural network segmentation,\nespecially for less-observed structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funka_Lea_G/0/1/0/all/0/1\">Gareth Funka-Lea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shaohua Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin Disease Classification versus Skin Lesion Characterization: Achieving Robust Diagnosis using Multi-label Deep Neural Networks. (arXiv:1812.03520v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.03520","description":"<p>In this study, we investigate what a practically useful approach is in order\nto achieve robust skin disease diagnosis. A direct approach is to target the\nground truth diagnosis labels, while an alternative approach instead focuses on\ndetermining skin lesion characteristics that are more visually consistent and\ndiscernible. We argue that, for computer-aided skin disease diagnosis, it is\nboth more realistic and more useful that lesion type tags should be considered\nas the target of an automated diagnosis system such that the system can first\nachieve a high accuracy in describing skin lesions, and in turn facilitate\ndisease diagnosis using lesion characteristics in conjunction with other\nevidence. To further meet such an objective, we employ convolutional neural\nnetworks (CNNs) for both the disease-targeted and lesion-targeted\nclassifications. We have collected a large-scale and diverse dataset of 75,665\nskin disease images from six publicly available dermatology atlantes. Then we\ntrain and compare both disease-targeted and lesion-targeted classifiers,\nrespectively. For disease-targeted classification, only 27.6% top-1 accuracy\nand 57.9% top-5 accuracy are achieved with a mean average precision (mAP) of\n0.42. In contrast, for lesion-targeted classification, we can achieve a much\nhigher mAP of 0.70.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Multi-task Learning Approach to Skin Lesion Classification. (arXiv:1812.03527v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.03527","description":"<p>Skin lesion identification is a key step toward dermatological diagnosis.\nWhen describing a skin lesion, it is very important to note its body site\ndistribution as many skin diseases commonly affect particular parts of the\nbody. To exploit the correlation between skin lesions and their body site\ndistributions, in this study, we investigate the possibility of improving skin\nlesion classification using the additional context information provided by body\nlocation. Specifically, we build a deep multi-task learning (MTL) framework to\njointly optimize skin lesion classification and body location classification\n(the latter is used as an inductive bias). Our MTL framework uses the\nstate-of-the-art ImageNet pretrained model with specialized loss functions for\nthe two related tasks. Our experiments show that the proposed MTL based method\nperforms more robustly than its standalone (single-task) counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Mask Pyramid Network for CT/CBCT Metal Artifact Reduction with Joint Projection-Sinogram Correction. (arXiv:1907.00294v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1907.00294","description":"<p>A conventional approach to computed tomography (CT) or cone beam CT (CBCT)\nmetal artifact reduction is to replace the X-ray projection data within the\nmetal trace with synthesized data. However, existing projection or sinogram\ncompletion methods cannot always produce anatomically consistent information to\nfill the metal trace, and thus, when the metallic implant is large, significant\nsecondary artifacts are often introduced. In this work, we propose to replace\nmetal artifact affected regions with anatomically consistent content through\njoint projection-sinogram correction as well as adversarial learning. To handle\nthe metallic implants of diverse shapes and large sizes, we also propose a\nnovel mask pyramid network that enforces the mask information across the\nnetwork's encoding layers and a mask fusion loss that reduces early saturation\nof adversarial training. Our experimental results show that the proposed\nprojection-sinogram correction designs are effective and our method recovers\ninformation from the metal traces better than the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_W/0/1/0/all/0/1\">Wei-An Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Z/0/1/0/all/0/1\">Zhimin Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vogelsang_L/0/1/0/all/0/1\">Levon Vogelsang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sehnert_W/0/1/0/all/0/1\">William J. Sehnert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Arbitrary-Oriented Object Detection: Classification based Approaches Revisited. (arXiv:2003.05597v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.05597","description":"<p>Arbitrary-oriented object detection has been a building block for rotation\nsensitive tasks. We first show that the boundary problem suffered in existing\ndominant regression-based rotation detectors, is caused by angular periodicity\nor corner ordering, according to the parameterization protocol. We also show\nthat the root cause is that the ideal predictions can be out of the defined\nrange. Accordingly, we transform the angular prediction task from a regression\nproblem to a classification one. For the resulting circularly distributed angle\nclassification problem, we first devise a Circular Smooth Label technique to\nhandle the periodicity of angle and increase the error tolerance to adjacent\nangles. To reduce the excessive model parameters by Circular Smooth Label, we\nfurther design a Densely Coded Labels, which greatly reduces the length of the\nencoding. Finally, we further develop an object heading detection module, which\ncan be useful when the exact heading orientation information is needed e.g. for\nship and plane heading detection. We release our OHD-SJTU dataset and OHDet\ndetector for heading detection. Extensive experimental results on three\nlarge-scale public datasets for aerial images i.e. DOTA, HRSC2016, OHD-SJTU,\nand face dataset FDDB, as well as scene text dataset ICDAR2015 and MLT, show\nthe effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Few-Shot Object Detection for Robotics. (arXiv:2005.02641v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.02641","description":"<p>Incremental few-shot learning is highly expected for practical robotics\napplications. On one hand, robot is desired to learn new tasks quickly and\nflexibly using only few annotated training samples; on the other hand, such new\nadditional tasks should be learned in a continuous and incremental manner\nwithout forgetting the previous learned knowledge dramatically. In this work,\nwe propose a novel Class-Incremental Few-Shot Object Detection (CI-FSOD)\nframework that enables deep object detection network to perform effective\ncontinual learning from just few-shot samples without re-accessing the previous\ntraining data. We achieve this by equipping the widely-used Faster-RCNN\ndetector with three elegant components. Firstly, to best preserve performance\non the pre-trained base classes, we propose a novel Dual-Embedding-Space (DES)\narchitecture which decouples the representation learning of base and novel\ncategories into different spaces. Secondly, to mitigate the catastrophic\nforgetting on the accumulated novel classes, we propose a Sequential Model\nFusion (SMF) method, which is able to achieve long-term memory without\nadditional storage cost. Thirdly, to promote inter-task class separation in\nfeature space, we propose a novel regularization technique that extends the\nclassification boundary further away from the previous classes to avoid\nmisclassification. Overall, our framework is simple yet effective and\noutperforms the previous SOTA with a significant margin of 2.4 points in AP\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haiyue Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Sichao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1\">Chek Sing Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Cheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadakkepat_P/0/1/0/all/0/1\">Prahlad Vadakkepat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tong Heng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse Surgical Instrument Usage for Context-aware Assistance. (arXiv:2007.00548v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.00548","description":"<p>Intra-operative anticipation of instrument usage is a necessary component for\ncontext-aware assistance in surgery, e.g. for instrument preparation or\nsemi-automation of robotic tasks. However, the sparsity of instrument\noccurrences in long videos poses a challenge. Current approaches are limited as\nthey assume knowledge on the timing of future actions or require dense temporal\nsegmentations during training and inference. We propose a novel learning task\nfor anticipation of instrument usage in laparoscopic videos that overcomes\nthese limitations. During training, only sparse instrument annotations are\nrequired and inference is done solely on image data. We train a probabilistic\nmodel to address the uncertainty associated with future events. Our approach\noutperforms several baselines and is competitive to a variant using richer\nannotations. We demonstrate the model's ability to quantify task-relevant\nuncertainties. To the best of our knowledge, we are the first to propose a\nmethod for anticipating instruments in surgery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rivoir_D/0/1/0/all/0/1\">Dominik Rivoir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenstedt_S/0/1/0/all/0/1\">Sebastian Bodenstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_I/0/1/0/all/0/1\">Isabel Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bechtolsheim_F/0/1/0/all/0/1\">Felix von Bechtolsheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Distler_M/0/1/0/all/0/1\">Marius Distler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weitz_J/0/1/0/all/0/1\">J&#xfc;rgen Weitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speidel_S/0/1/0/all/0/1\">Stefanie Speidel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Decomposition for Image Manipulation and Beyond. (arXiv:2011.00788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.00788","description":"<p>Representation disentanglement aims at learning interpretable features, so\nthat the output can be recovered or manipulated accordingly. While existing\nworks like infoGAN and AC-GAN exist, they choose to derive disjoint attribute\ncode for feature disentanglement, which is not applicable for existing/trained\ngenerative models. In this paper, we propose a decomposition-GAN (dec-GAN),\nwhich is able to achieve the decomposition of an existing latent representation\ninto content and attribute features. Guided by the classifier pre-trained on\nthe attributes of interest, our dec-GAN decomposes the attributes of interest\nfrom the latent representation, while data recovery and feature consistency\nobjectives enforce the learning of our proposed method. Our experiments on\nmultiple image datasets confirm the effectiveness and robustness of our dec-GAN\nover recent representation disentanglement models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shang-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jia-Wei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Ya-Fan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Chiang Frank Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLAD: Learning to Infer Shape Programs with Pseudo-Labels and Approximate Distributions. (arXiv:2011.13045v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13045","description":"<p>Inferring programs which generate 2D and 3D shapes is important for reverse\nengineering, editing, and more. Training models to perform this task is\ncomplicated because paired (shape, program) data is not readily available for\nmany domains, making exact supervised learning infeasible. However, it is\npossible to get paired data by compromising the accuracy of either the assigned\nprogram labels or the shape distribution. Wake-sleep methods use samples from a\ngenerative model of shape programs to approximate the distribution of real\nshapes. In self-training, shapes are passed through a recognition model, which\npredicts programs that are treated as \"pseudo-labels\" for those shapes. Related\nto these approaches, we introduce a novel self-training variant unique to\nprogram inference, where program pseudo-labels are paired with their executed\noutput shapes, avoiding label mismatch at the cost of an approximate shape\ndistribution. We propose to group these regimes under a single conceptual\nframework, where training is performed with maximum likelihood updates sourced\nfrom either Pseudo-Labels or an Approximate Distribution (PLAD). We evaluate\nthese techniques on multiple 2D and 3D shape program inference domains.\nCompared with policy gradient reinforcement learning, we show that PLAD\ntechniques infer more accurate shape programs and converge significantly\nfaster. Finally, we propose to combine updates from different PLAD methods\nwithin the training of a single model, and find that this approach outperforms\nany individual technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walke_H/0/1/0/all/0/1\">Homer Walke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN Inversion: A Survey. (arXiv:2101.05278v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05278","description":"<p>GAN inversion aims to invert a given image back into the latent space of a\npretrained GAN model, for the image to be faithfully reconstructed from the\ninverted code by the generator. As an emerging technique to bridge the real and\nfake image domains, GAN inversion plays an essential role in enabling the\npretrained GAN models such as StyleGAN and BigGAN to be used for real image\nediting applications. Meanwhile, GAN inversion also provides insights on the\ninterpretation of GAN's latent space and how the realistic images can be\ngenerated. In this paper, we provide an overview of GAN inversion with a focus\non its recent algorithms and applications. We cover important techniques of GAN\ninversion and their applications to image restoration and image manipulation.\nWe further elaborate on some trends and challenges for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Countering Malicious DeepFakes: Survey, Battleground, and Horizon. (arXiv:2103.00218v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00218","description":"<p>The creation or manipulation of facial appearance through deep generative\napproaches, known as DeepFake, have achieved significant progress and promoted\na wide range of benign and malicious applications, e.g., visual effect\nassistance in movie and misinformation generation by faking famous persons. The\nevil side of this new technique poses another popular study, i.e., DeepFake\ndetection aiming to identify the fake faces from the real ones. With the rapid\ndevelopment of the DeepFake-related studies in the community, both sides have\nformed the relationship of battleground, pushing the improvements of each other\nand inspiring new directions, e.g., the evasion of DeepFake detection.\nNevertheless, the overview of such battleground and the new direction is\nunclear and neglected by recent surveys due to the rapid increase of related\npublications, limiting the in-depth understanding of the tendency and future\nworks. To fill this gap, in this paper, we provide a comprehensive overview and\ndetailed analysis of the research work on the topic of DeepFake generation,\nDeepFake detection as well as evasion of DeepFake detection, with more than 318\nresearch papers carefully surveyed. We present the taxonomy of various DeepFake\ngeneration methods and the categorization of various DeepFake detection\nmethods, and more importantly, we showcase the battleground between the two\nparties with detailed interactions between the adversaries (DeepFake\ngeneration) and the defenders (DeepFake detection). The battleground allows\nfresh perspective into the latest landscape of the DeepFake research and can\nprovide valuable analysis towards the research challenges and opportunities as\nwell as research trends and future directions. We also elaborately design\ninteractive diagrams (<a href=\"http://www.xujuefei.com/dfsurvey\">this http URL</a>) to allow researchers to\nexplore their own interests on popular DeepFake generators or detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Run Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIBER: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework. (arXiv:2104.04182v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04182","description":"<p>We propose fill-in-the-blanks as a video understanding evaluation framework\nand introduce FIBER -- a novel dataset consisting of 28,000 videos and\ndescriptions in support of this evaluation framework. The fill-in-the-blanks\nsetting tests a model's understanding of a video by requiring it to predict a\nmasked noun phrase in the caption of the video, given the video and the\nsurrounding text. The FIBER benchmark does not share the weaknesses of the\ncurrent state-of-the-art language-informed video understanding tasks, namely:\n(1) video question answering using multiple-choice questions, where models\nperform relatively well because they exploit linguistic biases in the task\nformulation, thus making our framework challenging for the current\nstate-of-the-art systems to solve; and (2) video captioning, which relies on an\nopen-ended evaluation framework that is often inaccurate because system answers\nmay be perceived as incorrect if they differ in form from the ground truth. The\nFIBER dataset and our code are available at https://lit.eecs.umich.edu/fiber/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pingxuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_I/0/1/0/all/0/1\">Ian Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stroud_J/0/1/0/all/0/1\">Jonathan C. Stroud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point Clouds. (arXiv:2104.04891v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04891","description":"<p>Labelling point clouds fully is highly time-consuming and costly. As larger\npoint cloud datasets with billions of points become more common, we ask whether\nthe full annotation is even necessary, demonstrating that existing baselines\ndesigned under a fully annotated assumption only degrade slightly even when\nfaced with 1% random point annotations. However, beyond this point, e.g., at\n0.1% annotations, segmentation accuracy is unacceptably low. We observe that,\nas point clouds are samples of the 3D world, the distribution of points in a\nlocal neighborhood is relatively homogeneous, exhibiting strong semantic\nsimilarity. Motivated by this, we propose a new weak supervision method to\nimplicitly augment highly sparse supervision signals. Extensive experiments\ndemonstrate the proposed Semantic Query Network (SQN) achieves promising\nperformance on seven large-scale open datasets under weak supervision schemes,\nwhile requiring only 0.1% randomly annotated points for training, greatly\nreducing annotation cost and effort. The code is available at\nhttps://github.com/QingyongHu/SQN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1\">Guangchi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2104.06392","description":"<p>A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1\">David Charatan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Programmable 3D snapshot microscopy with Fourier convolutional networks. (arXiv:2104.10611v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.10611","description":"<p>3D snapshot microscopy enables fast volumetric imaging by capturing a 3D\nvolume in a single 2D camera image and performing computational reconstruction.\nFast volumetric imaging has a variety of biological applications such as whole\nbrain imaging of rapid neural activity in larval zebrafish. The optimal\nmicroscope design for this optical 3D-to-2D encoding is both sample- and\ntask-dependent, with no general solution known. Deep learning based decoders\ncan be combined with a differentiable simulation of an optical encoder for\nend-to-end optimization of both the deep learning decoder and optical encoder.\nThis technique has been used to engineer local optical encoders for other\nproblems such as depth estimation, 3D particle localization, and lensless\nphotography. However, 3D snapshot microscopy is known to require a highly\nnon-local optical encoder which existing UNet-based decoders are not able to\nengineer. We show that a neural network architecture based on global kernel\nFourier convolutional neural networks can efficiently decode information from\nmultiple depths in a volume, globally encoded across a 3D snapshot image. We\nshow in simulation that our proposed networks succeed in engineering and\nreconstructing optical encoders for 3D snapshot microscopy where the existing\nstate-of-the-art UNet architecture fails. We also show that our networks\noutperform the state-of-the-art learned reconstruction algorithms for a\ncomputational photography dataset collected on a prototype lensless camera\nwhich also uses a highly non-local optical encoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deb_D/0/1/0/all/0/1\">Diptodip Deb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiao_Z/0/1/0/all/0/1\">Zhenfei Jiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Alex B. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Broxton_M/0/1/0/all/0/1\">Michael Broxton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahrens_M/0/1/0/all/0/1\">Misha B. Ahrens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Podgorski_K/0/1/0/all/0/1\">Kaspar Podgorski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turaga_S/0/1/0/all/0/1\">Srinivas C. Turaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13450","description":"<p>Digital watermarking is widely used for copyright protection. Traditional 3D\nwatermarking approaches or commercial software are typically designed to embed\nmessages into 3D meshes, and later retrieve the messages directly from\ndistorted/undistorted watermarked 3D meshes. However, in many cases, users only\nhave access to rendered 2D images instead of 3D meshes. Unfortunately,\nretrieving messages from 2D renderings of 3D meshes is still challenging and\nunderexplored. We introduce a novel end-to-end learning framework to solve this\nproblem through: 1) an encoder to covertly embed messages in both mesh geometry\nand textures; 2) a differentiable renderer to render watermarked 3D objects\nfrom different camera angles and under varied lighting conditions; 3) a decoder\nto recover the messages from 2D rendered images. From our experiments, we show\nthat our model can learn to embed information visually imperceptible to humans,\nand to retrieve the embedded information from 2D renderings that undergo 3D\ndistortions. In addition, we demonstrate that our method can also work with\nother renderers, such as ray tracers and real-time renderers with and without\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_I/0/1/0/all/0/1\">Innfarn Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stava_O/0/1/0/all/0/1\">Ondrej Stava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Neurally-Guided Shape Parser: Grammar-based Labeling of 3D Shape Regions with Approximate Inference. (arXiv:2106.12026v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12026","description":"<p>We propose the Neurally-Guided Shape Parser (NGSP), a method that learns how\nto assign fine-grained semantic labels to regions of a 3D shape. NGSP solves\nthis problem via MAP inference, modeling the posterior probability of a label\nassignment conditioned on an input shape with a learned likelihood function. To\nmake this search tractable, NGSP employs a neural guide network that learns to\napproximate the posterior. NGSP finds high-probability label assignments by\nfirst sampling proposals with the guide network and then evaluating each\nproposal under the full likelihood. We evaluate NGSP on the task of\nfine-grained semantic segmentation of manufactured 3D shapes from PartNet,\nwhere shapes have been decomposed into regions that correspond to part instance\nover-segmentations. We find that NGSP delivers significant performance\nimprovements over comparison methods that (i) use regions to group per-point\npredictions, (ii) use regions as a self-supervisory signal or (iii) assign\nlabels to regions under alternative formulations. Further, we show that NGSP\nmaintains strong performance even with limited labeled data or noisy input\nshape regions. Finally, we demonstrate that NGSP can be directly applied to CAD\nshapes found in online repositories and validate its effectiveness with a\nperceptual study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habib_A/0/1/0/all/0/1\">Aalia Habib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2107.07436","description":"<p>Shapley values are widely used to explain black-box models, but they are\ncostly to calculate because they require many model evaluations. We introduce\nFastSHAP, a method for estimating Shapley values in a single forward pass using\na learned explainer model. FastSHAP amortizes the cost of explaining many\ninputs via a learning approach inspired by the Shapley value's weighted least\nsquares characterization, and it can be trained using standard stochastic\ngradient optimization. We compare FastSHAP to existing estimation approaches,\nrevealing that it generates high-quality explanations with orders of magnitude\nspeedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1\">Neil Jethani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1\">Mukund Sudarshan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Pose and Shape Estimation from Single Polarization Images. (arXiv:2108.06834v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06834","description":"<p>This paper focuses on a new problem of estimating human pose and shape from\nsingle polarization images. Polarization camera is known to be able to capture\nthe polarization of reflected lights that preserves rich geometric cues of an\nobject surface. Inspired by the recent applications in surface normal\nreconstruction from polarization images, in this paper, we attempt to estimate\nhuman pose and shape from single polarization images by leveraging the\npolarization-induced geometric cues. A dedicated two-stage pipeline is\nproposed: given a single polarization image, stage one (Polar2Normal) focuses\non the fine detailed human body surface normal estimation; stage two\n(Polar2Shape) then reconstructs clothed human shape from the polarization image\nand the estimated surface normal. To empirically validate our approach, a\ndedicated dataset (PHSPD) is constructed, consisting of over 500K frames with\naccurate pose and parametric shape annotations. Empirical evaluations on this\nreal-world dataset as well as a synthetic dataset, SURREAL, demonstrate the\neffectiveness of our approach. It suggests polarization camera as a promising\nalternative to the more conventional RGB camera for human pose and shape\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shihao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yiming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensor Data Augmentation by Resampling for Contrastive Learning in Human Activity Recognition. (arXiv:2109.02054v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2109.02054","description":"<p>While deep learning has contributed to the advancement of sensor-based Human\nActivity Recognition (HAR), it is usually a costly and challenging supervised\ntask with the needs of a large amount of labeled data. To alleviate this issue,\ncontrastive learning has been applied for sensor-based HAR. Data augmentation\nis an essential part of contrastive learning and has a significant impact on\nthe performance of downstream tasks. However, current popular augmentation\nmethods do not achieve competitive performance in contrastive learning for\nsensor-based HAR. Motivated by this issue, we propose a new sensor data\naugmentation method by resampling, which simulates more realistic activity data\nby varying the sampling frequency to maximize the coverage of the sampling\nspace. In addition, we extend MoCo, a popular contrastive learning framework,\nto MoCoHAR for HAR. The resampling augmentation method will be evaluated on two\ncontrastive learning frameworks, SimCLRHAR and MoCoHAR, using UCI-HAR,\nMotionSensor, and USC-HAD datasets. The experiment results show that the\nresampling augmentation method outperforms all state-of-the-art methods under a\nsmall amount of labeled data, on SimCLRHAR and MoCoHAR, with mean F1-score as\nthe evaluation metric. The results also demonstrate that not all data\naugmentation methods have positive effects in the contrastive learning\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1\">Jingyuan Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Huansheng Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yaping Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Autonomous Crop-Agnostic Visual Navigation in Arable Fields. (arXiv:2109.11936v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.11936","description":"<p>Autonomous navigation of a robot in agricultural fields is essential for\nevery task from crop monitoring to weed management and fertilizer application.\nMany current approaches rely on accurate GPS, however, such technology is\nexpensive and also prone to failure (e.g. through lack of coverage). As such,\nautonomous navigation through sensors that can interpret their environment\n(such as cameras) is important to achieve the goal of autonomy in agriculture.\nIn this paper, we introduce a purely vision-based navigation scheme that is\nable to reliably guide the robot through row-crop fields without manual\nintervention. Independent of any global localization or mapping, this approach\nis able to accurately follow the crop-rows and switch between the rows, only\nusing onboard cameras. With the help of a novel crop-row detection and a novel\ncrop-row switching technique, our navigation scheme can be deployed in a wide\nrange of fields with different canopy types in various growth stages with\nlimited parameter tuning, creating a crop agnostic navigation approach. We have\nextensively evaluated our approach in three different fields under various\nillumination conditions using our agricultural robotic platform (BonnBot-I).\nFor navigation, our approach is evaluated on five crop types and achieves an\naverage navigation accuracy of 3.82cm relative to manual teleoperation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1\">Alireza Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halstead_M/0/1/0/all/0/1\">Michael Halstead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCool_C/0/1/0/all/0/1\">Chris McCool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainability-Aware One Point Attack for Point Cloud Neural Networks. (arXiv:2110.04158v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04158","description":"<p>With the proposition of neural networks for point clouds, deep learning has\nstarted to shine in the field of 3D object recognition while researchers have\nshown an increased interest to investigate the reliability of point cloud\nnetworks by adversarial attacks. However, most of the existing studies aim to\ndeceive humans or defense algorithms, while the few that address the operation\nprinciples of the models themselves remain flawed in terms of critical point\nselection. In this work, we propose two adversarial methods: One Point Attack\n(OPA) and Critical Traversal Attack (CTA), which incorporate the explainability\ntechnologies and aim to explore the intrinsic operating principle of point\ncloud networks and their sensitivity against critical points perturbations. Our\nresults show that popular point cloud networks can be deceived with almost\n$100\\%$ success rate by shifting only one point from the input instance. In\naddition, we show the interesting impact of different point attribution\ndistributions on the adversarial robustness of point cloud networks. Finally,\nwe discuss how our approaches facilitate the explainability study for point\ncloud networks. To the best of our knowledge, this is the first\npoint-cloud-based adversarial approach concerning explainability. Our code is\navailable at https://github.com/Explain3D/Exp-One-Point-Atk-PC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection. (arXiv:2111.08644v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08644","description":"<p>Detecting abnormal events in video is commonly framed as a one-class\nclassification task, where training videos contain only normal events, while\ntest videos encompass both normal and abnormal events. In this scenario,\nanomaly detection is an open-set problem. However, some studies assimilate\nanomaly detection to action recognition. This is a closed-set scenario that\nfails to test the capability of systems at detecting new anomaly types. To this\nend, we propose UBnormal, a new supervised open-set benchmark composed of\nmultiple virtual scenes for video anomaly detection. Unlike existing data sets,\nwe introduce abnormal events annotated at the pixel level at training time, for\nthe first time enabling the use of fully-supervised learning methods for\nabnormal event detection. To preserve the typical open-set formulation, we make\nsure to include disjoint sets of anomaly types in our training and test\ncollections of videos. To our knowledge, UBnormal is the first video anomaly\ndetection benchmark to allow a fair head-to-head comparison between one-class\nopen-set models and supervised closed-set models, as shown in our experiments.\nMoreover, we provide empirical evidence showing that UBnormal can enhance the\nperformance of a state-of-the-art anomaly detection framework on two prominent\ndata sets, Avenue and ShanghaiTech. Our benchmark is freely available at\nhttps://github.com/lilygeorgescu/UBnormal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acsintoae_A/0/1/0/all/0/1\">Andra Acsintoae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florescu_A/0/1/0/all/0/1\">Andrei Florescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mare_T/0/1/0/all/0/1\">Tudor Mare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumedrea_P/0/1/0/all/0/1\">Paul Sumedrea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08918","description":"<p>Recent works with an implicit neural function shed light on representing\nimages in arbitrary resolution. However, a standalone multi-layer perceptron\nshows limited performance in learning high-frequency components. In this paper,\nwe propose a Local Texture Estimator (LTE), a dominant-frequency estimator for\nnatural images, enabling an implicit function to capture fine details while\nreconstructing images in a continuous manner. When jointly trained with a deep\nsuper-resolution (SR) architecture, LTE is capable of characterizing image\ntextures in 2D Fourier space. We show that an LTE-based neural function\nachieves favorable performance against existing deep SR methods within an\narbitrary-scale factor. Furthermore, we demonstrate that our implementation\ntakes the shortest running time compared to previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kyong Hwan Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v10 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-shape Transformer for Underwater Image Enhancement. (arXiv:2111.11843v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11843","description":"<p>The light absorption and scattering of underwater impurities lead to poor\nunderwater imaging quality. The existing data-driven based underwater image\nenhancement (UIE) techniques suffer from the lack of a large-scale dataset\ncontaining various underwater scenes and high-fidelity reference images.\nBesides, the inconsistent attenuation in different color channels and space\nareas is not fully considered for boosted enhancement. In this work, we\nconstructed a large-scale underwater image (LSUI) dataset including 5004 image\npairs, and reported an U-shape Transformer network where the transformer model\nis for the first time introduced to the UIE task. The U-shape Transformer is\nintegrated with a channel-wise multi-scale feature fusion transformer (CMSFFT)\nmodule and a spatial-wise global feature modeling transformer (SGFMT) module,\nwhich reinforce the network's attention to the color channels and space areas\nwith more serious attenuation. Meanwhile, in order to further improve the\ncontrast and saturation, a novel loss function combining RGB, LAB and LCH color\nspaces is designed following the human vision principle. The extensive\nexperiments on available datasets validate the state-of-the-art performance of\nthe reported technique with more than 2dB superiority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lintao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chunli Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_L/0/1/0/all/0/1\">Liheng Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Triangular 3D Models, Materials, and Lighting From Images. (arXiv:2111.12503v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12503","description":"<p>We present an efficient method for joint optimization of topology, materials\nand lighting from multi-view image observations. Unlike recent multi-view\nreconstruction approaches, which typically produce entangled 3D representations\nencoded in neural networks, we output triangle meshes with spatially-varying\nmaterials and environment lighting that can be deployed in any traditional\ngraphics engine unmodified. We leverage recent work in differentiable\nrendering, coordinate-based networks to compactly represent volumetric\ntexturing, alongside differentiable marching tetrahedrons to enable\ngradient-based optimization directly on the surface mesh. Finally, we introduce\na differentiable formulation of the split sum approximation of environment\nlighting to efficiently recover all-frequency lighting. Experiments show our\nextracted models used in advanced scene editing, material decomposition, and\nhigh quality view interpolation, all running at interactive rates in\ntriangle-based renderers (rasterizers and path tracers). Project website:\nhttps://nvlabs.github.io/nvdiffrec/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munkberg_J/0/1/0/all/0/1\">Jacob Munkberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasselgren_J/0/1/0/all/0/1\">Jon Hasselgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianchang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_A/0/1/0/all/0/1\">Alex Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QMagFace: Simple and Accurate Quality-Aware Face Recognition. (arXiv:2111.13475v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13475","description":"<p>Face recognition systems have to deal with large variabilities (such as\ndifferent poses, illuminations, and expressions) that might lead to incorrect\nmatching decisions. These variabilities can be measured in terms of face image\nquality which is defined over the utility of a sample for recognition. Previous\nworks on face recognition either do not employ this valuable information or\nmake use of non-inherently fit quality estimates. In this work, we propose a\nsimple and effective face recognition solution (QMagFace) that combines a\nquality-aware comparison score with a recognition model based on a\nmagnitude-aware angular margin loss. The proposed approach includes\nmodel-specific face image qualities in the comparison process to enhance the\nrecognition performance under unconstrained circumstances. Exploiting the\nlinearity between the qualities and their comparison scores induced by the\nutilized loss, our quality-aware comparison function is simple and highly\ngeneralizable. The experiments conducted on several face recognition databases\nand benchmarks demonstrate that the introduced quality-awareness leads to\nconsistent improvements in the recognition performance. Moreover, the proposed\nQMagFace approach performs especially well under challenging circumstances,\nsuch as cross-pose, cross-age, or cross-quality. Consequently, it leads to\nstate-of-the-art performances on several face recognition benchmarks, such as\n98.50% on AgeDB, 83.95% on XQLFQ, and 98.74% on CFP-FP. The code for QMagFace\nis publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1\">Philipp Terh&#xf6;rst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihlefeld_M/0/1/0/all/0/1\">Malte Ihlefeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Marco Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ranking Distance Calibration for Cross-Domain Few-Shot Learning. (arXiv:2112.00260v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00260","description":"<p>Recent progress in few-shot learning promotes a more realistic cross-domain\nsetting, where the source and target datasets are from different domains. Due\nto the domain gap and disjoint label spaces between source and target datasets,\ntheir shared knowledge is extremely limited. This encourages us to explore more\ninformation in the target domain rather than to overly elaborate training\nstrategies on the source domain as in many existing methods. Hence, we start\nfrom a generic representation pre-trained by a cross-entropy loss and a\nconventional distance-based classifier, along with an image retrieval view, to\nemploy a re-ranking process for calibrating a target distance matrix by\ndiscovering the reciprocal k-nearest neighbours within the task. Assuming the\npre-trained representation is biased towards the source, we construct a\nnon-linear subspace to minimise task-irrelevant features therewithin while keep\nmore transferrable discriminative information by a hyperbolic tangent\ntransformation. The calibrated distance in this target-aware non-linear\nsubspace is complementary to that in the pre-trained representation. To impose\nsuch distance calibration information onto the pre-trained representation, a\nKullback-Leibler divergence loss is employed to gradually guide the model\ntowards the calibrated distance-based distribution. Extensive evaluations on\neight target domains show that this target ranking calibration process can\nimprove conventional distance-based classifiers in few-shot learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Transformer Features for Image Quality Assessment. (arXiv:2112.00485v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00485","description":"<p>Objective image quality evaluation is a challenging task, which aims to\nmeasure the quality of a given image automatically. According to the\navailability of the reference images, there are Full-Reference and No-Reference\nIQA tasks, respectively. Most deep learning approaches use regression from deep\nfeatures extracted by Convolutional Neural Networks. For the FR task, another\noption is conducting a statistical comparison on deep features. For all these\nmethods, non-local information is usually neglected. In addition, the\nrelationship between FR and NR tasks is less explored. Motivated by the recent\nsuccess of transformers in modeling contextual information, we propose a\nunified IQA framework that utilizes CNN backbone and transformer encoder to\nextract features. The proposed framework is compatible with both FR and NR\nmodes and allows for a joint training scheme. Evaluation experiments on three\nstandard IQA datasets, i.e., LIVE, CSIQ and TID2013, and KONIQ-10K, show that\nour proposed model can achieve state-of-the-art FR performance. In addition,\ncomparable NR performance is achieved in extensive experiments, and the results\nshow that the NR performance can be leveraged by the joint training scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PartImageNet: A Large, High-Quality Dataset of Parts. (arXiv:2112.00933v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00933","description":"<p>It is natural to represent objects in terms of their parts. This has the\npotential to improve the performance of algorithms for object recognition and\nsegmentation but can also help for downstream tasks like activity recognition.\nResearch on part-based models, however, is hindered by the lack of datasets\nwith per-pixel part annotations. This is partly due to the difficulty and high\ncost of annotating object parts so it has rarely been done except for humans\n(where there exists a big literature on part-based models). To help address\nthis problem, we propose PartImageNet, a large, high-quality dataset with part\nsegmentation annotations. It consists of $158$ classes from ImageNet with\napproximately $24,000$ images. PartImageNet is unique because it offers\npart-level annotations on a general set of classes including non-rigid,\narticulated objects, while having an order of magnitude larger size compared to\nexisting part datasets (excluding datasets of humans). It can be utilized for\nmany vision tasks including Object Segmentation, Semantic Part Segmentation,\nFew-shot Learning and Part Discovery. We conduct comprehensive experiments\nwhich study these tasks and set up a set of baselines. The dataset and scripts\nare released at https://github.com/TACJu/PartImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Ju He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shaokang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoding Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie-Neng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qihang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training. (arXiv:2112.03552v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03552","description":"<p>Recently, vision Transformers (ViTs) are developing rapidly and starting to\nchallenge the domination of convolutional neural networks (CNNs) in the realm\nof computer vision (CV). With the general-purpose Transformer architecture\nreplacing the hard-coded inductive biases of convolution, ViTs have surpassed\nCNNs, especially in data-sufficient circumstances. However, ViTs are prone to\nover-fit on small datasets and thus rely on large-scale pre-training, which\nexpends enormous time. In this paper, we strive to liberate ViTs from\npre-training by introducing CNNs' inductive biases back to ViTs while\npreserving their network architectures for higher upper bound and setting up\nmore suitable optimization objectives. To begin with, an agent CNN is designed\nbased on the given ViT with inductive biases. Then a bootstrapping training\nalgorithm is proposed to jointly optimize the agent and ViT with weight\nsharing, during which the ViT learns inductive biases from the intermediate\nfeatures of the agent. Extensive experiments on CIFAR-10/100 and ImageNet-1k\nwith limited training data have shown encouraging results that the inductive\nbiases help ViTs converge significantly faster and outperform conventional CNNs\nwith even fewer parameters. Our code is publicly available at\nhttps://github.com/zhfeing/Bootstrapping-ViTs-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiarui Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mengqi Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Li Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watermarking Images in Self-Supervised Latent Spaces. (arXiv:2112.09581v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09581","description":"<p>We revisit watermarking techniques based on pre-trained deep networks, in the\nlight of self-supervised approaches. We present a way to embed both marks and\nbinary messages into their latent spaces, leveraging data augmentation at\nmarking time. Our method can operate at any resolution and creates watermarks\nrobust to a broad range of transformations (rotations, crops, JPEG, contrast,\netc). It significantly outperforms the previous zero-bit methods, and its\nperformance on multi-bit watermarking is on par with state-of-the-art\nencoder-decoder architectures trained end-to-end for watermarking. The code is\navailable at github.com/facebookresearch/ssl_watermarking\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1\">Pierre Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sablayrolles_A/0/1/0/all/0/1\">Alexandre Sablayrolles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1\">Teddy Furon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; J&#xe9;gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning. (arXiv:2112.10982v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10982","description":"<p>Generalized few-shot semantic segmentation was introduced to move beyond only\nevaluating few-shot segmentation models on novel classes to include testing\ntheir ability to remember base classes. While the current state-of-the-art\napproach is based on meta-learning, it performs poorly and saturates in\nlearning after observing only a few shots. We propose the first fine-tuning\nsolution, and demonstrate that it addresses the saturation problem while\nachieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We\nalso show that it outperforms existing methods, whether fine-tuning multiple\nfinal layers or only the final layer. Finally, we present a triplet loss\nregularization that shows how to redistribute the balance of performance\nbetween novel and base categories so that there is a smaller gap between them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Myers_Dean_J/0/1/0/all/0/1\">Josh Myers-Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yinan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image Segmentation using Partially Labeled Data. (arXiv:2112.12665v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.12665","description":"<p>Computer-assisted quantitative analysis on Giga-pixel pathology images has\nprovided a new avenue in histology examination. The innovations have been\nlargely focused on cancer pathology (i.e., tumor segmentation and\ncharacterization). In non-cancer pathology, the learning algorithms can be\nasked to examine more comprehensive tissue types simultaneously, as a\nmulti-label setting. The prior arts typically needed to train multiple\nsegmentation networks in order to match the domain-specific knowledge for\nheterogeneous tissue types (e.g., glomerular tuft, glomerular unit, proximal\ntubular, distal tubular, peritubular capillaries, and arteries). In this paper,\nwe propose a dynamic single segmentation network (Omni-Seg) that learns to\nsegment multiple tissue types using partially labeled images (i.e., only one\ntissue type is labeled for each training image) for renal pathology. By\nlearning from ~150,000 patch-wise pathological images from six tissue types,\nthe proposed Omni-Seg network achieved superior segmentation accuracy and less\nresource consumption when compared to the previous the multiple-network and\nmulti-head design. In the testing stage, the proposed method obtains\n\"completely labeled\" tissue segmentation results using only \"partially labeled\"\ntraining images. The source code is available at\nhttps://github.com/ddrrnn123/Omni-Seg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Behaviour of Vision Transformers with Token-consistent Stochastic Layers. (arXiv:2112.15111v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15111","description":"<p>We introduce token-consistent stochastic layers in vision transformers,\nwithout causing any severe drop in performance. The added stochasticity\nimproves network calibration, robustness and strengthens privacy. We use linear\nlayers with token-consistent stochastic parameters inside the multilayer\nperceptron blocks, without altering the architecture of the transformer. The\nstochastic parameters are sampled from the uniform distribution, both during\ntraining and inference. The applied linear operations preserve the topological\nstructure, formed by the set of tokens passing through the shared multilayer\nperceptron. This operation encourages the learning of the recognition task to\nrely on the topological structures of the tokens, instead of their values,\nwhich in turn offers the desired robustness and privacy of the visual features.\nThe effectiveness of the token-consistent stochasticity is demonstrated on\nthree different applications, namely, network calibration, adversarial\nrobustness, and feature privacy, by boosting the performance of the respective\nestablished baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nikola Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVPR: Transformer-based place recognition with multi-level attention aggregation. (arXiv:2201.02001v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02001","description":"<p>Visual place recognition is a challenging task for applications such as\nautonomous driving navigation and mobile robot localization. Distracting\nelements presenting in complex scenes often lead to deviations in the\nperception of visual place. To address this problem, it is crucial to integrate\ninformation from only task-relevant regions into image representations. In this\npaper, we introduce a novel holistic place recognition model, TransVPR, based\non vision Transformers. It benefits from the desirable property of the\nself-attention operation in Transformers which can naturally aggregate\ntask-relevant features. Attentions from multiple levels of the Transformer,\nwhich focus on different regions of interest, are further combined to generate\na global image representation. In addition, the output tokens from Transformer\nlayers filtered by the fused attention mask are considered as key-patch\ndescriptors, which are used to perform spatial matching to re-rank the\ncandidates retrieved by the global image features. The whole model allows\nend-to-end training with a single objective and image-level supervision.\nTransVPR achieves state-of-the-art performance on several real-world benchmarks\nwhile maintaining low computational time and storage requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruotong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Weiliang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields. (arXiv:2201.09968v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09968","description":"<p>High-resolution optical satellite sensors, combined with dense stereo\nalgorithms, have made it possible to reconstruct 3D city models from space.\nHowever, these models are, in practice, rather noisy and tend to miss small\ngeometric features that are clearly visible in the images. We argue that one\nreason for the limited quality may be a too early, heuristic reduction of the\ntriangulated 3D point cloud to an explicit height field or surface mesh. To\nmake full use of the point cloud and the underlying images, we introduce\nImpliCity, a neural representation of the 3D scene as an implicit, continuous\noccupancy field, driven by learned embeddings of the point cloud and a stereo\npair of ortho-photos. We show that this representation enables the extraction\nof high-quality DSMs: with image resolution 0.5$\\,$m, ImpliCity reaches a\nmedian height error of $\\approx\\,$0.7$\\,$m and outperforms competing methods,\nespecially w.r.t. building reconstruction, featuring intricate roof details,\nsmooth surfaces, and straight, regular outlines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stucker_C/0/1/0/all/0/1\">Corinne Stucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_B/0/1/0/all/0/1\">Bingxin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yuanwen Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armeni_I/0/1/0/all/0/1\">Iro Armeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection via Reverse Distillation from One-Class Embedding. (arXiv:2201.10703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10703","description":"<p>Knowledge distillation (KD) achieves promising results on the challenging\nproblem of unsupervised anomaly detection (AD).The representation discrepancy\nof anomalies in the teacher-student (T-S) model provides essential evidence for\nAD. However, using similar or identical architectures to build the teacher and\nstudent models in previous studies hinders the diversity of anomalous\nrepresentations. To tackle this problem, we propose a novel T-S model\nconsisting of a teacher encoder and a student decoder and introduce a simple\nyet effective \"reverse distillation\" paradigm accordingly. Instead of receiving\nraw images directly, the student network takes teacher model's one-class\nembedding as input and targets to restore the teacher's multiscale\nrepresentations. Inherently, knowledge distillation in this study starts from\nabstract, high-level presentations to low-level features. In addition, we\nintroduce a trainable one-class bottleneck embedding (OCBE) module in our T-S\nmodel. The obtained compact embedding effectively preserves essential\ninformation on normal patterns, but abandons anomaly perturbations. Extensive\nexperimentation on AD and one-class novelty detection benchmarks shows that our\nmethod surpasses SOTA performance, demonstrating our proposed approach's\neffectiveness and generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hanqiu Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Robust Convolutional Neural Networks with Relevant Feature Focusing via Explanations. (arXiv:2202.04237v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04237","description":"<p>Existing image recognition techniques based on convolutional neural networks\n(CNNs) basically assume that the training and test datasets are sampled from\ni.i.d distributions. However, this assumption is easily broken in the real\nworld because of the distribution shift that occurs when the co-occurrence\nrelations between objects and backgrounds in input images change. Under this\ntype of distribution shift, CNNs learn to focus on features that are not\ntask-relevant, such as backgrounds from the training data, and degrade their\naccuracy on the test data. To tackle this problem, we propose relevant feature\nfocusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via\nexplanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc\nexplanation modules, it can be easily applied to off-the-shelf CNNs.\nFurthermore, ReFF requires no additional inference cost at test time because it\nis only used for regularization while training. We demonstrate that CNNs\ntrained with ReFF focus on features relevant to the target task and that ReFF\nimproves the test-time accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adachi_K/0/1/0/all/0/1\">Kazuki Adachi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1\">Shin&#x27;ya Yamaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Fine-tuning for Backdoor Defense: Connecting Backdoor Attacks to Adversarial Attacks. (arXiv:2202.06312v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06312","description":"<p>Deep neural networks (DNNs) are known to be vulnerable to both backdoor\nattacks as well as adversarial attacks. In the literature, these two types of\nattacks are commonly treated as distinct problems and solved separately, since\nthey belong to training-time and inference-time attacks respectively. However,\nin this paper we find an intriguing connection between them: for a model\nplanted with backdoors, we observe that its adversarial examples have similar\nbehaviors as its triggered samples, i.e., both activate the same subset of DNN\nneurons. It indicates that planting a backdoor into a model will significantly\naffect the model's adversarial examples. Based on this observations, we design\na new Adversarial Fine-Tuning (AFT) algorithm to defend against backdoor\nattacks. We empirically show that, against 5 state-of-the-art backdoor attacks,\nour AFT can effectively erase the backdoor triggers without obvious performance\ndegradation on clean samples and significantly outperforms existing defense\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_B/0/1/0/all/0/1\">Bingxu Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.07508","description":"<p>In this paper, we tackle the problem of blind image super-resolution(SR) with\na reformulated degradation model and two novel modules. Following the common\npractices of blind SR, our method proposes to improve both the kernel\nestimation as well as the kernel-based high-resolution image restoration. To be\nmore specific, we first reformulate the degradation model such that the\ndeblurring kernel estimation can be transferred into the low-resolution space.\nOn top of this, we introduce a dynamic deep linear filter module. Instead of\nlearning a fixed kernel for all images, it can adaptively generate deblurring\nkernel weights conditional on the input and yield a more robust kernel\nestimation. Subsequently, a deep constrained least square filtering module is\napplied to generate clean features based on the reformulation and estimated\nkernel. The deblurred feature and the low input image feature are then fed into\na dual-path structured SR network and restore the final high-resolution result.\nTo evaluate our method, we further conduct evaluations on several benchmarks,\nincluding Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed\nmethod achieves better accuracy and visual improvements against\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_Z/0/1/0/all/0/1\">Ziwei Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Youwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08227","description":"<p>Digitizing physical objects into the virtual world has the potential to\nunlock new research and applications in embodied AI and mixed reality. This\nwork focuses on recreating interactive digital twins of real-world articulated\nobjects, which can be directly imported into virtual environments. We introduce\nDitto to learn articulation model estimation and 3D geometry reconstruction of\nan articulated object through interactive perception. Given a pair of visual\nobservations of an articulated object before and after interaction, Ditto\nreconstructs part-level geometry and estimates the articulation model of the\nobject. We employ implicit neural representations for joint geometry and\narticulation modeling. Our experiments show that Ditto effectively builds\ndigital twins of articulated objects in a category-agnostic way. We also apply\nDitto to real-world objects and deploy the recreated digital twins in physical\nsimulation. Code and additional results are available at\nhttps://ut-austin-rpl.github.io/Ditto\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhenyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Cheng-Chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Review of Computer Vision in Sports: Open Issues, Future Trends and Research Directions. (arXiv:2203.02281v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02281","description":"<p>Recent developments in video analysis of sports and computer vision\ntechniques have achieved significant improvements to enable a variety of\ncritical operations. To provide enhanced information, such as detailed complex\nanalysis in sports like soccer, basketball, cricket, badminton, etc., studies\nhave focused mainly on computer vision techniques employed to carry out\ndifferent tasks. This paper presents a comprehensive review of sports video\nanalysis for various applications high-level analysis such as detection and\nclassification of players, tracking player or ball in sports and predicting the\ntrajectories of player or ball, recognizing the teams strategies, classifying\nvarious events in sports. The paper further discusses published works in a\nvariety of application-specific tasks related to sports and the present\nresearchers views regarding them. Since there is a wide research scope in\nsports for deploying computer vision techniques in various sports, some of the\npublicly available datasets related to a particular sport have been provided.\nThis work reviews a detailed discussion on some of the artificial\nintelligence(AI)applications in sports vision, GPU-based work stations, and\nembedded platforms. Finally, this review identifies the research directions,\nprobable challenges, and future trends in the area of visual recognition in\nsports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_B/0/1/0/all/0/1\">Banoth Thulasya Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashmi_M/0/1/0/all/0/1\">Mohammad Farukh Hashmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bokde_N/0/1/0/all/0/1\">Neeraj Dhanraj Bokde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon. (arXiv:2203.03818v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03818","description":"<p>Estimating the risk level of adversarial examples is essential for safely\ndeploying machine learning models in the real world. One popular approach for\nphysical-world attacks is to adopt the \"sticker-pasting\" strategy, which\nhowever suffers from some limitations, including difficulties in access to the\ntarget or printing by valid colors. A new type of non-invasive attacks emerged\nrecently, which attempt to cast perturbation onto the target by optics based\ntools, such as laser beam and projector. However, the added optical patterns\nare artificial but not natural. Thus, they are still conspicuous and\nattention-grabbed, and can be easily noticed by humans. In this paper, we study\na new type of optical adversarial examples, in which the perturbations are\ngenerated by a very common natural phenomenon, shadow, to achieve naturalistic\nand stealthy physical-world adversarial attack under the black-box setting. We\nextensively evaluate the effectiveness of this new attack on both simulated and\nreal-world environments. Experimental results on traffic sign recognition\ndemonstrate that our algorithm can generate adversarial examples effectively,\nreaching 98.23% and 90.47% success rates on LISA and GTSRB test sets\nrespectively, while continuously misleading a moving camera over 95% of the\ntime in real-world scenarios. We also offer discussions about the limitations\nand the defense mechanism of this attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Deming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms other\nstate-of-the-art (SOTA) methods by 28.36 in mIOU score with a large margin,\nwhich demonstrates the effectiveness of visual prompt and the text span\npredictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Memory Learning for Fine-Grained Scene Graph Generation. (arXiv:2203.06907v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06907","description":"<p>As far as Scene Graph Generation (SGG), coarse and fine predicates mix in the\ndataset due to the crowd-sourced labeling, and the long-tail problem is also\npronounced. Given this tricky situation, many existing SGG methods treat the\npredicates equally and learn the model under the supervision of\nmixed-granularity predicates in one stage, leading to relatively coarse\npredictions. In order to alleviate the negative impact of the suboptimum\nmixed-granularity annotation and long-tail effect problems, this paper proposes\na novel Hierarchical Memory Learning (HML) framework to learn the model from\nsimple to complex, which is similar to the human beings' hierarchical memory\nlearning process. After the autonomous partition of coarse and fine predicates,\nthe model is first trained on the coarse predicates and then learns the fine\npredicates. In order to realize this hierarchical learning pattern, this paper,\nfor the first time, formulates the HML framework using the new Concept\nReconstruction (CR) and Model Reconstruction (MR) constraints. It is worth\nnoticing that the HML framework can be taken as one general optimization\nstrategy to improve various SGG models, and significant improvement can be\nachieved on the SGG benchmark (i.e., Visual Genome).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Youming Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yansheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation. (arXiv:2203.07697v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07697","description":"<p>In this paper, we present a novel Distribution-Aware Single-stage (DAS) model\nfor tackling the challenging multi-person 3D pose estimation problem. Different\nfrom existing top-down and bottom-up methods, the proposed DAS model\nsimultaneously localizes person positions and their corresponding body joints\nin the 3D camera space in a one-pass manner. This leads to a simplified\npipeline with enhanced efficiency. In addition, DAS learns the true\ndistribution of body joints for the regression of their positions, rather than\nmaking a simple Laplacian or Gaussian assumption as previous works. This\nprovides valuable priors for model prediction and thus boosts the\nregression-based scheme to achieve competitive performance with volumetric-base\nones. Moreover, DAS exploits a recursive update strategy for progressively\napproaching to regression target, alleviating the optimization difficulty and\nfurther lifting the regression performance. DAS is implemented with a fully\nConvolutional Neural Network and end-to-end learnable. Comprehensive\nexperiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior\nefficiency of the proposed DAS model, specifically 1.5x speedup over previous\nbest model, and its stat-of-the-art accuracy for multi-person 3D pose\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zitian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xuecheng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaochao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsic Neural Fields: Learning Functions on Manifolds. (arXiv:2203.07967v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07967","description":"<p>Neural fields have gained significant attention in the computer vision\ncommunity due to their excellent performance in novel view synthesis, geometry\nreconstruction, and generative modeling. Some of their advantages are a sound\ntheoretic foundation and an easy implementation in current deep learning\nframeworks. While neural fields have been applied to signals on manifolds,\ne.g., for texture reconstruction, their representation has been limited to\nextrinsically embedding the shape into Euclidean space. The extrinsic embedding\nignores known intrinsic manifold properties and is inflexible wrt. transfer of\nthe learned function. To overcome these limitations, this work introduces\nintrinsic neural fields, a novel and versatile representation for neural fields\non manifolds. Intrinsic neural fields combine the advantages of neural fields\nwith the spectral properties of the Laplace-Beltrami operator. We show\ntheoretically that intrinsic neural fields inherit many desirable properties of\nthe extrinsic neural field framework but exhibit additional intrinsic\nqualities, like isometry invariance. In experiments, we show intrinsic neural\nfields can reconstruct high-fidelity textures from images with state-of-the-art\nquality and are robust to the discretization of the underlying manifold. We\ndemonstrate the versatility of intrinsic neural fields by tackling various\napplications: texture transfer between deformed shapes &amp; different shapes,\ntexture reconstruction from real-world images with view dependence, and\ndiscretization-agnostic learning on meshes and point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koestler_L/0/1/0/all/0/1\">Lukas Koestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grittner_D/0/1/0/all/0/1\">Daniel Grittner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1\">Zorah L&#xe4;hner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Shot Adaptation of GAN in Just One CLIP. (arXiv:2203.09301v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09301","description":"<p>There are many recent research efforts to fine-tune a pre-trained generator\nwith a few target images to generate images of a novel domain. Unfortunately,\nthese methods often suffer from overfitting or under-fitting when fine-tuned\nwith a single target image. To address this, here we present a novel\nsingle-shot GAN adaptation method through unified CLIP space manipulations.\nSpecifically, our model employs a two-step training strategy: reference image\nsearch in the source generator using a CLIP-guided latent optimization,\nfollowed by generator fine-tuning with a novel loss function that imposes CLIP\nspace consistency between the source and adapted generators. To further improve\nthe adapted model to produce spatially consistent samples with respect to the\nsource generator, we also propose contrastive regularization for patchwise\nrelationships in the CLIP space. Experimental results show that our model\ngenerates diverse outputs with the target texture and outperforms the baseline\nmodels both qualitatively and quantitatively. Furthermore, we show that our\nCLIP space manipulation strategy allows more effective attribute editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gihyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09550","description":"<p>Few-shot semantic segmentation aims at recognizing the object regions of\nunseen categories with only a few annotated examples as supervision. The key to\nfew-shot segmentation is to establish a robust semantic relationship between\nthe support and query images and to prevent overfitting. In this paper, we\npropose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle\nthe few-shot semantic segmentation problem. In MSHNet, we propose a new\nGenerative Prototype Similarity (GPS), which together with cosine similarity\ncan establish a strong semantic relation between the support and query images.\nThe locally generated prototype similarity based on global feature is logically\ncomplementary to the global cosine similarity based on local feature, and the\nrelationship between the query image and the supported image can be expressed\nmore comprehensively by using the two similarities simultaneously. In addition,\nwe propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge\nmulti-layer, multi-shot and multi-similarity hyperrelational features. MSHNet\nis built on the basis of similarity rather than specific category features,\nwhich can achieve more general unity and effectively reduce overfitting. On two\nbenchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet\nachieves new state-of-the-art performances on 1-shot and 5-shot semantic\nsegmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangwen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhe Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaobing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Miao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianghong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regional Semantic Contrast and Aggregation for Weakly Supervised Semantic Segmentation. (arXiv:2203.09653v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09653","description":"<p>Learning semantic segmentation from weakly-labeled (e.g., image tags only)\ndata is challenging since it is hard to infer dense object regions from sparse\nsemantic tags. Despite being broadly studied, most current efforts directly\nlearn from limited semantic annotations carried by individual image or image\npairs, and struggle to obtain integral localization maps. Our work alleviates\nthis from a novel perspective, by exploring rich semantic contexts\nsynergistically among abundant weakly-labeled training data for network\nlearning and inference. In particular, we propose regional semantic contrast\nand aggregation (RCA) . RCA is equipped with a regional memory bank to store\nmassive, diverse object patterns appearing in training data, which acts as\nstrong support for exploration of dataset-level semantic structure.\nParticularly, we propose i) semantic contrast to drive network learning by\ncontrasting massive categorical object regions, leading to a more holistic\nobject pattern understanding, and ii) semantic aggregation to gather diverse\nrelational contexts in the memory to enrich semantic representations. In this\nmanner, RCA earns a strong capability of fine-grained semantic understanding,\nand eventually establishes new state-of-the-art results on two popular\nbenchmarks, i.e., PASCAL VOC 2012 and COCO 2014.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion. (arXiv:2203.09855v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09855","description":"<p>In this paper, we formulate a potentially valuable panoramic depth completion\n(PDC) task as panoramic 3D cameras often produce 360{\\deg} depth with missing\ndata in complex scenes. Its goal is to recover dense panoramic depths from raw\nsparse ones and panoramic RGB images. To deal with the PDC task, we train a\ndeep network that takes both depth and image as inputs for the dense panoramic\ndepth recovery. However, it needs to face a challenging optimization problem of\nthe network parameters due to its non-convex objective function. To address\nthis problem, we propose a simple yet effective approach termed M{^3}PT:\nmulti-modal masked pre-training. Specifically, during pre-training, we\nsimultaneously cover up patches of the panoramic RGB image and sparse depth by\nshared random mask, then reconstruct the sparse depth in the masked regions. To\nour best knowledge, it is the first time that we show the effectiveness of\nmasked pre-training in a multi-modal vision task, instead of the single-modal\ntask resolved by masked autoencoders (MAE). Different from MAE where\nfine-tuning completely discards the decoder part of pre-training, there is no\narchitectural difference between the pre-training and fine-tuning stages in our\nM$^{3}$PT as they only differ in the prediction density, which potentially\nmakes the transfer learning more convenient and effective. Extensive\nexperiments verify the effectiveness of M{^3}PT on three panoramic datasets.\nNotably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,\n51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.\nCodes and pre-trained models are available at\nhttps://github.com/anonymoustbd/MMMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unidirectional Thin Adapter for Efficient Adaptation of Deep Neural Networks. (arXiv:2203.10463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10463","description":"<p>In this paper, we propose a new adapter network for adapting a pre-trained\ndeep neural network to a target domain with minimal computation. The proposed\nmodel, unidirectional thin adapter (UDTA), helps the classifier adapt to new\ndata by providing auxiliary features that complement the backbone network. UDTA\ntakes outputs from multiple layers of the backbone as input features but does\nnot transmit any feature to the backbone. As a result, UDTA can learn without\ncomputing the gradient of the backbone, which saves computation for training\nsignificantly. In addition, since UDTA learns the target task without modifying\nthe backbone, a single backbone can adapt to multiple tasks by learning only\nUDTAs separately. In experiments on five fine-grained classification datasets\nconsisting of a small number of samples, UDTA significantly reduced computation\nand training time required for backpropagation while showing comparable or even\nimproved accuracy compared with conventional adapter models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Han Gyel Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1\">Hyunjae Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">HyunGyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Injung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning. (arXiv:2203.11096v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11096","description":"<p>Gameplay videos contain rich information about how players interact with the\ngame and how the game responds. Sharing gameplay videos on social media\nplatforms, such as Reddit, has become a common practice for many players.\nOften, players will share gameplay videos that showcase video game bugs. Such\ngameplay videos are software artifacts that can be utilized for game testing,\nas they provide insight for bug analysis. Although large repositories of\ngameplay videos exist, parsing and mining them in an effective and structured\nfashion has still remained a big challenge. In this paper, we propose a search\nmethod that accepts any English text query as input to retrieve relevant videos\nfrom large repositories of gameplay videos. Our approach does not rely on any\nexternal information (such as video metadata); it works solely based on the\ncontent of the video. By leveraging the zero-shot transfer capabilities of the\nContrastive Language-Image Pre-Training (CLIP) model, our approach does not\nrequire any data labeling or training. To evaluate our approach, we present the\n$\\texttt{GamePhysics}$ dataset consisting of 26,954 videos from 1,873 games,\nthat were collected from the GamePhysics section on the Reddit website. Our\napproach shows promising results in our extensive analysis of simple queries,\ncompound queries, and bug queries, indicating that our approach is useful for\nobject and event detection in gameplay videos. An example application of our\napproach is as a gameplay video search engine to aid in reproducing video game\nbugs. Please visit the following link for the code and the data:\nhttps://asgaardlab.github.io/CLIPxGamePhysics/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taesiri_M/0/1/0/all/0/1\">Mohammad Reza Taesiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macklon_F/0/1/0/all/0/1\">Finlay Macklon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bezemer_C/0/1/0/all/0/1\">Cor-Paul Bezemer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static Models by Fitting Feature-level Space-time Surfaces. (arXiv:2203.11113v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11113","description":"<p>Scene flow is a powerful tool for capturing the motion field of 3D point\nclouds. However, it is difficult to directly apply flow-based models to dynamic\npoint cloud classification since the unstructured points make it hard or even\nimpossible to efficiently and effectively trace point-wise correspondences. To\ncapture 3D motions without explicitly tracking correspondences, we propose a\nkinematics-inspired neural network (Kinet) by generalizing the kinematic\nconcept of ST-surfaces to the feature space. By unrolling the normal solver of\nST-surfaces in the feature space, Kinet implicitly encodes feature-level\ndynamics and gains advantages from the use of mature backbones for static point\ncloud processing. With only minor changes in network structures and low\ncomputing overhead, it is painless to jointly train and deploy our framework\nwith a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,\nand NTU-RGBD demonstrate its efficacy in performance, efficiency in both the\nnumber of parameters and computational complexity, as well as its versatility\nto various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%\non MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jia-Xing Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaichen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel Self-Supervision for Online Knowledge Distillation. (arXiv:2203.11660v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11660","description":"<p>Recently, researchers have shown an increased interest in the online\nknowledge distillation. Adopting an one-stage and end-to-end training fashion,\nonline knowledge distillation uses aggregated intermediated predictions of\nmultiple peer models for training. However, the absence of a powerful teacher\nmodel may result in the homogeneity problem between group peers, affecting the\neffectiveness of group distillation adversely. In this paper, we propose a\nnovel online knowledge distillation method, \\textbf{C}hannel\n\\textbf{S}elf-\\textbf{S}upervision for Online Knowledge Distillation (CSS),\nwhich structures diversity in terms of input, target, and network to alleviate\nthe homogenization problem. Specifically, we construct a dual-network\nmulti-branch structure and enhance inter-branch diversity through\nself-supervised learning, adopting the feature-level transformation and\naugmenting the corresponding labels. Meanwhile, the dual network structure has\na larger space of independent parameters to resist the homogenization problem\nduring distillation. Extensive quantitative experiments on CIFAR-100 illustrate\nthat our method provides greater diversity than OKDDip and we also give pretty\nperformance improvement, even over the state-of-the-art such as PCL. The\nresults on three fine-grained datasets (StanfordDogs, StanfordCars,\nCUB-200-211) also show the significant generalization capability of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shixiao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_P/0/1/0/all/0/1\">Pan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiali Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring and Evaluating Image Restoration Potential in Dynamic Scenes. (arXiv:2203.11754v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11754","description":"<p>In dynamic scenes, images often suffer from dynamic blur due to superposition\nof motions or low signal-noise ratio resulted from quick shutter speed when\navoiding motions. Recovering sharp and clean results from the captured images\nheavily depends on the ability of restoration methods and the quality of the\ninput. Although existing research on image restoration focuses on developing\nmodels for obtaining better restored results, fewer have studied to evaluate\nhow and which input image leads to superior restored quality. In this paper, to\nbetter study an image's potential value that can be explored for restoration,\nwe propose a novel concept, referring to image restoration potential (IRP).\nSpecifically, We first establish a dynamic scene imaging dataset containing\ncomposite distortions and applied image restoration processes to validate the\nrationality of the existence to IRP. Based on this dataset, we investigate\nseveral properties of IRP and propose a novel deep model to accurately predict\nIRP values. By gradually distilling and selective fusing the degradation\nfeatures, the proposed model shows its superiority in IRP prediction. Thanks to\nthe proposed model, we are then able to validate how various image restoration\nrelated applications are benefited from IRP prediction. We show the potential\nusages of IRP as a filtering principle to select valuable frames, an auxiliary\nguidance to improve restoration models, and even an indicator to optimize\ncamera settings for capturing better images under dynamic scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shaolin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingsen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jinqiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}